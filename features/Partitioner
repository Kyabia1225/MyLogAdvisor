[ {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "getPartition",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int getPartition(KEY key, VALUE value, int numPartitions)",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest\\files",
  "methodName" : "setDir",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setDir(final String dir)\n{\r\n    this.dir = dir;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest\\files",
  "methodName" : "getDir",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getDir()\n{\r\n    return dir;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest\\files",
  "methodName" : "getDestPath",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Path getDestPath()\n{\r\n    return unmarshallPath(dir);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest\\files",
  "methodName" : "getType",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int getType()\n{\r\n    return type;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest\\files",
  "methodName" : "setType",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setType(final int type)\n{\r\n    this.type = type;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest\\files",
  "methodName" : "setLevel",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setLevel(final int level)\n{\r\n    this.level = level;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest\\files",
  "methodName" : "getLevel",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int getLevel()\n{\r\n    return level;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest\\files",
  "methodName" : "getStatus",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "EntryStatus getStatus()\n{\r\n    return EntryStatus.toEntryStatus(type);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest\\files",
  "methodName" : "setStatus",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setStatus(EntryStatus status)\n{\r\n    setType(status.ordinal());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest\\files",
  "methodName" : "validate",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void validate() throws IOException\n{\r\n    final String s = toString();\r\n    verify(dir != null && dir.length() > 0, \"destination path is missing from \" + s);\r\n    verify(type >= 0, \"Invalid type in \" + s);\r\n    verify(level >= 0, \"Invalid level in \" + s);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest\\files",
  "methodName" : "toString",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String toString()\n{\r\n    return \"DirEntry{\" + \"dir='\" + dir + '\\'' + \", type=\" + type + \", level=\" + level + '}';\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest\\files",
  "methodName" : "equals",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "boolean equals(final Object o)\n{\r\n    if (this == o) {\r\n        return true;\r\n    }\r\n    if (o == null || getClass() != o.getClass()) {\r\n        return false;\r\n    }\r\n    DirEntry dirEntry = (DirEntry) o;\r\n    return dir.equals(dirEntry.dir);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest\\files",
  "methodName" : "hashCode",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "int hashCode()\n{\r\n    return Objects.hash(dir);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest\\files",
  "methodName" : "dirEntry",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "DirEntry dirEntry(Path dest, int type, int level)\n{\r\n    return new DirEntry(dest, type, level);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest\\files",
  "methodName" : "dirEntry",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "DirEntry dirEntry(Path dest, EntryStatus type, int level)\n{\r\n    return dirEntry(dest, type.ordinal(), level);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred\\join",
  "methodName" : "emit",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "V emit(TupleWritable dst)\n{\r\n    return (V) dst.iterator().next();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred\\join",
  "methodName" : "fillJoinCollector",
  "errType" : null,
  "containingMethodsNum" : 17,
  "sourceCodeText" : "void fillJoinCollector(K iterkey) throws IOException\n{\r\n    final PriorityQueue<ComposableRecordReader<K, ?>> q = getRecordReaderQueue();\r\n    if (!q.isEmpty()) {\r\n        int highpos = -1;\r\n        ArrayList<ComposableRecordReader<K, ?>> list = new ArrayList<ComposableRecordReader<K, ?>>(kids.length);\r\n        q.peek().key(iterkey);\r\n        final WritableComparator cmp = getComparator();\r\n        while (0 == cmp.compare(q.peek().key(), iterkey)) {\r\n            ComposableRecordReader<K, ?> t = q.poll();\r\n            if (-1 == highpos || list.get(highpos).id() < t.id()) {\r\n                highpos = list.size();\r\n            }\r\n            list.add(t);\r\n            if (q.isEmpty())\r\n                break;\r\n        }\r\n        ComposableRecordReader<K, ?> t = list.remove(highpos);\r\n        t.accept(jc, iterkey);\r\n        for (ComposableRecordReader<K, ?> rr : list) {\r\n            rr.skip(iterkey);\r\n        }\r\n        list.add(t);\r\n        for (ComposableRecordReader<K, ?> rr : list) {\r\n            if (rr.hasNext()) {\r\n                q.add(rr);\r\n            }\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getJobConf",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "JobConf getJobConf()\n{\r\n    return job;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getProgressible",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Progressable getProgressible()\n{\r\n    return progress;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\input",
  "methodName" : "getKeyClass",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Class getKeyClass()\n{\r\n    return Text.class;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\input",
  "methodName" : "initialize",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void initialize(InputSplit genericSplit, TaskAttemptContext context) throws IOException\n{\r\n    lineRecordReader.initialize(genericSplit, context);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\input",
  "methodName" : "findSeparator",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int findSeparator(byte[] utf, int start, int length, byte sep)\n{\r\n    for (int i = start; i < (start + length); i++) {\r\n        if (utf[i] == sep) {\r\n            return i;\r\n        }\r\n    }\r\n    return -1;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\input",
  "methodName" : "setKeyValue",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void setKeyValue(Text key, Text value, byte[] line, int lineLen, int pos)\n{\r\n    if (pos == -1) {\r\n        key.set(line, 0, lineLen);\r\n        value.set(\"\");\r\n    } else {\r\n        key.set(line, 0, pos);\r\n        value.set(line, pos + 1, lineLen - pos - 1);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\input",
  "methodName" : "nextKeyValue",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "boolean nextKeyValue() throws IOException\n{\r\n    byte[] line = null;\r\n    int lineLen = -1;\r\n    if (lineRecordReader.nextKeyValue()) {\r\n        innerValue = lineRecordReader.getCurrentValue();\r\n        line = innerValue.getBytes();\r\n        lineLen = innerValue.getLength();\r\n    } else {\r\n        return false;\r\n    }\r\n    if (line == null)\r\n        return false;\r\n    if (key == null) {\r\n        key = new Text();\r\n    }\r\n    if (value == null) {\r\n        value = new Text();\r\n    }\r\n    int pos = findSeparator(line, 0, lineLen, this.separator);\r\n    setKeyValue(key, value, line, lineLen, pos);\r\n    return true;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\input",
  "methodName" : "getCurrentKey",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Text getCurrentKey()\n{\r\n    return key;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\input",
  "methodName" : "getCurrentValue",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Text getCurrentValue()\n{\r\n    return value;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\input",
  "methodName" : "getProgress",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "float getProgress() throws IOException\n{\r\n    return lineRecordReader.getProgress();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\input",
  "methodName" : "close",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void close() throws IOException\n{\r\n    lineRecordReader.close();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\chain",
  "methodName" : "addMapper",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void addMapper(Job job, Class<? extends Mapper> klass, Class<?> inputKeyClass, Class<?> inputValueClass, Class<?> outputKeyClass, Class<?> outputValueClass, Configuration mapperConf) throws IOException\n{\r\n    job.setMapperClass(ChainMapper.class);\r\n    job.setMapOutputKeyClass(outputKeyClass);\r\n    job.setMapOutputValueClass(outputValueClass);\r\n    Chain.addMapper(true, job, klass, inputKeyClass, inputValueClass, outputKeyClass, outputValueClass, mapperConf);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\chain",
  "methodName" : "setup",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setup(Context context)\n{\r\n    chain = new Chain(true);\r\n    chain.setup(context.getConfiguration());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\chain",
  "methodName" : "run",
  "errType" : null,
  "containingMethodsNum" : 10,
  "sourceCodeText" : "void run(Context context) throws IOException, InterruptedException\n{\r\n    setup(context);\r\n    int numMappers = chain.getAllMappers().size();\r\n    if (numMappers == 0) {\r\n        return;\r\n    }\r\n    ChainBlockingQueue<Chain.KeyValuePair<?, ?>> inputqueue;\r\n    ChainBlockingQueue<Chain.KeyValuePair<?, ?>> outputqueue;\r\n    if (numMappers == 1) {\r\n        chain.runMapper(context, 0);\r\n    } else {\r\n        outputqueue = chain.createBlockingQueue();\r\n        chain.addMapper(context, outputqueue, 0);\r\n        for (int i = 1; i < numMappers - 1; i++) {\r\n            inputqueue = outputqueue;\r\n            outputqueue = chain.createBlockingQueue();\r\n            chain.addMapper(inputqueue, outputqueue, context, i);\r\n        }\r\n        chain.addMapper(outputqueue, context, numMappers - 1);\r\n    }\r\n    chain.startAllThreads();\r\n    chain.joinAllThreads();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getMasterAddress",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "String getMasterAddress(Configuration conf)\n{\r\n    String masterAddress = conf.get(MRConfig.MASTER_ADDRESS, \"localhost:8012\");\r\n    return NetUtils.createSocketAddr(masterAddress, 8012, MRConfig.MASTER_ADDRESS).getHostName();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getMasterPrincipal",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "String getMasterPrincipal(Configuration conf) throws IOException\n{\r\n    String masterPrincipal;\r\n    String framework = conf.get(MRConfig.FRAMEWORK_NAME, MRConfig.YARN_FRAMEWORK_NAME);\r\n    if (framework.equals(MRConfig.CLASSIC_FRAMEWORK_NAME)) {\r\n        String masterAddress = getMasterAddress(conf);\r\n        masterPrincipal = SecurityUtil.getServerPrincipal(conf.get(MRConfig.MASTER_USER_NAME), masterAddress);\r\n    } else {\r\n        masterPrincipal = YarnClientUtils.getRmPrincipal(conf);\r\n    }\r\n    return masterPrincipal;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\task\\reduce",
  "methodName" : "readFields",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void readFields(DataInput in) throws IOException\n{\r\n    mapId = WritableUtils.readStringSafely(in, MAX_ID_LENGTH);\r\n    compressedLength = WritableUtils.readVLong(in);\r\n    uncompressedLength = WritableUtils.readVLong(in);\r\n    forReduce = WritableUtils.readVInt(in);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\task\\reduce",
  "methodName" : "write",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void write(DataOutput out) throws IOException\n{\r\n    Text.writeString(out, mapId);\r\n    WritableUtils.writeVLong(out, compressedLength);\r\n    WritableUtils.writeVLong(out, uncompressedLength);\r\n    WritableUtils.writeVInt(out, forReduce);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "init",
  "errType" : null,
  "containingMethodsNum" : 9,
  "sourceCodeText" : "void init(Configuration conf)\n{\r\n    if (!MRConfig.YARN_FRAMEWORK_NAME.equals(conf.get(MRConfig.FRAMEWORK_NAME))) {\r\n        return;\r\n    }\r\n    if (!conf.getBoolean(YarnConfiguration.SHARED_CACHE_ENABLED, YarnConfiguration.DEFAULT_SHARED_CACHE_ENABLED)) {\r\n        return;\r\n    }\r\n    Collection<String> configs = StringUtils.getTrimmedStringCollection(conf.get(MRJobConfig.SHARED_CACHE_MODE, MRJobConfig.SHARED_CACHE_MODE_DEFAULT));\r\n    if (configs.contains(\"files\")) {\r\n        this.sharedCacheFilesEnabled = true;\r\n    }\r\n    if (configs.contains(\"libjars\")) {\r\n        this.sharedCacheLibjarsEnabled = true;\r\n    }\r\n    if (configs.contains(\"archives\")) {\r\n        this.sharedCacheArchivesEnabled = true;\r\n    }\r\n    if (configs.contains(\"jobjar\")) {\r\n        this.sharedCacheJobjarEnabled = true;\r\n    }\r\n    if (configs.contains(\"enabled\")) {\r\n        this.sharedCacheFilesEnabled = true;\r\n        this.sharedCacheLibjarsEnabled = true;\r\n        this.sharedCacheArchivesEnabled = true;\r\n        this.sharedCacheJobjarEnabled = true;\r\n    }\r\n    if (configs.contains(\"disabled\")) {\r\n        this.sharedCacheFilesEnabled = false;\r\n        this.sharedCacheLibjarsEnabled = false;\r\n        this.sharedCacheArchivesEnabled = false;\r\n        this.sharedCacheJobjarEnabled = false;\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "isSharedCacheFilesEnabled",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean isSharedCacheFilesEnabled()\n{\r\n    return sharedCacheFilesEnabled;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "isSharedCacheLibjarsEnabled",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean isSharedCacheLibjarsEnabled()\n{\r\n    return sharedCacheLibjarsEnabled;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "isSharedCacheArchivesEnabled",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean isSharedCacheArchivesEnabled()\n{\r\n    return sharedCacheArchivesEnabled;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "isSharedCacheJobjarEnabled",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean isSharedCacheJobjarEnabled()\n{\r\n    return sharedCacheJobjarEnabled;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "isSharedCacheEnabled",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean isSharedCacheEnabled()\n{\r\n    return (sharedCacheFilesEnabled || sharedCacheLibjarsEnabled || sharedCacheArchivesEnabled || sharedCacheJobjarEnabled);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\db",
  "methodName" : "split",
  "errType" : null,
  "containingMethodsNum" : 16,
  "sourceCodeText" : "List<InputSplit> split(Configuration conf, ResultSet results, String colName) throws SQLException\n{\r\n    long minVal = results.getLong(1);\r\n    long maxVal = results.getLong(2);\r\n    String lowClausePrefix = colName + \" >= \";\r\n    String highClausePrefix = colName + \" < \";\r\n    int numSplits = conf.getInt(MRJobConfig.NUM_MAPS, 1);\r\n    if (numSplits < 1) {\r\n        numSplits = 1;\r\n    }\r\n    if (results.getString(1) == null && results.getString(2) == null) {\r\n        List<InputSplit> splits = new ArrayList<InputSplit>();\r\n        splits.add(new DataDrivenDBInputFormat.DataDrivenDBInputSplit(colName + \" IS NULL\", colName + \" IS NULL\"));\r\n        return splits;\r\n    }\r\n    List<Long> splitPoints = split(numSplits, minVal, maxVal);\r\n    List<InputSplit> splits = new ArrayList<InputSplit>();\r\n    long start = splitPoints.get(0);\r\n    for (int i = 1; i < splitPoints.size(); i++) {\r\n        long end = splitPoints.get(i);\r\n        if (i == splitPoints.size() - 1) {\r\n            splits.add(new DataDrivenDBInputFormat.DataDrivenDBInputSplit(lowClausePrefix + Long.toString(start), colName + \" <= \" + Long.toString(end)));\r\n        } else {\r\n            splits.add(new DataDrivenDBInputFormat.DataDrivenDBInputSplit(lowClausePrefix + Long.toString(start), highClausePrefix + Long.toString(end)));\r\n        }\r\n        start = end;\r\n    }\r\n    if (results.getString(1) == null || results.getString(2) == null) {\r\n        splits.add(new DataDrivenDBInputFormat.DataDrivenDBInputSplit(colName + \" IS NULL\", colName + \" IS NULL\"));\r\n    }\r\n    return splits;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\db",
  "methodName" : "split",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "List<Long> split(long numSplits, long minVal, long maxVal) throws SQLException\n{\r\n    List<Long> splits = new ArrayList<Long>();\r\n    long splitSize = (maxVal - minVal) / numSplits;\r\n    if (splitSize < 1) {\r\n        splitSize = 1;\r\n    }\r\n    long curVal = minVal;\r\n    while (curVal <= maxVal) {\r\n        splits.add(curVal);\r\n        curVal += splitSize;\r\n    }\r\n    if (splits.get(splits.size() - 1) != maxVal || splits.size() == 1) {\r\n        splits.add(maxVal);\r\n    }\r\n    return splits;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest\\stages",
  "methodName" : "executeStage",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "Path executeStage(final Boolean deleteMarker) throws IOException\n{\r\n    final Path path = getJobAttemptDir();\r\n    LOG.info(\"{}: Creating Job Attempt directory {}\", getName(), path);\r\n    createNewDirectory(\"Job setup\", path);\r\n    createNewDirectory(\"Creating task manifest dir\", getTaskManifestDir());\r\n    if (deleteMarker) {\r\n        delete(getStageConfig().getJobSuccessMarkerPath(), false);\r\n    }\r\n    return path;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "cloneContext",
  "errType" : [ "InstantiationException", "IllegalAccessException", "InvocationTargetException" ],
  "containingMethodsNum" : 3,
  "sourceCodeText" : "JobContext cloneContext(JobContext original, Configuration conf) throws IOException, InterruptedException\n{\r\n    try {\r\n        if (original instanceof MapContext<?, ?, ?, ?>) {\r\n            return cloneMapContext((Mapper.Context) original, conf, null, null);\r\n        } else if (original instanceof ReduceContext<?, ?, ?, ?>) {\r\n            throw new IllegalArgumentException(\"can't clone ReduceContext\");\r\n        } else if (original instanceof TaskAttemptContext) {\r\n            TaskAttemptContext spec = (TaskAttemptContext) original;\r\n            return (JobContext) TASK_CONTEXT_CONSTRUCTOR.newInstance(conf, spec.getTaskAttemptID());\r\n        } else {\r\n            return (JobContext) JOB_CONTEXT_CONSTRUCTOR.newInstance(conf, original.getJobID());\r\n        }\r\n    } catch (InstantiationException e) {\r\n        throw new IllegalArgumentException(\"Can't clone object\", e);\r\n    } catch (IllegalAccessException e) {\r\n        throw new IllegalArgumentException(\"Can't clone object\", e);\r\n    } catch (InvocationTargetException e) {\r\n        throw new IllegalArgumentException(\"Can't clone object\", e);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "cloneMapContext",
  "errType" : [ "IllegalAccessException", "InstantiationException", "InvocationTargetException" ],
  "containingMethodsNum" : 8,
  "sourceCodeText" : "Mapper<K1, V1, K2, V2>.Context cloneMapContext(MapContext<K1, V1, K2, V2> context, Configuration conf, RecordReader<K1, V1> reader, RecordWriter<K2, V2> writer) throws IOException, InterruptedException\n{\r\n    try {\r\n        Object outer = OUTER_MAP_FIELD.get(context);\r\n        if (\"org.apache.hadoop.mapreduce.lib.map.WrappedMapper$Context\".equals(context.getClass().getName())) {\r\n            context = (MapContext<K1, V1, K2, V2>) WRAPPED_CONTEXT_FIELD.get(context);\r\n        }\r\n        if (reader == null) {\r\n            reader = (RecordReader<K1, V1>) READER_FIELD.get(context);\r\n        }\r\n        if (writer == null) {\r\n            writer = (RecordWriter<K2, V2>) WRITER_FIELD.get(context);\r\n        }\r\n        if (useV21) {\r\n            Object basis = MAP_CONTEXT_IMPL_CONSTRUCTOR.newInstance(conf, context.getTaskAttemptID(), reader, writer, context.getOutputCommitter(), REPORTER_FIELD.get(context), context.getInputSplit());\r\n            return (Mapper.Context) MAP_CONTEXT_CONSTRUCTOR.newInstance(outer, basis);\r\n        } else {\r\n            return (Mapper.Context) MAP_CONTEXT_CONSTRUCTOR.newInstance(outer, conf, context.getTaskAttemptID(), reader, writer, context.getOutputCommitter(), REPORTER_FIELD.get(context), context.getInputSplit());\r\n        }\r\n    } catch (IllegalAccessException e) {\r\n        throw new IllegalArgumentException(\"Can't access field\", e);\r\n    } catch (InstantiationException e) {\r\n        throw new IllegalArgumentException(\"Can't create object\", e);\r\n    } catch (InvocationTargetException e) {\r\n        throw new IllegalArgumentException(\"Can't invoke constructor\", e);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "init",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void init(JobConf conf) throws IOException\n{\r\n    setConf(conf);\r\n    cluster = new Cluster(conf);\r\n    clientUgi = UserGroupInformation.getCurrentUser();\r\n    maxRetry = conf.getInt(MRJobConfig.MR_CLIENT_JOB_MAX_RETRIES, MRJobConfig.DEFAULT_MR_CLIENT_JOB_MAX_RETRIES);\r\n    retryInterval = conf.getLong(MRJobConfig.MR_CLIENT_JOB_RETRY_INTERVAL, MRJobConfig.DEFAULT_MR_CLIENT_JOB_RETRY_INTERVAL);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "close",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void close() throws IOException\n{\r\n    cluster.close();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getFs",
  "errType" : [ "InterruptedException" ],
  "containingMethodsNum" : 1,
  "sourceCodeText" : "FileSystem getFs() throws IOException\n{\r\n    try {\r\n        return cluster.getFileSystem();\r\n    } catch (InterruptedException ie) {\r\n        throw new IOException(ie);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getClusterHandle",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Cluster getClusterHandle()\n{\r\n    return cluster;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "submitJob",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "RunningJob submitJob(String jobFile) throws FileNotFoundException, InvalidJobConfException, IOException\n{\r\n    JobConf job = new JobConf(jobFile);\r\n    return submitJob(job);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "submitJob",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "RunningJob submitJob(final JobConf conf) throws FileNotFoundException, IOException\n{\r\n    return submitJobInternal(conf);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "submitJobInternal",
  "errType" : [ "InterruptedException" ],
  "containingMethodsNum" : 5,
  "sourceCodeText" : "RunningJob submitJobInternal(final JobConf conf) throws FileNotFoundException, IOException\n{\r\n    try {\r\n        conf.setBooleanIfUnset(\"mapred.mapper.new-api\", false);\r\n        conf.setBooleanIfUnset(\"mapred.reducer.new-api\", false);\r\n        Job job = clientUgi.doAs(new PrivilegedExceptionAction<Job>() {\r\n\r\n            @Override\r\n            public Job run() throws IOException, ClassNotFoundException, InterruptedException {\r\n                Job job = Job.getInstance(conf);\r\n                job.submit();\r\n                return job;\r\n            }\r\n        });\r\n        Cluster prev = cluster;\r\n        cluster = job.getCluster();\r\n        if (prev != null) {\r\n            prev.close();\r\n        }\r\n        return new NetworkedJob(job);\r\n    } catch (InterruptedException ie) {\r\n        throw new IOException(\"interrupted\", ie);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getJobUsingCluster",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Job getJobUsingCluster(final JobID jobid) throws IOException, InterruptedException\n{\r\n    return clientUgi.doAs(new PrivilegedExceptionAction<Job>() {\r\n\r\n        public Job run() throws IOException, InterruptedException {\r\n            return cluster.getJob(jobid);\r\n        }\r\n    });\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getJobInner",
  "errType" : [ "InterruptedException" ],
  "containingMethodsNum" : 3,
  "sourceCodeText" : "RunningJob getJobInner(final JobID jobid) throws IOException\n{\r\n    try {\r\n        Job job = getJobUsingCluster(jobid);\r\n        if (job != null) {\r\n            JobStatus status = JobStatus.downgrade(job.getStatus());\r\n            if (status != null) {\r\n                return new NetworkedJob(status, cluster, new JobConf(job.getConfiguration()));\r\n            }\r\n        }\r\n    } catch (InterruptedException ie) {\r\n        throw new IOException(ie);\r\n    }\r\n    return null;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getJob",
  "errType" : [ "Exception" ],
  "containingMethodsNum" : 2,
  "sourceCodeText" : "RunningJob getJob(final JobID jobid) throws IOException\n{\r\n    for (int i = 0; i <= maxRetry; i++) {\r\n        if (i > 0) {\r\n            try {\r\n                Thread.sleep(retryInterval);\r\n            } catch (Exception e) {\r\n            }\r\n        }\r\n        RunningJob job = getJobInner(jobid);\r\n        if (job != null) {\r\n            return job;\r\n        }\r\n    }\r\n    return null;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getJob",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "RunningJob getJob(String jobid) throws IOException\n{\r\n    return getJob(JobID.forName(jobid));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getMapTaskReports",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "TaskReport[] getMapTaskReports(JobID jobId) throws IOException\n{\r\n    return getTaskReports(jobId, TaskType.MAP);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getTaskReports",
  "errType" : [ "InterruptedException" ],
  "containingMethodsNum" : 2,
  "sourceCodeText" : "TaskReport[] getTaskReports(final JobID jobId, TaskType type) throws IOException\n{\r\n    try {\r\n        Job j = getJobUsingCluster(jobId);\r\n        if (j == null) {\r\n            return EMPTY_TASK_REPORTS;\r\n        }\r\n        return TaskReport.downgradeArray(j.getTaskReports(type));\r\n    } catch (InterruptedException ie) {\r\n        throw new IOException(ie);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getMapTaskReports",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "TaskReport[] getMapTaskReports(String jobId) throws IOException\n{\r\n    return getMapTaskReports(JobID.forName(jobId));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getReduceTaskReports",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "TaskReport[] getReduceTaskReports(JobID jobId) throws IOException\n{\r\n    return getTaskReports(jobId, TaskType.REDUCE);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getCleanupTaskReports",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "TaskReport[] getCleanupTaskReports(JobID jobId) throws IOException\n{\r\n    return getTaskReports(jobId, TaskType.JOB_CLEANUP);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getSetupTaskReports",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "TaskReport[] getSetupTaskReports(JobID jobId) throws IOException\n{\r\n    return getTaskReports(jobId, TaskType.JOB_SETUP);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getReduceTaskReports",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "TaskReport[] getReduceTaskReports(String jobId) throws IOException\n{\r\n    return getReduceTaskReports(JobID.forName(jobId));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "displayTasks",
  "errType" : [ "InterruptedException" ],
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void displayTasks(final JobID jobId, String type, String state) throws IOException\n{\r\n    try {\r\n        Job job = getJobUsingCluster(jobId);\r\n        super.displayTasks(job, type, state);\r\n    } catch (InterruptedException ie) {\r\n        throw new IOException(ie);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getClusterStatus",
  "errType" : [ "InterruptedException" ],
  "containingMethodsNum" : 1,
  "sourceCodeText" : "ClusterStatus getClusterStatus() throws IOException\n{\r\n    try {\r\n        return clientUgi.doAs(new PrivilegedExceptionAction<ClusterStatus>() {\r\n\r\n            public ClusterStatus run() throws IOException, InterruptedException {\r\n                ClusterMetrics metrics = cluster.getClusterStatus();\r\n                return new ClusterStatus(metrics.getTaskTrackerCount(), metrics.getBlackListedTaskTrackerCount(), cluster.getTaskTrackerExpiryInterval(), metrics.getOccupiedMapSlots(), metrics.getOccupiedReduceSlots(), metrics.getMapSlotCapacity(), metrics.getReduceSlotCapacity(), cluster.getJobTrackerStatus(), metrics.getDecommissionedTaskTrackerCount(), metrics.getGrayListedTaskTrackerCount());\r\n            }\r\n        });\r\n    } catch (InterruptedException ie) {\r\n        throw new IOException(ie);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "arrayToStringList",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Collection<String> arrayToStringList(TaskTrackerInfo[] objs)\n{\r\n    Collection<String> list = new ArrayList<String>();\r\n    for (TaskTrackerInfo info : objs) {\r\n        list.add(info.getTaskTrackerName());\r\n    }\r\n    return list;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "arrayToBlackListInfo",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "Collection<BlackListInfo> arrayToBlackListInfo(TaskTrackerInfo[] objs)\n{\r\n    Collection<BlackListInfo> list = new ArrayList<BlackListInfo>();\r\n    for (TaskTrackerInfo info : objs) {\r\n        BlackListInfo binfo = new BlackListInfo();\r\n        binfo.setTrackerName(info.getTaskTrackerName());\r\n        binfo.setReasonForBlackListing(info.getReasonForBlacklist());\r\n        binfo.setBlackListReport(info.getBlacklistReport());\r\n        list.add(binfo);\r\n    }\r\n    return list;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getClusterStatus",
  "errType" : [ "InterruptedException" ],
  "containingMethodsNum" : 1,
  "sourceCodeText" : "ClusterStatus getClusterStatus(boolean detailed) throws IOException\n{\r\n    try {\r\n        return clientUgi.doAs(new PrivilegedExceptionAction<ClusterStatus>() {\r\n\r\n            public ClusterStatus run() throws IOException, InterruptedException {\r\n                ClusterMetrics metrics = cluster.getClusterStatus();\r\n                return new ClusterStatus(arrayToStringList(cluster.getActiveTaskTrackers()), arrayToBlackListInfo(cluster.getBlackListedTaskTrackers()), cluster.getTaskTrackerExpiryInterval(), metrics.getOccupiedMapSlots(), metrics.getOccupiedReduceSlots(), metrics.getMapSlotCapacity(), metrics.getReduceSlotCapacity(), cluster.getJobTrackerStatus());\r\n            }\r\n        });\r\n    } catch (InterruptedException ie) {\r\n        throw new IOException(ie);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "jobsToComplete",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "JobStatus[] jobsToComplete() throws IOException\n{\r\n    List<JobStatus> stats = new ArrayList<JobStatus>();\r\n    for (JobStatus stat : getAllJobs()) {\r\n        if (!stat.isJobComplete()) {\r\n            stats.add(stat);\r\n        }\r\n    }\r\n    return stats.toArray(new JobStatus[0]);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getAllJobs",
  "errType" : [ "InterruptedException" ],
  "containingMethodsNum" : 2,
  "sourceCodeText" : "JobStatus[] getAllJobs() throws IOException\n{\r\n    try {\r\n        org.apache.hadoop.mapreduce.JobStatus[] jobs = clientUgi.doAs(new PrivilegedExceptionAction<org.apache.hadoop.mapreduce.JobStatus[]>() {\r\n\r\n            public org.apache.hadoop.mapreduce.JobStatus[] run() throws IOException, InterruptedException {\r\n                return cluster.getAllJobStatuses();\r\n            }\r\n        });\r\n        JobStatus[] stats = new JobStatus[jobs.length];\r\n        for (int i = 0; i < jobs.length; i++) {\r\n            stats[i] = JobStatus.downgrade(jobs[i]);\r\n        }\r\n        return stats;\r\n    } catch (InterruptedException ie) {\r\n        throw new IOException(ie);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "runJob",
  "errType" : [ "InterruptedException" ],
  "containingMethodsNum" : 3,
  "sourceCodeText" : "RunningJob runJob(JobConf job) throws IOException\n{\r\n    JobClient jc = new JobClient(job);\r\n    RunningJob rj = jc.submitJob(job);\r\n    try {\r\n        if (!jc.monitorAndPrintJob(job, rj)) {\r\n            throw new IOException(\"Job failed!\");\r\n        }\r\n    } catch (InterruptedException ie) {\r\n        Thread.currentThread().interrupt();\r\n    }\r\n    return rj;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "monitorAndPrintJob",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "boolean monitorAndPrintJob(JobConf conf, RunningJob job) throws IOException, InterruptedException\n{\r\n    return ((NetworkedJob) job).monitorAndPrintJob();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getTaskLogURL",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getTaskLogURL(TaskAttemptID taskId, String baseUrl)\n{\r\n    return (baseUrl + \"/tasklog?plaintext=true&attemptid=\" + taskId);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getConfiguration",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "Configuration getConfiguration(String jobTrackerSpec)\n{\r\n    Configuration conf = new Configuration();\r\n    if (jobTrackerSpec != null) {\r\n        if (jobTrackerSpec.indexOf(\":\") >= 0) {\r\n            conf.set(\"mapred.job.tracker\", jobTrackerSpec);\r\n        } else {\r\n            String classpathFile = \"hadoop-\" + jobTrackerSpec + \".xml\";\r\n            URL validate = conf.getResource(classpathFile);\r\n            if (validate == null) {\r\n                throw new RuntimeException(classpathFile + \" not found on CLASSPATH\");\r\n            }\r\n            conf.addResource(classpathFile);\r\n        }\r\n    }\r\n    return conf;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "setTaskOutputFilter",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setTaskOutputFilter(TaskStatusFilter newValue)\n{\r\n    this.taskOutputFilter = newValue;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getTaskOutputFilter",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "TaskStatusFilter getTaskOutputFilter(JobConf job)\n{\r\n    return TaskStatusFilter.valueOf(job.get(\"jobclient.output.filter\", \"FAILED\"));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "setTaskOutputFilter",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setTaskOutputFilter(JobConf job, TaskStatusFilter newValue)\n{\r\n    job.set(\"jobclient.output.filter\", newValue.toString());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getTaskOutputFilter",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "TaskStatusFilter getTaskOutputFilter()\n{\r\n    return this.taskOutputFilter;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getCounter",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "long getCounter(org.apache.hadoop.mapreduce.Counters cntrs, String counterGroupName, String counterName) throws IOException\n{\r\n    Counters counters = Counters.downgrade(cntrs);\r\n    return counters.findCounter(counterGroupName, counterName).getValue();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getDefaultMaps",
  "errType" : [ "InterruptedException" ],
  "containingMethodsNum" : 1,
  "sourceCodeText" : "int getDefaultMaps() throws IOException\n{\r\n    try {\r\n        return clientUgi.doAs(new PrivilegedExceptionAction<Integer>() {\r\n\r\n            @Override\r\n            public Integer run() throws IOException, InterruptedException {\r\n                return cluster.getClusterStatus().getMapSlotCapacity();\r\n            }\r\n        });\r\n    } catch (InterruptedException ie) {\r\n        throw new IOException(ie);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getDefaultReduces",
  "errType" : [ "InterruptedException" ],
  "containingMethodsNum" : 1,
  "sourceCodeText" : "int getDefaultReduces() throws IOException\n{\r\n    try {\r\n        return clientUgi.doAs(new PrivilegedExceptionAction<Integer>() {\r\n\r\n            @Override\r\n            public Integer run() throws IOException, InterruptedException {\r\n                return cluster.getClusterStatus().getReduceSlotCapacity();\r\n            }\r\n        });\r\n    } catch (InterruptedException ie) {\r\n        throw new IOException(ie);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getSystemDir",
  "errType" : [ "IOException", "InterruptedException" ],
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Path getSystemDir()\n{\r\n    try {\r\n        return clientUgi.doAs(new PrivilegedExceptionAction<Path>() {\r\n\r\n            @Override\r\n            public Path run() throws IOException, InterruptedException {\r\n                return cluster.getSystemDir();\r\n            }\r\n        });\r\n    } catch (IOException ioe) {\r\n        return null;\r\n    } catch (InterruptedException ie) {\r\n        return null;\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "isJobDirValid",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "boolean isJobDirValid(Path jobDirPath, FileSystem fs) throws IOException\n{\r\n    FileStatus[] contents = fs.listStatus(jobDirPath);\r\n    int matchCount = 0;\r\n    if (contents != null && contents.length >= 2) {\r\n        for (FileStatus status : contents) {\r\n            if (\"job.xml\".equals(status.getPath().getName())) {\r\n                ++matchCount;\r\n            }\r\n            if (\"job.split\".equals(status.getPath().getName())) {\r\n                ++matchCount;\r\n            }\r\n        }\r\n        if (matchCount == 2) {\r\n            return true;\r\n        }\r\n    }\r\n    return false;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getStagingAreaDir",
  "errType" : [ "InterruptedException" ],
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Path getStagingAreaDir() throws IOException\n{\r\n    try {\r\n        return clientUgi.doAs(new PrivilegedExceptionAction<Path>() {\r\n\r\n            @Override\r\n            public Path run() throws IOException, InterruptedException {\r\n                return cluster.getStagingAreaDir();\r\n            }\r\n        });\r\n    } catch (InterruptedException ie) {\r\n        throw new RuntimeException(ie);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getJobQueueInfo",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "JobQueueInfo getJobQueueInfo(QueueInfo queue)\n{\r\n    JobQueueInfo ret = new JobQueueInfo(queue);\r\n    if (queue.getQueueChildren().size() > 0) {\r\n        List<JobQueueInfo> childQueues = new ArrayList<JobQueueInfo>(queue.getQueueChildren().size());\r\n        for (QueueInfo child : queue.getQueueChildren()) {\r\n            childQueues.add(getJobQueueInfo(child));\r\n        }\r\n        ret.setChildren(childQueues);\r\n    }\r\n    return ret;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getJobQueueInfoArray",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "JobQueueInfo[] getJobQueueInfoArray(QueueInfo[] queues) throws IOException\n{\r\n    JobQueueInfo[] ret = new JobQueueInfo[queues.length];\r\n    for (int i = 0; i < queues.length; i++) {\r\n        ret[i] = getJobQueueInfo(queues[i]);\r\n    }\r\n    return ret;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getRootQueues",
  "errType" : [ "InterruptedException" ],
  "containingMethodsNum" : 1,
  "sourceCodeText" : "JobQueueInfo[] getRootQueues() throws IOException\n{\r\n    try {\r\n        return clientUgi.doAs(new PrivilegedExceptionAction<JobQueueInfo[]>() {\r\n\r\n            public JobQueueInfo[] run() throws IOException, InterruptedException {\r\n                return getJobQueueInfoArray(cluster.getRootQueues());\r\n            }\r\n        });\r\n    } catch (InterruptedException ie) {\r\n        throw new IOException(ie);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getChildQueues",
  "errType" : [ "InterruptedException" ],
  "containingMethodsNum" : 1,
  "sourceCodeText" : "JobQueueInfo[] getChildQueues(final String queueName) throws IOException\n{\r\n    try {\r\n        return clientUgi.doAs(new PrivilegedExceptionAction<JobQueueInfo[]>() {\r\n\r\n            public JobQueueInfo[] run() throws IOException, InterruptedException {\r\n                return getJobQueueInfoArray(cluster.getChildQueues(queueName));\r\n            }\r\n        });\r\n    } catch (InterruptedException ie) {\r\n        throw new IOException(ie);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getQueues",
  "errType" : [ "InterruptedException" ],
  "containingMethodsNum" : 1,
  "sourceCodeText" : "JobQueueInfo[] getQueues() throws IOException\n{\r\n    try {\r\n        return clientUgi.doAs(new PrivilegedExceptionAction<JobQueueInfo[]>() {\r\n\r\n            public JobQueueInfo[] run() throws IOException, InterruptedException {\r\n                return getJobQueueInfoArray(cluster.getQueues());\r\n            }\r\n        });\r\n    } catch (InterruptedException ie) {\r\n        throw new IOException(ie);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getJobsFromQueue",
  "errType" : [ "InterruptedException" ],
  "containingMethodsNum" : 3,
  "sourceCodeText" : "JobStatus[] getJobsFromQueue(final String queueName) throws IOException\n{\r\n    try {\r\n        QueueInfo queue = clientUgi.doAs(new PrivilegedExceptionAction<QueueInfo>() {\r\n\r\n            @Override\r\n            public QueueInfo run() throws IOException, InterruptedException {\r\n                return cluster.getQueue(queueName);\r\n            }\r\n        });\r\n        if (queue == null) {\r\n            return null;\r\n        }\r\n        org.apache.hadoop.mapreduce.JobStatus[] stats = queue.getJobStatuses();\r\n        JobStatus[] ret = new JobStatus[stats.length];\r\n        for (int i = 0; i < stats.length; i++) {\r\n            ret[i] = JobStatus.downgrade(stats[i]);\r\n        }\r\n        return ret;\r\n    } catch (InterruptedException ie) {\r\n        throw new IOException(ie);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getQueueInfo",
  "errType" : [ "InterruptedException" ],
  "containingMethodsNum" : 1,
  "sourceCodeText" : "JobQueueInfo getQueueInfo(final String queueName) throws IOException\n{\r\n    try {\r\n        QueueInfo queueInfo = clientUgi.doAs(new PrivilegedExceptionAction<QueueInfo>() {\r\n\r\n            public QueueInfo run() throws IOException, InterruptedException {\r\n                return cluster.getQueue(queueName);\r\n            }\r\n        });\r\n        if (queueInfo != null) {\r\n            return new JobQueueInfo(queueInfo);\r\n        }\r\n        return null;\r\n    } catch (InterruptedException ie) {\r\n        throw new IOException(ie);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getQueueAclsForCurrentUser",
  "errType" : [ "InterruptedException" ],
  "containingMethodsNum" : 2,
  "sourceCodeText" : "QueueAclsInfo[] getQueueAclsForCurrentUser() throws IOException\n{\r\n    try {\r\n        org.apache.hadoop.mapreduce.QueueAclsInfo[] acls = clientUgi.doAs(new PrivilegedExceptionAction<org.apache.hadoop.mapreduce.QueueAclsInfo[]>() {\r\n\r\n            public org.apache.hadoop.mapreduce.QueueAclsInfo[] run() throws IOException, InterruptedException {\r\n                return cluster.getQueueAclsForCurrentUser();\r\n            }\r\n        });\r\n        QueueAclsInfo[] ret = new QueueAclsInfo[acls.length];\r\n        for (int i = 0; i < acls.length; i++) {\r\n            ret[i] = QueueAclsInfo.downgrade(acls[i]);\r\n        }\r\n        return ret;\r\n    } catch (InterruptedException ie) {\r\n        throw new IOException(ie);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getDelegationToken",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Token<DelegationTokenIdentifier> getDelegationToken(final Text renewer) throws IOException, InterruptedException\n{\r\n    return clientUgi.doAs(new PrivilegedExceptionAction<Token<DelegationTokenIdentifier>>() {\r\n\r\n        public Token<DelegationTokenIdentifier> run() throws IOException, InterruptedException {\r\n            return cluster.getDelegationToken(renewer);\r\n        }\r\n    });\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "renewDelegationToken",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "long renewDelegationToken(Token<DelegationTokenIdentifier> token) throws InvalidToken, IOException, InterruptedException\n{\r\n    return token.renew(getConf());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "cancelDelegationToken",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void cancelDelegationToken(Token<DelegationTokenIdentifier> token) throws InvalidToken, IOException, InterruptedException\n{\r\n    token.cancel(getConf());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "main",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void main(String[] argv) throws Exception\n{\r\n    int res = ToolRunner.run(new JobClient(), argv);\r\n    System.exit(res);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output",
  "methodName" : "createOutputCommitter",
  "errType" : [ "NoSuchMethodException|InstantiationException|IllegalAccessException|InvocationTargetException" ],
  "containingMethodsNum" : 4,
  "sourceCodeText" : "PathOutputCommitter createOutputCommitter(Path outputPath, TaskAttemptContext context) throws IOException\n{\r\n    Class<? extends PathOutputCommitter> clazz = loadCommitterClass(context);\r\n    LOG.debug(\"Using PathOutputCommitter implementation {}\", clazz);\r\n    try {\r\n        Constructor<? extends PathOutputCommitter> ctor = clazz.getConstructor(Path.class, TaskAttemptContext.class);\r\n        return ctor.newInstance(outputPath, context);\r\n    } catch (NoSuchMethodException | InstantiationException | IllegalAccessException | InvocationTargetException e) {\r\n        throw new IOException(\"Failed to create \" + clazz + \":\" + e, e);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output",
  "methodName" : "loadCommitterClass",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "Class<? extends PathOutputCommitter> loadCommitterClass(JobContext context) throws IOException\n{\r\n    Preconditions.checkNotNull(context, \"null context\");\r\n    Configuration conf = context.getConfiguration();\r\n    String value = conf.get(NAMED_COMMITTER_CLASS, \"\");\r\n    if (value.isEmpty()) {\r\n        throw new IOException(\"No committer defined in \" + NAMED_COMMITTER_CLASS);\r\n    }\r\n    return conf.getClass(NAMED_COMMITTER_CLASS, FileOutputCommitter.class, PathOutputCommitter.class);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest\\stages",
  "methodName" : "executeStage",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "LoadManifestsStage.Result executeStage(final Boolean prune) throws IOException\n{\r\n    final Path manifestDir = getTaskManifestDir();\r\n    LOG.info(\"{}: Executing Manifest Job Commit with manifests in {}\", getName(), manifestDir);\r\n    pruneManifests = prune;\r\n    msync(manifestDir);\r\n    final RemoteIterator<FileStatus> manifestFiles = listManifests();\r\n    final List<TaskManifest> manifestList = loadAllManifests(manifestFiles);\r\n    LOG.info(\"{}: Summary of {} manifests loaded in {}: {}\", getName(), manifestList.size(), manifestDir, summaryInfo);\r\n    maybeAddIOStatistics(getIOStatistics(), manifestFiles);\r\n    return new LoadManifestsStage.Result(summaryInfo, manifestList);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest\\stages",
  "methodName" : "loadAllManifests",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "List<TaskManifest> loadAllManifests(final RemoteIterator<FileStatus> manifestFiles) throws IOException\n{\r\n    trackDurationOfInvocation(getIOStatistics(), OP_LOAD_ALL_MANIFESTS, () -> TaskPool.foreach(manifestFiles).executeWith(getIOProcessors()).stopOnFailure().run(this::processOneManifest));\r\n    return manifests;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest\\stages",
  "methodName" : "processOneManifest",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void processOneManifest(FileStatus status) throws IOException\n{\r\n    updateAuditContext(OP_LOAD_ALL_MANIFESTS);\r\n    TaskManifest m = fetchTaskManifest(status);\r\n    progress();\r\n    synchronized (manifests) {\r\n        manifests.add(m);\r\n        summaryInfo.add(m);\r\n    }\r\n    if (pruneManifests) {\r\n        m.setIOStatistics(null);\r\n        m.getExtraData().clear();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest\\stages",
  "methodName" : "fetchTaskManifest",
  "errType" : null,
  "containingMethodsNum" : 9,
  "sourceCodeText" : "TaskManifest fetchTaskManifest(FileStatus status) throws IOException\n{\r\n    if (status.getLen() == 0 || !status.isFile()) {\r\n        throw new PathIOException(status.getPath().toString(), \"Not a valid manifest file; file status = \" + status);\r\n    }\r\n    final TaskManifest manifest = loadManifest(status);\r\n    final String id = manifest.getTaskAttemptID();\r\n    final int filecount = manifest.getFilesToCommit().size();\r\n    final long size = manifest.getTotalFileSize();\r\n    LOG.info(\"{}: Task Attempt {} file {}: File count: {}; data size={}\", getName(), id, status.getPath(), filecount, size);\r\n    getIOStatistics().addMeanStatisticSample(COMMITTER_TASK_MANIFEST_FILE_SIZE, status.getLen());\r\n    return manifest;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\map",
  "methodName" : "map",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void map(K key, V value, Context context) throws IOException, InterruptedException\n{\r\n    context.write(value, key);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\chain",
  "methodName" : "setReducer",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void setReducer(Job job, Class<? extends Reducer> klass, Class<?> inputKeyClass, Class<?> inputValueClass, Class<?> outputKeyClass, Class<?> outputValueClass, Configuration reducerConf)\n{\r\n    job.setReducerClass(ChainReducer.class);\r\n    job.setOutputKeyClass(outputKeyClass);\r\n    job.setOutputValueClass(outputValueClass);\r\n    Chain.setReducer(job, klass, inputKeyClass, inputValueClass, outputKeyClass, outputValueClass, reducerConf);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\chain",
  "methodName" : "addMapper",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void addMapper(Job job, Class<? extends Mapper> klass, Class<?> inputKeyClass, Class<?> inputValueClass, Class<?> outputKeyClass, Class<?> outputValueClass, Configuration mapperConf) throws IOException\n{\r\n    job.setOutputKeyClass(outputKeyClass);\r\n    job.setOutputValueClass(outputValueClass);\r\n    Chain.addMapper(false, job, klass, inputKeyClass, inputValueClass, outputKeyClass, outputValueClass, mapperConf);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\chain",
  "methodName" : "setup",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setup(Context context)\n{\r\n    chain = new Chain(false);\r\n    chain.setup(context.getConfiguration());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\chain",
  "methodName" : "run",
  "errType" : null,
  "containingMethodsNum" : 11,
  "sourceCodeText" : "void run(Context context) throws IOException, InterruptedException\n{\r\n    setup(context);\r\n    if (chain.getReducer() == null) {\r\n        return;\r\n    }\r\n    int numMappers = chain.getAllMappers().size();\r\n    if (numMappers == 0) {\r\n        chain.runReducer(context);\r\n        return;\r\n    }\r\n    ChainBlockingQueue<Chain.KeyValuePair<?, ?>> inputqueue;\r\n    ChainBlockingQueue<Chain.KeyValuePair<?, ?>> outputqueue;\r\n    outputqueue = chain.createBlockingQueue();\r\n    chain.addReducer(context, outputqueue);\r\n    for (int i = 0; i < numMappers - 1; i++) {\r\n        inputqueue = outputqueue;\r\n        outputqueue = chain.createBlockingQueue();\r\n        chain.addMapper(inputqueue, outputqueue, context, i);\r\n    }\r\n    chain.addMapper(outputqueue, context, numMappers - 1);\r\n    chain.startAllThreads();\r\n    chain.joinAllThreads();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "getCounter",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Counter getCounter(Enum<?> name)",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "getCounter",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Counter getCounter(String group, String name)",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "progress",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void progress()",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "getProgress",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "float getProgress()",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "setStatus",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setStatus(String status)",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\aggregate",
  "methodName" : "addNextValue",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void addNextValue(Object val)\n{\r\n    String valCountStr = val.toString();\r\n    int pos = valCountStr.lastIndexOf(\"\\t\");\r\n    String valStr = valCountStr;\r\n    String countStr = \"1\";\r\n    if (pos >= 0) {\r\n        valStr = valCountStr.substring(0, pos);\r\n        countStr = valCountStr.substring(pos + 1);\r\n    }\r\n    Long count = (Long) this.items.get(valStr);\r\n    long inc = Long.parseLong(countStr);\r\n    if (count == null) {\r\n        count = inc;\r\n    } else {\r\n        count = count.longValue() + inc;\r\n    }\r\n    items.put(valStr, count);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\aggregate",
  "methodName" : "getReport",
  "errType" : null,
  "containingMethodsNum" : 14,
  "sourceCodeText" : "String getReport()\n{\r\n    long[] counts = new long[items.size()];\r\n    StringBuffer sb = new StringBuffer();\r\n    Iterator<Object> iter = items.values().iterator();\r\n    int i = 0;\r\n    while (iter.hasNext()) {\r\n        Long count = (Long) iter.next();\r\n        counts[i] = count.longValue();\r\n        i += 1;\r\n    }\r\n    Arrays.sort(counts);\r\n    sb.append(counts.length);\r\n    i = 0;\r\n    long acc = 0;\r\n    while (i < counts.length) {\r\n        long nextVal = counts[i];\r\n        int j = i + 1;\r\n        while (j < counts.length && counts[j] == nextVal) {\r\n            j++;\r\n        }\r\n        acc += nextVal * (j - i);\r\n        i = j;\r\n    }\r\n    double average = 0.0;\r\n    double sd = 0.0;\r\n    if (counts.length > 0) {\r\n        sb.append(\"\\t\").append(counts[0]);\r\n        sb.append(\"\\t\").append(counts[counts.length / 2]);\r\n        sb.append(\"\\t\").append(counts[counts.length - 1]);\r\n        average = acc * 1.0 / counts.length;\r\n        sb.append(\"\\t\").append(average);\r\n        i = 0;\r\n        while (i < counts.length) {\r\n            double nextDiff = counts[i] - average;\r\n            sd += nextDiff * nextDiff;\r\n            i += 1;\r\n        }\r\n        sd = Math.sqrt(sd / counts.length);\r\n        sb.append(\"\\t\").append(sd);\r\n    }\r\n    return sb.toString();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\aggregate",
  "methodName" : "getReportDetails",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "String getReportDetails()\n{\r\n    StringBuffer sb = new StringBuffer();\r\n    Iterator<Entry<Object, Object>> iter = items.entrySet().iterator();\r\n    while (iter.hasNext()) {\r\n        Entry<Object, Object> en = iter.next();\r\n        Object val = en.getKey();\r\n        Long count = (Long) en.getValue();\r\n        sb.append(\"\\t\").append(val.toString()).append(\"\\t\").append(count.longValue()).append(\"\\n\");\r\n    }\r\n    return sb.toString();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\aggregate",
  "methodName" : "getCombinerOutput",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "ArrayList<String> getCombinerOutput()\n{\r\n    ArrayList<String> retv = new ArrayList<String>();\r\n    Iterator<Entry<Object, Object>> iter = items.entrySet().iterator();\r\n    while (iter.hasNext()) {\r\n        Entry<Object, Object> en = iter.next();\r\n        Object val = en.getKey();\r\n        Long count = (Long) en.getValue();\r\n        retv.add(val.toString() + \"\\t\" + count.longValue());\r\n    }\r\n    return retv;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\aggregate",
  "methodName" : "getReportItems",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "TreeMap<Object, Object> getReportItems()\n{\r\n    return items;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\aggregate",
  "methodName" : "reset",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void reset()\n{\r\n    items = new TreeMap<Object, Object>();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "createQueues",
  "errType" : [ "Throwable" ],
  "containingMethodsNum" : 5,
  "sourceCodeText" : "List<Queue> createQueues(Configuration conf)\n{\r\n    String[] queueNameValues = conf.getStrings(MAPRED_QUEUE_NAMES_KEY);\r\n    List<Queue> list = new ArrayList<Queue>();\r\n    for (String name : queueNameValues) {\r\n        try {\r\n            Map<String, AccessControlList> acls = getQueueAcls(name, conf);\r\n            QueueState state = getQueueState(name, conf);\r\n            Queue q = new Queue(name, acls, state);\r\n            list.add(q);\r\n        } catch (Throwable t) {\r\n            LOG.warn(\"Not able to initialize queue \" + name);\r\n        }\r\n    }\r\n    return list;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getQueueState",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "QueueState getQueueState(String name, Configuration conf)\n{\r\n    String stateVal = conf.get(toFullPropertyName(name, \"state\"), QueueState.RUNNING.getStateName());\r\n    return QueueState.getState(stateVal);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "deprecatedConf",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "boolean deprecatedConf(Configuration conf)\n{\r\n    String[] queues = null;\r\n    String queueNameValues = getQueueNames(conf);\r\n    if (queueNameValues == null) {\r\n        return false;\r\n    } else {\r\n        LOG.warn(\"Configuring \\\"\" + MAPRED_QUEUE_NAMES_KEY + \"\\\" in mapred-site.xml or \" + \"hadoop-site.xml is deprecated and will overshadow \" + QUEUE_CONF_FILE_NAME + \". Remove this property and configure \" + \"queue hierarchy in \" + QUEUE_CONF_FILE_NAME);\r\n        queues = conf.getStrings(MAPRED_QUEUE_NAMES_KEY);\r\n    }\r\n    if (queues != null) {\r\n        for (String queue : queues) {\r\n            for (QueueACL qAcl : QueueACL.values()) {\r\n                String key = toFullPropertyName(queue, qAcl.getAclName());\r\n                String aclString = conf.get(key);\r\n                if (aclString != null) {\r\n                    LOG.warn(\"Configuring queue ACLs in mapred-site.xml or \" + \"hadoop-site.xml is deprecated. Configure queue ACLs in \" + QUEUE_CONF_FILE_NAME);\r\n                    return true;\r\n                }\r\n            }\r\n        }\r\n    }\r\n    return true;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getQueueNames",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String getQueueNames(Configuration conf)\n{\r\n    String queueNameValues = conf.get(MAPRED_QUEUE_NAMES_KEY);\r\n    return queueNameValues;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getQueueAcls",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "Map<String, AccessControlList> getQueueAcls(String name, Configuration conf)\n{\r\n    HashMap<String, AccessControlList> map = new HashMap<String, AccessControlList>();\r\n    for (QueueACL qAcl : QueueACL.values()) {\r\n        String aclKey = toFullPropertyName(name, qAcl.getAclName());\r\n        map.put(aclKey, new AccessControlList(conf.get(aclKey, \"*\")));\r\n    }\r\n    return map;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\map",
  "methodName" : "getMapContext",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Mapper<KEYIN, VALUEIN, KEYOUT, VALUEOUT>.Context getMapContext(MapContext<KEYIN, VALUEIN, KEYOUT, VALUEOUT> mapContext)\n{\r\n    return new Context(mapContext);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\checkpoint",
  "methodName" : "write",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void write(DataOutput out) throws IOException\n{\r\n    counters.write(out);\r\n    WritableUtils.writeVLong(out, partialOutput.size());\r\n    for (Path p : partialOutput) {\r\n        Text.writeString(out, p.toString());\r\n    }\r\n    rawId.write(out);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\checkpoint",
  "methodName" : "readFields",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void readFields(DataInput in) throws IOException\n{\r\n    partialOutput.clear();\r\n    counters.readFields(in);\r\n    long numPout = WritableUtils.readVLong(in);\r\n    for (int i = 0; i < numPout; i++) {\r\n        partialOutput.add(new Path(Text.readString(in)));\r\n    }\r\n    rawId.readFields(in);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\checkpoint",
  "methodName" : "equals",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "boolean equals(Object other)\n{\r\n    if (other instanceof TaskCheckpointID) {\r\n        TaskCheckpointID o = (TaskCheckpointID) other;\r\n        return rawId.equals(o.rawId) && counters.equals(o.counters) && partialOutput.containsAll(o.partialOutput) && o.partialOutput.containsAll(partialOutput);\r\n    }\r\n    return false;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\checkpoint",
  "methodName" : "hashCode",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "int hashCode()\n{\r\n    return rawId.hashCode();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\checkpoint",
  "methodName" : "getCheckpointBytes",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "long getCheckpointBytes()\n{\r\n    return counters.findCounter(EnumCounter.CHECKPOINT_BYTES).getValue();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\checkpoint",
  "methodName" : "getCheckpointTime",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "long getCheckpointTime()\n{\r\n    return counters.findCounter(EnumCounter.CHECKPOINT_MS).getValue();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\checkpoint",
  "methodName" : "toString",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String toString()\n{\r\n    return rawId.toString() + \" counters:\" + counters;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\checkpoint",
  "methodName" : "getPartialCommittedOutput",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "List<Path> getPartialCommittedOutput()\n{\r\n    return partialOutput;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\checkpoint",
  "methodName" : "getCounters",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Counters getCounters()\n{\r\n    return counters;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred\\lib\\db",
  "methodName" : "configure",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void configure(JobConf job)\n{\r\n    super.setConf(job);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred\\lib\\db",
  "methodName" : "getRecordReader",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "RecordReader<LongWritable, T> getRecordReader(InputSplit split, JobConf job, Reporter reporter) throws IOException\n{\r\n    return new DBRecordReaderWrapper<T>((org.apache.hadoop.mapreduce.lib.db.DBRecordReader<T>) createDBRecordReader((org.apache.hadoop.mapreduce.lib.db.DBInputFormat.DBInputSplit) split, job));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred\\lib\\db",
  "methodName" : "getSplits",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "InputSplit[] getSplits(JobConf job, int chunks) throws IOException\n{\r\n    List<org.apache.hadoop.mapreduce.InputSplit> newSplits = super.getSplits(Job.getInstance(job));\r\n    InputSplit[] ret = new InputSplit[newSplits.size()];\r\n    int i = 0;\r\n    for (org.apache.hadoop.mapreduce.InputSplit s : newSplits) {\r\n        org.apache.hadoop.mapreduce.lib.db.DBInputFormat.DBInputSplit split = (org.apache.hadoop.mapreduce.lib.db.DBInputFormat.DBInputSplit) s;\r\n        ret[i++] = new DBInputSplit(split.getStart(), split.getEnd());\r\n    }\r\n    return ret;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred\\lib\\db",
  "methodName" : "setInput",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void setInput(JobConf job, Class<? extends DBWritable> inputClass, String tableName, String conditions, String orderBy, String... fieldNames)\n{\r\n    job.setInputFormat(DBInputFormat.class);\r\n    DBConfiguration dbConf = new DBConfiguration(job);\r\n    dbConf.setInputClass(inputClass);\r\n    dbConf.setInputTableName(tableName);\r\n    dbConf.setInputFieldNames(fieldNames);\r\n    dbConf.setInputConditions(conditions);\r\n    dbConf.setInputOrderBy(orderBy);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred\\lib\\db",
  "methodName" : "setInput",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void setInput(JobConf job, Class<? extends DBWritable> inputClass, String inputQuery, String inputCountQuery)\n{\r\n    job.setInputFormat(DBInputFormat.class);\r\n    DBConfiguration dbConf = new DBConfiguration(job);\r\n    dbConf.setInputClass(inputClass);\r\n    dbConf.setInputQuery(inputQuery);\r\n    dbConf.setInputCountQuery(inputCountQuery);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\util",
  "methodName" : "getTaskLogUrl",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getTaskLogUrl(String scheme, String taskTrackerHostName, String httpPort, String taskAttemptID)\n{\r\n    return (scheme + taskTrackerHostName + \":\" + httpPort + \"/tasklog?attemptid=\" + taskAttemptID);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\util",
  "methodName" : "getTaskLogUrl",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getTaskLogUrl(String taskTrackerHostName, String httpPort, String taskAttemptID)\n{\r\n    throw new RuntimeException(\"This method is not supposed to be called at runtime. \" + \"Use HostUtil.getTaskLogUrl(String, String, String, String) instead.\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\util",
  "methodName" : "convertTrackerNameToHostName",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "String convertTrackerNameToHostName(String trackerName)\n{\r\n    int indexOfColon = trackerName.indexOf(\":\");\r\n    String trackerHostName = (indexOfColon == -1) ? trackerName : trackerName.substring(0, indexOfColon);\r\n    return trackerHostName.substring(\"tracker_\".length());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\chain",
  "methodName" : "getThrowable",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Throwable getThrowable()\n{\r\n    return throwable;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\chain",
  "methodName" : "setIfUnsetThrowable",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean setIfUnsetThrowable(Throwable th)\n{\r\n    if (throwable == null) {\r\n        throwable = th;\r\n        return true;\r\n    }\r\n    return false;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\chain",
  "methodName" : "getConf",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Configuration getConf(int index)\n{\r\n    return confList.get(index);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\chain",
  "methodName" : "createMapContext",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Mapper<KEYIN, VALUEIN, KEYOUT, VALUEOUT>.Context createMapContext(RecordReader<KEYIN, VALUEIN> rr, RecordWriter<KEYOUT, VALUEOUT> rw, TaskInputOutputContext<KEYIN, VALUEIN, KEYOUT, VALUEOUT> context, Configuration conf)\n{\r\n    MapContext<KEYIN, VALUEIN, KEYOUT, VALUEOUT> mapContext = new ChainMapContextImpl<KEYIN, VALUEIN, KEYOUT, VALUEOUT>(context, rr, rw, conf);\r\n    Mapper<KEYIN, VALUEIN, KEYOUT, VALUEOUT>.Context mapperContext = new WrappedMapper<KEYIN, VALUEIN, KEYOUT, VALUEOUT>().getMapContext(mapContext);\r\n    return mapperContext;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\chain",
  "methodName" : "runMapper",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void runMapper(TaskInputOutputContext context, int index) throws IOException, InterruptedException\n{\r\n    Mapper mapper = mappers.get(index);\r\n    RecordReader rr = new ChainRecordReader(context);\r\n    RecordWriter rw = new ChainRecordWriter(context);\r\n    Mapper.Context mapperContext = createMapContext(rr, rw, context, getConf(index));\r\n    mapper.run(mapperContext);\r\n    rr.close();\r\n    rw.close(context);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\chain",
  "methodName" : "addMapper",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void addMapper(TaskInputOutputContext inputContext, ChainBlockingQueue<KeyValuePair<?, ?>> output, int index) throws IOException, InterruptedException\n{\r\n    Configuration conf = getConf(index);\r\n    Class<?> keyOutClass = conf.getClass(MAPPER_OUTPUT_KEY_CLASS, Object.class);\r\n    Class<?> valueOutClass = conf.getClass(MAPPER_OUTPUT_VALUE_CLASS, Object.class);\r\n    RecordReader rr = new ChainRecordReader(inputContext);\r\n    RecordWriter rw = new ChainRecordWriter(keyOutClass, valueOutClass, output, conf);\r\n    Mapper.Context mapperContext = createMapContext(rr, rw, (MapContext) inputContext, getConf(index));\r\n    MapRunner runner = new MapRunner(mappers.get(index), mapperContext, rr, rw);\r\n    threads.add(runner);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\chain",
  "methodName" : "addMapper",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void addMapper(ChainBlockingQueue<KeyValuePair<?, ?>> input, TaskInputOutputContext outputContext, int index) throws IOException, InterruptedException\n{\r\n    Configuration conf = getConf(index);\r\n    Class<?> keyClass = conf.getClass(MAPPER_INPUT_KEY_CLASS, Object.class);\r\n    Class<?> valueClass = conf.getClass(MAPPER_INPUT_VALUE_CLASS, Object.class);\r\n    RecordReader rr = new ChainRecordReader(keyClass, valueClass, input, conf);\r\n    RecordWriter rw = new ChainRecordWriter(outputContext);\r\n    MapRunner runner = new MapRunner(mappers.get(index), createMapContext(rr, rw, outputContext, getConf(index)), rr, rw);\r\n    threads.add(runner);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\chain",
  "methodName" : "addMapper",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void addMapper(ChainBlockingQueue<KeyValuePair<?, ?>> input, ChainBlockingQueue<KeyValuePair<?, ?>> output, TaskInputOutputContext context, int index) throws IOException, InterruptedException\n{\r\n    Configuration conf = getConf(index);\r\n    Class<?> keyClass = conf.getClass(MAPPER_INPUT_KEY_CLASS, Object.class);\r\n    Class<?> valueClass = conf.getClass(MAPPER_INPUT_VALUE_CLASS, Object.class);\r\n    Class<?> keyOutClass = conf.getClass(MAPPER_OUTPUT_KEY_CLASS, Object.class);\r\n    Class<?> valueOutClass = conf.getClass(MAPPER_OUTPUT_VALUE_CLASS, Object.class);\r\n    RecordReader rr = new ChainRecordReader(keyClass, valueClass, input, conf);\r\n    RecordWriter rw = new ChainRecordWriter(keyOutClass, valueOutClass, output, conf);\r\n    MapRunner runner = new MapRunner(mappers.get(index), createMapContext(rr, rw, context, getConf(index)), rr, rw);\r\n    threads.add(runner);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\chain",
  "methodName" : "createReduceContext",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Reducer<KEYIN, VALUEIN, KEYOUT, VALUEOUT>.Context createReduceContext(RecordWriter<KEYOUT, VALUEOUT> rw, ReduceContext<KEYIN, VALUEIN, KEYOUT, VALUEOUT> context, Configuration conf)\n{\r\n    ReduceContext<KEYIN, VALUEIN, KEYOUT, VALUEOUT> reduceContext = new ChainReduceContextImpl<KEYIN, VALUEIN, KEYOUT, VALUEOUT>(context, rw, conf);\r\n    Reducer<KEYIN, VALUEIN, KEYOUT, VALUEOUT>.Context reducerContext = new WrappedReducer<KEYIN, VALUEIN, KEYOUT, VALUEOUT>().getReducerContext(reduceContext);\r\n    return reducerContext;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\chain",
  "methodName" : "runReducer",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void runReducer(TaskInputOutputContext<KEYIN, VALUEIN, KEYOUT, VALUEOUT> context) throws IOException, InterruptedException\n{\r\n    RecordWriter<KEYOUT, VALUEOUT> rw = new ChainRecordWriter<KEYOUT, VALUEOUT>(context);\r\n    Reducer.Context reducerContext = createReduceContext(rw, (ReduceContext) context, rConf);\r\n    reducer.run(reducerContext);\r\n    rw.close(context);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\chain",
  "methodName" : "addReducer",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void addReducer(TaskInputOutputContext inputContext, ChainBlockingQueue<KeyValuePair<?, ?>> outputQueue) throws IOException, InterruptedException\n{\r\n    Class<?> keyOutClass = rConf.getClass(REDUCER_OUTPUT_KEY_CLASS, Object.class);\r\n    Class<?> valueOutClass = rConf.getClass(REDUCER_OUTPUT_VALUE_CLASS, Object.class);\r\n    RecordWriter rw = new ChainRecordWriter(keyOutClass, valueOutClass, outputQueue, rConf);\r\n    Reducer.Context reducerContext = createReduceContext(rw, (ReduceContext) inputContext, rConf);\r\n    ReduceRunner runner = new ReduceRunner(reducerContext, reducer, rw);\r\n    threads.add(runner);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\chain",
  "methodName" : "startAllThreads",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void startAllThreads()\n{\r\n    for (Thread thread : threads) {\r\n        thread.start();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\chain",
  "methodName" : "joinAllThreads",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void joinAllThreads() throws IOException, InterruptedException\n{\r\n    for (Thread thread : threads) {\r\n        thread.join();\r\n    }\r\n    Throwable th = getThrowable();\r\n    if (th != null) {\r\n        if (th instanceof IOException) {\r\n            throw (IOException) th;\r\n        } else if (th instanceof InterruptedException) {\r\n            throw (InterruptedException) th;\r\n        } else {\r\n            throw new RuntimeException(th);\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\chain",
  "methodName" : "interruptAllThreads",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void interruptAllThreads()\n{\r\n    for (Thread th : threads) {\r\n        th.interrupt();\r\n    }\r\n    for (ChainBlockingQueue<?> queue : blockingQueues) {\r\n        queue.interrupt();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\chain",
  "methodName" : "getPrefix",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getPrefix(boolean isMap)\n{\r\n    return (isMap) ? CHAIN_MAPPER : CHAIN_REDUCER;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\chain",
  "methodName" : "getIndex",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "int getIndex(Configuration conf, String prefix)\n{\r\n    return conf.getInt(prefix + CHAIN_MAPPER_SIZE, 0);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\chain",
  "methodName" : "getChainElementConf",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 3,
  "sourceCodeText" : "Configuration getChainElementConf(Configuration jobConf, String confKey)\n{\r\n    Configuration conf = null;\r\n    try (Stringifier<Configuration> stringifier = new DefaultStringifier<Configuration>(jobConf, Configuration.class)) {\r\n        String confString = jobConf.get(confKey, null);\r\n        if (confString != null) {\r\n            conf = stringifier.fromString(jobConf.get(confKey, null));\r\n        }\r\n    } catch (IOException ioex) {\r\n        throw new RuntimeException(ioex);\r\n    }\r\n    jobConf = new Configuration(jobConf);\r\n    if (conf != null) {\r\n        for (Map.Entry<String, String> entry : conf) {\r\n            jobConf.set(entry.getKey(), entry.getValue());\r\n        }\r\n    }\r\n    return jobConf;\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\chain",
  "methodName" : "addMapper",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void addMapper(boolean isMap, Job job, Class<? extends Mapper> klass, Class<?> inputKeyClass, Class<?> inputValueClass, Class<?> outputKeyClass, Class<?> outputValueClass, Configuration mapperConf)\n{\r\n    String prefix = getPrefix(isMap);\r\n    Configuration jobConf = job.getConfiguration();\r\n    checkReducerAlreadySet(isMap, jobConf, prefix, true);\r\n    int index = getIndex(jobConf, prefix);\r\n    jobConf.setClass(prefix + CHAIN_MAPPER_CLASS + index, klass, Mapper.class);\r\n    validateKeyValueTypes(isMap, jobConf, inputKeyClass, inputValueClass, outputKeyClass, outputValueClass, index, prefix);\r\n    setMapperConf(isMap, jobConf, inputKeyClass, inputValueClass, outputKeyClass, outputValueClass, mapperConf, index, prefix);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\chain",
  "methodName" : "checkReducerAlreadySet",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void checkReducerAlreadySet(boolean isMap, Configuration jobConf, String prefix, boolean shouldSet)\n{\r\n    if (!isMap) {\r\n        if (shouldSet) {\r\n            if (jobConf.getClass(prefix + CHAIN_REDUCER_CLASS, null) == null) {\r\n                throw new IllegalStateException(\"A Mapper can be added to the chain only after the Reducer has \" + \"been set\");\r\n            }\r\n        } else {\r\n            if (jobConf.getClass(prefix + CHAIN_REDUCER_CLASS, null) != null) {\r\n                throw new IllegalStateException(\"Reducer has been already set\");\r\n            }\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\chain",
  "methodName" : "validateKeyValueTypes",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void validateKeyValueTypes(boolean isMap, Configuration jobConf, Class<?> inputKeyClass, Class<?> inputValueClass, Class<?> outputKeyClass, Class<?> outputValueClass, int index, String prefix)\n{\r\n    if (!isMap && index == 0) {\r\n        Configuration reducerConf = getChainElementConf(jobConf, prefix + CHAIN_REDUCER_CONFIG);\r\n        if (!inputKeyClass.isAssignableFrom(reducerConf.getClass(REDUCER_OUTPUT_KEY_CLASS, null))) {\r\n            throw new IllegalArgumentException(\"The Reducer output key class does\" + \" not match the Mapper input key class\");\r\n        }\r\n        if (!inputValueClass.isAssignableFrom(reducerConf.getClass(REDUCER_OUTPUT_VALUE_CLASS, null))) {\r\n            throw new IllegalArgumentException(\"The Reducer output value class\" + \" does not match the Mapper input value class\");\r\n        }\r\n    } else if (index > 0) {\r\n        Configuration previousMapperConf = getChainElementConf(jobConf, prefix + CHAIN_MAPPER_CONFIG + (index - 1));\r\n        if (!inputKeyClass.isAssignableFrom(previousMapperConf.getClass(MAPPER_OUTPUT_KEY_CLASS, null))) {\r\n            throw new IllegalArgumentException(\"The specified Mapper input key class does\" + \" not match the previous Mapper's output key class.\");\r\n        }\r\n        if (!inputValueClass.isAssignableFrom(previousMapperConf.getClass(MAPPER_OUTPUT_VALUE_CLASS, null))) {\r\n            throw new IllegalArgumentException(\"The specified Mapper input value class\" + \" does not match the previous Mapper's output value class.\");\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\chain",
  "methodName" : "setMapperConf",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void setMapperConf(boolean isMap, Configuration jobConf, Class<?> inputKeyClass, Class<?> inputValueClass, Class<?> outputKeyClass, Class<?> outputValueClass, Configuration mapperConf, int index, String prefix)\n{\r\n    if (mapperConf == null) {\r\n        mapperConf = new Configuration(true);\r\n    }\r\n    mapperConf.setClass(MAPPER_INPUT_KEY_CLASS, inputKeyClass, Object.class);\r\n    mapperConf.setClass(MAPPER_INPUT_VALUE_CLASS, inputValueClass, Object.class);\r\n    mapperConf.setClass(MAPPER_OUTPUT_KEY_CLASS, outputKeyClass, Object.class);\r\n    mapperConf.setClass(MAPPER_OUTPUT_VALUE_CLASS, outputValueClass, Object.class);\r\n    Stringifier<Configuration> stringifier = new DefaultStringifier<Configuration>(jobConf, Configuration.class);\r\n    try {\r\n        jobConf.set(prefix + CHAIN_MAPPER_CONFIG + index, stringifier.toString(new Configuration(mapperConf)));\r\n    } catch (IOException ioEx) {\r\n        throw new RuntimeException(ioEx);\r\n    }\r\n    jobConf.setInt(prefix + CHAIN_MAPPER_SIZE, index + 1);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\chain",
  "methodName" : "setReducer",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void setReducer(Job job, Class<? extends Reducer> klass, Class<?> inputKeyClass, Class<?> inputValueClass, Class<?> outputKeyClass, Class<?> outputValueClass, Configuration reducerConf)\n{\r\n    String prefix = getPrefix(false);\r\n    Configuration jobConf = job.getConfiguration();\r\n    checkReducerAlreadySet(false, jobConf, prefix, false);\r\n    jobConf.setClass(prefix + CHAIN_REDUCER_CLASS, klass, Reducer.class);\r\n    setReducerConf(jobConf, inputKeyClass, inputValueClass, outputKeyClass, outputValueClass, reducerConf, prefix);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\chain",
  "methodName" : "setReducerConf",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void setReducerConf(Configuration jobConf, Class<?> inputKeyClass, Class<?> inputValueClass, Class<?> outputKeyClass, Class<?> outputValueClass, Configuration reducerConf, String prefix)\n{\r\n    if (reducerConf == null) {\r\n        reducerConf = new Configuration(false);\r\n    }\r\n    reducerConf.setClass(REDUCER_INPUT_KEY_CLASS, inputKeyClass, Object.class);\r\n    reducerConf.setClass(REDUCER_INPUT_VALUE_CLASS, inputValueClass, Object.class);\r\n    reducerConf.setClass(REDUCER_OUTPUT_KEY_CLASS, outputKeyClass, Object.class);\r\n    reducerConf.setClass(REDUCER_OUTPUT_VALUE_CLASS, outputValueClass, Object.class);\r\n    Stringifier<Configuration> stringifier = new DefaultStringifier<Configuration>(jobConf, Configuration.class);\r\n    try {\r\n        jobConf.set(prefix + CHAIN_REDUCER_CONFIG, stringifier.toString(new Configuration(reducerConf)));\r\n    } catch (IOException ioEx) {\r\n        throw new RuntimeException(ioEx);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\chain",
  "methodName" : "setup",
  "errType" : null,
  "containingMethodsNum" : 10,
  "sourceCodeText" : "void setup(Configuration jobConf)\n{\r\n    String prefix = getPrefix(isMap);\r\n    int index = jobConf.getInt(prefix + CHAIN_MAPPER_SIZE, 0);\r\n    for (int i = 0; i < index; i++) {\r\n        Class<? extends Mapper> klass = jobConf.getClass(prefix + CHAIN_MAPPER_CLASS + i, null, Mapper.class);\r\n        Configuration mConf = getChainElementConf(jobConf, prefix + CHAIN_MAPPER_CONFIG + i);\r\n        confList.add(mConf);\r\n        Mapper mapper = ReflectionUtils.newInstance(klass, mConf);\r\n        mappers.add(mapper);\r\n    }\r\n    Class<? extends Reducer> klass = jobConf.getClass(prefix + CHAIN_REDUCER_CLASS, null, Reducer.class);\r\n    if (klass != null) {\r\n        rConf = getChainElementConf(jobConf, prefix + CHAIN_REDUCER_CONFIG);\r\n        reducer = ReflectionUtils.newInstance(klass, rConf);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\chain",
  "methodName" : "getAllMappers",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "List<Mapper> getAllMappers()\n{\r\n    return mappers;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\chain",
  "methodName" : "getReducer",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Reducer<?, ?, ?, ?> getReducer()\n{\r\n    return reducer;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\chain",
  "methodName" : "createBlockingQueue",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "ChainBlockingQueue<KeyValuePair<?, ?>> createBlockingQueue()\n{\r\n    return new ChainBlockingQueue<KeyValuePair<?, ?>>();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred\\join",
  "methodName" : "id",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int id()\n{\r\n    return id;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred\\join",
  "methodName" : "key",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "K key()\n{\r\n    return khead;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred\\join",
  "methodName" : "key",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void key(K qkey) throws IOException\n{\r\n    WritableUtils.cloneInto(qkey, khead);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred\\join",
  "methodName" : "hasNext",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean hasNext()\n{\r\n    return !empty;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred\\join",
  "methodName" : "skip",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void skip(K key) throws IOException\n{\r\n    if (hasNext()) {\r\n        while (cmp.compare(khead, key) <= 0 && next()) ;\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred\\join",
  "methodName" : "next",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "boolean next() throws IOException\n{\r\n    empty = !rr.next(khead, vhead);\r\n    return hasNext();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred\\join",
  "methodName" : "accept",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void accept(CompositeRecordReader.JoinCollector i, K key) throws IOException\n{\r\n    vjoin.clear();\r\n    if (0 == cmp.compare(key, khead)) {\r\n        do {\r\n            vjoin.add(vhead);\r\n        } while (next() && 0 == cmp.compare(key, khead));\r\n    }\r\n    i.add(id, vjoin);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred\\join",
  "methodName" : "next",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "boolean next(K key, U value) throws IOException\n{\r\n    if (hasNext()) {\r\n        WritableUtils.cloneInto(key, khead);\r\n        WritableUtils.cloneInto(value, vhead);\r\n        next();\r\n        return true;\r\n    }\r\n    return false;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred\\join",
  "methodName" : "createKey",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "K createKey()\n{\r\n    return rr.createKey();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred\\join",
  "methodName" : "createValue",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "U createValue()\n{\r\n    return rr.createValue();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred\\join",
  "methodName" : "getProgress",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "float getProgress() throws IOException\n{\r\n    return rr.getProgress();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred\\join",
  "methodName" : "getPos",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "long getPos() throws IOException\n{\r\n    return rr.getPos();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred\\join",
  "methodName" : "close",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void close() throws IOException\n{\r\n    rr.close();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred\\join",
  "methodName" : "compareTo",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "int compareTo(ComposableRecordReader<K, ?> other)\n{\r\n    return cmp.compare(key(), other.key());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred\\join",
  "methodName" : "equals",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "boolean equals(Object other)\n{\r\n    return other instanceof ComposableRecordReader && 0 == compareTo((ComposableRecordReader) other);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred\\join",
  "methodName" : "hashCode",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int hashCode()\n{\r\n    assert false : \"hashCode not designed\";\r\n    return 42;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred\\join",
  "methodName" : "setConf",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setConf(Configuration conf)\n{\r\n    this.conf = conf;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred\\join",
  "methodName" : "getConf",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Configuration getConf()\n{\r\n    return conf;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "getRunningMaps",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int getRunningMaps()\n{\r\n    return runningMaps;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "getRunningReduces",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int getRunningReduces()\n{\r\n    return runningReduces;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "getOccupiedMapSlots",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int getOccupiedMapSlots()\n{\r\n    return occupiedMapSlots;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "getOccupiedReduceSlots",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int getOccupiedReduceSlots()\n{\r\n    return occupiedReduceSlots;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "getReservedMapSlots",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int getReservedMapSlots()\n{\r\n    return reservedMapSlots;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "getReservedReduceSlots",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int getReservedReduceSlots()\n{\r\n    return reservedReduceSlots;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "getMapSlotCapacity",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int getMapSlotCapacity()\n{\r\n    return totalMapSlots;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "getReduceSlotCapacity",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int getReduceSlotCapacity()\n{\r\n    return totalReduceSlots;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "getTotalJobSubmissions",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int getTotalJobSubmissions()\n{\r\n    return totalJobSubmissions;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "getTaskTrackerCount",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int getTaskTrackerCount()\n{\r\n    return numTrackers;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "getBlackListedTaskTrackerCount",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int getBlackListedTaskTrackerCount()\n{\r\n    return numBlacklistedTrackers;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "getGrayListedTaskTrackerCount",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int getGrayListedTaskTrackerCount()\n{\r\n    return numGraylistedTrackers;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "getDecommissionedTaskTrackerCount",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int getDecommissionedTaskTrackerCount()\n{\r\n    return numDecommissionedTrackers;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "readFields",
  "errType" : null,
  "containingMethodsNum" : 13,
  "sourceCodeText" : "void readFields(DataInput in) throws IOException\n{\r\n    runningMaps = in.readInt();\r\n    runningReduces = in.readInt();\r\n    occupiedMapSlots = in.readInt();\r\n    occupiedReduceSlots = in.readInt();\r\n    reservedMapSlots = in.readInt();\r\n    reservedReduceSlots = in.readInt();\r\n    totalMapSlots = in.readInt();\r\n    totalReduceSlots = in.readInt();\r\n    totalJobSubmissions = in.readInt();\r\n    numTrackers = in.readInt();\r\n    numBlacklistedTrackers = in.readInt();\r\n    numGraylistedTrackers = in.readInt();\r\n    numDecommissionedTrackers = in.readInt();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "write",
  "errType" : null,
  "containingMethodsNum" : 13,
  "sourceCodeText" : "void write(DataOutput out) throws IOException\n{\r\n    out.writeInt(runningMaps);\r\n    out.writeInt(runningReduces);\r\n    out.writeInt(occupiedMapSlots);\r\n    out.writeInt(occupiedReduceSlots);\r\n    out.writeInt(reservedMapSlots);\r\n    out.writeInt(reservedReduceSlots);\r\n    out.writeInt(totalMapSlots);\r\n    out.writeInt(totalReduceSlots);\r\n    out.writeInt(totalJobSubmissions);\r\n    out.writeInt(numTrackers);\r\n    out.writeInt(numBlacklistedTrackers);\r\n    out.writeInt(numGraylistedTrackers);\r\n    out.writeInt(numDecommissionedTrackers);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\aggregate",
  "methodName" : "createInstance",
  "errType" : [ "Exception" ],
  "containingMethodsNum" : 5,
  "sourceCodeText" : "Object createInstance(String className)\n{\r\n    Object retv = null;\r\n    try {\r\n        ClassLoader classLoader = Thread.currentThread().getContextClassLoader();\r\n        Class<?> theFilterClass = Class.forName(className, true, classLoader);\r\n        Constructor<?> meth = theFilterClass.getDeclaredConstructor(argArray);\r\n        meth.setAccessible(true);\r\n        retv = meth.newInstance();\r\n    } catch (Exception e) {\r\n        throw new RuntimeException(e);\r\n    }\r\n    return retv;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\aggregate",
  "methodName" : "createAggregator",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void createAggregator(Configuration conf)\n{\r\n    if (theAggregatorDescriptor == null) {\r\n        theAggregatorDescriptor = (ValueAggregatorDescriptor) createInstance(this.className);\r\n        theAggregatorDescriptor.configure(conf);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\aggregate",
  "methodName" : "generateKeyValPairs",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "ArrayList<Entry<Text, Text>> generateKeyValPairs(Object key, Object val)\n{\r\n    ArrayList<Entry<Text, Text>> retv = new ArrayList<Entry<Text, Text>>();\r\n    if (this.theAggregatorDescriptor != null) {\r\n        retv = this.theAggregatorDescriptor.generateKeyValPairs(key, val);\r\n    }\r\n    return retv;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\aggregate",
  "methodName" : "toString",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String toString()\n{\r\n    return \"UserDefinedValueAggregatorDescriptor with class name:\" + \"\\t\" + this.className;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\aggregate",
  "methodName" : "configure",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void configure(Configuration conf)\n{\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\security\\token\\delegation",
  "methodName" : "getKind",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Text getKind()\n{\r\n    return MAPREDUCE_DELEGATION_KIND;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred\\lib",
  "methodName" : "writePartitionFile",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void writePartitionFile(JobConf job, Sampler<K, V> sampler) throws IOException, ClassNotFoundException, InterruptedException\n{\r\n    writePartitionFile(Job.getInstance(job), sampler);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\db",
  "methodName" : "getSplitter",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "DBSplitter getSplitter(int sqlDataType)\n{\r\n    switch(sqlDataType) {\r\n        case Types.DATE:\r\n        case Types.TIME:\r\n        case Types.TIMESTAMP:\r\n            return new OracleDateSplitter();\r\n        default:\r\n            return super.getSplitter(sqlDataType);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\db",
  "methodName" : "createDBRecordReader",
  "errType" : [ "SQLException" ],
  "containingMethodsNum" : 7,
  "sourceCodeText" : "RecordReader<LongWritable, T> createDBRecordReader(DBInputSplit split, Configuration conf) throws IOException\n{\r\n    DBConfiguration dbConf = getDBConf();\r\n    @SuppressWarnings(\"unchecked\")\r\n    Class<T> inputClass = (Class<T>) (dbConf.getInputClass());\r\n    try {\r\n        return new OracleDataDrivenDBRecordReader<T>(split, inputClass, conf, createConnection(), dbConf, dbConf.getInputConditions(), dbConf.getInputFieldNames(), dbConf.getInputTableName());\r\n    } catch (SQLException ex) {\r\n        throw new IOException(ex.getMessage());\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred\\lib",
  "methodName" : "map",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void map(K key, V value, OutputCollector<V, K> output, Reporter reporter) throws IOException\n{\r\n    output.collect(value, key);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "extendInternal",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void extendInternal(double newProgress, int newValue)\n{\r\n    if (state == null) {\r\n        return;\r\n    }\r\n    double mean = ((double) newValue + (double) state.oldValue) / 2.0D;\r\n    state.currentAccumulation += mean * (newProgress - state.oldProgress) * count;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\aggregate",
  "methodName" : "addNextValue",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void addNextValue(Object val)\n{\r\n    this.sum += Long.parseLong(val.toString());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\aggregate",
  "methodName" : "addNextValue",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void addNextValue(long val)\n{\r\n    this.sum += val;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\aggregate",
  "methodName" : "getSum",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getSum()\n{\r\n    return this.sum;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\aggregate",
  "methodName" : "getReport",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getReport()\n{\r\n    return \"\" + sum;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\aggregate",
  "methodName" : "reset",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void reset()\n{\r\n    sum = 0;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\aggregate",
  "methodName" : "getCombinerOutput",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "ArrayList<String> getCombinerOutput()\n{\r\n    ArrayList<String> retv = new ArrayList<String>(1);\r\n    retv.add(\"\" + sum);\r\n    return retv;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest\\stages",
  "methodName" : "getStageName",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getStageName(ManifestSuccessData arguments)\n{\r\n    return OP_STAGE_JOB_COMMIT;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest\\stages",
  "methodName" : "executeStage",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "Path executeStage(final ManifestSuccessData successData) throws IOException\n{\r\n    Path successFile = getStageConfig().getJobSuccessMarkerPath();\r\n    Path successTempFile = new Path(getJobAttemptDir(), SUCCESS_MARKER + TMP_SUFFIX);\r\n    LOG.debug(\"{}: Saving _SUCCESS file to {} via {}\", successFile, getName(), successTempFile);\r\n    save(successData, successTempFile, successFile);\r\n    return successFile;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output",
  "methodName" : "setCompressOutput",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setCompressOutput(Job job, boolean compress)\n{\r\n    job.getConfiguration().setBoolean(FileOutputFormat.COMPRESS, compress);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output",
  "methodName" : "getCompressOutput",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "boolean getCompressOutput(JobContext job)\n{\r\n    return job.getConfiguration().getBoolean(FileOutputFormat.COMPRESS, false);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output",
  "methodName" : "setOutputCompressorClass",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void setOutputCompressorClass(Job job, Class<? extends CompressionCodec> codecClass)\n{\r\n    setCompressOutput(job, true);\r\n    job.getConfiguration().setClass(FileOutputFormat.COMPRESS_CODEC, codecClass, CompressionCodec.class);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output",
  "methodName" : "getOutputCompressorClass",
  "errType" : [ "ClassNotFoundException" ],
  "containingMethodsNum" : 3,
  "sourceCodeText" : "Class<? extends CompressionCodec> getOutputCompressorClass(JobContext job, Class<? extends CompressionCodec> defaultValue)\n{\r\n    Class<? extends CompressionCodec> codecClass = defaultValue;\r\n    Configuration conf = job.getConfiguration();\r\n    String name = conf.get(FileOutputFormat.COMPRESS_CODEC);\r\n    if (name != null) {\r\n        try {\r\n            codecClass = conf.getClassByName(name).asSubclass(CompressionCodec.class);\r\n        } catch (ClassNotFoundException e) {\r\n            throw new IllegalArgumentException(\"Compression codec \" + name + \" was not found.\", e);\r\n        }\r\n    }\r\n    return codecClass;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output",
  "methodName" : "getRecordWriter",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "RecordWriter<K, V> getRecordWriter(TaskAttemptContext job) throws IOException, InterruptedException",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output",
  "methodName" : "checkOutputSpecs",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void checkOutputSpecs(JobContext job) throws FileAlreadyExistsException, IOException\n{\r\n    Path outDir = getOutputPath(job);\r\n    if (outDir == null) {\r\n        throw new InvalidJobConfException(\"Output directory not set.\");\r\n    }\r\n    TokenCache.obtainTokensForNamenodes(job.getCredentials(), new Path[] { outDir }, job.getConfiguration());\r\n    if (outDir.getFileSystem(job.getConfiguration()).exists(outDir)) {\r\n        throw new FileAlreadyExistsException(\"Output directory \" + outDir + \" already exists\");\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output",
  "methodName" : "setOutputPath",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void setOutputPath(Job job, Path outputDir)\n{\r\n    try {\r\n        outputDir = outputDir.getFileSystem(job.getConfiguration()).makeQualified(outputDir);\r\n    } catch (IOException e) {\r\n        throw new RuntimeException(e);\r\n    }\r\n    job.getConfiguration().set(FileOutputFormat.OUTDIR, outputDir.toString());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output",
  "methodName" : "getOutputPath",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Path getOutputPath(JobContext job)\n{\r\n    String name = job.getConfiguration().get(FileOutputFormat.OUTDIR);\r\n    return name == null ? null : new Path(name);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output",
  "methodName" : "getWorkOutputPath",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "Path getWorkOutputPath(TaskInputOutputContext<?, ?, ?, ?> context) throws IOException, InterruptedException\n{\r\n    PathOutputCommitter committer = (PathOutputCommitter) context.getOutputCommitter();\r\n    Path workPath = committer.getWorkPath();\r\n    LOG.debug(\"Work path is {}\", workPath);\r\n    return workPath;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output",
  "methodName" : "getPathForWorkFile",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "Path getPathForWorkFile(TaskInputOutputContext<?, ?, ?, ?> context, String name, String extension) throws IOException, InterruptedException\n{\r\n    return new Path(getWorkOutputPath(context), getUniqueFile(context, name, extension));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output",
  "methodName" : "getUniqueFile",
  "errType" : null,
  "containingMethodsNum" : 9,
  "sourceCodeText" : "String getUniqueFile(TaskAttemptContext context, String name, String extension)\n{\r\n    TaskID taskId = context.getTaskAttemptID().getTaskID();\r\n    int partition = taskId.getId();\r\n    StringBuilder result = new StringBuilder();\r\n    result.append(name);\r\n    result.append('-');\r\n    result.append(TaskID.getRepresentingCharacter(taskId.getTaskType()));\r\n    result.append('-');\r\n    result.append(NUMBER_FORMAT.format(partition));\r\n    result.append(extension);\r\n    return result.toString();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output",
  "methodName" : "getDefaultWorkFile",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "Path getDefaultWorkFile(TaskAttemptContext context, String extension) throws IOException\n{\r\n    OutputCommitter c = getOutputCommitter(context);\r\n    Preconditions.checkState(c instanceof PathOutputCommitter, \"Committer %s is not a PathOutputCommitter\", c);\r\n    Path workPath = ((PathOutputCommitter) c).getWorkPath();\r\n    Preconditions.checkNotNull(workPath, \"Null workPath returned by committer %s\", c);\r\n    Path workFile = new Path(workPath, getUniqueFile(context, getOutputName(context), extension));\r\n    LOG.debug(\"Work file for {} extension '{}' is {}\", context, extension, workFile);\r\n    return workFile;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output",
  "methodName" : "getOutputName",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String getOutputName(JobContext job)\n{\r\n    return job.getConfiguration().get(BASE_OUTPUT_NAME, PART);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output",
  "methodName" : "setOutputName",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setOutputName(JobContext job, String name)\n{\r\n    job.getConfiguration().set(BASE_OUTPUT_NAME, name);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output",
  "methodName" : "getOutputCommitter",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "OutputCommitter getOutputCommitter(TaskAttemptContext context) throws IOException\n{\r\n    if (committer == null) {\r\n        Path output = getOutputPath(context);\r\n        committer = PathOutputCommitterFactory.getCommitterFactory(output, context.getConfiguration()).createOutputCommitter(output, context);\r\n    }\r\n    return committer;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\checkpoint",
  "methodName" : "getNewName",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String getNewName()\n{\r\n    return \"checkpoint_\" + RandomStringUtils.randomAlphanumeric(8);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\security",
  "methodName" : "getFilesMap",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Map<Path, Set<Long>> getFilesMap(Configuration config)\n{\r\n    if (CryptoUtils.isEncryptedSpillEnabled(config)) {\r\n        return encryptedSpillFiles;\r\n    }\r\n    return spillFiles;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\security",
  "methodName" : "writeSpillFileCB",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void writeSpillFileCB(Path path, FSDataOutputStream out, Configuration conf)\n{\r\n    long outPos = out.getPos();\r\n    getFilesMap(conf).computeIfAbsent(path, p -> ConcurrentHashMap.newKeySet()).add(outPos);\r\n    LOG.debug(\"writeSpillFileCB.. path:{}; pos:{}\", path, outPos);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\security",
  "methodName" : "getSpillFileCB",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 9,
  "sourceCodeText" : "void getSpillFileCB(Path path, InputStream is, Configuration conf)\n{\r\n    if (path == null) {\r\n        return;\r\n    }\r\n    Set<Long> pathEntries = getFilesMap(conf).get(path);\r\n    if (pathEntries != null) {\r\n        try {\r\n            long isPos = CryptoStreamUtils.getInputStreamOffset(is);\r\n            if (pathEntries.contains(isPos)) {\r\n                LOG.debug(\"getSpillFileCB... Path {}; Pos: {}\", path, isPos);\r\n                return;\r\n            }\r\n            invalidAccessMap.computeIfAbsent(path, p -> ConcurrentHashMap.newKeySet()).add(isPos);\r\n            LOG.debug(\"getSpillFileCB... access incorrect position.. \" + \"Path {}; Pos: {}\", path, isPos);\r\n        } catch (IOException e) {\r\n            LOG.error(\"Could not get inputStream position.. Path {}\", path, e);\r\n        }\r\n        return;\r\n    }\r\n    negativeCache.add(path);\r\n    LOG.warn(\"getSpillFileCB.. Could not find spilled file .. Path: {}\", path);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\security",
  "methodName" : "getSpilledFileReport",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "String getSpilledFileReport()\n{\r\n    StringBuilder strBuilder = new StringBuilder(\"\\n++++++++ Spill Report ++++++++\").append(dumpMapEntries(\"Encrypted Spilled Files\", encryptedSpillFiles)).append(dumpMapEntries(\"Non-Encrypted Spilled Files\", spillFiles)).append(dumpMapEntries(\"Invalid Spill Access\", invalidAccessMap)).append(\"\\n ----- Spilled Index Files ----- \").append(indexSpillFiles.size());\r\n    for (Path p : indexSpillFiles) {\r\n        strBuilder.append(\"\\n\\t index-path: \").append(p.toString());\r\n    }\r\n    strBuilder.append(\"\\n ----- Negative Cache files ----- \").append(negativeCache.size());\r\n    for (Path p : negativeCache) {\r\n        strBuilder.append(\"\\n\\t path: \").append(p.toString());\r\n    }\r\n    return strBuilder.toString();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\security",
  "methodName" : "addSpillIndexFileCB",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void addSpillIndexFileCB(Path path, Configuration conf)\n{\r\n    if (path == null) {\r\n        return;\r\n    }\r\n    indexSpillFiles.add(path);\r\n    LOG.debug(\"addSpillIndexFileCB... Path: {}\", path);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\security",
  "methodName" : "validateSpillIndexFileCB",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void validateSpillIndexFileCB(Path path, Configuration conf)\n{\r\n    if (path == null) {\r\n        return;\r\n    }\r\n    if (indexSpillFiles.contains(path)) {\r\n        LOG.debug(\"validateSpillIndexFileCB.. Path: {}\", path);\r\n        return;\r\n    }\r\n    LOG.warn(\"validateSpillIndexFileCB.. could not retrieve indexFile.. \" + \"Path: {}\", path);\r\n    negativeCache.add(path);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\security",
  "methodName" : "getEncryptedSpilledFiles",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Set<Path> getEncryptedSpilledFiles()\n{\r\n    return Collections.unmodifiableSet(encryptedSpillFiles.keySet());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\security",
  "methodName" : "getInvalidSpillEntries",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "Set<String> getInvalidSpillEntries()\n{\r\n    Set<String> result = new LinkedHashSet<>();\r\n    for (Entry<Path, Set<Long>> spillMapEntry : invalidAccessMap.entrySet()) {\r\n        for (Long singleEntry : spillMapEntry.getValue()) {\r\n            result.add(String.format(\"%s[%d]\", spillMapEntry.getKey(), singleEntry));\r\n        }\r\n    }\r\n    return result;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\security",
  "methodName" : "dumpMapEntries",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "String dumpMapEntries(String label, Map<Path, Set<Long>> entriesMap)\n{\r\n    StringBuilder strBuilder = new StringBuilder(String.format(\"%n ----- %s ----- %d\", label, entriesMap.size()));\r\n    for (Entry<Path, Set<Long>> encryptedSpillEntry : entriesMap.entrySet()) {\r\n        strBuilder.append(String.format(\"%n\\t\\tpath: %s\", encryptedSpillEntry.getKey()));\r\n        for (Long singlePos : encryptedSpillEntry.getValue()) {\r\n            strBuilder.append(String.format(\"%n\\t\\t\\tentry: %d\", singlePos));\r\n        }\r\n    }\r\n    return strBuilder.toString();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\input",
  "methodName" : "createRecordReader",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "RecordReader<BytesWritable, BytesWritable> createRecordReader(InputSplit split, TaskAttemptContext context) throws IOException\n{\r\n    return new SequenceFileAsBinaryRecordReader();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred\\pipes",
  "methodName" : "close",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void close() throws IOException, InterruptedException\n{\r\n    LOG.debug(\"closing connection\");\r\n    stream.close();\r\n    uplink.closeConnection();\r\n    uplink.interrupt();\r\n    uplink.join();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred\\pipes",
  "methodName" : "authenticate",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void authenticate(String digest, String challenge) throws IOException\n{\r\n    LOG.debug(\"Sending AUTHENTICATION_REQ, digest=\" + digest + \", challenge=\" + challenge);\r\n    WritableUtils.writeVInt(stream, MessageType.AUTHENTICATION_REQ.code);\r\n    Text.writeString(stream, digest);\r\n    Text.writeString(stream, challenge);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred\\pipes",
  "methodName" : "start",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void start() throws IOException\n{\r\n    LOG.debug(\"starting downlink\");\r\n    WritableUtils.writeVInt(stream, MessageType.START.code);\r\n    WritableUtils.writeVInt(stream, CURRENT_PROTOCOL_VERSION);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred\\pipes",
  "methodName" : "setJobConf",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void setJobConf(JobConf job) throws IOException\n{\r\n    WritableUtils.writeVInt(stream, MessageType.SET_JOB_CONF.code);\r\n    List<String> list = new ArrayList<String>();\r\n    for (Map.Entry<String, String> itm : job) {\r\n        list.add(itm.getKey());\r\n        list.add(itm.getValue());\r\n    }\r\n    WritableUtils.writeVInt(stream, list.size());\r\n    for (String entry : list) {\r\n        Text.writeString(stream, entry);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred\\pipes",
  "methodName" : "setInputTypes",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void setInputTypes(String keyType, String valueType) throws IOException\n{\r\n    WritableUtils.writeVInt(stream, MessageType.SET_INPUT_TYPES.code);\r\n    Text.writeString(stream, keyType);\r\n    Text.writeString(stream, valueType);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred\\pipes",
  "methodName" : "runMap",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void runMap(InputSplit split, int numReduces, boolean pipedInput) throws IOException\n{\r\n    WritableUtils.writeVInt(stream, MessageType.RUN_MAP.code);\r\n    writeObject(split);\r\n    WritableUtils.writeVInt(stream, numReduces);\r\n    WritableUtils.writeVInt(stream, pipedInput ? 1 : 0);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred\\pipes",
  "methodName" : "mapItem",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void mapItem(WritableComparable key, Writable value) throws IOException\n{\r\n    WritableUtils.writeVInt(stream, MessageType.MAP_ITEM.code);\r\n    writeObject(key);\r\n    writeObject(value);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred\\pipes",
  "methodName" : "runReduce",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void runReduce(int reduce, boolean pipedOutput) throws IOException\n{\r\n    WritableUtils.writeVInt(stream, MessageType.RUN_REDUCE.code);\r\n    WritableUtils.writeVInt(stream, reduce);\r\n    WritableUtils.writeVInt(stream, pipedOutput ? 1 : 0);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred\\pipes",
  "methodName" : "reduceKey",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void reduceKey(WritableComparable key) throws IOException\n{\r\n    WritableUtils.writeVInt(stream, MessageType.REDUCE_KEY.code);\r\n    writeObject(key);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred\\pipes",
  "methodName" : "reduceValue",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void reduceValue(Writable value) throws IOException\n{\r\n    WritableUtils.writeVInt(stream, MessageType.REDUCE_VALUE.code);\r\n    writeObject(value);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred\\pipes",
  "methodName" : "endOfInput",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void endOfInput() throws IOException\n{\r\n    WritableUtils.writeVInt(stream, MessageType.CLOSE.code);\r\n    LOG.debug(\"Sent close command\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred\\pipes",
  "methodName" : "abort",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void abort() throws IOException\n{\r\n    WritableUtils.writeVInt(stream, MessageType.ABORT.code);\r\n    LOG.debug(\"Sent abort command\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred\\pipes",
  "methodName" : "flush",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void flush() throws IOException\n{\r\n    stream.flush();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred\\pipes",
  "methodName" : "writeObject",
  "errType" : null,
  "containingMethodsNum" : 11,
  "sourceCodeText" : "void writeObject(Writable obj) throws IOException\n{\r\n    if (obj instanceof Text) {\r\n        Text t = (Text) obj;\r\n        int len = t.getLength();\r\n        WritableUtils.writeVInt(stream, len);\r\n        stream.write(t.getBytes(), 0, len);\r\n    } else if (obj instanceof BytesWritable) {\r\n        BytesWritable b = (BytesWritable) obj;\r\n        int len = b.getLength();\r\n        WritableUtils.writeVInt(stream, len);\r\n        stream.write(b.getBytes(), 0, len);\r\n    } else {\r\n        buffer.reset();\r\n        obj.write(buffer);\r\n        int length = buffer.getLength();\r\n        WritableUtils.writeVInt(stream, length);\r\n        stream.write(buffer.getData(), 0, length);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "setTaskFound",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setTaskFound(boolean t)\n{\r\n    taskFound = t;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getTaskFound",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean getTaskFound()\n{\r\n    return taskFound;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "setPreemption",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setPreemption(boolean preemption)\n{\r\n    this.preemption = preemption;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getPreemption",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean getPreemption()\n{\r\n    return preemption;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "write",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void write(DataOutput out) throws IOException\n{\r\n    out.writeBoolean(taskFound);\r\n    out.writeBoolean(preemption);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "readFields",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void readFields(DataInput in) throws IOException\n{\r\n    taskFound = in.readBoolean();\r\n    preemption = in.readBoolean();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\util",
  "methodName" : "loadResources",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void loadResources()\n{\r\n    addDeprecatedKeys();\r\n    Configuration.addDefaultResource(\"mapred-default.xml\");\r\n    Configuration.addDefaultResource(\"mapred-site.xml\");\r\n    Configuration.addDefaultResource(\"yarn-default.xml\");\r\n    Configuration.addDefaultResource(\"yarn-site.xml\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\util",
  "methodName" : "addDeprecatedKeys",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void addDeprecatedKeys()\n{\r\n    Configuration.addDeprecations(new DeprecationDelta[] { new DeprecationDelta(\"mapred.temp.dir\", MRConfig.TEMP_DIR), new DeprecationDelta(\"mapred.local.dir\", MRConfig.LOCAL_DIR), new DeprecationDelta(\"mapred.cluster.map.memory.mb\", MRConfig.MAPMEMORY_MB), new DeprecationDelta(\"mapred.cluster.reduce.memory.mb\", MRConfig.REDUCEMEMORY_MB), new DeprecationDelta(\"mapred.acls.enabled\", MRConfig.MR_ACLS_ENABLED), new DeprecationDelta(\"mapred.cluster.max.map.memory.mb\", JTConfig.JT_MAX_MAPMEMORY_MB), new DeprecationDelta(\"mapred.cluster.max.reduce.memory.mb\", JTConfig.JT_MAX_REDUCEMEMORY_MB), new DeprecationDelta(\"mapred.system.dir\", JTConfig.JT_SYSTEM_DIR), new DeprecationDelta(\"mapred.job.tracker\", JTConfig.JT_IPC_ADDRESS), new DeprecationDelta(\"mapred.job.tracker.persist.jobstatus.active\", JTConfig.JT_PERSIST_JOBSTATUS), new DeprecationDelta(\"mapred.permissions.supergroup\", MRConfig.MR_SUPERGROUP), new DeprecationDelta(\"mapred.task.cache.levels\", JTConfig.JT_TASKCACHE_LEVELS), new DeprecationDelta(\"mapred.job.tracker.retire.jobs\", JTConfig.JT_RETIREJOBS), new DeprecationDelta(\"mapred.tasktracker.indexcache.mb\", TTConfig.TT_INDEX_CACHE), new DeprecationDelta(\"mapred.tasktracker.map.tasks.maximum\", TTConfig.TT_MAP_SLOTS), new DeprecationDelta(\"mapred.tasktracker.memory_calculator_plugin\", TTConfig.TT_RESOURCE_CALCULATOR_PLUGIN), new DeprecationDelta(\"mapred.tasktracker.memorycalculatorplugin\", TTConfig.TT_RESOURCE_CALCULATOR_PLUGIN), new DeprecationDelta(\"yarn.app.mapreduce.yarn.app.mapreduce.client-am.ipc.max-retries-on-timeouts\", MRJobConfig.MR_CLIENT_TO_AM_IPC_MAX_RETRIES_ON_TIMEOUTS), new DeprecationDelta(\"job.end.notification.url\", MRJobConfig.MR_JOB_END_NOTIFICATION_URL), new DeprecationDelta(\"job.end.retry.attempts\", MRJobConfig.MR_JOB_END_RETRY_ATTEMPTS), new DeprecationDelta(\"job.end.retry.interval\", MRJobConfig.MR_JOB_END_RETRY_INTERVAL), new DeprecationDelta(\"mapred.committer.job.setup.cleanup.needed\", MRJobConfig.SETUP_CLEANUP_NEEDED), new DeprecationDelta(\"mapred.jar\", MRJobConfig.JAR), new DeprecationDelta(\"mapred.job.id\", MRJobConfig.ID), new DeprecationDelta(\"mapred.job.name\", MRJobConfig.JOB_NAME), new DeprecationDelta(\"mapred.job.priority\", MRJobConfig.PRIORITY), new DeprecationDelta(\"mapred.job.queue.name\", MRJobConfig.QUEUE_NAME), new DeprecationDelta(\"mapred.job.reuse.jvm.num.tasks\", MRJobConfig.JVM_NUMTASKS_TORUN), new DeprecationDelta(\"mapred.map.tasks\", MRJobConfig.NUM_MAPS), new DeprecationDelta(\"mapred.max.tracker.failures\", MRJobConfig.MAX_TASK_FAILURES_PER_TRACKER), new DeprecationDelta(\"mapred.reduce.slowstart.completed.maps\", MRJobConfig.COMPLETED_MAPS_FOR_REDUCE_SLOWSTART), new DeprecationDelta(\"mapred.reduce.tasks\", MRJobConfig.NUM_REDUCES), new DeprecationDelta(\"mapred.skip.on\", MRJobConfig.SKIP_RECORDS), new DeprecationDelta(\"mapred.skip.out.dir\", MRJobConfig.SKIP_OUTDIR), new DeprecationDelta(\"mapred.speculative.execution.slowTaskThreshold\", MRJobConfig.SPECULATIVE_SLOWTASK_THRESHOLD), new DeprecationDelta(\"mapred.speculative.execution.speculativeCap\", MRJobConfig.SPECULATIVECAP_RUNNING_TASKS), new DeprecationDelta(\"job.local.dir\", MRJobConfig.JOB_LOCAL_DIR), new DeprecationDelta(\"mapreduce.inputformat.class\", MRJobConfig.INPUT_FORMAT_CLASS_ATTR), new DeprecationDelta(\"mapreduce.map.class\", MRJobConfig.MAP_CLASS_ATTR), new DeprecationDelta(\"mapreduce.combine.class\", MRJobConfig.COMBINE_CLASS_ATTR), new DeprecationDelta(\"mapreduce.reduce.class\", MRJobConfig.REDUCE_CLASS_ATTR), new DeprecationDelta(\"mapreduce.outputformat.class\", MRJobConfig.OUTPUT_FORMAT_CLASS_ATTR), new DeprecationDelta(\"mapreduce.partitioner.class\", MRJobConfig.PARTITIONER_CLASS_ATTR), new DeprecationDelta(\"mapred.job.classpath.archives\", MRJobConfig.CLASSPATH_ARCHIVES), new DeprecationDelta(\"mapred.job.classpath.files\", MRJobConfig.CLASSPATH_FILES), new DeprecationDelta(\"mapred.cache.files\", MRJobConfig.CACHE_FILES), new DeprecationDelta(\"mapred.cache.archives\", MRJobConfig.CACHE_ARCHIVES), new DeprecationDelta(\"mapred.cache.localFiles\", MRJobConfig.CACHE_LOCALFILES), new DeprecationDelta(\"mapred.cache.localArchives\", MRJobConfig.CACHE_LOCALARCHIVES), new DeprecationDelta(\"mapred.cache.files.filesizes\", MRJobConfig.CACHE_FILES_SIZES), new DeprecationDelta(\"mapred.cache.archives.filesizes\", MRJobConfig.CACHE_ARCHIVES_SIZES), new DeprecationDelta(\"mapred.cache.files.timestamps\", MRJobConfig.CACHE_FILE_TIMESTAMPS), new DeprecationDelta(\"mapred.cache.archives.timestamps\", MRJobConfig.CACHE_ARCHIVES_TIMESTAMPS), new DeprecationDelta(\"mapred.working.dir\", MRJobConfig.WORKING_DIR), new DeprecationDelta(\"user.name\", MRJobConfig.USER_NAME), new DeprecationDelta(\"mapred.output.key.class\", MRJobConfig.OUTPUT_KEY_CLASS), new DeprecationDelta(\"mapred.output.value.class\", MRJobConfig.OUTPUT_VALUE_CLASS), new DeprecationDelta(\"mapred.output.value.groupfn.class\", MRJobConfig.GROUP_COMPARATOR_CLASS), new DeprecationDelta(\"mapred.output.key.comparator.class\", MRJobConfig.KEY_COMPARATOR), new DeprecationDelta(\"io.sort.factor\", MRJobConfig.IO_SORT_FACTOR), new DeprecationDelta(\"io.sort.mb\", MRJobConfig.IO_SORT_MB), new DeprecationDelta(\"keep.failed.task.files\", MRJobConfig.PRESERVE_FAILED_TASK_FILES), new DeprecationDelta(\"keep.task.files.pattern\", MRJobConfig.PRESERVE_FILES_PATTERN), new DeprecationDelta(\"mapred.debug.out.lines\", MRJobConfig.TASK_DEBUGOUT_LINES), new DeprecationDelta(\"mapred.merge.recordsBeforeProgress\", MRJobConfig.RECORDS_BEFORE_PROGRESS), new DeprecationDelta(\"mapred.merge.recordsBeforeProgress\", MRJobConfig.COMBINE_RECORDS_BEFORE_PROGRESS), new DeprecationDelta(\"mapred.skip.attempts.to.start.skipping\", MRJobConfig.SKIP_START_ATTEMPTS), new DeprecationDelta(\"mapred.task.id\", MRJobConfig.TASK_ATTEMPT_ID), new DeprecationDelta(\"mapred.task.is.map\", MRJobConfig.TASK_ISMAP), new DeprecationDelta(\"mapred.task.partition\", MRJobConfig.TASK_PARTITION), new DeprecationDelta(\"mapred.task.profile\", MRJobConfig.TASK_PROFILE), new DeprecationDelta(\"mapred.task.profile.maps\", MRJobConfig.NUM_MAP_PROFILES), new DeprecationDelta(\"mapred.task.profile.reduces\", MRJobConfig.NUM_REDUCE_PROFILES), new DeprecationDelta(\"mapred.task.timeout\", MRJobConfig.TASK_TIMEOUT), new DeprecationDelta(\"mapred.tip.id\", MRJobConfig.TASK_ID), new DeprecationDelta(\"mapred.work.output.dir\", MRJobConfig.TASK_OUTPUT_DIR), new DeprecationDelta(\"mapred.userlog.limit.kb\", MRJobConfig.TASK_USERLOG_LIMIT), new DeprecationDelta(\"mapred.task.profile.params\", MRJobConfig.TASK_PROFILE_PARAMS), new DeprecationDelta(\"io.sort.spill.percent\", MRJobConfig.MAP_SORT_SPILL_PERCENT), new DeprecationDelta(\"map.input.file\", MRJobConfig.MAP_INPUT_FILE), new DeprecationDelta(\"map.input.length\", MRJobConfig.MAP_INPUT_PATH), new DeprecationDelta(\"map.input.start\", MRJobConfig.MAP_INPUT_START), new DeprecationDelta(\"mapred.job.map.memory.mb\", MRJobConfig.MAP_MEMORY_MB), new DeprecationDelta(\"mapred.map.child.env\", MRJobConfig.MAP_ENV), new DeprecationDelta(\"mapred.map.child.java.opts\", MRJobConfig.MAP_JAVA_OPTS), new DeprecationDelta(\"mapred.map.max.attempts\", MRJobConfig.MAP_MAX_ATTEMPTS), new DeprecationDelta(\"mapred.map.task.debug.script\", MRJobConfig.MAP_DEBUG_SCRIPT), new DeprecationDelta(\"mapred.map.tasks.speculative.execution\", MRJobConfig.MAP_SPECULATIVE), new DeprecationDelta(\"mapred.max.map.failures.percent\", MRJobConfig.MAP_FAILURES_MAX_PERCENT), new DeprecationDelta(\"mapred.skip.map.auto.incr.proc.count\", MRJobConfig.MAP_SKIP_INCR_PROC_COUNT), new DeprecationDelta(\"mapred.skip.map.max.skip.records\", MRJobConfig.MAP_SKIP_MAX_RECORDS), new DeprecationDelta(\"min.num.spills.for.combine\", MRJobConfig.MAP_COMBINE_MIN_SPILLS), new DeprecationDelta(\"mapred.compress.map.output\", MRJobConfig.MAP_OUTPUT_COMPRESS), new DeprecationDelta(\"mapred.map.output.compression.codec\", MRJobConfig.MAP_OUTPUT_COMPRESS_CODEC), new DeprecationDelta(\"mapred.mapoutput.key.class\", MRJobConfig.MAP_OUTPUT_KEY_CLASS), new DeprecationDelta(\"mapred.mapoutput.value.class\", MRJobConfig.MAP_OUTPUT_VALUE_CLASS), new DeprecationDelta(\"map.output.key.field.separator\", MRJobConfig.MAP_OUTPUT_KEY_FIELD_SEPARATOR), new DeprecationDelta(\"mapred.map.child.log.level\", MRJobConfig.MAP_LOG_LEVEL), new DeprecationDelta(\"mapred.inmem.merge.threshold\", MRJobConfig.REDUCE_MERGE_INMEM_THRESHOLD), new DeprecationDelta(\"mapred.job.reduce.input.buffer.percent\", MRJobConfig.REDUCE_INPUT_BUFFER_PERCENT), new DeprecationDelta(\"mapred.job.reduce.markreset.buffer.percent\", MRJobConfig.REDUCE_MARKRESET_BUFFER_PERCENT), new DeprecationDelta(\"mapred.job.reduce.memory.mb\", MRJobConfig.REDUCE_MEMORY_MB), new DeprecationDelta(\"mapred.job.reduce.total.mem.bytes\", MRJobConfig.REDUCE_MEMORY_TOTAL_BYTES), new DeprecationDelta(\"mapred.job.shuffle.input.buffer.percent\", MRJobConfig.SHUFFLE_INPUT_BUFFER_PERCENT), new DeprecationDelta(\"mapred.job.shuffle.merge.percent\", MRJobConfig.SHUFFLE_MERGE_PERCENT), new DeprecationDelta(\"mapred.max.reduce.failures.percent\", MRJobConfig.REDUCE_FAILURES_MAXPERCENT), new DeprecationDelta(\"mapred.reduce.child.env\", MRJobConfig.REDUCE_ENV), new DeprecationDelta(\"mapred.reduce.child.java.opts\", MRJobConfig.REDUCE_JAVA_OPTS), new DeprecationDelta(\"mapred.reduce.max.attempts\", MRJobConfig.REDUCE_MAX_ATTEMPTS), new DeprecationDelta(\"mapred.reduce.parallel.copies\", MRJobConfig.SHUFFLE_PARALLEL_COPIES), new DeprecationDelta(\"mapred.reduce.task.debug.script\", MRJobConfig.REDUCE_DEBUG_SCRIPT), new DeprecationDelta(\"mapred.reduce.tasks.speculative.execution\", MRJobConfig.REDUCE_SPECULATIVE), new DeprecationDelta(\"mapred.shuffle.connect.timeout\", MRJobConfig.SHUFFLE_CONNECT_TIMEOUT), new DeprecationDelta(\"mapred.shuffle.read.timeout\", MRJobConfig.SHUFFLE_READ_TIMEOUT), new DeprecationDelta(\"mapred.skip.reduce.auto.incr.proc.count\", MRJobConfig.REDUCE_SKIP_INCR_PROC_COUNT), new DeprecationDelta(\"mapred.skip.reduce.max.skip.groups\", MRJobConfig.REDUCE_SKIP_MAXGROUPS), new DeprecationDelta(\"mapred.reduce.child.log.level\", MRJobConfig.REDUCE_LOG_LEVEL), new DeprecationDelta(\"mapreduce.job.counters.limit\", MRJobConfig.COUNTERS_MAX_KEY), new DeprecationDelta(\"jobclient.completion.poll.interval\", Job.COMPLETION_POLL_INTERVAL_KEY), new DeprecationDelta(\"jobclient.progress.monitor.poll.interval\", Job.PROGRESS_MONITOR_POLL_INTERVAL_KEY), new DeprecationDelta(\"jobclient.output.filter\", Job.OUTPUT_FILTER), new DeprecationDelta(\"mapred.submit.replication\", Job.SUBMIT_REPLICATION), new DeprecationDelta(\"mapred.used.genericoptionsparser\", Job.USED_GENERIC_PARSER), new DeprecationDelta(\"mapred.input.dir\", org.apache.hadoop.mapreduce.lib.input.FileInputFormat.INPUT_DIR), new DeprecationDelta(\"mapred.input.pathFilter.class\", org.apache.hadoop.mapreduce.lib.input.FileInputFormat.PATHFILTER_CLASS), new DeprecationDelta(\"mapred.max.split.size\", org.apache.hadoop.mapreduce.lib.input.FileInputFormat.SPLIT_MAXSIZE), new DeprecationDelta(\"mapred.min.split.size\", org.apache.hadoop.mapreduce.lib.input.FileInputFormat.SPLIT_MINSIZE), new DeprecationDelta(\"mapred.output.compress\", org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.COMPRESS), new DeprecationDelta(\"mapred.output.compression.codec\", org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.COMPRESS_CODEC), new DeprecationDelta(\"mapred.output.compression.type\", org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.COMPRESS_TYPE), new DeprecationDelta(\"mapred.output.dir\", org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.OUTDIR), new DeprecationDelta(\"mapred.seqbinary.output.key.class\", org.apache.hadoop.mapreduce.lib.output.SequenceFileAsBinaryOutputFormat.KEY_CLASS), new DeprecationDelta(\"mapred.seqbinary.output.value.class\", org.apache.hadoop.mapreduce.lib.output.SequenceFileAsBinaryOutputFormat.VALUE_CLASS), new DeprecationDelta(\"sequencefile.filter.class\", org.apache.hadoop.mapreduce.lib.input.SequenceFileInputFilter.FILTER_CLASS), new DeprecationDelta(\"sequencefile.filter.regex\", org.apache.hadoop.mapreduce.lib.input.SequenceFileInputFilter.FILTER_REGEX), new DeprecationDelta(\"sequencefile.filter.frequency\", org.apache.hadoop.mapreduce.lib.input.SequenceFileInputFilter.FILTER_FREQUENCY), new DeprecationDelta(\"mapred.input.dir.mappers\", org.apache.hadoop.mapreduce.lib.input.MultipleInputs.DIR_MAPPERS), new DeprecationDelta(\"mapred.input.dir.formats\", org.apache.hadoop.mapreduce.lib.input.MultipleInputs.DIR_FORMATS), new DeprecationDelta(\"mapred.line.input.format.linespermap\", org.apache.hadoop.mapreduce.lib.input.NLineInputFormat.LINES_PER_MAP), new DeprecationDelta(\"mapred.binary.partitioner.left.offset\", org.apache.hadoop.mapreduce.lib.partition.BinaryPartitioner.LEFT_OFFSET_PROPERTY_NAME), new DeprecationDelta(\"mapred.binary.partitioner.right.offset\", org.apache.hadoop.mapreduce.lib.partition.BinaryPartitioner.RIGHT_OFFSET_PROPERTY_NAME), new DeprecationDelta(\"mapred.text.key.comparator.options\", org.apache.hadoop.mapreduce.lib.partition.KeyFieldBasedComparator.COMPARATOR_OPTIONS), new DeprecationDelta(\"mapred.text.key.partitioner.options\", org.apache.hadoop.mapreduce.lib.partition.KeyFieldBasedPartitioner.PARTITIONER_OPTIONS), new DeprecationDelta(\"mapred.mapper.regex.group\", org.apache.hadoop.mapreduce.lib.map.RegexMapper.GROUP), new DeprecationDelta(\"mapred.mapper.regex\", org.apache.hadoop.mapreduce.lib.map.RegexMapper.PATTERN), new DeprecationDelta(\"create.empty.dir.if.nonexist\", org.apache.hadoop.mapreduce.lib.jobcontrol.ControlledJob.CREATE_DIR), new DeprecationDelta(\"mapred.data.field.separator\", org.apache.hadoop.mapreduce.lib.fieldsel.FieldSelectionHelper.DATA_FIELD_SEPARATOR), new DeprecationDelta(\"map.output.key.value.fields.spec\", org.apache.hadoop.mapreduce.lib.fieldsel.FieldSelectionHelper.MAP_OUTPUT_KEY_VALUE_SPEC), new DeprecationDelta(\"reduce.output.key.value.fields.spec\", org.apache.hadoop.mapreduce.lib.fieldsel.FieldSelectionHelper.REDUCE_OUTPUT_KEY_VALUE_SPEC), new DeprecationDelta(\"mapred.min.split.size.per.node\", org.apache.hadoop.mapreduce.lib.input.CombineFileInputFormat.SPLIT_MINSIZE_PERNODE), new DeprecationDelta(\"mapred.min.split.size.per.rack\", org.apache.hadoop.mapreduce.lib.input.CombineFileInputFormat.SPLIT_MINSIZE_PERRACK), new DeprecationDelta(\"key.value.separator.in.input.line\", org.apache.hadoop.mapreduce.lib.input.KeyValueLineRecordReader.KEY_VALUE_SEPARATOR), new DeprecationDelta(\"mapred.linerecordreader.maxlength\", org.apache.hadoop.mapreduce.lib.input.LineRecordReader.MAX_LINE_LENGTH), new DeprecationDelta(\"mapred.lazy.output.format\", org.apache.hadoop.mapreduce.lib.output.LazyOutputFormat.OUTPUT_FORMAT), new DeprecationDelta(\"mapred.textoutputformat.separator\", org.apache.hadoop.mapreduce.lib.output.TextOutputFormat.SEPARATOR), new DeprecationDelta(\"mapred.join.expr\", org.apache.hadoop.mapreduce.lib.join.CompositeInputFormat.JOIN_EXPR), new DeprecationDelta(\"mapred.join.keycomparator\", org.apache.hadoop.mapreduce.lib.join.CompositeInputFormat.JOIN_COMPARATOR), new DeprecationDelta(\"hadoop.pipes.command-file.keep\", org.apache.hadoop.mapred.pipes.Submitter.PRESERVE_COMMANDFILE), new DeprecationDelta(\"hadoop.pipes.executable\", org.apache.hadoop.mapred.pipes.Submitter.EXECUTABLE), new DeprecationDelta(\"hadoop.pipes.executable.interpretor\", org.apache.hadoop.mapred.pipes.Submitter.INTERPRETOR), new DeprecationDelta(\"hadoop.pipes.java.mapper\", org.apache.hadoop.mapred.pipes.Submitter.IS_JAVA_MAP), new DeprecationDelta(\"hadoop.pipes.java.recordreader\", org.apache.hadoop.mapred.pipes.Submitter.IS_JAVA_RR), new DeprecationDelta(\"hadoop.pipes.java.recordwriter\", org.apache.hadoop.mapred.pipes.Submitter.IS_JAVA_RW), new DeprecationDelta(\"hadoop.pipes.java.reducer\", org.apache.hadoop.mapred.pipes.Submitter.IS_JAVA_REDUCE), new DeprecationDelta(\"hadoop.pipes.partitioner\", org.apache.hadoop.mapred.pipes.Submitter.PARTITIONER), new DeprecationDelta(\"mapred.pipes.user.inputformat\", org.apache.hadoop.mapred.pipes.Submitter.INPUT_FORMAT), new DeprecationDelta(\"security.task.umbilical.protocol.acl\", MRJobConfig.MR_AM_SECURITY_SERVICE_AUTHORIZATION_TASK_UMBILICAL), new DeprecationDelta(\"security.job.submission.protocol.acl\", MRJobConfig.MR_AM_SECURITY_SERVICE_AUTHORIZATION_CLIENT), new DeprecationDelta(\"mapreduce.user.classpath.first\", MRJobConfig.MAPREDUCE_JOB_USER_CLASSPATH_FIRST), new DeprecationDelta(\"mapred.input.dir.recursive\", FileInputFormat.INPUT_DIR_RECURSIVE) });\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\util",
  "methodName" : "main",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void main(String[] args)\n{\r\n    loadResources();\r\n    Configuration.dumpDeprecatedKeys();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\input",
  "methodName" : "createRecordReader",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "RecordReader<K, V> createRecordReader(InputSplit split, TaskAttemptContext context) throws IOException\n{\r\n    return new CombineFileRecordReader((CombineFileSplit) split, context, SequenceFileRecordReaderWrapper.class);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "getQueueName",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getQueueName()\n{\r\n    return queueName;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "setQueueName",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setQueueName(String queueName)\n{\r\n    this.queueName = queueName;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "getOperations",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String[] getOperations()\n{\r\n    return operations;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "readFields",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void readFields(DataInput in) throws IOException\n{\r\n    queueName = StringInterner.weakIntern(Text.readString(in));\r\n    operations = WritableUtils.readStringArray(in);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "write",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void write(DataOutput out) throws IOException\n{\r\n    Text.writeString(out, queueName);\r\n    WritableUtils.writeStringArray(out, operations);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\join",
  "methodName" : "combine",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "boolean combine(Object[] srcs, TupleWritable dst)\n{\r\n    assert srcs.length == dst.size();\r\n    for (int i = 0; i < srcs.length; ++i) {\r\n        if (!dst.has(i)) {\r\n            return false;\r\n        }\r\n    }\r\n    return true;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\fieldsel",
  "methodName" : "setup",
  "errType" : [ "ClassNotFoundException" ],
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void setup(Context context) throws IOException, InterruptedException\n{\r\n    Configuration conf = context.getConfiguration();\r\n    this.fieldSeparator = conf.get(FieldSelectionHelper.DATA_FIELD_SEPARATOR, \"\\t\");\r\n    this.mapOutputKeyValueSpec = conf.get(FieldSelectionHelper.MAP_OUTPUT_KEY_VALUE_SPEC, \"0-:\");\r\n    try {\r\n        this.ignoreInputKey = TextInputFormat.class.getCanonicalName().equals(context.getInputFormatClass().getCanonicalName());\r\n    } catch (ClassNotFoundException e) {\r\n        throw new IOException(\"Input format class not found\", e);\r\n    }\r\n    allMapValueFieldsFrom = FieldSelectionHelper.parseOutputKeyValueSpec(mapOutputKeyValueSpec, mapOutputKeyFieldList, mapOutputValueFieldList);\r\n    LOG.info(FieldSelectionHelper.specToString(fieldSeparator, mapOutputKeyValueSpec, allMapValueFieldsFrom, mapOutputKeyFieldList, mapOutputValueFieldList) + \"\\nignoreInputKey:\" + ignoreInputKey);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\fieldsel",
  "methodName" : "map",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void map(K key, V val, Context context) throws IOException, InterruptedException\n{\r\n    FieldSelectionHelper helper = new FieldSelectionHelper(FieldSelectionHelper.emptyText, FieldSelectionHelper.emptyText);\r\n    helper.extractOutputKeyValue(key.toString(), val.toString(), fieldSeparator, mapOutputKeyFieldList, mapOutputValueFieldList, allMapValueFieldsFrom, ignoreInputKey, true);\r\n    context.write(helper.getKey(), helper.getValue());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\task",
  "methodName" : "nextKeyValue",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean nextKeyValue() throws IOException, InterruptedException",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\task",
  "methodName" : "getCurrentKey",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "KEYIN getCurrentKey() throws IOException, InterruptedException",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\task",
  "methodName" : "getCurrentValue",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "VALUEIN getCurrentValue() throws IOException, InterruptedException",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\task",
  "methodName" : "write",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void write(KEYOUT key, VALUEOUT value) throws IOException, InterruptedException\n{\r\n    output.write(key, value);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\task",
  "methodName" : "getOutputCommitter",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "OutputCommitter getOutputCommitter()\n{\r\n    return committer;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred\\lib\\aggregate",
  "methodName" : "createValueAggregatorJobs",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "JobControl createValueAggregatorJobs(String[] args, Class<? extends ValueAggregatorDescriptor>[] descriptors) throws IOException\n{\r\n    JobControl theControl = new JobControl(\"ValueAggregatorJobs\");\r\n    ArrayList<Job> dependingJobs = new ArrayList<Job>();\r\n    JobConf aJobConf = createValueAggregatorJob(args);\r\n    if (descriptors != null)\r\n        setAggregatorDescriptors(aJobConf, descriptors);\r\n    Job aJob = new Job(aJobConf, dependingJobs);\r\n    theControl.addJob(aJob);\r\n    return theControl;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred\\lib\\aggregate",
  "methodName" : "createValueAggregatorJobs",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "JobControl createValueAggregatorJobs(String[] args) throws IOException\n{\r\n    return createValueAggregatorJobs(args, null);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred\\lib\\aggregate",
  "methodName" : "createValueAggregatorJob",
  "errType" : null,
  "containingMethodsNum" : 24,
  "sourceCodeText" : "JobConf createValueAggregatorJob(String[] args, Class<?> caller) throws IOException\n{\r\n    Configuration conf = new Configuration();\r\n    GenericOptionsParser genericParser = new GenericOptionsParser(conf, args);\r\n    args = genericParser.getRemainingArgs();\r\n    if (args.length < 2) {\r\n        System.out.println(\"usage: inputDirs outDir \" + \"[numOfReducer [textinputformat|seq [specfile [jobName]]]]\");\r\n        GenericOptionsParser.printGenericCommandUsage(System.out);\r\n        System.exit(1);\r\n    }\r\n    String inputDir = args[0];\r\n    String outputDir = args[1];\r\n    int numOfReducers = 1;\r\n    if (args.length > 2) {\r\n        numOfReducers = Integer.parseInt(args[2]);\r\n    }\r\n    Class<? extends InputFormat> theInputFormat = TextInputFormat.class;\r\n    if (args.length > 3 && args[3].compareToIgnoreCase(\"textinputformat\") == 0) {\r\n        theInputFormat = TextInputFormat.class;\r\n    } else {\r\n        theInputFormat = SequenceFileInputFormat.class;\r\n    }\r\n    Path specFile = null;\r\n    if (args.length > 4) {\r\n        specFile = new Path(args[4]);\r\n    }\r\n    String jobName = \"\";\r\n    if (args.length > 5) {\r\n        jobName = args[5];\r\n    }\r\n    JobConf theJob = new JobConf(conf);\r\n    if (specFile != null) {\r\n        theJob.addResource(specFile);\r\n    }\r\n    String userJarFile = theJob.get(\"user.jar.file\");\r\n    if (userJarFile == null) {\r\n        theJob.setJarByClass(caller != null ? caller : ValueAggregatorJob.class);\r\n    } else {\r\n        theJob.setJar(userJarFile);\r\n    }\r\n    theJob.setJobName(\"ValueAggregatorJob: \" + jobName);\r\n    FileInputFormat.addInputPaths(theJob, inputDir);\r\n    theJob.setInputFormat(theInputFormat);\r\n    theJob.setMapperClass(ValueAggregatorMapper.class);\r\n    FileOutputFormat.setOutputPath(theJob, new Path(outputDir));\r\n    theJob.setOutputFormat(TextOutputFormat.class);\r\n    theJob.setMapOutputKeyClass(Text.class);\r\n    theJob.setMapOutputValueClass(Text.class);\r\n    theJob.setOutputKeyClass(Text.class);\r\n    theJob.setOutputValueClass(Text.class);\r\n    theJob.setReducerClass(ValueAggregatorReducer.class);\r\n    theJob.setCombinerClass(ValueAggregatorCombiner.class);\r\n    theJob.setNumMapTasks(1);\r\n    theJob.setNumReduceTasks(numOfReducers);\r\n    return theJob;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred\\lib\\aggregate",
  "methodName" : "createValueAggregatorJob",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "JobConf createValueAggregatorJob(String[] args) throws IOException\n{\r\n    return createValueAggregatorJob(args, ValueAggregator.class);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred\\lib\\aggregate",
  "methodName" : "createValueAggregatorJob",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "JobConf createValueAggregatorJob(String[] args, Class<? extends ValueAggregatorDescriptor>[] descriptors) throws IOException\n{\r\n    JobConf job = createValueAggregatorJob(args);\r\n    setAggregatorDescriptors(job, descriptors);\r\n    return job;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred\\lib\\aggregate",
  "methodName" : "setAggregatorDescriptors",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void setAggregatorDescriptors(JobConf job, Class<? extends ValueAggregatorDescriptor>[] descriptors)\n{\r\n    job.setInt(\"aggregator.descriptor.num\", descriptors.length);\r\n    for (int i = 0; i < descriptors.length; i++) {\r\n        job.set(\"aggregator.descriptor.\" + i, \"UserDefined,\" + descriptors[i].getName());\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred\\lib\\aggregate",
  "methodName" : "createValueAggregatorJob",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "JobConf createValueAggregatorJob(String[] args, Class<? extends ValueAggregatorDescriptor>[] descriptors, Class<?> caller) throws IOException\n{\r\n    JobConf job = createValueAggregatorJob(args, caller);\r\n    setAggregatorDescriptors(job, descriptors);\r\n    return job;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred\\lib\\aggregate",
  "methodName" : "main",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void main(String[] args) throws IOException\n{\r\n    JobConf job = ValueAggregatorJob.createValueAggregatorJob(args);\r\n    JobClient.runJob(job);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest\\impl",
  "methodName" : "updateCommonContextOnCommitterEntry",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void updateCommonContextOnCommitterEntry(ManifestCommitterConfig committerConfig)\n{\r\n    CommonAuditContext context = currentAuditContext();\r\n    context.put(PARAM_JOB_ID, committerConfig.getJobUniqueId());\r\n    if (!committerConfig.getTaskAttemptId().isEmpty()) {\r\n        context.put(CONTEXT_ATTR_TASK_ATTEMPT_ID, committerConfig.getTaskAttemptId());\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest\\impl",
  "methodName" : "enterStage",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void enterStage(String stage)\n{\r\n    currentAuditContext().put(CONTEXT_ATTR_STAGE, stage);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest\\impl",
  "methodName" : "exitStage",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void exitStage()\n{\r\n    currentAuditContext().remove(CONTEXT_ATTR_STAGE);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest\\impl",
  "methodName" : "updateCommonContextOnCommitterExit",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void updateCommonContextOnCommitterExit()\n{\r\n    currentAuditContext().remove(PARAM_JOB_ID);\r\n    currentAuditContext().remove(CONTEXT_ATTR_TASK_ATTEMPT_ID);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest\\impl",
  "methodName" : "enterStageWorker",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void enterStageWorker(String jobId, String stage)\n{\r\n    CommonAuditContext context = currentAuditContext();\r\n    context.put(PARAM_JOB_ID, jobId);\r\n    context.put(CONTEXT_ATTR_STAGE, stage);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "isEncryptedSpillEnabled",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "boolean isEncryptedSpillEnabled(Configuration conf)\n{\r\n    return conf.getBoolean(MRJobConfig.MR_ENCRYPTED_INTERMEDIATE_DATA, MRJobConfig.DEFAULT_MR_ENCRYPTED_INTERMEDIATE_DATA);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "createIV",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "byte[] createIV(Configuration conf) throws IOException\n{\r\n    CryptoCodec cryptoCodec = CryptoCodec.getInstance(conf);\r\n    if (isEncryptedSpillEnabled(conf)) {\r\n        byte[] iv = new byte[cryptoCodec.getCipherSuite().getAlgorithmBlockSize()];\r\n        cryptoCodec.generateSecureRandom(iv);\r\n        cryptoCodec.close();\r\n        return iv;\r\n    } else {\r\n        return null;\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "cryptoPadding",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "int cryptoPadding(Configuration conf) throws IOException\n{\r\n    if (!isEncryptedSpillEnabled(conf)) {\r\n        return 0;\r\n    }\r\n    final CryptoCodec cryptoCodec = CryptoCodec.getInstance(conf);\r\n    try {\r\n        return cryptoCodec.getCipherSuite().getAlgorithmBlockSize() + 8;\r\n    } finally {\r\n        cryptoCodec.close();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "getEncryptionKey",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "byte[] getEncryptionKey() throws IOException\n{\r\n    return TokenCache.getEncryptedSpillKey(UserGroupInformation.getCurrentUser().getCredentials());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "getBufferSize",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "int getBufferSize(Configuration conf)\n{\r\n    return conf.getInt(MRJobConfig.MR_ENCRYPTED_INTERMEDIATE_DATA_BUFFER_KB, MRJobConfig.DEFAULT_MR_ENCRYPTED_INTERMEDIATE_DATA_BUFFER_KB) * 1024;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "wrapIfNecessary",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "FSDataOutputStream wrapIfNecessary(Configuration conf, FSDataOutputStream out) throws IOException\n{\r\n    return wrapIfNecessary(conf, out, true);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "wrapIfNecessary",
  "errType" : null,
  "containingMethodsNum" : 9,
  "sourceCodeText" : "FSDataOutputStream wrapIfNecessary(Configuration conf, FSDataOutputStream out, boolean closeOutputStream) throws IOException\n{\r\n    if (isEncryptedSpillEnabled(conf)) {\r\n        out.write(ByteBuffer.allocate(8).putLong(out.getPos()).array());\r\n        byte[] iv = createIV(conf);\r\n        out.write(iv);\r\n        if (LOG.isDebugEnabled()) {\r\n            LOG.debug(\"IV written to Stream [\" + Base64.encodeBase64URLSafeString(iv) + \"]\");\r\n        }\r\n        return new CryptoFSDataOutputStream(out, CryptoCodec.getInstance(conf), getBufferSize(conf), getEncryptionKey(), iv, closeOutputStream);\r\n    } else {\r\n        return out;\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "wrapIfNecessary",
  "errType" : null,
  "containingMethodsNum" : 11,
  "sourceCodeText" : "InputStream wrapIfNecessary(Configuration conf, InputStream in, long length) throws IOException\n{\r\n    if (isEncryptedSpillEnabled(conf)) {\r\n        int bufferSize = getBufferSize(conf);\r\n        if (length > -1) {\r\n            in = new LimitInputStream(in, length);\r\n        }\r\n        byte[] offsetArray = new byte[8];\r\n        IOUtils.readFully(in, offsetArray, 0, 8);\r\n        long offset = ByteBuffer.wrap(offsetArray).getLong();\r\n        CryptoCodec cryptoCodec = CryptoCodec.getInstance(conf);\r\n        byte[] iv = new byte[cryptoCodec.getCipherSuite().getAlgorithmBlockSize()];\r\n        IOUtils.readFully(in, iv, 0, cryptoCodec.getCipherSuite().getAlgorithmBlockSize());\r\n        if (LOG.isDebugEnabled()) {\r\n            LOG.debug(\"IV read from [\" + Base64.encodeBase64URLSafeString(iv) + \"]\");\r\n        }\r\n        return new CryptoInputStream(in, cryptoCodec, bufferSize, getEncryptionKey(), iv, offset + cryptoPadding(conf));\r\n    } else {\r\n        return in;\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "wrapIfNecessary",
  "errType" : null,
  "containingMethodsNum" : 9,
  "sourceCodeText" : "FSDataInputStream wrapIfNecessary(Configuration conf, FSDataInputStream in) throws IOException\n{\r\n    if (isEncryptedSpillEnabled(conf)) {\r\n        CryptoCodec cryptoCodec = CryptoCodec.getInstance(conf);\r\n        int bufferSize = getBufferSize(conf);\r\n        IOUtils.readFully(in, new byte[8], 0, 8);\r\n        byte[] iv = new byte[cryptoCodec.getCipherSuite().getAlgorithmBlockSize()];\r\n        IOUtils.readFully(in, iv, 0, cryptoCodec.getCipherSuite().getAlgorithmBlockSize());\r\n        if (LOG.isDebugEnabled()) {\r\n            LOG.debug(\"IV read from Stream [\" + Base64.encodeBase64URLSafeString(iv) + \"]\");\r\n        }\r\n        return new CryptoFSDataInputStream(in, cryptoCodec, bufferSize, getEncryptionKey(), iv);\r\n    } else {\r\n        return in;\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\input",
  "methodName" : "setInputDirRecursive",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setInputDirRecursive(Job job, boolean inputDirRecursive)\n{\r\n    job.getConfiguration().setBoolean(INPUT_DIR_RECURSIVE, inputDirRecursive);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\input",
  "methodName" : "getInputDirRecursive",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "boolean getInputDirRecursive(JobContext job)\n{\r\n    return job.getConfiguration().getBoolean(INPUT_DIR_RECURSIVE, false);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\input",
  "methodName" : "getFormatMinSplitSize",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getFormatMinSplitSize()\n{\r\n    return 1;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\input",
  "methodName" : "isSplitable",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean isSplitable(JobContext context, Path filename)\n{\r\n    return true;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\input",
  "methodName" : "setInputPathFilter",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setInputPathFilter(Job job, Class<? extends PathFilter> filter)\n{\r\n    job.getConfiguration().setClass(PATHFILTER_CLASS, filter, PathFilter.class);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\input",
  "methodName" : "setMinInputSplitSize",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setMinInputSplitSize(Job job, long size)\n{\r\n    job.getConfiguration().setLong(SPLIT_MINSIZE, size);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\input",
  "methodName" : "getMinSplitSize",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "long getMinSplitSize(JobContext job)\n{\r\n    return job.getConfiguration().getLong(SPLIT_MINSIZE, 1L);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\input",
  "methodName" : "setMaxInputSplitSize",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setMaxInputSplitSize(Job job, long size)\n{\r\n    job.getConfiguration().setLong(SPLIT_MAXSIZE, size);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\input",
  "methodName" : "getMaxSplitSize",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "long getMaxSplitSize(JobContext context)\n{\r\n    return context.getConfiguration().getLong(SPLIT_MAXSIZE, Long.MAX_VALUE);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\input",
  "methodName" : "getInputPathFilter",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "PathFilter getInputPathFilter(JobContext context)\n{\r\n    Configuration conf = context.getConfiguration();\r\n    Class<?> filterClass = conf.getClass(PATHFILTER_CLASS, null, PathFilter.class);\r\n    return (filterClass != null) ? (PathFilter) ReflectionUtils.newInstance(filterClass, conf) : null;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\input",
  "methodName" : "listStatus",
  "errType" : [ "InterruptedException" ],
  "containingMethodsNum" : 17,
  "sourceCodeText" : "List<FileStatus> listStatus(JobContext job) throws IOException\n{\r\n    Path[] dirs = getInputPaths(job);\r\n    if (dirs.length == 0) {\r\n        throw new IOException(\"No input paths specified in job\");\r\n    }\r\n    TokenCache.obtainTokensForNamenodes(job.getCredentials(), dirs, job.getConfiguration());\r\n    boolean recursive = getInputDirRecursive(job);\r\n    List<PathFilter> filters = new ArrayList<PathFilter>();\r\n    filters.add(hiddenFileFilter);\r\n    PathFilter jobFilter = getInputPathFilter(job);\r\n    if (jobFilter != null) {\r\n        filters.add(jobFilter);\r\n    }\r\n    PathFilter inputFilter = new MultiPathFilter(filters);\r\n    List<FileStatus> result = null;\r\n    int numThreads = job.getConfiguration().getInt(LIST_STATUS_NUM_THREADS, DEFAULT_LIST_STATUS_NUM_THREADS);\r\n    StopWatch sw = new StopWatch().start();\r\n    if (numThreads == 1) {\r\n        result = singleThreadedListStatus(job, dirs, inputFilter, recursive);\r\n    } else {\r\n        Iterable<FileStatus> locatedFiles = null;\r\n        try {\r\n            LocatedFileStatusFetcher locatedFileStatusFetcher = new LocatedFileStatusFetcher(job.getConfiguration(), dirs, recursive, inputFilter, true);\r\n            locatedFiles = locatedFileStatusFetcher.getFileStatuses();\r\n        } catch (InterruptedException e) {\r\n            throw (IOException) new InterruptedIOException(\"Interrupted while getting file statuses\").initCause(e);\r\n        }\r\n        result = Lists.newArrayList(locatedFiles);\r\n    }\r\n    sw.stop();\r\n    if (LOG.isDebugEnabled()) {\r\n        LOG.debug(\"Time taken to get FileStatuses: \" + sw.now(TimeUnit.MILLISECONDS));\r\n    }\r\n    LOG.info(\"Total input files to process : \" + result.size());\r\n    return result;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\input",
  "methodName" : "singleThreadedListStatus",
  "errType" : null,
  "containingMethodsNum" : 14,
  "sourceCodeText" : "List<FileStatus> singleThreadedListStatus(JobContext job, Path[] dirs, PathFilter inputFilter, boolean recursive) throws IOException\n{\r\n    List<FileStatus> result = new ArrayList<FileStatus>();\r\n    List<IOException> errors = new ArrayList<IOException>();\r\n    for (int i = 0; i < dirs.length; ++i) {\r\n        Path p = dirs[i];\r\n        FileSystem fs = p.getFileSystem(job.getConfiguration());\r\n        FileStatus[] matches = fs.globStatus(p, inputFilter);\r\n        if (matches == null) {\r\n            errors.add(new IOException(\"Input path does not exist: \" + p));\r\n        } else if (matches.length == 0) {\r\n            errors.add(new IOException(\"Input Pattern \" + p + \" matches 0 files\"));\r\n        } else {\r\n            for (FileStatus globStat : matches) {\r\n                if (globStat.isDirectory()) {\r\n                    RemoteIterator<LocatedFileStatus> iter = fs.listLocatedStatus(globStat.getPath());\r\n                    while (iter.hasNext()) {\r\n                        LocatedFileStatus stat = iter.next();\r\n                        if (inputFilter.accept(stat.getPath())) {\r\n                            if (recursive && stat.isDirectory()) {\r\n                                addInputPathRecursively(result, fs, stat.getPath(), inputFilter);\r\n                            } else {\r\n                                result.add(shrinkStatus(stat));\r\n                            }\r\n                        }\r\n                    }\r\n                } else {\r\n                    result.add(globStat);\r\n                }\r\n            }\r\n        }\r\n    }\r\n    if (!errors.isEmpty()) {\r\n        throw new InvalidInputException(errors);\r\n    }\r\n    return result;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\input",
  "methodName" : "addInputPathRecursively",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void addInputPathRecursively(List<FileStatus> result, FileSystem fs, Path path, PathFilter inputFilter) throws IOException\n{\r\n    RemoteIterator<LocatedFileStatus> iter = fs.listLocatedStatus(path);\r\n    while (iter.hasNext()) {\r\n        LocatedFileStatus stat = iter.next();\r\n        if (inputFilter.accept(stat.getPath())) {\r\n            if (stat.isDirectory()) {\r\n                addInputPathRecursively(result, fs, stat.getPath(), inputFilter);\r\n            } else {\r\n                result.add(shrinkStatus(stat));\r\n            }\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\input",
  "methodName" : "shrinkStatus",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "FileStatus shrinkStatus(FileStatus origStat)\n{\r\n    if (origStat.isDirectory() || origStat.getLen() == 0 || !(origStat instanceof LocatedFileStatus)) {\r\n        return origStat;\r\n    } else {\r\n        BlockLocation[] blockLocations = ((LocatedFileStatus) origStat).getBlockLocations();\r\n        BlockLocation[] locs = new BlockLocation[blockLocations.length];\r\n        int i = 0;\r\n        for (BlockLocation location : blockLocations) {\r\n            locs[i++] = new BlockLocation(location);\r\n        }\r\n        LocatedFileStatus newStat = new LocatedFileStatus(origStat, locs);\r\n        return newStat;\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\input",
  "methodName" : "makeSplit",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "FileSplit makeSplit(Path file, long start, long length, String[] hosts)\n{\r\n    return new FileSplit(file, start, length, hosts);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\input",
  "methodName" : "makeSplit",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "FileSplit makeSplit(Path file, long start, long length, String[] hosts, String[] inMemoryHosts)\n{\r\n    return new FileSplit(file, start, length, hosts, inMemoryHosts);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\input",
  "methodName" : "getSplits",
  "errType" : null,
  "containingMethodsNum" : 28,
  "sourceCodeText" : "List<InputSplit> getSplits(JobContext job) throws IOException\n{\r\n    StopWatch sw = new StopWatch().start();\r\n    long minSize = Math.max(getFormatMinSplitSize(), getMinSplitSize(job));\r\n    long maxSize = getMaxSplitSize(job);\r\n    List<InputSplit> splits = new ArrayList<InputSplit>();\r\n    List<FileStatus> files = listStatus(job);\r\n    boolean ignoreDirs = !getInputDirRecursive(job) && job.getConfiguration().getBoolean(INPUT_DIR_NONRECURSIVE_IGNORE_SUBDIRS, false);\r\n    for (FileStatus file : files) {\r\n        if (ignoreDirs && file.isDirectory()) {\r\n            continue;\r\n        }\r\n        Path path = file.getPath();\r\n        long length = file.getLen();\r\n        if (length != 0) {\r\n            BlockLocation[] blkLocations;\r\n            if (file instanceof LocatedFileStatus) {\r\n                blkLocations = ((LocatedFileStatus) file).getBlockLocations();\r\n            } else {\r\n                FileSystem fs = path.getFileSystem(job.getConfiguration());\r\n                blkLocations = fs.getFileBlockLocations(file, 0, length);\r\n            }\r\n            if (isSplitable(job, path)) {\r\n                long blockSize = file.getBlockSize();\r\n                long splitSize = computeSplitSize(blockSize, minSize, maxSize);\r\n                long bytesRemaining = length;\r\n                while (((double) bytesRemaining) / splitSize > SPLIT_SLOP) {\r\n                    int blkIndex = getBlockIndex(blkLocations, length - bytesRemaining);\r\n                    splits.add(makeSplit(path, length - bytesRemaining, splitSize, blkLocations[blkIndex].getHosts(), blkLocations[blkIndex].getCachedHosts()));\r\n                    bytesRemaining -= splitSize;\r\n                }\r\n                if (bytesRemaining != 0) {\r\n                    int blkIndex = getBlockIndex(blkLocations, length - bytesRemaining);\r\n                    splits.add(makeSplit(path, length - bytesRemaining, bytesRemaining, blkLocations[blkIndex].getHosts(), blkLocations[blkIndex].getCachedHosts()));\r\n                }\r\n            } else {\r\n                if (LOG.isDebugEnabled()) {\r\n                    if (length > Math.min(file.getBlockSize(), minSize)) {\r\n                        LOG.debug(\"File is not splittable so no parallelization \" + \"is possible: \" + file.getPath());\r\n                    }\r\n                }\r\n                splits.add(makeSplit(path, 0, length, blkLocations[0].getHosts(), blkLocations[0].getCachedHosts()));\r\n            }\r\n        } else {\r\n            splits.add(makeSplit(path, 0, length, new String[0]));\r\n        }\r\n    }\r\n    job.getConfiguration().setLong(NUM_INPUT_FILES, files.size());\r\n    sw.stop();\r\n    if (LOG.isDebugEnabled()) {\r\n        LOG.debug(\"Total # of splits generated by getSplits: \" + splits.size() + \", TimeTaken: \" + sw.now(TimeUnit.MILLISECONDS));\r\n    }\r\n    return splits;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\input",
  "methodName" : "computeSplitSize",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "long computeSplitSize(long blockSize, long minSize, long maxSize)\n{\r\n    return Math.max(minSize, Math.min(maxSize, blockSize));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\input",
  "methodName" : "getBlockIndex",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "int getBlockIndex(BlockLocation[] blkLocations, long offset)\n{\r\n    for (int i = 0; i < blkLocations.length; i++) {\r\n        if ((blkLocations[i].getOffset() <= offset) && (offset < blkLocations[i].getOffset() + blkLocations[i].getLength())) {\r\n            return i;\r\n        }\r\n    }\r\n    BlockLocation last = blkLocations[blkLocations.length - 1];\r\n    long fileLength = last.getOffset() + last.getLength() - 1;\r\n    throw new IllegalArgumentException(\"Offset \" + offset + \" is outside of file (0..\" + fileLength + \")\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\input",
  "methodName" : "setInputPaths",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setInputPaths(Job job, String commaSeparatedPaths) throws IOException\n{\r\n    setInputPaths(job, StringUtils.stringToPath(getPathStrings(commaSeparatedPaths)));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\input",
  "methodName" : "addInputPaths",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void addInputPaths(Job job, String commaSeparatedPaths) throws IOException\n{\r\n    for (String str : getPathStrings(commaSeparatedPaths)) {\r\n        addInputPath(job, new Path(str));\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\input",
  "methodName" : "setInputPaths",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void setInputPaths(Job job, Path... inputPaths) throws IOException\n{\r\n    Configuration conf = job.getConfiguration();\r\n    Path path = inputPaths[0].getFileSystem(conf).makeQualified(inputPaths[0]);\r\n    StringBuffer str = new StringBuffer(StringUtils.escapeString(path.toString()));\r\n    for (int i = 1; i < inputPaths.length; i++) {\r\n        str.append(StringUtils.COMMA_STR);\r\n        path = inputPaths[i].getFileSystem(conf).makeQualified(inputPaths[i]);\r\n        str.append(StringUtils.escapeString(path.toString()));\r\n    }\r\n    conf.set(INPUT_DIR, str.toString());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\input",
  "methodName" : "addInputPath",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void addInputPath(Job job, Path path) throws IOException\n{\r\n    Configuration conf = job.getConfiguration();\r\n    path = path.getFileSystem(conf).makeQualified(path);\r\n    String dirStr = StringUtils.escapeString(path.toString());\r\n    String dirs = conf.get(INPUT_DIR);\r\n    conf.set(INPUT_DIR, dirs == null ? dirStr : dirs + \",\" + dirStr);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\input",
  "methodName" : "getPathStrings",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "String[] getPathStrings(String commaSeparatedPaths)\n{\r\n    int length = commaSeparatedPaths.length();\r\n    int curlyOpen = 0;\r\n    int pathStart = 0;\r\n    boolean globPattern = false;\r\n    List<String> pathStrings = new ArrayList<String>();\r\n    for (int i = 0; i < length; i++) {\r\n        char ch = commaSeparatedPaths.charAt(i);\r\n        switch(ch) {\r\n            case '{':\r\n                {\r\n                    curlyOpen++;\r\n                    if (!globPattern) {\r\n                        globPattern = true;\r\n                    }\r\n                    break;\r\n                }\r\n            case '}':\r\n                {\r\n                    curlyOpen--;\r\n                    if (curlyOpen == 0 && globPattern) {\r\n                        globPattern = false;\r\n                    }\r\n                    break;\r\n                }\r\n            case ',':\r\n                {\r\n                    if (!globPattern) {\r\n                        pathStrings.add(commaSeparatedPaths.substring(pathStart, i));\r\n                        pathStart = i + 1;\r\n                    }\r\n                    break;\r\n                }\r\n            default:\r\n                continue;\r\n        }\r\n    }\r\n    pathStrings.add(commaSeparatedPaths.substring(pathStart, length));\r\n    return pathStrings.toArray(new String[0]);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\input",
  "methodName" : "getInputPaths",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "Path[] getInputPaths(JobContext context)\n{\r\n    String dirs = context.getConfiguration().get(INPUT_DIR, \"\");\r\n    String[] list = StringUtils.split(dirs);\r\n    Path[] result = new Path[list.length];\r\n    for (int i = 0; i < list.length; i++) {\r\n        result[i] = new Path(StringUtils.unEscapeString(list[i]));\r\n    }\r\n    return result;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest\\stages",
  "methodName" : "bindProcessor",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "TaskPool.Submitter bindProcessor(final boolean required, final TaskPool.Submitter processor)\n{\r\n    return required ? requireNonNull(processor, \"required IO processor is null\") : null;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest\\stages",
  "methodName" : "apply",
  "errType" : [ "IOException|RuntimeException" ],
  "containingMethodsNum" : 15,
  "sourceCodeText" : "OUT apply(final IN arguments) throws IOException\n{\r\n    executeOnlyOnce();\r\n    progress();\r\n    String stageName = getStageName(arguments);\r\n    getStageConfig().enterStage(stageName);\r\n    String statisticName = getStageStatisticName(arguments);\r\n    LOG.info(\"{}: Executing Stage {}\", getName(), stageName);\r\n    stageExecutionTracker = createTracker(getIOStatistics(), statisticName);\r\n    try {\r\n        final OUT out = executeStage(arguments);\r\n        LOG.info(\"{}: Stage {} completed after {}\", getName(), stageName, OperationDuration.humanTime(stageExecutionTracker.asDuration().toMillis()));\r\n        return out;\r\n    } catch (IOException | RuntimeException e) {\r\n        LOG.error(\"{}: Stage {} failed: after {}: {}\", getName(), stageName, OperationDuration.humanTime(stageExecutionTracker.asDuration().toMillis()), e.toString());\r\n        LOG.debug(\"{}: Stage failure:\", getName(), e);\r\n        stageExecutionTracker.failed();\r\n        throw e;\r\n    } finally {\r\n        stageExecutionTracker.close();\r\n        progress();\r\n        getStageConfig().exitStage(stageName);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest\\stages",
  "methodName" : "executeStage",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "OUT executeStage(IN arguments) throws IOException",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest\\stages",
  "methodName" : "executeOnlyOnce",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void executeOnlyOnce()\n{\r\n    Preconditions.checkState(!executed.getAndSet(true), \"Stage attempted twice\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest\\stages",
  "methodName" : "getStageStatisticName",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getStageStatisticName(IN arguments)\n{\r\n    return stageStatisticName;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest\\stages",
  "methodName" : "getStageName",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String getStageName(IN arguments)\n{\r\n    return getStageStatisticName(arguments);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest\\stages",
  "methodName" : "getStageExecutionTracker",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "DurationTracker getStageExecutionTracker()\n{\r\n    return stageExecutionTracker;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest\\stages",
  "methodName" : "addExecutionDurationToStatistics",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void addExecutionDurationToStatistics(IOStatisticsStore iostats, String statistic)\n{\r\n    iostats.addTimedOperation(statistic, getStageExecutionTracker().asDuration());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest\\stages",
  "methodName" : "noteAnyRateLimiting",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void noteAnyRateLimiting(String statistic, Duration wait)\n{\r\n    if (!wait.isZero()) {\r\n        getIOStatistics().addTimedOperation(statistic, wait.toMillis());\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest\\stages",
  "methodName" : "toString",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "String toString()\n{\r\n    final StringBuilder sb = new StringBuilder(\"AbstractJobOrTaskStage{\");\r\n    sb.append(isTaskStage ? \"Task Stage\" : \"Job Stage\");\r\n    sb.append(\" name='\").append(name).append('\\'');\r\n    sb.append(\" stage='\").append(stageStatisticName).append('\\'');\r\n    sb.append('}');\r\n    return sb.toString();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest\\stages",
  "methodName" : "getStageConfig",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "StageConfig getStageConfig()\n{\r\n    return stageConfig;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest\\stages",
  "methodName" : "updateAuditContext",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void updateAuditContext(final String stage)\n{\r\n    enterStageWorker(stageConfig.getJobId(), stage);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest\\stages",
  "methodName" : "getIOStatistics",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "IOStatisticsStore getIOStatistics()\n{\r\n    return stageConfig.getIOStatistics();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest\\stages",
  "methodName" : "progress",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void progress()\n{\r\n    if (stageConfig.getProgressable() != null) {\r\n        stageConfig.getProgressable().progress();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest\\stages",
  "methodName" : "getFileStatusOrNull",
  "errType" : [ "FileNotFoundException" ],
  "containingMethodsNum" : 1,
  "sourceCodeText" : "FileStatus getFileStatusOrNull(final Path path) throws IOException\n{\r\n    try {\r\n        return getFileStatus(path);\r\n    } catch (FileNotFoundException e) {\r\n        return null;\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest\\stages",
  "methodName" : "getFileStatus",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "FileStatus getFileStatus(final Path path) throws IOException\n{\r\n    LOG.trace(\"{}: getFileStatus('{}')\", getName(), path);\r\n    requireNonNull(path, () -> String.format(\"%s: Null path for getFileStatus() call\", getName()));\r\n    return trackDuration(getIOStatistics(), OP_GET_FILE_STATUS, () -> operations.getFileStatus(path));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest\\stages",
  "methodName" : "isFile",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "boolean isFile(final Path path) throws IOException\n{\r\n    LOG.trace(\"{}: isFile('{}')\", getName(), path);\r\n    return trackDuration(getIOStatistics(), OP_IS_FILE, () -> {\r\n        return operations.isFile(path);\r\n    });\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest\\stages",
  "methodName" : "delete",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "boolean delete(final Path path, final boolean recursive) throws IOException\n{\r\n    LOG.trace(\"{}: delete('{}, {}')\", getName(), path, recursive);\r\n    return delete(path, recursive, OP_DELETE);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest\\stages",
  "methodName" : "delete",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Boolean delete(final Path path, final boolean recursive, final String statistic) throws IOException\n{\r\n    return trackDuration(getIOStatistics(), statistic, () -> {\r\n        return operations.delete(path, recursive);\r\n    });\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest\\stages",
  "methodName" : "mkdirs",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "boolean mkdirs(final Path path, final boolean escalateFailure) throws IOException\n{\r\n    LOG.trace(\"{}: mkdirs('{}')\", getName(), path);\r\n    return trackDuration(getIOStatistics(), OP_MKDIRS, () -> {\r\n        boolean success = operations.mkdirs(path);\r\n        if (!success && escalateFailure) {\r\n            throw new PathIOException(path.toUri().toString(), stageStatisticName + \": mkdirs() returned false\");\r\n        }\r\n        return success;\r\n    });\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest\\stages",
  "methodName" : "listStatusIterator",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "RemoteIterator<FileStatus> listStatusIterator(final Path path) throws IOException\n{\r\n    LOG.trace(\"{}: listStatusIterator('{}')\", getName(), path);\r\n    return trackDuration(getIOStatistics(), OP_LIST_STATUS, () -> operations.listStatusIterator(path));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest\\stages",
  "methodName" : "loadManifest",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "TaskManifest loadManifest(final FileStatus status) throws IOException\n{\r\n    LOG.trace(\"{}: loadManifest('{}')\", getName(), status);\r\n    return trackDuration(getIOStatistics(), OP_LOAD_MANIFEST, () -> operations.loadTaskManifest(stageConfig.currentManifestSerializer(), status));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest\\stages",
  "methodName" : "listManifests",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "RemoteIterator<FileStatus> listManifests() throws IOException\n{\r\n    return RemoteIterators.filteringRemoteIterator(listStatusIterator(getTaskManifestDir()), st -> st.getPath().toUri().toString().endsWith(MANIFEST_SUFFIX));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest\\stages",
  "methodName" : "msync",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void msync(Path path) throws IOException\n{\r\n    LOG.trace(\"{}: msync('{}')\", getName(), path);\r\n    trackDurationOfInvocation(getIOStatistics(), OP_MSYNC, () -> operations.msync(path));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest\\stages",
  "methodName" : "createNewDirectory",
  "errType" : [ "FileNotFoundException" ],
  "containingMethodsNum" : 4,
  "sourceCodeText" : "Path createNewDirectory(final String operation, final Path path) throws IOException\n{\r\n    LOG.trace(\"{}: {} createNewDirectory('{}')\", getName(), operation, path);\r\n    requireNonNull(path, () -> String.format(\"%s: Null path for operation %s\", getName(), operation));\r\n    try {\r\n        final FileStatus status = getFileStatus(path);\r\n        throw new FileAlreadyExistsException(operation + \": path \" + path + \" already exists and has status \" + status);\r\n    } catch (FileNotFoundException e) {\r\n        mkdirs(path, true);\r\n        return path;\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest\\stages",
  "methodName" : "directoryMustExist",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "Path directoryMustExist(final String operation, final Path path) throws IOException\n{\r\n    final FileStatus status = getFileStatus(path);\r\n    if (!status.isDirectory()) {\r\n        throw new PathIOException(path.toString(), operation + \": Path is not a directory; its status is :\" + status);\r\n    }\r\n    return path;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest\\stages",
  "methodName" : "save",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void save(T manifestData, final Path tempPath, final Path finalPath) throws IOException\n{\r\n    LOG.trace(\"{}: save('{}, {}, {}')\", getName(), manifestData, tempPath, finalPath);\r\n    trackDurationOfInvocation(getIOStatistics(), OP_SAVE_TASK_MANIFEST, () -> operations.save(manifestData, tempPath, true));\r\n    renameFile(tempPath, finalPath);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest\\stages",
  "methodName" : "getEtag",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String getEtag(FileStatus status)\n{\r\n    return operations.getEtag(status);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest\\stages",
  "methodName" : "renameFile",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void renameFile(final Path source, final Path dest) throws IOException\n{\r\n    maybeDeleteDest(true, dest);\r\n    executeRenamingOperation(\"renameFile\", source, dest, OP_RENAME_FILE, () -> operations.renameFile(source, dest));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest\\stages",
  "methodName" : "renameDir",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void renameDir(final Path source, final Path dest) throws IOException\n{\r\n    maybeDeleteDest(true, dest);\r\n    executeRenamingOperation(\"renameDir\", source, dest, OP_RENAME_FILE, () -> operations.renameDir(source, dest));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest\\stages",
  "methodName" : "commitFile",
  "errType" : null,
  "containingMethodsNum" : 10,
  "sourceCodeText" : "CommitOutcome commitFile(FileEntry entry, boolean deleteDest) throws IOException\n{\r\n    final Path source = entry.getSourcePath();\r\n    final Path dest = entry.getDestPath();\r\n    maybeDeleteDest(deleteDest, dest);\r\n    if (storeSupportsResilientCommit()) {\r\n        final ManifestStoreOperations.CommitFileResult result = trackDuration(getIOStatistics(), OP_COMMIT_FILE_RENAME, () -> operations.commitFile(entry));\r\n        if (result.recovered()) {\r\n            getIOStatistics().incrementCounter(OP_COMMIT_FILE_RENAME_RECOVERED);\r\n        }\r\n        if (result.getWaitTime() != null) {\r\n            noteAnyRateLimiting(STORE_IO_RATE_LIMITED, result.getWaitTime());\r\n        }\r\n    } else {\r\n        executeRenamingOperation(\"renameFile\", source, dest, OP_COMMIT_FILE_RENAME, () -> operations.renameFile(source, dest));\r\n    }\r\n    return new CommitOutcome();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest\\stages",
  "methodName" : "storeSupportsResilientCommit",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "boolean storeSupportsResilientCommit()\n{\r\n    return operations.storeSupportsResilientCommit();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest\\stages",
  "methodName" : "maybeDeleteDest",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void maybeDeleteDest(final boolean deleteDest, final Path dest) throws IOException\n{\r\n    if (deleteDest) {\r\n        boolean deleted = delete(dest, true);\r\n        LOG.debug(\"{}: delete('{}') returned {}'\", getName(), dest, deleted);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest\\stages",
  "methodName" : "executeRenamingOperation",
  "errType" : [ "IOException|RuntimeException" ],
  "containingMethodsNum" : 11,
  "sourceCodeText" : "void executeRenamingOperation(String operation, Path source, Path dest, String statistic, CallableRaisingIOE<Boolean> action) throws IOException\n{\r\n    LOG.debug(\"{}: {} '{}' to '{}')\", getName(), operation, source, dest);\r\n    requireNonNull(source, \"Null source\");\r\n    requireNonNull(dest, \"Null dest\");\r\n    DurationTracker tracker = createTracker(getIOStatistics(), statistic);\r\n    boolean success;\r\n    try {\r\n        success = action.apply();\r\n        if (!success) {\r\n            tracker.failed();\r\n        }\r\n    } catch (IOException | RuntimeException e) {\r\n        LOG.info(\"{}: {} raised an exception: {}\", getName(), operation, e.toString());\r\n        LOG.debug(\"{}: {} stack trace\", getName(), operation, e);\r\n        tracker.failed();\r\n        throw e;\r\n    } finally {\r\n        tracker.close();\r\n    }\r\n    if (!success) {\r\n        throw escalateRenameFailure(operation, source, dest);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest\\stages",
  "methodName" : "escalateRenameFailure",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "PathIOException escalateRenameFailure(String operation, Path source, Path dest) throws IOException\n{\r\n    final FileStatus sourceStatus = getFileStatus(source);\r\n    final FileStatus destStatus = getFileStatusOrNull(dest);\r\n    LOG.error(\"{}: failure to {} {} to {} with\" + \" source status {} \" + \" and destination status {}\", getName(), operation, source, dest, sourceStatus, destStatus);\r\n    return new PathIOException(source.toString(), FAILED_TO_RENAME_PREFIX + operation + \" to \" + dest);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest\\stages",
  "methodName" : "getJobId",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String getJobId()\n{\r\n    return stageConfig.getJobId();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest\\stages",
  "methodName" : "getJobAttemptNumber",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "int getJobAttemptNumber()\n{\r\n    return stageConfig.getJobAttemptNumber();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest\\stages",
  "methodName" : "getTaskId",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String getTaskId()\n{\r\n    return stageConfig.getTaskId();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest\\stages",
  "methodName" : "getRequiredTaskId",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String getRequiredTaskId()\n{\r\n    return requireNonNull(getTaskId(), \"No Task ID in stage config\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest\\stages",
  "methodName" : "getTaskAttemptId",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String getTaskAttemptId()\n{\r\n    return stageConfig.getTaskAttemptId();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest\\stages",
  "methodName" : "getRequiredTaskAttemptId",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String getRequiredTaskAttemptId()\n{\r\n    return requireNonNull(getTaskAttemptId(), \"No Task Attempt ID in stage config\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest\\stages",
  "methodName" : "getJobAttemptDir",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Path getJobAttemptDir()\n{\r\n    return stageConfig.getJobAttemptDir();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest\\stages",
  "methodName" : "getTaskManifestDir",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Path getTaskManifestDir()\n{\r\n    return stageConfig.getTaskManifestDir();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest\\stages",
  "methodName" : "getTaskAttemptDir",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Path getTaskAttemptDir()\n{\r\n    return stageConfig.getTaskAttemptDir();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest\\stages",
  "methodName" : "getRequiredTaskAttemptDir",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Path getRequiredTaskAttemptDir()\n{\r\n    return requireNonNull(getTaskAttemptDir(), \"No Task Attempt Dir\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest\\stages",
  "methodName" : "getDestinationDir",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Path getDestinationDir()\n{\r\n    return stageConfig.getDestinationDir();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest\\stages",
  "methodName" : "getName",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getName()\n{\r\n    return name;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest\\stages",
  "methodName" : "getIOProcessors",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "TaskPool.Submitter getIOProcessors()\n{\r\n    return ioProcessors;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest\\stages",
  "methodName" : "getIOProcessors",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "TaskPool.Submitter getIOProcessors(int size)\n{\r\n    return size > 1 ? getIOProcessors() : null;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest\\stages",
  "methodName" : "deleteDir",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 2,
  "sourceCodeText" : "IOException deleteDir(final Path dir, final Boolean suppressExceptions) throws IOException\n{\r\n    try {\r\n        delete(dir, true);\r\n        return null;\r\n    } catch (IOException ex) {\r\n        LOG.info(\"Error deleting {}: {}\", dir, ex.toString());\r\n        if (!suppressExceptions) {\r\n            throw ex;\r\n        } else {\r\n            return ex;\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest\\stages",
  "methodName" : "fileEntry",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "FileEntry fileEntry(FileStatus status, Path destDir)\n{\r\n    Path dest = new Path(destDir, status.getPath().getName());\r\n    return new FileEntry(status.getPath(), dest, status.getLen(), getEtag(status));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred\\jobcontrol",
  "methodName" : "getAssignedJobID",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "JobID getAssignedJobID()\n{\r\n    org.apache.hadoop.mapreduce.JobID temp = super.getMapredJobId();\r\n    if (temp == null) {\r\n        return null;\r\n    }\r\n    return JobID.downgrade(temp);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred\\jobcontrol",
  "methodName" : "setAssignedJobID",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setAssignedJobID(JobID mapredJobID)\n{\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred\\jobcontrol",
  "methodName" : "getJobConf",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "JobConf getJobConf()\n{\r\n    return new JobConf(super.getJob().getConfiguration());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred\\jobcontrol",
  "methodName" : "setJobConf",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void setJobConf(JobConf jobConf)\n{\r\n    try {\r\n        super.setJob(org.apache.hadoop.mapreduce.Job.getInstance(jobConf));\r\n    } catch (IOException ioe) {\r\n        LOG.info(\"Exception\" + ioe);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred\\jobcontrol",
  "methodName" : "getState",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "int getState()\n{\r\n    State state = super.getJobState();\r\n    if (state == State.SUCCESS) {\r\n        return SUCCESS;\r\n    }\r\n    if (state == State.WAITING) {\r\n        return WAITING;\r\n    }\r\n    if (state == State.RUNNING) {\r\n        return RUNNING;\r\n    }\r\n    if (state == State.READY) {\r\n        return READY;\r\n    }\r\n    if (state == State.FAILED) {\r\n        return FAILED;\r\n    }\r\n    if (state == State.DEPENDENT_FAILED) {\r\n        return DEPENDENT_FAILED;\r\n    }\r\n    return -1;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred\\jobcontrol",
  "methodName" : "setState",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setState(int state)\n{\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred\\jobcontrol",
  "methodName" : "addDependingJob",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "boolean addDependingJob(Job dependingJob)\n{\r\n    return super.addDependingJob(dependingJob);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred\\jobcontrol",
  "methodName" : "getJobClient",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 1,
  "sourceCodeText" : "JobClient getJobClient()\n{\r\n    try {\r\n        return new JobClient(super.getJob().getConfiguration());\r\n    } catch (IOException ioe) {\r\n        return null;\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred\\jobcontrol",
  "methodName" : "getDependingJobs",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "ArrayList<Job> getDependingJobs()\n{\r\n    return JobControl.castToJobList(super.getDependentJobs());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred\\jobcontrol",
  "methodName" : "getMapredJobID",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "String getMapredJobID()\n{\r\n    if (super.getMapredJobId() != null) {\r\n        return super.getMapredJobId().toString();\r\n    }\r\n    return null;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred\\jobcontrol",
  "methodName" : "setMapredJobID",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setMapredJobID(String mapredJobID)\n{\r\n    setAssignedJobID(JobID.forName(mapredJobID));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getLocations",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "String[] getLocations() throws IOException\n{\r\n    HashSet<String> hostSet = new HashSet<String>();\r\n    for (Path file : getPaths()) {\r\n        FileSystem fs = file.getFileSystem(getJob());\r\n        FileStatus status = fs.getFileStatus(file);\r\n        BlockLocation[] blkLocations = fs.getFileBlockLocations(status, 0, status.getLen());\r\n        if (blkLocations != null && blkLocations.length > 0) {\r\n            addToSet(hostSet, blkLocations[0].getHosts());\r\n        }\r\n    }\r\n    return hostSet.toArray(new String[hostSet.size()]);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "addToSet",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void addToSet(Set<String> set, String[] array)\n{\r\n    for (String s : array) set.add(s);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "toString",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "String toString()\n{\r\n    StringBuffer sb = new StringBuffer();\r\n    for (int i = 0; i < getPaths().length; i++) {\r\n        sb.append(getPath(i).toUri().getPath() + \":0+\" + getLength(i));\r\n        if (i < getPaths().length - 1) {\r\n            sb.append(\"\\n\");\r\n        }\r\n    }\r\n    return sb.toString();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred\\lib\\aggregate",
  "methodName" : "generateEntry",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Entry<Text, Text> generateEntry(String type, String id, Text val)\n{\r\n    return org.apache.hadoop.mapreduce.lib.aggregate.ValueAggregatorBaseDescriptor.generateEntry(type, id, val);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred\\lib\\aggregate",
  "methodName" : "generateValueAggregator",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "ValueAggregator generateValueAggregator(String type)\n{\r\n    ValueAggregator retv = null;\r\n    if (type.compareToIgnoreCase(LONG_VALUE_SUM) == 0) {\r\n        retv = new LongValueSum();\r\n    }\r\n    if (type.compareToIgnoreCase(LONG_VALUE_MAX) == 0) {\r\n        retv = new LongValueMax();\r\n    } else if (type.compareToIgnoreCase(LONG_VALUE_MIN) == 0) {\r\n        retv = new LongValueMin();\r\n    } else if (type.compareToIgnoreCase(STRING_VALUE_MAX) == 0) {\r\n        retv = new StringValueMax();\r\n    } else if (type.compareToIgnoreCase(STRING_VALUE_MIN) == 0) {\r\n        retv = new StringValueMin();\r\n    } else if (type.compareToIgnoreCase(DOUBLE_VALUE_SUM) == 0) {\r\n        retv = new DoubleValueSum();\r\n    } else if (type.compareToIgnoreCase(UNIQ_VALUE_COUNT) == 0) {\r\n        retv = new UniqValueCount(maxNumItems);\r\n    } else if (type.compareToIgnoreCase(VALUE_HISTOGRAM) == 0) {\r\n        retv = new ValueHistogram();\r\n    }\r\n    return retv;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred\\lib\\aggregate",
  "methodName" : "configure",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void configure(JobConf job)\n{\r\n    super.configure(job);\r\n    maxNumItems = job.getLong(\"aggregate.max.num.unique.values\", Long.MAX_VALUE);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\util",
  "methodName" : "isSetsidSupported",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 3,
  "sourceCodeText" : "boolean isSetsidSupported()\n{\r\n    ShellCommandExecutor shexec = null;\r\n    boolean setsidSupported = true;\r\n    try {\r\n        String[] args = { \"setsid\", \"bash\", \"-c\", \"echo $$\" };\r\n        shexec = new ShellCommandExecutor(args);\r\n        shexec.execute();\r\n    } catch (IOException ioe) {\r\n        LOG.warn(\"setsid is not available on this machine. So not using it.\");\r\n        setsidSupported = false;\r\n    } finally {\r\n        LOG.info(\"setsid exited with exit code \" + shexec.getExitCode());\r\n    }\r\n    return setsidSupported;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\util",
  "methodName" : "destroy",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void destroy(String pid, long sleeptimeBeforeSigkill, boolean isProcessGroup, boolean inBackground)\n{\r\n    if (isProcessGroup) {\r\n        destroyProcessGroup(pid, sleeptimeBeforeSigkill, inBackground);\r\n    } else {\r\n        destroyProcess(pid, sleeptimeBeforeSigkill, inBackground);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\util",
  "methodName" : "destroyProcess",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void destroyProcess(String pid, long sleeptimeBeforeSigkill, boolean inBackground)\n{\r\n    terminateProcess(pid);\r\n    sigKill(pid, false, sleeptimeBeforeSigkill, inBackground);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\util",
  "methodName" : "destroyProcessGroup",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void destroyProcessGroup(String pgrpId, long sleeptimeBeforeSigkill, boolean inBackground)\n{\r\n    terminateProcessGroup(pgrpId);\r\n    sigKill(pgrpId, true, sleeptimeBeforeSigkill, inBackground);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\util",
  "methodName" : "sendSignal",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void sendSignal(String pid, int signalNum, String signalName)\n{\r\n    ShellCommandExecutor shexec = null;\r\n    try {\r\n        String[] args = { \"kill\", \"-\" + signalNum, pid };\r\n        shexec = new ShellCommandExecutor(args);\r\n        shexec.execute();\r\n    } catch (IOException ioe) {\r\n        LOG.warn(\"Error executing shell command \" + ioe);\r\n    } finally {\r\n        if (pid.startsWith(\"-\")) {\r\n            LOG.info(\"Sending signal to all members of process group \" + pid + \": \" + signalName + \". Exit code \" + shexec.getExitCode());\r\n        } else {\r\n            LOG.info(\"Signaling process \" + pid + \" with \" + signalName + \". Exit code \" + shexec.getExitCode());\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\util",
  "methodName" : "maybeSignalProcess",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void maybeSignalProcess(String pid, int signalNum, String signalName, boolean alwaysSignal)\n{\r\n    if (alwaysSignal || ProcessTree.isAlive(pid)) {\r\n        sendSignal(pid, signalNum, signalName);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\util",
  "methodName" : "maybeSignalProcessGroup",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void maybeSignalProcessGroup(String pgrpId, int signalNum, String signalName, boolean alwaysSignal)\n{\r\n    if (alwaysSignal || ProcessTree.isProcessGroupAlive(pgrpId)) {\r\n        sendSignal(\"-\" + pgrpId, signalNum, signalName);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\util",
  "methodName" : "terminateProcess",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void terminateProcess(String pid)\n{\r\n    maybeSignalProcess(pid, SIGTERM, SIGTERM_STR, true);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\util",
  "methodName" : "terminateProcessGroup",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void terminateProcessGroup(String pgrpId)\n{\r\n    maybeSignalProcessGroup(pgrpId, SIGTERM, SIGTERM_STR, true);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\util",
  "methodName" : "sigKillInCurrentThread",
  "errType" : [ "InterruptedException" ],
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void sigKillInCurrentThread(String pid, boolean isProcessGroup, long sleepTimeBeforeSigKill)\n{\r\n    if (isProcessGroup || ProcessTree.isAlive(pid)) {\r\n        try {\r\n            Thread.sleep(sleepTimeBeforeSigKill);\r\n        } catch (InterruptedException i) {\r\n            LOG.warn(\"Thread sleep is interrupted.\");\r\n        }\r\n        if (isProcessGroup) {\r\n            killProcessGroup(pid);\r\n        } else {\r\n            killProcess(pid);\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\util",
  "methodName" : "sigKill",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void sigKill(String pid, boolean isProcessGroup, long sleeptimeBeforeSigkill, boolean inBackground)\n{\r\n    if (inBackground) {\r\n        SigKillThread sigKillThread = new SigKillThread(pid, isProcessGroup, sleeptimeBeforeSigkill);\r\n        sigKillThread.setDaemon(true);\r\n        sigKillThread.start();\r\n    } else {\r\n        sigKillInCurrentThread(pid, isProcessGroup, sleeptimeBeforeSigkill);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\util",
  "methodName" : "killProcess",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void killProcess(String pid)\n{\r\n    maybeSignalProcess(pid, SIGKILL, SIGKILL_STR, false);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\util",
  "methodName" : "sigQuitProcess",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void sigQuitProcess(String pid)\n{\r\n    maybeSignalProcess(pid, SIGQUIT, SIGQUIT_STR, false);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\util",
  "methodName" : "killProcessGroup",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void killProcessGroup(String pgrpId)\n{\r\n    maybeSignalProcessGroup(pgrpId, SIGKILL, SIGKILL_STR, false);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\util",
  "methodName" : "sigQuitProcessGroup",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void sigQuitProcessGroup(String pgrpId)\n{\r\n    maybeSignalProcessGroup(pgrpId, SIGQUIT, SIGQUIT_STR, false);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\util",
  "methodName" : "isAlive",
  "errType" : [ "ExitCodeException", "IOException" ],
  "containingMethodsNum" : 3,
  "sourceCodeText" : "boolean isAlive(String pid)\n{\r\n    ShellCommandExecutor shexec = null;\r\n    try {\r\n        String[] args = { \"kill\", \"-0\", pid };\r\n        shexec = new ShellCommandExecutor(args);\r\n        shexec.execute();\r\n    } catch (ExitCodeException ee) {\r\n        return false;\r\n    } catch (IOException ioe) {\r\n        LOG.warn(\"Error executing shell command \" + shexec.toString() + ioe);\r\n        return false;\r\n    }\r\n    return (shexec.getExitCode() == 0 ? true : false);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\util",
  "methodName" : "isProcessGroupAlive",
  "errType" : [ "ExitCodeException", "IOException" ],
  "containingMethodsNum" : 3,
  "sourceCodeText" : "boolean isProcessGroupAlive(String pgrpId)\n{\r\n    ShellCommandExecutor shexec = null;\r\n    try {\r\n        String[] args = { \"kill\", \"-0\", \"-\" + pgrpId };\r\n        shexec = new ShellCommandExecutor(args);\r\n        shexec.execute();\r\n    } catch (ExitCodeException ee) {\r\n        return false;\r\n    } catch (IOException ioe) {\r\n        LOG.warn(\"Error executing shell command \" + shexec.toString() + ioe);\r\n        return false;\r\n    }\r\n    return (shexec.getExitCode() == 0 ? true : false);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "setMinSplitSize",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setMinSplitSize(long minSplitSize)\n{\r\n    this.minSplitSize = minSplitSize;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "isSplitable",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean isSplitable(FileSystem fs, Path filename)\n{\r\n    return true;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getRecordReader",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "RecordReader<K, V> getRecordReader(InputSplit split, JobConf job, Reporter reporter) throws IOException",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "setInputPathFilter",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setInputPathFilter(JobConf conf, Class<? extends PathFilter> filter)\n{\r\n    conf.setClass(org.apache.hadoop.mapreduce.lib.input.FileInputFormat.PATHFILTER_CLASS, filter, PathFilter.class);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getInputPathFilter",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "PathFilter getInputPathFilter(JobConf conf)\n{\r\n    Class<? extends PathFilter> filterClass = conf.getClass(org.apache.hadoop.mapreduce.lib.input.FileInputFormat.PATHFILTER_CLASS, null, PathFilter.class);\r\n    return (filterClass != null) ? ReflectionUtils.newInstance(filterClass, conf) : null;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "addInputPathRecursively",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void addInputPathRecursively(List<FileStatus> result, FileSystem fs, Path path, PathFilter inputFilter) throws IOException\n{\r\n    RemoteIterator<LocatedFileStatus> iter = fs.listLocatedStatus(path);\r\n    while (iter.hasNext()) {\r\n        LocatedFileStatus stat = iter.next();\r\n        if (inputFilter.accept(stat.getPath())) {\r\n            if (stat.isDirectory()) {\r\n                addInputPathRecursively(result, fs, stat.getPath(), inputFilter);\r\n            } else {\r\n                result.add(org.apache.hadoop.mapreduce.lib.input.FileInputFormat.shrinkStatus(stat));\r\n            }\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "listStatus",
  "errType" : [ "InterruptedException" ],
  "containingMethodsNum" : 17,
  "sourceCodeText" : "FileStatus[] listStatus(JobConf job) throws IOException\n{\r\n    Path[] dirs = getInputPaths(job);\r\n    if (dirs.length == 0) {\r\n        throw new IOException(\"No input paths specified in job\");\r\n    }\r\n    TokenCache.obtainTokensForNamenodes(job.getCredentials(), dirs, job);\r\n    boolean recursive = job.getBoolean(INPUT_DIR_RECURSIVE, false);\r\n    List<PathFilter> filters = new ArrayList<PathFilter>();\r\n    filters.add(hiddenFileFilter);\r\n    PathFilter jobFilter = getInputPathFilter(job);\r\n    if (jobFilter != null) {\r\n        filters.add(jobFilter);\r\n    }\r\n    PathFilter inputFilter = new MultiPathFilter(filters);\r\n    FileStatus[] result;\r\n    int numThreads = job.getInt(org.apache.hadoop.mapreduce.lib.input.FileInputFormat.LIST_STATUS_NUM_THREADS, org.apache.hadoop.mapreduce.lib.input.FileInputFormat.DEFAULT_LIST_STATUS_NUM_THREADS);\r\n    StopWatch sw = new StopWatch().start();\r\n    if (numThreads == 1) {\r\n        List<FileStatus> locatedFiles = singleThreadedListStatus(job, dirs, inputFilter, recursive);\r\n        result = locatedFiles.toArray(new FileStatus[locatedFiles.size()]);\r\n    } else {\r\n        Iterable<FileStatus> locatedFiles = null;\r\n        try {\r\n            LocatedFileStatusFetcher locatedFileStatusFetcher = new LocatedFileStatusFetcher(job, dirs, recursive, inputFilter, false);\r\n            locatedFiles = locatedFileStatusFetcher.getFileStatuses();\r\n        } catch (InterruptedException e) {\r\n            throw (IOException) new InterruptedIOException(\"Interrupted while getting file statuses\").initCause(e);\r\n        }\r\n        result = Iterables.toArray(locatedFiles, FileStatus.class);\r\n    }\r\n    sw.stop();\r\n    if (LOG.isDebugEnabled()) {\r\n        LOG.debug(\"Time taken to get FileStatuses: \" + sw.now(TimeUnit.MILLISECONDS));\r\n    }\r\n    LOG.info(\"Total input files to process : \" + result.length);\r\n    return result;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "singleThreadedListStatus",
  "errType" : null,
  "containingMethodsNum" : 14,
  "sourceCodeText" : "List<FileStatus> singleThreadedListStatus(JobConf job, Path[] dirs, PathFilter inputFilter, boolean recursive) throws IOException\n{\r\n    List<FileStatus> result = new ArrayList<FileStatus>();\r\n    List<IOException> errors = new ArrayList<IOException>();\r\n    for (Path p : dirs) {\r\n        FileSystem fs = p.getFileSystem(job);\r\n        FileStatus[] matches = fs.globStatus(p, inputFilter);\r\n        if (matches == null) {\r\n            errors.add(new IOException(\"Input path does not exist: \" + p));\r\n        } else if (matches.length == 0) {\r\n            errors.add(new IOException(\"Input Pattern \" + p + \" matches 0 files\"));\r\n        } else {\r\n            for (FileStatus globStat : matches) {\r\n                if (globStat.isDirectory()) {\r\n                    RemoteIterator<LocatedFileStatus> iter = fs.listLocatedStatus(globStat.getPath());\r\n                    while (iter.hasNext()) {\r\n                        LocatedFileStatus stat = iter.next();\r\n                        if (inputFilter.accept(stat.getPath())) {\r\n                            if (recursive && stat.isDirectory()) {\r\n                                addInputPathRecursively(result, fs, stat.getPath(), inputFilter);\r\n                            } else {\r\n                                result.add(org.apache.hadoop.mapreduce.lib.input.FileInputFormat.shrinkStatus(stat));\r\n                            }\r\n                        }\r\n                    }\r\n                } else {\r\n                    result.add(globStat);\r\n                }\r\n            }\r\n        }\r\n    }\r\n    if (!errors.isEmpty()) {\r\n        throw new InvalidInputException(errors);\r\n    }\r\n    return result;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "makeSplit",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "FileSplit makeSplit(Path file, long start, long length, String[] hosts)\n{\r\n    return new FileSplit(file, start, length, hosts);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "makeSplit",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "FileSplit makeSplit(Path file, long start, long length, String[] hosts, String[] inMemoryHosts)\n{\r\n    return new FileSplit(file, start, length, hosts, inMemoryHosts);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getSplits",
  "errType" : null,
  "containingMethodsNum" : 32,
  "sourceCodeText" : "InputSplit[] getSplits(JobConf job, int numSplits) throws IOException\n{\r\n    StopWatch sw = new StopWatch().start();\r\n    FileStatus[] stats = listStatus(job);\r\n    job.setLong(NUM_INPUT_FILES, stats.length);\r\n    long totalSize = 0;\r\n    boolean ignoreDirs = !job.getBoolean(INPUT_DIR_RECURSIVE, false) && job.getBoolean(INPUT_DIR_NONRECURSIVE_IGNORE_SUBDIRS, false);\r\n    List<FileStatus> files = new ArrayList<>(stats.length);\r\n    for (FileStatus file : stats) {\r\n        if (file.isDirectory()) {\r\n            if (!ignoreDirs) {\r\n                throw new IOException(\"Not a file: \" + file.getPath());\r\n            }\r\n        } else {\r\n            files.add(file);\r\n            totalSize += file.getLen();\r\n        }\r\n    }\r\n    long goalSize = totalSize / (numSplits == 0 ? 1 : numSplits);\r\n    long minSize = Math.max(job.getLong(org.apache.hadoop.mapreduce.lib.input.FileInputFormat.SPLIT_MINSIZE, 1), minSplitSize);\r\n    ArrayList<FileSplit> splits = new ArrayList<FileSplit>(numSplits);\r\n    NetworkTopology clusterMap = new NetworkTopology();\r\n    for (FileStatus file : files) {\r\n        Path path = file.getPath();\r\n        long length = file.getLen();\r\n        if (length != 0) {\r\n            FileSystem fs = path.getFileSystem(job);\r\n            BlockLocation[] blkLocations;\r\n            if (file instanceof LocatedFileStatus) {\r\n                blkLocations = ((LocatedFileStatus) file).getBlockLocations();\r\n            } else {\r\n                blkLocations = fs.getFileBlockLocations(file, 0, length);\r\n            }\r\n            if (isSplitable(fs, path)) {\r\n                long blockSize = file.getBlockSize();\r\n                long splitSize = computeSplitSize(goalSize, minSize, blockSize);\r\n                long bytesRemaining = length;\r\n                while (((double) bytesRemaining) / splitSize > SPLIT_SLOP) {\r\n                    String[][] splitHosts = getSplitHostsAndCachedHosts(blkLocations, length - bytesRemaining, splitSize, clusterMap);\r\n                    splits.add(makeSplit(path, length - bytesRemaining, splitSize, splitHosts[0], splitHosts[1]));\r\n                    bytesRemaining -= splitSize;\r\n                }\r\n                if (bytesRemaining != 0) {\r\n                    String[][] splitHosts = getSplitHostsAndCachedHosts(blkLocations, length - bytesRemaining, bytesRemaining, clusterMap);\r\n                    splits.add(makeSplit(path, length - bytesRemaining, bytesRemaining, splitHosts[0], splitHosts[1]));\r\n                }\r\n            } else {\r\n                if (LOG.isDebugEnabled()) {\r\n                    if (length > Math.min(file.getBlockSize(), minSize)) {\r\n                        LOG.debug(\"File is not splittable so no parallelization \" + \"is possible: \" + file.getPath());\r\n                    }\r\n                }\r\n                String[][] splitHosts = getSplitHostsAndCachedHosts(blkLocations, 0, length, clusterMap);\r\n                splits.add(makeSplit(path, 0, length, splitHosts[0], splitHosts[1]));\r\n            }\r\n        } else {\r\n            splits.add(makeSplit(path, 0, length, new String[0]));\r\n        }\r\n    }\r\n    sw.stop();\r\n    if (LOG.isDebugEnabled()) {\r\n        LOG.debug(\"Total # of splits generated by getSplits: \" + splits.size() + \", TimeTaken: \" + sw.now(TimeUnit.MILLISECONDS));\r\n    }\r\n    return splits.toArray(new FileSplit[splits.size()]);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "computeSplitSize",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "long computeSplitSize(long goalSize, long minSize, long blockSize)\n{\r\n    return Math.max(minSize, Math.min(goalSize, blockSize));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getBlockIndex",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "int getBlockIndex(BlockLocation[] blkLocations, long offset)\n{\r\n    for (int i = 0; i < blkLocations.length; i++) {\r\n        if ((blkLocations[i].getOffset() <= offset) && (offset < blkLocations[i].getOffset() + blkLocations[i].getLength())) {\r\n            return i;\r\n        }\r\n    }\r\n    BlockLocation last = blkLocations[blkLocations.length - 1];\r\n    long fileLength = last.getOffset() + last.getLength() - 1;\r\n    throw new IllegalArgumentException(\"Offset \" + offset + \" is outside of file (0..\" + fileLength + \")\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "setInputPaths",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setInputPaths(JobConf conf, String commaSeparatedPaths)\n{\r\n    setInputPaths(conf, StringUtils.stringToPath(getPathStrings(commaSeparatedPaths)));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "addInputPaths",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void addInputPaths(JobConf conf, String commaSeparatedPaths)\n{\r\n    for (String str : getPathStrings(commaSeparatedPaths)) {\r\n        addInputPath(conf, new Path(str));\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "setInputPaths",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void setInputPaths(JobConf conf, Path... inputPaths)\n{\r\n    Path path = new Path(conf.getWorkingDirectory(), inputPaths[0]);\r\n    StringBuffer str = new StringBuffer(StringUtils.escapeString(path.toString()));\r\n    for (int i = 1; i < inputPaths.length; i++) {\r\n        str.append(StringUtils.COMMA_STR);\r\n        path = new Path(conf.getWorkingDirectory(), inputPaths[i]);\r\n        str.append(StringUtils.escapeString(path.toString()));\r\n    }\r\n    conf.set(org.apache.hadoop.mapreduce.lib.input.FileInputFormat.INPUT_DIR, str.toString());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "addInputPath",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void addInputPath(JobConf conf, Path path)\n{\r\n    path = new Path(conf.getWorkingDirectory(), path);\r\n    String dirStr = StringUtils.escapeString(path.toString());\r\n    String dirs = conf.get(org.apache.hadoop.mapreduce.lib.input.FileInputFormat.INPUT_DIR);\r\n    conf.set(org.apache.hadoop.mapreduce.lib.input.FileInputFormat.INPUT_DIR, dirs == null ? dirStr : dirs + StringUtils.COMMA_STR + dirStr);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getPathStrings",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "String[] getPathStrings(String commaSeparatedPaths)\n{\r\n    int length = commaSeparatedPaths.length();\r\n    int curlyOpen = 0;\r\n    int pathStart = 0;\r\n    boolean globPattern = false;\r\n    List<String> pathStrings = new ArrayList<String>();\r\n    for (int i = 0; i < length; i++) {\r\n        char ch = commaSeparatedPaths.charAt(i);\r\n        switch(ch) {\r\n            case '{':\r\n                {\r\n                    curlyOpen++;\r\n                    if (!globPattern) {\r\n                        globPattern = true;\r\n                    }\r\n                    break;\r\n                }\r\n            case '}':\r\n                {\r\n                    curlyOpen--;\r\n                    if (curlyOpen == 0 && globPattern) {\r\n                        globPattern = false;\r\n                    }\r\n                    break;\r\n                }\r\n            case ',':\r\n                {\r\n                    if (!globPattern) {\r\n                        pathStrings.add(commaSeparatedPaths.substring(pathStart, i));\r\n                        pathStart = i + 1;\r\n                    }\r\n                    break;\r\n                }\r\n            default:\r\n                continue;\r\n        }\r\n    }\r\n    pathStrings.add(commaSeparatedPaths.substring(pathStart, length));\r\n    return pathStrings.toArray(new String[0]);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getInputPaths",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "Path[] getInputPaths(JobConf conf)\n{\r\n    String dirs = conf.get(org.apache.hadoop.mapreduce.lib.input.FileInputFormat.INPUT_DIR, \"\");\r\n    String[] list = StringUtils.split(dirs);\r\n    Path[] result = new Path[list.length];\r\n    for (int i = 0; i < list.length; i++) {\r\n        result[i] = new Path(StringUtils.unEscapeString(list[i]));\r\n    }\r\n    return result;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "sortInDescendingOrder",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void sortInDescendingOrder(List<NodeInfo> mylist)\n{\r\n    Collections.sort(mylist, new Comparator<NodeInfo>() {\r\n\r\n        public int compare(NodeInfo obj1, NodeInfo obj2) {\r\n            if (obj1 == null || obj2 == null)\r\n                return -1;\r\n            if (obj1.getValue() == obj2.getValue()) {\r\n                return 0;\r\n            } else {\r\n                return ((obj1.getValue() < obj2.getValue()) ? 1 : -1);\r\n            }\r\n        }\r\n    });\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getSplitHosts",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String[] getSplitHosts(BlockLocation[] blkLocations, long offset, long splitSize, NetworkTopology clusterMap) throws IOException\n{\r\n    return getSplitHostsAndCachedHosts(blkLocations, offset, splitSize, clusterMap)[0];\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getSplitHostsAndCachedHosts",
  "errType" : null,
  "containingMethodsNum" : 23,
  "sourceCodeText" : "String[][] getSplitHostsAndCachedHosts(BlockLocation[] blkLocations, long offset, long splitSize, NetworkTopology clusterMap) throws IOException\n{\r\n    int startIndex = getBlockIndex(blkLocations, offset);\r\n    long bytesInThisBlock = blkLocations[startIndex].getOffset() + blkLocations[startIndex].getLength() - offset;\r\n    if (bytesInThisBlock >= splitSize) {\r\n        return new String[][] { blkLocations[startIndex].getHosts(), blkLocations[startIndex].getCachedHosts() };\r\n    }\r\n    long bytesInFirstBlock = bytesInThisBlock;\r\n    int index = startIndex + 1;\r\n    splitSize -= bytesInThisBlock;\r\n    while (splitSize > 0) {\r\n        bytesInThisBlock = Math.min(splitSize, blkLocations[index++].getLength());\r\n        splitSize -= bytesInThisBlock;\r\n    }\r\n    long bytesInLastBlock = bytesInThisBlock;\r\n    int endIndex = index - 1;\r\n    Map<Node, NodeInfo> hostsMap = new IdentityHashMap<Node, NodeInfo>();\r\n    Map<Node, NodeInfo> racksMap = new IdentityHashMap<Node, NodeInfo>();\r\n    String[] allTopos = new String[0];\r\n    for (index = startIndex; index <= endIndex; index++) {\r\n        if (index == startIndex) {\r\n            bytesInThisBlock = bytesInFirstBlock;\r\n        } else if (index == endIndex) {\r\n            bytesInThisBlock = bytesInLastBlock;\r\n        } else {\r\n            bytesInThisBlock = blkLocations[index].getLength();\r\n        }\r\n        allTopos = blkLocations[index].getTopologyPaths();\r\n        if (allTopos.length == 0) {\r\n            allTopos = fakeRacks(blkLocations, index);\r\n        }\r\n        for (String topo : allTopos) {\r\n            Node node, parentNode;\r\n            NodeInfo nodeInfo, parentNodeInfo;\r\n            node = clusterMap.getNode(topo);\r\n            if (node == null) {\r\n                node = new NodeBase(topo);\r\n                clusterMap.add(node);\r\n            }\r\n            nodeInfo = hostsMap.get(node);\r\n            if (nodeInfo == null) {\r\n                nodeInfo = new NodeInfo(node);\r\n                hostsMap.put(node, nodeInfo);\r\n                parentNode = node.getParent();\r\n                parentNodeInfo = racksMap.get(parentNode);\r\n                if (parentNodeInfo == null) {\r\n                    parentNodeInfo = new NodeInfo(parentNode);\r\n                    racksMap.put(parentNode, parentNodeInfo);\r\n                }\r\n                parentNodeInfo.addLeaf(nodeInfo);\r\n            } else {\r\n                nodeInfo = hostsMap.get(node);\r\n                parentNode = node.getParent();\r\n                parentNodeInfo = racksMap.get(parentNode);\r\n            }\r\n            nodeInfo.addValue(index, bytesInThisBlock);\r\n            parentNodeInfo.addValue(index, bytesInThisBlock);\r\n        }\r\n    }\r\n    return new String[][] { identifyHosts(allTopos.length, racksMap), new String[0] };\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "identifyHosts",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "String[] identifyHosts(int replicationFactor, Map<Node, NodeInfo> racksMap)\n{\r\n    String[] retVal = new String[replicationFactor];\r\n    List<NodeInfo> rackList = new LinkedList<NodeInfo>();\r\n    rackList.addAll(racksMap.values());\r\n    sortInDescendingOrder(rackList);\r\n    boolean done = false;\r\n    int index = 0;\r\n    for (NodeInfo ni : rackList) {\r\n        Set<NodeInfo> hostSet = ni.getLeaves();\r\n        List<NodeInfo> hostList = new LinkedList<NodeInfo>();\r\n        hostList.addAll(hostSet);\r\n        sortInDescendingOrder(hostList);\r\n        for (NodeInfo host : hostList) {\r\n            retVal[index++] = host.node.getName().split(\":\")[0];\r\n            if (index == replicationFactor) {\r\n                done = true;\r\n                break;\r\n            }\r\n        }\r\n        if (done == true) {\r\n            break;\r\n        }\r\n    }\r\n    return retVal;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "fakeRacks",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String[] fakeRacks(BlockLocation[] blkLocations, int index) throws IOException\n{\r\n    String[] allHosts = blkLocations[index].getHosts();\r\n    String[] allTopos = new String[allHosts.length];\r\n    for (int i = 0; i < allHosts.length; i++) {\r\n        allTopos[i] = NetworkTopology.DEFAULT_RACK + \"/\" + allHosts[i];\r\n    }\r\n    return allTopos;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "print",
  "errType" : [ "JSONException" ],
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void print(PrintStream ps) throws IOException\n{\r\n    json = new JSONObject();\r\n    Writer writer = null;\r\n    try {\r\n        printJobDetails();\r\n        printTaskSummary();\r\n        printTasks();\r\n        writer = new OutputStreamWriter(ps, \"UTF-8\");\r\n        json.write(writer);\r\n        writer.flush();\r\n    } catch (JSONException je) {\r\n        throw new IOException(\"Failure parsing JSON document: \" + je.getMessage(), je);\r\n    } finally {\r\n        if (writer != null) {\r\n            writer.close();\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "printJobDetails",
  "errType" : null,
  "containingMethodsNum" : 9,
  "sourceCodeText" : "void printJobDetails() throws JSONException\n{\r\n    json.put(\"hadoopJob\", job.getJobId().toString());\r\n    json.put(\"user\", job.getUsername());\r\n    json.put(\"jobName\", job.getJobname());\r\n    json.put(\"jobConf\", job.getJobConfPath());\r\n    json.put(\"submittedAt\", job.getSubmitTime());\r\n    json.put(\"launchedAt\", job.getLaunchTime());\r\n    json.put(\"finishedAt\", job.getFinishTime());\r\n    json.put(\"status\", ((job.getJobStatus() == null) ? \"Incomplete\" : job.getJobStatus()));\r\n    printJobCounters(job.getTotalCounters(), job.getMapCounters(), job.getReduceCounters());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "printJobCounters",
  "errType" : null,
  "containingMethodsNum" : 18,
  "sourceCodeText" : "void printJobCounters(Counters totalCounters, Counters mapCounters, Counters reduceCounters) throws JSONException\n{\r\n    if (totalCounters != null) {\r\n        JSONObject jGroups = new JSONObject();\r\n        for (CounterGroup counterGroup : totalCounters) {\r\n            String groupName = counterGroup.getName();\r\n            CounterGroup totalGroup = totalCounters.getGroup(groupName);\r\n            CounterGroup mapGroup = mapCounters.getGroup(groupName);\r\n            CounterGroup reduceGroup = reduceCounters.getGroup(groupName);\r\n            Iterator<Counter> ctrItr = totalGroup.iterator();\r\n            JSONArray jGroup = new JSONArray();\r\n            while (ctrItr.hasNext()) {\r\n                JSONObject jCounter = new JSONObject();\r\n                org.apache.hadoop.mapreduce.Counter counter = ctrItr.next();\r\n                String name = counter.getName();\r\n                long mapValue = mapGroup.findCounter(name).getValue();\r\n                long reduceValue = reduceGroup.findCounter(name).getValue();\r\n                long totalValue = counter.getValue();\r\n                jCounter.put(\"counterName\", name);\r\n                jCounter.put(\"mapValue\", mapValue);\r\n                jCounter.put(\"reduceValue\", reduceValue);\r\n                jCounter.put(\"totalValue\", totalValue);\r\n                jGroup.put(jCounter);\r\n            }\r\n            jGroups.put(fixGroupNameForShuffleErrors(totalGroup.getName()), jGroup);\r\n        }\r\n        json.put(\"counters\", jGroups);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "printTaskSummary",
  "errType" : null,
  "containingMethodsNum" : 29,
  "sourceCodeText" : "void printTaskSummary() throws JSONException\n{\r\n    HistoryViewer.SummarizedJob ts = new HistoryViewer.SummarizedJob(job);\r\n    JSONObject jSums = new JSONObject();\r\n    JSONObject jSumSetup = new JSONObject();\r\n    jSumSetup.put(\"total\", ts.totalSetups);\r\n    jSumSetup.put(\"successful\", ts.numFinishedSetups);\r\n    jSumSetup.put(\"failed\", ts.numFailedSetups);\r\n    jSumSetup.put(\"killed\", ts.numKilledSetups);\r\n    jSumSetup.put(\"startTime\", ts.setupStarted);\r\n    jSumSetup.put(\"finishTime\", ts.setupFinished);\r\n    jSums.put(\"setup\", jSumSetup);\r\n    JSONObject jSumMap = new JSONObject();\r\n    jSumMap.put(\"total\", ts.totalMaps);\r\n    jSumMap.put(\"successful\", job.getSucceededMaps());\r\n    jSumMap.put(\"failed\", ts.numFailedMaps);\r\n    jSumMap.put(\"killed\", ts.numKilledMaps);\r\n    jSumMap.put(\"startTime\", ts.mapStarted);\r\n    jSumMap.put(\"finishTime\", ts.mapFinished);\r\n    jSums.put(\"map\", jSumMap);\r\n    JSONObject jSumReduce = new JSONObject();\r\n    jSumReduce.put(\"total\", ts.totalReduces);\r\n    jSumReduce.put(\"successful\", job.getSucceededReduces());\r\n    jSumReduce.put(\"failed\", ts.numFailedReduces);\r\n    jSumReduce.put(\"killed\", ts.numKilledReduces);\r\n    jSumReduce.put(\"startTime\", ts.reduceStarted);\r\n    jSumReduce.put(\"finishTime\", ts.reduceFinished);\r\n    jSums.put(\"reduce\", jSumReduce);\r\n    JSONObject jSumCleanup = new JSONObject();\r\n    jSumCleanup.put(\"total\", ts.totalCleanups);\r\n    jSumCleanup.put(\"successful\", ts.numFinishedCleanups);\r\n    jSumCleanup.put(\"failed\", ts.numFailedCleanups);\r\n    jSumCleanup.put(\"killed\", ts.numKilledCleanups);\r\n    jSumCleanup.put(\"startTime\", ts.cleanupStarted);\r\n    jSumCleanup.put(\"finishTime\", ts.cleanupFinished);\r\n    jSums.put(\"cleanup\", jSumCleanup);\r\n    json.put(\"taskSummary\", jSums);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "printTasks",
  "errType" : null,
  "containingMethodsNum" : 31,
  "sourceCodeText" : "void printTasks() throws JSONException\n{\r\n    Map<TaskID, JobHistoryParser.TaskInfo> tasks = job.getAllTasks();\r\n    JSONArray jTasks = new JSONArray();\r\n    for (JobHistoryParser.TaskInfo task : tasks.values()) {\r\n        if (!task.getTaskType().equals(TaskType.TASK_CLEANUP) && ((printAll && task.getTaskStatus().equals(TaskStatus.State.SUCCEEDED.toString())) || task.getTaskStatus().equals(TaskStatus.State.KILLED.toString()) || task.getTaskStatus().equals(TaskStatus.State.FAILED.toString()))) {\r\n            JSONObject jTask = new JSONObject();\r\n            jTask.put(\"taskId\", task.getTaskId().toString());\r\n            jTask.put(\"type\", task.getTaskType().toString());\r\n            jTask.put(\"status\", task.getTaskStatus());\r\n            jTask.put(\"startTime\", task.getStartTime());\r\n            jTask.put(\"finishTime\", task.getFinishTime());\r\n            if (!task.getError().isEmpty()) {\r\n                jTask.put(\"error\", task.getError());\r\n            }\r\n            if (task.getTaskType().equals(TaskType.MAP)) {\r\n                jTask.put(\"inputSplits\", task.getSplitLocations());\r\n            }\r\n            if (printAll) {\r\n                printTaskCounters(jTask, task.getCounters());\r\n                JSONObject jAtt = new JSONObject();\r\n                for (JobHistoryParser.TaskAttemptInfo attempt : task.getAllTaskAttempts().values()) {\r\n                    jAtt.put(\"attemptId\", attempt.getAttemptId());\r\n                    jAtt.put(\"startTime\", attempt.getStartTime());\r\n                    if (task.getTaskType().equals(TaskType.REDUCE)) {\r\n                        jAtt.put(\"shuffleFinished\", attempt.getShuffleFinishTime());\r\n                        jAtt.put(\"sortFinished\", attempt.getSortFinishTime());\r\n                    }\r\n                    jAtt.put(\"finishTime\", attempt.getFinishTime());\r\n                    jAtt.put(\"hostName\", attempt.getHostname());\r\n                    if (!attempt.getError().isEmpty()) {\r\n                        jAtt.put(\"error\", task.getError());\r\n                    }\r\n                    String taskLogsUrl = HistoryViewer.getTaskLogsUrl(scheme, attempt);\r\n                    if (taskLogsUrl != null) {\r\n                        jAtt.put(\"taskLogs\", taskLogsUrl);\r\n                    }\r\n                }\r\n                jTask.put(\"attempts\", jAtt);\r\n            }\r\n            jTasks.put(jTask);\r\n        }\r\n        json.put(\"tasks\", jTasks);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "printTaskCounters",
  "errType" : null,
  "containingMethodsNum" : 10,
  "sourceCodeText" : "void printTaskCounters(JSONObject jTask, Counters taskCounters) throws JSONException\n{\r\n    if (taskCounters != null) {\r\n        JSONObject jGroups = new JSONObject();\r\n        for (String groupName : taskCounters.getGroupNames()) {\r\n            CounterGroup group = taskCounters.getGroup(groupName);\r\n            Iterator<Counter> ctrItr = group.iterator();\r\n            JSONArray jGroup = new JSONArray();\r\n            while (ctrItr.hasNext()) {\r\n                JSONObject jCounter = new JSONObject();\r\n                org.apache.hadoop.mapreduce.Counter counter = ctrItr.next();\r\n                jCounter.put(\"counterName\", counter.getName());\r\n                jCounter.put(\"value\", counter.getValue());\r\n                jGroup.put(jCounter);\r\n            }\r\n            jGroups.put(fixGroupNameForShuffleErrors(group.getName()), jGroup);\r\n        }\r\n        jTask.put(\"counters\", jGroups);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "fixGroupNameForShuffleErrors",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String fixGroupNameForShuffleErrors(String name)\n{\r\n    String retName = name;\r\n    if (name.equals(\"Shuffle Errors\")) {\r\n        retName = \"org.apache.hadoop.mapreduce.task.reduce.Fetcher.ShuffleErrors\";\r\n    }\r\n    return retName;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\input",
  "methodName" : "initSplit",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void initSplit(Path[] files, long[] start, long[] lengths, String[] locations)\n{\r\n    this.startoffset = start;\r\n    this.lengths = lengths;\r\n    this.paths = files;\r\n    this.totLength = 0;\r\n    this.locations = locations;\r\n    for (long length : lengths) {\r\n        totLength += length;\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\input",
  "methodName" : "getLength",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getLength()\n{\r\n    return totLength;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\input",
  "methodName" : "getStartOffsets",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long[] getStartOffsets()\n{\r\n    return startoffset;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\input",
  "methodName" : "getLengths",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long[] getLengths()\n{\r\n    return lengths;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\input",
  "methodName" : "getOffset",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getOffset(int i)\n{\r\n    return startoffset[i];\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\input",
  "methodName" : "getLength",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getLength(int i)\n{\r\n    return lengths[i];\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\input",
  "methodName" : "getNumPaths",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int getNumPaths()\n{\r\n    return paths.length;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\input",
  "methodName" : "getPath",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Path getPath(int i)\n{\r\n    return paths[i];\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\input",
  "methodName" : "getPaths",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Path[] getPaths()\n{\r\n    return paths;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\input",
  "methodName" : "getLocations",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String[] getLocations() throws IOException\n{\r\n    return locations;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\input",
  "methodName" : "readFields",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void readFields(DataInput in) throws IOException\n{\r\n    totLength = in.readLong();\r\n    int arrLength = in.readInt();\r\n    lengths = new long[arrLength];\r\n    for (int i = 0; i < arrLength; i++) {\r\n        lengths[i] = in.readLong();\r\n    }\r\n    int filesLength = in.readInt();\r\n    paths = new Path[filesLength];\r\n    for (int i = 0; i < filesLength; i++) {\r\n        paths[i] = new Path(Text.readString(in));\r\n    }\r\n    arrLength = in.readInt();\r\n    startoffset = new long[arrLength];\r\n    for (int i = 0; i < arrLength; i++) {\r\n        startoffset[i] = in.readLong();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\input",
  "methodName" : "write",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void write(DataOutput out) throws IOException\n{\r\n    out.writeLong(totLength);\r\n    out.writeInt(lengths.length);\r\n    for (long length : lengths) {\r\n        out.writeLong(length);\r\n    }\r\n    out.writeInt(paths.length);\r\n    for (Path p : paths) {\r\n        Text.writeString(out, p.toString());\r\n    }\r\n    out.writeInt(startoffset.length);\r\n    for (long length : startoffset) {\r\n        out.writeLong(length);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\input",
  "methodName" : "toString",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "String toString()\n{\r\n    StringBuffer sb = new StringBuffer();\r\n    for (int i = 0; i < paths.length; i++) {\r\n        if (i == 0) {\r\n            sb.append(\"Paths:\");\r\n        }\r\n        sb.append(paths[i].toUri().getPath() + \":\" + startoffset[i] + \"+\" + lengths[i]);\r\n        if (i < paths.length - 1) {\r\n            sb.append(\",\");\r\n        }\r\n    }\r\n    if (locations != null) {\r\n        String locs = \"\";\r\n        StringBuffer locsb = new StringBuffer();\r\n        for (int i = 0; i < locations.length; i++) {\r\n            locsb.append(locations[i] + \":\");\r\n        }\r\n        locs = locsb.toString();\r\n        sb.append(\" Locations:\" + locs + \"; \");\r\n    }\r\n    return sb.toString();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred\\pipes",
  "methodName" : "getRecordReader",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "RecordReader<FloatWritable, NullWritable> getRecordReader(InputSplit genericSplit, JobConf job, Reporter reporter) throws IOException\n{\r\n    return new PipesDummyRecordReader(job, genericSplit);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred\\pipes",
  "methodName" : "getSplits",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "InputSplit[] getSplits(JobConf job, int numSplits) throws IOException\n{\r\n    return ReflectionUtils.newInstance(job.getClass(Submitter.INPUT_FORMAT, TextInputFormat.class, InputFormat.class), job).getSplits(job, numSplits);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "areACLsEnabled",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "boolean areACLsEnabled()\n{\r\n    return conf.getBoolean(MRConfig.MR_ACLS_ENABLED, false);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "constructJobACLs",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "Map<JobACL, AccessControlList> constructJobACLs(Configuration conf)\n{\r\n    Map<JobACL, AccessControlList> acls = new HashMap<JobACL, AccessControlList>();\r\n    if (!areACLsEnabled()) {\r\n        return acls;\r\n    }\r\n    for (JobACL aclName : JobACL.values()) {\r\n        String aclConfigName = aclName.getAclName();\r\n        String aclConfigured = conf.get(aclConfigName);\r\n        if (aclConfigured == null) {\r\n            aclConfigured = \" \";\r\n        }\r\n        acls.put(aclName, new AccessControlList(aclConfigured));\r\n    }\r\n    return acls;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "isMRAdmin",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "boolean isMRAdmin(UserGroupInformation callerUGI)\n{\r\n    if (adminAcl.isUserAllowed(callerUGI)) {\r\n        return true;\r\n    }\r\n    return false;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "checkAccess",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "boolean checkAccess(UserGroupInformation callerUGI, JobACL jobOperation, String jobOwner, AccessControlList jobACL)\n{\r\n    if (LOG.isDebugEnabled()) {\r\n        LOG.debug(\"checkAccess job acls, jobOwner: \" + jobOwner + \" jobacl: \" + jobOperation.toString() + \" user: \" + callerUGI.getShortUserName());\r\n    }\r\n    String user = callerUGI.getShortUserName();\r\n    if (!areACLsEnabled()) {\r\n        return true;\r\n    }\r\n    if (isMRAdmin(callerUGI) || user.equals(jobOwner) || (null != jobACL && jobACL.isUserAllowed(callerUGI))) {\r\n        return true;\r\n    }\r\n    return false;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\partition",
  "methodName" : "getPartition",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "int getPartition(K key, V value, int numReduceTasks)\n{\r\n    int h = SEED ^ key.hashCode();\r\n    h ^= (h >>> 20) ^ (h >>> 12);\r\n    h = h ^ (h >>> 7) ^ (h >>> 4);\r\n    return (h & Integer.MAX_VALUE) % numReduceTasks;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest\\stages",
  "methodName" : "executeStage",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "Result executeStage(final List<TaskManifest> taskManifests) throws IOException\n{\r\n    final List<Path> directories = createAllDirectories(taskManifests);\r\n    LOG.debug(\"{}: Created {} directories\", getName(), directories.size());\r\n    return new Result(new HashSet<>(directories), dirMap);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest\\stages",
  "methodName" : "createAllDirectories",
  "errType" : null,
  "containingMethodsNum" : 15,
  "sourceCodeText" : "List<Path> createAllDirectories(final List<TaskManifest> taskManifests) throws IOException\n{\r\n    final Map<Path, DirEntry> leaves = new HashMap<>();\r\n    final Map<Path, DirEntry> parents = new HashMap<>();\r\n    final Set<Path> filesToDelete = new HashSet<>();\r\n    for (TaskManifest task : taskManifests) {\r\n        final List<DirEntry> destDirectories = task.getDestDirectories();\r\n        Collections.sort(destDirectories, (o1, o2) -> o1.getLevel() - o2.getLevel());\r\n        for (DirEntry entry : destDirectories) {\r\n            final Path path = entry.getDestPath();\r\n            if (!leaves.containsKey(path)) {\r\n                leaves.put(path, entry);\r\n                if (entry.getStatus() == EntryStatus.file) {\r\n                    filesToDelete.add(path);\r\n                }\r\n                final Path parent = path.getParent();\r\n                if (parent != null && leaves.containsKey(parent)) {\r\n                    parents.put(parent, leaves.remove(parent));\r\n                }\r\n            }\r\n        }\r\n    }\r\n    deleteFiles(filesToDelete);\r\n    final int createCount = leaves.size();\r\n    LOG.info(\"Preparing {} directory/directories\", createCount);\r\n    Duration d = measureDurationOfInvocation(getIOStatistics(), OP_CREATE_DIRECTORIES, () -> TaskPool.foreach(leaves.values()).executeWith(getIOProcessors(createCount)).onFailure(this::reportMkDirFailure).stopOnFailure().run(this::createOneDirectory));\r\n    LOG.info(\"Time to prepare directories {}\", humanTime(d.toMillis()));\r\n    return createdDirectories;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest\\stages",
  "methodName" : "reportMkDirFailure",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void reportMkDirFailure(DirEntry dirEntry, Exception e)\n{\r\n    Path path = dirEntry.getDestPath();\r\n    final int count = failureCount.incrementAndGet();\r\n    LOG.warn(\"{}: mkdir failure #{} Failed to create directory \\\"{}\\\": {}\", getName(), count, path, e.toString());\r\n    LOG.debug(\"{}: Full exception details\", getName(), e);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest\\stages",
  "methodName" : "deleteFiles",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void deleteFiles(final Set<Path> filesToDelete) throws IOException\n{\r\n    final int size = filesToDelete.size();\r\n    if (size == 0) {\r\n        return;\r\n    }\r\n    LOG.info(\"{}: Directory entries containing files to delete: {}\", getName(), size);\r\n    Duration d = measureDurationOfInvocation(getIOStatistics(), OP_PREPARE_DIR_ANCESTORS, () -> TaskPool.foreach(filesToDelete).executeWith(getIOProcessors(size)).stopOnFailure().run(dir -> {\r\n        updateAuditContext(OP_PREPARE_DIR_ANCESTORS);\r\n        deleteDirWithFile(dir);\r\n    }));\r\n    LOG.info(\"Time to delete files {}\", humanTime(d.toMillis()));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest\\stages",
  "methodName" : "deleteDirWithFile",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void deleteDirWithFile(Path dir) throws IOException\n{\r\n    progress();\r\n    LOG.info(\"{}: Deleting file {}\", getName(), dir);\r\n    delete(dir, false, OP_DELETE);\r\n    addToDirectoryMap(dir, DirMapState.fileNowDeleted);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest\\stages",
  "methodName" : "createOneDirectory",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void createOneDirectory(final DirEntry dirEntry) throws IOException\n{\r\n    progress();\r\n    final Path dir = dirEntry.getDestPath();\r\n    updateAuditContext(OP_STAGE_JOB_CREATE_TARGET_DIRS);\r\n    final DirMapState state = maybeCreateOneDirectory(dirEntry);\r\n    switch(state) {\r\n        case dirFoundInStore:\r\n            addToDirectoryMap(dir, state);\r\n            break;\r\n        case dirWasCreated:\r\n        case dirCreatedOnSecondAttempt:\r\n            addCreatedDirectory(dir);\r\n            addToDirectoryMap(dir, state);\r\n            break;\r\n        default:\r\n            break;\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest\\stages",
  "methodName" : "maybeCreateOneDirectory",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 17,
  "sourceCodeText" : "DirMapState maybeCreateOneDirectory(DirEntry dirEntry) throws IOException\n{\r\n    final EntryStatus status = dirEntry.getStatus();\r\n    if (status == EntryStatus.dir) {\r\n        return DirMapState.dirFoundInStore;\r\n    }\r\n    if (status == EntryStatus.created_dir) {\r\n        return DirMapState.dirWasCreated;\r\n    }\r\n    final Path path = dirEntry.getDestPath();\r\n    LOG.info(\"Creating directory {}\", path);\r\n    try {\r\n        if (mkdirs(path, false)) {\r\n            return DirMapState.dirWasCreated;\r\n        }\r\n        getIOStatistics().incrementCounter(OP_MKDIRS_RETURNED_FALSE);\r\n        LOG.info(\"{}: mkdirs({}) returned false, attempting to recover\", getName(), path);\r\n    } catch (IOException e) {\r\n        LOG.info(\"{}: mkdir({}) raised exception {}\", getName(), path, e.toString());\r\n        LOG.debug(\"{}: Mkdir stack\", getName(), e);\r\n    }\r\n    FileStatus st = getFileStatusOrNull(path);\r\n    if (st != null) {\r\n        if (!st.isDirectory()) {\r\n            LOG.info(\"{}: Deleting file where a directory should go: {}\", getName(), st);\r\n            delete(path, false, OP_DELETE_FILE_UNDER_DESTINATION);\r\n        } else {\r\n            LOG.warn(\"{}: Even though mkdirs({}) failed, there is now a directory there\", getName(), path);\r\n            return DirMapState.dirFoundInStore;\r\n        }\r\n    } else {\r\n        LOG.warn(\"{}: Although mkdirs({}) returned false, there's nothing at that path to prevent it\", getName(), path);\r\n    }\r\n    if (!mkdirs(path, false)) {\r\n        getIOStatistics().incrementCounter(OP_MKDIRS_RETURNED_FALSE);\r\n        directoryMustExist(\"Creating directory \", path);\r\n    }\r\n    return DirMapState.dirCreatedOnSecondAttempt;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest\\stages",
  "methodName" : "addCreatedDirectory",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void addCreatedDirectory(final Path dir)\n{\r\n    synchronized (createdDirectories) {\r\n        createdDirectories.add(dir);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest\\stages",
  "methodName" : "addToDirectoryMap",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void addToDirectoryMap(final Path dir, DirMapState state)\n{\r\n    if (!dirMap.containsKey(dir)) {\r\n        dirMap.put(dir, state);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest\\stages",
  "methodName" : "executeStage",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "CommitTaskStage.Result executeStage(final Void arguments) throws IOException\n{\r\n    LOG.info(\"{}: Committing task \\\"{}\\\"\", getName(), getTaskAttemptId());\r\n    final TaskAttemptScanDirectoryStage scanStage = new TaskAttemptScanDirectoryStage(getStageConfig());\r\n    TaskManifest manifest = scanStage.apply(arguments);\r\n    scanStage.addExecutionDurationToStatistics(getIOStatistics(), OP_STAGE_TASK_COMMIT);\r\n    final IOStatisticsSnapshot manifestStats = snapshotIOStatistics();\r\n    manifestStats.aggregate(getIOStatistics());\r\n    manifest.setIOStatistics(manifestStats);\r\n    Path manifestPath = new SaveTaskManifestStage(getStageConfig()).apply(manifest);\r\n    return new CommitTaskStage.Result(manifestPath, manifest);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output",
  "methodName" : "getRecordWriter",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "RecordWriter<K, V> getRecordWriter(TaskAttemptContext context) throws IOException, InterruptedException\n{\r\n    return getBaseOut().getRecordWriter(context);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output",
  "methodName" : "checkOutputSpecs",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void checkOutputSpecs(JobContext context) throws IOException, InterruptedException\n{\r\n    getBaseOut().checkOutputSpecs(context);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output",
  "methodName" : "getOutputCommitter",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "OutputCommitter getOutputCommitter(TaskAttemptContext context) throws IOException, InterruptedException\n{\r\n    return getBaseOut().getOutputCommitter(context);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output",
  "methodName" : "getBaseOut",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "OutputFormat<K, V> getBaseOut() throws IOException\n{\r\n    if (baseOut == null) {\r\n        throw new IOException(\"OutputFormat not set for FilterOutputFormat\");\r\n    }\r\n    return baseOut;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\split",
  "methodName" : "createSplitFiles",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void createSplitFiles(Path jobSubmitDir, Configuration conf, FileSystem fs, List<InputSplit> splits) throws IOException, InterruptedException\n{\r\n    T[] array = (T[]) splits.toArray(new InputSplit[splits.size()]);\r\n    createSplitFiles(jobSubmitDir, conf, fs, array);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\split",
  "methodName" : "createSplitFiles",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void createSplitFiles(Path jobSubmitDir, Configuration conf, FileSystem fs, T[] splits) throws IOException, InterruptedException\n{\r\n    FSDataOutputStream out = createFile(fs, JobSubmissionFiles.getJobSplitFile(jobSubmitDir), conf);\r\n    SplitMetaInfo[] info = writeNewSplits(conf, splits, out);\r\n    out.close();\r\n    writeJobSplitMetaInfo(fs, JobSubmissionFiles.getJobSplitMetaFile(jobSubmitDir), new FsPermission(JobSubmissionFiles.JOB_FILE_PERMISSION), splitVersion, info);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\split",
  "methodName" : "createSplitFiles",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void createSplitFiles(Path jobSubmitDir, Configuration conf, FileSystem fs, org.apache.hadoop.mapred.InputSplit[] splits) throws IOException\n{\r\n    FSDataOutputStream out = createFile(fs, JobSubmissionFiles.getJobSplitFile(jobSubmitDir), conf);\r\n    SplitMetaInfo[] info = writeOldSplits(splits, out, conf);\r\n    out.close();\r\n    writeJobSplitMetaInfo(fs, JobSubmissionFiles.getJobSplitMetaFile(jobSubmitDir), new FsPermission(JobSubmissionFiles.JOB_FILE_PERMISSION), splitVersion, info);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\split",
  "methodName" : "createFile",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "FSDataOutputStream createFile(FileSystem fs, Path splitFile, Configuration job) throws IOException\n{\r\n    FSDataOutputStream out = FileSystem.create(fs, splitFile, new FsPermission(JobSubmissionFiles.JOB_FILE_PERMISSION));\r\n    int replication = job.getInt(Job.SUBMIT_REPLICATION, 10);\r\n    fs.setReplication(splitFile, (short) replication);\r\n    writeSplitHeader(out);\r\n    return out;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\split",
  "methodName" : "writeSplitHeader",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void writeSplitHeader(FSDataOutputStream out) throws IOException\n{\r\n    out.write(SPLIT_FILE_HEADER);\r\n    out.writeInt(splitVersion);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\split",
  "methodName" : "writeNewSplits",
  "errType" : null,
  "containingMethodsNum" : 12,
  "sourceCodeText" : "SplitMetaInfo[] writeNewSplits(Configuration conf, T[] array, FSDataOutputStream out) throws IOException, InterruptedException\n{\r\n    SplitMetaInfo[] info = new SplitMetaInfo[array.length];\r\n    if (array.length != 0) {\r\n        SerializationFactory factory = new SerializationFactory(conf);\r\n        int i = 0;\r\n        int maxBlockLocations = conf.getInt(MRConfig.MAX_BLOCK_LOCATIONS_KEY, MRConfig.MAX_BLOCK_LOCATIONS_DEFAULT);\r\n        long offset = out.getPos();\r\n        for (T split : array) {\r\n            long prevCount = out.getPos();\r\n            Text.writeString(out, split.getClass().getName());\r\n            Serializer<T> serializer = factory.getSerializer((Class<T>) split.getClass());\r\n            serializer.open(out);\r\n            serializer.serialize(split);\r\n            long currCount = out.getPos();\r\n            String[] locations = split.getLocations();\r\n            if (locations.length > maxBlockLocations) {\r\n                LOG.warn(\"Max block location exceeded for split: \" + split + \" splitsize: \" + locations.length + \" maxsize: \" + maxBlockLocations);\r\n                locations = Arrays.copyOf(locations, maxBlockLocations);\r\n            }\r\n            info[i++] = new JobSplit.SplitMetaInfo(locations, offset, split.getLength());\r\n            offset += currCount - prevCount;\r\n        }\r\n    }\r\n    return info;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\split",
  "methodName" : "writeOldSplits",
  "errType" : null,
  "containingMethodsNum" : 10,
  "sourceCodeText" : "SplitMetaInfo[] writeOldSplits(org.apache.hadoop.mapred.InputSplit[] splits, FSDataOutputStream out, Configuration conf) throws IOException\n{\r\n    SplitMetaInfo[] info = new SplitMetaInfo[splits.length];\r\n    if (splits.length != 0) {\r\n        int i = 0;\r\n        long offset = out.getPos();\r\n        int maxBlockLocations = conf.getInt(MRConfig.MAX_BLOCK_LOCATIONS_KEY, MRConfig.MAX_BLOCK_LOCATIONS_DEFAULT);\r\n        for (org.apache.hadoop.mapred.InputSplit split : splits) {\r\n            long prevLen = out.getPos();\r\n            Text.writeString(out, split.getClass().getName());\r\n            split.write(out);\r\n            long currLen = out.getPos();\r\n            String[] locations = split.getLocations();\r\n            if (locations.length > maxBlockLocations) {\r\n                LOG.warn(\"Max block location exceeded for split: \" + split + \" splitsize: \" + locations.length + \" maxsize: \" + maxBlockLocations);\r\n                locations = Arrays.copyOf(locations, maxBlockLocations);\r\n            }\r\n            info[i++] = new JobSplit.SplitMetaInfo(locations, offset, split.getLength());\r\n            offset += currLen - prevLen;\r\n        }\r\n    }\r\n    return info;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\split",
  "methodName" : "writeJobSplitMetaInfo",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void writeJobSplitMetaInfo(FileSystem fs, Path filename, FsPermission p, int splitMetaInfoVersion, JobSplit.SplitMetaInfo[] allSplitMetaInfo) throws IOException\n{\r\n    FSDataOutputStream out = FileSystem.create(fs, filename, p);\r\n    out.write(JobSplit.META_SPLIT_FILE_HEADER);\r\n    WritableUtils.writeVInt(out, splitMetaInfoVersion);\r\n    WritableUtils.writeVInt(out, allSplitMetaInfo.length);\r\n    for (JobSplit.SplitMetaInfo splitMetaInfo : allSplitMetaInfo) {\r\n        splitMetaInfo.write(out);\r\n    }\r\n    out.close();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\checkpoint",
  "methodName" : "create",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "CheckpointWriteChannel create() throws IOException\n{\r\n    String name = namingPolicy.getNewName();\r\n    Path p = new Path(name);\r\n    if (p.isUriPathAbsolute()) {\r\n        throw new IOException(\"Checkpoint cannot be an absolute path\");\r\n    }\r\n    return createInternal(new Path(base, p));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\checkpoint",
  "methodName" : "createInternal",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "CheckpointWriteChannel createInternal(Path name) throws IOException\n{\r\n    return new FSCheckpointWriteChannel(name, fs.create(tmpfile(name), replication));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\checkpoint",
  "methodName" : "open",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "CheckpointReadChannel open(CheckpointID id) throws IOException, InterruptedException\n{\r\n    if (!(id instanceof FSCheckpointID)) {\r\n        throw new IllegalArgumentException(\"Mismatched checkpoint type: \" + id.getClass());\r\n    }\r\n    return new FSCheckpointReadChannel(fs.open(((FSCheckpointID) id).getPath()));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\checkpoint",
  "methodName" : "commit",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "CheckpointID commit(CheckpointWriteChannel ch) throws IOException, InterruptedException\n{\r\n    if (ch.isOpen()) {\r\n        ch.close();\r\n    }\r\n    FSCheckpointWriteChannel hch = (FSCheckpointWriteChannel) ch;\r\n    Path dst = hch.getDestination();\r\n    if (!fs.rename(tmpfile(dst), dst)) {\r\n        abort(ch);\r\n        throw new IOException(\"Failed to promote checkpoint\" + tmpfile(dst) + \" -> \" + dst);\r\n    }\r\n    return new FSCheckpointID(hch.getDestination());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\checkpoint",
  "methodName" : "abort",
  "errType" : [ "FileNotFoundException" ],
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void abort(CheckpointWriteChannel ch) throws IOException\n{\r\n    if (ch.isOpen()) {\r\n        ch.close();\r\n    }\r\n    FSCheckpointWriteChannel hch = (FSCheckpointWriteChannel) ch;\r\n    Path tmp = tmpfile(hch.getDestination());\r\n    try {\r\n        if (!fs.delete(tmp, false)) {\r\n            throw new IOException(\"Failed to delete checkpoint during abort\");\r\n        }\r\n    } catch (FileNotFoundException e) {\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\checkpoint",
  "methodName" : "delete",
  "errType" : [ "FileNotFoundException" ],
  "containingMethodsNum" : 3,
  "sourceCodeText" : "boolean delete(CheckpointID id) throws IOException, InterruptedException\n{\r\n    if (!(id instanceof FSCheckpointID)) {\r\n        throw new IllegalArgumentException(\"Mismatched checkpoint type: \" + id.getClass());\r\n    }\r\n    Path tmp = ((FSCheckpointID) id).getPath();\r\n    try {\r\n        return fs.delete(tmp, false);\r\n    } catch (FileNotFoundException e) {\r\n    }\r\n    return true;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\checkpoint",
  "methodName" : "tmpfile",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "Path tmpfile(Path p)\n{\r\n    return new Path(p.getParent(), p.getName() + \".tmp\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred\\join",
  "methodName" : "setWritten",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setWritten(int i)\n{\r\n    written.set(i);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred\\join",
  "methodName" : "clearWritten",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void clearWritten(int i)\n{\r\n    written.clear(i);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred\\join",
  "methodName" : "clearWritten",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void clearWritten()\n{\r\n    written.clear();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred\\lib",
  "methodName" : "setOutputFormatClass",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void setOutputFormatClass(JobConf job, Class<? extends OutputFormat> theClass)\n{\r\n    job.setOutputFormat(LazyOutputFormat.class);\r\n    job.setClass(\"mapreduce.output.lazyoutputformat.outputformat\", theClass, OutputFormat.class);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred\\lib",
  "methodName" : "getRecordWriter",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "RecordWriter<K, V> getRecordWriter(FileSystem ignored, JobConf job, String name, Progressable progress) throws IOException\n{\r\n    if (baseOut == null) {\r\n        getBaseOutputFormat(job);\r\n    }\r\n    return new LazyRecordWriter<K, V>(job, baseOut, name, progress);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred\\lib",
  "methodName" : "checkOutputSpecs",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void checkOutputSpecs(FileSystem ignored, JobConf job) throws IOException\n{\r\n    if (baseOut == null) {\r\n        getBaseOutputFormat(job);\r\n    }\r\n    super.checkOutputSpecs(ignored, job);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred\\lib",
  "methodName" : "getBaseOutputFormat",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void getBaseOutputFormat(JobConf job) throws IOException\n{\r\n    baseOut = ReflectionUtils.newInstance(job.getClass(\"mapreduce.output.lazyoutputformat.outputformat\", null, OutputFormat.class), job);\r\n    if (baseOut == null) {\r\n        throw new IOException(\"Ouput format not set for LazyOutputFormat\");\r\n    }\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "isMapJVM",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean isMapJVM()\n{\r\n    return isMap;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getJobId",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "JobID getJobId()\n{\r\n    return jobId;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "equals",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "boolean equals(Object o)\n{\r\n    if (this == o) {\r\n        return true;\r\n    }\r\n    if (o == null || getClass() != o.getClass()) {\r\n        return false;\r\n    }\r\n    JVMId jvmId1 = (JVMId) o;\r\n    if (isMap != jvmId1.isMap) {\r\n        return false;\r\n    }\r\n    if (jvmId != jvmId1.jvmId) {\r\n        return false;\r\n    }\r\n    if (!jobId.equals(jvmId1.jobId)) {\r\n        return false;\r\n    }\r\n    return true;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "hashCode",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "int hashCode()\n{\r\n    int result = (isMap ? 1 : 0);\r\n    result = 31 * result + jobId.hashCode();\r\n    result = 31 * result + (int) (jvmId ^ (jvmId >>> 32));\r\n    return result;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "compareTo",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "int compareTo(JVMId that)\n{\r\n    int jobComp = this.jobId.compareTo(that.jobId);\r\n    if (jobComp == 0) {\r\n        if (this.isMap == that.isMap) {\r\n            return Long.compare(this.jvmId, that.jvmId);\r\n        } else {\r\n            return this.isMap ? -1 : 1;\r\n        }\r\n    } else {\r\n        return jobComp;\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "toString",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String toString()\n{\r\n    return appendTo(new StringBuilder(JVM)).toString();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getId",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getId()\n{\r\n    return jvmId;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "appendTo",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "StringBuilder appendTo(StringBuilder builder)\n{\r\n    return jobId.appendTo(builder).append(SEPARATOR).append(isMap ? 'm' : 'r').append(SEPARATOR).append(idFormat.format(jvmId));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "readFields",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void readFields(DataInput in) throws IOException\n{\r\n    this.jvmId = in.readLong();\r\n    this.jobId.readFields(in);\r\n    this.isMap = in.readBoolean();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "write",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void write(DataOutput out) throws IOException\n{\r\n    out.writeLong(jvmId);\r\n    jobId.write(out);\r\n    out.writeBoolean(isMap);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "forName",
  "errType" : [ "Exception" ],
  "containingMethodsNum" : 6,
  "sourceCodeText" : "JVMId forName(String str) throws IllegalArgumentException\n{\r\n    if (str == null)\r\n        return null;\r\n    try {\r\n        String[] parts = str.split(\"_\");\r\n        if (parts.length == 5) {\r\n            if (parts[0].equals(JVM)) {\r\n                boolean isMap = false;\r\n                if (parts[3].equals(\"m\"))\r\n                    isMap = true;\r\n                else if (parts[3].equals(\"r\"))\r\n                    isMap = false;\r\n                else\r\n                    throw new Exception();\r\n                return new JVMId(parts[1], Integer.parseInt(parts[2]), isMap, Integer.parseInt(parts[4]));\r\n            }\r\n        }\r\n    } catch (Exception ex) {\r\n    }\r\n    throw new IllegalArgumentException(\"TaskId string : \" + str + \" is not properly formed\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred\\lib",
  "methodName" : "getSplits",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "InputSplit[] getSplits(JobConf job, int numSplits) throws IOException\n{\r\n    List<org.apache.hadoop.mapreduce.InputSplit> newStyleSplits = super.getSplits(Job.getInstance(job));\r\n    InputSplit[] ret = new InputSplit[newStyleSplits.size()];\r\n    for (int pos = 0; pos < newStyleSplits.size(); ++pos) {\r\n        org.apache.hadoop.mapreduce.lib.input.CombineFileSplit newStyleSplit = (org.apache.hadoop.mapreduce.lib.input.CombineFileSplit) newStyleSplits.get(pos);\r\n        ret[pos] = new CombineFileSplit(job, newStyleSplit.getPaths(), newStyleSplit.getStartOffsets(), newStyleSplit.getLengths(), newStyleSplit.getLocations());\r\n    }\r\n    return ret;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred\\lib",
  "methodName" : "createPool",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void createPool(JobConf conf, List<PathFilter> filters)\n{\r\n    createPool(filters);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred\\lib",
  "methodName" : "createPool",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void createPool(JobConf conf, PathFilter... filters)\n{\r\n    createPool(filters);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred\\lib",
  "methodName" : "getRecordReader",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "RecordReader<K, V> getRecordReader(InputSplit split, JobConf job, Reporter reporter) throws IOException",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred\\lib",
  "methodName" : "createRecordReader",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "org.apache.hadoop.mapreduce.RecordReader<K, V> createRecordReader(org.apache.hadoop.mapreduce.InputSplit split, TaskAttemptContext context) throws IOException\n{\r\n    return null;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred\\lib",
  "methodName" : "listStatus",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "FileStatus[] listStatus(JobConf job) throws IOException\n{\r\n    List<FileStatus> result = super.listStatus(Job.getInstance(job));\r\n    return result.toArray(new FileStatus[result.size()]);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred\\lib",
  "methodName" : "isSplitable",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 1,
  "sourceCodeText" : "boolean isSplitable(JobContext context, Path file)\n{\r\n    try {\r\n        return isSplitable(FileSystem.get(context.getConfiguration()), file);\r\n    } catch (IOException ioe) {\r\n        throw new RuntimeException(ioe);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred\\lib",
  "methodName" : "isSplitable",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "boolean isSplitable(FileSystem fs, Path file)\n{\r\n    final CompressionCodec codec = new CompressionCodecFactory(fs.getConf()).getCodec(file);\r\n    if (null == codec) {\r\n        return true;\r\n    }\r\n    return codec instanceof SplittableCompressionCodec;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "getInstance",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Job getInstance() throws IOException\n{\r\n    return getInstance(new Configuration());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "getInstance",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Job getInstance(Configuration conf) throws IOException\n{\r\n    JobConf jobConf = new JobConf(conf);\r\n    return new Job(jobConf);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "getInstance",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "Job getInstance(Configuration conf, String jobName) throws IOException\n{\r\n    Job result = getInstance(conf);\r\n    result.setJobName(jobName);\r\n    return result;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "getInstance",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Job getInstance(JobStatus status, Configuration conf) throws IOException\n{\r\n    return new Job(status, new JobConf(conf));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "getInstance",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Job getInstance(Cluster ignored) throws IOException\n{\r\n    return getInstance();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "getInstance",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Job getInstance(Cluster ignored, Configuration conf) throws IOException\n{\r\n    return getInstance(conf);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "getInstance",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "Job getInstance(Cluster cluster, JobStatus status, Configuration conf) throws IOException\n{\r\n    Job job = getInstance(status, conf);\r\n    job.setCluster(cluster);\r\n    return job;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "ensureState",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void ensureState(JobState state) throws IllegalStateException\n{\r\n    if (state != this.state) {\r\n        throw new IllegalStateException(\"Job in state \" + this.state + \" instead of \" + state);\r\n    }\r\n    if (state == JobState.RUNNING && cluster == null) {\r\n        throw new IllegalStateException(\"Job in state \" + this.state + \", but it isn't attached to any job tracker!\");\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "ensureFreshStatus",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void ensureFreshStatus() throws IOException\n{\r\n    if (System.currentTimeMillis() - statustime > MAX_JOBSTATUS_AGE) {\r\n        updateStatus();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "updateStatus",
  "errType" : [ "InterruptedException" ],
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void updateStatus() throws IOException\n{\r\n    try {\r\n        this.status = ugi.doAs(new PrivilegedExceptionAction<JobStatus>() {\r\n\r\n            @Override\r\n            public JobStatus run() throws IOException, InterruptedException {\r\n                return cluster.getClient().getJobStatus(getJobID());\r\n            }\r\n        });\r\n    } catch (InterruptedException ie) {\r\n        throw new IOException(ie);\r\n    }\r\n    if (this.status == null) {\r\n        throw new IOException(\"Job status not available \");\r\n    }\r\n    this.statustime = System.currentTimeMillis();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "getStatus",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "JobStatus getStatus() throws IOException, InterruptedException\n{\r\n    ensureState(JobState.RUNNING);\r\n    updateStatus();\r\n    return status;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "getJobState",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "JobStatus.State getJobState() throws IOException, InterruptedException\n{\r\n    ensureState(JobState.RUNNING);\r\n    updateStatus();\r\n    return status.getState();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "getTrackingURL",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "String getTrackingURL()\n{\r\n    ensureState(JobState.RUNNING);\r\n    return status.getTrackingUrl().toString();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "getJobFile",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "String getJobFile()\n{\r\n    ensureState(JobState.RUNNING);\r\n    return status.getJobFile();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "getStartTime",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "long getStartTime()\n{\r\n    ensureState(JobState.RUNNING);\r\n    return status.getStartTime();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "getFinishTime",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "long getFinishTime() throws IOException, InterruptedException\n{\r\n    ensureState(JobState.RUNNING);\r\n    updateStatus();\r\n    return status.getFinishTime();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "getSchedulingInfo",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "String getSchedulingInfo()\n{\r\n    ensureState(JobState.RUNNING);\r\n    return status.getSchedulingInfo();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "getPriority",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "JobPriority getPriority() throws IOException, InterruptedException\n{\r\n    ensureState(JobState.RUNNING);\r\n    updateStatus();\r\n    return status.getPriority();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "getJobName",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "String getJobName()\n{\r\n    if (state == JobState.DEFINE || status == null) {\r\n        return super.getJobName();\r\n    }\r\n    ensureState(JobState.RUNNING);\r\n    return status.getJobName();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "getHistoryUrl",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "String getHistoryUrl() throws IOException, InterruptedException\n{\r\n    ensureState(JobState.RUNNING);\r\n    updateStatus();\r\n    return status.getHistoryFile();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "isRetired",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "boolean isRetired() throws IOException, InterruptedException\n{\r\n    ensureState(JobState.RUNNING);\r\n    updateStatus();\r\n    return status.isRetired();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "getCluster",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Cluster getCluster()\n{\r\n    return cluster;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "setCluster",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setCluster(Cluster cluster)\n{\r\n    this.cluster = cluster;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "toString",
  "errType" : [ "IOException", "InterruptedException" ],
  "containingMethodsNum" : 22,
  "sourceCodeText" : "String toString()\n{\r\n    ensureState(JobState.RUNNING);\r\n    String reasonforFailure = \" \";\r\n    int numMaps = 0;\r\n    int numReduces = 0;\r\n    try {\r\n        updateStatus();\r\n        if (status.getState().equals(JobStatus.State.FAILED))\r\n            reasonforFailure = getTaskFailureEventString();\r\n        numMaps = getTaskReports(TaskType.MAP).length;\r\n        numReduces = getTaskReports(TaskType.REDUCE).length;\r\n    } catch (IOException e) {\r\n    } catch (InterruptedException ie) {\r\n    }\r\n    StringBuffer sb = new StringBuffer();\r\n    sb.append(\"Job: \").append(status.getJobID()).append(\"\\n\");\r\n    sb.append(\"Job File: \").append(status.getJobFile()).append(\"\\n\");\r\n    sb.append(\"Job Tracking URL : \").append(status.getTrackingUrl());\r\n    sb.append(\"\\n\");\r\n    sb.append(\"Uber job : \").append(status.isUber()).append(\"\\n\");\r\n    sb.append(\"Number of maps: \").append(numMaps).append(\"\\n\");\r\n    sb.append(\"Number of reduces: \").append(numReduces).append(\"\\n\");\r\n    sb.append(\"map() completion: \");\r\n    sb.append(status.getMapProgress()).append(\"\\n\");\r\n    sb.append(\"reduce() completion: \");\r\n    sb.append(status.getReduceProgress()).append(\"\\n\");\r\n    sb.append(\"Job state: \");\r\n    sb.append(status.getState()).append(\"\\n\");\r\n    sb.append(\"retired: \").append(status.isRetired()).append(\"\\n\");\r\n    sb.append(\"reason for failure: \").append(reasonforFailure);\r\n    return sb.toString();\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "getTaskFailureEventString",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "String getTaskFailureEventString() throws IOException, InterruptedException\n{\r\n    int failCount = 1;\r\n    TaskCompletionEvent lastEvent = null;\r\n    TaskCompletionEvent[] events = ugi.doAs(new PrivilegedExceptionAction<TaskCompletionEvent[]>() {\r\n\r\n        @Override\r\n        public TaskCompletionEvent[] run() throws IOException, InterruptedException {\r\n            return cluster.getClient().getTaskCompletionEvents(status.getJobID(), 0, 10);\r\n        }\r\n    });\r\n    for (TaskCompletionEvent event : events) {\r\n        if (event.getStatus().equals(TaskCompletionEvent.Status.FAILED)) {\r\n            failCount++;\r\n            lastEvent = event;\r\n        }\r\n    }\r\n    if (lastEvent == null) {\r\n        return \"There are no failed tasks for the job. \" + \"Job is failed due to some other reason and reason \" + \"can be found in the logs.\";\r\n    }\r\n    String[] taskAttemptID = lastEvent.getTaskAttemptId().toString().split(\"_\", 2);\r\n    String taskID = taskAttemptID[1].substring(0, taskAttemptID[1].length() - 2);\r\n    return (\" task \" + taskID + \" failed \" + failCount + \" times \" + \"For details check tasktracker at: \" + lastEvent.getTaskTrackerHttp());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "getTaskReports",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "TaskReport[] getTaskReports(TaskType type) throws IOException, InterruptedException\n{\r\n    ensureState(JobState.RUNNING);\r\n    final TaskType tmpType = type;\r\n    return ugi.doAs(new PrivilegedExceptionAction<TaskReport[]>() {\r\n\r\n        public TaskReport[] run() throws IOException, InterruptedException {\r\n            return cluster.getClient().getTaskReports(getJobID(), tmpType);\r\n        }\r\n    });\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "mapProgress",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "float mapProgress() throws IOException\n{\r\n    ensureState(JobState.RUNNING);\r\n    ensureFreshStatus();\r\n    return status.getMapProgress();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "reduceProgress",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "float reduceProgress() throws IOException\n{\r\n    ensureState(JobState.RUNNING);\r\n    ensureFreshStatus();\r\n    return status.getReduceProgress();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "cleanupProgress",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "float cleanupProgress() throws IOException, InterruptedException\n{\r\n    ensureState(JobState.RUNNING);\r\n    ensureFreshStatus();\r\n    return status.getCleanupProgress();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "setupProgress",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "float setupProgress() throws IOException\n{\r\n    ensureState(JobState.RUNNING);\r\n    ensureFreshStatus();\r\n    return status.getSetupProgress();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "isComplete",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "boolean isComplete() throws IOException\n{\r\n    ensureState(JobState.RUNNING);\r\n    updateStatus();\r\n    return status.isJobComplete();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "isSuccessful",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "boolean isSuccessful() throws IOException\n{\r\n    ensureState(JobState.RUNNING);\r\n    updateStatus();\r\n    return status.getState() == JobStatus.State.SUCCEEDED;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "killJob",
  "errType" : [ "InterruptedException" ],
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void killJob() throws IOException\n{\r\n    ensureState(JobState.RUNNING);\r\n    try {\r\n        cluster.getClient().killJob(getJobID());\r\n    } catch (InterruptedException ie) {\r\n        throw new IOException(ie);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "setPriority",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void setPriority(JobPriority jobPriority) throws IOException, InterruptedException\n{\r\n    if (state == JobState.DEFINE) {\r\n        if (jobPriority == JobPriority.UNDEFINED_PRIORITY) {\r\n            conf.setJobPriorityAsInteger(convertPriorityToInteger(jobPriority));\r\n        } else {\r\n            conf.setJobPriority(org.apache.hadoop.mapred.JobPriority.valueOf(jobPriority.name()));\r\n        }\r\n    } else {\r\n        ensureState(JobState.RUNNING);\r\n        final int tmpPriority = convertPriorityToInteger(jobPriority);\r\n        ugi.doAs(new PrivilegedExceptionAction<Object>() {\r\n\r\n            @Override\r\n            public Object run() throws IOException, InterruptedException {\r\n                cluster.getClient().setJobPriority(getJobID(), Integer.toString(tmpPriority));\r\n                return null;\r\n            }\r\n        });\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "setPriorityAsInteger",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void setPriorityAsInteger(int jobPriority) throws IOException, InterruptedException\n{\r\n    if (state == JobState.DEFINE) {\r\n        conf.setJobPriorityAsInteger(jobPriority);\r\n    } else {\r\n        ensureState(JobState.RUNNING);\r\n        final int tmpPriority = jobPriority;\r\n        ugi.doAs(new PrivilegedExceptionAction<Object>() {\r\n\r\n            @Override\r\n            public Object run() throws IOException, InterruptedException {\r\n                cluster.getClient().setJobPriority(getJobID(), Integer.toString(tmpPriority));\r\n                return null;\r\n            }\r\n        });\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "convertPriorityToInteger",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int convertPriorityToInteger(JobPriority jobPriority)\n{\r\n    switch(jobPriority) {\r\n        case VERY_HIGH:\r\n            return 5;\r\n        case HIGH:\r\n            return 4;\r\n        case NORMAL:\r\n            return 3;\r\n        case LOW:\r\n            return 2;\r\n        case VERY_LOW:\r\n            return 1;\r\n        case DEFAULT:\r\n            return 0;\r\n        default:\r\n            break;\r\n    }\r\n    return 0;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "getTaskCompletionEvents",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "TaskCompletionEvent[] getTaskCompletionEvents(final int startFrom, final int numEvents) throws IOException, InterruptedException\n{\r\n    ensureState(JobState.RUNNING);\r\n    return ugi.doAs(new PrivilegedExceptionAction<TaskCompletionEvent[]>() {\r\n\r\n        @Override\r\n        public TaskCompletionEvent[] run() throws IOException, InterruptedException {\r\n            return cluster.getClient().getTaskCompletionEvents(getJobID(), startFrom, numEvents);\r\n        }\r\n    });\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "getTaskCompletionEvents",
  "errType" : [ "InterruptedException" ],
  "containingMethodsNum" : 2,
  "sourceCodeText" : "org.apache.hadoop.mapred.TaskCompletionEvent[] getTaskCompletionEvents(final int startFrom) throws IOException\n{\r\n    try {\r\n        TaskCompletionEvent[] events = getTaskCompletionEvents(startFrom, 10);\r\n        org.apache.hadoop.mapred.TaskCompletionEvent[] retEvents = new org.apache.hadoop.mapred.TaskCompletionEvent[events.length];\r\n        for (int i = 0; i < events.length; i++) {\r\n            retEvents[i] = org.apache.hadoop.mapred.TaskCompletionEvent.downgrade(events[i]);\r\n        }\r\n        return retEvents;\r\n    } catch (InterruptedException ie) {\r\n        throw new IOException(ie);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "killTask",
  "errType" : [ "InterruptedException" ],
  "containingMethodsNum" : 2,
  "sourceCodeText" : "boolean killTask(final TaskAttemptID taskId, final boolean shouldFail) throws IOException\n{\r\n    ensureState(JobState.RUNNING);\r\n    try {\r\n        return ugi.doAs(new PrivilegedExceptionAction<Boolean>() {\r\n\r\n            public Boolean run() throws IOException, InterruptedException {\r\n                return cluster.getClient().killTask(taskId, shouldFail);\r\n            }\r\n        });\r\n    } catch (InterruptedException ie) {\r\n        throw new IOException(ie);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "killTask",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void killTask(final TaskAttemptID taskId) throws IOException\n{\r\n    killTask(taskId, false);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "failTask",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void failTask(final TaskAttemptID taskId) throws IOException\n{\r\n    killTask(taskId, true);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "getCounters",
  "errType" : [ "InterruptedException" ],
  "containingMethodsNum" : 2,
  "sourceCodeText" : "Counters getCounters() throws IOException\n{\r\n    ensureState(JobState.RUNNING);\r\n    try {\r\n        return ugi.doAs(new PrivilegedExceptionAction<Counters>() {\r\n\r\n            @Override\r\n            public Counters run() throws IOException, InterruptedException {\r\n                return cluster.getClient().getJobCounters(getJobID());\r\n            }\r\n        });\r\n    } catch (InterruptedException ie) {\r\n        throw new IOException(ie);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "getTaskDiagnostics",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "String[] getTaskDiagnostics(final TaskAttemptID taskid) throws IOException, InterruptedException\n{\r\n    ensureState(JobState.RUNNING);\r\n    return ugi.doAs(new PrivilegedExceptionAction<String[]>() {\r\n\r\n        @Override\r\n        public String[] run() throws IOException, InterruptedException {\r\n            return cluster.getClient().getTaskDiagnostics(taskid);\r\n        }\r\n    });\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "setNumReduceTasks",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void setNumReduceTasks(int tasks) throws IllegalStateException\n{\r\n    ensureState(JobState.DEFINE);\r\n    conf.setNumReduceTasks(tasks);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "setWorkingDirectory",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void setWorkingDirectory(Path dir) throws IOException\n{\r\n    ensureState(JobState.DEFINE);\r\n    conf.setWorkingDirectory(dir);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "setInputFormatClass",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void setInputFormatClass(Class<? extends InputFormat> cls) throws IllegalStateException\n{\r\n    ensureState(JobState.DEFINE);\r\n    conf.setClass(INPUT_FORMAT_CLASS_ATTR, cls, InputFormat.class);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "setOutputFormatClass",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void setOutputFormatClass(Class<? extends OutputFormat> cls) throws IllegalStateException\n{\r\n    ensureState(JobState.DEFINE);\r\n    conf.setClass(OUTPUT_FORMAT_CLASS_ATTR, cls, OutputFormat.class);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "setMapperClass",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void setMapperClass(Class<? extends Mapper> cls) throws IllegalStateException\n{\r\n    ensureState(JobState.DEFINE);\r\n    conf.setClass(MAP_CLASS_ATTR, cls, Mapper.class);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "setJarByClass",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void setJarByClass(Class<?> cls)\n{\r\n    ensureState(JobState.DEFINE);\r\n    conf.setJarByClass(cls);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "setJar",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void setJar(String jar)\n{\r\n    ensureState(JobState.DEFINE);\r\n    conf.setJar(jar);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "setUser",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void setUser(String user)\n{\r\n    ensureState(JobState.DEFINE);\r\n    conf.setUser(user);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "setCombinerClass",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void setCombinerClass(Class<? extends Reducer> cls) throws IllegalStateException\n{\r\n    ensureState(JobState.DEFINE);\r\n    conf.setClass(COMBINE_CLASS_ATTR, cls, Reducer.class);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "setReducerClass",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void setReducerClass(Class<? extends Reducer> cls) throws IllegalStateException\n{\r\n    ensureState(JobState.DEFINE);\r\n    conf.setClass(REDUCE_CLASS_ATTR, cls, Reducer.class);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "setPartitionerClass",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void setPartitionerClass(Class<? extends Partitioner> cls) throws IllegalStateException\n{\r\n    ensureState(JobState.DEFINE);\r\n    conf.setClass(PARTITIONER_CLASS_ATTR, cls, Partitioner.class);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "setMapOutputKeyClass",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void setMapOutputKeyClass(Class<?> theClass) throws IllegalStateException\n{\r\n    ensureState(JobState.DEFINE);\r\n    conf.setMapOutputKeyClass(theClass);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "setMapOutputValueClass",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void setMapOutputValueClass(Class<?> theClass) throws IllegalStateException\n{\r\n    ensureState(JobState.DEFINE);\r\n    conf.setMapOutputValueClass(theClass);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "setOutputKeyClass",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void setOutputKeyClass(Class<?> theClass) throws IllegalStateException\n{\r\n    ensureState(JobState.DEFINE);\r\n    conf.setOutputKeyClass(theClass);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "setOutputValueClass",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void setOutputValueClass(Class<?> theClass) throws IllegalStateException\n{\r\n    ensureState(JobState.DEFINE);\r\n    conf.setOutputValueClass(theClass);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "setCombinerKeyGroupingComparatorClass",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void setCombinerKeyGroupingComparatorClass(Class<? extends RawComparator> cls) throws IllegalStateException\n{\r\n    ensureState(JobState.DEFINE);\r\n    conf.setCombinerKeyGroupingComparator(cls);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "setSortComparatorClass",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void setSortComparatorClass(Class<? extends RawComparator> cls) throws IllegalStateException\n{\r\n    ensureState(JobState.DEFINE);\r\n    conf.setOutputKeyComparatorClass(cls);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "setGroupingComparatorClass",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void setGroupingComparatorClass(Class<? extends RawComparator> cls) throws IllegalStateException\n{\r\n    ensureState(JobState.DEFINE);\r\n    conf.setOutputValueGroupingComparator(cls);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "setJobName",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void setJobName(String name) throws IllegalStateException\n{\r\n    ensureState(JobState.DEFINE);\r\n    conf.setJobName(name);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "setSpeculativeExecution",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void setSpeculativeExecution(boolean speculativeExecution)\n{\r\n    ensureState(JobState.DEFINE);\r\n    conf.setSpeculativeExecution(speculativeExecution);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "setMapSpeculativeExecution",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void setMapSpeculativeExecution(boolean speculativeExecution)\n{\r\n    ensureState(JobState.DEFINE);\r\n    conf.setMapSpeculativeExecution(speculativeExecution);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "setReduceSpeculativeExecution",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void setReduceSpeculativeExecution(boolean speculativeExecution)\n{\r\n    ensureState(JobState.DEFINE);\r\n    conf.setReduceSpeculativeExecution(speculativeExecution);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "setJobSetupCleanupNeeded",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void setJobSetupCleanupNeeded(boolean needed)\n{\r\n    ensureState(JobState.DEFINE);\r\n    conf.setBoolean(SETUP_CLEANUP_NEEDED, needed);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "setCacheArchives",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void setCacheArchives(URI[] archives)\n{\r\n    ensureState(JobState.DEFINE);\r\n    setCacheArchives(archives, conf);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "setCacheArchives",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void setCacheArchives(URI[] archives, Configuration conf)\n{\r\n    String cacheArchives = StringUtils.uriToString(archives);\r\n    conf.set(MRJobConfig.CACHE_ARCHIVES, cacheArchives);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "setCacheFiles",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void setCacheFiles(URI[] files)\n{\r\n    ensureState(JobState.DEFINE);\r\n    setCacheFiles(files, conf);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "setCacheFiles",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void setCacheFiles(URI[] files, Configuration conf)\n{\r\n    String cacheFiles = StringUtils.uriToString(files);\r\n    conf.set(MRJobConfig.CACHE_FILES, cacheFiles);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "addCacheArchive",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void addCacheArchive(URI uri)\n{\r\n    ensureState(JobState.DEFINE);\r\n    addCacheArchive(uri, conf);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "addCacheArchive",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void addCacheArchive(URI uri, Configuration conf)\n{\r\n    String archives = conf.get(MRJobConfig.CACHE_ARCHIVES);\r\n    conf.set(MRJobConfig.CACHE_ARCHIVES, archives == null ? uri.toString() : archives + \",\" + uri.toString());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "addCacheFile",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void addCacheFile(URI uri)\n{\r\n    ensureState(JobState.DEFINE);\r\n    addCacheFile(uri, conf);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "addCacheFile",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void addCacheFile(URI uri, Configuration conf)\n{\r\n    String files = conf.get(MRJobConfig.CACHE_FILES);\r\n    conf.set(MRJobConfig.CACHE_FILES, files == null ? uri.toString() : files + \",\" + uri.toString());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "addFileToClassPath",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void addFileToClassPath(Path file) throws IOException\n{\r\n    ensureState(JobState.DEFINE);\r\n    addFileToClassPath(file, conf, file.getFileSystem(conf));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "addFileToClassPath",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void addFileToClassPath(Path file, Configuration conf, FileSystem fs)\n{\r\n    addFileToClassPath(file, conf, fs, true);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "addFileToClassPath",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void addFileToClassPath(Path file, Configuration conf, FileSystem fs, boolean addToCache)\n{\r\n    String classpath = conf.get(MRJobConfig.CLASSPATH_FILES);\r\n    conf.set(MRJobConfig.CLASSPATH_FILES, classpath == null ? file.toString() : classpath + \",\" + file.toString());\r\n    if (addToCache) {\r\n        URI uri = fs.makeQualified(file).toUri();\r\n        Job.addCacheFile(uri, conf);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "addArchiveToClassPath",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void addArchiveToClassPath(Path archive) throws IOException\n{\r\n    ensureState(JobState.DEFINE);\r\n    addArchiveToClassPath(archive, conf, archive.getFileSystem(conf));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "addArchiveToClassPath",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void addArchiveToClassPath(Path archive, Configuration conf, FileSystem fs)\n{\r\n    String classpath = conf.get(MRJobConfig.CLASSPATH_ARCHIVES);\r\n    conf.set(MRJobConfig.CLASSPATH_ARCHIVES, classpath == null ? archive.toString() : classpath + \",\" + archive.toString());\r\n    URI uri = fs.makeQualified(archive).toUri();\r\n    Job.addCacheArchive(uri, conf);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "createSymlink",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void createSymlink()\n{\r\n    ensureState(JobState.DEFINE);\r\n    DistributedCache.createSymlink(conf);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "setMaxMapAttempts",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void setMaxMapAttempts(int n)\n{\r\n    ensureState(JobState.DEFINE);\r\n    conf.setMaxMapAttempts(n);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "setMaxReduceAttempts",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void setMaxReduceAttempts(int n)\n{\r\n    ensureState(JobState.DEFINE);\r\n    conf.setMaxReduceAttempts(n);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "setProfileEnabled",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void setProfileEnabled(boolean newValue)\n{\r\n    ensureState(JobState.DEFINE);\r\n    conf.setProfileEnabled(newValue);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "setProfileParams",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void setProfileParams(String value)\n{\r\n    ensureState(JobState.DEFINE);\r\n    conf.setProfileParams(value);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "setProfileTaskRange",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void setProfileTaskRange(boolean isMap, String newValue)\n{\r\n    ensureState(JobState.DEFINE);\r\n    conf.setProfileTaskRange(isMap, newValue);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "ensureNotSet",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void ensureNotSet(String attr, String msg) throws IOException\n{\r\n    if (conf.get(attr) != null) {\r\n        throw new IOException(attr + \" is incompatible with \" + msg + \" mode.\");\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "setCancelDelegationTokenUponJobCompletion",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void setCancelDelegationTokenUponJobCompletion(boolean value)\n{\r\n    ensureState(JobState.DEFINE);\r\n    conf.setBoolean(JOB_CANCEL_DELEGATION_TOKEN, value);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "setUseNewAPI",
  "errType" : null,
  "containingMethodsNum" : 17,
  "sourceCodeText" : "void setUseNewAPI() throws IOException\n{\r\n    int numReduces = conf.getNumReduceTasks();\r\n    String oldMapperClass = \"mapred.mapper.class\";\r\n    String oldReduceClass = \"mapred.reducer.class\";\r\n    conf.setBooleanIfUnset(\"mapred.mapper.new-api\", conf.get(oldMapperClass) == null);\r\n    if (conf.getUseNewMapper()) {\r\n        String mode = \"new map API\";\r\n        ensureNotSet(\"mapred.input.format.class\", mode);\r\n        ensureNotSet(oldMapperClass, mode);\r\n        if (numReduces != 0) {\r\n            ensureNotSet(\"mapred.partitioner.class\", mode);\r\n        } else {\r\n            ensureNotSet(\"mapred.output.format.class\", mode);\r\n        }\r\n    } else {\r\n        String mode = \"map compatibility\";\r\n        ensureNotSet(INPUT_FORMAT_CLASS_ATTR, mode);\r\n        ensureNotSet(MAP_CLASS_ATTR, mode);\r\n        if (numReduces != 0) {\r\n            ensureNotSet(PARTITIONER_CLASS_ATTR, mode);\r\n        } else {\r\n            ensureNotSet(OUTPUT_FORMAT_CLASS_ATTR, mode);\r\n        }\r\n    }\r\n    if (numReduces != 0) {\r\n        conf.setBooleanIfUnset(\"mapred.reducer.new-api\", conf.get(oldReduceClass) == null);\r\n        if (conf.getUseNewReducer()) {\r\n            String mode = \"new reduce API\";\r\n            ensureNotSet(\"mapred.output.format.class\", mode);\r\n            ensureNotSet(oldReduceClass, mode);\r\n        } else {\r\n            String mode = \"reduce compatibility\";\r\n            ensureNotSet(OUTPUT_FORMAT_CLASS_ATTR, mode);\r\n            ensureNotSet(REDUCE_CLASS_ATTR, mode);\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "addFileToSharedCache",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "boolean addFileToSharedCache(URI resource, Configuration conf)\n{\r\n    SharedCacheConfig scConfig = new SharedCacheConfig();\r\n    scConfig.init(conf);\r\n    if (scConfig.isSharedCacheFilesEnabled()) {\r\n        String files = conf.get(MRJobConfig.FILES_FOR_SHARED_CACHE);\r\n        conf.set(MRJobConfig.FILES_FOR_SHARED_CACHE, files == null ? resource.toString() : files + \",\" + resource.toString());\r\n        return true;\r\n    } else {\r\n        return false;\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "addFileToSharedCacheAndClasspath",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "boolean addFileToSharedCacheAndClasspath(URI resource, Configuration conf)\n{\r\n    SharedCacheConfig scConfig = new SharedCacheConfig();\r\n    scConfig.init(conf);\r\n    if (scConfig.isSharedCacheLibjarsEnabled()) {\r\n        String files = conf.get(MRJobConfig.FILES_FOR_CLASSPATH_AND_SHARED_CACHE);\r\n        conf.set(MRJobConfig.FILES_FOR_CLASSPATH_AND_SHARED_CACHE, files == null ? resource.toString() : files + \",\" + resource.toString());\r\n        return true;\r\n    } else {\r\n        return false;\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "addArchiveToSharedCache",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "boolean addArchiveToSharedCache(URI resource, Configuration conf)\n{\r\n    SharedCacheConfig scConfig = new SharedCacheConfig();\r\n    scConfig.init(conf);\r\n    if (scConfig.isSharedCacheArchivesEnabled()) {\r\n        String files = conf.get(MRJobConfig.ARCHIVES_FOR_SHARED_CACHE);\r\n        conf.set(MRJobConfig.ARCHIVES_FOR_SHARED_CACHE, files == null ? resource.toString() : files + \",\" + resource.toString());\r\n        return true;\r\n    } else {\r\n        return false;\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "setFileSharedCacheUploadPolicies",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setFileSharedCacheUploadPolicies(Configuration conf, Map<String, Boolean> policies)\n{\r\n    setSharedCacheUploadPolicies(conf, policies, true);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "setArchiveSharedCacheUploadPolicies",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setArchiveSharedCacheUploadPolicies(Configuration conf, Map<String, Boolean> policies)\n{\r\n    setSharedCacheUploadPolicies(conf, policies, false);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "setSharedCacheUploadPolicies",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void setSharedCacheUploadPolicies(Configuration conf, Map<String, Boolean> policies, boolean areFiles)\n{\r\n    String confParam = areFiles ? MRJobConfig.CACHE_FILES_SHARED_CACHE_UPLOAD_POLICIES : MRJobConfig.CACHE_ARCHIVES_SHARED_CACHE_UPLOAD_POLICIES;\r\n    if (policies == null || policies.size() == 0) {\r\n        conf.set(confParam, \"\");\r\n        return;\r\n    }\r\n    StringBuilder sb = new StringBuilder();\r\n    policies.forEach((k, v) -> sb.append(k).append(DELIM).append(v).append(\",\"));\r\n    sb.deleteCharAt(sb.length() - 1);\r\n    conf.set(confParam, sb.toString());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "getSharedCacheUploadPolicies",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "Map<String, Boolean> getSharedCacheUploadPolicies(Configuration conf, boolean areFiles)\n{\r\n    String confParam = areFiles ? MRJobConfig.CACHE_FILES_SHARED_CACHE_UPLOAD_POLICIES : MRJobConfig.CACHE_ARCHIVES_SHARED_CACHE_UPLOAD_POLICIES;\r\n    Collection<String> policies = conf.getStringCollection(confParam);\r\n    String[] policy;\r\n    Map<String, Boolean> policyMap = new LinkedHashMap<String, Boolean>();\r\n    for (String s : policies) {\r\n        policy = s.split(DELIM);\r\n        if (policy.length != 2) {\r\n            LOG.error(confParam + \" is mis-formatted, returning empty shared cache upload policies.\" + \" Error on [\" + s + \"]\");\r\n            return new LinkedHashMap<String, Boolean>();\r\n        }\r\n        policyMap.put(policy[0], Boolean.parseBoolean(policy[1]));\r\n    }\r\n    return policyMap;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "getFileSharedCacheUploadPolicies",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Map<String, Boolean> getFileSharedCacheUploadPolicies(Configuration conf)\n{\r\n    return getSharedCacheUploadPolicies(conf, true);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "getArchiveSharedCacheUploadPolicies",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Map<String, Boolean> getArchiveSharedCacheUploadPolicies(Configuration conf)\n{\r\n    return getSharedCacheUploadPolicies(conf, false);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "connect",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void connect() throws IOException, InterruptedException, ClassNotFoundException\n{\r\n    if (cluster == null) {\r\n        cluster = ugi.doAs(new PrivilegedExceptionAction<Cluster>() {\r\n\r\n            public Cluster run() throws IOException, InterruptedException, ClassNotFoundException {\r\n                return new Cluster(getConfiguration());\r\n            }\r\n        });\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "isConnected",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean isConnected()\n{\r\n    return cluster != null;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "getJobSubmitter",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "JobSubmitter getJobSubmitter(FileSystem fs, ClientProtocol submitClient) throws IOException\n{\r\n    return new JobSubmitter(fs, submitClient);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "submit",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void submit() throws IOException, InterruptedException, ClassNotFoundException\n{\r\n    ensureState(JobState.DEFINE);\r\n    setUseNewAPI();\r\n    connect();\r\n    final JobSubmitter submitter = getJobSubmitter(cluster.getFileSystem(), cluster.getClient());\r\n    status = ugi.doAs(new PrivilegedExceptionAction<JobStatus>() {\r\n\r\n        public JobStatus run() throws IOException, InterruptedException, ClassNotFoundException {\r\n            return submitter.submitJobInternal(Job.this, cluster);\r\n        }\r\n    });\r\n    state = JobState.RUNNING;\r\n    LOG.info(\"The url to track the job: \" + getTrackingURL());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "waitForCompletion",
  "errType" : [ "InterruptedException" ],
  "containingMethodsNum" : 6,
  "sourceCodeText" : "boolean waitForCompletion(boolean verbose) throws IOException, InterruptedException, ClassNotFoundException\n{\r\n    if (state == JobState.DEFINE) {\r\n        submit();\r\n    }\r\n    if (verbose) {\r\n        monitorAndPrintJob();\r\n    } else {\r\n        int completionPollIntervalMillis = Job.getCompletionPollInterval(cluster.getConf());\r\n        while (!isComplete()) {\r\n            try {\r\n                Thread.sleep(completionPollIntervalMillis);\r\n            } catch (InterruptedException ie) {\r\n            }\r\n        }\r\n    }\r\n    return isSuccessful();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "monitorAndPrintJob",
  "errType" : null,
  "containingMethodsNum" : 24,
  "sourceCodeText" : "boolean monitorAndPrintJob() throws IOException, InterruptedException\n{\r\n    String lastReport = null;\r\n    Job.TaskStatusFilter filter;\r\n    Configuration clientConf = getConfiguration();\r\n    filter = Job.getTaskOutputFilter(clientConf);\r\n    JobID jobId = getJobID();\r\n    LOG.info(\"Running job: \" + jobId);\r\n    int eventCounter = 0;\r\n    boolean profiling = getProfileEnabled();\r\n    IntegerRanges mapRanges = getProfileTaskRange(true);\r\n    IntegerRanges reduceRanges = getProfileTaskRange(false);\r\n    int progMonitorPollIntervalMillis = Job.getProgressPollInterval(clientConf);\r\n    boolean reportedAfterCompletion = false;\r\n    boolean reportedUberMode = false;\r\n    while (!isComplete() || !reportedAfterCompletion) {\r\n        if (isComplete()) {\r\n            reportedAfterCompletion = true;\r\n        } else {\r\n            Thread.sleep(progMonitorPollIntervalMillis);\r\n        }\r\n        if (status.getState() == JobStatus.State.PREP) {\r\n            continue;\r\n        }\r\n        if (!reportedUberMode) {\r\n            reportedUberMode = true;\r\n            LOG.info(\"Job \" + jobId + \" running in uber mode : \" + isUber());\r\n        }\r\n        String report = (\" map \" + StringUtils.formatPercent(mapProgress(), 0) + \" reduce \" + StringUtils.formatPercent(reduceProgress(), 0));\r\n        if (!report.equals(lastReport)) {\r\n            LOG.info(report);\r\n            lastReport = report;\r\n        }\r\n        TaskCompletionEvent[] events = getTaskCompletionEvents(eventCounter, 10);\r\n        eventCounter += events.length;\r\n        printTaskEvents(events, filter, profiling, mapRanges, reduceRanges);\r\n    }\r\n    boolean success = isSuccessful();\r\n    if (success) {\r\n        LOG.info(\"Job \" + jobId + \" completed successfully\");\r\n    } else {\r\n        LOG.info(\"Job \" + jobId + \" failed with state \" + status.getState() + \" due to: \" + status.getFailureInfo());\r\n    }\r\n    Counters counters = getCounters();\r\n    if (counters != null) {\r\n        LOG.info(counters.toString());\r\n    }\r\n    return success;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "printTaskEvents",
  "errType" : null,
  "containingMethodsNum" : 10,
  "sourceCodeText" : "void printTaskEvents(TaskCompletionEvent[] events, Job.TaskStatusFilter filter, boolean profiling, IntegerRanges mapRanges, IntegerRanges reduceRanges) throws IOException, InterruptedException\n{\r\n    for (TaskCompletionEvent event : events) {\r\n        switch(filter) {\r\n            case NONE:\r\n                break;\r\n            case SUCCEEDED:\r\n                if (event.getStatus() == TaskCompletionEvent.Status.SUCCEEDED) {\r\n                    LOG.info(event.toString());\r\n                }\r\n                break;\r\n            case FAILED:\r\n                if (event.getStatus() == TaskCompletionEvent.Status.FAILED) {\r\n                    LOG.info(event.toString());\r\n                    TaskAttemptID taskId = event.getTaskAttemptId();\r\n                    String[] taskDiagnostics = getTaskDiagnostics(taskId);\r\n                    if (taskDiagnostics != null) {\r\n                        for (String diagnostics : taskDiagnostics) {\r\n                            System.err.println(diagnostics);\r\n                        }\r\n                    }\r\n                }\r\n                break;\r\n            case KILLED:\r\n                if (event.getStatus() == TaskCompletionEvent.Status.KILLED) {\r\n                    LOG.info(event.toString());\r\n                }\r\n                break;\r\n            case ALL:\r\n                LOG.info(event.toString());\r\n                break;\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "getProgressPollInterval",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "int getProgressPollInterval(Configuration conf)\n{\r\n    int progMonitorPollIntervalMillis = conf.getInt(PROGRESS_MONITOR_POLL_INTERVAL_KEY, DEFAULT_MONITOR_POLL_INTERVAL);\r\n    if (progMonitorPollIntervalMillis < 1) {\r\n        LOG.warn(PROGRESS_MONITOR_POLL_INTERVAL_KEY + \" has been set to an invalid value; \" + \" replacing with \" + DEFAULT_MONITOR_POLL_INTERVAL);\r\n        progMonitorPollIntervalMillis = DEFAULT_MONITOR_POLL_INTERVAL;\r\n    }\r\n    return progMonitorPollIntervalMillis;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "getCompletionPollInterval",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "int getCompletionPollInterval(Configuration conf)\n{\r\n    int completionPollIntervalMillis = conf.getInt(COMPLETION_POLL_INTERVAL_KEY, DEFAULT_COMPLETION_POLL_INTERVAL);\r\n    if (completionPollIntervalMillis < 1) {\r\n        LOG.warn(COMPLETION_POLL_INTERVAL_KEY + \" has been set to an invalid value; \" + \"replacing with \" + DEFAULT_COMPLETION_POLL_INTERVAL);\r\n        completionPollIntervalMillis = DEFAULT_COMPLETION_POLL_INTERVAL;\r\n    }\r\n    return completionPollIntervalMillis;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "getTaskOutputFilter",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "TaskStatusFilter getTaskOutputFilter(Configuration conf)\n{\r\n    return TaskStatusFilter.valueOf(conf.get(Job.OUTPUT_FILTER, \"FAILED\"));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "setTaskOutputFilter",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setTaskOutputFilter(Configuration conf, TaskStatusFilter newValue)\n{\r\n    conf.set(Job.OUTPUT_FILTER, newValue.toString());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "isUber",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "boolean isUber() throws IOException, InterruptedException\n{\r\n    ensureState(JobState.RUNNING);\r\n    updateStatus();\r\n    return status.isUber();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "getReservationId",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "ReservationId getReservationId()\n{\r\n    return reservationId;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "setReservationId",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setReservationId(ReservationId reservationId)\n{\r\n    this.reservationId = reservationId;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "close",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void close() throws IOException\n{\r\n    if (cluster != null) {\r\n        cluster.close();\r\n        cluster = null;\r\n    }\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "configure",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void configure(JobConf conf)\n{\r\n    compressionCodecs = new CompressionCodecFactory(conf);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "isSplitable",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "boolean isSplitable(FileSystem fs, Path file)\n{\r\n    final CompressionCodec codec = compressionCodecs.getCodec(file);\r\n    if (null == codec) {\r\n        return true;\r\n    }\r\n    return codec instanceof SplittableCompressionCodec;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getRecordReader",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "RecordReader<LongWritable, Text> getRecordReader(InputSplit genericSplit, JobConf job, Reporter reporter) throws IOException\n{\r\n    reporter.setStatus(genericSplit.toString());\r\n    String delimiter = job.get(\"textinputformat.record.delimiter\");\r\n    byte[] recordDelimiterBytes = null;\r\n    if (null != delimiter) {\r\n        recordDelimiterBytes = delimiter.getBytes(Charsets.UTF_8);\r\n    }\r\n    return new LineRecordReader(job, (FileSplit) genericSplit, recordDelimiterBytes);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred\\lib",
  "methodName" : "configure",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void configure(JobConf job)\n{\r\n    super.setConf(job);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred\\lib",
  "methodName" : "setPartitionFile",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setPartitionFile(JobConf job, Path p)\n{\r\n    org.apache.hadoop.mapreduce.lib.partition.TotalOrderPartitioner.setPartitionFile(job, p);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred\\lib",
  "methodName" : "getPartitionFile",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String getPartitionFile(JobConf job)\n{\r\n    return org.apache.hadoop.mapreduce.lib.partition.TotalOrderPartitioner.getPartitionFile(job);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "getDatum",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Object getDatum()\n{\r\n    return datum;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "setDatum",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setDatum(Object datum)\n{\r\n    this.datum = (JobInfoChange) datum;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "getJobId",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "JobID getJobId()\n{\r\n    return JobID.forName(datum.getJobid().toString());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "getSubmitTime",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "long getSubmitTime()\n{\r\n    return datum.getSubmitTime();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "getLaunchTime",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "long getLaunchTime()\n{\r\n    return datum.getLaunchTime();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "getEventType",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "EventType getEventType()\n{\r\n    return EventType.JOB_INFO_CHANGED;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "toTimelineEvent",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "TimelineEvent toTimelineEvent()\n{\r\n    TimelineEvent tEvent = new TimelineEvent();\r\n    tEvent.setId(StringUtils.toUpperCase(getEventType().name()));\r\n    tEvent.addInfo(\"SUBMIT_TIME\", getSubmitTime());\r\n    tEvent.addInfo(\"LAUNCH_TIME\", getLaunchTime());\r\n    return tEvent;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "getTimelineMetrics",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Set<TimelineMetric> getTimelineMetrics()\n{\r\n    return null;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getTime",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "long getTime()\n{\r\n    return System.currentTimeMillis();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred\\join",
  "methodName" : "setFormat",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void setFormat(JobConf job) throws IOException\n{\r\n    addDefaults();\r\n    addUserIdentifiers(job);\r\n    root = Parser.parse(job.get(\"mapred.join.expr\", null), job);\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred\\join",
  "methodName" : "addDefaults",
  "errType" : [ "NoSuchMethodException" ],
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void addDefaults()\n{\r\n    try {\r\n        Parser.CNode.addIdentifier(\"inner\", InnerJoinRecordReader.class);\r\n        Parser.CNode.addIdentifier(\"outer\", OuterJoinRecordReader.class);\r\n        Parser.CNode.addIdentifier(\"override\", OverrideRecordReader.class);\r\n        Parser.WNode.addIdentifier(\"tbl\", WrappedRecordReader.class);\r\n    } catch (NoSuchMethodException e) {\r\n        throw new RuntimeException(\"FATAL: Failed to init defaults\", e);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred\\join",
  "methodName" : "addUserIdentifiers",
  "errType" : [ "NoSuchMethodException" ],
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void addUserIdentifiers(JobConf job) throws IOException\n{\r\n    Pattern x = Pattern.compile(\"^mapred\\\\.join\\\\.define\\\\.(\\\\w+)$\");\r\n    for (Map.Entry<String, String> kv : job) {\r\n        Matcher m = x.matcher(kv.getKey());\r\n        if (m.matches()) {\r\n            try {\r\n                Parser.CNode.addIdentifier(m.group(1), job.getClass(m.group(0), null, ComposableRecordReader.class));\r\n            } catch (NoSuchMethodException e) {\r\n                throw (IOException) new IOException(\"Invalid define for \" + m.group(1)).initCause(e);\r\n            }\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred\\join",
  "methodName" : "getSplits",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "InputSplit[] getSplits(JobConf job, int numSplits) throws IOException\n{\r\n    setFormat(job);\r\n    job.setLong(\"mapred.min.split.size\", Long.MAX_VALUE);\r\n    return root.getSplits(job, numSplits);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred\\join",
  "methodName" : "getRecordReader",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "ComposableRecordReader<K, TupleWritable> getRecordReader(InputSplit split, JobConf job, Reporter reporter) throws IOException\n{\r\n    setFormat(job);\r\n    return root.getRecordReader(split, job, reporter);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred\\join",
  "methodName" : "compose",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String compose(Class<? extends InputFormat> inf, String path)\n{\r\n    return compose(inf.getName().intern(), path, new StringBuffer()).toString();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred\\join",
  "methodName" : "compose",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "String compose(String op, Class<? extends InputFormat> inf, String... path)\n{\r\n    final String infname = inf.getName();\r\n    StringBuffer ret = new StringBuffer(op + '(');\r\n    for (String p : path) {\r\n        compose(infname, p, ret);\r\n        ret.append(',');\r\n    }\r\n    ret.setCharAt(ret.length() - 1, ')');\r\n    return ret.toString();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred\\join",
  "methodName" : "compose",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "String compose(String op, Class<? extends InputFormat> inf, Path... path)\n{\r\n    ArrayList<String> tmp = new ArrayList<String>(path.length);\r\n    for (Path p : path) {\r\n        tmp.add(p.toString());\r\n    }\r\n    return compose(op, inf, tmp.toArray(new String[0]));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred\\join",
  "methodName" : "compose",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "StringBuffer compose(String inf, String path, StringBuffer sb)\n{\r\n    sb.append(\"tbl(\" + inf + \",\\\"\");\r\n    sb.append(path);\r\n    sb.append(\"\\\")\");\r\n    return sb;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred\\lib",
  "methodName" : "getRecordWriter",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "RecordWriter<K, V> getRecordWriter(FileSystem ignored, JobConf job, String name, Progressable progress)\n{\r\n    return new RecordWriter<K, V>() {\r\n\r\n        public void write(K key, V value) {\r\n        }\r\n\r\n        public void close(Reporter reporter) {\r\n        }\r\n    };\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred\\lib",
  "methodName" : "checkOutputSpecs",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void checkOutputSpecs(FileSystem ignored, JobConf job)\n{\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "getSplits",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "List<InputSplit> getSplits(JobContext context) throws IOException, InterruptedException",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "createRecordReader",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "RecordReader<K, V> createRecordReader(InputSplit split, TaskAttemptContext context) throws IOException, InterruptedException",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\aggregate",
  "methodName" : "reduce",
  "errType" : null,
  "containingMethodsNum" : 11,
  "sourceCodeText" : "void reduce(Text key, Iterable<Text> values, Context context) throws IOException, InterruptedException\n{\r\n    String keyStr = key.toString();\r\n    int pos = keyStr.indexOf(ValueAggregatorDescriptor.TYPE_SEPARATOR);\r\n    String type = keyStr.substring(0, pos);\r\n    long uniqCount = context.getConfiguration().getLong(UniqValueCount.MAX_NUM_UNIQUE_VALUES, Long.MAX_VALUE);\r\n    ValueAggregator aggregator = ValueAggregatorBaseDescriptor.generateValueAggregator(type, uniqCount);\r\n    for (Text val : values) {\r\n        aggregator.addNextValue(val);\r\n    }\r\n    Iterator<?> outputs = aggregator.getCombinerOutput().iterator();\r\n    while (outputs.hasNext()) {\r\n        Object v = outputs.next();\r\n        if (v instanceof Text) {\r\n            context.write(key, (Text) v);\r\n        } else {\r\n            context.write(key, new Text(v.toString()));\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "configure",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void configure(JobConf conf)\n{\r\n    comparator = conf.getOutputKeyComparator();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "setProgressable",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setProgressable(Progressable reporter)\n{\r\n    this.reporter = reporter;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "addKeyValue",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void addKeyValue(int recordOffset, int keyLength, int valLength)\n{\r\n    if (startOffsets == null || count == startOffsets.length)\r\n        grow();\r\n    startOffsets[count] = recordOffset;\r\n    keyLengths[count] = keyLength;\r\n    if (keyLength > maxKeyLength) {\r\n        maxKeyLength = keyLength;\r\n    }\r\n    if (valLength > maxValLength) {\r\n        maxValLength = valLength;\r\n    }\r\n    valueLengths[count] = valLength;\r\n    pointers[count] = count;\r\n    count++;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "setInputBuffer",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setInputBuffer(OutputBuffer buffer)\n{\r\n    this.keyValBuffer = buffer;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getMemoryUtilized",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getMemoryUtilized()\n{\r\n    if (startOffsets != null) {\r\n        return (startOffsets.length) * BUFFERED_KEY_VAL_OVERHEAD + maxKeyLength + maxValLength;\r\n    } else {\r\n        return 0;\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "sort",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "RawKeyValueIterator sort()",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "close",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void close()\n{\r\n    count = 0;\r\n    startOffsets = null;\r\n    keyLengths = null;\r\n    valueLengths = null;\r\n    pointers = null;\r\n    maxKeyLength = 0;\r\n    maxValLength = 0;\r\n    keyValBuffer = null;\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "grow",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void grow()\n{\r\n    int currLength = 0;\r\n    if (startOffsets != null) {\r\n        currLength = startOffsets.length;\r\n    }\r\n    int newLength = (int) (currLength * 1.1) + 1;\r\n    startOffsets = grow(startOffsets, newLength);\r\n    keyLengths = grow(keyLengths, newLength);\r\n    valueLengths = grow(valueLengths, newLength);\r\n    pointers = grow(pointers, newLength);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "grow",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "int[] grow(int[] old, int newLength)\n{\r\n    int[] result = new int[newLength];\r\n    if (old != null) {\r\n        System.arraycopy(old, 0, result, 0, old.length);\r\n    }\r\n    return result;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getProgress",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Progress getProgress()\n{\r\n    return null;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getKey",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "DataOutputBuffer getKey() throws IOException\n{\r\n    int currKeyOffset = startOffsets[currStartOffsetIndex];\r\n    int currKeyLength = keyLengths[currStartOffsetIndex];\r\n    key.reset();\r\n    key.write(keyValBuffer.getData(), currKeyOffset, currKeyLength);\r\n    return key;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getValue",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "ValueBytes getValue() throws IOException\n{\r\n    value.reset(keyValBuffer, startOffsets[currStartOffsetIndex] + keyLengths[currStartOffsetIndex], valLengths[currStartOffsetIndex]);\r\n    return value;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "next",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean next() throws IOException\n{\r\n    if (count == currIndexInPointers)\r\n        return false;\r\n    currStartOffsetIndex = pointers[currIndexInPointers];\r\n    currIndexInPointers++;\r\n    return true;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "close",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void close()\n{\r\n    return;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "getNextEvent",
  "errType" : [ "EOFException" ],
  "containingMethodsNum" : 4,
  "sourceCodeText" : "HistoryEvent getNextEvent() throws IOException\n{\r\n    Event wrapper;\r\n    try {\r\n        wrapper = (Event) reader.read(null, decoder);\r\n    } catch (EOFException e) {\r\n        return null;\r\n    }\r\n    HistoryEvent result;\r\n    switch(wrapper.getType()) {\r\n        case JOB_SUBMITTED:\r\n            result = new JobSubmittedEvent();\r\n            break;\r\n        case JOB_INITED:\r\n            result = new JobInitedEvent();\r\n            break;\r\n        case JOB_FINISHED:\r\n            result = new JobFinishedEvent();\r\n            break;\r\n        case JOB_PRIORITY_CHANGED:\r\n            result = new JobPriorityChangeEvent();\r\n            break;\r\n        case JOB_QUEUE_CHANGED:\r\n            result = new JobQueueChangeEvent();\r\n            break;\r\n        case JOB_STATUS_CHANGED:\r\n            result = new JobStatusChangedEvent();\r\n            break;\r\n        case JOB_FAILED:\r\n            result = new JobUnsuccessfulCompletionEvent();\r\n            break;\r\n        case JOB_KILLED:\r\n            result = new JobUnsuccessfulCompletionEvent();\r\n            break;\r\n        case JOB_ERROR:\r\n            result = new JobUnsuccessfulCompletionEvent();\r\n            break;\r\n        case JOB_INFO_CHANGED:\r\n            result = new JobInfoChangeEvent();\r\n            break;\r\n        case TASK_STARTED:\r\n            result = new TaskStartedEvent();\r\n            break;\r\n        case TASK_FINISHED:\r\n            result = new TaskFinishedEvent();\r\n            break;\r\n        case TASK_FAILED:\r\n            result = new TaskFailedEvent();\r\n            break;\r\n        case TASK_UPDATED:\r\n            result = new TaskUpdatedEvent();\r\n            break;\r\n        case MAP_ATTEMPT_STARTED:\r\n            result = new TaskAttemptStartedEvent();\r\n            break;\r\n        case MAP_ATTEMPT_FINISHED:\r\n            result = new MapAttemptFinishedEvent();\r\n            break;\r\n        case MAP_ATTEMPT_FAILED:\r\n            result = new TaskAttemptUnsuccessfulCompletionEvent();\r\n            break;\r\n        case MAP_ATTEMPT_KILLED:\r\n            result = new TaskAttemptUnsuccessfulCompletionEvent();\r\n            break;\r\n        case REDUCE_ATTEMPT_STARTED:\r\n            result = new TaskAttemptStartedEvent();\r\n            break;\r\n        case REDUCE_ATTEMPT_FINISHED:\r\n            result = new ReduceAttemptFinishedEvent();\r\n            break;\r\n        case REDUCE_ATTEMPT_FAILED:\r\n            result = new TaskAttemptUnsuccessfulCompletionEvent();\r\n            break;\r\n        case REDUCE_ATTEMPT_KILLED:\r\n            result = new TaskAttemptUnsuccessfulCompletionEvent();\r\n            break;\r\n        case SETUP_ATTEMPT_STARTED:\r\n            result = new TaskAttemptStartedEvent();\r\n            break;\r\n        case SETUP_ATTEMPT_FINISHED:\r\n            result = new TaskAttemptFinishedEvent();\r\n            break;\r\n        case SETUP_ATTEMPT_FAILED:\r\n            result = new TaskAttemptUnsuccessfulCompletionEvent();\r\n            break;\r\n        case SETUP_ATTEMPT_KILLED:\r\n            result = new TaskAttemptUnsuccessfulCompletionEvent();\r\n            break;\r\n        case CLEANUP_ATTEMPT_STARTED:\r\n            result = new TaskAttemptStartedEvent();\r\n            break;\r\n        case CLEANUP_ATTEMPT_FINISHED:\r\n            result = new TaskAttemptFinishedEvent();\r\n            break;\r\n        case CLEANUP_ATTEMPT_FAILED:\r\n            result = new TaskAttemptUnsuccessfulCompletionEvent();\r\n            break;\r\n        case CLEANUP_ATTEMPT_KILLED:\r\n            result = new TaskAttemptUnsuccessfulCompletionEvent();\r\n            break;\r\n        case AM_STARTED:\r\n            result = new AMStartedEvent();\r\n            break;\r\n        default:\r\n            throw new RuntimeException(\"unexpected event type: \" + wrapper.getType());\r\n    }\r\n    result.setDatum(wrapper.getEvent());\r\n    return result;\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "close",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void close() throws IOException\n{\r\n    if (in != null) {\r\n        in.close();\r\n    }\r\n    in = null;\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "fromAvro",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "Counters fromAvro(JhCounters counters)\n{\r\n    Counters result = new Counters();\r\n    if (counters != null) {\r\n        for (JhCounterGroup g : counters.getGroups()) {\r\n            CounterGroup group = result.addGroup(StringInterner.weakIntern(g.getName().toString()), StringInterner.weakIntern(g.getDisplayName().toString()));\r\n            for (JhCounter c : g.getCounts()) {\r\n                group.addCounter(StringInterner.weakIntern(c.getName().toString()), StringInterner.weakIntern(c.getDisplayName().toString()), c.getValue());\r\n            }\r\n        }\r\n    }\r\n    return result;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred\\pipes",
  "methodName" : "output",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void output(K key, V value) throws IOException\n{\r\n    collector.collect(key, value);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred\\pipes",
  "methodName" : "partitionedOutput",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void partitionedOutput(int reduce, K key, V value) throws IOException\n{\r\n    PipesPartitioner.setNextPartition(reduce);\r\n    collector.collect(key, value);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred\\pipes",
  "methodName" : "status",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void status(String msg)\n{\r\n    reporter.setStatus(msg);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred\\pipes",
  "methodName" : "progress",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void progress(float progress) throws IOException\n{\r\n    progressValue = progress;\r\n    reporter.progress();\r\n    if (recordReader != null) {\r\n        progressKey.set(progress);\r\n        recordReader.next(progressKey, nullValue);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred\\pipes",
  "methodName" : "done",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void done() throws IOException\n{\r\n    synchronized (this) {\r\n        done = true;\r\n        notify();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred\\pipes",
  "methodName" : "getProgress",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "float getProgress()\n{\r\n    return progressValue;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred\\pipes",
  "methodName" : "failed",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void failed(Throwable e)\n{\r\n    synchronized (this) {\r\n        exception = e;\r\n        notify();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred\\pipes",
  "methodName" : "waitForFinish",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "boolean waitForFinish() throws Throwable\n{\r\n    while (!done && exception == null) {\r\n        wait();\r\n    }\r\n    if (exception != null) {\r\n        throw exception;\r\n    }\r\n    return done;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred\\pipes",
  "methodName" : "registerCounter",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void registerCounter(int id, String group, String name) throws IOException\n{\r\n    Counters.Counter counter = reporter.getCounter(group, name);\r\n    registeredCounters.put(id, counter);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred\\pipes",
  "methodName" : "incrementCounter",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void incrementCounter(int id, long amount) throws IOException\n{\r\n    if (id < registeredCounters.size()) {\r\n        Counters.Counter counter = registeredCounters.get(id);\r\n        counter.increment(amount);\r\n    } else {\r\n        throw new IOException(\"Invalid counter with id: \" + id);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred\\pipes",
  "methodName" : "authenticate",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "boolean authenticate(String digest) throws IOException\n{\r\n    boolean success = true;\r\n    if (!expectedDigest.equals(digest)) {\r\n        exception = new IOException(\"Authentication Failed: Expected digest=\" + expectedDigest + \", received=\" + digestReceived);\r\n        success = false;\r\n    }\r\n    digestReceived = true;\r\n    notify();\r\n    return success;\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred\\pipes",
  "methodName" : "waitForAuthentication",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void waitForAuthentication() throws IOException, InterruptedException\n{\r\n    while (digestReceived == false && exception == null) {\r\n        wait();\r\n    }\r\n    if (exception != null) {\r\n        throw new IOException(exception.getMessage());\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getProblems",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "List<IOException> getProblems()\n{\r\n    return problems;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getMessage",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "String getMessage()\n{\r\n    StringBuffer result = new StringBuffer();\r\n    Iterator<IOException> itr = problems.iterator();\r\n    while (itr.hasNext()) {\r\n        result.append(itr.next().getMessage());\r\n        if (itr.hasNext()) {\r\n            result.append(\"\\n\");\r\n        }\r\n    }\r\n    return result.toString();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\db",
  "methodName" : "split",
  "errType" : null,
  "containingMethodsNum" : 16,
  "sourceCodeText" : "List<InputSplit> split(Configuration conf, ResultSet results, String colName) throws SQLException\n{\r\n    LOG.warn(\"Generating splits for a floating-point index column. Due to the\");\r\n    LOG.warn(\"imprecise representation of floating-point values in Java, this\");\r\n    LOG.warn(\"may result in an incomplete import.\");\r\n    LOG.warn(\"You are strongly encouraged to choose an integral split column.\");\r\n    List<InputSplit> splits = new ArrayList<InputSplit>();\r\n    if (results.getString(1) == null && results.getString(2) == null) {\r\n        splits.add(new DataDrivenDBInputFormat.DataDrivenDBInputSplit(colName + \" IS NULL\", colName + \" IS NULL\"));\r\n        return splits;\r\n    }\r\n    double minVal = results.getDouble(1);\r\n    double maxVal = results.getDouble(2);\r\n    int numSplits = conf.getInt(MRJobConfig.NUM_MAPS, 1);\r\n    double splitSize = (maxVal - minVal) / (double) numSplits;\r\n    if (splitSize < MIN_INCREMENT) {\r\n        splitSize = MIN_INCREMENT;\r\n    }\r\n    String lowClausePrefix = colName + \" >= \";\r\n    String highClausePrefix = colName + \" < \";\r\n    double curLower = minVal;\r\n    double curUpper = curLower + splitSize;\r\n    while (curUpper < maxVal) {\r\n        splits.add(new DataDrivenDBInputFormat.DataDrivenDBInputSplit(lowClausePrefix + Double.toString(curLower), highClausePrefix + Double.toString(curUpper)));\r\n        curLower = curUpper;\r\n        curUpper += splitSize;\r\n    }\r\n    if (curLower <= maxVal || splits.size() == 1) {\r\n        splits.add(new DataDrivenDBInputFormat.DataDrivenDBInputSplit(lowClausePrefix + Double.toString(curLower), colName + \" <= \" + Double.toString(maxVal)));\r\n    }\r\n    if (results.getString(1) == null || results.getString(2) == null) {\r\n        splits.add(new DataDrivenDBInputFormat.DataDrivenDBInputSplit(colName + \" IS NULL\", colName + \" IS NULL\"));\r\n    }\r\n    return splits;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred\\lib",
  "methodName" : "getJob",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "JobConf getJob()\n{\r\n    return job;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\task\\reduce",
  "methodName" : "run",
  "errType" : [ "InterruptedException", "Throwable", "InterruptedException", "IOException" ],
  "containingMethodsNum" : 12,
  "sourceCodeText" : "void run()\n{\r\n    int failures = 0;\r\n    LOG.info(reduce + \" Thread started: \" + getName());\r\n    try {\r\n        while (!stopped && !Thread.currentThread().isInterrupted()) {\r\n            try {\r\n                int numNewMaps = getMapCompletionEvents();\r\n                failures = 0;\r\n                if (numNewMaps > 0) {\r\n                    LOG.info(reduce + \": \" + \"Got \" + numNewMaps + \" new map-outputs\");\r\n                }\r\n                LOG.debug(\"GetMapEventsThread about to sleep for \" + SLEEP_TIME);\r\n                if (!Thread.currentThread().isInterrupted()) {\r\n                    Thread.sleep(SLEEP_TIME);\r\n                }\r\n            } catch (InterruptedException e) {\r\n                LOG.info(\"EventFetcher is interrupted.. Returning\");\r\n                return;\r\n            } catch (IOException ie) {\r\n                LOG.info(\"Exception in getting events\", ie);\r\n                if (++failures >= MAX_RETRIES) {\r\n                    throw new IOException(\"too many failures downloading events\", ie);\r\n                }\r\n                if (!Thread.currentThread().isInterrupted()) {\r\n                    Thread.sleep(RETRY_PERIOD);\r\n                }\r\n            }\r\n        }\r\n    } catch (InterruptedException e) {\r\n        return;\r\n    } catch (Throwable t) {\r\n        exceptionReporter.reportException(t);\r\n        return;\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 2,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\task\\reduce",
  "methodName" : "shutDown",
  "errType" : [ "InterruptedException" ],
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void shutDown()\n{\r\n    this.stopped = true;\r\n    interrupt();\r\n    try {\r\n        join(5000);\r\n    } catch (InterruptedException ie) {\r\n        LOG.warn(\"Got interrupted while joining \" + getName(), ie);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\task\\reduce",
  "methodName" : "getMapCompletionEvents",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "int getMapCompletionEvents() throws IOException, InterruptedException\n{\r\n    int numNewMaps = 0;\r\n    TaskCompletionEvent[] events = null;\r\n    do {\r\n        MapTaskCompletionEventsUpdate update = umbilical.getMapCompletionEvents((org.apache.hadoop.mapred.JobID) reduce.getJobID(), fromEventIdx, maxEventsToFetch, (org.apache.hadoop.mapred.TaskAttemptID) reduce);\r\n        events = update.getMapTaskCompletionEvents();\r\n        LOG.debug(\"Got \" + events.length + \" map completion events from \" + fromEventIdx);\r\n        assert !update.shouldReset() : \"Unexpected legacy state\";\r\n        fromEventIdx += events.length;\r\n        for (TaskCompletionEvent event : events) {\r\n            scheduler.resolve(event);\r\n            if (TaskCompletionEvent.Status.SUCCEEDED == event.getTaskStatus()) {\r\n                ++numNewMaps;\r\n            }\r\n        }\r\n    } while (events.length == maxEventsToFetch);\r\n    return numNewMaps;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "getDatum",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Object getDatum()\n{\r\n    return datum;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "setDatum",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setDatum(Object datum)\n{\r\n    this.datum = (JobSubmitted) datum;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "getJobId",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "JobID getJobId()\n{\r\n    return JobID.forName(datum.getJobid().toString());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "getJobName",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String getJobName()\n{\r\n    return datum.getJobName().toString();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "getJobQueueName",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "String getJobQueueName()\n{\r\n    if (datum.getJobQueueName() != null) {\r\n        return datum.getJobQueueName().toString();\r\n    }\r\n    return null;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "getUserName",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String getUserName()\n{\r\n    return datum.getUserName().toString();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "getSubmitTime",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "long getSubmitTime()\n{\r\n    return datum.getSubmitTime();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "getJobConfPath",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String getJobConfPath()\n{\r\n    return datum.getJobConfPath().toString();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "getJobAcls",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "Map<JobACL, AccessControlList> getJobAcls()\n{\r\n    Map<JobACL, AccessControlList> jobAcls = new HashMap<JobACL, AccessControlList>();\r\n    for (JobACL jobACL : JobACL.values()) {\r\n        Utf8 jobACLsUtf8 = new Utf8(jobACL.getAclName());\r\n        if (datum.getAcls().containsKey(jobACLsUtf8)) {\r\n            jobAcls.put(jobACL, new AccessControlList(datum.getAcls().get(jobACLsUtf8).toString()));\r\n        }\r\n    }\r\n    return jobAcls;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "getWorkflowId",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "String getWorkflowId()\n{\r\n    if (datum.getWorkflowId() != null) {\r\n        return datum.getWorkflowId().toString();\r\n    }\r\n    return null;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "getWorkflowName",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "String getWorkflowName()\n{\r\n    if (datum.getWorkflowName() != null) {\r\n        return datum.getWorkflowName().toString();\r\n    }\r\n    return null;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "getWorkflowNodeName",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "String getWorkflowNodeName()\n{\r\n    if (datum.getWorkflowNodeName() != null) {\r\n        return datum.getWorkflowNodeName().toString();\r\n    }\r\n    return null;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "getWorkflowAdjacencies",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "String getWorkflowAdjacencies()\n{\r\n    if (datum.getWorkflowAdjacencies() != null) {\r\n        return datum.getWorkflowAdjacencies().toString();\r\n    }\r\n    return null;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "getWorkflowTags",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "String getWorkflowTags()\n{\r\n    if (datum.getWorkflowTags() != null) {\r\n        return datum.getWorkflowTags().toString();\r\n    }\r\n    return null;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "getEventType",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "EventType getEventType()\n{\r\n    return EventType.JOB_SUBMITTED;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "getJobConf",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "JobConf getJobConf()\n{\r\n    return jobConf;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "toTimelineEvent",
  "errType" : null,
  "containingMethodsNum" : 13,
  "sourceCodeText" : "TimelineEvent toTimelineEvent()\n{\r\n    TimelineEvent tEvent = new TimelineEvent();\r\n    tEvent.setId(StringUtils.toUpperCase(getEventType().name()));\r\n    tEvent.addInfo(\"SUBMIT_TIME\", getSubmitTime());\r\n    tEvent.addInfo(\"QUEUE_NAME\", getJobQueueName());\r\n    tEvent.addInfo(\"JOB_NAME\", getJobName());\r\n    tEvent.addInfo(\"USER_NAME\", getUserName());\r\n    tEvent.addInfo(\"JOB_CONF_PATH\", getJobConfPath());\r\n    tEvent.addInfo(\"ACLS\", getJobAcls());\r\n    tEvent.addInfo(\"JOB_QUEUE_NAME\", getJobQueueName());\r\n    tEvent.addInfo(\"WORKLFOW_ID\", getWorkflowId());\r\n    tEvent.addInfo(\"WORKFLOW_NAME\", getWorkflowName());\r\n    tEvent.addInfo(\"WORKFLOW_NODE_NAME\", getWorkflowNodeName());\r\n    tEvent.addInfo(\"WORKFLOW_ADJACENCIES\", getWorkflowAdjacencies());\r\n    tEvent.addInfo(\"WORKFLOW_TAGS\", getWorkflowTags());\r\n    return tEvent;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "getTimelineMetrics",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Set<TimelineMetric> getTimelineMetrics()\n{\r\n    return null;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\reduce",
  "methodName" : "reduce",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void reduce(Key key, Iterable<IntWritable> values, Context context) throws IOException, InterruptedException\n{\r\n    int sum = 0;\r\n    for (IntWritable val : values) {\r\n        sum += val.get();\r\n    }\r\n    result.set(sum);\r\n    context.write(key, result);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\db",
  "methodName" : "setConf",
  "errType" : [ "Exception" ],
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void setConf(Configuration conf)\n{\r\n    dbConf = new DBConfiguration(conf);\r\n    try {\r\n        this.connection = createConnection();\r\n        DatabaseMetaData dbMeta = connection.getMetaData();\r\n        this.dbProductName = StringUtils.toUpperCase(dbMeta.getDatabaseProductName());\r\n    } catch (Exception ex) {\r\n        throw new RuntimeException(ex);\r\n    }\r\n    tableName = dbConf.getInputTableName();\r\n    fieldNames = dbConf.getInputFieldNames();\r\n    conditions = dbConf.getInputConditions();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\db",
  "methodName" : "getConf",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Configuration getConf()\n{\r\n    return dbConf.getConf();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\db",
  "methodName" : "getDBConf",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "DBConfiguration getDBConf()\n{\r\n    return dbConf;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\db",
  "methodName" : "getConnection",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Connection getConnection()\n{\r\n    if (this.connection == null) {\r\n        this.connection = createConnection();\r\n    }\r\n    return this.connection;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\db",
  "methodName" : "createConnection",
  "errType" : [ "Exception" ],
  "containingMethodsNum" : 3,
  "sourceCodeText" : "Connection createConnection()\n{\r\n    try {\r\n        Connection newConnection = dbConf.getConnection();\r\n        newConnection.setAutoCommit(false);\r\n        newConnection.setTransactionIsolation(Connection.TRANSACTION_SERIALIZABLE);\r\n        return newConnection;\r\n    } catch (Exception e) {\r\n        throw new RuntimeException(e);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\db",
  "methodName" : "getDBProductName",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getDBProductName()\n{\r\n    return dbProductName;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\db",
  "methodName" : "createDBRecordReader",
  "errType" : [ "SQLException" ],
  "containingMethodsNum" : 10,
  "sourceCodeText" : "RecordReader<LongWritable, T> createDBRecordReader(DBInputSplit split, Configuration conf) throws IOException\n{\r\n    @SuppressWarnings(\"unchecked\")\r\n    Class<T> inputClass = (Class<T>) (dbConf.getInputClass());\r\n    try {\r\n        if (dbProductName.startsWith(\"ORACLE\")) {\r\n            return new OracleDBRecordReader<T>(split, inputClass, conf, createConnection(), getDBConf(), conditions, fieldNames, tableName);\r\n        } else if (dbProductName.startsWith(\"MYSQL\")) {\r\n            return new MySQLDBRecordReader<T>(split, inputClass, conf, createConnection(), getDBConf(), conditions, fieldNames, tableName);\r\n        } else {\r\n            return new DBRecordReader<T>(split, inputClass, conf, createConnection(), getDBConf(), conditions, fieldNames, tableName);\r\n        }\r\n    } catch (SQLException ex) {\r\n        throw new IOException(ex.getMessage());\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\db",
  "methodName" : "createRecordReader",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "RecordReader<LongWritable, T> createRecordReader(InputSplit split, TaskAttemptContext context) throws IOException, InterruptedException\n{\r\n    return createDBRecordReader((DBInputSplit) split, context.getConfiguration());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\db",
  "methodName" : "getSplits",
  "errType" : [ "SQLException", "SQLException", "SQLException" ],
  "containingMethodsNum" : 12,
  "sourceCodeText" : "List<InputSplit> getSplits(JobContext job) throws IOException\n{\r\n    ResultSet results = null;\r\n    Statement statement = null;\r\n    try {\r\n        statement = connection.createStatement();\r\n        results = statement.executeQuery(getCountQuery());\r\n        results.next();\r\n        long count = results.getLong(1);\r\n        int chunks = job.getConfiguration().getInt(MRJobConfig.NUM_MAPS, 1);\r\n        long chunkSize = (count / chunks);\r\n        results.close();\r\n        statement.close();\r\n        List<InputSplit> splits = new ArrayList<InputSplit>();\r\n        for (int i = 0; i < chunks; i++) {\r\n            DBInputSplit split;\r\n            if ((i + 1) == chunks)\r\n                split = new DBInputSplit(i * chunkSize, count);\r\n            else\r\n                split = new DBInputSplit(i * chunkSize, (i * chunkSize) + chunkSize);\r\n            splits.add(split);\r\n        }\r\n        connection.commit();\r\n        return splits;\r\n    } catch (SQLException e) {\r\n        throw new IOException(\"Got SQLException\", e);\r\n    } finally {\r\n        try {\r\n            if (results != null) {\r\n                results.close();\r\n            }\r\n        } catch (SQLException e1) {\r\n        }\r\n        try {\r\n            if (statement != null) {\r\n                statement.close();\r\n            }\r\n        } catch (SQLException e1) {\r\n        }\r\n        closeConnection();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 3,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\db",
  "methodName" : "getCountQuery",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "String getCountQuery()\n{\r\n    if (dbConf.getInputCountQuery() != null) {\r\n        return dbConf.getInputCountQuery();\r\n    }\r\n    StringBuilder query = new StringBuilder();\r\n    query.append(\"SELECT COUNT(*) FROM \" + tableName);\r\n    if (conditions != null && conditions.length() > 0)\r\n        query.append(\" WHERE \" + conditions);\r\n    return query.toString();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\db",
  "methodName" : "setInput",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void setInput(Job job, Class<? extends DBWritable> inputClass, String tableName, String conditions, String orderBy, String... fieldNames)\n{\r\n    job.setInputFormatClass(DBInputFormat.class);\r\n    DBConfiguration dbConf = new DBConfiguration(job.getConfiguration());\r\n    dbConf.setInputClass(inputClass);\r\n    dbConf.setInputTableName(tableName);\r\n    dbConf.setInputFieldNames(fieldNames);\r\n    dbConf.setInputConditions(conditions);\r\n    dbConf.setInputOrderBy(orderBy);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\db",
  "methodName" : "setInput",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void setInput(Job job, Class<? extends DBWritable> inputClass, String inputQuery, String inputCountQuery)\n{\r\n    job.setInputFormatClass(DBInputFormat.class);\r\n    DBConfiguration dbConf = new DBConfiguration(job.getConfiguration());\r\n    dbConf.setInputClass(inputClass);\r\n    dbConf.setInputQuery(inputQuery);\r\n    dbConf.setInputCountQuery(inputCountQuery);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\db",
  "methodName" : "closeConnection",
  "errType" : [ "SQLException" ],
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void closeConnection()\n{\r\n    try {\r\n        if (null != this.connection) {\r\n            this.connection.close();\r\n            this.connection = null;\r\n        }\r\n    } catch (SQLException sqlE) {\r\n        LOG.debug(\"Exception on close\", sqlE);\r\n    }\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getRecordReader",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "RecordReader<BytesWritable, BytesWritable> getRecordReader(InputSplit split, JobConf job, Reporter reporter) throws IOException\n{\r\n    return new SequenceFileAsBinaryRecordReader(job, (FileSplit) split);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "activateOptions",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void activateOptions()\n{\r\n    synchronized (this) {\r\n        setOptionsFromSystemProperties();\r\n        if (maxEvents > 0) {\r\n            tail = new LinkedList<LoggingEvent>();\r\n        }\r\n        setFile(TaskLog.getTaskLogFile(TaskAttemptID.forName(taskId), isCleanup, TaskLog.LogName.SYSLOG).toString());\r\n        setAppend(true);\r\n        super.activateOptions();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "setOptionsFromSystemProperties",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void setOptionsFromSystemProperties()\n{\r\n    if (isCleanup == null) {\r\n        String propValue = System.getProperty(ISCLEANUP_PROPERTY, \"false\");\r\n        isCleanup = Boolean.valueOf(propValue);\r\n    }\r\n    if (taskId == null) {\r\n        taskId = System.getProperty(TASKID_PROPERTY);\r\n    }\r\n    if (maxEvents == null) {\r\n        String propValue = System.getProperty(LOGSIZE_PROPERTY, \"0\");\r\n        setTotalLogFileSize(Long.parseLong(propValue));\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "append",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void append(LoggingEvent event)\n{\r\n    synchronized (this) {\r\n        if (tail == null) {\r\n            super.append(event);\r\n        } else {\r\n            if (tail.size() >= maxEvents) {\r\n                tail.remove();\r\n            }\r\n            tail.add(event);\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "flush",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void flush()\n{\r\n    if (qw != null) {\r\n        qw.flush();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "close",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void close()\n{\r\n    if (tail != null) {\r\n        for (LoggingEvent event : tail) {\r\n            super.append(event);\r\n        }\r\n    }\r\n    super.close();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getTaskId",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getTaskId()\n{\r\n    return taskId;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "setTaskId",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setTaskId(String taskId)\n{\r\n    this.taskId = taskId;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getTotalLogFileSize",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getTotalLogFileSize()\n{\r\n    return maxEvents * EVENT_SIZE;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "setTotalLogFileSize",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setTotalLogFileSize(long logSize)\n{\r\n    maxEvents = (int) logSize / EVENT_SIZE;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "setIsCleanup",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setIsCleanup(boolean isCleanup)\n{\r\n    this.isCleanup = isCleanup;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getIsCleanup",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean getIsCleanup()\n{\r\n    return isCleanup;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest\\impl",
  "methodName" : "close",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void close() throws IOException\n{\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest\\impl",
  "methodName" : "getFileSystem",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "FileSystem getFileSystem()\n{\r\n    return fileSystem;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest\\impl",
  "methodName" : "bindToFileSystem",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void bindToFileSystem(FileSystem filesystem, Path path) throws IOException\n{\r\n    fileSystem = filesystem;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest\\impl",
  "methodName" : "getFileStatus",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "FileStatus getFileStatus(Path path) throws IOException\n{\r\n    return fileSystem.getFileStatus(path);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest\\impl",
  "methodName" : "isFile",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "boolean isFile(Path path) throws IOException\n{\r\n    return fileSystem.isFile(path);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest\\impl",
  "methodName" : "delete",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "boolean delete(Path path, boolean recursive) throws IOException\n{\r\n    return fileSystem.delete(path, recursive);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest\\impl",
  "methodName" : "mkdirs",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "boolean mkdirs(Path path) throws IOException\n{\r\n    return fileSystem.mkdirs(path);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest\\impl",
  "methodName" : "renameFile",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "boolean renameFile(Path source, Path dest) throws IOException\n{\r\n    return fileSystem.rename(source, dest);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest\\impl",
  "methodName" : "listStatusIterator",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "RemoteIterator<FileStatus> listStatusIterator(Path path) throws IOException\n{\r\n    return fileSystem.listStatusIterator(path);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest\\impl",
  "methodName" : "loadTaskManifest",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "TaskManifest loadTaskManifest(JsonSerialization<TaskManifest> serializer, FileStatus st) throws IOException\n{\r\n    return TaskManifest.load(serializer, fileSystem, st.getPath(), st);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest\\impl",
  "methodName" : "save",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void save(final T manifestData, final Path path, final boolean overwrite) throws IOException\n{\r\n    manifestData.save(fileSystem, path, overwrite);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest\\impl",
  "methodName" : "storePreservesEtagsThroughRenames",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 1,
  "sourceCodeText" : "boolean storePreservesEtagsThroughRenames(Path path)\n{\r\n    try {\r\n        return fileSystem.hasPathCapability(path, CommonPathCapabilities.ETAGS_PRESERVED_IN_RENAME);\r\n    } catch (IOException ignored) {\r\n        return false;\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest\\impl",
  "methodName" : "msync",
  "errType" : [ "UnsupportedOperationException" ],
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void msync(Path path) throws IOException\n{\r\n    if (msyncUnsupported) {\r\n        return;\r\n    }\r\n    fileSystem.makeQualified(path);\r\n    try {\r\n        fileSystem.msync();\r\n    } catch (UnsupportedOperationException ignored) {\r\n        msyncUnsupported = true;\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest\\files",
  "methodName" : "validate",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "ManifestSuccessData validate() throws IOException\n{\r\n    verify(name != null, \"Incompatible file format: no 'name' field\");\r\n    verify(NAME.equals(name), \"Incompatible file format: \" + name);\r\n    return this;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest\\files",
  "methodName" : "createSerializer",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "JsonSerialization<ManifestSuccessData> createSerializer()\n{\r\n    return serializer();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest\\files",
  "methodName" : "toBytes",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "byte[] toBytes() throws IOException\n{\r\n    return serializer().toBytes(this);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest\\files",
  "methodName" : "toJson",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String toJson() throws IOException\n{\r\n    return serializer().toJson(this);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest\\files",
  "methodName" : "save",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void save(FileSystem fs, Path path, boolean overwrite) throws IOException\n{\r\n    name = NAME;\r\n    serializer().save(fs, path, this, overwrite);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest\\files",
  "methodName" : "toString",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "String toString()\n{\r\n    final StringBuilder sb = new StringBuilder(\"ManifestSuccessData{\");\r\n    sb.append(\"committer='\").append(committer).append('\\'');\r\n    sb.append(\", hostname='\").append(hostname).append('\\'');\r\n    sb.append(\", description='\").append(description).append('\\'');\r\n    sb.append(\", date='\").append(date).append('\\'');\r\n    sb.append(\", filenames=[\").append(StringUtils.join(filenames, \", \")).append(\"]\");\r\n    sb.append('}');\r\n    return sb.toString();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest\\files",
  "methodName" : "dumpMetrics",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String dumpMetrics(String prefix, String middle, String suffix)\n{\r\n    return joinMap(metrics, prefix, middle, suffix);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest\\files",
  "methodName" : "dumpDiagnostics",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String dumpDiagnostics(String prefix, String middle, String suffix)\n{\r\n    return joinMap(diagnostics, prefix, middle, suffix);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest\\files",
  "methodName" : "joinMap",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "String joinMap(Map<String, ?> map, String prefix, String middle, String suffix)\n{\r\n    if (map == null) {\r\n        return \"\";\r\n    }\r\n    List<String> list = new ArrayList<>(map.keySet());\r\n    Collections.sort(list);\r\n    StringBuilder sb = new StringBuilder(list.size() * 32);\r\n    for (String k : list) {\r\n        sb.append(prefix).append(k).append(middle).append(map.get(k)).append(suffix);\r\n    }\r\n    return sb.toString();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest\\files",
  "methodName" : "load",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "ManifestSuccessData load(FileSystem fs, Path path) throws IOException\n{\r\n    LOG.debug(\"Reading success data from {}\", path);\r\n    ManifestSuccessData instance = serializer().load(fs, path);\r\n    instance.validate();\r\n    return instance;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest\\files",
  "methodName" : "serializer",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "JsonSerialization<ManifestSuccessData> serializer()\n{\r\n    return new JsonSerialization<>(ManifestSuccessData.class, false, true);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest\\files",
  "methodName" : "getName",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getName()\n{\r\n    return name;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest\\files",
  "methodName" : "setName",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setName(String name)\n{\r\n    this.name = name;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest\\files",
  "methodName" : "getTimestamp",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getTimestamp()\n{\r\n    return timestamp;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest\\files",
  "methodName" : "setTimestamp",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setTimestamp(long timestamp)\n{\r\n    this.timestamp = timestamp;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest\\files",
  "methodName" : "getDate",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getDate()\n{\r\n    return date;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest\\files",
  "methodName" : "setDate",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setDate(String date)\n{\r\n    this.date = date;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest\\files",
  "methodName" : "getHostname",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getHostname()\n{\r\n    return hostname;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest\\files",
  "methodName" : "setHostname",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setHostname(String hostname)\n{\r\n    this.hostname = hostname;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest\\files",
  "methodName" : "getCommitter",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getCommitter()\n{\r\n    return committer;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest\\files",
  "methodName" : "setCommitter",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setCommitter(String committer)\n{\r\n    this.committer = committer;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest\\files",
  "methodName" : "getDescription",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getDescription()\n{\r\n    return description;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest\\files",
  "methodName" : "setDescription",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setDescription(String description)\n{\r\n    this.description = description;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest\\files",
  "methodName" : "getMetrics",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Map<String, Long> getMetrics()\n{\r\n    return metrics;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest\\files",
  "methodName" : "setMetrics",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setMetrics(TreeMap<String, Long> metrics)\n{\r\n    this.metrics = metrics;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest\\files",
  "methodName" : "getFilenames",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "List<String> getFilenames()\n{\r\n    return filenames;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest\\files",
  "methodName" : "getFilenamePaths",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "List<Path> getFilenamePaths()\n{\r\n    return getFilenames().stream().map(AbstractManifestData::unmarshallPath).collect(Collectors.toList());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest\\files",
  "methodName" : "setFilenamePaths",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setFilenamePaths(List<Path> paths)\n{\r\n    setFilenames(new ArrayList<>(paths.stream().map(AbstractManifestData::marshallPath).collect(Collectors.toList())));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest\\files",
  "methodName" : "setFilenames",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setFilenames(ArrayList<String> filenames)\n{\r\n    this.filenames = filenames;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest\\files",
  "methodName" : "getDiagnostics",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Map<String, String> getDiagnostics()\n{\r\n    return diagnostics;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest\\files",
  "methodName" : "setDiagnostics",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setDiagnostics(TreeMap<String, String> diagnostics)\n{\r\n    this.diagnostics = diagnostics;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest\\files",
  "methodName" : "putDiagnostic",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void putDiagnostic(String key, String value)\n{\r\n    diagnostics.put(key, value);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest\\files",
  "methodName" : "getJobId",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getJobId()\n{\r\n    return jobId;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest\\files",
  "methodName" : "setJobId",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setJobId(String jobId)\n{\r\n    this.jobId = jobId;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest\\files",
  "methodName" : "getJobIdSource",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getJobIdSource()\n{\r\n    return jobIdSource;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest\\files",
  "methodName" : "setJobIdSource",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setJobIdSource(final String jobIdSource)\n{\r\n    this.jobIdSource = jobIdSource;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest\\files",
  "methodName" : "getIOStatistics",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "IOStatisticsSnapshot getIOStatistics()\n{\r\n    return iostatistics;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest\\files",
  "methodName" : "setIOStatistics",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setIOStatistics(final IOStatisticsSnapshot ioStatistics)\n{\r\n    this.iostatistics = ioStatistics;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest\\files",
  "methodName" : "snapshotIOStatistics",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void snapshotIOStatistics(IOStatistics iostats)\n{\r\n    setIOStatistics(IOStatisticsSupport.snapshotIOStatistics(iostats));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest\\files",
  "methodName" : "setSuccess",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setSuccess(boolean success)\n{\r\n    this.success = success;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest\\files",
  "methodName" : "getSuccess",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean getSuccess()\n{\r\n    return success;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest\\files",
  "methodName" : "getState",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getState()\n{\r\n    return state;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest\\files",
  "methodName" : "setState",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setState(String state)\n{\r\n    this.state = state;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest\\files",
  "methodName" : "getStage",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getStage()\n{\r\n    return stage;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest\\files",
  "methodName" : "recordJobFailure",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void recordJobFailure(Throwable thrown)\n{\r\n    setSuccess(false);\r\n    String stacktrace = ExceptionUtils.getStackTrace(thrown);\r\n    diagnostics.put(DiagnosticKeys.EXCEPTION, thrown.toString());\r\n    diagnostics.put(DiagnosticKeys.STACKTRACE, stacktrace);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\security",
  "methodName" : "wrapIfNecessary",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "FSDataOutputStream wrapIfNecessary(Configuration conf, FSDataOutputStream out, Path outPath) throws IOException\n{\r\n    SpillCallBackInjector.get().writeSpillFileCB(outPath, out, conf);\r\n    return CryptoUtils.wrapIfNecessary(conf, out, true);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\security",
  "methodName" : "wrapIfNecessary",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "FSDataOutputStream wrapIfNecessary(Configuration conf, FSDataOutputStream out, boolean closeOutputStream, Path outPath) throws IOException\n{\r\n    SpillCallBackInjector.get().writeSpillFileCB(outPath, out, conf);\r\n    return CryptoUtils.wrapIfNecessary(conf, out, closeOutputStream);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\security",
  "methodName" : "wrapIfNecessary",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "FSDataInputStream wrapIfNecessary(Configuration conf, FSDataInputStream in, Path inputPath) throws IOException\n{\r\n    SpillCallBackInjector.get().getSpillFileCB(inputPath, in, conf);\r\n    return CryptoUtils.wrapIfNecessary(conf, in);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\security",
  "methodName" : "wrapIfNecessary",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "InputStream wrapIfNecessary(Configuration conf, InputStream in, long length, Path inputPath) throws IOException\n{\r\n    SpillCallBackInjector.get().getSpillFileCB(inputPath, in, conf);\r\n    return CryptoUtils.wrapIfNecessary(conf, in, length);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\security",
  "methodName" : "addSpillIndexFile",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void addSpillIndexFile(Path indexFilename, Configuration conf)\n{\r\n    SpillCallBackInjector.get().addSpillIndexFileCB(indexFilename, conf);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\security",
  "methodName" : "validateSpillIndexFile",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void validateSpillIndexFile(Path indexFilename, Configuration conf)\n{\r\n    SpillCallBackInjector.get().validateSpillIndexFileCB(indexFilename, conf);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\security",
  "methodName" : "resetSpillCBInjector",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "SpillCallBackInjector resetSpillCBInjector()\n{\r\n    return setSpillCBInjector(prevSpillCBInjector);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\security",
  "methodName" : "setSpillCBInjector",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "SpillCallBackInjector setSpillCBInjector(SpillCallBackInjector spillInjector)\n{\r\n    prevSpillCBInjector = SpillCallBackInjector.getAndSet(spillInjector);\r\n    return spillInjector;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getOutputFile",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Path getOutputFile() throws IOException",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getOutputFileForWrite",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Path getOutputFileForWrite(long size) throws IOException",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getOutputFileForWriteInVolume",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Path getOutputFileForWriteInVolume(Path existing)",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getOutputIndexFile",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Path getOutputIndexFile() throws IOException",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getOutputIndexFileForWrite",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Path getOutputIndexFileForWrite(long size) throws IOException",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getOutputIndexFileForWriteInVolume",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Path getOutputIndexFileForWriteInVolume(Path existing)",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getSpillFile",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Path getSpillFile(int spillNumber) throws IOException",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getSpillFileForWrite",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Path getSpillFileForWrite(int spillNumber, long size) throws IOException",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getSpillIndexFile",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Path getSpillIndexFile(int spillNumber) throws IOException",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getSpillIndexFileForWrite",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Path getSpillIndexFileForWrite(int spillNumber, long size) throws IOException",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getInputFile",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Path getInputFile(int mapId) throws IOException",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getInputFileForWrite",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Path getInputFileForWrite(org.apache.hadoop.mapreduce.TaskID mapId, long size) throws IOException",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "removeAll",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void removeAll() throws IOException",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "setConf",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setConf(Configuration conf)\n{\r\n    this.conf = conf;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getConf",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Configuration getConf()\n{\r\n    return conf;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\jobcontrol",
  "methodName" : "toList",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "List<ControlledJob> toList(LinkedList<ControlledJob> jobs)\n{\r\n    ArrayList<ControlledJob> retv = new ArrayList<ControlledJob>();\r\n    for (ControlledJob job : jobs) {\r\n        retv.add(job);\r\n    }\r\n    return retv;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\jobcontrol",
  "methodName" : "getJobsIn",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "List<ControlledJob> getJobsIn(State state)\n{\r\n    LinkedList<ControlledJob> l = new LinkedList<ControlledJob>();\r\n    for (ControlledJob j : jobsInProgress) {\r\n        if (j.getJobState() == state) {\r\n            l.add(j);\r\n        }\r\n    }\r\n    return l;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\jobcontrol",
  "methodName" : "getWaitingJobList",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "List<ControlledJob> getWaitingJobList()\n{\r\n    return getJobsIn(State.WAITING);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\jobcontrol",
  "methodName" : "getRunningJobList",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "List<ControlledJob> getRunningJobList()\n{\r\n    return getJobsIn(State.RUNNING);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\jobcontrol",
  "methodName" : "getReadyJobsList",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "List<ControlledJob> getReadyJobsList()\n{\r\n    return getJobsIn(State.READY);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\jobcontrol",
  "methodName" : "getSuccessfulJobList",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "List<ControlledJob> getSuccessfulJobList()\n{\r\n    return toList(this.successfulJobs);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\jobcontrol",
  "methodName" : "getFailedJobList",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "List<ControlledJob> getFailedJobList()\n{\r\n    return toList(this.failedJobs);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\jobcontrol",
  "methodName" : "getNextJobID",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getNextJobID()\n{\r\n    nextJobID += 1;\r\n    return this.groupName + this.nextJobID;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\jobcontrol",
  "methodName" : "addJob",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "String addJob(ControlledJob aJob)\n{\r\n    String id = this.getNextJobID();\r\n    aJob.setJobID(id);\r\n    aJob.setJobState(State.WAITING);\r\n    jobsInProgress.add(aJob);\r\n    return id;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\jobcontrol",
  "methodName" : "addJob",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String addJob(Job aJob)\n{\r\n    return addJob((ControlledJob) aJob);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\jobcontrol",
  "methodName" : "addJobCollection",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void addJobCollection(Collection<ControlledJob> jobs)\n{\r\n    for (ControlledJob job : jobs) {\r\n        addJob(job);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\jobcontrol",
  "methodName" : "getThreadState",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "ThreadState getThreadState()\n{\r\n    return this.runnerState;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\jobcontrol",
  "methodName" : "stop",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void stop()\n{\r\n    this.runnerState = ThreadState.STOPPING;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\jobcontrol",
  "methodName" : "suspend",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void suspend()\n{\r\n    if (this.runnerState == ThreadState.RUNNING) {\r\n        this.runnerState = ThreadState.SUSPENDED;\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\jobcontrol",
  "methodName" : "resume",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void resume()\n{\r\n    if (this.runnerState == ThreadState.SUSPENDED) {\r\n        this.runnerState = ThreadState.RUNNING;\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\jobcontrol",
  "methodName" : "allFinished",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "boolean allFinished()\n{\r\n    return jobsInProgress.isEmpty();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\jobcontrol",
  "methodName" : "run",
  "errType" : [ "Throwable", "Exception", "Exception" ],
  "containingMethodsNum" : 15,
  "sourceCodeText" : "void run()\n{\r\n    if (isCircular(jobsInProgress)) {\r\n        throw new IllegalArgumentException(\"job control has circular dependency\");\r\n    }\r\n    try {\r\n        this.runnerState = ThreadState.RUNNING;\r\n        while (true) {\r\n            while (this.runnerState == ThreadState.SUSPENDED) {\r\n                try {\r\n                    Thread.sleep(5000);\r\n                } catch (Exception e) {\r\n                }\r\n            }\r\n            synchronized (this) {\r\n                Iterator<ControlledJob> it = jobsInProgress.iterator();\r\n                while (it.hasNext()) {\r\n                    ControlledJob j = it.next();\r\n                    LOG.debug(\"Checking state of job \" + j);\r\n                    switch(j.checkState()) {\r\n                        case SUCCESS:\r\n                            successfulJobs.add(j);\r\n                            it.remove();\r\n                            break;\r\n                        case FAILED:\r\n                        case DEPENDENT_FAILED:\r\n                            failedJobs.add(j);\r\n                            it.remove();\r\n                            break;\r\n                        case READY:\r\n                            j.submit();\r\n                            break;\r\n                        case RUNNING:\r\n                        case WAITING:\r\n                            break;\r\n                    }\r\n                }\r\n            }\r\n            if (this.runnerState != ThreadState.RUNNING && this.runnerState != ThreadState.SUSPENDED) {\r\n                break;\r\n            }\r\n            try {\r\n                Thread.sleep(5000);\r\n            } catch (Exception e) {\r\n            }\r\n            if (this.runnerState != ThreadState.RUNNING && this.runnerState != ThreadState.SUSPENDED) {\r\n                break;\r\n            }\r\n        }\r\n    } catch (Throwable t) {\r\n        LOG.error(\"Error while trying to run jobs.\", t);\r\n        failAllJobs(t);\r\n    }\r\n    this.runnerState = ThreadState.STOPPED;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 3,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\jobcontrol",
  "methodName" : "failAllJobs",
  "errType" : [ "IOException", "InterruptedException" ],
  "containingMethodsNum" : 9,
  "sourceCodeText" : "void failAllJobs(Throwable t)\n{\r\n    String message = \"Unexpected System Error Occurred: \" + StringUtils.stringifyException(t);\r\n    Iterator<ControlledJob> it = jobsInProgress.iterator();\r\n    while (it.hasNext()) {\r\n        ControlledJob j = it.next();\r\n        try {\r\n            j.failJob(message);\r\n        } catch (IOException e) {\r\n            LOG.error(\"Error while tyring to clean up \" + j.getJobName(), e);\r\n        } catch (InterruptedException e) {\r\n            LOG.error(\"Error while tyring to clean up \" + j.getJobName(), e);\r\n        } finally {\r\n            failedJobs.add(j);\r\n            it.remove();\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\jobcontrol",
  "methodName" : "isCircular",
  "errType" : null,
  "containingMethodsNum" : 16,
  "sourceCodeText" : "boolean isCircular(final List<ControlledJob> jobList)\n{\r\n    boolean cyclePresent = false;\r\n    HashSet<ControlledJob> SourceSet = new HashSet<ControlledJob>();\r\n    HashMap<ControlledJob, List<ControlledJob>> processedMap = new HashMap<ControlledJob, List<ControlledJob>>();\r\n    for (ControlledJob n : jobList) {\r\n        processedMap.put(n, new ArrayList<ControlledJob>());\r\n    }\r\n    for (ControlledJob n : jobList) {\r\n        if (!hasInComingEdge(n, jobList, processedMap)) {\r\n            SourceSet.add(n);\r\n        }\r\n    }\r\n    while (!SourceSet.isEmpty()) {\r\n        ControlledJob controlledJob = SourceSet.iterator().next();\r\n        SourceSet.remove(controlledJob);\r\n        if (controlledJob.getDependentJobs() != null) {\r\n            for (int i = 0; i < controlledJob.getDependentJobs().size(); i++) {\r\n                ControlledJob depenControlledJob = controlledJob.getDependentJobs().get(i);\r\n                processedMap.get(controlledJob).add(depenControlledJob);\r\n                if (!hasInComingEdge(controlledJob, jobList, processedMap)) {\r\n                    SourceSet.add(depenControlledJob);\r\n                }\r\n            }\r\n        }\r\n    }\r\n    for (ControlledJob controlledJob : jobList) {\r\n        if (controlledJob.getDependentJobs() != null && controlledJob.getDependentJobs().size() != processedMap.get(controlledJob).size()) {\r\n            cyclePresent = true;\r\n            LOG.error(\"Job control has circular dependency for the  job \" + controlledJob.getJobName());\r\n            break;\r\n        }\r\n    }\r\n    return cyclePresent;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\jobcontrol",
  "methodName" : "hasInComingEdge",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "boolean hasInComingEdge(ControlledJob controlledJob, List<ControlledJob> controlledJobList, HashMap<ControlledJob, List<ControlledJob>> processedMap)\n{\r\n    boolean hasIncomingEdge = false;\r\n    for (ControlledJob k : controlledJobList) {\r\n        if (k != controlledJob && k.getDependentJobs() != null && !processedMap.get(k).contains(controlledJob) && k.getDependentJobs().contains(controlledJob)) {\r\n            hasIncomingEdge = true;\r\n            break;\r\n        }\r\n    }\r\n    return hasIncomingEdge;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\task\\reduce",
  "methodName" : "getMemory",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "byte[] getMemory()\n{\r\n    return memory;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\task\\reduce",
  "methodName" : "getArrayStream",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "BoundedByteArrayOutputStream getArrayStream()\n{\r\n    return byteStream;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\task\\reduce",
  "methodName" : "doShuffle",
  "errType" : null,
  "containingMethodsNum" : 9,
  "sourceCodeText" : "void doShuffle(MapHost host, IFileInputStream iFin, long compressedLength, long decompressedLength, ShuffleClientMetrics metrics, Reporter reporter) throws IOException\n{\r\n    InputStream input = iFin;\r\n    if (codec != null) {\r\n        decompressor.reset();\r\n        input = codec.createInputStream(input, decompressor);\r\n    }\r\n    try {\r\n        IOUtils.readFully(input, memory, 0, memory.length);\r\n        metrics.inputBytes(memory.length);\r\n        reporter.progress();\r\n        LOG.info(\"Read \" + memory.length + \" bytes from map-output for \" + getMapId());\r\n        if (input.read() >= 0) {\r\n            throw new IOException(\"Unexpected extra bytes from input stream for \" + getMapId());\r\n        }\r\n    } finally {\r\n        CodecPool.returnDecompressor(decompressor);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\task\\reduce",
  "methodName" : "commit",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void commit() throws IOException\n{\r\n    getMerger().closeInMemoryFile(this);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\task\\reduce",
  "methodName" : "abort",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void abort()\n{\r\n    getMerger().unreserve(memory.length);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\task\\reduce",
  "methodName" : "getDescription",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getDescription()\n{\r\n    return \"MEMORY\";\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\task",
  "methodName" : "getInputSplit",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "InputSplit getInputSplit()\n{\r\n    return split;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\task",
  "methodName" : "getCurrentKey",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "KEYIN getCurrentKey() throws IOException, InterruptedException\n{\r\n    return reader.getCurrentKey();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\task",
  "methodName" : "getCurrentValue",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "VALUEIN getCurrentValue() throws IOException, InterruptedException\n{\r\n    return reader.getCurrentValue();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\task",
  "methodName" : "nextKeyValue",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "boolean nextKeyValue() throws IOException, InterruptedException\n{\r\n    return reader.nextKeyValue();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\security\\token",
  "methodName" : "getKind",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Text getKind()\n{\r\n    return KIND_NAME;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\security\\token",
  "methodName" : "getUser",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "UserGroupInformation getUser()\n{\r\n    if (jobid == null || \"\".equals(jobid.toString())) {\r\n        return null;\r\n    }\r\n    return UserGroupInformation.createRemoteUser(jobid.toString());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\security\\token",
  "methodName" : "getJobId",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Text getJobId()\n{\r\n    return jobid;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\security\\token",
  "methodName" : "readFields",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void readFields(DataInput in) throws IOException\n{\r\n    jobid.readFields(in);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\security\\token",
  "methodName" : "write",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void write(DataOutput out) throws IOException\n{\r\n    jobid.write(out);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\counters",
  "methodName" : "init",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void init(Configuration conf)\n{\r\n    if (!isInited) {\r\n        if (conf == null) {\r\n            conf = new JobConf();\r\n        }\r\n        GROUP_NAME_MAX = conf.getInt(COUNTER_GROUP_NAME_MAX_KEY, COUNTER_GROUP_NAME_MAX_DEFAULT);\r\n        COUNTER_NAME_MAX = conf.getInt(COUNTER_NAME_MAX_KEY, COUNTER_NAME_MAX_DEFAULT);\r\n        GROUPS_MAX = conf.getInt(COUNTER_GROUPS_MAX_KEY, COUNTER_GROUPS_MAX_DEFAULT);\r\n        COUNTERS_MAX = conf.getInt(COUNTERS_MAX_KEY, COUNTERS_MAX_DEFAULT);\r\n    }\r\n    isInited = true;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\counters",
  "methodName" : "getGroupNameMax",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "int getGroupNameMax()\n{\r\n    if (!isInited) {\r\n        init(null);\r\n    }\r\n    return GROUP_NAME_MAX;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\counters",
  "methodName" : "getCounterNameMax",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "int getCounterNameMax()\n{\r\n    if (!isInited) {\r\n        init(null);\r\n    }\r\n    return COUNTER_NAME_MAX;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\counters",
  "methodName" : "getGroupsMax",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "int getGroupsMax()\n{\r\n    if (!isInited) {\r\n        init(null);\r\n    }\r\n    return GROUPS_MAX;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\counters",
  "methodName" : "getCountersMax",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "int getCountersMax()\n{\r\n    if (!isInited) {\r\n        init(null);\r\n    }\r\n    return COUNTERS_MAX;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\counters",
  "methodName" : "filterName",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "String filterName(String name, int maxLen)\n{\r\n    return name.length() > maxLen ? name.substring(0, maxLen - 1) : name;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\counters",
  "methodName" : "filterCounterName",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String filterCounterName(String name)\n{\r\n    return filterName(name, getCounterNameMax());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\counters",
  "methodName" : "filterGroupName",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String filterGroupName(String name)\n{\r\n    return filterName(name, getGroupNameMax());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\counters",
  "methodName" : "checkCounters",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void checkCounters(int size)\n{\r\n    if (firstViolation != null) {\r\n        throw new LimitExceededException(firstViolation);\r\n    }\r\n    int countersMax = getCountersMax();\r\n    if (size > countersMax) {\r\n        firstViolation = new LimitExceededException(\"Too many counters: \" + size + \" max=\" + countersMax);\r\n        throw firstViolation;\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\counters",
  "methodName" : "incrCounters",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void incrCounters()\n{\r\n    checkCounters(totalCounters + 1);\r\n    ++totalCounters;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\counters",
  "methodName" : "checkGroups",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void checkGroups(int size)\n{\r\n    if (firstViolation != null) {\r\n        throw new LimitExceededException(firstViolation);\r\n    }\r\n    int groupsMax = getGroupsMax();\r\n    if (size > groupsMax) {\r\n        firstViolation = new LimitExceededException(\"Too many counter groups: \" + size + \" max=\" + groupsMax);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\counters",
  "methodName" : "violation",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "LimitExceededException violation()\n{\r\n    return firstViolation;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred\\lib",
  "methodName" : "getSplits",
  "errType" : null,
  "containingMethodsNum" : 22,
  "sourceCodeText" : "InputSplit[] getSplits(JobConf conf, int numSplits) throws IOException\n{\r\n    JobConf confCopy = new JobConf(conf);\r\n    List<InputSplit> splits = new ArrayList<InputSplit>();\r\n    Map<Path, InputFormat> formatMap = MultipleInputs.getInputFormatMap(conf);\r\n    Map<Path, Class<? extends Mapper>> mapperMap = MultipleInputs.getMapperTypeMap(conf);\r\n    Map<Class<? extends InputFormat>, List<Path>> formatPaths = new HashMap<Class<? extends InputFormat>, List<Path>>();\r\n    for (Entry<Path, InputFormat> entry : formatMap.entrySet()) {\r\n        if (!formatPaths.containsKey(entry.getValue().getClass())) {\r\n            formatPaths.put(entry.getValue().getClass(), new LinkedList<Path>());\r\n        }\r\n        formatPaths.get(entry.getValue().getClass()).add(entry.getKey());\r\n    }\r\n    for (Entry<Class<? extends InputFormat>, List<Path>> formatEntry : formatPaths.entrySet()) {\r\n        Class<? extends InputFormat> formatClass = formatEntry.getKey();\r\n        InputFormat format = (InputFormat) ReflectionUtils.newInstance(formatClass, conf);\r\n        List<Path> paths = formatEntry.getValue();\r\n        Map<Class<? extends Mapper>, List<Path>> mapperPaths = new HashMap<Class<? extends Mapper>, List<Path>>();\r\n        for (Path path : paths) {\r\n            Class<? extends Mapper> mapperClass = mapperMap.get(path);\r\n            if (!mapperPaths.containsKey(mapperClass)) {\r\n                mapperPaths.put(mapperClass, new LinkedList<Path>());\r\n            }\r\n            mapperPaths.get(mapperClass).add(path);\r\n        }\r\n        for (Entry<Class<? extends Mapper>, List<Path>> mapEntry : mapperPaths.entrySet()) {\r\n            paths = mapEntry.getValue();\r\n            Class<? extends Mapper> mapperClass = mapEntry.getKey();\r\n            if (mapperClass == null) {\r\n                mapperClass = conf.getMapperClass();\r\n            }\r\n            FileInputFormat.setInputPaths(confCopy, paths.toArray(new Path[paths.size()]));\r\n            InputSplit[] pathSplits = format.getSplits(confCopy, numSplits);\r\n            for (InputSplit pathSplit : pathSplits) {\r\n                splits.add(new TaggedInputSplit(pathSplit, conf, format.getClass(), mapperClass));\r\n            }\r\n        }\r\n    }\r\n    return splits.toArray(new InputSplit[splits.size()]);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred\\lib",
  "methodName" : "getRecordReader",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "RecordReader<K, V> getRecordReader(InputSplit split, JobConf conf, Reporter reporter) throws IOException\n{\r\n    TaggedInputSplit taggedInputSplit = (TaggedInputSplit) split;\r\n    InputFormat<K, V> inputFormat = (InputFormat<K, V>) ReflectionUtils.newInstance(taggedInputSplit.getInputFormatClass(), conf);\r\n    return inputFormat.getRecordReader(taggedInputSplit.getInputSplit(), conf, reporter);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\input",
  "methodName" : "getProblems",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "List<IOException> getProblems()\n{\r\n    return problems;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\input",
  "methodName" : "getMessage",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "String getMessage()\n{\r\n    StringBuffer result = new StringBuffer();\r\n    Iterator<IOException> itr = problems.iterator();\r\n    while (itr.hasNext()) {\r\n        result.append(itr.next().getMessage());\r\n        if (itr.hasNext()) {\r\n            result.append(\"\\n\");\r\n        }\r\n    }\r\n    return result.toString();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "setup",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setup(Context context) throws IOException, InterruptedException\n{\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "map",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void map(KEYIN key, VALUEIN value, Context context) throws IOException, InterruptedException\n{\r\n    context.write((KEYOUT) key, (VALUEOUT) value);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "cleanup",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void cleanup(Context context) throws IOException, InterruptedException\n{\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "run",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void run(Context context) throws IOException, InterruptedException\n{\r\n    setup(context);\r\n    try {\r\n        while (context.nextKeyValue()) {\r\n            map(context.getCurrentKey(), context.getCurrentValue(), context);\r\n        }\r\n    } finally {\r\n        cleanup(context);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\chain",
  "methodName" : "getCurrentKey",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "KEYIN getCurrentKey() throws IOException, InterruptedException\n{\r\n    return reader.getCurrentKey();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\chain",
  "methodName" : "getCurrentValue",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "VALUEIN getCurrentValue() throws IOException, InterruptedException\n{\r\n    return reader.getCurrentValue();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\chain",
  "methodName" : "nextKeyValue",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "boolean nextKeyValue() throws IOException, InterruptedException\n{\r\n    return reader.nextKeyValue();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\chain",
  "methodName" : "getInputSplit",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "InputSplit getInputSplit()\n{\r\n    if (base instanceof MapContext) {\r\n        MapContext<KEYIN, VALUEIN, KEYOUT, VALUEOUT> mc = (MapContext<KEYIN, VALUEIN, KEYOUT, VALUEOUT>) base;\r\n        return mc.getInputSplit();\r\n    } else {\r\n        return null;\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\chain",
  "methodName" : "getCounter",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Counter getCounter(Enum<?> counterName)\n{\r\n    return base.getCounter(counterName);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\chain",
  "methodName" : "getCounter",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Counter getCounter(String groupName, String counterName)\n{\r\n    return base.getCounter(groupName, counterName);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\chain",
  "methodName" : "getOutputCommitter",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "OutputCommitter getOutputCommitter()\n{\r\n    return base.getOutputCommitter();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\chain",
  "methodName" : "write",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void write(KEYOUT key, VALUEOUT value) throws IOException, InterruptedException\n{\r\n    output.write(key, value);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\chain",
  "methodName" : "getStatus",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String getStatus()\n{\r\n    return base.getStatus();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\chain",
  "methodName" : "getTaskAttemptID",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "TaskAttemptID getTaskAttemptID()\n{\r\n    return base.getTaskAttemptID();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\chain",
  "methodName" : "setStatus",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setStatus(String msg)\n{\r\n    base.setStatus(msg);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\chain",
  "methodName" : "getArchiveClassPaths",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Path[] getArchiveClassPaths()\n{\r\n    return base.getArchiveClassPaths();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\chain",
  "methodName" : "getArchiveTimestamps",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String[] getArchiveTimestamps()\n{\r\n    return base.getArchiveTimestamps();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\chain",
  "methodName" : "getCacheArchives",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "URI[] getCacheArchives() throws IOException\n{\r\n    return base.getCacheArchives();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\chain",
  "methodName" : "getCacheFiles",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "URI[] getCacheFiles() throws IOException\n{\r\n    return base.getCacheFiles();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\chain",
  "methodName" : "getCombinerClass",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Class<? extends Reducer<?, ?, ?, ?>> getCombinerClass() throws ClassNotFoundException\n{\r\n    return base.getCombinerClass();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\chain",
  "methodName" : "getConfiguration",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Configuration getConfiguration()\n{\r\n    return conf;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\chain",
  "methodName" : "getFileClassPaths",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Path[] getFileClassPaths()\n{\r\n    return base.getFileClassPaths();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\chain",
  "methodName" : "getFileTimestamps",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String[] getFileTimestamps()\n{\r\n    return base.getFileTimestamps();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\chain",
  "methodName" : "getCombinerKeyGroupingComparator",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "RawComparator<?> getCombinerKeyGroupingComparator()\n{\r\n    return base.getCombinerKeyGroupingComparator();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\chain",
  "methodName" : "getGroupingComparator",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "RawComparator<?> getGroupingComparator()\n{\r\n    return base.getGroupingComparator();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\chain",
  "methodName" : "getInputFormatClass",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Class<? extends InputFormat<?, ?>> getInputFormatClass() throws ClassNotFoundException\n{\r\n    return base.getInputFormatClass();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\chain",
  "methodName" : "getJar",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String getJar()\n{\r\n    return base.getJar();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\chain",
  "methodName" : "getJobID",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "JobID getJobID()\n{\r\n    return base.getJobID();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\chain",
  "methodName" : "getJobName",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String getJobName()\n{\r\n    return base.getJobName();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\chain",
  "methodName" : "getJobSetupCleanupNeeded",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "boolean getJobSetupCleanupNeeded()\n{\r\n    return base.getJobSetupCleanupNeeded();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\chain",
  "methodName" : "getTaskCleanupNeeded",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "boolean getTaskCleanupNeeded()\n{\r\n    return base.getTaskCleanupNeeded();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\chain",
  "methodName" : "getLocalCacheArchives",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Path[] getLocalCacheArchives() throws IOException\n{\r\n    return base.getLocalCacheArchives();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\chain",
  "methodName" : "getLocalCacheFiles",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Path[] getLocalCacheFiles() throws IOException\n{\r\n    return base.getLocalCacheArchives();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\chain",
  "methodName" : "getMapOutputKeyClass",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Class<?> getMapOutputKeyClass()\n{\r\n    return base.getMapOutputKeyClass();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\chain",
  "methodName" : "getMapOutputValueClass",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Class<?> getMapOutputValueClass()\n{\r\n    return base.getMapOutputValueClass();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\chain",
  "methodName" : "getMapperClass",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Class<? extends Mapper<?, ?, ?, ?>> getMapperClass() throws ClassNotFoundException\n{\r\n    return base.getMapperClass();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\chain",
  "methodName" : "getMaxMapAttempts",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "int getMaxMapAttempts()\n{\r\n    return base.getMaxMapAttempts();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\chain",
  "methodName" : "getMaxReduceAttempts",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "int getMaxReduceAttempts()\n{\r\n    return base.getMaxReduceAttempts();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\chain",
  "methodName" : "getNumReduceTasks",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "int getNumReduceTasks()\n{\r\n    return base.getNumReduceTasks();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\chain",
  "methodName" : "getOutputFormatClass",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Class<? extends OutputFormat<?, ?>> getOutputFormatClass() throws ClassNotFoundException\n{\r\n    return base.getOutputFormatClass();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\chain",
  "methodName" : "getOutputKeyClass",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Class<?> getOutputKeyClass()\n{\r\n    return base.getMapOutputKeyClass();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\chain",
  "methodName" : "getOutputValueClass",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Class<?> getOutputValueClass()\n{\r\n    return base.getOutputValueClass();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\chain",
  "methodName" : "getPartitionerClass",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Class<? extends Partitioner<?, ?>> getPartitionerClass() throws ClassNotFoundException\n{\r\n    return base.getPartitionerClass();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\chain",
  "methodName" : "getProfileEnabled",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "boolean getProfileEnabled()\n{\r\n    return base.getProfileEnabled();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\chain",
  "methodName" : "getProfileParams",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String getProfileParams()\n{\r\n    return base.getProfileParams();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\chain",
  "methodName" : "getProfileTaskRange",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "IntegerRanges getProfileTaskRange(boolean isMap)\n{\r\n    return base.getProfileTaskRange(isMap);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\chain",
  "methodName" : "getReducerClass",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Class<? extends Reducer<?, ?, ?, ?>> getReducerClass() throws ClassNotFoundException\n{\r\n    return base.getReducerClass();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\chain",
  "methodName" : "getSortComparator",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "RawComparator<?> getSortComparator()\n{\r\n    return base.getSortComparator();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\chain",
  "methodName" : "getSymlink",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "boolean getSymlink()\n{\r\n    return base.getSymlink();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\chain",
  "methodName" : "getUser",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String getUser()\n{\r\n    return base.getUser();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\chain",
  "methodName" : "getWorkingDirectory",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Path getWorkingDirectory() throws IOException\n{\r\n    return base.getWorkingDirectory();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\chain",
  "methodName" : "progress",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void progress()\n{\r\n    base.progress();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\chain",
  "methodName" : "getCredentials",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Credentials getCredentials()\n{\r\n    return base.getCredentials();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\chain",
  "methodName" : "getProgress",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "float getProgress()\n{\r\n    return base.getProgress();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getJobRunState",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getJobRunState(int state)\n{\r\n    if (state < 1 || state >= runStates.length) {\r\n        return UNKNOWN;\r\n    }\r\n    return runStates[state];\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getEnum",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "org.apache.hadoop.mapreduce.JobStatus.State getEnum(int state)\n{\r\n    switch(state) {\r\n        case 1:\r\n            return org.apache.hadoop.mapreduce.JobStatus.State.RUNNING;\r\n        case 2:\r\n            return org.apache.hadoop.mapreduce.JobStatus.State.SUCCEEDED;\r\n        case 3:\r\n            return org.apache.hadoop.mapreduce.JobStatus.State.FAILED;\r\n        case 4:\r\n            return org.apache.hadoop.mapreduce.JobStatus.State.PREP;\r\n        case 5:\r\n            return org.apache.hadoop.mapreduce.JobStatus.State.KILLED;\r\n    }\r\n    return null;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "downgrade",
  "errType" : null,
  "containingMethodsNum" : 17,
  "sourceCodeText" : "JobStatus downgrade(org.apache.hadoop.mapreduce.JobStatus stat)\n{\r\n    JobStatus old = new JobStatus(JobID.downgrade(stat.getJobID()), stat.getSetupProgress(), stat.getMapProgress(), stat.getReduceProgress(), stat.getCleanupProgress(), stat.getState().getValue(), JobPriority.valueOf(stat.getPriority().name()), stat.getUsername(), stat.getJobName(), stat.getQueue(), stat.getJobFile(), stat.getTrackingUrl(), stat.isUber());\r\n    old.setStartTime(stat.getStartTime());\r\n    old.setFinishTime(stat.getFinishTime());\r\n    old.setSchedulingInfo(stat.getSchedulingInfo());\r\n    old.setHistoryFile(stat.getHistoryFile());\r\n    return old;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getJobId",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String getJobId()\n{\r\n    return getJobID().toString();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getJobID",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "JobID getJobID()\n{\r\n    return JobID.downgrade(super.getJobID());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getJobPriority",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "JobPriority getJobPriority()\n{\r\n    return JobPriority.valueOf(super.getPriority().name());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "setMapProgress",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setMapProgress(float p)\n{\r\n    super.setMapProgress(p);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "setCleanupProgress",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setCleanupProgress(float p)\n{\r\n    super.setCleanupProgress(p);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "setSetupProgress",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setSetupProgress(float p)\n{\r\n    super.setSetupProgress(p);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "setReduceProgress",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setReduceProgress(float p)\n{\r\n    super.setReduceProgress(p);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "setFinishTime",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setFinishTime(long finishTime)\n{\r\n    super.setFinishTime(finishTime);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "setHistoryFile",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setHistoryFile(String historyFile)\n{\r\n    super.setHistoryFile(historyFile);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "setTrackingUrl",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setTrackingUrl(String trackingUrl)\n{\r\n    super.setTrackingUrl(trackingUrl);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "setRetired",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setRetired()\n{\r\n    super.setRetired();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "setRunState",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setRunState(int state)\n{\r\n    super.setState(getEnum(state));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getRunState",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "int getRunState()\n{\r\n    return super.getState().getValue();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "setStartTime",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setStartTime(long startTime)\n{\r\n    super.setStartTime(startTime);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "setUsername",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setUsername(String userName)\n{\r\n    super.setUsername(userName);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "setSchedulingInfo",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setSchedulingInfo(String schedulingInfo)\n{\r\n    super.setSchedulingInfo(schedulingInfo);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "setJobACLs",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setJobACLs(Map<JobACL, AccessControlList> acls)\n{\r\n    super.setJobACLs(acls);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "setFailureInfo",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setFailureInfo(String failureInfo)\n{\r\n    super.setFailureInfo(failureInfo);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "setJobPriority",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setJobPriority(JobPriority jp)\n{\r\n    super.setPriority(org.apache.hadoop.mapreduce.JobPriority.valueOf(jp.name()));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "mapProgress",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "float mapProgress()\n{\r\n    return super.getMapProgress();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "cleanupProgress",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "float cleanupProgress()\n{\r\n    return super.getCleanupProgress();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "setupProgress",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "float setupProgress()\n{\r\n    return super.getSetupProgress();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "reduceProgress",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "float reduceProgress()\n{\r\n    return super.getReduceProgress();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getOldNewJobRunState",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "int getOldNewJobRunState(org.apache.hadoop.mapreduce.JobStatus.State state)\n{\r\n    return state.getValue();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\task\\reduce",
  "methodName" : "append",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void append(K key, V value) throws IOException\n{\r\n    throw new UnsupportedOperationException(\"InMemoryWriter.append(K key, V value\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\task\\reduce",
  "methodName" : "append",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void append(DataInputBuffer key, DataInputBuffer value) throws IOException\n{\r\n    int keyLength = key.getLength() - key.getPosition();\r\n    if (keyLength < 0) {\r\n        throw new IOException(\"Negative key-length not allowed: \" + keyLength + \" for \" + key);\r\n    }\r\n    int valueLength = value.getLength() - value.getPosition();\r\n    if (valueLength < 0) {\r\n        throw new IOException(\"Negative value-length not allowed: \" + valueLength + \" for \" + value);\r\n    }\r\n    WritableUtils.writeVInt(out, keyLength);\r\n    WritableUtils.writeVInt(out, valueLength);\r\n    out.write(key.getData(), key.getPosition(), keyLength);\r\n    out.write(value.getData(), value.getPosition(), valueLength);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\task\\reduce",
  "methodName" : "close",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void close() throws IOException\n{\r\n    WritableUtils.writeVInt(out, IFile.EOF_MARKER);\r\n    WritableUtils.writeVInt(out, IFile.EOF_MARKER);\r\n    out.close();\r\n    out = null;\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred\\lib",
  "methodName" : "reduce",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void reduce(K key, Iterator<LongWritable> values, OutputCollector<K, LongWritable> output, Reporter reporter) throws IOException\n{\r\n    long sum = 0;\r\n    while (values.hasNext()) {\r\n        sum += values.next().get();\r\n    }\r\n    output.collect(key, new LongWritable(sum));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\partition",
  "methodName" : "setConf",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void setConf(Configuration conf)\n{\r\n    this.conf = conf;\r\n    String option = conf.get(COMPARATOR_OPTIONS);\r\n    String keyFieldSeparator = conf.get(MRJobConfig.MAP_OUTPUT_KEY_FIELD_SEPARATOR, \"\\t\");\r\n    keyFieldHelper.setKeyFieldSeparator(keyFieldSeparator);\r\n    keyFieldHelper.parseOption(option);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\partition",
  "methodName" : "getConf",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Configuration getConf()\n{\r\n    return conf;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\partition",
  "methodName" : "compare",
  "errType" : null,
  "containingMethodsNum" : 12,
  "sourceCodeText" : "int compare(byte[] b1, int s1, int l1, byte[] b2, int s2, int l2)\n{\r\n    int n1 = WritableUtils.decodeVIntSize(b1[s1]);\r\n    int n2 = WritableUtils.decodeVIntSize(b2[s2]);\r\n    List<KeyDescription> allKeySpecs = keyFieldHelper.keySpecs();\r\n    if (allKeySpecs.size() == 0) {\r\n        return compareBytes(b1, s1 + n1, l1 - n1, b2, s2 + n2, l2 - n2);\r\n    }\r\n    int[] lengthIndicesFirst = keyFieldHelper.getWordLengths(b1, s1 + n1, s1 + l1);\r\n    int[] lengthIndicesSecond = keyFieldHelper.getWordLengths(b2, s2 + n2, s2 + l2);\r\n    for (KeyDescription keySpec : allKeySpecs) {\r\n        int startCharFirst = keyFieldHelper.getStartOffset(b1, s1 + n1, s1 + l1, lengthIndicesFirst, keySpec);\r\n        int endCharFirst = keyFieldHelper.getEndOffset(b1, s1 + n1, s1 + l1, lengthIndicesFirst, keySpec);\r\n        int startCharSecond = keyFieldHelper.getStartOffset(b2, s2 + n2, s2 + l2, lengthIndicesSecond, keySpec);\r\n        int endCharSecond = keyFieldHelper.getEndOffset(b2, s2 + n2, s2 + l2, lengthIndicesSecond, keySpec);\r\n        int result;\r\n        if ((result = compareByteSequence(b1, startCharFirst, endCharFirst, b2, startCharSecond, endCharSecond, keySpec)) != 0) {\r\n            return result;\r\n        }\r\n    }\r\n    return 0;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\partition",
  "methodName" : "compareByteSequence",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "int compareByteSequence(byte[] first, int start1, int end1, byte[] second, int start2, int end2, KeyDescription key)\n{\r\n    if (start1 == -1) {\r\n        if (key.reverse) {\r\n            return 1;\r\n        }\r\n        return -1;\r\n    }\r\n    if (start2 == -1) {\r\n        if (key.reverse) {\r\n            return -1;\r\n        }\r\n        return 1;\r\n    }\r\n    int compareResult = 0;\r\n    if (!key.numeric) {\r\n        compareResult = compareBytes(first, start1, end1 - start1 + 1, second, start2, end2 - start2 + 1);\r\n    }\r\n    if (key.numeric) {\r\n        compareResult = numericalCompare(first, start1, end1, second, start2, end2);\r\n    }\r\n    if (key.reverse) {\r\n        return -compareResult;\r\n    }\r\n    return compareResult;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\partition",
  "methodName" : "numericalCompare",
  "errType" : null,
  "containingMethodsNum" : 9,
  "sourceCodeText" : "int numericalCompare(byte[] a, int start1, int end1, byte[] b, int start2, int end2)\n{\r\n    int i = start1;\r\n    int j = start2;\r\n    int mul = 1;\r\n    byte first_a = a[i];\r\n    byte first_b = b[j];\r\n    if (first_a == NEGATIVE) {\r\n        if (first_b != NEGATIVE) {\r\n            return oneNegativeCompare(a, start1 + 1, end1, b, start2, end2);\r\n        }\r\n        i++;\r\n    }\r\n    if (first_b == NEGATIVE) {\r\n        if (first_a != NEGATIVE) {\r\n            return -oneNegativeCompare(b, start2 + 1, end2, a, start1, end1);\r\n        }\r\n        j++;\r\n    }\r\n    if (first_b == NEGATIVE && first_a == NEGATIVE) {\r\n        mul = -1;\r\n    }\r\n    while (i <= end1) {\r\n        if (a[i] != ZERO) {\r\n            break;\r\n        }\r\n        i++;\r\n    }\r\n    while (j <= end2) {\r\n        if (b[j] != ZERO) {\r\n            break;\r\n        }\r\n        j++;\r\n    }\r\n    while (i <= end1 && j <= end2) {\r\n        if (!isdigit(a[i]) || a[i] != b[j]) {\r\n            break;\r\n        }\r\n        i++;\r\n        j++;\r\n    }\r\n    if (i <= end1) {\r\n        first_a = a[i];\r\n    }\r\n    if (j <= end2) {\r\n        first_b = b[j];\r\n    }\r\n    int firstResult = first_a - first_b;\r\n    if ((first_a == DECIMAL && (!isdigit(first_b) || j > end2)) || (first_b == DECIMAL && (!isdigit(first_a) || i > end1))) {\r\n        return ((mul < 0) ? -decimalCompare(a, i, end1, b, j, end2) : decimalCompare(a, i, end1, b, j, end2));\r\n    }\r\n    int numRemainDigits_a = 0;\r\n    int numRemainDigits_b = 0;\r\n    while (i <= end1) {\r\n        if (isdigit(a[i++])) {\r\n            numRemainDigits_a++;\r\n        } else\r\n            break;\r\n    }\r\n    while (j <= end2) {\r\n        if (isdigit(b[j++])) {\r\n            numRemainDigits_b++;\r\n        } else\r\n            break;\r\n    }\r\n    int ret = numRemainDigits_a - numRemainDigits_b;\r\n    if (ret == 0) {\r\n        return ((mul < 0) ? -firstResult : firstResult);\r\n    } else {\r\n        return ((mul < 0) ? -ret : ret);\r\n    }\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\partition",
  "methodName" : "isdigit",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean isdigit(byte b)\n{\r\n    if ('0' <= b && b <= '9') {\r\n        return true;\r\n    }\r\n    return false;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\partition",
  "methodName" : "decimalCompare",
  "errType" : null,
  "containingMethodsNum" : 10,
  "sourceCodeText" : "int decimalCompare(byte[] a, int i, int end1, byte[] b, int j, int end2)\n{\r\n    if (i > end1) {\r\n        return -decimalCompare1(b, ++j, end2);\r\n    }\r\n    if (j > end2) {\r\n        return decimalCompare1(a, ++i, end1);\r\n    }\r\n    if (a[i] == DECIMAL && b[j] == DECIMAL) {\r\n        while (i <= end1 && j <= end2) {\r\n            if (a[i] != b[j]) {\r\n                if (isdigit(a[i]) && isdigit(b[j])) {\r\n                    return a[i] - b[j];\r\n                }\r\n                if (isdigit(a[i])) {\r\n                    return 1;\r\n                }\r\n                if (isdigit(b[j])) {\r\n                    return -1;\r\n                }\r\n                return 0;\r\n            }\r\n            i++;\r\n            j++;\r\n        }\r\n        if (i > end1 && j > end2) {\r\n            return 0;\r\n        }\r\n        if (i > end1) {\r\n            return -decimalCompare1(b, j, end2);\r\n        }\r\n        if (j > end2) {\r\n            return decimalCompare1(a, i, end1);\r\n        }\r\n    } else if (a[i] == DECIMAL) {\r\n        return decimalCompare1(a, ++i, end1);\r\n    } else if (b[j] == DECIMAL) {\r\n        return -decimalCompare1(b, ++j, end2);\r\n    }\r\n    return 0;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\partition",
  "methodName" : "decimalCompare1",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "int decimalCompare1(byte[] a, int i, int end)\n{\r\n    while (i <= end) {\r\n        if (a[i] == ZERO) {\r\n            i++;\r\n            continue;\r\n        }\r\n        if (isdigit(a[i])) {\r\n            return 1;\r\n        } else {\r\n            return 0;\r\n        }\r\n    }\r\n    return 0;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\partition",
  "methodName" : "oneNegativeCompare",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "int oneNegativeCompare(byte[] a, int start1, int end1, byte[] b, int start2, int end2)\n{\r\n    if (!isZero(a, start1, end1)) {\r\n        return -1;\r\n    }\r\n    if (!isZero(b, start2, end2)) {\r\n        return -1;\r\n    }\r\n    return 0;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\partition",
  "methodName" : "isZero",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "boolean isZero(byte[] a, int start, int end)\n{\r\n    int i = start;\r\n    while (i <= end) {\r\n        if (a[i] != ZERO) {\r\n            if (a[i] != DECIMAL && isdigit(a[i])) {\r\n                return false;\r\n            }\r\n            break;\r\n        }\r\n        i++;\r\n    }\r\n    if (i != (end + 1) && a[i++] == DECIMAL) {\r\n        while (i <= end) {\r\n            if (a[i] != ZERO) {\r\n                if (isdigit(a[i])) {\r\n                    return false;\r\n                }\r\n                break;\r\n            }\r\n            i++;\r\n        }\r\n    }\r\n    return true;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\partition",
  "methodName" : "setKeyFieldComparatorOptions",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setKeyFieldComparatorOptions(Job job, String keySpec)\n{\r\n    job.getConfiguration().set(COMPARATOR_OPTIONS, keySpec);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\partition",
  "methodName" : "getKeyFieldComparatorOption",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String getKeyFieldComparatorOption(JobContext job)\n{\r\n    return job.getConfiguration().get(COMPARATOR_OPTIONS);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred\\join",
  "methodName" : "combine",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "boolean combine(Object[] srcs, TupleWritable dst)\n{\r\n    assert srcs.length == dst.size();\r\n    return true;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\input",
  "methodName" : "initialize",
  "errType" : null,
  "containingMethodsNum" : 9,
  "sourceCodeText" : "void initialize(InputSplit split, TaskAttemptContext context) throws IOException, InterruptedException\n{\r\n    FileSplit fileSplit = (FileSplit) split;\r\n    conf = context.getConfiguration();\r\n    Path path = fileSplit.getPath();\r\n    FileSystem fs = path.getFileSystem(conf);\r\n    this.in = new SequenceFile.Reader(fs, path, conf);\r\n    this.end = fileSplit.getStart() + fileSplit.getLength();\r\n    if (fileSplit.getStart() > in.getPosition()) {\r\n        in.sync(fileSplit.getStart());\r\n    }\r\n    this.start = in.getPosition();\r\n    more = start < end;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\input",
  "methodName" : "nextKeyValue",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "boolean nextKeyValue() throws IOException, InterruptedException\n{\r\n    if (!more) {\r\n        return false;\r\n    }\r\n    long pos = in.getPosition();\r\n    key = (K) in.next(key);\r\n    if (key == null || (pos >= end && in.syncSeen())) {\r\n        more = false;\r\n        key = null;\r\n        value = null;\r\n    } else {\r\n        value = (V) in.getCurrentValue(value);\r\n    }\r\n    return more;\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\input",
  "methodName" : "getCurrentKey",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "K getCurrentKey()\n{\r\n    return key;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\input",
  "methodName" : "getCurrentValue",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "V getCurrentValue()\n{\r\n    return value;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\input",
  "methodName" : "getProgress",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "float getProgress() throws IOException\n{\r\n    if (end == start) {\r\n        return 0.0f;\r\n    } else {\r\n        return Math.min(1.0f, (in.getPosition() - start) / (float) (end - start));\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\input",
  "methodName" : "close",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void close() throws IOException\n{\r\n    in.close();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\aggregate",
  "methodName" : "addNextValue",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void addNextValue(Object val)\n{\r\n    String newVal = val.toString();\r\n    if (this.maxVal == null || this.maxVal.compareTo(newVal) < 0) {\r\n        this.maxVal = newVal;\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\aggregate",
  "methodName" : "getVal",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getVal()\n{\r\n    return this.maxVal;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\aggregate",
  "methodName" : "getReport",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getReport()\n{\r\n    return maxVal;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\aggregate",
  "methodName" : "reset",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void reset()\n{\r\n    maxVal = null;\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\aggregate",
  "methodName" : "getCombinerOutput",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "ArrayList<String> getCombinerOutput()\n{\r\n    ArrayList<String> retv = new ArrayList<String>(1);\r\n    retv.add(maxVal);\r\n    return retv;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\task\\reduce",
  "methodName" : "run",
  "errType" : [ "InterruptedException", "Throwable" ],
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void run()\n{\r\n    try {\r\n        while (!stopped && !Thread.currentThread().isInterrupted()) {\r\n            MapHost host = null;\r\n            try {\r\n                merger.waitForResource();\r\n                host = scheduler.getHost();\r\n                metrics.threadBusy();\r\n                copyFromHost(host);\r\n            } finally {\r\n                if (host != null) {\r\n                    scheduler.freeHost(host);\r\n                    metrics.threadFree();\r\n                }\r\n            }\r\n        }\r\n    } catch (InterruptedException ie) {\r\n        return;\r\n    } catch (Throwable t) {\r\n        exceptionReporter.reportException(t);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 2,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\task\\reduce",
  "methodName" : "interrupt",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void interrupt()\n{\r\n    try {\r\n        closeConnection();\r\n    } finally {\r\n        super.interrupt();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\task\\reduce",
  "methodName" : "shutDown",
  "errType" : [ "InterruptedException" ],
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void shutDown() throws InterruptedException\n{\r\n    this.stopped = true;\r\n    interrupt();\r\n    try {\r\n        join(5000);\r\n    } catch (InterruptedException ie) {\r\n        LOG.warn(\"Got interrupt while joining \" + getName(), ie);\r\n    }\r\n    if (sslFactory != null) {\r\n        sslFactory.destroy();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\task\\reduce",
  "methodName" : "openConnection",
  "errType" : [ "GeneralSecurityException" ],
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void openConnection(URL url) throws IOException\n{\r\n    HttpURLConnection conn = (HttpURLConnection) url.openConnection();\r\n    if (sslShuffle) {\r\n        HttpsURLConnection httpsConn = (HttpsURLConnection) conn;\r\n        try {\r\n            httpsConn.setSSLSocketFactory(sslFactory.createSSLSocketFactory());\r\n        } catch (GeneralSecurityException ex) {\r\n            throw new IOException(ex);\r\n        }\r\n        httpsConn.setHostnameVerifier(sslFactory.getHostnameVerifier());\r\n    }\r\n    connection = conn;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\task\\reduce",
  "methodName" : "closeConnection",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void closeConnection()\n{\r\n    if (connection != null) {\r\n        connection.disconnect();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\task\\reduce",
  "methodName" : "abortConnect",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void abortConnect(MapHost host, Set<TaskAttemptID> remaining)\n{\r\n    for (TaskAttemptID left : remaining) {\r\n        scheduler.putBackKnownMapOutput(host, left);\r\n    }\r\n    closeConnection();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\task\\reduce",
  "methodName" : "openShuffleUrl",
  "errType" : [ "TryAgainLaterException", "IOException" ],
  "containingMethodsNum" : 9,
  "sourceCodeText" : "DataInputStream openShuffleUrl(MapHost host, Set<TaskAttemptID> remaining, URL url)\n{\r\n    DataInputStream input = null;\r\n    try {\r\n        setupConnectionsWithRetry(url);\r\n        if (stopped) {\r\n            abortConnect(host, remaining);\r\n        } else {\r\n            input = new DataInputStream(connection.getInputStream());\r\n        }\r\n    } catch (TryAgainLaterException te) {\r\n        LOG.warn(\"Connection rejected by the host \" + te.host + \". Will retry later.\");\r\n        scheduler.penalize(host, te.backoff);\r\n    } catch (IOException ie) {\r\n        boolean connectExcpt = ie instanceof ConnectException;\r\n        ioErrs.increment(1);\r\n        LOG.warn(\"Failed to connect to \" + host + \" with \" + remaining.size() + \" map outputs\", ie);\r\n        scheduler.hostFailed(host.getHostName());\r\n        for (TaskAttemptID left : remaining) {\r\n            scheduler.copyFailed(left, host, false, connectExcpt);\r\n        }\r\n    }\r\n    return input;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\task\\reduce",
  "methodName" : "copyFromHost",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 20,
  "sourceCodeText" : "void copyFromHost(MapHost host) throws IOException\n{\r\n    retryStartTime = 0;\r\n    List<TaskAttemptID> maps = scheduler.getMapsForHost(host);\r\n    if (maps.size() == 0) {\r\n        return;\r\n    }\r\n    if (LOG.isDebugEnabled()) {\r\n        LOG.debug(\"Fetcher \" + id + \" going to fetch from \" + host + \" for: \" + maps);\r\n    }\r\n    Set<TaskAttemptID> remaining = new HashSet<TaskAttemptID>(maps);\r\n    URL url = getMapOutputURL(host, maps);\r\n    DataInputStream input = null;\r\n    try {\r\n        input = openShuffleUrl(host, remaining, url);\r\n        if (input == null) {\r\n            return;\r\n        }\r\n        TaskAttemptID[] failedTasks = null;\r\n        while (!remaining.isEmpty() && failedTasks == null) {\r\n            try {\r\n                failedTasks = copyMapOutput(host, input, remaining, fetchRetryEnabled);\r\n            } catch (IOException e) {\r\n                IOUtils.cleanupWithLogger(LOG, input);\r\n                connection.disconnect();\r\n                url = getMapOutputURL(host, remaining);\r\n                input = openShuffleUrl(host, remaining, url);\r\n                if (input == null) {\r\n                    return;\r\n                }\r\n            }\r\n        }\r\n        if (failedTasks != null && failedTasks.length > 0) {\r\n            LOG.warn(\"copyMapOutput failed for tasks \" + Arrays.toString(failedTasks));\r\n            scheduler.hostFailed(host.getHostName());\r\n            for (TaskAttemptID left : failedTasks) {\r\n                scheduler.copyFailed(left, host, true, false);\r\n            }\r\n        }\r\n        if (failedTasks == null && !remaining.isEmpty()) {\r\n            throw new IOException(\"server didn't return all expected map outputs: \" + remaining.size() + \" left.\");\r\n        }\r\n        input.close();\r\n        input = null;\r\n    } finally {\r\n        if (input != null) {\r\n            IOUtils.cleanupWithLogger(LOG, input);\r\n            input = null;\r\n        }\r\n        for (TaskAttemptID left : remaining) {\r\n            scheduler.putBackKnownMapOutput(host, left);\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 2,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\task\\reduce",
  "methodName" : "setupConnectionsWithRetry",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void setupConnectionsWithRetry(URL url) throws IOException\n{\r\n    openConnectionWithRetry(url);\r\n    if (stopped) {\r\n        return;\r\n    }\r\n    String msgToEncode = SecureShuffleUtils.buildMsgFrom(url);\r\n    String encHash = SecureShuffleUtils.hashFromString(msgToEncode, shuffleSecretKey);\r\n    setupShuffleConnection(encHash);\r\n    connect(connection, connectionTimeout);\r\n    if (stopped) {\r\n        return;\r\n    }\r\n    verifyConnection(url, msgToEncode, encHash);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\task\\reduce",
  "methodName" : "openConnectionWithRetry",
  "errType" : [ "IOException", "InterruptedException" ],
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void openConnectionWithRetry(URL url) throws IOException\n{\r\n    long startTime = Time.monotonicNow();\r\n    boolean shouldWait = true;\r\n    while (shouldWait) {\r\n        try {\r\n            openConnection(url);\r\n            shouldWait = false;\r\n        } catch (IOException e) {\r\n            if (!fetchRetryEnabled) {\r\n                throw e;\r\n            }\r\n            if ((Time.monotonicNow() - startTime) >= this.fetchRetryTimeout) {\r\n                LOG.warn(\"Failed to connect to host: \" + url + \"after \" + fetchRetryTimeout + \" milliseconds.\");\r\n                throw e;\r\n            }\r\n            try {\r\n                Thread.sleep(this.fetchRetryInterval);\r\n            } catch (InterruptedException e1) {\r\n                if (stopped) {\r\n                    return;\r\n                }\r\n            }\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 2,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\task\\reduce",
  "methodName" : "verifyConnection",
  "errType" : null,
  "containingMethodsNum" : 11,
  "sourceCodeText" : "void verifyConnection(URL url, String msgToEncode, String encHash) throws IOException\n{\r\n    int rc = connection.getResponseCode();\r\n    if (rc == TOO_MANY_REQ_STATUS_CODE) {\r\n        long backoff = connection.getHeaderFieldLong(FETCH_RETRY_AFTER_HEADER, FETCH_RETRY_DELAY_DEFAULT);\r\n        if (backoff < 0) {\r\n            backoff = FETCH_RETRY_DELAY_DEFAULT;\r\n            LOG.warn(\"Get a negative backoff value from ShuffleHandler. Setting\" + \" it to the default value \" + FETCH_RETRY_DELAY_DEFAULT);\r\n        }\r\n        throw new TryAgainLaterException(backoff, url.getHost());\r\n    }\r\n    if (rc != HttpURLConnection.HTTP_OK) {\r\n        throw new IOException(\"Got invalid response code \" + rc + \" from \" + url + \": \" + connection.getResponseMessage());\r\n    }\r\n    if (!ShuffleHeader.DEFAULT_HTTP_HEADER_NAME.equals(connection.getHeaderField(ShuffleHeader.HTTP_HEADER_NAME)) || !ShuffleHeader.DEFAULT_HTTP_HEADER_VERSION.equals(connection.getHeaderField(ShuffleHeader.HTTP_HEADER_VERSION))) {\r\n        throw new IOException(\"Incompatible shuffle response version\");\r\n    }\r\n    String replyHash = connection.getHeaderField(SecureShuffleUtils.HTTP_HEADER_REPLY_URL_HASH);\r\n    if (replyHash == null) {\r\n        throw new IOException(\"security validation of TT Map output failed\");\r\n    }\r\n    LOG.debug(\"url=\" + msgToEncode + \";encHash=\" + encHash + \";replyHash=\" + replyHash);\r\n    SecureShuffleUtils.verifyReply(replyHash, encHash, shuffleSecretKey);\r\n    LOG.debug(\"for url=\" + msgToEncode + \" sent hash and received reply\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\task\\reduce",
  "methodName" : "setupShuffleConnection",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void setupShuffleConnection(String encHash)\n{\r\n    connection.addRequestProperty(SecureShuffleUtils.HTTP_HEADER_URL_HASH, encHash);\r\n    connection.setReadTimeout(readTimeout);\r\n    connection.addRequestProperty(ShuffleHeader.HTTP_HEADER_NAME, ShuffleHeader.DEFAULT_HTTP_HEADER_NAME);\r\n    connection.addRequestProperty(ShuffleHeader.HTTP_HEADER_VERSION, ShuffleHeader.DEFAULT_HTTP_HEADER_VERSION);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\task\\reduce",
  "methodName" : "copyMapOutput",
  "errType" : [ "IOException", "IllegalArgumentException", "IOException", "java.lang.InternalError|Exception" ],
  "containingMethodsNum" : 30,
  "sourceCodeText" : "TaskAttemptID[] copyMapOutput(MapHost host, DataInputStream input, Set<TaskAttemptID> remaining, boolean canRetry) throws IOException\n{\r\n    MapOutput<K, V> mapOutput = null;\r\n    TaskAttemptID mapId = null;\r\n    long decompressedLength = -1;\r\n    long compressedLength = -1;\r\n    try {\r\n        long startTime = Time.monotonicNow();\r\n        int forReduce = -1;\r\n        try {\r\n            ShuffleHeader header = new ShuffleHeader();\r\n            header.readFields(input);\r\n            mapId = TaskAttemptID.forName(header.mapId);\r\n            compressedLength = header.compressedLength;\r\n            decompressedLength = header.uncompressedLength;\r\n            forReduce = header.forReduce;\r\n        } catch (IllegalArgumentException e) {\r\n            badIdErrs.increment(1);\r\n            LOG.warn(\"Invalid map id \", e);\r\n            return remaining.toArray(new TaskAttemptID[remaining.size()]);\r\n        }\r\n        InputStream is = input;\r\n        is = IntermediateEncryptedStream.wrapIfNecessary(jobConf, is, compressedLength, null);\r\n        compressedLength -= CryptoUtils.cryptoPadding(jobConf);\r\n        decompressedLength -= CryptoUtils.cryptoPadding(jobConf);\r\n        if (!verifySanity(compressedLength, decompressedLength, forReduce, remaining, mapId)) {\r\n            return new TaskAttemptID[] { mapId };\r\n        }\r\n        if (LOG.isDebugEnabled()) {\r\n            LOG.debug(\"header: \" + mapId + \", len: \" + compressedLength + \", decomp len: \" + decompressedLength);\r\n        }\r\n        try {\r\n            mapOutput = merger.reserve(mapId, decompressedLength, id);\r\n        } catch (IOException ioe) {\r\n            ioErrs.increment(1);\r\n            scheduler.reportLocalError(ioe);\r\n            return EMPTY_ATTEMPT_ID_ARRAY;\r\n        }\r\n        if (mapOutput == null) {\r\n            LOG.info(\"fetcher#\" + id + \" - MergeManager returned status WAIT ...\");\r\n            return EMPTY_ATTEMPT_ID_ARRAY;\r\n        }\r\n        try {\r\n            LOG.info(\"fetcher#\" + id + \" about to shuffle output of map \" + mapOutput.getMapId() + \" decomp: \" + decompressedLength + \" len: \" + compressedLength + \" to \" + mapOutput.getDescription());\r\n            mapOutput.shuffle(host, is, compressedLength, decompressedLength, metrics, reporter);\r\n        } catch (java.lang.InternalError | Exception e) {\r\n            LOG.warn(\"Failed to shuffle for fetcher#\" + id, e);\r\n            throw new IOException(e);\r\n        }\r\n        long endTime = Time.monotonicNow();\r\n        retryStartTime = 0;\r\n        scheduler.copySucceeded(mapId, host, compressedLength, startTime, endTime, mapOutput);\r\n        remaining.remove(mapId);\r\n        metrics.successFetch();\r\n        return null;\r\n    } catch (IOException ioe) {\r\n        if (mapOutput != null) {\r\n            mapOutput.abort();\r\n        }\r\n        if (canRetry) {\r\n            checkTimeoutOrRetry(host, ioe);\r\n        }\r\n        ioErrs.increment(1);\r\n        if (mapId == null || mapOutput == null) {\r\n            LOG.warn(\"fetcher#\" + id + \" failed to read map header\" + mapId + \" decomp: \" + decompressedLength + \", \" + compressedLength, ioe);\r\n            if (mapId == null) {\r\n                return remaining.toArray(new TaskAttemptID[remaining.size()]);\r\n            } else {\r\n                return new TaskAttemptID[] { mapId };\r\n            }\r\n        }\r\n        LOG.warn(\"Failed to shuffle output of \" + mapId + \" from \" + host.getHostName(), ioe);\r\n        metrics.failedFetch();\r\n        return new TaskAttemptID[] { mapId };\r\n    }\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 4,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\task\\reduce",
  "methodName" : "checkTimeoutOrRetry",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void checkTimeoutOrRetry(MapHost host, IOException ioe) throws IOException\n{\r\n    long currentTime = Time.monotonicNow();\r\n    if (retryStartTime == 0) {\r\n        retryStartTime = currentTime;\r\n    }\r\n    if (currentTime - retryStartTime < this.fetchRetryTimeout) {\r\n        LOG.warn(\"Shuffle output from \" + host.getHostName() + \" failed, retry it.\", ioe);\r\n        throw ioe;\r\n    } else {\r\n        LOG.warn(\"Timeout for copying MapOutput with retry on host \" + host + \"after \" + fetchRetryTimeout + \" milliseconds.\");\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\task\\reduce",
  "methodName" : "verifySanity",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "boolean verifySanity(long compressedLength, long decompressedLength, int forReduce, Set<TaskAttemptID> remaining, TaskAttemptID mapId)\n{\r\n    if (compressedLength < 0 || decompressedLength < 0) {\r\n        wrongLengthErrs.increment(1);\r\n        LOG.warn(getName() + \" invalid lengths in map output header: id: \" + mapId + \" len: \" + compressedLength + \", decomp len: \" + decompressedLength);\r\n        return false;\r\n    }\r\n    if (forReduce != reduce) {\r\n        wrongReduceErrs.increment(1);\r\n        LOG.warn(getName() + \" data for the wrong reduce map: \" + mapId + \" len: \" + compressedLength + \" decomp len: \" + decompressedLength + \" for reduce \" + forReduce);\r\n        return false;\r\n    }\r\n    if (!remaining.contains(mapId)) {\r\n        wrongMapErrs.increment(1);\r\n        LOG.warn(\"Invalid map-output! Received output for \" + mapId);\r\n        return false;\r\n    }\r\n    return true;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\task\\reduce",
  "methodName" : "getMapOutputURL",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "URL getMapOutputURL(MapHost host, Collection<TaskAttemptID> maps) throws MalformedURLException\n{\r\n    StringBuffer url = new StringBuffer(host.getBaseUrl());\r\n    boolean first = true;\r\n    for (TaskAttemptID mapId : maps) {\r\n        if (!first) {\r\n            url.append(\",\");\r\n        }\r\n        url.append(mapId);\r\n        first = false;\r\n    }\r\n    LOG.debug(\"MapOutput URL for \" + host + \" -> \" + url.toString());\r\n    return new URL(url.toString());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\task\\reduce",
  "methodName" : "connect",
  "errType" : [ "IOException", "InterruptedException" ],
  "containingMethodsNum" : 10,
  "sourceCodeText" : "void connect(URLConnection connection, int connectionTimeout) throws IOException\n{\r\n    int unit = 0;\r\n    if (connectionTimeout < 0) {\r\n        throw new IOException(\"Invalid timeout \" + \"[timeout = \" + connectionTimeout + \" ms]\");\r\n    } else if (connectionTimeout > 0) {\r\n        unit = Math.min(UNIT_CONNECT_TIMEOUT, connectionTimeout);\r\n    }\r\n    long startTime = Time.monotonicNow();\r\n    long lastTime = startTime;\r\n    int attempts = 0;\r\n    connection.setConnectTimeout(unit);\r\n    while (true) {\r\n        try {\r\n            attempts++;\r\n            connection.connect();\r\n            break;\r\n        } catch (IOException ioe) {\r\n            long currentTime = Time.monotonicNow();\r\n            long retryTime = currentTime - startTime;\r\n            long leftTime = connectionTimeout - retryTime;\r\n            long timeSinceLastIteration = currentTime - lastTime;\r\n            if (leftTime <= 0) {\r\n                int retryTimeInSeconds = (int) retryTime / 1000;\r\n                LOG.error(\"Connection retry failed with \" + attempts + \" attempts in \" + retryTimeInSeconds + \" seconds\");\r\n                throw ioe;\r\n            }\r\n            if (leftTime < unit) {\r\n                unit = (int) leftTime;\r\n                connection.setConnectTimeout(unit);\r\n            }\r\n            if (timeSinceLastIteration < unit) {\r\n                try {\r\n                    sleep(unit - timeSinceLastIteration);\r\n                } catch (InterruptedException e) {\r\n                    LOG.warn(\"Sleep in connection retry get interrupted.\");\r\n                    if (stopped) {\r\n                        return;\r\n                    }\r\n                }\r\n            }\r\n            lastTime = Time.monotonicNow();\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 2,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest\\impl",
  "methodName" : "bindToFileSystem",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void bindToFileSystem(FileSystem fileSystem, Path path) throws IOException\n{\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest\\impl",
  "methodName" : "getFileStatus",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "FileStatus getFileStatus(Path path) throws IOException",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest\\impl",
  "methodName" : "isFile",
  "errType" : [ "FileNotFoundException" ],
  "containingMethodsNum" : 1,
  "sourceCodeText" : "boolean isFile(Path path) throws IOException\n{\r\n    try {\r\n        return getFileStatus(path).isFile();\r\n    } catch (FileNotFoundException e) {\r\n        return false;\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest\\impl",
  "methodName" : "delete",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean delete(Path path, boolean recursive) throws IOException",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest\\impl",
  "methodName" : "mkdirs",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean mkdirs(Path path) throws IOException",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest\\impl",
  "methodName" : "renameFile",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean renameFile(Path source, Path dest) throws IOException",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest\\impl",
  "methodName" : "renameDir",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "boolean renameDir(Path source, Path dest) throws IOException\n{\r\n    return renameFile(source, dest);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest\\impl",
  "methodName" : "listStatusIterator",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "RemoteIterator<FileStatus> listStatusIterator(Path path) throws IOException",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest\\impl",
  "methodName" : "loadTaskManifest",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "TaskManifest loadTaskManifest(JsonSerialization<TaskManifest> serializer, FileStatus st) throws IOException",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest\\impl",
  "methodName" : "save",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void save(T manifestData, Path path, boolean overwrite) throws IOException",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest\\impl",
  "methodName" : "msync",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void msync(Path path) throws IOException\n{\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest\\impl",
  "methodName" : "getEtag",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String getEtag(FileStatus status)\n{\r\n    return ManifestCommitterSupport.getEtag(status);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest\\impl",
  "methodName" : "storePreservesEtagsThroughRenames",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean storePreservesEtagsThroughRenames(Path path)\n{\r\n    return false;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest\\impl",
  "methodName" : "storeSupportsResilientCommit",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean storeSupportsResilientCommit()\n{\r\n    return false;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest\\impl",
  "methodName" : "commitFile",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "CommitFileResult commitFile(FileEntry entry) throws IOException\n{\r\n    throw new UnsupportedOperationException(\"Resilient commit not supported\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred\\lib",
  "methodName" : "configure",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void configure(JobConf job)\n{\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred\\lib",
  "methodName" : "getPartition",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "int getPartition(K2 key, V2 value, int numReduceTasks)\n{\r\n    return (key.hashCode() & Integer.MAX_VALUE) % numReduceTasks;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "start",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void start()\n{\r\n    if (started) {\r\n        return;\r\n    }\r\n    Timer timer = new Timer(\"Timer thread for monitoring \", true);\r\n    TimerTask task = new TimerTask() {\r\n\r\n        public void run() {\r\n            update();\r\n        }\r\n    };\r\n    long millis = period * 1000;\r\n    timer.scheduleAtFixedRate(task, millis, millis);\r\n    started = true;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "update",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void update()\n{\r\n    for (StatUpdater c : updaters.values()) {\r\n        c.update();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getUpdaters",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Map<TimeWindow, StatUpdater> getUpdaters()\n{\r\n    return Collections.unmodifiableMap(updaters);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getStatistics",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Map<String, Stat> getStatistics()\n{\r\n    return Collections.unmodifiableMap(statistics);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "createStat",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Stat createStat(String name)\n{\r\n    return createStat(name, DEFAULT_COLLECT_WINDOWS);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "createStat",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "Stat createStat(String name, TimeWindow[] windows)\n{\r\n    if (statistics.get(name) != null) {\r\n        throw new RuntimeException(\"Stat with name \" + name + \" is already defined\");\r\n    }\r\n    Map<TimeWindow, TimeStat> timeStats = new LinkedHashMap<TimeWindow, TimeStat>();\r\n    for (TimeWindow window : windows) {\r\n        StatUpdater collector = updaters.get(window);\r\n        if (collector == null) {\r\n            if (SINCE_START.equals(window)) {\r\n                collector = new StatUpdater();\r\n            } else {\r\n                collector = new TimeWindowStatUpdater(window, period);\r\n            }\r\n            updaters.put(window, collector);\r\n        }\r\n        TimeStat timeStat = new TimeStat();\r\n        collector.addTimeStat(name, timeStat);\r\n        timeStats.put(window, timeStat);\r\n    }\r\n    Stat stat = new Stat(name, timeStats);\r\n    statistics.put(name, stat);\r\n    return stat;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "removeStat",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "Stat removeStat(String name)\n{\r\n    Stat stat = statistics.remove(name);\r\n    if (stat != null) {\r\n        for (StatUpdater collector : updaters.values()) {\r\n            collector.removeTimeStat(name);\r\n        }\r\n    }\r\n    return stat;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred\\jobcontrol",
  "methodName" : "castToJobList",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "ArrayList<Job> castToJobList(List<ControlledJob> cjobs)\n{\r\n    ArrayList<Job> ret = new ArrayList<Job>();\r\n    for (ControlledJob job : cjobs) {\r\n        ret.add((Job) job);\r\n    }\r\n    return ret;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred\\jobcontrol",
  "methodName" : "getWaitingJobs",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "ArrayList<Job> getWaitingJobs()\n{\r\n    return castToJobList(super.getWaitingJobList());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred\\jobcontrol",
  "methodName" : "getRunningJobs",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "ArrayList<Job> getRunningJobs()\n{\r\n    return castToJobList(super.getRunningJobList());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred\\jobcontrol",
  "methodName" : "getReadyJobs",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "ArrayList<Job> getReadyJobs()\n{\r\n    return castToJobList(super.getReadyJobsList());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred\\jobcontrol",
  "methodName" : "getSuccessfulJobs",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "ArrayList<Job> getSuccessfulJobs()\n{\r\n    return castToJobList(super.getSuccessfulJobList());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred\\jobcontrol",
  "methodName" : "getFailedJobs",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "ArrayList<Job> getFailedJobs()\n{\r\n    return castToJobList(super.getFailedJobList());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred\\jobcontrol",
  "methodName" : "addJobs",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void addJobs(Collection<Job> jobs)\n{\r\n    for (Job job : jobs) {\r\n        addJob(job);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred\\jobcontrol",
  "methodName" : "getState",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "int getState()\n{\r\n    ThreadState state = super.getThreadState();\r\n    if (state == ThreadState.RUNNING) {\r\n        return 0;\r\n    }\r\n    if (state == ThreadState.SUSPENDED) {\r\n        return 1;\r\n    }\r\n    if (state == ThreadState.STOPPED) {\r\n        return 2;\r\n    }\r\n    if (state == ThreadState.STOPPING) {\r\n        return 3;\r\n    }\r\n    if (state == ThreadState.READY) {\r\n        return 4;\r\n    }\r\n    return -1;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "getDatum",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Object getDatum()\n{\r\n    return datum;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "setDatum",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setDatum(Object datum)\n{\r\n    this.datum = (JobPriorityChange) datum;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "getJobId",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "JobID getJobId()\n{\r\n    return JobID.forName(datum.getJobid().toString());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "getPriority",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "JobPriority getPriority()\n{\r\n    return JobPriority.valueOf(datum.getPriority().toString());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "getEventType",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "EventType getEventType()\n{\r\n    return EventType.JOB_PRIORITY_CHANGED;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "toTimelineEvent",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "TimelineEvent toTimelineEvent()\n{\r\n    TimelineEvent tEvent = new TimelineEvent();\r\n    tEvent.setId(StringUtils.toUpperCase(getEventType().name()));\r\n    tEvent.addInfo(\"PRIORITY\", getPriority().toString());\r\n    return tEvent;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "getTimelineMetrics",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Set<TimelineMetric> getTimelineMetrics()\n{\r\n    return null;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\join",
  "methodName" : "id",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int id()",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\join",
  "methodName" : "key",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "K key()",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\join",
  "methodName" : "key",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void key(K key) throws IOException",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\join",
  "methodName" : "createKey",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "K createKey()",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\join",
  "methodName" : "createValue",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "V createValue()",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\join",
  "methodName" : "hasNext",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean hasNext()",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\join",
  "methodName" : "skip",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void skip(K key) throws IOException, InterruptedException",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\join",
  "methodName" : "accept",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void accept(CompositeRecordReader.JoinCollector jc, K key) throws IOException, InterruptedException",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getRecordWriter",
  "errType" : null,
  "containingMethodsNum" : 9,
  "sourceCodeText" : "RecordWriter<K, V> getRecordWriter(FileSystem ignored, JobConf job, String name, Progressable progress) throws IOException\n{\r\n    Path file = FileOutputFormat.getTaskOutputPath(job, name);\r\n    FileSystem fs = file.getFileSystem(job);\r\n    CompressionCodec codec = null;\r\n    CompressionType compressionType = CompressionType.NONE;\r\n    if (getCompressOutput(job)) {\r\n        compressionType = getOutputCompressionType(job);\r\n        Class<? extends CompressionCodec> codecClass = getOutputCompressorClass(job, DefaultCodec.class);\r\n        codec = ReflectionUtils.newInstance(codecClass, job);\r\n    }\r\n    final SequenceFile.Writer out = SequenceFile.createWriter(fs, job, file, job.getOutputKeyClass(), job.getOutputValueClass(), compressionType, codec, progress);\r\n    return new RecordWriter<K, V>() {\r\n\r\n        public void write(K key, V value) throws IOException {\r\n            out.append(key, value);\r\n        }\r\n\r\n        public void close(Reporter reporter) throws IOException {\r\n            out.close();\r\n        }\r\n    };\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getReaders",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "SequenceFile.Reader[] getReaders(Configuration conf, Path dir) throws IOException\n{\r\n    FileSystem fs = dir.getFileSystem(conf);\r\n    Path[] names = FileUtil.stat2Paths(fs.listStatus(dir));\r\n    Arrays.sort(names);\r\n    SequenceFile.Reader[] parts = new SequenceFile.Reader[names.length];\r\n    for (int i = 0; i < names.length; i++) {\r\n        parts[i] = new SequenceFile.Reader(fs, names[i], conf);\r\n    }\r\n    return parts;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getOutputCompressionType",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "CompressionType getOutputCompressionType(JobConf conf)\n{\r\n    String val = conf.get(org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.COMPRESS_TYPE, CompressionType.RECORD.toString());\r\n    return CompressionType.valueOf(val);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "setOutputCompressionType",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void setOutputCompressionType(JobConf conf, CompressionType style)\n{\r\n    setCompressOutput(conf, true);\r\n    conf.set(org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.COMPRESS_TYPE, style.toString());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\join",
  "methodName" : "initialize",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void initialize(InputSplit split, TaskAttemptContext context) throws IOException, InterruptedException\n{\r\n    rr.initialize(split, context);\r\n    conf = context.getConfiguration();\r\n    nextKeyValue();\r\n    if (!empty) {\r\n        keyclass = key.getClass().asSubclass(WritableComparable.class);\r\n        valueclass = value.getClass();\r\n        if (cmp == null) {\r\n            cmp = WritableComparator.get(keyclass, conf);\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\join",
  "methodName" : "createKey",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "K createKey()\n{\r\n    if (keyclass != null) {\r\n        return (K) ReflectionUtils.newInstance(keyclass, conf);\r\n    }\r\n    return (K) NullWritable.get();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\join",
  "methodName" : "createValue",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "U createValue()\n{\r\n    if (valueclass != null) {\r\n        return (U) ReflectionUtils.newInstance(valueclass, conf);\r\n    }\r\n    return (U) NullWritable.get();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\join",
  "methodName" : "id",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int id()\n{\r\n    return id;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\join",
  "methodName" : "key",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "K key()\n{\r\n    return key;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\join",
  "methodName" : "key",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void key(K qkey) throws IOException\n{\r\n    ReflectionUtils.copy(conf, key, qkey);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\join",
  "methodName" : "hasNext",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean hasNext()\n{\r\n    return !empty;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\join",
  "methodName" : "skip",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void skip(K key) throws IOException, InterruptedException\n{\r\n    if (hasNext()) {\r\n        while (cmp.compare(key(), key) <= 0 && next()) ;\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\join",
  "methodName" : "accept",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void accept(CompositeRecordReader.JoinCollector i, K key) throws IOException, InterruptedException\n{\r\n    vjoin.clear();\r\n    if (key() != null && 0 == cmp.compare(key, key())) {\r\n        do {\r\n            vjoin.add(value);\r\n        } while (next() && 0 == cmp.compare(key, key()));\r\n    }\r\n    i.add(id, vjoin);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\join",
  "methodName" : "nextKeyValue",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "boolean nextKeyValue() throws IOException, InterruptedException\n{\r\n    if (hasNext()) {\r\n        next();\r\n        return true;\r\n    }\r\n    return false;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\join",
  "methodName" : "next",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "boolean next() throws IOException, InterruptedException\n{\r\n    empty = !rr.nextKeyValue();\r\n    key = rr.getCurrentKey();\r\n    value = rr.getCurrentValue();\r\n    return !empty;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\join",
  "methodName" : "getCurrentKey",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "K getCurrentKey() throws IOException, InterruptedException\n{\r\n    return rr.getCurrentKey();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\join",
  "methodName" : "getCurrentValue",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "U getCurrentValue() throws IOException, InterruptedException\n{\r\n    return rr.getCurrentValue();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\join",
  "methodName" : "getProgress",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "float getProgress() throws IOException, InterruptedException\n{\r\n    return rr.getProgress();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\join",
  "methodName" : "close",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void close() throws IOException\n{\r\n    rr.close();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\join",
  "methodName" : "compareTo",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "int compareTo(ComposableRecordReader<K, ?> other)\n{\r\n    return cmp.compare(key(), other.key());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\join",
  "methodName" : "equals",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "boolean equals(Object other)\n{\r\n    return other instanceof ComposableRecordReader && 0 == compareTo((ComposableRecordReader) other);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\join",
  "methodName" : "hashCode",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int hashCode()\n{\r\n    assert false : \"hashCode not designed\";\r\n    return 42;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "loadFrom",
  "errType" : [ "ParserConfigurationException", "SAXException", "IOException" ],
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void loadFrom(InputStream xmlInput)\n{\r\n    try {\r\n        this.root = loadResource(xmlInput);\r\n    } catch (ParserConfigurationException e) {\r\n        throw new RuntimeException(e);\r\n    } catch (SAXException e) {\r\n        throw new RuntimeException(e);\r\n    } catch (IOException e) {\r\n        throw new RuntimeException(e);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "setAclsEnabled",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setAclsEnabled(boolean aclsEnabled)\n{\r\n    this.aclsEnabled = aclsEnabled;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "isAclsEnabled",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean isAclsEnabled()\n{\r\n    return aclsEnabled;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getRoot",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Queue getRoot()\n{\r\n    return root;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "setRoot",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setRoot(Queue root)\n{\r\n    this.root = root;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "loadResource",
  "errType" : [ "UnsupportedOperationException" ],
  "containingMethodsNum" : 9,
  "sourceCodeText" : "Queue loadResource(InputStream resourceInput) throws ParserConfigurationException, SAXException, IOException\n{\r\n    DocumentBuilderFactory docBuilderFactory = DocumentBuilderFactory.newInstance();\r\n    docBuilderFactory.setIgnoringComments(true);\r\n    docBuilderFactory.setNamespaceAware(true);\r\n    try {\r\n        docBuilderFactory.setXIncludeAware(true);\r\n    } catch (UnsupportedOperationException e) {\r\n        LOG.info(\"Failed to set setXIncludeAware(true) for parser \" + docBuilderFactory + NAME_SEPARATOR + e);\r\n    }\r\n    DocumentBuilder builder = docBuilderFactory.newDocumentBuilder();\r\n    Document doc = null;\r\n    Element queuesNode = null;\r\n    doc = builder.parse(resourceInput);\r\n    queuesNode = doc.getDocumentElement();\r\n    return this.parseResource(queuesNode);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "parseResource",
  "errType" : [ "DOMException" ],
  "containingMethodsNum" : 16,
  "sourceCodeText" : "Queue parseResource(Element queuesNode)\n{\r\n    Queue rootNode = null;\r\n    try {\r\n        if (!QUEUES_TAG.equals(queuesNode.getTagName())) {\r\n            LOG.info(\"Bad conf file: top-level element not <queues>\");\r\n            throw new RuntimeException(\"No queues defined \");\r\n        }\r\n        NamedNodeMap nmp = queuesNode.getAttributes();\r\n        Node acls = nmp.getNamedItem(ACLS_ENABLED_TAG);\r\n        if (acls != null) {\r\n            LOG.warn(\"Configuring \" + ACLS_ENABLED_TAG + \" flag in \" + QueueManager.QUEUE_CONF_FILE_NAME + \" is not valid. \" + \"This tag is ignored. Configure \" + MRConfig.MR_ACLS_ENABLED + \" in mapred-site.xml. See the \" + \" documentation of \" + MRConfig.MR_ACLS_ENABLED + \", which is used for enabling job level authorization and \" + \" queue level authorization.\");\r\n        }\r\n        NodeList props = queuesNode.getChildNodes();\r\n        if (props == null || props.getLength() <= 0) {\r\n            LOG.info(\" Bad configuration no queues defined \");\r\n            throw new RuntimeException(\" No queues defined \");\r\n        }\r\n        for (int i = 0; i < props.getLength(); i++) {\r\n            Node propNode = props.item(i);\r\n            if (!(propNode instanceof Element)) {\r\n                continue;\r\n            }\r\n            if (!propNode.getNodeName().equals(QUEUE_TAG)) {\r\n                LOG.info(\"At root level only \\\" queue \\\" tags are allowed \");\r\n                throw new RuntimeException(\"Malformed xml document no queue defined \");\r\n            }\r\n            Element prop = (Element) propNode;\r\n            Queue q = createHierarchy(\"\", prop);\r\n            if (rootNode == null) {\r\n                rootNode = new Queue();\r\n                rootNode.setName(\"\");\r\n            }\r\n            rootNode.addChild(q);\r\n        }\r\n        return rootNode;\r\n    } catch (DOMException e) {\r\n        LOG.info(\"Error parsing conf file: \" + e);\r\n        throw new RuntimeException(e);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "createHierarchy",
  "errType" : null,
  "containingMethodsNum" : 33,
  "sourceCodeText" : "Queue createHierarchy(String parent, Element queueNode)\n{\r\n    if (queueNode == null) {\r\n        return null;\r\n    }\r\n    String name = \"\";\r\n    Queue newQueue = new Queue();\r\n    Map<String, AccessControlList> acls = new HashMap<String, AccessControlList>();\r\n    NodeList fields = queueNode.getChildNodes();\r\n    validate(queueNode);\r\n    List<Element> subQueues = new ArrayList<Element>();\r\n    String submitKey = \"\";\r\n    String adminKey = \"\";\r\n    for (int j = 0; j < fields.getLength(); j++) {\r\n        Node fieldNode = fields.item(j);\r\n        if (!(fieldNode instanceof Element)) {\r\n            continue;\r\n        }\r\n        Element field = (Element) fieldNode;\r\n        if (QUEUE_NAME_TAG.equals(field.getTagName())) {\r\n            String nameValue = field.getTextContent();\r\n            if (field.getTextContent() == null || field.getTextContent().trim().equals(\"\") || field.getTextContent().contains(NAME_SEPARATOR)) {\r\n                throw new RuntimeException(\"Improper queue name : \" + nameValue);\r\n            }\r\n            if (!parent.equals(\"\")) {\r\n                name += parent + NAME_SEPARATOR;\r\n            }\r\n            name += nameValue;\r\n            newQueue.setName(name);\r\n            submitKey = toFullPropertyName(name, QueueACL.SUBMIT_JOB.getAclName());\r\n            adminKey = toFullPropertyName(name, QueueACL.ADMINISTER_JOBS.getAclName());\r\n        }\r\n        if (QUEUE_TAG.equals(field.getTagName()) && field.hasChildNodes()) {\r\n            subQueues.add(field);\r\n        }\r\n        if (isAclsEnabled()) {\r\n            if (ACL_SUBMIT_JOB_TAG.equals(field.getTagName())) {\r\n                acls.put(submitKey, new AccessControlList(field.getTextContent()));\r\n            }\r\n            if (ACL_ADMINISTER_JOB_TAG.equals(field.getTagName())) {\r\n                acls.put(adminKey, new AccessControlList(field.getTextContent()));\r\n            }\r\n        }\r\n        if (PROPERTIES_TAG.equals(field.getTagName())) {\r\n            Properties properties = populateProperties(field);\r\n            newQueue.setProperties(properties);\r\n        }\r\n        if (STATE_TAG.equals(field.getTagName())) {\r\n            String state = field.getTextContent();\r\n            newQueue.setState(QueueState.getState(state));\r\n        }\r\n    }\r\n    if (!acls.containsKey(submitKey)) {\r\n        acls.put(submitKey, new AccessControlList(\" \"));\r\n    }\r\n    if (!acls.containsKey(adminKey)) {\r\n        acls.put(adminKey, new AccessControlList(\" \"));\r\n    }\r\n    newQueue.setAcls(acls);\r\n    for (Element field : subQueues) {\r\n        newQueue.addChild(createHierarchy(newQueue.getName(), field));\r\n    }\r\n    return newQueue;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "populateProperties",
  "errType" : null,
  "containingMethodsNum" : 9,
  "sourceCodeText" : "Properties populateProperties(Element field)\n{\r\n    Properties props = new Properties();\r\n    NodeList propfields = field.getChildNodes();\r\n    for (int i = 0; i < propfields.getLength(); i++) {\r\n        Node prop = propfields.item(i);\r\n        if (!(prop instanceof Element)) {\r\n            continue;\r\n        }\r\n        if (PROPERTY_TAG.equals(prop.getNodeName())) {\r\n            if (prop.hasAttributes()) {\r\n                NamedNodeMap nmp = prop.getAttributes();\r\n                if (nmp.getNamedItem(KEY_TAG) != null && nmp.getNamedItem(VALUE_TAG) != null) {\r\n                    props.setProperty(nmp.getNamedItem(KEY_TAG).getTextContent(), nmp.getNamedItem(VALUE_TAG).getTextContent());\r\n                }\r\n            }\r\n        }\r\n    }\r\n    return props;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "validate",
  "errType" : null,
  "containingMethodsNum" : 9,
  "sourceCodeText" : "void validate(Node node)\n{\r\n    NodeList fields = node.getChildNodes();\r\n    Set<String> siblings = new HashSet<String>();\r\n    for (int i = 0; i < fields.getLength(); i++) {\r\n        if (!(fields.item(i) instanceof Element)) {\r\n            continue;\r\n        }\r\n        siblings.add((fields.item(i)).getNodeName());\r\n    }\r\n    if (!siblings.contains(QUEUE_NAME_TAG)) {\r\n        throw new RuntimeException(\" Malformed xml formation queue name not specified \");\r\n    }\r\n    if (siblings.contains(QUEUE_TAG) && (siblings.contains(ACL_ADMINISTER_JOB_TAG) || siblings.contains(ACL_SUBMIT_JOB_TAG) || siblings.contains(STATE_TAG))) {\r\n        throw new RuntimeException(\" Malformed xml formation queue tag and acls \" + \"tags or state tags are siblings \");\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getSimpleQueueName",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "String getSimpleQueueName(String fullQName)\n{\r\n    int index = fullQName.lastIndexOf(NAME_SEPARATOR);\r\n    if (index < 0) {\r\n        return fullQName;\r\n    }\r\n    return fullQName.substring(index + 1, fullQName.length());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getQueueElement",
  "errType" : null,
  "containingMethodsNum" : 19,
  "sourceCodeText" : "Element getQueueElement(Document document, JobQueueInfo jqi)\n{\r\n    Element q = document.createElement(QUEUE_TAG);\r\n    Element qName = document.createElement(QUEUE_NAME_TAG);\r\n    qName.setTextContent(getSimpleQueueName(jqi.getQueueName()));\r\n    q.appendChild(qName);\r\n    Properties props = jqi.getProperties();\r\n    Element propsElement = document.createElement(PROPERTIES_TAG);\r\n    if (props != null) {\r\n        Set<String> propList = props.stringPropertyNames();\r\n        for (String prop : propList) {\r\n            Element propertyElement = document.createElement(PROPERTY_TAG);\r\n            propertyElement.setAttribute(KEY_TAG, prop);\r\n            propertyElement.setAttribute(VALUE_TAG, (String) props.get(prop));\r\n            propsElement.appendChild(propertyElement);\r\n        }\r\n    }\r\n    q.appendChild(propsElement);\r\n    String queueState = jqi.getState().getStateName();\r\n    if (queueState != null && !queueState.equals(QueueState.UNDEFINED.getStateName())) {\r\n        Element qStateElement = document.createElement(STATE_TAG);\r\n        qStateElement.setTextContent(queueState);\r\n        q.appendChild(qStateElement);\r\n    }\r\n    List<JobQueueInfo> children = jqi.getChildren();\r\n    if (children != null) {\r\n        for (JobQueueInfo child : children) {\r\n            q.appendChild(getQueueElement(document, child));\r\n        }\r\n    }\r\n    return q;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "getRecordWriter",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "RecordWriter<K, V> getRecordWriter(TaskAttemptContext context) throws IOException, InterruptedException",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "checkOutputSpecs",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void checkOutputSpecs(JobContext context) throws IOException, InterruptedException",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "getOutputCommitter",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "OutputCommitter getOutputCommitter(TaskAttemptContext context) throws IOException, InterruptedException",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "setMapProgress",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setMapProgress(float p)\n{\r\n    this.mapProgress = (float) Math.min(1.0, Math.max(0.0, p));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "setCleanupProgress",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setCleanupProgress(float p)\n{\r\n    this.cleanupProgress = (float) Math.min(1.0, Math.max(0.0, p));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "setSetupProgress",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setSetupProgress(float p)\n{\r\n    this.setupProgress = (float) Math.min(1.0, Math.max(0.0, p));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "setReduceProgress",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setReduceProgress(float p)\n{\r\n    this.reduceProgress = (float) Math.min(1.0, Math.max(0.0, p));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "setPriority",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setPriority(JobPriority jp)\n{\r\n    if (jp == null) {\r\n        throw new IllegalArgumentException(\"Job priority cannot be null.\");\r\n    }\r\n    priority = jp;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "setFinishTime",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setFinishTime(long finishTime)\n{\r\n    this.finishTime = finishTime;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "setHistoryFile",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setHistoryFile(String historyFile)\n{\r\n    this.historyFile = historyFile;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "setTrackingUrl",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setTrackingUrl(String trackingUrl)\n{\r\n    this.trackingUrl = trackingUrl;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "setRetired",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setRetired()\n{\r\n    this.isRetired = true;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "setState",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setState(State state)\n{\r\n    this.runState = state;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "setStartTime",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setStartTime(long startTime)\n{\r\n    this.startTime = startTime;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "setUsername",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setUsername(String userName)\n{\r\n    this.user = userName;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "setSchedulingInfo",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setSchedulingInfo(String schedulingInfo)\n{\r\n    this.schedulingInfo = schedulingInfo;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "setJobACLs",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setJobACLs(Map<JobACL, AccessControlList> acls)\n{\r\n    this.jobACLs = acls;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "setQueue",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setQueue(String queue)\n{\r\n    this.queue = queue;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "setFailureInfo",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setFailureInfo(String failureInfo)\n{\r\n    this.failureInfo = failureInfo;\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "getQueue",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getQueue()\n{\r\n    return queue;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "getMapProgress",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "float getMapProgress()\n{\r\n    return mapProgress;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "getCleanupProgress",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "float getCleanupProgress()\n{\r\n    return cleanupProgress;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "getSetupProgress",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "float getSetupProgress()\n{\r\n    return setupProgress;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "getReduceProgress",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "float getReduceProgress()\n{\r\n    return reduceProgress;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "getState",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "State getState()\n{\r\n    return runState;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "getStartTime",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getStartTime()\n{\r\n    return startTime;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "clone",
  "errType" : [ "CloneNotSupportedException" ],
  "containingMethodsNum" : 2,
  "sourceCodeText" : "Object clone()\n{\r\n    try {\r\n        return super.clone();\r\n    } catch (CloneNotSupportedException cnse) {\r\n        throw new InternalError(cnse.toString());\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "getJobID",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "JobID getJobID()\n{\r\n    return jobid;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "getUsername",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getUsername()\n{\r\n    return this.user;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "getSchedulingInfo",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getSchedulingInfo()\n{\r\n    return schedulingInfo;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "getJobACLs",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Map<JobACL, AccessControlList> getJobACLs()\n{\r\n    return jobACLs;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "getPriority",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "JobPriority getPriority()\n{\r\n    return priority;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "getFailureInfo",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getFailureInfo()\n{\r\n    return this.failureInfo;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "isJobComplete",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean isJobComplete()\n{\r\n    return (runState == JobStatus.State.SUCCEEDED || runState == JobStatus.State.FAILED || runState == JobStatus.State.KILLED);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "write",
  "errType" : null,
  "containingMethodsNum" : 21,
  "sourceCodeText" : "void write(DataOutput out) throws IOException\n{\r\n    jobid.write(out);\r\n    out.writeFloat(setupProgress);\r\n    out.writeFloat(mapProgress);\r\n    out.writeFloat(reduceProgress);\r\n    out.writeFloat(cleanupProgress);\r\n    WritableUtils.writeEnum(out, runState);\r\n    out.writeLong(startTime);\r\n    Text.writeString(out, user);\r\n    WritableUtils.writeEnum(out, priority);\r\n    Text.writeString(out, schedulingInfo);\r\n    out.writeLong(finishTime);\r\n    out.writeBoolean(isRetired);\r\n    Text.writeString(out, historyFile);\r\n    Text.writeString(out, jobName);\r\n    Text.writeString(out, trackingUrl);\r\n    Text.writeString(out, jobFile);\r\n    out.writeBoolean(isUber);\r\n    out.writeInt(jobACLs.size());\r\n    for (Entry<JobACL, AccessControlList> entry : jobACLs.entrySet()) {\r\n        WritableUtils.writeEnum(out, entry.getKey());\r\n        entry.getValue().write(out);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "readFields",
  "errType" : null,
  "containingMethodsNum" : 21,
  "sourceCodeText" : "void readFields(DataInput in) throws IOException\n{\r\n    this.jobid = new JobID();\r\n    this.jobid.readFields(in);\r\n    this.setupProgress = in.readFloat();\r\n    this.mapProgress = in.readFloat();\r\n    this.reduceProgress = in.readFloat();\r\n    this.cleanupProgress = in.readFloat();\r\n    this.runState = WritableUtils.readEnum(in, State.class);\r\n    this.startTime = in.readLong();\r\n    this.user = StringInterner.weakIntern(Text.readString(in));\r\n    this.priority = WritableUtils.readEnum(in, JobPriority.class);\r\n    this.schedulingInfo = StringInterner.weakIntern(Text.readString(in));\r\n    this.finishTime = in.readLong();\r\n    this.isRetired = in.readBoolean();\r\n    this.historyFile = StringInterner.weakIntern(Text.readString(in));\r\n    this.jobName = StringInterner.weakIntern(Text.readString(in));\r\n    this.trackingUrl = StringInterner.weakIntern(Text.readString(in));\r\n    this.jobFile = StringInterner.weakIntern(Text.readString(in));\r\n    this.isUber = in.readBoolean();\r\n    int numACLs = in.readInt();\r\n    for (int i = 0; i < numACLs; i++) {\r\n        JobACL aclType = WritableUtils.readEnum(in, JobACL.class);\r\n        AccessControlList acl = new AccessControlList(\" \");\r\n        acl.readFields(in);\r\n        this.jobACLs.put(aclType, acl);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "getJobName",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getJobName()\n{\r\n    return jobName;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "getJobFile",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getJobFile()\n{\r\n    return jobFile;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "getTrackingUrl",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getTrackingUrl()\n{\r\n    return trackingUrl;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "getFinishTime",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getFinishTime()\n{\r\n    return finishTime;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "isRetired",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean isRetired()\n{\r\n    return isRetired;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "getHistoryFile",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getHistoryFile()\n{\r\n    return historyFile;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "getNumUsedSlots",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int getNumUsedSlots()\n{\r\n    return numUsedSlots;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "setNumUsedSlots",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setNumUsedSlots(int n)\n{\r\n    numUsedSlots = n;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "getNumReservedSlots",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int getNumReservedSlots()\n{\r\n    return numReservedSlots;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "setNumReservedSlots",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setNumReservedSlots(int n)\n{\r\n    this.numReservedSlots = n;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "getUsedMem",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int getUsedMem()\n{\r\n    return usedMem;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "setUsedMem",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setUsedMem(int m)\n{\r\n    this.usedMem = m;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "getReservedMem",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int getReservedMem()\n{\r\n    return reservedMem;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "setReservedMem",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setReservedMem(int r)\n{\r\n    this.reservedMem = r;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "getNeededMem",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int getNeededMem()\n{\r\n    return neededMem;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "setNeededMem",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setNeededMem(int n)\n{\r\n    this.neededMem = n;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "isUber",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean isUber()\n{\r\n    return isUber;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "setUber",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setUber(boolean isUber)\n{\r\n    this.isUber = isUber;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "toString",
  "errType" : null,
  "containingMethodsNum" : 17,
  "sourceCodeText" : "String toString()\n{\r\n    StringBuffer buffer = new StringBuffer();\r\n    buffer.append(\"job-id : \" + jobid);\r\n    buffer.append(\"uber-mode : \" + isUber);\r\n    buffer.append(\"map-progress : \" + mapProgress);\r\n    buffer.append(\"reduce-progress : \" + reduceProgress);\r\n    buffer.append(\"cleanup-progress : \" + cleanupProgress);\r\n    buffer.append(\"setup-progress : \" + setupProgress);\r\n    buffer.append(\"runstate : \" + runState);\r\n    buffer.append(\"start-time : \" + startTime);\r\n    buffer.append(\"user-name : \" + user);\r\n    buffer.append(\"priority : \" + priority);\r\n    buffer.append(\"scheduling-info : \" + schedulingInfo);\r\n    buffer.append(\"num-used-slots\" + numUsedSlots);\r\n    buffer.append(\"num-reserved-slots\" + numReservedSlots);\r\n    buffer.append(\"used-mem\" + usedMem);\r\n    buffer.append(\"reserved-mem\" + reservedMem);\r\n    buffer.append(\"needed-mem\" + neededMem);\r\n    return buffer.toString();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\map",
  "methodName" : "map",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void map(Object key, Text value, Context context) throws IOException, InterruptedException\n{\r\n    StringTokenizer itr = new StringTokenizer(value.toString());\r\n    while (itr.hasMoreTokens()) {\r\n        word.set(itr.nextToken());\r\n        context.write(word, one);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\partition",
  "methodName" : "printUsage",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "int printUsage()\n{\r\n    System.out.println(\"sampler -r <reduces>\\n\" + \"      [-inFormat <input format class>]\\n\" + \"      [-keyClass <map input & output key class>]\\n\" + \"      [-splitRandom <double pcnt> <numSamples> <maxsplits> | \" + \"             // Sample from random splits at random (general)\\n\" + \"       -splitSample <numSamples> <maxsplits> | \" + \"             // Sample from first records in splits (random data)\\n\" + \"       -splitInterval <double pcnt> <maxsplits>]\" + \"             // Sample from splits at intervals (sorted data)\");\r\n    System.out.println(\"Default sampler: -splitRandom 0.1 10000 10\");\r\n    ToolRunner.printGenericCommandUsage(System.out);\r\n    return -1;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\partition",
  "methodName" : "writePartitionFile",
  "errType" : null,
  "containingMethodsNum" : 16,
  "sourceCodeText" : "void writePartitionFile(Job job, Sampler<K, V> sampler) throws IOException, ClassNotFoundException, InterruptedException\n{\r\n    Configuration conf = job.getConfiguration();\r\n    final InputFormat inf = ReflectionUtils.newInstance(job.getInputFormatClass(), conf);\r\n    int numPartitions = job.getNumReduceTasks();\r\n    K[] samples = (K[]) sampler.getSample(inf, job);\r\n    LOG.info(\"Using \" + samples.length + \" samples\");\r\n    RawComparator<K> comparator = (RawComparator<K>) job.getSortComparator();\r\n    Arrays.sort(samples, comparator);\r\n    Path dst = new Path(TotalOrderPartitioner.getPartitionFile(conf));\r\n    FileSystem fs = dst.getFileSystem(conf);\r\n    fs.delete(dst, false);\r\n    SequenceFile.Writer writer = SequenceFile.createWriter(fs, conf, dst, job.getMapOutputKeyClass(), NullWritable.class);\r\n    NullWritable nullValue = NullWritable.get();\r\n    float stepSize = samples.length / (float) numPartitions;\r\n    int last = -1;\r\n    for (int i = 1; i < numPartitions; ++i) {\r\n        int k = Math.round(stepSize * i);\r\n        while (last >= k && comparator.compare(samples[last], samples[k]) == 0) {\r\n            ++k;\r\n        }\r\n        writer.append(samples[k], nullValue);\r\n        last = k;\r\n    }\r\n    writer.close();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\partition",
  "methodName" : "run",
  "errType" : [ "NumberFormatException", "ArrayIndexOutOfBoundsException" ],
  "containingMethodsNum" : 32,
  "sourceCodeText" : "int run(String[] args) throws Exception\n{\r\n    Job job = Job.getInstance(getConf());\r\n    ArrayList<String> otherArgs = new ArrayList<String>();\r\n    Sampler<K, V> sampler = null;\r\n    for (int i = 0; i < args.length; ++i) {\r\n        try {\r\n            if (\"-r\".equals(args[i])) {\r\n                job.setNumReduceTasks(Integer.parseInt(args[++i]));\r\n            } else if (\"-inFormat\".equals(args[i])) {\r\n                job.setInputFormatClass(Class.forName(args[++i]).asSubclass(InputFormat.class));\r\n            } else if (\"-keyClass\".equals(args[i])) {\r\n                job.setMapOutputKeyClass(Class.forName(args[++i]).asSubclass(WritableComparable.class));\r\n            } else if (\"-splitSample\".equals(args[i])) {\r\n                int numSamples = Integer.parseInt(args[++i]);\r\n                int maxSplits = Integer.parseInt(args[++i]);\r\n                if (0 >= maxSplits)\r\n                    maxSplits = Integer.MAX_VALUE;\r\n                sampler = new SplitSampler<K, V>(numSamples, maxSplits);\r\n            } else if (\"-splitRandom\".equals(args[i])) {\r\n                double pcnt = Double.parseDouble(args[++i]);\r\n                int numSamples = Integer.parseInt(args[++i]);\r\n                int maxSplits = Integer.parseInt(args[++i]);\r\n                if (0 >= maxSplits)\r\n                    maxSplits = Integer.MAX_VALUE;\r\n                sampler = new RandomSampler<K, V>(pcnt, numSamples, maxSplits);\r\n            } else if (\"-splitInterval\".equals(args[i])) {\r\n                double pcnt = Double.parseDouble(args[++i]);\r\n                int maxSplits = Integer.parseInt(args[++i]);\r\n                if (0 >= maxSplits)\r\n                    maxSplits = Integer.MAX_VALUE;\r\n                sampler = new IntervalSampler<K, V>(pcnt, maxSplits);\r\n            } else {\r\n                otherArgs.add(args[i]);\r\n            }\r\n        } catch (NumberFormatException except) {\r\n            System.out.println(\"ERROR: Integer expected instead of \" + args[i]);\r\n            return printUsage();\r\n        } catch (ArrayIndexOutOfBoundsException except) {\r\n            System.out.println(\"ERROR: Required parameter missing from \" + args[i - 1]);\r\n            return printUsage();\r\n        }\r\n    }\r\n    if (job.getNumReduceTasks() <= 1) {\r\n        System.err.println(\"Sampler requires more than one reducer\");\r\n        return printUsage();\r\n    }\r\n    if (otherArgs.size() < 2) {\r\n        System.out.println(\"ERROR: Wrong number of parameters: \");\r\n        return printUsage();\r\n    }\r\n    if (null == sampler) {\r\n        sampler = new RandomSampler<K, V>(0.1, 10000, 10);\r\n    }\r\n    Path outf = new Path(otherArgs.remove(otherArgs.size() - 1));\r\n    TotalOrderPartitioner.setPartitionFile(getConf(), outf);\r\n    for (String s : otherArgs) {\r\n        FileInputFormat.addInputPath(job, new Path(s));\r\n    }\r\n    InputSampler.<K, V>writePartitionFile(job, sampler);\r\n    return 0;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\partition",
  "methodName" : "main",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void main(String[] args) throws Exception\n{\r\n    InputSampler<?, ?> sampler = new InputSampler(new Configuration());\r\n    int res = ToolRunner.run(sampler, args);\r\n    System.exit(res);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\db",
  "methodName" : "configureDB",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void configureDB(Configuration conf, String driverClass, String dbUrl, String userName, String passwd)\n{\r\n    conf.set(DRIVER_CLASS_PROPERTY, driverClass);\r\n    conf.set(URL_PROPERTY, dbUrl);\r\n    if (userName != null) {\r\n        conf.set(USERNAME_PROPERTY, userName);\r\n    }\r\n    if (passwd != null) {\r\n        conf.set(PASSWORD_PROPERTY, passwd);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\db",
  "methodName" : "configureDB",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void configureDB(Configuration job, String driverClass, String dbUrl)\n{\r\n    configureDB(job, driverClass, dbUrl, null, null);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\db",
  "methodName" : "getConnection",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "Connection getConnection() throws ClassNotFoundException, SQLException\n{\r\n    Class.forName(conf.get(DBConfiguration.DRIVER_CLASS_PROPERTY));\r\n    if (conf.get(DBConfiguration.USERNAME_PROPERTY) == null) {\r\n        return DriverManager.getConnection(conf.get(DBConfiguration.URL_PROPERTY));\r\n    } else {\r\n        return DriverManager.getConnection(conf.get(DBConfiguration.URL_PROPERTY), conf.get(DBConfiguration.USERNAME_PROPERTY), conf.get(DBConfiguration.PASSWORD_PROPERTY));\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\db",
  "methodName" : "getConf",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Configuration getConf()\n{\r\n    return conf;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\db",
  "methodName" : "getInputTableName",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String getInputTableName()\n{\r\n    return conf.get(DBConfiguration.INPUT_TABLE_NAME_PROPERTY);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\db",
  "methodName" : "setInputTableName",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setInputTableName(String tableName)\n{\r\n    conf.set(DBConfiguration.INPUT_TABLE_NAME_PROPERTY, tableName);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\db",
  "methodName" : "getInputFieldNames",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String[] getInputFieldNames()\n{\r\n    return conf.getStrings(DBConfiguration.INPUT_FIELD_NAMES_PROPERTY);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\db",
  "methodName" : "setInputFieldNames",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setInputFieldNames(String... fieldNames)\n{\r\n    conf.setStrings(DBConfiguration.INPUT_FIELD_NAMES_PROPERTY, fieldNames);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\db",
  "methodName" : "getInputConditions",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String getInputConditions()\n{\r\n    return conf.get(DBConfiguration.INPUT_CONDITIONS_PROPERTY);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\db",
  "methodName" : "setInputConditions",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void setInputConditions(String conditions)\n{\r\n    if (conditions != null && conditions.length() > 0)\r\n        conf.set(DBConfiguration.INPUT_CONDITIONS_PROPERTY, conditions);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\db",
  "methodName" : "getInputOrderBy",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String getInputOrderBy()\n{\r\n    return conf.get(DBConfiguration.INPUT_ORDER_BY_PROPERTY);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\db",
  "methodName" : "setInputOrderBy",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void setInputOrderBy(String orderby)\n{\r\n    if (orderby != null && orderby.length() > 0) {\r\n        conf.set(DBConfiguration.INPUT_ORDER_BY_PROPERTY, orderby);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\db",
  "methodName" : "getInputQuery",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String getInputQuery()\n{\r\n    return conf.get(DBConfiguration.INPUT_QUERY);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\db",
  "methodName" : "setInputQuery",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void setInputQuery(String query)\n{\r\n    if (query != null && query.length() > 0) {\r\n        conf.set(DBConfiguration.INPUT_QUERY, query);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\db",
  "methodName" : "getInputCountQuery",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String getInputCountQuery()\n{\r\n    return conf.get(DBConfiguration.INPUT_COUNT_QUERY);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\db",
  "methodName" : "setInputCountQuery",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void setInputCountQuery(String query)\n{\r\n    if (query != null && query.length() > 0) {\r\n        conf.set(DBConfiguration.INPUT_COUNT_QUERY, query);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\db",
  "methodName" : "setInputBoundingQuery",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void setInputBoundingQuery(String query)\n{\r\n    if (query != null && query.length() > 0) {\r\n        conf.set(DBConfiguration.INPUT_BOUNDING_QUERY, query);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\db",
  "methodName" : "getInputBoundingQuery",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String getInputBoundingQuery()\n{\r\n    return conf.get(DBConfiguration.INPUT_BOUNDING_QUERY);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\db",
  "methodName" : "getInputClass",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Class<?> getInputClass()\n{\r\n    return conf.getClass(DBConfiguration.INPUT_CLASS_PROPERTY, NullDBWritable.class);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\db",
  "methodName" : "setInputClass",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setInputClass(Class<? extends DBWritable> inputClass)\n{\r\n    conf.setClass(DBConfiguration.INPUT_CLASS_PROPERTY, inputClass, DBWritable.class);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\db",
  "methodName" : "getOutputTableName",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String getOutputTableName()\n{\r\n    return conf.get(DBConfiguration.OUTPUT_TABLE_NAME_PROPERTY);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\db",
  "methodName" : "setOutputTableName",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setOutputTableName(String tableName)\n{\r\n    conf.set(DBConfiguration.OUTPUT_TABLE_NAME_PROPERTY, tableName);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\db",
  "methodName" : "getOutputFieldNames",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String[] getOutputFieldNames()\n{\r\n    return conf.getStrings(DBConfiguration.OUTPUT_FIELD_NAMES_PROPERTY);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\db",
  "methodName" : "setOutputFieldNames",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setOutputFieldNames(String... fieldNames)\n{\r\n    conf.setStrings(DBConfiguration.OUTPUT_FIELD_NAMES_PROPERTY, fieldNames);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\db",
  "methodName" : "setOutputFieldCount",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setOutputFieldCount(int fieldCount)\n{\r\n    conf.setInt(DBConfiguration.OUTPUT_FIELD_COUNT_PROPERTY, fieldCount);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\db",
  "methodName" : "getOutputFieldCount",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "int getOutputFieldCount()\n{\r\n    return conf.getInt(OUTPUT_FIELD_COUNT_PROPERTY, 0);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\db",
  "methodName" : "split",
  "errType" : null,
  "containingMethodsNum" : 21,
  "sourceCodeText" : "List<InputSplit> split(Configuration conf, ResultSet results, String colName) throws SQLException\n{\r\n    LOG.warn(\"Generating splits for a textual index column.\");\r\n    LOG.warn(\"If your database sorts in a case-insensitive order, \" + \"this may result in a partial import or duplicate records.\");\r\n    LOG.warn(\"You are strongly encouraged to choose an integral split column.\");\r\n    String minString = results.getString(1);\r\n    String maxString = results.getString(2);\r\n    boolean minIsNull = false;\r\n    if (null == minString) {\r\n        minString = \"\";\r\n        minIsNull = true;\r\n    }\r\n    if (null == maxString) {\r\n        List<InputSplit> splits = new ArrayList<InputSplit>();\r\n        splits.add(new DataDrivenDBInputFormat.DataDrivenDBInputSplit(colName + \" IS NULL\", colName + \" IS NULL\"));\r\n        return splits;\r\n    }\r\n    int numSplits = conf.getInt(MRJobConfig.NUM_MAPS, 1);\r\n    String lowClausePrefix = colName + \" >= '\";\r\n    String highClausePrefix = colName + \" < '\";\r\n    int maxPrefixLen = Math.min(minString.length(), maxString.length());\r\n    int sharedLen;\r\n    for (sharedLen = 0; sharedLen < maxPrefixLen; sharedLen++) {\r\n        char c1 = minString.charAt(sharedLen);\r\n        char c2 = maxString.charAt(sharedLen);\r\n        if (c1 != c2) {\r\n            break;\r\n        }\r\n    }\r\n    String commonPrefix = minString.substring(0, sharedLen);\r\n    minString = minString.substring(sharedLen);\r\n    maxString = maxString.substring(sharedLen);\r\n    List<String> splitStrings = split(numSplits, minString, maxString, commonPrefix);\r\n    List<InputSplit> splits = new ArrayList<InputSplit>();\r\n    String start = splitStrings.get(0);\r\n    for (int i = 1; i < splitStrings.size(); i++) {\r\n        String end = splitStrings.get(i);\r\n        if (i == splitStrings.size() - 1) {\r\n            splits.add(new DataDrivenDBInputFormat.DataDrivenDBInputSplit(lowClausePrefix + start + \"'\", colName + \" <= '\" + end + \"'\"));\r\n        } else {\r\n            splits.add(new DataDrivenDBInputFormat.DataDrivenDBInputSplit(lowClausePrefix + start + \"'\", highClausePrefix + end + \"'\"));\r\n        }\r\n    }\r\n    if (minIsNull) {\r\n        splits.add(new DataDrivenDBInputFormat.DataDrivenDBInputSplit(colName + \" IS NULL\", colName + \" IS NULL\"));\r\n    }\r\n    return splits;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\db",
  "methodName" : "split",
  "errType" : null,
  "containingMethodsNum" : 10,
  "sourceCodeText" : "List<String> split(int numSplits, String minString, String maxString, String commonPrefix) throws SQLException\n{\r\n    BigDecimal minVal = stringToBigDecimal(minString);\r\n    BigDecimal maxVal = stringToBigDecimal(maxString);\r\n    List<BigDecimal> splitPoints = split(new BigDecimal(numSplits), minVal, maxVal);\r\n    List<String> splitStrings = new ArrayList<String>();\r\n    for (BigDecimal bd : splitPoints) {\r\n        splitStrings.add(commonPrefix + bigDecimalToString(bd));\r\n    }\r\n    if (splitStrings.size() == 0 || !splitStrings.get(0).equals(commonPrefix + minString)) {\r\n        splitStrings.add(0, commonPrefix + minString);\r\n    }\r\n    if (splitStrings.size() == 1 || !splitStrings.get(splitStrings.size() - 1).equals(commonPrefix + maxString)) {\r\n        splitStrings.add(commonPrefix + maxString);\r\n    }\r\n    return splitStrings;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\db",
  "methodName" : "stringToBigDecimal",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "BigDecimal stringToBigDecimal(String str)\n{\r\n    BigDecimal result = BigDecimal.ZERO;\r\n    BigDecimal curPlace = ONE_PLACE;\r\n    int len = Math.min(str.length(), MAX_CHARS);\r\n    for (int i = 0; i < len; i++) {\r\n        int codePoint = str.codePointAt(i);\r\n        result = result.add(tryDivide(new BigDecimal(codePoint), curPlace));\r\n        curPlace = curPlace.multiply(ONE_PLACE);\r\n    }\r\n    return result;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\db",
  "methodName" : "bigDecimalToString",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "String bigDecimalToString(BigDecimal bd)\n{\r\n    BigDecimal cur = bd.stripTrailingZeros();\r\n    StringBuilder sb = new StringBuilder();\r\n    for (int numConverted = 0; numConverted < MAX_CHARS; numConverted++) {\r\n        cur = cur.multiply(ONE_PLACE);\r\n        int curCodePoint = cur.intValue();\r\n        if (0 == curCodePoint) {\r\n            break;\r\n        }\r\n        cur = cur.subtract(new BigDecimal(curCodePoint));\r\n        sb.append(Character.toChars(curCodePoint));\r\n    }\r\n    return sb.toString();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getTask",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Task getTask()\n{\r\n    return t;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "shouldDie",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean shouldDie()\n{\r\n    return shouldDie;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "write",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void write(DataOutput out) throws IOException\n{\r\n    out.writeBoolean(shouldDie);\r\n    if (t != null) {\r\n        out.writeBoolean(true);\r\n        out.writeBoolean(t.isMapTask());\r\n        t.write(out);\r\n    } else {\r\n        out.writeBoolean(false);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "readFields",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void readFields(DataInput in) throws IOException\n{\r\n    shouldDie = in.readBoolean();\r\n    boolean taskComing = in.readBoolean();\r\n    if (taskComing) {\r\n        boolean isMap = in.readBoolean();\r\n        if (isMap) {\r\n            t = new MapTask();\r\n        } else {\r\n            t = new ReduceTask();\r\n        }\r\n        t.readFields(in);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\partition",
  "methodName" : "setConf",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void setConf(Configuration conf)\n{\r\n    this.conf = conf;\r\n    keyFieldHelper = new KeyFieldHelper();\r\n    String keyFieldSeparator = conf.get(MRJobConfig.MAP_OUTPUT_KEY_FIELD_SEPARATOR, \"\\t\");\r\n    keyFieldHelper.setKeyFieldSeparator(keyFieldSeparator);\r\n    if (conf.get(\"num.key.fields.for.partition\") != null) {\r\n        LOG.warn(\"Using deprecated num.key.fields.for.partition. \" + \"Use mapreduce.partition.keypartitioner.options instead\");\r\n        this.numOfPartitionFields = conf.getInt(\"num.key.fields.for.partition\", 0);\r\n        keyFieldHelper.setKeyFieldSpec(1, numOfPartitionFields);\r\n    } else {\r\n        String option = conf.get(PARTITIONER_OPTIONS);\r\n        keyFieldHelper.parseOption(option);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\partition",
  "methodName" : "getConf",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Configuration getConf()\n{\r\n    return conf;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\partition",
  "methodName" : "getPartition",
  "errType" : [ "UnsupportedEncodingException" ],
  "containingMethodsNum" : 9,
  "sourceCodeText" : "int getPartition(K2 key, V2 value, int numReduceTasks)\n{\r\n    byte[] keyBytes;\r\n    List<KeyDescription> allKeySpecs = keyFieldHelper.keySpecs();\r\n    if (allKeySpecs.size() == 0) {\r\n        return getPartition(key.toString().hashCode(), numReduceTasks);\r\n    }\r\n    try {\r\n        keyBytes = key.toString().getBytes(\"UTF-8\");\r\n    } catch (UnsupportedEncodingException e) {\r\n        throw new RuntimeException(\"The current system does not \" + \"support UTF-8 encoding!\", e);\r\n    }\r\n    if (keyBytes.length == 0) {\r\n        return 0;\r\n    }\r\n    int[] lengthIndicesFirst = keyFieldHelper.getWordLengths(keyBytes, 0, keyBytes.length);\r\n    int currentHash = 0;\r\n    for (KeyDescription keySpec : allKeySpecs) {\r\n        int startChar = keyFieldHelper.getStartOffset(keyBytes, 0, keyBytes.length, lengthIndicesFirst, keySpec);\r\n        if (startChar < 0) {\r\n            continue;\r\n        }\r\n        int endChar = keyFieldHelper.getEndOffset(keyBytes, 0, keyBytes.length, lengthIndicesFirst, keySpec);\r\n        currentHash = hashCode(keyBytes, startChar, endChar, currentHash);\r\n    }\r\n    return getPartition(currentHash, numReduceTasks);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\partition",
  "methodName" : "hashCode",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int hashCode(byte[] b, int start, int end, int currentHash)\n{\r\n    for (int i = start; i <= end; i++) {\r\n        currentHash = 31 * currentHash + b[i];\r\n    }\r\n    return currentHash;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\partition",
  "methodName" : "getPartition",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int getPartition(int hash, int numReduceTasks)\n{\r\n    return (hash & Integer.MAX_VALUE) % numReduceTasks;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\partition",
  "methodName" : "setKeyFieldPartitionerOptions",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setKeyFieldPartitionerOptions(Job job, String keySpec)\n{\r\n    job.getConfiguration().set(PARTITIONER_OPTIONS, keySpec);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\partition",
  "methodName" : "getKeyFieldPartitionerOption",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String getKeyFieldPartitionerOption(JobContext job)\n{\r\n    return job.getConfiguration().get(PARTITIONER_OPTIONS);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getKeyClass",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Class getKeyClass()\n{\r\n    return in.getKeyClass();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getValueClass",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Class getValueClass()\n{\r\n    return in.getValueClass();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "createKey",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "K createKey()\n{\r\n    return (K) ReflectionUtils.newInstance(getKeyClass(), conf);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "createValue",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "V createValue()\n{\r\n    return (V) ReflectionUtils.newInstance(getValueClass(), conf);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "next",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "boolean next(K key, V value) throws IOException\n{\r\n    if (!more)\r\n        return false;\r\n    long pos = in.getPosition();\r\n    boolean remaining = (in.next(key) != null);\r\n    if (remaining) {\r\n        getCurrentValue(value);\r\n    }\r\n    if (pos >= end && in.syncSeen()) {\r\n        more = false;\r\n    } else {\r\n        more = remaining;\r\n    }\r\n    return more;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "next",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "boolean next(K key) throws IOException\n{\r\n    if (!more)\r\n        return false;\r\n    long pos = in.getPosition();\r\n    boolean remaining = (in.next(key) != null);\r\n    if (pos >= end && in.syncSeen()) {\r\n        more = false;\r\n    } else {\r\n        more = remaining;\r\n    }\r\n    return more;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getCurrentValue",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void getCurrentValue(V value) throws IOException\n{\r\n    in.getCurrentValue(value);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getProgress",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "float getProgress() throws IOException\n{\r\n    if (end == start) {\r\n        return 0.0f;\r\n    } else {\r\n        return Math.min(1.0f, (in.getPosition() - start) / (float) (end - start));\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getPos",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "long getPos() throws IOException\n{\r\n    return in.getPosition();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "seek",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void seek(long pos) throws IOException\n{\r\n    in.seek(pos);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "close",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void close() throws IOException\n{\r\n    in.close();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\aggregate",
  "methodName" : "addNextValue",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void addNextValue(Object val)\n{\r\n    long newVal = Long.parseLong(val.toString());\r\n    if (this.maxVal < newVal) {\r\n        this.maxVal = newVal;\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\aggregate",
  "methodName" : "addNextValue",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void addNextValue(long newVal)\n{\r\n    if (this.maxVal < newVal) {\r\n        this.maxVal = newVal;\r\n    }\r\n    ;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\aggregate",
  "methodName" : "getVal",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getVal()\n{\r\n    return this.maxVal;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\aggregate",
  "methodName" : "getReport",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getReport()\n{\r\n    return \"\" + maxVal;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\aggregate",
  "methodName" : "reset",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void reset()\n{\r\n    maxVal = Long.MIN_VALUE;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\aggregate",
  "methodName" : "getCombinerOutput",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "ArrayList<String> getCombinerOutput()\n{\r\n    ArrayList<String> retv = new ArrayList<String>(1);\r\n    ;\r\n    retv.add(\"\" + maxVal);\r\n    return retv;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\security",
  "methodName" : "getSecretKey",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "byte[] getSecretKey(Credentials credentials, Text alias)\n{\r\n    if (credentials == null)\r\n        return null;\r\n    return credentials.getSecretKey(alias);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\security",
  "methodName" : "obtainTokensForNamenodes",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void obtainTokensForNamenodes(Credentials credentials, Path[] ps, Configuration conf) throws IOException\n{\r\n    if (!UserGroupInformation.isSecurityEnabled()) {\r\n        return;\r\n    }\r\n    obtainTokensForNamenodesInternal(credentials, ps, conf);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\security",
  "methodName" : "cleanUpTokenReferral",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void cleanUpTokenReferral(Configuration conf)\n{\r\n    conf.unset(MRJobConfig.MAPREDUCE_JOB_CREDENTIALS_BINARY);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\security",
  "methodName" : "obtainTokensForNamenodesInternal",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void obtainTokensForNamenodesInternal(Credentials credentials, Path[] ps, Configuration conf) throws IOException\n{\r\n    Set<FileSystem> fsSet = new HashSet<FileSystem>();\r\n    for (Path p : ps) {\r\n        fsSet.add(p.getFileSystem(conf));\r\n    }\r\n    String masterPrincipal = Master.getMasterPrincipal(conf);\r\n    for (FileSystem fs : fsSet) {\r\n        obtainTokensForNamenodesInternal(fs, credentials, conf, masterPrincipal);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\security",
  "methodName" : "isTokenRenewalExcluded",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "boolean isTokenRenewalExcluded(FileSystem fs, Configuration conf)\n{\r\n    String[] nns = conf.getStrings(MRJobConfig.JOB_NAMENODES_TOKEN_RENEWAL_EXCLUDE);\r\n    if (nns != null) {\r\n        String host = fs.getUri().getHost();\r\n        for (int i = 0; i < nns.length; i++) {\r\n            if (nns[i].equals(host)) {\r\n                return true;\r\n            }\r\n        }\r\n    }\r\n    return false;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\security",
  "methodName" : "obtainTokensForNamenodesInternal",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void obtainTokensForNamenodesInternal(FileSystem fs, Credentials credentials, Configuration conf, String renewer) throws IOException\n{\r\n    String delegTokenRenewer = \"\";\r\n    if (!isTokenRenewalExcluded(fs, conf)) {\r\n        if (StringUtils.isEmpty(renewer)) {\r\n            throw new IOException(\"Can't get Master Kerberos principal for use as renewer\");\r\n        } else {\r\n            delegTokenRenewer = renewer;\r\n        }\r\n    }\r\n    mergeBinaryTokens(credentials, conf);\r\n    final Token<?>[] tokens = fs.addDelegationTokens(delegTokenRenewer, credentials);\r\n    if (tokens != null) {\r\n        for (Token<?> token : tokens) {\r\n            LOG.info(\"Got dt for \" + fs.getUri() + \"; \" + token);\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\security",
  "methodName" : "mergeBinaryTokens",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void mergeBinaryTokens(Credentials creds, Configuration conf)\n{\r\n    String binaryTokenFilename = conf.get(MRJobConfig.MAPREDUCE_JOB_CREDENTIALS_BINARY);\r\n    if (binaryTokenFilename != null) {\r\n        Credentials binary;\r\n        try {\r\n            binary = Credentials.readTokenStorageFile(FileSystem.getLocal(conf).makeQualified(new Path(binaryTokenFilename)), conf);\r\n        } catch (IOException e) {\r\n            throw new RuntimeException(e);\r\n        }\r\n        creds.mergeAll(binary);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\security",
  "methodName" : "loadTokens",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "Credentials loadTokens(String jobTokenFile, JobConf conf) throws IOException\n{\r\n    Path localJobTokenFile = new Path(\"file:///\" + jobTokenFile);\r\n    Credentials ts = Credentials.readTokenStorageFile(localJobTokenFile, conf);\r\n    if (LOG.isDebugEnabled()) {\r\n        LOG.debug(\"Task: Loaded jobTokenFile from: \" + localJobTokenFile.toUri().getPath() + \"; num of sec keys  = \" + ts.numberOfSecretKeys() + \" Number of tokens \" + ts.numberOfTokens());\r\n    }\r\n    return ts;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\security",
  "methodName" : "loadTokens",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Credentials loadTokens(String jobTokenFile, Configuration conf) throws IOException\n{\r\n    return loadTokens(jobTokenFile, new JobConf(conf));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\security",
  "methodName" : "setJobToken",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setJobToken(Token<? extends TokenIdentifier> t, Credentials credentials)\n{\r\n    credentials.addToken(JOB_TOKEN, t);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\security",
  "methodName" : "getJobToken",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Token<JobTokenIdentifier> getJobToken(Credentials credentials)\n{\r\n    return (Token<JobTokenIdentifier>) credentials.getToken(JOB_TOKEN);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\security",
  "methodName" : "setShuffleSecretKey",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setShuffleSecretKey(byte[] key, Credentials credentials)\n{\r\n    credentials.addSecretKey(SHUFFLE_TOKEN, key);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\security",
  "methodName" : "getShuffleSecretKey",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "byte[] getShuffleSecretKey(Credentials credentials)\n{\r\n    return getSecretKey(credentials, SHUFFLE_TOKEN);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\security",
  "methodName" : "setEncryptedSpillKey",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setEncryptedSpillKey(byte[] key, Credentials credentials)\n{\r\n    credentials.addSecretKey(ENC_SPILL_KEY, key);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\security",
  "methodName" : "getEncryptedSpillKey",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "byte[] getEncryptedSpillKey(Credentials credentials)\n{\r\n    return getSecretKey(credentials, ENC_SPILL_KEY);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\security",
  "methodName" : "getDelegationToken",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Token<?> getDelegationToken(Credentials credentials, String namenode)\n{\r\n    return (Token<?>) credentials.getToken(new Text(namenode));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred\\lib",
  "methodName" : "specToString",
  "errType" : null,
  "containingMethodsNum" : 18,
  "sourceCodeText" : "String specToString()\n{\r\n    StringBuffer sb = new StringBuffer();\r\n    sb.append(\"fieldSeparator: \").append(fieldSeparator).append(\"\\n\");\r\n    sb.append(\"mapOutputKeyValueSpec: \").append(mapOutputKeyValueSpec).append(\"\\n\");\r\n    sb.append(\"reduceOutputKeyValueSpec: \").append(reduceOutputKeyValueSpec).append(\"\\n\");\r\n    sb.append(\"allMapValueFieldsFrom: \").append(allMapValueFieldsFrom).append(\"\\n\");\r\n    sb.append(\"allReduceValueFieldsFrom: \").append(allReduceValueFieldsFrom).append(\"\\n\");\r\n    int i = 0;\r\n    sb.append(\"mapOutputKeyFieldList.length: \").append(mapOutputKeyFieldList.size()).append(\"\\n\");\r\n    for (i = 0; i < mapOutputKeyFieldList.size(); i++) {\r\n        sb.append(\"\\t\").append(mapOutputKeyFieldList.get(i)).append(\"\\n\");\r\n    }\r\n    sb.append(\"mapOutputValueFieldList.length: \").append(mapOutputValueFieldList.size()).append(\"\\n\");\r\n    for (i = 0; i < mapOutputValueFieldList.size(); i++) {\r\n        sb.append(\"\\t\").append(mapOutputValueFieldList.get(i)).append(\"\\n\");\r\n    }\r\n    sb.append(\"reduceOutputKeyFieldList.length: \").append(reduceOutputKeyFieldList.size()).append(\"\\n\");\r\n    for (i = 0; i < reduceOutputKeyFieldList.size(); i++) {\r\n        sb.append(\"\\t\").append(reduceOutputKeyFieldList.get(i)).append(\"\\n\");\r\n    }\r\n    sb.append(\"reduceOutputValueFieldList.length: \").append(reduceOutputValueFieldList.size()).append(\"\\n\");\r\n    for (i = 0; i < reduceOutputValueFieldList.size(); i++) {\r\n        sb.append(\"\\t\").append(reduceOutputValueFieldList.get(i)).append(\"\\n\");\r\n    }\r\n    return sb.toString();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred\\lib",
  "methodName" : "map",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void map(K key, V val, OutputCollector<Text, Text> output, Reporter reporter) throws IOException\n{\r\n    FieldSelectionHelper helper = new FieldSelectionHelper(FieldSelectionHelper.emptyText, FieldSelectionHelper.emptyText);\r\n    helper.extractOutputKeyValue(key.toString(), val.toString(), fieldSeparator, mapOutputKeyFieldList, mapOutputValueFieldList, allMapValueFieldsFrom, ignoreInputKey, true);\r\n    output.collect(helper.getKey(), helper.getValue());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred\\lib",
  "methodName" : "parseOutputKeyValueSpec",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void parseOutputKeyValueSpec()\n{\r\n    allMapValueFieldsFrom = FieldSelectionHelper.parseOutputKeyValueSpec(mapOutputKeyValueSpec, mapOutputKeyFieldList, mapOutputValueFieldList);\r\n    allReduceValueFieldsFrom = FieldSelectionHelper.parseOutputKeyValueSpec(reduceOutputKeyValueSpec, reduceOutputKeyFieldList, reduceOutputValueFieldList);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred\\lib",
  "methodName" : "configure",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void configure(JobConf job)\n{\r\n    this.fieldSeparator = job.get(FieldSelectionHelper.DATA_FIELD_SEPARATOR, \"\\t\");\r\n    this.mapOutputKeyValueSpec = job.get(FieldSelectionHelper.MAP_OUTPUT_KEY_VALUE_SPEC, \"0-:\");\r\n    this.ignoreInputKey = TextInputFormat.class.getCanonicalName().equals(job.getInputFormat().getClass().getCanonicalName());\r\n    this.reduceOutputKeyValueSpec = job.get(FieldSelectionHelper.REDUCE_OUTPUT_KEY_VALUE_SPEC, \"0-:\");\r\n    parseOutputKeyValueSpec();\r\n    LOG.info(specToString());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred\\lib",
  "methodName" : "close",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void close() throws IOException\n{\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred\\lib",
  "methodName" : "reduce",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void reduce(Text key, Iterator<Text> values, OutputCollector<Text, Text> output, Reporter reporter) throws IOException\n{\r\n    String keyStr = key.toString() + this.fieldSeparator;\r\n    while (values.hasNext()) {\r\n        FieldSelectionHelper helper = new FieldSelectionHelper();\r\n        helper.extractOutputKeyValue(keyStr, values.next().toString(), fieldSeparator, reduceOutputKeyFieldList, reduceOutputValueFieldList, allReduceValueFieldsFrom, false, false);\r\n        output.collect(helper.getKey(), helper.getValue());\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred\\lib",
  "methodName" : "reduce",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void reduce(K key, Iterator<V> values, OutputCollector<K, V> output, Reporter reporter) throws IOException\n{\r\n    while (values.hasNext()) {\r\n        output.collect(key, values.next());\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getSplits",
  "errType" : null,
  "containingMethodsNum" : 9,
  "sourceCodeText" : "InputSplit[] getSplits(JobConf job, int numSplits) throws IOException\n{\r\n    Path[] paths = FileUtil.stat2Paths(listStatus(job));\r\n    List<MultiFileSplit> splits = new ArrayList<MultiFileSplit>(Math.min(numSplits, paths.length));\r\n    if (paths.length != 0) {\r\n        long[] lengths = new long[paths.length];\r\n        long totLength = 0;\r\n        for (int i = 0; i < paths.length; i++) {\r\n            FileSystem fs = paths[i].getFileSystem(job);\r\n            lengths[i] = fs.getContentSummary(paths[i]).getLength();\r\n            totLength += lengths[i];\r\n        }\r\n        double avgLengthPerSplit = ((double) totLength) / numSplits;\r\n        long cumulativeLength = 0;\r\n        int startIndex = 0;\r\n        for (int i = 0; i < numSplits; i++) {\r\n            int splitSize = findSize(i, avgLengthPerSplit, cumulativeLength, startIndex, lengths);\r\n            if (splitSize != 0) {\r\n                Path[] splitPaths = new Path[splitSize];\r\n                long[] splitLengths = new long[splitSize];\r\n                System.arraycopy(paths, startIndex, splitPaths, 0, splitSize);\r\n                System.arraycopy(lengths, startIndex, splitLengths, 0, splitSize);\r\n                splits.add(new MultiFileSplit(job, splitPaths, splitLengths));\r\n                startIndex += splitSize;\r\n                for (long l : splitLengths) {\r\n                    cumulativeLength += l;\r\n                }\r\n            }\r\n        }\r\n    }\r\n    return splits.toArray(new MultiFileSplit[splits.size()]);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "findSize",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int findSize(int splitIndex, double avgLengthPerSplit, long cumulativeLength, int startIndex, long[] lengths)\n{\r\n    if (splitIndex == lengths.length - 1)\r\n        return lengths.length - startIndex;\r\n    long goalLength = (long) ((splitIndex + 1) * avgLengthPerSplit);\r\n    long partialLength = 0;\r\n    for (int i = startIndex; i < lengths.length; i++) {\r\n        partialLength += lengths[i];\r\n        if (partialLength + cumulativeLength >= goalLength) {\r\n            return i - startIndex + 1;\r\n        }\r\n    }\r\n    return lengths.length - startIndex;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getRecordReader",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "RecordReader<K, V> getRecordReader(InputSplit split, JobConf job, Reporter reporter) throws IOException",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest\\stages",
  "methodName" : "executeStage",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "Path executeStage(final Boolean suppressExceptions) throws IOException\n{\r\n    final Path dir = getTaskAttemptDir();\r\n    if (dir != null) {\r\n        LOG.info(\"{}: Deleting task attempt directory {}\", getName(), dir);\r\n        deleteDir(dir, suppressExceptions);\r\n    }\r\n    return dir;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred\\lib",
  "methodName" : "addInputPath",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void addInputPath(JobConf conf, Path path, Class<? extends InputFormat> inputFormatClass)\n{\r\n    String inputFormatMapping = path.toString() + \";\" + inputFormatClass.getName();\r\n    String inputFormats = conf.get(\"mapreduce.input.multipleinputs.dir.formats\");\r\n    conf.set(\"mapreduce.input.multipleinputs.dir.formats\", inputFormats == null ? inputFormatMapping : inputFormats + \",\" + inputFormatMapping);\r\n    conf.setInputFormat(DelegatingInputFormat.class);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred\\lib",
  "methodName" : "addInputPath",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void addInputPath(JobConf conf, Path path, Class<? extends InputFormat> inputFormatClass, Class<? extends Mapper> mapperClass)\n{\r\n    addInputPath(conf, path, inputFormatClass);\r\n    String mapperMapping = path.toString() + \";\" + mapperClass.getName();\r\n    String mappers = conf.get(\"mapreduce.input.multipleinputs.dir.mappers\");\r\n    conf.set(\"mapreduce.input.multipleinputs.dir.mappers\", mappers == null ? mapperMapping : mappers + \",\" + mapperMapping);\r\n    conf.setMapperClass(DelegatingMapper.class);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred\\lib",
  "methodName" : "getInputFormatMap",
  "errType" : [ "ClassNotFoundException" ],
  "containingMethodsNum" : 4,
  "sourceCodeText" : "Map<Path, InputFormat> getInputFormatMap(JobConf conf)\n{\r\n    Map<Path, InputFormat> m = new HashMap<Path, InputFormat>();\r\n    String[] pathMappings = conf.get(\"mapreduce.input.multipleinputs.dir.formats\").split(\",\");\r\n    for (String pathMapping : pathMappings) {\r\n        String[] split = pathMapping.split(\";\");\r\n        InputFormat inputFormat;\r\n        try {\r\n            inputFormat = (InputFormat) ReflectionUtils.newInstance(conf.getClassByName(split[1]), conf);\r\n        } catch (ClassNotFoundException e) {\r\n            throw new RuntimeException(e);\r\n        }\r\n        m.put(new Path(split[0]), inputFormat);\r\n    }\r\n    return m;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred\\lib",
  "methodName" : "getMapperTypeMap",
  "errType" : [ "ClassNotFoundException" ],
  "containingMethodsNum" : 6,
  "sourceCodeText" : "Map<Path, Class<? extends Mapper>> getMapperTypeMap(JobConf conf)\n{\r\n    if (conf.get(\"mapreduce.input.multipleinputs.dir.mappers\") == null) {\r\n        return Collections.emptyMap();\r\n    }\r\n    Map<Path, Class<? extends Mapper>> m = new HashMap<Path, Class<? extends Mapper>>();\r\n    String[] pathMappings = conf.get(\"mapreduce.input.multipleinputs.dir.mappers\").split(\",\");\r\n    for (String pathMapping : pathMappings) {\r\n        String[] split = pathMapping.split(\";\");\r\n        Class<? extends Mapper> mapClass;\r\n        try {\r\n            mapClass = (Class<? extends Mapper>) conf.getClassByName(split[1]);\r\n        } catch (ClassNotFoundException e) {\r\n            throw new RuntimeException(e);\r\n        }\r\n        m.put(new Path(split[0]), mapClass);\r\n    }\r\n    return m;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\aggregate",
  "methodName" : "generateEntry",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Entry<Text, Text> generateEntry(String type, String id, Text val)\n{\r\n    Text key = new Text(type + TYPE_SEPARATOR + id);\r\n    return new MyEntry(key, val);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\aggregate",
  "methodName" : "generateValueAggregator",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "ValueAggregator generateValueAggregator(String type, long uniqCount)\n{\r\n    if (type.compareToIgnoreCase(LONG_VALUE_SUM) == 0) {\r\n        return new LongValueSum();\r\n    }\r\n    if (type.compareToIgnoreCase(LONG_VALUE_MAX) == 0) {\r\n        return new LongValueMax();\r\n    } else if (type.compareToIgnoreCase(LONG_VALUE_MIN) == 0) {\r\n        return new LongValueMin();\r\n    } else if (type.compareToIgnoreCase(STRING_VALUE_MAX) == 0) {\r\n        return new StringValueMax();\r\n    } else if (type.compareToIgnoreCase(STRING_VALUE_MIN) == 0) {\r\n        return new StringValueMin();\r\n    } else if (type.compareToIgnoreCase(DOUBLE_VALUE_SUM) == 0) {\r\n        return new DoubleValueSum();\r\n    } else if (type.compareToIgnoreCase(UNIQ_VALUE_COUNT) == 0) {\r\n        return new UniqValueCount(uniqCount);\r\n    } else if (type.compareToIgnoreCase(VALUE_HISTOGRAM) == 0) {\r\n        return new ValueHistogram();\r\n    }\r\n    return null;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\aggregate",
  "methodName" : "generateKeyValPairs",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "ArrayList<Entry<Text, Text>> generateKeyValPairs(Object key, Object val)\n{\r\n    ArrayList<Entry<Text, Text>> retv = new ArrayList<Entry<Text, Text>>();\r\n    String countType = LONG_VALUE_SUM;\r\n    String id = \"record_count\";\r\n    Entry<Text, Text> e = generateEntry(countType, id, ONE);\r\n    if (e != null) {\r\n        retv.add(e);\r\n    }\r\n    if (this.inputFile != null) {\r\n        e = generateEntry(countType, this.inputFile, ONE);\r\n        if (e != null) {\r\n            retv.add(e);\r\n        }\r\n    }\r\n    return retv;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\aggregate",
  "methodName" : "configure",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void configure(Configuration conf)\n{\r\n    this.inputFile = conf.get(MRJobConfig.MAP_INPUT_FILE);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred\\pipes",
  "methodName" : "getSecurityChallenge",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "String getSecurityChallenge()\n{\r\n    Random rand = new Random(System.currentTimeMillis());\r\n    StringBuilder strBuilder = new StringBuilder();\r\n    strBuilder.append(rand.nextInt(0x7fffffff));\r\n    strBuilder.append(rand.nextInt(0x7fffffff));\r\n    strBuilder.append(rand.nextInt(0x7fffffff));\r\n    strBuilder.append(rand.nextInt(0x7fffffff));\r\n    return strBuilder.toString();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred\\pipes",
  "methodName" : "writePasswordToLocalFile",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void writePasswordToLocalFile(String localPasswordFile, byte[] password, JobConf conf) throws IOException\n{\r\n    FileSystem localFs = FileSystem.getLocal(conf);\r\n    Path localPath = new Path(localPasswordFile);\r\n    FSDataOutputStream out = FileSystem.create(localFs, localPath, new FsPermission(\"400\"));\r\n    out.write(password);\r\n    out.close();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred\\pipes",
  "methodName" : "getDownlink",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "DownwardProtocol<K1, V1> getDownlink()\n{\r\n    return downlink;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred\\pipes",
  "methodName" : "waitForAuthentication",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void waitForAuthentication() throws IOException, InterruptedException\n{\r\n    downlink.flush();\r\n    LOG.debug(\"Waiting for authentication response\");\r\n    handler.waitForAuthentication();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred\\pipes",
  "methodName" : "waitForFinish",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "boolean waitForFinish() throws Throwable\n{\r\n    downlink.flush();\r\n    return handler.waitForFinish();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred\\pipes",
  "methodName" : "abort",
  "errType" : [ "IOException", "Throwable" ],
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void abort(Throwable t) throws IOException\n{\r\n    LOG.info(\"Aborting because of \" + StringUtils.stringifyException(t));\r\n    try {\r\n        downlink.abort();\r\n        downlink.flush();\r\n    } catch (IOException e) {\r\n    }\r\n    try {\r\n        handler.waitForFinish();\r\n    } catch (Throwable ignored) {\r\n        process.destroy();\r\n    }\r\n    IOException wrapper = new IOException(\"pipe child exception\");\r\n    wrapper.initCause(t);\r\n    throw wrapper;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 2,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred\\pipes",
  "methodName" : "cleanup",
  "errType" : [ "InterruptedException" ],
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void cleanup() throws IOException\n{\r\n    serverSocket.close();\r\n    try {\r\n        downlink.close();\r\n        socketCleaner.interrupt();\r\n    } catch (InterruptedException ie) {\r\n        Thread.currentThread().interrupt();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred\\pipes",
  "methodName" : "runClient",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "Process runClient(List<String> command, Map<String, String> env) throws IOException\n{\r\n    ProcessBuilder builder = new ProcessBuilder(command);\r\n    if (env != null) {\r\n        builder.environment().putAll(env);\r\n    }\r\n    Process result = builder.start();\r\n    return result;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred\\pipes",
  "methodName" : "createDigest",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "String createDigest(byte[] password, String data) throws IOException\n{\r\n    SecretKey key = JobTokenSecretManager.createSecretKey(password);\r\n    return SecureShuffleUtils.hashFromString(data, key);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "getDatum",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Object getDatum()\n{\r\n    return datum;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "setDatum",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setDatum(Object datum)\n{\r\n    this.datum = (AMStarted) datum;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "getAppAttemptId",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "ApplicationAttemptId getAppAttemptId()\n{\r\n    return ApplicationAttemptId.fromString(datum.getApplicationAttemptId().toString());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "getStartTime",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "long getStartTime()\n{\r\n    return datum.getStartTime();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "getContainerId",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "ContainerId getContainerId()\n{\r\n    return ContainerId.fromString(datum.getContainerId().toString());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "getNodeManagerHost",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String getNodeManagerHost()\n{\r\n    return datum.getNodeManagerHost().toString();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "getNodeManagerPort",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "int getNodeManagerPort()\n{\r\n    return datum.getNodeManagerPort();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "getNodeManagerHttpPort",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "int getNodeManagerHttpPort()\n{\r\n    return datum.getNodeManagerHttpPort();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "getForcedJobStateOnShutDown",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getForcedJobStateOnShutDown()\n{\r\n    return this.forcedJobStateOnShutDown;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "getSubmitTime",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getSubmitTime()\n{\r\n    return this.submitTime;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "getEventType",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "EventType getEventType()\n{\r\n    return EventType.AM_STARTED;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "toTimelineEvent",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "TimelineEvent toTimelineEvent()\n{\r\n    TimelineEvent tEvent = new TimelineEvent();\r\n    tEvent.setId(StringUtils.toUpperCase(getEventType().name()));\r\n    tEvent.addInfo(\"APPLICATION_ATTEMPT_ID\", getAppAttemptId() == null ? \"\" : getAppAttemptId().toString());\r\n    tEvent.addInfo(\"CONTAINER_ID\", getContainerId() == null ? \"\" : getContainerId().toString());\r\n    tEvent.addInfo(\"NODE_MANAGER_HOST\", getNodeManagerHost());\r\n    tEvent.addInfo(\"NODE_MANAGER_PORT\", getNodeManagerPort());\r\n    tEvent.addInfo(\"NODE_MANAGER_HTTP_PORT\", getNodeManagerHttpPort());\r\n    tEvent.addInfo(\"START_TIME\", getStartTime());\r\n    return tEvent;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "getTimelineMetrics",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Set<TimelineMetric> getTimelineMetrics()\n{\r\n    return null;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\security\\token\\delegation",
  "methodName" : "createIdentifier",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "DelegationTokenIdentifier createIdentifier()\n{\r\n    return new DelegationTokenIdentifier();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\counters",
  "methodName" : "getName",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getName()\n{\r\n    return name;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\counters",
  "methodName" : "getDisplayName",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getDisplayName()\n{\r\n    return displayName;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\counters",
  "methodName" : "setDisplayName",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setDisplayName(String displayName)\n{\r\n    this.displayName = displayName;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\counters",
  "methodName" : "addCounter",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void addCounter(T counter)\n{\r\n    counters.put(counter.getName(), counter);\r\n    limits.incrCounters();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\counters",
  "methodName" : "addCounter",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "T addCounter(String counterName, String displayName, long value)\n{\r\n    String saveName = Limits.filterCounterName(counterName);\r\n    T counter = findCounterImpl(saveName, false);\r\n    if (counter == null) {\r\n        return addCounterImpl(saveName, displayName, value);\r\n    }\r\n    counter.setValue(value);\r\n    return counter;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\counters",
  "methodName" : "addCounterImpl",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "T addCounterImpl(String name, String displayName, long value)\n{\r\n    T counter = newCounter(name, displayName, value);\r\n    addCounter(counter);\r\n    return counter;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\counters",
  "methodName" : "findCounter",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "T findCounter(String counterName, String displayName)\n{\r\n    String saveName = Limits.filterCounterName(counterName);\r\n    T counter = findCounterImpl(saveName, false);\r\n    if (counter == null) {\r\n        return addCounterImpl(saveName, displayName, 0);\r\n    }\r\n    return counter;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\counters",
  "methodName" : "findCounter",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "T findCounter(String counterName, boolean create)\n{\r\n    return findCounterImpl(Limits.filterCounterName(counterName), create);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\counters",
  "methodName" : "findCounterImpl",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "T findCounterImpl(String counterName, boolean create)\n{\r\n    T counter = counters.get(counterName);\r\n    if (counter == null && create) {\r\n        String localized = ResourceBundles.getCounterName(getName(), counterName, counterName);\r\n        return addCounterImpl(counterName, localized, 0);\r\n    }\r\n    return counter;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\counters",
  "methodName" : "findCounter",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "T findCounter(String counterName)\n{\r\n    return findCounter(counterName, true);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\counters",
  "methodName" : "newCounter",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "T newCounter(String counterName, String displayName, long value)",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\counters",
  "methodName" : "newCounter",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "T newCounter()",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\counters",
  "methodName" : "iterator",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Iterator<T> iterator()\n{\r\n    return counters.values().iterator();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\counters",
  "methodName" : "write",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void write(DataOutput out) throws IOException\n{\r\n    Text.writeString(out, displayName);\r\n    WritableUtils.writeVInt(out, counters.size());\r\n    for (Counter counter : counters.values()) {\r\n        counter.write(out);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\counters",
  "methodName" : "readFields",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void readFields(DataInput in) throws IOException\n{\r\n    displayName = StringInterner.weakIntern(Text.readString(in));\r\n    counters.clear();\r\n    int size = WritableUtils.readVInt(in);\r\n    for (int i = 0; i < size; i++) {\r\n        T counter = newCounter();\r\n        counter.readFields(in);\r\n        counters.put(counter.getName(), counter);\r\n        limits.incrCounters();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\counters",
  "methodName" : "size",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "int size()\n{\r\n    return counters.size();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\counters",
  "methodName" : "equals",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "boolean equals(Object genericRight)\n{\r\n    if (genericRight instanceof CounterGroupBase<?>) {\r\n        @SuppressWarnings(\"unchecked\")\r\n        CounterGroupBase<T> right = (CounterGroupBase<T>) genericRight;\r\n        return Iterators.elementsEqual(iterator(), right.iterator());\r\n    }\r\n    return false;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\counters",
  "methodName" : "hashCode",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "int hashCode()\n{\r\n    return counters.hashCode();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\counters",
  "methodName" : "incrAllCounters",
  "errType" : [ "LimitExceededException" ],
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void incrAllCounters(CounterGroupBase<T> rightGroup)\n{\r\n    try {\r\n        for (Counter right : rightGroup) {\r\n            Counter left = findCounter(right.getName(), right.getDisplayName());\r\n            left.increment(right.getValue());\r\n        }\r\n    } catch (LimitExceededException e) {\r\n        counters.clear();\r\n        throw e;\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "getEventId",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int getEventId()\n{\r\n    return eventId;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "getTaskAttemptId",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "TaskAttemptID getTaskAttemptId()\n{\r\n    return taskId;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "getStatus",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Status getStatus()\n{\r\n    return status;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "getTaskTrackerHttp",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getTaskTrackerHttp()\n{\r\n    return taskTrackerHttp;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "getTaskRunTime",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int getTaskRunTime()\n{\r\n    return taskRunTime;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "setTaskRunTime",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setTaskRunTime(int taskCompletionTime)\n{\r\n    this.taskRunTime = taskCompletionTime;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "setEventId",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setEventId(int eventId)\n{\r\n    this.eventId = eventId;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "setTaskAttemptId",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setTaskAttemptId(TaskAttemptID taskId)\n{\r\n    this.taskId = taskId;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "setTaskStatus",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setTaskStatus(Status status)\n{\r\n    this.status = status;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "setTaskTrackerHttp",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setTaskTrackerHttp(String taskTrackerHttp)\n{\r\n    this.taskTrackerHttp = taskTrackerHttp;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "toString",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "String toString()\n{\r\n    StringBuffer buf = new StringBuffer();\r\n    buf.append(\"Task Id : \");\r\n    buf.append(taskId);\r\n    buf.append(\", Status : \");\r\n    buf.append(status.name());\r\n    return buf.toString();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "equals",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "boolean equals(Object o)\n{\r\n    if (o == null)\r\n        return false;\r\n    if (o.getClass().equals(this.getClass())) {\r\n        TaskCompletionEvent event = (TaskCompletionEvent) o;\r\n        return this.isMap == event.isMapTask() && this.eventId == event.getEventId() && this.idWithinJob == event.idWithinJob() && this.status.equals(event.getStatus()) && this.taskId.equals(event.getTaskAttemptId()) && this.taskRunTime == event.getTaskRunTime() && this.taskTrackerHttp.equals(event.getTaskTrackerHttp());\r\n    }\r\n    return false;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "hashCode",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "int hashCode()\n{\r\n    return toString().hashCode();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "isMapTask",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean isMapTask()\n{\r\n    return isMap;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "idWithinJob",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int idWithinJob()\n{\r\n    return idWithinJob;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "write",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void write(DataOutput out) throws IOException\n{\r\n    taskId.write(out);\r\n    WritableUtils.writeVInt(out, idWithinJob);\r\n    out.writeBoolean(isMap);\r\n    WritableUtils.writeEnum(out, status);\r\n    WritableUtils.writeString(out, taskTrackerHttp);\r\n    WritableUtils.writeVInt(out, taskRunTime);\r\n    WritableUtils.writeVInt(out, eventId);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "readFields",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void readFields(DataInput in) throws IOException\n{\r\n    taskId.readFields(in);\r\n    idWithinJob = WritableUtils.readVInt(in);\r\n    isMap = in.readBoolean();\r\n    status = WritableUtils.readEnum(in, Status.class);\r\n    taskTrackerHttp = WritableUtils.readString(in);\r\n    taskRunTime = WritableUtils.readVInt(in);\r\n    eventId = WritableUtils.readVInt(in);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred\\pipes",
  "methodName" : "configure",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void configure(JobConf conf)\n{\r\n    part = ReflectionUtils.newInstance(Submitter.getJavaPartitioner(conf), conf);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred\\pipes",
  "methodName" : "setNextPartition",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setNextPartition(int newValue)\n{\r\n    CACHE.set(newValue);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred\\pipes",
  "methodName" : "getPartition",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "int getPartition(K key, V value, int numPartitions)\n{\r\n    Integer result = CACHE.get();\r\n    if (result == null) {\r\n        return part.getPartition(key, value, numPartitions);\r\n    } else {\r\n        return result;\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "shouldReset",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean shouldReset()\n{\r\n    return reset;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getMapTaskCompletionEvents",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "TaskCompletionEvent[] getMapTaskCompletionEvents()\n{\r\n    return events;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "write",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void write(DataOutput out) throws IOException\n{\r\n    out.writeBoolean(reset);\r\n    out.writeInt(events.length);\r\n    for (TaskCompletionEvent event : events) {\r\n        event.write(out);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "readFields",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void readFields(DataInput in) throws IOException\n{\r\n    reset = in.readBoolean();\r\n    events = new TaskCompletionEvent[in.readInt()];\r\n    for (int i = 0; i < events.length; ++i) {\r\n        events[i] = new TaskCompletionEvent();\r\n        events[i].readFields(in);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "clone",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Object clone()\n{\r\n    ReduceTaskStatus myClone = (ReduceTaskStatus) super.clone();\r\n    myClone.failedFetchTasks = new ArrayList<TaskAttemptID>(failedFetchTasks);\r\n    return myClone;\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getIsMap",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean getIsMap()\n{\r\n    return false;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "setFinishTime",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setFinishTime(long finishTime)\n{\r\n    if (shuffleFinishTime == 0) {\r\n        this.shuffleFinishTime = finishTime;\r\n    }\r\n    if (sortFinishTime == 0) {\r\n        this.sortFinishTime = finishTime;\r\n    }\r\n    super.setFinishTime(finishTime);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getShuffleFinishTime",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getShuffleFinishTime()\n{\r\n    return shuffleFinishTime;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "setShuffleFinishTime",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setShuffleFinishTime(long shuffleFinishTime)\n{\r\n    this.shuffleFinishTime = shuffleFinishTime;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getSortFinishTime",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getSortFinishTime()\n{\r\n    return sortFinishTime;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "setSortFinishTime",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setSortFinishTime(long sortFinishTime)\n{\r\n    this.sortFinishTime = sortFinishTime;\r\n    if (0 == this.shuffleFinishTime) {\r\n        this.shuffleFinishTime = sortFinishTime;\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getMapFinishTime",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getMapFinishTime()\n{\r\n    throw new UnsupportedOperationException(\"getMapFinishTime() not supported for ReduceTask\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "setMapFinishTime",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setMapFinishTime(long shuffleFinishTime)\n{\r\n    throw new UnsupportedOperationException(\"setMapFinishTime() not supported for ReduceTask\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getFetchFailedMaps",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "List<TaskAttemptID> getFetchFailedMaps()\n{\r\n    return failedFetchTasks;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "addFetchFailedMap",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void addFetchFailedMap(TaskAttemptID mapTaskId)\n{\r\n    failedFetchTasks.add(mapTaskId);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "statusUpdate",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void statusUpdate(TaskStatus status)\n{\r\n    super.statusUpdate(status);\r\n    if (status.getShuffleFinishTime() != 0) {\r\n        this.shuffleFinishTime = status.getShuffleFinishTime();\r\n    }\r\n    if (status.getSortFinishTime() != 0) {\r\n        sortFinishTime = status.getSortFinishTime();\r\n    }\r\n    List<TaskAttemptID> newFetchFailedMaps = status.getFetchFailedMaps();\r\n    if (failedFetchTasks == null) {\r\n        failedFetchTasks = newFetchFailedMaps;\r\n    } else if (newFetchFailedMaps != null) {\r\n        failedFetchTasks.addAll(newFetchFailedMaps);\r\n    }\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "clearStatus",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void clearStatus()\n{\r\n    super.clearStatus();\r\n    failedFetchTasks.clear();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "readFields",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void readFields(DataInput in) throws IOException\n{\r\n    super.readFields(in);\r\n    shuffleFinishTime = in.readLong();\r\n    sortFinishTime = in.readLong();\r\n    int noFailedFetchTasks = in.readInt();\r\n    failedFetchTasks = new ArrayList<TaskAttemptID>(noFailedFetchTasks);\r\n    for (int i = 0; i < noFailedFetchTasks; ++i) {\r\n        TaskAttemptID id = new TaskAttemptID();\r\n        id.readFields(in);\r\n        failedFetchTasks.add(id);\r\n    }\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "write",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void write(DataOutput out) throws IOException\n{\r\n    super.write(out);\r\n    out.writeLong(shuffleFinishTime);\r\n    out.writeLong(sortFinishTime);\r\n    out.writeInt(failedFetchTasks.size());\r\n    for (TaskAttemptID taskId : failedFetchTasks) {\r\n        taskId.write(out);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\protocol",
  "methodName" : "create",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "ClientProtocol create(Configuration conf) throws IOException",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\protocol",
  "methodName" : "create",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "ClientProtocol create(InetSocketAddress addr, Configuration conf) throws IOException",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\protocol",
  "methodName" : "close",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void close(ClientProtocol clientProtocol) throws IOException",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output",
  "methodName" : "getCommittedTaskPath",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "Path getCommittedTaskPath(int appAttemptId, TaskAttemptContext context)\n{\r\n    return new Path(getJobAttemptPath(appAttemptId), String.valueOf(context.getTaskAttemptID()));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output",
  "methodName" : "fsFor",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "FileSystem fsFor(Path p, Configuration conf) throws IOException\n{\r\n    return p.getFileSystem(conf);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output",
  "methodName" : "cleanUpPartialOutputForTask",
  "errType" : null,
  "containingMethodsNum" : 10,
  "sourceCodeText" : "void cleanUpPartialOutputForTask(TaskAttemptContext context) throws IOException\n{\r\n    if (!this.getClass().isAnnotationPresent(Checkpointable.class)) {\r\n        throw new IllegalStateException(\"Invoking cleanUpPartialOutputForTask() \" + \"from non @Preemptable class\");\r\n    }\r\n    FileSystem fs = fsFor(getTaskAttemptPath(context), context.getConfiguration());\r\n    LOG.info(\"cleanUpPartialOutputForTask: removing everything belonging to \" + context.getTaskAttemptID().getTaskID() + \" in: \" + getCommittedTaskPath(context).getParent());\r\n    final TaskAttemptID taid = context.getTaskAttemptID();\r\n    final TaskID tid = taid.getTaskID();\r\n    Path pCommit = getCommittedTaskPath(context).getParent();\r\n    for (int i = 0; i < taid.getId(); ++i) {\r\n        TaskAttemptID oldId = new TaskAttemptID(tid, i);\r\n        Path pTask = new Path(pCommit, oldId.toString());\r\n        if (!fs.delete(pTask, true) && fs.exists(pTask)) {\r\n            throw new IOException(\"Failed to delete \" + pTask);\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred\\lib\\aggregate",
  "methodName" : "configure",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void configure(JobConf job)\n{\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred\\lib\\aggregate",
  "methodName" : "reduce",
  "errType" : null,
  "containingMethodsNum" : 11,
  "sourceCodeText" : "void reduce(Text key, Iterator<Text> values, OutputCollector<Text, Text> output, Reporter reporter) throws IOException\n{\r\n    String keyStr = key.toString();\r\n    int pos = keyStr.indexOf(ValueAggregatorDescriptor.TYPE_SEPARATOR);\r\n    String type = keyStr.substring(0, pos);\r\n    ValueAggregator aggregator = ValueAggregatorBaseDescriptor.generateValueAggregator(type);\r\n    while (values.hasNext()) {\r\n        aggregator.addNextValue(values.next());\r\n    }\r\n    Iterator outputs = aggregator.getCombinerOutput().iterator();\r\n    while (outputs.hasNext()) {\r\n        Object v = outputs.next();\r\n        if (v instanceof Text) {\r\n            output.collect(key, (Text) v);\r\n        } else {\r\n            output.collect(key, new Text(v.toString()));\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred\\lib\\aggregate",
  "methodName" : "close",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void close() throws IOException\n{\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred\\lib\\aggregate",
  "methodName" : "map",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void map(K1 arg0, V1 arg1, OutputCollector<Text, Text> arg2, Reporter arg3) throws IOException\n{\r\n    throw new IOException(\"should not be called\\n\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\join",
  "methodName" : "nextKeyValue",
  "errType" : null,
  "containingMethodsNum" : 13,
  "sourceCodeText" : "boolean nextKeyValue() throws IOException, InterruptedException\n{\r\n    if (key == null) {\r\n        key = createKey();\r\n    }\r\n    if (jc.flush(value)) {\r\n        ReflectionUtils.copy(conf, jc.key(), key);\r\n        return true;\r\n    }\r\n    jc.clear();\r\n    if (value == null) {\r\n        value = createValue();\r\n    }\r\n    final PriorityQueue<ComposableRecordReader<K, ?>> q = getRecordReaderQueue();\r\n    K iterkey = createKey();\r\n    while (q != null && !q.isEmpty()) {\r\n        fillJoinCollector(iterkey);\r\n        jc.reset(iterkey);\r\n        if (jc.flush(value)) {\r\n            ReflectionUtils.copy(conf, jc.key(), key);\r\n            return true;\r\n        }\r\n        jc.clear();\r\n    }\r\n    return false;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\join",
  "methodName" : "createValue",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "TupleWritable createValue()\n{\r\n    return createTupleWritable();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\join",
  "methodName" : "getDelegate",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "ResetableIterator<TupleWritable> getDelegate()\n{\r\n    return new JoinDelegationIterator();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "initDepricatedMap",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void initDepricatedMap()\n{\r\n    depricatedCounterMap.put(FileInputFormat.Counter.class.getName(), FileInputFormatCounter.class.getName());\r\n    depricatedCounterMap.put(FileOutputFormat.Counter.class.getName(), FileOutputFormatCounter.class.getName());\r\n    depricatedCounterMap.put(org.apache.hadoop.mapreduce.lib.input.FileInputFormat.Counter.class.getName(), FileInputFormatCounter.class.getName());\r\n    depricatedCounterMap.put(org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.Counter.class.getName(), FileOutputFormatCounter.class.getName());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getNewGroupKey",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "String getNewGroupKey(String oldGroup)\n{\r\n    if (depricatedCounterMap.containsKey(oldGroup)) {\r\n        return depricatedCounterMap.get(oldGroup);\r\n    }\r\n    return null;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "downgrade",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Counters downgrade(org.apache.hadoop.mapreduce.Counters newCounters)\n{\r\n    return new Counters(newCounters);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getGroup",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Group getGroup(String groupName)\n{\r\n    return super.getGroup(groupName);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getGroupNames",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Collection<String> getGroupNames()\n{\r\n    return IteratorUtils.toList(super.getGroupNames().iterator());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "makeCompactString",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "String makeCompactString()\n{\r\n    StringBuilder builder = new StringBuilder();\r\n    boolean first = true;\r\n    for (Group group : this) {\r\n        for (Counter counter : group) {\r\n            if (first) {\r\n                first = false;\r\n            } else {\r\n                builder.append(',');\r\n            }\r\n            builder.append(group.getDisplayName());\r\n            builder.append('.');\r\n            builder.append(counter.getDisplayName());\r\n            builder.append(':');\r\n            builder.append(counter.getCounter());\r\n        }\r\n    }\r\n    return builder.toString();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getCounterValue",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "long getCounterValue(CounterGroupBase<Counter> group, String counterName)\n{\r\n    Counter counter = group.findCounter(counterName, false);\r\n    if (counter != null)\r\n        return counter.getValue();\r\n    return 0L;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "findCounter",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "Counter findCounter(String group, String name)\n{\r\n    if (name.equals(\"MAP_INPUT_BYTES\")) {\r\n        LOG.warn(\"Counter name MAP_INPUT_BYTES is deprecated. \" + \"Use FileInputFormatCounters as group name and \" + \" BYTES_READ as counter name instead\");\r\n        return findCounter(FileInputFormatCounter.BYTES_READ);\r\n    }\r\n    String newGroupKey = getNewGroupKey(group);\r\n    if (newGroupKey != null) {\r\n        group = newGroupKey;\r\n    }\r\n    return getGroup(group).getCounterForName(name);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "findCounter",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Counter findCounter(String group, int id, String name)\n{\r\n    return findCounter(group, name);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "incrCounter",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void incrCounter(Enum<?> key, long amount)\n{\r\n    findCounter(key).increment(amount);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "incrCounter",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void incrCounter(String group, String counter, long amount)\n{\r\n    findCounter(group, counter).increment(amount);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getCounter",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "long getCounter(Enum<?> key)\n{\r\n    return findCounter(key).getValue();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "incrAllCounters",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void incrAllCounters(Counters other)\n{\r\n    for (Group otherGroup : other) {\r\n        Group group = getGroup(otherGroup.getName());\r\n        group.setDisplayName(otherGroup.getDisplayName());\r\n        for (Counter otherCounter : otherGroup) {\r\n            Counter counter = group.getCounterForName(otherCounter.getName());\r\n            counter.setDisplayName(otherCounter.getDisplayName());\r\n            counter.increment(otherCounter.getValue());\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "size",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "int size()\n{\r\n    return countCounters();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "sum",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "Counters sum(Counters a, Counters b)\n{\r\n    Counters counters = new Counters();\r\n    counters.incrAllCounters(a);\r\n    counters.incrAllCounters(b);\r\n    return counters;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "log",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void log(Logger log)\n{\r\n    log.info(\"Counters: \" + size());\r\n    for (Group group : this) {\r\n        log.info(\"  \" + group.getDisplayName());\r\n        for (Counter counter : group) {\r\n            log.info(\"    \" + counter.getDisplayName() + \"=\" + counter.getCounter());\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "makeEscapedCompactString",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String makeEscapedCompactString()\n{\r\n    return toEscapedCompactString(this);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "fromEscapedCompactString",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Counters fromEscapedCompactString(String compactString) throws ParseException\n{\r\n    return parseEscapedCompactString(compactString, new Counters());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getRecordWriter",
  "errType" : null,
  "containingMethodsNum" : 11,
  "sourceCodeText" : "RecordWriter<K, V> getRecordWriter(FileSystem ignored, JobConf job, String name, Progressable progress) throws IOException\n{\r\n    boolean isCompressed = getCompressOutput(job);\r\n    String keyValueSeparator = job.get(\"mapreduce.output.textoutputformat.separator\", \"\\t\");\r\n    if (!isCompressed) {\r\n        Path file = FileOutputFormat.getTaskOutputPath(job, name);\r\n        FileSystem fs = file.getFileSystem(job);\r\n        FSDataOutputStream fileOut = fs.create(file, progress);\r\n        return new LineRecordWriter<K, V>(fileOut, keyValueSeparator);\r\n    } else {\r\n        Class<? extends CompressionCodec> codecClass = getOutputCompressorClass(job, GzipCodec.class);\r\n        CompressionCodec codec = ReflectionUtils.newInstance(codecClass, job);\r\n        Path file = FileOutputFormat.getTaskOutputPath(job, name + codec.getDefaultExtension());\r\n        FileSystem fs = file.getFileSystem(job);\r\n        FSDataOutputStream fileOut = fs.create(file, progress);\r\n        return new LineRecordWriter<K, V>(new DataOutputStream(codec.createOutputStream(fileOut)), keyValueSeparator);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "write",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void write(HistoryEvent event) throws IOException\n{\r\n    Event wrapper = new Event();\r\n    wrapper.setType(event.getEventType());\r\n    wrapper.setEvent(event.getDatum());\r\n    writer.write(wrapper, encoder);\r\n    if (this.jsonOutput) {\r\n        encoder.flush();\r\n        out.writeBytes(\"\\n\");\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "flush",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void flush() throws IOException\n{\r\n    encoder.flush();\r\n    out.flush();\r\n    out.hflush();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "close",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void close() throws IOException\n{\r\n    try {\r\n        encoder.flush();\r\n        out.close();\r\n        out = null;\r\n    } finally {\r\n        IOUtils.cleanupWithLogger(LOG, out);\r\n    }\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "toAvro",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "JhCounters toAvro(Counters counters)\n{\r\n    return toAvro(counters, \"COUNTERS\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "toAvro",
  "errType" : null,
  "containingMethodsNum" : 10,
  "sourceCodeText" : "JhCounters toAvro(Counters counters, String name)\n{\r\n    JhCounters result = new JhCounters();\r\n    result.setName(new Utf8(name));\r\n    result.setGroups(new ArrayList<JhCounterGroup>(0));\r\n    if (counters == null)\r\n        return result;\r\n    for (CounterGroup group : counters) {\r\n        JhCounterGroup g = new JhCounterGroup();\r\n        g.setName(new Utf8(group.getName()));\r\n        g.setDisplayName(new Utf8(group.getDisplayName()));\r\n        g.setCounts(new ArrayList<JhCounter>(group.size()));\r\n        for (Counter counter : group) {\r\n            JhCounter c = new JhCounter();\r\n            c.setName(new Utf8(counter.getName()));\r\n            c.setDisplayName(new Utf8(counter.getDisplayName()));\r\n            c.setValue(counter.getValue());\r\n            g.getCounts().add(c);\r\n        }\r\n        result.getGroups().add(g);\r\n    }\r\n    return result;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output",
  "methodName" : "getOutputPath",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Path getOutputPath()",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output",
  "methodName" : "hasOutputPath",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "boolean hasOutputPath()\n{\r\n    return getOutputPath() != null;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output",
  "methodName" : "getWorkPath",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Path getWorkPath() throws IOException",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output",
  "methodName" : "toString",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String toString()\n{\r\n    return \"PathOutputCommitter{context=\" + context + \"; \" + super.toString() + '}';\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred\\lib\\aggregate",
  "methodName" : "createInstance",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Object createInstance(String className)\n{\r\n    return org.apache.hadoop.mapreduce.lib.aggregate.UserDefinedValueAggregatorDescriptor.createInstance(className);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred\\lib\\aggregate",
  "methodName" : "configure",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void configure(JobConf job)\n{\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\chain",
  "methodName" : "getValues",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Iterable<VALUEIN> getValues() throws IOException, InterruptedException\n{\r\n    return base.getValues();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\chain",
  "methodName" : "nextKey",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "boolean nextKey() throws IOException, InterruptedException\n{\r\n    return base.nextKey();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\chain",
  "methodName" : "getCounter",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Counter getCounter(Enum<?> counterName)\n{\r\n    return base.getCounter(counterName);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\chain",
  "methodName" : "getCounter",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Counter getCounter(String groupName, String counterName)\n{\r\n    return base.getCounter(groupName, counterName);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\chain",
  "methodName" : "getCurrentKey",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "KEYIN getCurrentKey() throws IOException, InterruptedException\n{\r\n    return base.getCurrentKey();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\chain",
  "methodName" : "getCurrentValue",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "VALUEIN getCurrentValue() throws IOException, InterruptedException\n{\r\n    return base.getCurrentValue();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\chain",
  "methodName" : "getOutputCommitter",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "OutputCommitter getOutputCommitter()\n{\r\n    return base.getOutputCommitter();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\chain",
  "methodName" : "nextKeyValue",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "boolean nextKeyValue() throws IOException, InterruptedException\n{\r\n    return base.nextKeyValue();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\chain",
  "methodName" : "write",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void write(KEYOUT key, VALUEOUT value) throws IOException, InterruptedException\n{\r\n    rw.write(key, value);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\chain",
  "methodName" : "getStatus",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String getStatus()\n{\r\n    return base.getStatus();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\chain",
  "methodName" : "getTaskAttemptID",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "TaskAttemptID getTaskAttemptID()\n{\r\n    return base.getTaskAttemptID();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\chain",
  "methodName" : "setStatus",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setStatus(String msg)\n{\r\n    base.setStatus(msg);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\chain",
  "methodName" : "getArchiveClassPaths",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Path[] getArchiveClassPaths()\n{\r\n    return base.getArchiveClassPaths();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\chain",
  "methodName" : "getArchiveTimestamps",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String[] getArchiveTimestamps()\n{\r\n    return base.getArchiveTimestamps();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\chain",
  "methodName" : "getCacheArchives",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "URI[] getCacheArchives() throws IOException\n{\r\n    return base.getCacheArchives();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\chain",
  "methodName" : "getCacheFiles",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "URI[] getCacheFiles() throws IOException\n{\r\n    return base.getCacheFiles();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\chain",
  "methodName" : "getCombinerClass",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Class<? extends Reducer<?, ?, ?, ?>> getCombinerClass() throws ClassNotFoundException\n{\r\n    return base.getCombinerClass();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\chain",
  "methodName" : "getConfiguration",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Configuration getConfiguration()\n{\r\n    return conf;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\chain",
  "methodName" : "getFileClassPaths",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Path[] getFileClassPaths()\n{\r\n    return base.getFileClassPaths();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\chain",
  "methodName" : "getFileTimestamps",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String[] getFileTimestamps()\n{\r\n    return base.getFileTimestamps();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\chain",
  "methodName" : "getCombinerKeyGroupingComparator",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "RawComparator<?> getCombinerKeyGroupingComparator()\n{\r\n    return base.getCombinerKeyGroupingComparator();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\chain",
  "methodName" : "getGroupingComparator",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "RawComparator<?> getGroupingComparator()\n{\r\n    return base.getGroupingComparator();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\chain",
  "methodName" : "getInputFormatClass",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Class<? extends InputFormat<?, ?>> getInputFormatClass() throws ClassNotFoundException\n{\r\n    return base.getInputFormatClass();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\chain",
  "methodName" : "getJar",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String getJar()\n{\r\n    return base.getJar();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\chain",
  "methodName" : "getJobID",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "JobID getJobID()\n{\r\n    return base.getJobID();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\chain",
  "methodName" : "getJobName",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String getJobName()\n{\r\n    return base.getJobName();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\chain",
  "methodName" : "getJobSetupCleanupNeeded",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "boolean getJobSetupCleanupNeeded()\n{\r\n    return base.getJobSetupCleanupNeeded();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\chain",
  "methodName" : "getTaskCleanupNeeded",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "boolean getTaskCleanupNeeded()\n{\r\n    return base.getTaskCleanupNeeded();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\chain",
  "methodName" : "getLocalCacheArchives",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Path[] getLocalCacheArchives() throws IOException\n{\r\n    return base.getLocalCacheArchives();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\chain",
  "methodName" : "getLocalCacheFiles",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Path[] getLocalCacheFiles() throws IOException\n{\r\n    return base.getLocalCacheFiles();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\chain",
  "methodName" : "getMapOutputKeyClass",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Class<?> getMapOutputKeyClass()\n{\r\n    return base.getMapOutputKeyClass();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\chain",
  "methodName" : "getMapOutputValueClass",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Class<?> getMapOutputValueClass()\n{\r\n    return base.getMapOutputValueClass();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\chain",
  "methodName" : "getMapperClass",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Class<? extends Mapper<?, ?, ?, ?>> getMapperClass() throws ClassNotFoundException\n{\r\n    return base.getMapperClass();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\chain",
  "methodName" : "getMaxMapAttempts",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "int getMaxMapAttempts()\n{\r\n    return base.getMaxMapAttempts();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\chain",
  "methodName" : "getMaxReduceAttempts",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "int getMaxReduceAttempts()\n{\r\n    return base.getMaxMapAttempts();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\chain",
  "methodName" : "getNumReduceTasks",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "int getNumReduceTasks()\n{\r\n    return base.getNumReduceTasks();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\chain",
  "methodName" : "getOutputFormatClass",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Class<? extends OutputFormat<?, ?>> getOutputFormatClass() throws ClassNotFoundException\n{\r\n    return base.getOutputFormatClass();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\chain",
  "methodName" : "getOutputKeyClass",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Class<?> getOutputKeyClass()\n{\r\n    return base.getOutputKeyClass();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\chain",
  "methodName" : "getOutputValueClass",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Class<?> getOutputValueClass()\n{\r\n    return base.getOutputValueClass();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\chain",
  "methodName" : "getPartitionerClass",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Class<? extends Partitioner<?, ?>> getPartitionerClass() throws ClassNotFoundException\n{\r\n    return base.getPartitionerClass();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\chain",
  "methodName" : "getProfileEnabled",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "boolean getProfileEnabled()\n{\r\n    return base.getProfileEnabled();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\chain",
  "methodName" : "getProfileParams",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String getProfileParams()\n{\r\n    return base.getProfileParams();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\chain",
  "methodName" : "getProfileTaskRange",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "IntegerRanges getProfileTaskRange(boolean isMap)\n{\r\n    return base.getProfileTaskRange(isMap);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\chain",
  "methodName" : "getReducerClass",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Class<? extends Reducer<?, ?, ?, ?>> getReducerClass() throws ClassNotFoundException\n{\r\n    return base.getReducerClass();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\chain",
  "methodName" : "getSortComparator",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "RawComparator<?> getSortComparator()\n{\r\n    return base.getSortComparator();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\chain",
  "methodName" : "getSymlink",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "boolean getSymlink()\n{\r\n    return base.getSymlink();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\chain",
  "methodName" : "getUser",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String getUser()\n{\r\n    return base.getUser();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\chain",
  "methodName" : "getWorkingDirectory",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Path getWorkingDirectory() throws IOException\n{\r\n    return base.getWorkingDirectory();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\chain",
  "methodName" : "progress",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void progress()\n{\r\n    base.progress();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\chain",
  "methodName" : "getCredentials",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Credentials getCredentials()\n{\r\n    return base.getCredentials();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\chain",
  "methodName" : "getProgress",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "float getProgress()\n{\r\n    return base.getProgress();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\task\\reduce",
  "methodName" : "getState",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "State getState()\n{\r\n    return state;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\task\\reduce",
  "methodName" : "getHostName",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getHostName()\n{\r\n    return hostName;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\task\\reduce",
  "methodName" : "getBaseUrl",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getBaseUrl()\n{\r\n    return baseUrl;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\task\\reduce",
  "methodName" : "addKnownMap",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void addKnownMap(TaskAttemptID mapId)\n{\r\n    maps.add(mapId);\r\n    if (state == State.IDLE) {\r\n        state = State.PENDING;\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\task\\reduce",
  "methodName" : "getAndClearKnownMaps",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "List<TaskAttemptID> getAndClearKnownMaps()\n{\r\n    List<TaskAttemptID> currentKnownMaps = maps;\r\n    maps = new ArrayList<TaskAttemptID>();\r\n    return currentKnownMaps;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\task\\reduce",
  "methodName" : "markBusy",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void markBusy()\n{\r\n    state = State.BUSY;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\task\\reduce",
  "methodName" : "getNumKnownMapOutputs",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "int getNumKnownMapOutputs()\n{\r\n    return maps.size();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\task\\reduce",
  "methodName" : "markAvailable",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "State markAvailable()\n{\r\n    if (maps.isEmpty()) {\r\n        state = State.IDLE;\r\n    } else {\r\n        state = State.PENDING;\r\n    }\r\n    return state;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\task\\reduce",
  "methodName" : "toString",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String toString()\n{\r\n    return hostName;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\task\\reduce",
  "methodName" : "penalize",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void penalize()\n{\r\n    state = State.PENALIZED;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "createKey",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "LongWritable createKey()\n{\r\n    return new LongWritable();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "createValue",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "BytesWritable createValue()\n{\r\n    return new BytesWritable(new byte[recordLength]);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "next",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "boolean next(LongWritable key, BytesWritable value) throws IOException\n{\r\n    boolean dataRead = reader.nextKeyValue();\r\n    if (dataRead) {\r\n        LongWritable newKey = reader.getCurrentKey();\r\n        BytesWritable newValue = reader.getCurrentValue();\r\n        key.set(newKey.get());\r\n        value.set(newValue);\r\n    }\r\n    return dataRead;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getProgress",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "float getProgress() throws IOException\n{\r\n    return reader.getProgress();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getPos",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "long getPos() throws IOException\n{\r\n    return reader.getPos();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "close",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void close() throws IOException\n{\r\n    reader.close();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\db",
  "methodName" : "getSelectQuery",
  "errType" : null,
  "containingMethodsNum" : 25,
  "sourceCodeText" : "String getSelectQuery()\n{\r\n    StringBuilder query = new StringBuilder();\r\n    DataDrivenDBInputFormat.DataDrivenDBInputSplit dataSplit = (DataDrivenDBInputFormat.DataDrivenDBInputSplit) getSplit();\r\n    DBConfiguration dbConf = getDBConf();\r\n    String[] fieldNames = getFieldNames();\r\n    String tableName = getTableName();\r\n    String conditions = getConditions();\r\n    StringBuilder conditionClauses = new StringBuilder();\r\n    conditionClauses.append(\"( \").append(dataSplit.getLowerClause());\r\n    conditionClauses.append(\" ) AND ( \").append(dataSplit.getUpperClause());\r\n    conditionClauses.append(\" )\");\r\n    if (dbConf.getInputQuery() == null) {\r\n        query.append(\"SELECT \");\r\n        for (int i = 0; i < fieldNames.length; i++) {\r\n            query.append(fieldNames[i]);\r\n            if (i != fieldNames.length - 1) {\r\n                query.append(\", \");\r\n            }\r\n        }\r\n        query.append(\" FROM \").append(tableName);\r\n        if (!dbProductName.startsWith(\"ORACLE\")) {\r\n            query.append(\" AS \").append(tableName);\r\n        }\r\n        query.append(\" WHERE \");\r\n        if (conditions != null && conditions.length() > 0) {\r\n            query.append(\"( \").append(conditions).append(\" ) AND \");\r\n        }\r\n        query.append(conditionClauses.toString());\r\n    } else {\r\n        String inputQuery = dbConf.getInputQuery();\r\n        if (inputQuery.indexOf(DataDrivenDBInputFormat.SUBSTITUTE_TOKEN) == -1) {\r\n            LOG.error(\"Could not find the clause substitution token \" + DataDrivenDBInputFormat.SUBSTITUTE_TOKEN + \" in the query: [\" + inputQuery + \"]. Parallel splits may not work correctly.\");\r\n        }\r\n        query.append(inputQuery.replace(DataDrivenDBInputFormat.SUBSTITUTE_TOKEN, conditionClauses.toString()));\r\n    }\r\n    LOG.debug(\"Using query: \" + query.toString());\r\n    return query.toString();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\input",
  "methodName" : "initialize",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void initialize(InputSplit split, TaskAttemptContext context) throws IOException, InterruptedException\n{\r\n    this.split = (CombineFileSplit) split;\r\n    this.context = context;\r\n    if (null != this.curReader) {\r\n        this.curReader.initialize(split, context);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\input",
  "methodName" : "nextKeyValue",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "boolean nextKeyValue() throws IOException, InterruptedException\n{\r\n    while ((curReader == null) || !curReader.nextKeyValue()) {\r\n        if (!initNextRecordReader()) {\r\n            return false;\r\n        }\r\n    }\r\n    return true;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\input",
  "methodName" : "getCurrentKey",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "K getCurrentKey() throws IOException, InterruptedException\n{\r\n    return curReader.getCurrentKey();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\input",
  "methodName" : "getCurrentValue",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "V getCurrentValue() throws IOException, InterruptedException\n{\r\n    return curReader.getCurrentValue();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\input",
  "methodName" : "close",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void close() throws IOException\n{\r\n    if (curReader != null) {\r\n        curReader.close();\r\n        curReader = null;\r\n    }\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\input",
  "methodName" : "getProgress",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "float getProgress() throws IOException, InterruptedException\n{\r\n    long subprogress = 0;\r\n    if (null != curReader) {\r\n        subprogress = (long) (curReader.getProgress() * split.getLength(idx - 1));\r\n    }\r\n    return Math.min(1.0f, (progress + subprogress) / (float) (split.getLength()));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\input",
  "methodName" : "initNextRecordReader",
  "errType" : [ "Exception" ],
  "containingMethodsNum" : 10,
  "sourceCodeText" : "boolean initNextRecordReader() throws IOException\n{\r\n    if (curReader != null) {\r\n        curReader.close();\r\n        curReader = null;\r\n        if (idx > 0) {\r\n            progress += split.getLength(idx - 1);\r\n        }\r\n    }\r\n    if (idx == split.getNumPaths()) {\r\n        return false;\r\n    }\r\n    context.progress();\r\n    try {\r\n        Configuration conf = context.getConfiguration();\r\n        conf.set(MRJobConfig.MAP_INPUT_FILE, split.getPath(idx).toString());\r\n        conf.setLong(MRJobConfig.MAP_INPUT_START, split.getOffset(idx));\r\n        conf.setLong(MRJobConfig.MAP_INPUT_PATH, split.getLength(idx));\r\n        curReader = rrConstructor.newInstance(new Object[] { split, context, Integer.valueOf(idx) });\r\n        if (idx > 0) {\r\n            curReader.initialize(split, context);\r\n        }\r\n    } catch (Exception e) {\r\n        throw new RuntimeException(e);\r\n    }\r\n    idx++;\r\n    return true;\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\input",
  "methodName" : "createRecordReader",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "RecordReader<Text, Text> createRecordReader(InputSplit split, TaskAttemptContext context) throws IOException\n{\r\n    context.setStatus(split.toString());\r\n    return new SequenceFileAsTextRecordReader();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\partition",
  "methodName" : "getPartition",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "int getPartition(K key, V value, int numReduceTasks)\n{\r\n    return (key.hashCode() & Integer.MAX_VALUE) % numReduceTasks;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getJobID",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "org.apache.hadoop.mapreduce.JobID getJobID()\n{\r\n    return id;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getUser",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Text getUser()\n{\r\n    return user;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getJobSubmitDir",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Path getJobSubmitDir()\n{\r\n    return this.jobSubmitDir;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "readFields",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void readFields(DataInput in) throws IOException\n{\r\n    id = new org.apache.hadoop.mapreduce.JobID();\r\n    id.readFields(in);\r\n    user = new Text();\r\n    user.readFields(in);\r\n    jobSubmitDir = new Path(WritableUtils.readString(in));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "write",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void write(DataOutput out) throws IOException\n{\r\n    id.write(out);\r\n    user.write(out);\r\n    WritableUtils.writeString(out, jobSubmitDir.toString());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\counters",
  "methodName" : "addGroup",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "G addGroup(G group)\n{\r\n    String name = group.getName();\r\n    if (isFrameworkGroup(name)) {\r\n        fgroups.put(name, group);\r\n    } else {\r\n        limits.checkGroups(groups.size() + 1);\r\n        groups.put(name, group);\r\n    }\r\n    return group;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\counters",
  "methodName" : "addGroup",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "G addGroup(String name, String displayName)\n{\r\n    return addGroup(groupFactory.newGroup(name, displayName, limits));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\counters",
  "methodName" : "findCounter",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "C findCounter(String groupName, String counterName)\n{\r\n    G grp = getGroup(groupName);\r\n    return grp.findCounter(counterName);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\counters",
  "methodName" : "findCounter",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "C findCounter(Enum<?> key)\n{\r\n    C counter = cache.get(key);\r\n    if (counter == null) {\r\n        counter = findCounter(key.getDeclaringClass().getName(), key.name());\r\n        cache.put(key, counter);\r\n    }\r\n    return counter;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\counters",
  "methodName" : "findCounter",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "C findCounter(String scheme, FileSystemCounter key)\n{\r\n    return ((FileSystemCounterGroup<C>) getGroup(FileSystemCounter.class.getName()).getUnderlyingGroup()).findCounter(scheme, key);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\counters",
  "methodName" : "getGroupNames",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "Iterable<String> getGroupNames()\n{\r\n    HashSet<String> deprecated = new HashSet<String>();\r\n    for (Map.Entry<String, String> entry : legacyMap.entrySet()) {\r\n        String newGroup = entry.getValue();\r\n        boolean isFGroup = isFrameworkGroup(newGroup);\r\n        if (isFGroup ? fgroups.containsKey(newGroup) : groups.containsKey(newGroup)) {\r\n            deprecated.add(entry.getKey());\r\n        }\r\n    }\r\n    return Iterables.concat(fgroups.keySet(), groups.keySet(), deprecated);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\counters",
  "methodName" : "iterator",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Iterator<G> iterator()\n{\r\n    return Iterators.concat(fgroups.values().iterator(), groups.values().iterator());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\counters",
  "methodName" : "getGroup",
  "errType" : null,
  "containingMethodsNum" : 10,
  "sourceCodeText" : "G getGroup(String groupName)\n{\r\n    boolean groupNameInLegacyMap = true;\r\n    String newGroupName = legacyMap.get(groupName);\r\n    if (newGroupName == null) {\r\n        groupNameInLegacyMap = false;\r\n        newGroupName = Limits.filterGroupName(groupName);\r\n    }\r\n    boolean isFGroup = isFrameworkGroup(newGroupName);\r\n    G group = isFGroup ? fgroups.get(newGroupName) : groups.get(newGroupName);\r\n    if (group == null) {\r\n        group = groupFactory.newGroup(newGroupName, limits);\r\n        if (isFGroup) {\r\n            fgroups.put(newGroupName, group);\r\n        } else {\r\n            limits.checkGroups(groups.size() + 1);\r\n            groups.put(newGroupName, group);\r\n        }\r\n        if (groupNameInLegacyMap) {\r\n            LOG.warn(\"Group \" + groupName + \" is deprecated. Use \" + newGroupName + \" instead\");\r\n        }\r\n    }\r\n    return group;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\counters",
  "methodName" : "countCounters",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "int countCounters()\n{\r\n    int result = 0;\r\n    for (G group : this) {\r\n        result += group.size();\r\n    }\r\n    return result;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\counters",
  "methodName" : "write",
  "errType" : null,
  "containingMethodsNum" : 15,
  "sourceCodeText" : "void write(DataOutput out) throws IOException\n{\r\n    WritableUtils.writeVInt(out, groupFactory.version());\r\n    WritableUtils.writeVInt(out, fgroups.size());\r\n    for (G group : fgroups.values()) {\r\n        if (group.getUnderlyingGroup() instanceof FrameworkCounterGroup<?, ?>) {\r\n            WritableUtils.writeVInt(out, GroupType.FRAMEWORK.ordinal());\r\n            WritableUtils.writeVInt(out, getFrameworkGroupId(group.getName()));\r\n            group.write(out);\r\n        } else if (group.getUnderlyingGroup() instanceof FileSystemCounterGroup<?>) {\r\n            WritableUtils.writeVInt(out, GroupType.FILESYSTEM.ordinal());\r\n            group.write(out);\r\n        }\r\n    }\r\n    if (writeAllCounters) {\r\n        WritableUtils.writeVInt(out, groups.size());\r\n        for (G group : groups.values()) {\r\n            Text.writeString(out, group.getName());\r\n            group.write(out);\r\n        }\r\n    } else {\r\n        WritableUtils.writeVInt(out, 0);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\counters",
  "methodName" : "readFields",
  "errType" : null,
  "containingMethodsNum" : 16,
  "sourceCodeText" : "void readFields(DataInput in) throws IOException\n{\r\n    int version = WritableUtils.readVInt(in);\r\n    if (version != groupFactory.version()) {\r\n        throw new IOException(\"Counters version mismatch, expected \" + groupFactory.version() + \" got \" + version);\r\n    }\r\n    int numFGroups = WritableUtils.readVInt(in);\r\n    fgroups.clear();\r\n    GroupType[] groupTypes = GroupType.values();\r\n    while (numFGroups-- > 0) {\r\n        GroupType groupType = groupTypes[WritableUtils.readVInt(in)];\r\n        G group;\r\n        switch(groupType) {\r\n            case FILESYSTEM:\r\n                group = groupFactory.newFileSystemGroup();\r\n                break;\r\n            case FRAMEWORK:\r\n                group = groupFactory.newFrameworkGroup(WritableUtils.readVInt(in));\r\n                break;\r\n            default:\r\n                throw new IOException(\"Unexpected counter group type: \" + groupType);\r\n        }\r\n        group.readFields(in);\r\n        fgroups.put(group.getName(), group);\r\n    }\r\n    int numGroups = WritableUtils.readVInt(in);\r\n    while (numGroups-- > 0) {\r\n        limits.checkGroups(groups.size() + 1);\r\n        G group = groupFactory.newGenericGroup(StringInterner.weakIntern(Text.readString(in)), null, limits);\r\n        group.readFields(in);\r\n        groups.put(group.getName(), group);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\counters",
  "methodName" : "toString",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "String toString()\n{\r\n    StringBuilder sb = new StringBuilder(\"Counters: \" + countCounters());\r\n    for (G group : this) {\r\n        sb.append(\"\\n\\t\").append(group.getDisplayName());\r\n        for (Counter counter : group) {\r\n            sb.append(\"\\n\\t\\t\").append(counter.getDisplayName()).append(\"=\").append(counter.getValue());\r\n        }\r\n    }\r\n    return sb.toString();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\counters",
  "methodName" : "incrAllCounters",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void incrAllCounters(AbstractCounters<C, G> other)\n{\r\n    for (G right : other) {\r\n        String groupName = right.getName();\r\n        G left = (isFrameworkGroup(groupName) ? fgroups : groups).get(groupName);\r\n        if (left == null) {\r\n            left = addGroup(groupName, right.getDisplayName());\r\n        }\r\n        left.incrAllCounters(right);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\counters",
  "methodName" : "equals",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "boolean equals(Object genericRight)\n{\r\n    if (genericRight instanceof AbstractCounters<?, ?>) {\r\n        return Iterators.elementsEqual(iterator(), ((AbstractCounters<C, G>) genericRight).iterator());\r\n    }\r\n    return false;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\counters",
  "methodName" : "hashCode",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "int hashCode()\n{\r\n    return groups.hashCode();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\counters",
  "methodName" : "setWriteAllCounters",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setWriteAllCounters(boolean send)\n{\r\n    writeAllCounters = send;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\counters",
  "methodName" : "getWriteAllCounters",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean getWriteAllCounters()\n{\r\n    return writeAllCounters;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\counters",
  "methodName" : "limits",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Limits limits()\n{\r\n    return limits;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest\\stages",
  "methodName" : "executeStage",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Path executeStage(final String name) throws IOException\n{\r\n    return createNewDirectory(\"Task setup \" + name, requireNonNull(getTaskAttemptDir(), \"No task attempt directory\"));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest\\stages",
  "methodName" : "executeStage",
  "errType" : null,
  "containingMethodsNum" : 16,
  "sourceCodeText" : "TaskManifest executeStage(final Void arguments) throws IOException\n{\r\n    final Path taskAttemptDir = getRequiredTaskAttemptDir();\r\n    final TaskManifest manifest = createTaskManifest(getStageConfig());\r\n    LOG.info(\"{}: scanning directory {}\", getName(), taskAttemptDir);\r\n    final int depth = scanDirectoryTree(manifest, taskAttemptDir, getDestinationDir(), 0, true);\r\n    List<FileEntry> filesToCommit = manifest.getFilesToCommit();\r\n    LongSummaryStatistics fileSummary = filesToCommit.stream().mapToLong(FileEntry::getSize).summaryStatistics();\r\n    long fileDataSize = fileSummary.getSum();\r\n    long fileCount = fileSummary.getCount();\r\n    int dirCount = manifest.getDestDirectories().size();\r\n    LOG.info(\"{}: directory {} contained {} file(s); data size {}\", getName(), taskAttemptDir, fileCount, fileDataSize);\r\n    LOG.info(\"{}: Directory count = {}; maximum depth {}\", getName(), dirCount, depth);\r\n    IOStatisticsStore iostats = getIOStatistics();\r\n    iostats.addSample(COMMITTER_TASK_DIRECTORY_COUNT_MEAN, dirCount);\r\n    iostats.addSample(COMMITTER_TASK_DIRECTORY_DEPTH_MEAN, depth);\r\n    iostats.addSample(COMMITTER_TASK_FILE_COUNT_MEAN, fileCount);\r\n    iostats.addSample(COMMITTER_TASK_FILE_SIZE_MEAN, fileDataSize);\r\n    return manifest;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest\\stages",
  "methodName" : "scanDirectoryTree",
  "errType" : null,
  "containingMethodsNum" : 20,
  "sourceCodeText" : "int scanDirectoryTree(TaskManifest manifest, Path srcDir, Path destDir, int depth, boolean parentDirExists) throws IOException\n{\r\n    progress();\r\n    int maxDepth = 0;\r\n    int files = 0;\r\n    boolean dirExists = parentDirExists;\r\n    List<FileStatus> subdirs = new ArrayList<>();\r\n    try (DurationInfo ignored = new DurationInfo(LOG, false, \"Task Attempt %s source dir %s, dest dir %s\", getTaskAttemptId(), srcDir, destDir)) {\r\n        final RemoteIterator<FileStatus> listing = listStatusIterator(srcDir);\r\n        if (depth > 0) {\r\n            final EntryStatus status;\r\n            if (parentDirExists) {\r\n                final FileStatus destDirStatus = getFileStatusOrNull(destDir);\r\n                status = EntryStatus.toEntryStatus(destDirStatus);\r\n                dirExists = destDirStatus != null;\r\n            } else {\r\n                status = EntryStatus.not_found;\r\n            }\r\n            manifest.addDirectory(DirEntry.dirEntry(destDir, status, depth));\r\n        }\r\n        while (listing.hasNext()) {\r\n            final FileStatus st = listing.next();\r\n            if (st.isFile()) {\r\n                files++;\r\n                final FileEntry entry = fileEntry(st, destDir);\r\n                manifest.addFileToCommit(entry);\r\n                LOG.debug(\"To rename: {}\", entry);\r\n            } else {\r\n                if (st.isDirectory()) {\r\n                    subdirs.add(st);\r\n                } else {\r\n                    LOG.info(\"Ignoring FS object {}\", st);\r\n                }\r\n            }\r\n        }\r\n        maybeAddIOStatistics(getIOStatistics(), listing);\r\n    }\r\n    LOG.debug(\"{}: Number of subdirectories under {} found: {}; file count {}\", getName(), srcDir, subdirs.size(), files);\r\n    for (FileStatus st : subdirs) {\r\n        Path destSubDir = new Path(destDir, st.getPath().getName());\r\n        final int d = scanDirectoryTree(manifest, st.getPath(), destSubDir, depth + 1, dirExists);\r\n        maxDepth = Math.max(maxDepth, d);\r\n    }\r\n    return 1 + maxDepth;\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred\\lib",
  "methodName" : "next",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "boolean next(K key, V value) throws IOException\n{\r\n    return delegate.next(key, value);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred\\lib",
  "methodName" : "createKey",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "K createKey()\n{\r\n    return delegate.createKey();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred\\lib",
  "methodName" : "createValue",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "V createValue()\n{\r\n    return delegate.createValue();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred\\lib",
  "methodName" : "getPos",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "long getPos() throws IOException\n{\r\n    return delegate.getPos();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred\\lib",
  "methodName" : "close",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void close() throws IOException\n{\r\n    delegate.close();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred\\lib",
  "methodName" : "getProgress",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "float getProgress() throws IOException\n{\r\n    return delegate.getProgress();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\task\\reduce",
  "methodName" : "resolve",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void resolve(TaskCompletionEvent event)\n{\r\n    switch(event.getTaskStatus()) {\r\n        case SUCCEEDED:\r\n            URI u = getBaseURI(reduceId, event.getTaskTrackerHttp());\r\n            addKnownMapOutput(u.getHost() + \":\" + u.getPort(), u.toString(), event.getTaskAttemptId());\r\n            maxMapRuntime = Math.max(maxMapRuntime, event.getTaskRunTime());\r\n            break;\r\n        case FAILED:\r\n        case KILLED:\r\n        case OBSOLETE:\r\n            obsoleteMapOutput(event.getTaskAttemptId());\r\n            LOG.info(\"Ignoring obsolete output of \" + event.getTaskStatus() + \" map-task: '\" + event.getTaskAttemptId() + \"'\");\r\n            break;\r\n        case TIPFAILED:\r\n            tipFailed(event.getTaskAttemptId().getTaskID());\r\n            LOG.info(\"Ignoring output of failed map TIP: '\" + event.getTaskAttemptId() + \"'\");\r\n            break;\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\task\\reduce",
  "methodName" : "getBaseURI",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "URI getBaseURI(TaskAttemptID reduceId, String url)\n{\r\n    StringBuffer baseUrl = new StringBuffer(url);\r\n    if (!url.endsWith(\"/\")) {\r\n        baseUrl.append(\"/\");\r\n    }\r\n    baseUrl.append(\"mapOutput?job=\");\r\n    baseUrl.append(reduceId.getJobID());\r\n    baseUrl.append(\"&reduce=\");\r\n    baseUrl.append(reduceId.getTaskID().getId());\r\n    baseUrl.append(\"&map=\");\r\n    URI u = URI.create(baseUrl.toString());\r\n    return u;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\task\\reduce",
  "methodName" : "copySucceeded",
  "errType" : null,
  "containingMethodsNum" : 14,
  "sourceCodeText" : "void copySucceeded(TaskAttemptID mapId, MapHost host, long bytes, long startMillis, long endMillis, MapOutput<K, V> output) throws IOException\n{\r\n    failureCounts.remove(mapId);\r\n    hostFailures.remove(host.getHostName());\r\n    int mapIndex = mapId.getTaskID().getId();\r\n    if (!finishedMaps[mapIndex]) {\r\n        output.commit();\r\n        finishedMaps[mapIndex] = true;\r\n        shuffledMapsCounter.increment(1);\r\n        if (--remainingMaps == 0) {\r\n            notifyAll();\r\n        }\r\n        long copyMillis = (endMillis - startMillis);\r\n        if (copyMillis == 0)\r\n            copyMillis = 1;\r\n        float bytesPerMillis = (float) bytes / copyMillis;\r\n        float transferRate = bytesPerMillis * BYTES_PER_MILLIS_TO_MBS;\r\n        String individualProgress = \"copy task(\" + mapId + \" succeeded\" + \" at \" + mbpsFormat.format(transferRate) + \" MB/s)\";\r\n        copyTimeTracker.add(startMillis, endMillis);\r\n        totalBytesShuffledTillNow += bytes;\r\n        updateStatus(individualProgress);\r\n        reduceShuffleBytes.increment(bytes);\r\n        lastProgressTime = Time.monotonicNow();\r\n        LOG.debug(\"map \" + mapId + \" done \" + status.getStateString());\r\n    } else {\r\n        LOG.warn(\"Aborting already-finished MapOutput for \" + mapId);\r\n        output.abort();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\task\\reduce",
  "methodName" : "updateStatus",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void updateStatus(String individualProgress)\n{\r\n    int mapsDone = totalMaps - remainingMaps;\r\n    long totalCopyMillis = copyTimeTracker.getCopyMillis();\r\n    if (totalCopyMillis == 0)\r\n        totalCopyMillis = 1;\r\n    float bytesPerMillis = (float) totalBytesShuffledTillNow / totalCopyMillis;\r\n    float transferRate = bytesPerMillis * BYTES_PER_MILLIS_TO_MBS;\r\n    progress.set((float) mapsDone / totalMaps);\r\n    String statusString = mapsDone + \" / \" + totalMaps + \" copied.\";\r\n    status.setStateString(statusString);\r\n    if (individualProgress != null) {\r\n        progress.setStatus(individualProgress + \" Aggregated copy rate(\" + mapsDone + \" of \" + totalMaps + \" at \" + mbpsFormat.format(transferRate) + \" MB/s)\");\r\n    } else {\r\n        progress.setStatus(\"copy(\" + mapsDone + \" of \" + totalMaps + \" at \" + mbpsFormat.format(transferRate) + \" MB/s)\");\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\task\\reduce",
  "methodName" : "updateStatus",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void updateStatus()\n{\r\n    updateStatus(null);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\task\\reduce",
  "methodName" : "hostFailed",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void hostFailed(String hostname)\n{\r\n    if (hostFailures.containsKey(hostname)) {\r\n        IntWritable x = hostFailures.get(hostname);\r\n        x.set(x.get() + 1);\r\n    } else {\r\n        hostFailures.put(hostname, new IntWritable(1));\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\task\\reduce",
  "methodName" : "hostFailureCount",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "int hostFailureCount(String hostname)\n{\r\n    int failures = 0;\r\n    if (hostFailures.containsKey(hostname)) {\r\n        failures = hostFailures.get(hostname).get();\r\n    }\r\n    return failures;\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\task\\reduce",
  "methodName" : "fetchFailureCount",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "int fetchFailureCount(TaskAttemptID mapId)\n{\r\n    int failures = 0;\r\n    if (failureCounts.containsKey(mapId)) {\r\n        failures = failureCounts.get(mapId).get();\r\n    }\r\n    return failures;\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\task\\reduce",
  "methodName" : "copyFailed",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 16,
  "sourceCodeText" : "void copyFailed(TaskAttemptID mapId, MapHost host, boolean readError, boolean connectExcpt)\n{\r\n    int failures = 1;\r\n    if (failureCounts.containsKey(mapId)) {\r\n        IntWritable x = failureCounts.get(mapId);\r\n        x.set(x.get() + 1);\r\n        failures = x.get();\r\n    } else {\r\n        failureCounts.put(mapId, new IntWritable(1));\r\n    }\r\n    String hostname = host.getHostName();\r\n    IntWritable hostFailedNum = hostFailures.get(hostname);\r\n    if (hostFailedNum == null) {\r\n        hostFailures.put(hostname, new IntWritable(1));\r\n    }\r\n    boolean hostFail = hostFailures.get(hostname).get() > getMaxHostFailures() ? true : false;\r\n    if (failures >= abortFailureLimit) {\r\n        try {\r\n            throw new IOException(failures + \" failures downloading \" + mapId);\r\n        } catch (IOException ie) {\r\n            reporter.reportException(ie);\r\n        }\r\n    }\r\n    checkAndInformMRAppMaster(failures, mapId, readError, connectExcpt, hostFail);\r\n    checkReducerHealth();\r\n    long delay = (long) (INITIAL_PENALTY * Math.pow(PENALTY_GROWTH_RATE, failures));\r\n    penalize(host, Math.min(delay, maxPenalty));\r\n    failedShuffleCounter.increment(1);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\task\\reduce",
  "methodName" : "penalize",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void penalize(MapHost host, long delay)\n{\r\n    host.penalize();\r\n    penalties.add(new Penalty(host, delay));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\task\\reduce",
  "methodName" : "reportLocalError",
  "errType" : [ "UnknownHostException" ],
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void reportLocalError(IOException ioe)\n{\r\n    try {\r\n        LOG.error(\"Shuffle failed : local error on this node: \" + InetAddress.getLocalHost());\r\n    } catch (UnknownHostException e) {\r\n        LOG.error(\"Shuffle failed : local error on this node\");\r\n    }\r\n    reporter.reportException(ioe);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\task\\reduce",
  "methodName" : "checkAndInformMRAppMaster",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void checkAndInformMRAppMaster(int failures, TaskAttemptID mapId, boolean readError, boolean connectExcpt, boolean hostFailed)\n{\r\n    if (connectExcpt || (reportReadErrorImmediately && readError) || ((failures % maxFetchFailuresBeforeReporting) == 0) || hostFailed) {\r\n        LOG.info(\"Reporting fetch failure for \" + mapId + \" to MRAppMaster.\");\r\n        status.addFetchFailedMap((org.apache.hadoop.mapred.TaskAttemptID) mapId);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\task\\reduce",
  "methodName" : "checkReducerHealth",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void checkReducerHealth()\n{\r\n    final float MAX_ALLOWED_FAILED_FETCH_ATTEMPT_PERCENT = 0.5f;\r\n    final float MIN_REQUIRED_PROGRESS_PERCENT = 0.5f;\r\n    final float MAX_ALLOWED_STALL_TIME_PERCENT = 0.5f;\r\n    long totalFailures = failedShuffleCounter.getValue();\r\n    int doneMaps = totalMaps - remainingMaps;\r\n    boolean reducerHealthy = (((float) totalFailures / (totalFailures + doneMaps)) < MAX_ALLOWED_FAILED_FETCH_ATTEMPT_PERCENT);\r\n    boolean reducerProgressedEnough = (((float) doneMaps / totalMaps) >= MIN_REQUIRED_PROGRESS_PERCENT);\r\n    int stallDuration = (int) (Time.monotonicNow() - lastProgressTime);\r\n    int shuffleProgressDuration = (int) (lastProgressTime - startTime);\r\n    int minShuffleRunDuration = Math.max(shuffleProgressDuration, maxMapRuntime);\r\n    boolean reducerStalled = (((float) stallDuration / minShuffleRunDuration) >= MAX_ALLOWED_STALL_TIME_PERCENT);\r\n    if ((failureCounts.size() >= maxFailedUniqueFetches || failureCounts.size() == (totalMaps - doneMaps)) && !reducerHealthy && (!reducerProgressedEnough || reducerStalled)) {\r\n        LOG.error(\"Shuffle failed with too many fetch failures \" + \"and insufficient progress!\");\r\n        String errorMsg = \"Exceeded MAX_FAILED_UNIQUE_FETCHES; bailing-out.\";\r\n        reporter.reportException(new IOException(errorMsg));\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\task\\reduce",
  "methodName" : "tipFailed",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void tipFailed(TaskID taskId)\n{\r\n    if (!finishedMaps[taskId.getId()]) {\r\n        finishedMaps[taskId.getId()] = true;\r\n        if (--remainingMaps == 0) {\r\n            notifyAll();\r\n        }\r\n        updateStatus();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\task\\reduce",
  "methodName" : "addKnownMapOutput",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void addKnownMapOutput(String hostName, String hostUrl, TaskAttemptID mapId)\n{\r\n    MapHost host = mapLocations.get(hostName);\r\n    if (host == null) {\r\n        host = new MapHost(hostName, hostUrl);\r\n        mapLocations.put(hostName, host);\r\n    }\r\n    host.addKnownMap(mapId);\r\n    if (host.getState() == State.PENDING) {\r\n        pendingHosts.add(host);\r\n        notifyAll();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\task\\reduce",
  "methodName" : "obsoleteMapOutput",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void obsoleteMapOutput(TaskAttemptID mapId)\n{\r\n    obsoleteMaps.add(mapId);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\task\\reduce",
  "methodName" : "putBackKnownMapOutput",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void putBackKnownMapOutput(MapHost host, TaskAttemptID mapId)\n{\r\n    host.addKnownMap(mapId);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\task\\reduce",
  "methodName" : "getHost",
  "errType" : null,
  "containingMethodsNum" : 11,
  "sourceCodeText" : "MapHost getHost() throws InterruptedException\n{\r\n    while (pendingHosts.isEmpty()) {\r\n        wait();\r\n    }\r\n    Iterator<MapHost> iter = pendingHosts.iterator();\r\n    MapHost host = iter.next();\r\n    int numToPick = random.nextInt(pendingHosts.size());\r\n    for (int i = 0; i < numToPick; ++i) {\r\n        host = iter.next();\r\n    }\r\n    pendingHosts.remove(host);\r\n    host.markBusy();\r\n    if (LOG.isDebugEnabled()) {\r\n        LOG.debug(\"Assigning \" + host + \" with \" + host.getNumKnownMapOutputs() + \" to \" + Thread.currentThread().getName());\r\n    }\r\n    SHUFFLE_START.set(Time.monotonicNow());\r\n    return host;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\task\\reduce",
  "methodName" : "getMapsForHost",
  "errType" : null,
  "containingMethodsNum" : 15,
  "sourceCodeText" : "List<TaskAttemptID> getMapsForHost(MapHost host)\n{\r\n    List<TaskAttemptID> list = host.getAndClearKnownMaps();\r\n    Iterator<TaskAttemptID> itr = list.iterator();\r\n    List<TaskAttemptID> result = new ArrayList<TaskAttemptID>();\r\n    int includedMaps = 0;\r\n    int totalSize = list.size();\r\n    while (itr.hasNext()) {\r\n        TaskAttemptID id = itr.next();\r\n        if (!obsoleteMaps.contains(id) && !finishedMaps[id.getTaskID().getId()]) {\r\n            result.add(id);\r\n            if (++includedMaps >= MAX_MAPS_AT_ONCE) {\r\n                break;\r\n            }\r\n        }\r\n    }\r\n    while (itr.hasNext()) {\r\n        TaskAttemptID id = itr.next();\r\n        if (!obsoleteMaps.contains(id) && !finishedMaps[id.getTaskID().getId()]) {\r\n            host.addKnownMap(id);\r\n        }\r\n    }\r\n    if (LOG.isDebugEnabled()) {\r\n        LOG.debug(\"assigned \" + includedMaps + \" of \" + totalSize + \" to \" + host + \" to \" + Thread.currentThread().getName());\r\n    }\r\n    return result;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\task\\reduce",
  "methodName" : "freeHost",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void freeHost(MapHost host)\n{\r\n    if (host.getState() != State.PENALIZED) {\r\n        if (host.markAvailable() == State.PENDING) {\r\n            pendingHosts.add(host);\r\n            notifyAll();\r\n        }\r\n    }\r\n    LOG.info(host + \" freed by \" + Thread.currentThread().getName() + \" in \" + (Time.monotonicNow() - SHUFFLE_START.get()) + \"ms\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\task\\reduce",
  "methodName" : "resetKnownMaps",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void resetKnownMaps()\n{\r\n    mapLocations.clear();\r\n    obsoleteMaps.clear();\r\n    pendingHosts.clear();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\task\\reduce",
  "methodName" : "waitUntilDone",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "boolean waitUntilDone(int millis) throws InterruptedException\n{\r\n    if (remainingMaps > 0) {\r\n        wait(millis);\r\n        return remainingMaps == 0;\r\n    }\r\n    return true;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\task\\reduce",
  "methodName" : "close",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void close() throws InterruptedException\n{\r\n    referee.interrupt();\r\n    referee.join();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\task\\reduce",
  "methodName" : "getMaxHostFailures",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int getMaxHostFailures()\n{\r\n    return maxHostFailures;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output",
  "methodName" : "setOutputFormatClass",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void setOutputFormatClass(Job job, Class<? extends OutputFormat> theClass)\n{\r\n    job.setOutputFormatClass(LazyOutputFormat.class);\r\n    job.getConfiguration().setClass(OUTPUT_FORMAT, theClass, OutputFormat.class);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output",
  "methodName" : "getBaseOutputFormat",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void getBaseOutputFormat(Configuration conf) throws IOException\n{\r\n    baseOut = ((OutputFormat<K, V>) ReflectionUtils.newInstance(conf.getClass(OUTPUT_FORMAT, null), conf));\r\n    if (baseOut == null) {\r\n        throw new IOException(\"Output Format not set for LazyOutputFormat\");\r\n    }\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output",
  "methodName" : "getRecordWriter",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "RecordWriter<K, V> getRecordWriter(TaskAttemptContext context) throws IOException, InterruptedException\n{\r\n    if (baseOut == null) {\r\n        getBaseOutputFormat(context.getConfiguration());\r\n    }\r\n    return new LazyRecordWriter<K, V>(baseOut, context);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output",
  "methodName" : "checkOutputSpecs",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void checkOutputSpecs(JobContext context) throws IOException, InterruptedException\n{\r\n    if (baseOut == null) {\r\n        getBaseOutputFormat(context.getConfiguration());\r\n    }\r\n    super.checkOutputSpecs(context);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output",
  "methodName" : "getOutputCommitter",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "OutputCommitter getOutputCommitter(TaskAttemptContext context) throws IOException, InterruptedException\n{\r\n    if (baseOut == null) {\r\n        getBaseOutputFormat(context.getConfiguration());\r\n    }\r\n    return super.getOutputCommitter(context);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getOutputFile",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Path getOutputFile() throws IOException\n{\r\n    return lDirAlloc.getLocalPathToRead(MRJobConfig.OUTPUT + Path.SEPARATOR + MAP_OUTPUT_FILENAME_STRING, getConf());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getOutputFileForWrite",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Path getOutputFileForWrite(long size) throws IOException\n{\r\n    return lDirAlloc.getLocalPathForWrite(MRJobConfig.OUTPUT + Path.SEPARATOR + MAP_OUTPUT_FILENAME_STRING, size, getConf());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getOutputFileForWriteInVolume",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Path getOutputFileForWriteInVolume(Path existing)\n{\r\n    return new Path(existing.getParent(), MAP_OUTPUT_FILENAME_STRING);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getOutputIndexFile",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Path getOutputIndexFile() throws IOException\n{\r\n    return lDirAlloc.getLocalPathToRead(MRJobConfig.OUTPUT + Path.SEPARATOR + MAP_OUTPUT_FILENAME_STRING + MAP_OUTPUT_INDEX_SUFFIX_STRING, getConf());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getOutputIndexFileForWrite",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Path getOutputIndexFileForWrite(long size) throws IOException\n{\r\n    return lDirAlloc.getLocalPathForWrite(MRJobConfig.OUTPUT + Path.SEPARATOR + MAP_OUTPUT_FILENAME_STRING + MAP_OUTPUT_INDEX_SUFFIX_STRING, size, getConf());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getOutputIndexFileForWriteInVolume",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Path getOutputIndexFileForWriteInVolume(Path existing)\n{\r\n    return new Path(existing.getParent(), MAP_OUTPUT_FILENAME_STRING + MAP_OUTPUT_INDEX_SUFFIX_STRING);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getSpillFile",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Path getSpillFile(int spillNumber) throws IOException\n{\r\n    return lDirAlloc.getLocalPathToRead(MRJobConfig.OUTPUT + \"/spill\" + spillNumber + \".out\", getConf());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getSpillFileForWrite",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Path getSpillFileForWrite(int spillNumber, long size) throws IOException\n{\r\n    return lDirAlloc.getLocalPathForWrite(MRJobConfig.OUTPUT + \"/spill\" + spillNumber + \".out\", size, getConf());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getSpillIndexFile",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Path getSpillIndexFile(int spillNumber) throws IOException\n{\r\n    return lDirAlloc.getLocalPathToRead(MRJobConfig.OUTPUT + \"/spill\" + spillNumber + \".out.index\", getConf());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getSpillIndexFileForWrite",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Path getSpillIndexFileForWrite(int spillNumber, long size) throws IOException\n{\r\n    return lDirAlloc.getLocalPathForWrite(MRJobConfig.OUTPUT + \"/spill\" + spillNumber + \".out.index\", size, getConf());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getInputFile",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Path getInputFile(int mapId) throws IOException\n{\r\n    return lDirAlloc.getLocalPathToRead(String.format(REDUCE_INPUT_FILE_FORMAT_STRING, MRJobConfig.OUTPUT, Integer.valueOf(mapId)), getConf());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getInputFileForWrite",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Path getInputFileForWrite(org.apache.hadoop.mapreduce.TaskID mapId, long size) throws IOException\n{\r\n    return lDirAlloc.getLocalPathForWrite(String.format(REDUCE_INPUT_FILE_FORMAT_STRING, MRJobConfig.OUTPUT, mapId.getId()), size, getConf());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "removeAll",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void removeAll() throws IOException\n{\r\n    ((JobConf) getConf()).deleteLocalFiles(MRJobConfig.OUTPUT);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "setConf",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setConf(Configuration conf)\n{\r\n    if (!(conf instanceof JobConf)) {\r\n        conf = new JobConf(conf);\r\n    }\r\n    super.setConf(conf);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\util",
  "methodName" : "getBundle",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "ResourceBundle getBundle(String bundleName)\n{\r\n    return ResourceBundle.getBundle(bundleName.replace('$', '_'), Locale.getDefault(), Thread.currentThread().getContextClassLoader());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\util",
  "methodName" : "getValue",
  "errType" : [ "Exception" ],
  "containingMethodsNum" : 2,
  "sourceCodeText" : "T getValue(String bundleName, String key, String suffix, T defaultValue)\n{\r\n    T value;\r\n    try {\r\n        ResourceBundle bundle = getBundle(bundleName);\r\n        value = (T) bundle.getObject(getLookupKey(key, suffix));\r\n    } catch (Exception e) {\r\n        return defaultValue;\r\n    }\r\n    return value;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\util",
  "methodName" : "getLookupKey",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String getLookupKey(String key, String suffix)\n{\r\n    if (suffix == null || suffix.isEmpty())\r\n        return key;\r\n    return key + suffix;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\util",
  "methodName" : "getCounterGroupName",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String getCounterGroupName(String group, String defaultValue)\n{\r\n    return getValue(group, \"CounterGroupName\", \"\", defaultValue);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\util",
  "methodName" : "getCounterName",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String getCounterName(String group, String counter, String defaultValue)\n{\r\n    return getValue(group, counter, \".name\", defaultValue);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\filecache",
  "methodName" : "determineTimestampsAndCacheVisibilities",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void determineTimestampsAndCacheVisibilities(Configuration job) throws IOException\n{\r\n    Map<URI, FileStatus> statCache = new HashMap<URI, FileStatus>();\r\n    determineTimestampsAndCacheVisibilities(job, statCache);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\filecache",
  "methodName" : "determineTimestampsAndCacheVisibilities",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void determineTimestampsAndCacheVisibilities(Configuration job, Map<URI, FileStatus> statCache) throws IOException\n{\r\n    determineTimestamps(job, statCache);\r\n    determineCacheVisibilities(job, statCache);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\filecache",
  "methodName" : "determineTimestamps",
  "errType" : null,
  "containingMethodsNum" : 22,
  "sourceCodeText" : "void determineTimestamps(Configuration job, Map<URI, FileStatus> statCache) throws IOException\n{\r\n    URI[] tarchives = JobContextImpl.getCacheArchives(job);\r\n    if (tarchives != null) {\r\n        FileStatus status = getFileStatus(job, tarchives[0], statCache);\r\n        StringBuilder archiveFileSizes = new StringBuilder(String.valueOf(status.getLen()));\r\n        StringBuilder archiveTimestamps = new StringBuilder(String.valueOf(status.getModificationTime()));\r\n        for (int i = 1; i < tarchives.length; i++) {\r\n            status = getFileStatus(job, tarchives[i], statCache);\r\n            archiveFileSizes.append(\",\");\r\n            archiveFileSizes.append(String.valueOf(status.getLen()));\r\n            archiveTimestamps.append(\",\");\r\n            archiveTimestamps.append(String.valueOf(status.getModificationTime()));\r\n        }\r\n        job.set(MRJobConfig.CACHE_ARCHIVES_SIZES, archiveFileSizes.toString());\r\n        setArchiveTimestamps(job, archiveTimestamps.toString());\r\n    }\r\n    URI[] tfiles = JobContextImpl.getCacheFiles(job);\r\n    if (tfiles != null) {\r\n        FileStatus status = getFileStatus(job, tfiles[0], statCache);\r\n        StringBuilder fileSizes = new StringBuilder(String.valueOf(status.getLen()));\r\n        StringBuilder fileTimestamps = new StringBuilder(String.valueOf(status.getModificationTime()));\r\n        for (int i = 1; i < tfiles.length; i++) {\r\n            status = getFileStatus(job, tfiles[i], statCache);\r\n            fileSizes.append(\",\");\r\n            fileSizes.append(String.valueOf(status.getLen()));\r\n            fileTimestamps.append(\",\");\r\n            fileTimestamps.append(String.valueOf(status.getModificationTime()));\r\n        }\r\n        job.set(MRJobConfig.CACHE_FILES_SIZES, fileSizes.toString());\r\n        setFileTimestamps(job, fileTimestamps.toString());\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\filecache",
  "methodName" : "getDelegationTokens",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void getDelegationTokens(Configuration job, Credentials credentials) throws IOException\n{\r\n    URI[] tarchives = JobContextImpl.getCacheArchives(job);\r\n    URI[] tfiles = JobContextImpl.getCacheFiles(job);\r\n    int size = (tarchives != null ? tarchives.length : 0) + (tfiles != null ? tfiles.length : 0);\r\n    Path[] ps = new Path[size];\r\n    int i = 0;\r\n    if (tarchives != null) {\r\n        for (i = 0; i < tarchives.length; i++) {\r\n            ps[i] = new Path(tarchives[i].toString());\r\n        }\r\n    }\r\n    if (tfiles != null) {\r\n        for (int j = 0; j < tfiles.length; j++) {\r\n            ps[i + j] = new Path(tfiles[j].toString());\r\n        }\r\n    }\r\n    TokenCache.obtainTokensForNamenodes(credentials, ps, job);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\filecache",
  "methodName" : "determineCacheVisibilities",
  "errType" : null,
  "containingMethodsNum" : 10,
  "sourceCodeText" : "void determineCacheVisibilities(Configuration job, Map<URI, FileStatus> statCache) throws IOException\n{\r\n    URI[] tarchives = JobContextImpl.getCacheArchives(job);\r\n    if (tarchives != null) {\r\n        StringBuilder archiveVisibilities = new StringBuilder(String.valueOf(isPublic(job, tarchives[0], statCache)));\r\n        for (int i = 1; i < tarchives.length; i++) {\r\n            archiveVisibilities.append(\",\");\r\n            archiveVisibilities.append(String.valueOf(isPublic(job, tarchives[i], statCache)));\r\n        }\r\n        setArchiveVisibilities(job, archiveVisibilities.toString());\r\n    }\r\n    URI[] tfiles = JobContextImpl.getCacheFiles(job);\r\n    if (tfiles != null) {\r\n        StringBuilder fileVisibilities = new StringBuilder(String.valueOf(isPublic(job, tfiles[0], statCache)));\r\n        for (int i = 1; i < tfiles.length; i++) {\r\n            fileVisibilities.append(\",\");\r\n            fileVisibilities.append(String.valueOf(isPublic(job, tfiles[i], statCache)));\r\n        }\r\n        setFileVisibilities(job, fileVisibilities.toString());\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\filecache",
  "methodName" : "setArchiveVisibilities",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setArchiveVisibilities(Configuration conf, String booleans)\n{\r\n    conf.set(MRJobConfig.CACHE_ARCHIVES_VISIBILITIES, booleans);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\filecache",
  "methodName" : "setFileVisibilities",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setFileVisibilities(Configuration conf, String booleans)\n{\r\n    conf.set(MRJobConfig.CACHE_FILE_VISIBILITIES, booleans);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\filecache",
  "methodName" : "setArchiveTimestamps",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setArchiveTimestamps(Configuration conf, String timestamps)\n{\r\n    conf.set(MRJobConfig.CACHE_ARCHIVES_TIMESTAMPS, timestamps);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\filecache",
  "methodName" : "setFileTimestamps",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setFileTimestamps(Configuration conf, String timestamps)\n{\r\n    conf.set(MRJobConfig.CACHE_FILE_TIMESTAMPS, timestamps);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\filecache",
  "methodName" : "getFileStatus",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "FileStatus getFileStatus(Configuration job, URI uri, Map<URI, FileStatus> statCache) throws IOException\n{\r\n    FileSystem fileSystem = FileSystem.get(uri, job);\r\n    return getFileStatus(fileSystem, uri, statCache);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\filecache",
  "methodName" : "isPublic",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "boolean isPublic(Configuration conf, URI uri, Map<URI, FileStatus> statCache) throws IOException\n{\r\n    boolean isPublic = true;\r\n    FileSystem fs = FileSystem.get(uri, conf);\r\n    Path current = new Path(uri.getPath());\r\n    current = fs.makeQualified(current);\r\n    if (!current.getName().equals(DistributedCache.WILDCARD)) {\r\n        isPublic = checkPermissionOfOther(fs, current, FsAction.READ, statCache);\r\n    }\r\n    return isPublic && ancestorsHaveExecutePermissions(fs, current.getParent(), statCache);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\filecache",
  "methodName" : "ancestorsHaveExecutePermissions",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "boolean ancestorsHaveExecutePermissions(FileSystem fs, Path path, Map<URI, FileStatus> statCache) throws IOException\n{\r\n    Path current = path;\r\n    while (current != null) {\r\n        if (!checkPermissionOfOther(fs, current, FsAction.EXECUTE, statCache)) {\r\n            return false;\r\n        }\r\n        current = current.getParent();\r\n    }\r\n    return true;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\filecache",
  "methodName" : "checkPermissionOfOther",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "boolean checkPermissionOfOther(FileSystem fs, Path path, FsAction action, Map<URI, FileStatus> statCache) throws IOException\n{\r\n    FileStatus status = getFileStatus(fs, path.toUri(), statCache);\r\n    if (!status.isEncrypted()) {\r\n        FsAction otherAction = status.getPermission().getOtherAction();\r\n        if (otherAction.implies(action)) {\r\n            return true;\r\n        }\r\n    }\r\n    return false;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\filecache",
  "methodName" : "getFileStatus",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "FileStatus getFileStatus(FileSystem fs, URI uri, Map<URI, FileStatus> statCache) throws IOException\n{\r\n    Path path = new Path(uri);\r\n    if (path.getName().equals(DistributedCache.WILDCARD)) {\r\n        path = path.getParent();\r\n        uri = path.toUri();\r\n    }\r\n    FileStatus stat = statCache.get(uri);\r\n    if (stat == null) {\r\n        stat = fs.getFileStatus(path);\r\n        statCache.put(uri, stat);\r\n    }\r\n    return stat;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getQueueConfigurationParser",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 6,
  "sourceCodeText" : "QueueConfigurationParser getQueueConfigurationParser(Configuration conf, boolean reloadConf, boolean areAclsEnabled)\n{\r\n    if (conf != null && conf.get(DeprecatedQueueConfigurationParser.MAPRED_QUEUE_NAMES_KEY) != null) {\r\n        if (reloadConf) {\r\n            conf.reloadConfiguration();\r\n        }\r\n        return new DeprecatedQueueConfigurationParser(conf);\r\n    } else {\r\n        URL xmlInUrl = Thread.currentThread().getContextClassLoader().getResource(QUEUE_CONF_FILE_NAME);\r\n        if (xmlInUrl == null) {\r\n            xmlInUrl = Thread.currentThread().getContextClassLoader().getResource(QUEUE_CONF_DEFAULT_FILE_NAME);\r\n            assert xmlInUrl != null;\r\n        }\r\n        InputStream stream = null;\r\n        try {\r\n            stream = xmlInUrl.openStream();\r\n            return new QueueConfigurationParser(new BufferedInputStream(stream), areAclsEnabled);\r\n        } catch (IOException ioe) {\r\n            throw new RuntimeException(\"Couldn't open queue configuration at \" + xmlInUrl, ioe);\r\n        } finally {\r\n            IOUtils.closeStream(stream);\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "initialize",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void initialize(QueueConfigurationParser cp)\n{\r\n    this.root = cp.getRoot();\r\n    leafQueues.clear();\r\n    allQueues.clear();\r\n    leafQueues = getRoot().getLeafQueues();\r\n    allQueues.putAll(getRoot().getInnerQueues());\r\n    allQueues.putAll(leafQueues);\r\n    LOG.info(\"AllQueues : \" + allQueues + \"; LeafQueues : \" + leafQueues);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getLeafQueueNames",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Set<String> getLeafQueueNames()\n{\r\n    return leafQueues.keySet();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "hasAccess",
  "errType" : null,
  "containingMethodsNum" : 10,
  "sourceCodeText" : "boolean hasAccess(String queueName, QueueACL qACL, UserGroupInformation ugi)\n{\r\n    Queue q = leafQueues.get(queueName);\r\n    if (q == null) {\r\n        LOG.info(\"Queue \" + queueName + \" is not present\");\r\n        return false;\r\n    }\r\n    if (q.getChildren() != null && !q.getChildren().isEmpty()) {\r\n        LOG.info(\"Cannot submit job to parent queue \" + q.getName());\r\n        return false;\r\n    }\r\n    if (!areAclsEnabled()) {\r\n        return true;\r\n    }\r\n    if (LOG.isDebugEnabled()) {\r\n        LOG.debug(\"Checking access for the acl \" + toFullPropertyName(queueName, qACL.getAclName()) + \" for user \" + ugi.getShortUserName());\r\n    }\r\n    AccessControlList acl = q.getAcls().get(toFullPropertyName(queueName, qACL.getAclName()));\r\n    if (acl == null) {\r\n        return false;\r\n    }\r\n    return acl.isUserAllowed(ugi);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "isRunning",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "boolean isRunning(String queueName)\n{\r\n    Queue q = leafQueues.get(queueName);\r\n    if (q != null) {\r\n        return q.getState().equals(QueueState.RUNNING);\r\n    }\r\n    return false;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "setSchedulerInfo",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void setSchedulerInfo(String queueName, Object queueInfo)\n{\r\n    if (allQueues.get(queueName) != null) {\r\n        allQueues.get(queueName).setSchedulingInfo(queueInfo);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getSchedulerInfo",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "Object getSchedulerInfo(String queueName)\n{\r\n    if (allQueues.get(queueName) != null) {\r\n        return allQueues.get(queueName).getSchedulingInfo();\r\n    }\r\n    return null;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "refreshQueues",
  "errType" : [ "Throwable" ],
  "containingMethodsNum" : 12,
  "sourceCodeText" : "void refreshQueues(Configuration conf, QueueRefresher schedulerRefresher) throws IOException\n{\r\n    QueueConfigurationParser cp = getQueueConfigurationParser(conf, true, areAclsEnabled);\r\n    if (!root.isHierarchySameAs(cp.getRoot())) {\r\n        LOG.warn(MSG_REFRESH_FAILURE_WITH_CHANGE_OF_HIERARCHY);\r\n        throw new IOException(MSG_REFRESH_FAILURE_WITH_CHANGE_OF_HIERARCHY);\r\n    }\r\n    if (schedulerRefresher != null) {\r\n        try {\r\n            schedulerRefresher.refreshQueues(cp.getRoot().getJobQueueInfo().getChildren());\r\n        } catch (Throwable e) {\r\n            StringBuilder msg = new StringBuilder(\"Scheduler's refresh-queues failed with the exception : \" + StringUtils.stringifyException(e));\r\n            msg.append(\"\\n\");\r\n            msg.append(MSG_REFRESH_FAILURE_WITH_SCHEDULER_FAILURE);\r\n            LOG.error(msg.toString());\r\n            throw new IOException(msg.toString());\r\n        }\r\n    }\r\n    cp.getRoot().copySchedulingInfo(this.root);\r\n    initialize(cp);\r\n    LOG.info(\"Queue configuration is refreshed successfully.\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "toFullPropertyName",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String toFullPropertyName(String queue, String property)\n{\r\n    return QUEUE_CONF_PROPERTY_NAME_PREFIX + queue + \".\" + property;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getJobQueueInfos",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "JobQueueInfo[] getJobQueueInfos()\n{\r\n    ArrayList<JobQueueInfo> queueInfoList = new ArrayList<JobQueueInfo>();\r\n    for (String queue : allQueues.keySet()) {\r\n        JobQueueInfo queueInfo = getJobQueueInfo(queue);\r\n        if (queueInfo != null) {\r\n            queueInfoList.add(queueInfo);\r\n        }\r\n    }\r\n    return queueInfoList.toArray(new JobQueueInfo[queueInfoList.size()]);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getJobQueueInfo",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "JobQueueInfo getJobQueueInfo(String queue)\n{\r\n    if (allQueues.containsKey(queue)) {\r\n        return allQueues.get(queue).getJobQueueInfo();\r\n    }\r\n    return null;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getJobQueueInfoMapping",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "Map<String, JobQueueInfo> getJobQueueInfoMapping()\n{\r\n    Map<String, JobQueueInfo> m = new HashMap<String, JobQueueInfo>();\r\n    for (Map.Entry<String, Queue> entry : allQueues.entrySet()) {\r\n        m.put(entry.getKey(), entry.getValue().getJobQueueInfo());\r\n    }\r\n    return m;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getQueueAcls",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "QueueAclsInfo[] getQueueAcls(UserGroupInformation ugi) throws IOException\n{\r\n    ArrayList<QueueAclsInfo> queueAclsInfolist = new ArrayList<QueueAclsInfo>();\r\n    QueueACL[] qAcls = QueueACL.values();\r\n    for (String queueName : leafQueues.keySet()) {\r\n        QueueAclsInfo queueAclsInfo = null;\r\n        ArrayList<String> operationsAllowed = null;\r\n        for (QueueACL qAcl : qAcls) {\r\n            if (hasAccess(queueName, qAcl, ugi)) {\r\n                if (operationsAllowed == null) {\r\n                    operationsAllowed = new ArrayList<String>();\r\n                }\r\n                operationsAllowed.add(qAcl.getAclName());\r\n            }\r\n        }\r\n        if (operationsAllowed != null) {\r\n            queueAclsInfo = new QueueAclsInfo(queueName, operationsAllowed.toArray(new String[operationsAllowed.size()]));\r\n            queueAclsInfolist.add(queueAclsInfo);\r\n        }\r\n    }\r\n    return queueAclsInfolist.toArray(new QueueAclsInfo[queueAclsInfolist.size()]);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "areAclsEnabled",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean areAclsEnabled()\n{\r\n    return areAclsEnabled;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getRoot",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Queue getRoot()\n{\r\n    return root;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "dumpConfiguration",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void dumpConfiguration(Writer out, Configuration conf) throws IOException\n{\r\n    dumpConfiguration(out, null, conf);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "dumpConfiguration",
  "errType" : null,
  "containingMethodsNum" : 12,
  "sourceCodeText" : "void dumpConfiguration(Writer out, String configFile, Configuration conf) throws IOException\n{\r\n    if (conf != null && conf.get(DeprecatedQueueConfigurationParser.MAPRED_QUEUE_NAMES_KEY) != null) {\r\n        return;\r\n    }\r\n    JsonFactory dumpFactory = new JsonFactory();\r\n    JsonGenerator dumpGenerator = dumpFactory.createGenerator(out);\r\n    QueueConfigurationParser parser;\r\n    boolean aclsEnabled = false;\r\n    if (conf != null) {\r\n        aclsEnabled = conf.getBoolean(MRConfig.MR_ACLS_ENABLED, false);\r\n    }\r\n    if (configFile != null && !\"\".equals(configFile)) {\r\n        parser = new QueueConfigurationParser(configFile, aclsEnabled);\r\n    } else {\r\n        parser = getQueueConfigurationParser(null, false, aclsEnabled);\r\n    }\r\n    dumpGenerator.writeStartObject();\r\n    dumpGenerator.writeFieldName(\"queues\");\r\n    dumpGenerator.writeStartArray();\r\n    dumpConfiguration(dumpGenerator, parser.getRoot().getChildren());\r\n    dumpGenerator.writeEndArray();\r\n    dumpGenerator.writeEndObject();\r\n    dumpGenerator.flush();\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "dumpConfiguration",
  "errType" : null,
  "containingMethodsNum" : 26,
  "sourceCodeText" : "void dumpConfiguration(JsonGenerator dumpGenerator, Set<Queue> rootQueues) throws JsonGenerationException, IOException\n{\r\n    for (Queue queue : rootQueues) {\r\n        dumpGenerator.writeStartObject();\r\n        dumpGenerator.writeStringField(\"name\", queue.getName());\r\n        dumpGenerator.writeStringField(\"state\", queue.getState().toString());\r\n        AccessControlList submitJobList = null;\r\n        AccessControlList administerJobsList = null;\r\n        if (queue.getAcls() != null) {\r\n            submitJobList = queue.getAcls().get(toFullPropertyName(queue.getName(), QueueACL.SUBMIT_JOB.getAclName()));\r\n            administerJobsList = queue.getAcls().get(toFullPropertyName(queue.getName(), QueueACL.ADMINISTER_JOBS.getAclName()));\r\n        }\r\n        String aclsSubmitJobValue = \" \";\r\n        if (submitJobList != null) {\r\n            aclsSubmitJobValue = submitJobList.getAclString();\r\n        }\r\n        dumpGenerator.writeStringField(\"acl_submit_job\", aclsSubmitJobValue);\r\n        String aclsAdministerValue = \" \";\r\n        if (administerJobsList != null) {\r\n            aclsAdministerValue = administerJobsList.getAclString();\r\n        }\r\n        dumpGenerator.writeStringField(\"acl_administer_jobs\", aclsAdministerValue);\r\n        dumpGenerator.writeFieldName(\"properties\");\r\n        dumpGenerator.writeStartArray();\r\n        if (queue.getProperties() != null) {\r\n            for (Map.Entry<Object, Object> property : queue.getProperties().entrySet()) {\r\n                dumpGenerator.writeStartObject();\r\n                dumpGenerator.writeStringField(\"key\", (String) property.getKey());\r\n                dumpGenerator.writeStringField(\"value\", (String) property.getValue());\r\n                dumpGenerator.writeEndObject();\r\n            }\r\n        }\r\n        dumpGenerator.writeEndArray();\r\n        Set<Queue> childQueues = queue.getChildren();\r\n        dumpGenerator.writeFieldName(\"children\");\r\n        dumpGenerator.writeStartArray();\r\n        if (childQueues != null && childQueues.size() > 0) {\r\n            dumpConfiguration(dumpGenerator, childQueues);\r\n        }\r\n        dumpGenerator.writeEndArray();\r\n        dumpGenerator.writeEndObject();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getFileStatuses",
  "errType" : null,
  "containingMethodsNum" : 18,
  "sourceCodeText" : "Iterable<FileStatus> getFileStatuses() throws InterruptedException, IOException\n{\r\n    runningTasks.incrementAndGet();\r\n    for (Path p : inputDirs) {\r\n        LOG.debug(\"Queuing scan of directory {}\", p);\r\n        runningTasks.incrementAndGet();\r\n        ListenableFuture<ProcessInitialInputPathCallable.Result> future = exec.submit(new ProcessInitialInputPathCallable(p, conf, inputFilter));\r\n        Futures.addCallback(future, processInitialInputPathCallback, MoreExecutors.directExecutor());\r\n    }\r\n    runningTasks.decrementAndGet();\r\n    lock.lock();\r\n    try {\r\n        LOG.debug(\"Waiting scan completion\");\r\n        while (runningTasks.get() != 0 && unknownError == null) {\r\n            condition.await();\r\n        }\r\n    } finally {\r\n        lock.unlock();\r\n        LOG.debug(\"Scan complete: shutting down\");\r\n        this.exec.shutdownNow();\r\n    }\r\n    if (this.unknownError != null) {\r\n        LOG.debug(\"Scan failed\", this.unknownError);\r\n        if (this.unknownError instanceof Error) {\r\n            throw (Error) this.unknownError;\r\n        } else if (this.unknownError instanceof RuntimeException) {\r\n            throw (RuntimeException) this.unknownError;\r\n        } else if (this.unknownError instanceof IOException) {\r\n            throw (IOException) this.unknownError;\r\n        } else if (this.unknownError instanceof InterruptedException) {\r\n            throw (InterruptedException) this.unknownError;\r\n        } else {\r\n            throw new IOException(this.unknownError);\r\n        }\r\n    }\r\n    if (!this.invalidInputErrors.isEmpty()) {\r\n        LOG.debug(\"Invalid Input Errors raised\");\r\n        for (IOException error : invalidInputErrors) {\r\n            LOG.debug(\"Error\", error);\r\n        }\r\n        if (this.newApi) {\r\n            throw new org.apache.hadoop.mapreduce.lib.input.InvalidInputException(invalidInputErrors);\r\n        } else {\r\n            throw new InvalidInputException(invalidInputErrors);\r\n        }\r\n    }\r\n    return Iterables.concat(resultQueue);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "registerInvalidInputError",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void registerInvalidInputError(List<IOException> errors)\n{\r\n    synchronized (this) {\r\n        this.invalidInputErrors.addAll(errors);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "registerError",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void registerError(Throwable t)\n{\r\n    LOG.debug(\"Error\", t);\r\n    lock.lock();\r\n    try {\r\n        if (unknownError == null) {\r\n            unknownError = t;\r\n            condition.signal();\r\n        }\r\n    } finally {\r\n        lock.unlock();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "decrementRunningAndCheckCompletion",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void decrementRunningAndCheckCompletion()\n{\r\n    lock.lock();\r\n    try {\r\n        if (runningTasks.decrementAndGet() == 0) {\r\n            condition.signal();\r\n        }\r\n    } finally {\r\n        lock.unlock();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getIOStatistics",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "IOStatistics getIOStatistics()\n{\r\n    return iostats;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "addResultStatistics",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void addResultStatistics(IOStatistics stats)\n{\r\n    if (stats != null) {\r\n        synchronized (this) {\r\n            LOG.debug(\"Adding IOStatistics: {}\", stats);\r\n            if (iostats == null) {\r\n                iostats = snapshotIOStatistics(stats);\r\n            } else {\r\n                iostats.aggregate(stats);\r\n            }\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "toString",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "String toString()\n{\r\n    final IOStatistics ioStatistics = getIOStatistics();\r\n    StringJoiner stringJoiner = new StringJoiner(\", \", LocatedFileStatusFetcher.class.getSimpleName() + \"[\", \"]\");\r\n    if (ioStatistics != null) {\r\n        stringJoiner.add(\"IOStatistics=\" + ioStatistics);\r\n    }\r\n    return stringJoiner.toString();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getListeningExecutorService",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "ListeningExecutorService getListeningExecutorService()\n{\r\n    return exec;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\task\\reduce",
  "methodName" : "isPrimaryMapOutput",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean isPrimaryMapOutput()\n{\r\n    return primaryMapOutput;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\task\\reduce",
  "methodName" : "equals",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean equals(Object obj)\n{\r\n    if (obj instanceof MapOutput) {\r\n        return id == ((MapOutput) obj).id;\r\n    }\r\n    return false;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\task\\reduce",
  "methodName" : "hashCode",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int hashCode()\n{\r\n    return id;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\task\\reduce",
  "methodName" : "getMapId",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "TaskAttemptID getMapId()\n{\r\n    return mapId;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\task\\reduce",
  "methodName" : "getSize",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getSize()\n{\r\n    return size;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\task\\reduce",
  "methodName" : "shuffle",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void shuffle(MapHost host, InputStream input, long compressedLength, long decompressedLength, ShuffleClientMetrics metrics, Reporter reporter) throws IOException",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\task\\reduce",
  "methodName" : "commit",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void commit() throws IOException",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\task\\reduce",
  "methodName" : "abort",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void abort()",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\task\\reduce",
  "methodName" : "getDescription",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getDescription()",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\task\\reduce",
  "methodName" : "toString",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String toString()\n{\r\n    return \"MapOutput(\" + mapId + \", \" + getDescription() + \")\";\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "getDatum",
  "errType" : null,
  "containingMethodsNum" : 16,
  "sourceCodeText" : "Object getDatum()\n{\r\n    if (datum == null) {\r\n        datum = new ReduceAttemptFinished();\r\n        datum.setTaskid(new Utf8(attemptId.getTaskID().toString()));\r\n        datum.setAttemptId(new Utf8(attemptId.toString()));\r\n        datum.setTaskType(new Utf8(taskType.name()));\r\n        datum.setTaskStatus(new Utf8(taskStatus));\r\n        datum.setShuffleFinishTime(shuffleFinishTime);\r\n        datum.setSortFinishTime(sortFinishTime);\r\n        datum.setFinishTime(finishTime);\r\n        datum.setHostname(new Utf8(hostname));\r\n        datum.setPort(port);\r\n        if (rackName != null) {\r\n            datum.setRackname(new Utf8(rackName));\r\n        }\r\n        datum.setState(new Utf8(state));\r\n        datum.setCounters(EventWriter.toAvro(counters));\r\n        datum.setClockSplits(AvroArrayUtils.toAvro(ProgressSplitsBlock.arrayGetWallclockTime(allSplits)));\r\n        datum.setCpuUsages(AvroArrayUtils.toAvro(ProgressSplitsBlock.arrayGetCPUTime(allSplits)));\r\n        datum.setVMemKbytes(AvroArrayUtils.toAvro(ProgressSplitsBlock.arrayGetVMemKbytes(allSplits)));\r\n        datum.setPhysMemKbytes(AvroArrayUtils.toAvro(ProgressSplitsBlock.arrayGetPhysMemKbytes(allSplits)));\r\n    }\r\n    return datum;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "setDatum",
  "errType" : null,
  "containingMethodsNum" : 15,
  "sourceCodeText" : "void setDatum(Object oDatum)\n{\r\n    this.datum = (ReduceAttemptFinished) oDatum;\r\n    this.attemptId = TaskAttemptID.forName(datum.getAttemptId().toString());\r\n    this.taskType = TaskType.valueOf(datum.getTaskType().toString());\r\n    this.taskStatus = datum.getTaskStatus().toString();\r\n    this.shuffleFinishTime = datum.getShuffleFinishTime();\r\n    this.sortFinishTime = datum.getSortFinishTime();\r\n    this.finishTime = datum.getFinishTime();\r\n    this.hostname = datum.getHostname().toString();\r\n    this.rackName = datum.getRackname().toString();\r\n    this.port = datum.getPort();\r\n    this.state = datum.getState().toString();\r\n    this.counters = EventReader.fromAvro(datum.getCounters());\r\n    this.clockSplits = AvroArrayUtils.fromAvro(datum.getClockSplits());\r\n    this.cpuUsages = AvroArrayUtils.fromAvro(datum.getCpuUsages());\r\n    this.vMemKbytes = AvroArrayUtils.fromAvro(datum.getVMemKbytes());\r\n    this.physMemKbytes = AvroArrayUtils.fromAvro(datum.getPhysMemKbytes());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "getTaskId",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "TaskID getTaskId()\n{\r\n    return attemptId.getTaskID();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "getAttemptId",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "TaskAttemptID getAttemptId()\n{\r\n    return attemptId;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "getTaskType",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "TaskType getTaskType()\n{\r\n    return taskType;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "getTaskStatus",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String getTaskStatus()\n{\r\n    return taskStatus.toString();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "getSortFinishTime",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getSortFinishTime()\n{\r\n    return sortFinishTime;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "getShuffleFinishTime",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getShuffleFinishTime()\n{\r\n    return shuffleFinishTime;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "getFinishTime",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getFinishTime()\n{\r\n    return finishTime;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "getStartTime",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getStartTime()\n{\r\n    return startTime;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "getHostname",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String getHostname()\n{\r\n    return hostname.toString();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "getPort",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int getPort()\n{\r\n    return port;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "getRackName",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String getRackName()\n{\r\n    return rackName == null ? null : rackName.toString();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "getState",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String getState()\n{\r\n    return state.toString();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "getCounters",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Counters getCounters()\n{\r\n    return counters;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "getEventType",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "EventType getEventType()\n{\r\n    return EventType.REDUCE_ATTEMPT_FINISHED;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "getClockSplits",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int[] getClockSplits()\n{\r\n    return clockSplits;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "getCpuUsages",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int[] getCpuUsages()\n{\r\n    return cpuUsages;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "getVMemKbytes",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int[] getVMemKbytes()\n{\r\n    return vMemKbytes;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "getPhysMemKbytes",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int[] getPhysMemKbytes()\n{\r\n    return physMemKbytes;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "toTimelineEvent",
  "errType" : null,
  "containingMethodsNum" : 11,
  "sourceCodeText" : "TimelineEvent toTimelineEvent()\n{\r\n    TimelineEvent tEvent = new TimelineEvent();\r\n    tEvent.setId(StringUtils.toUpperCase(getEventType().name()));\r\n    tEvent.addInfo(\"TASK_TYPE\", getTaskType().toString());\r\n    tEvent.addInfo(\"ATTEMPT_ID\", getAttemptId() == null ? \"\" : getAttemptId().toString());\r\n    tEvent.addInfo(\"FINISH_TIME\", getFinishTime());\r\n    tEvent.addInfo(\"STATUS\", getTaskStatus());\r\n    tEvent.addInfo(\"STATE\", getState());\r\n    tEvent.addInfo(\"SHUFFLE_FINISH_TIME\", getShuffleFinishTime());\r\n    tEvent.addInfo(\"SORT_FINISH_TIME\", getSortFinishTime());\r\n    tEvent.addInfo(\"HOSTNAME\", getHostname());\r\n    tEvent.addInfo(\"PORT\", getPort());\r\n    tEvent.addInfo(\"RACK_NAME\", getRackName());\r\n    return tEvent;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "getTimelineMetrics",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Set<TimelineMetric> getTimelineMetrics()\n{\r\n    Set<TimelineMetric> metrics = JobHistoryEventUtils.countersToTimelineMetric(getCounters(), finishTime);\r\n    return metrics;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred\\lib\\aggregate",
  "methodName" : "reduce",
  "errType" : null,
  "containingMethodsNum" : 9,
  "sourceCodeText" : "void reduce(Text key, Iterator<Text> values, OutputCollector<Text, Text> output, Reporter reporter) throws IOException\n{\r\n    String keyStr = key.toString();\r\n    int pos = keyStr.indexOf(ValueAggregatorDescriptor.TYPE_SEPARATOR);\r\n    String type = keyStr.substring(0, pos);\r\n    keyStr = keyStr.substring(pos + ValueAggregatorDescriptor.TYPE_SEPARATOR.length());\r\n    ValueAggregator aggregator = ValueAggregatorBaseDescriptor.generateValueAggregator(type);\r\n    while (values.hasNext()) {\r\n        aggregator.addNextValue(values.next());\r\n    }\r\n    String val = aggregator.getReport();\r\n    key = new Text(keyStr);\r\n    output.collect(key, new Text(val));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred\\lib\\aggregate",
  "methodName" : "map",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void map(K1 arg0, V1 arg1, OutputCollector<Text, Text> arg2, Reporter arg3) throws IOException\n{\r\n    throw new IOException(\"should not be called\\n\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred\\lib",
  "methodName" : "configure",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void configure(JobConf jobConf)\n{\r\n    int numberOfThreads = jobConf.getInt(MultithreadedMapper.NUM_THREADS, 10);\r\n    if (LOG.isDebugEnabled()) {\r\n        LOG.debug(\"Configuring jobConf \" + jobConf.getJobName() + \" to use \" + numberOfThreads + \" threads\");\r\n    }\r\n    this.job = jobConf;\r\n    this.incrProcCount = SkipBadRecords.getMapperMaxSkipRecords(job) > 0 && SkipBadRecords.getAutoIncrMapperProcCount(job);\r\n    this.mapper = ReflectionUtils.newInstance(jobConf.getMapperClass(), jobConf);\r\n    executorService = new HadoopThreadPoolExecutor(numberOfThreads, numberOfThreads, 0L, TimeUnit.MILLISECONDS, new BlockingArrayQueue(numberOfThreads));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred\\lib",
  "methodName" : "checkForExceptionsFromProcessingThreads",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void checkForExceptionsFromProcessingThreads() throws IOException, RuntimeException\n{\r\n    if (ioException != null) {\r\n        throw ioException;\r\n    }\r\n    if (runtimeException != null) {\r\n        throw runtimeException;\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred\\lib",
  "methodName" : "run",
  "errType" : [ "IOException", "InterruptedException" ],
  "containingMethodsNum" : 17,
  "sourceCodeText" : "void run(RecordReader<K1, V1> input, OutputCollector<K2, V2> output, Reporter reporter) throws IOException\n{\r\n    try {\r\n        K1 key = input.createKey();\r\n        V1 value = input.createValue();\r\n        while (input.next(key, value)) {\r\n            executorService.execute(new MapperInvokeRunable(key, value, output, reporter));\r\n            checkForExceptionsFromProcessingThreads();\r\n            key = input.createKey();\r\n            value = input.createValue();\r\n        }\r\n        if (LOG.isDebugEnabled()) {\r\n            LOG.debug(\"Finished dispatching all Mappper.map calls, job \" + job.getJobName());\r\n        }\r\n        executorService.shutdown();\r\n        try {\r\n            while (!executorService.awaitTermination(100, TimeUnit.MILLISECONDS)) {\r\n                if (LOG.isDebugEnabled()) {\r\n                    LOG.debug(\"Awaiting all running Mappper.map calls to finish, job \" + job.getJobName());\r\n                }\r\n                checkForExceptionsFromProcessingThreads();\r\n            }\r\n            checkForExceptionsFromProcessingThreads();\r\n        } catch (IOException ioEx) {\r\n            executorService.shutdownNow();\r\n            throw ioEx;\r\n        } catch (InterruptedException iEx) {\r\n            throw new RuntimeException(iEx);\r\n        }\r\n    } finally {\r\n        mapper.close();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 2,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred\\lib",
  "methodName" : "addMapper",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void addMapper(boolean isMap, JobConf jobConf, Class<? extends Mapper<K1, V1, K2, V2>> klass, Class<? extends K1> inputKeyClass, Class<? extends V1> inputValueClass, Class<? extends K2> outputKeyClass, Class<? extends V2> outputValueClass, boolean byValue, JobConf mapperConf)\n{\r\n    String prefix = getPrefix(isMap);\r\n    checkReducerAlreadySet(isMap, jobConf, prefix, true);\r\n    int index = getIndex(jobConf, prefix);\r\n    jobConf.setClass(prefix + CHAIN_MAPPER_CLASS + index, klass, Mapper.class);\r\n    validateKeyValueTypes(isMap, jobConf, inputKeyClass, inputValueClass, outputKeyClass, outputValueClass, index, prefix);\r\n    if (mapperConf == null) {\r\n        mapperConf = new JobConf(true);\r\n    }\r\n    mapperConf.setBoolean(MAPPER_BY_VALUE, byValue);\r\n    setMapperConf(isMap, jobConf, inputKeyClass, inputValueClass, outputKeyClass, outputValueClass, mapperConf, index, prefix);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred\\lib",
  "methodName" : "setReducer",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void setReducer(JobConf jobConf, Class<? extends Reducer<K1, V1, K2, V2>> klass, Class<? extends K1> inputKeyClass, Class<? extends V1> inputValueClass, Class<? extends K2> outputKeyClass, Class<? extends V2> outputValueClass, boolean byValue, JobConf reducerConf)\n{\r\n    String prefix = getPrefix(false);\r\n    checkReducerAlreadySet(false, jobConf, prefix, false);\r\n    jobConf.setClass(prefix + CHAIN_REDUCER_CLASS, klass, Reducer.class);\r\n    if (reducerConf == null) {\r\n        reducerConf = new JobConf(false);\r\n    }\r\n    reducerConf.setBoolean(REDUCER_BY_VALUE, byValue);\r\n    setReducerConf(jobConf, inputKeyClass, inputValueClass, outputKeyClass, outputValueClass, reducerConf, prefix);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred\\lib",
  "methodName" : "configure",
  "errType" : null,
  "containingMethodsNum" : 17,
  "sourceCodeText" : "void configure(JobConf jobConf)\n{\r\n    String prefix = getPrefix(isMap);\r\n    chainJobConf = jobConf;\r\n    SerializationFactory serializationFactory = new SerializationFactory(chainJobConf);\r\n    int index = jobConf.getInt(prefix + CHAIN_MAPPER_SIZE, 0);\r\n    for (int i = 0; i < index; i++) {\r\n        Class<? extends Mapper> klass = jobConf.getClass(prefix + CHAIN_MAPPER_CLASS + i, null, Mapper.class);\r\n        JobConf mConf = new JobConf(getChainElementConf(jobConf, prefix + CHAIN_MAPPER_CONFIG + i));\r\n        Mapper mapper = ReflectionUtils.newInstance(klass, mConf);\r\n        mappers.add(mapper);\r\n        if (mConf.getBoolean(MAPPER_BY_VALUE, true)) {\r\n            mappersKeySerialization.add(serializationFactory.getSerialization(mConf.getClass(MAPPER_OUTPUT_KEY_CLASS, null)));\r\n            mappersValueSerialization.add(serializationFactory.getSerialization(mConf.getClass(MAPPER_OUTPUT_VALUE_CLASS, null)));\r\n        } else {\r\n            mappersKeySerialization.add(null);\r\n            mappersValueSerialization.add(null);\r\n        }\r\n    }\r\n    Class<? extends Reducer> klass = jobConf.getClass(prefix + CHAIN_REDUCER_CLASS, null, Reducer.class);\r\n    if (klass != null) {\r\n        JobConf rConf = new JobConf(getChainElementConf(jobConf, prefix + CHAIN_REDUCER_CONFIG));\r\n        reducer = ReflectionUtils.newInstance(klass, rConf);\r\n        if (rConf.getBoolean(REDUCER_BY_VALUE, true)) {\r\n            reducerKeySerialization = serializationFactory.getSerialization(rConf.getClass(REDUCER_OUTPUT_KEY_CLASS, null));\r\n            reducerValueSerialization = serializationFactory.getSerialization(rConf.getClass(REDUCER_OUTPUT_VALUE_CLASS, null));\r\n        } else {\r\n            reducerKeySerialization = null;\r\n            reducerValueSerialization = null;\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred\\lib",
  "methodName" : "getChainJobConf",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "JobConf getChainJobConf()\n{\r\n    return chainJobConf;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred\\lib",
  "methodName" : "getFirstMap",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "Mapper getFirstMap()\n{\r\n    return (mappers.size() > 0) ? mappers.get(0) : null;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred\\lib",
  "methodName" : "getReducer",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Reducer getReducer()\n{\r\n    return reducer;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred\\lib",
  "methodName" : "getMapperCollector",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "OutputCollector getMapperCollector(int mapperIndex, OutputCollector output, Reporter reporter)\n{\r\n    Serialization keySerialization = mappersKeySerialization.get(mapperIndex);\r\n    Serialization valueSerialization = mappersValueSerialization.get(mapperIndex);\r\n    return new ChainOutputCollector(mapperIndex, keySerialization, valueSerialization, output, reporter);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred\\lib",
  "methodName" : "getReducerCollector",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "OutputCollector getReducerCollector(OutputCollector output, Reporter reporter)\n{\r\n    return new ChainOutputCollector(reducerKeySerialization, reducerValueSerialization, output, reporter);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred\\lib",
  "methodName" : "close",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void close() throws IOException\n{\r\n    for (Mapper map : mappers) {\r\n        map.close();\r\n    }\r\n    if (reducer != null) {\r\n        reducer.close();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2",
  "methodName" : "getContainerId",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getContainerId()\n{\r\n    return containerId;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2",
  "methodName" : "setContainerId",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setContainerId(String containerId)\n{\r\n    this.containerId = containerId;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2",
  "methodName" : "getApplicationId",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getApplicationId()\n{\r\n    return applicationId;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2",
  "methodName" : "setApplicationId",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setApplicationId(String applicationId)\n{\r\n    this.applicationId = applicationId;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2",
  "methodName" : "getNodeId",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getNodeId()\n{\r\n    return nodeId;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2",
  "methodName" : "setNodeId",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setNodeId(String nodeId)\n{\r\n    this.nodeId = nodeId;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2",
  "methodName" : "getOwner",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getOwner()\n{\r\n    return this.owner;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\v2",
  "methodName" : "setOwner",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String setOwner(String owner)\n{\r\n    return this.owner;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred\\lib\\aggregate",
  "methodName" : "map",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void map(K1 key, V1 value, OutputCollector<Text, Text> output, Reporter reporter) throws IOException\n{\r\n    Iterator iter = this.aggregatorDescriptorList.iterator();\r\n    while (iter.hasNext()) {\r\n        ValueAggregatorDescriptor ad = (ValueAggregatorDescriptor) iter.next();\r\n        Iterator<Entry<Text, Text>> ens = ad.generateKeyValPairs(key, value).iterator();\r\n        while (ens.hasNext()) {\r\n            Entry<Text, Text> en = ens.next();\r\n            output.collect(en.getKey(), en.getValue());\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred\\lib\\aggregate",
  "methodName" : "reduce",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void reduce(Text arg0, Iterator<Text> arg1, OutputCollector<Text, Text> arg2, Reporter arg3) throws IOException\n{\r\n    throw new IOException(\"should not be called\\n\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "downgrade",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "TaskAttemptID downgrade(org.apache.hadoop.mapreduce.TaskAttemptID old)\n{\r\n    if (old instanceof TaskAttemptID) {\r\n        return (TaskAttemptID) old;\r\n    } else {\r\n        return new TaskAttemptID(TaskID.downgrade(old.getTaskID()), old.getId());\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getTaskID",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "TaskID getTaskID()\n{\r\n    return (TaskID) super.getTaskID();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getJobID",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "JobID getJobID()\n{\r\n    return (JobID) super.getJobID();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "read",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "TaskAttemptID read(DataInput in) throws IOException\n{\r\n    TaskAttemptID taskId = new TaskAttemptID();\r\n    taskId.readFields(in);\r\n    return taskId;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "forName",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "TaskAttemptID forName(String str) throws IllegalArgumentException\n{\r\n    return (TaskAttemptID) org.apache.hadoop.mapreduce.TaskAttemptID.forName(str);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getTaskAttemptIDsPattern",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String getTaskAttemptIDsPattern(String jtIdentifier, Integer jobId, Boolean isMap, Integer taskId, Integer attemptId)\n{\r\n    return getTaskAttemptIDsPattern(jtIdentifier, jobId, isMap ? TaskType.MAP : TaskType.REDUCE, taskId, attemptId);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getTaskAttemptIDsPattern",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "String getTaskAttemptIDsPattern(String jtIdentifier, Integer jobId, TaskType type, Integer taskId, Integer attemptId)\n{\r\n    StringBuilder builder = new StringBuilder(ATTEMPT).append(SEPARATOR);\r\n    builder.append(getTaskAttemptIDsPatternWOPrefix(jtIdentifier, jobId, type, taskId, attemptId));\r\n    return builder.toString();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getTaskAttemptIDsPatternWOPrefix",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "StringBuilder getTaskAttemptIDsPatternWOPrefix(String jtIdentifier, Integer jobId, TaskType type, Integer taskId, Integer attemptId)\n{\r\n    StringBuilder builder = new StringBuilder();\r\n    builder.append(TaskID.getTaskIDsPatternWOPrefix(jtIdentifier, jobId, type, taskId)).append(SEPARATOR).append(attemptId != null ? attemptId : \"[0-9]*\");\r\n    return builder;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred\\lib",
  "methodName" : "checkNamedOutput",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void checkNamedOutput(JobConf conf, String namedOutput, boolean alreadyDefined)\n{\r\n    List<String> definedChannels = getNamedOutputsList(conf);\r\n    if (alreadyDefined && definedChannels.contains(namedOutput)) {\r\n        throw new IllegalArgumentException(\"Named output '\" + namedOutput + \"' already alreadyDefined\");\r\n    } else if (!alreadyDefined && !definedChannels.contains(namedOutput)) {\r\n        throw new IllegalArgumentException(\"Named output '\" + namedOutput + \"' not defined\");\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred\\lib",
  "methodName" : "checkTokenName",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void checkTokenName(String namedOutput)\n{\r\n    if (namedOutput == null || namedOutput.length() == 0) {\r\n        throw new IllegalArgumentException(\"Name cannot be NULL or emtpy\");\r\n    }\r\n    for (char ch : namedOutput.toCharArray()) {\r\n        if ((ch >= 'A') && (ch <= 'Z')) {\r\n            continue;\r\n        }\r\n        if ((ch >= 'a') && (ch <= 'z')) {\r\n            continue;\r\n        }\r\n        if ((ch >= '0') && (ch <= '9')) {\r\n            continue;\r\n        }\r\n        throw new IllegalArgumentException(\"Name cannot be have a '\" + ch + \"' char\");\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred\\lib",
  "methodName" : "checkNamedOutputName",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void checkNamedOutputName(String namedOutput)\n{\r\n    checkTokenName(namedOutput);\r\n    if (namedOutput.equals(\"part\")) {\r\n        throw new IllegalArgumentException(\"Named output name cannot be 'part'\");\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred\\lib",
  "methodName" : "getNamedOutputsList",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "List<String> getNamedOutputsList(JobConf conf)\n{\r\n    List<String> names = new ArrayList<String>();\r\n    StringTokenizer st = new StringTokenizer(conf.get(NAMED_OUTPUTS, \"\"), \" \");\r\n    while (st.hasMoreTokens()) {\r\n        names.add(st.nextToken());\r\n    }\r\n    return names;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred\\lib",
  "methodName" : "isMultiNamedOutput",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "boolean isMultiNamedOutput(JobConf conf, String namedOutput)\n{\r\n    checkNamedOutput(conf, namedOutput, false);\r\n    return conf.getBoolean(MO_PREFIX + namedOutput + MULTI, false);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred\\lib",
  "methodName" : "getNamedOutputFormatClass",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "Class<? extends OutputFormat> getNamedOutputFormatClass(JobConf conf, String namedOutput)\n{\r\n    checkNamedOutput(conf, namedOutput, false);\r\n    return conf.getClass(MO_PREFIX + namedOutput + FORMAT, null, OutputFormat.class);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred\\lib",
  "methodName" : "getNamedOutputKeyClass",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "Class<?> getNamedOutputKeyClass(JobConf conf, String namedOutput)\n{\r\n    checkNamedOutput(conf, namedOutput, false);\r\n    return conf.getClass(MO_PREFIX + namedOutput + KEY, null, Object.class);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred\\lib",
  "methodName" : "getNamedOutputValueClass",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "Class<?> getNamedOutputValueClass(JobConf conf, String namedOutput)\n{\r\n    checkNamedOutput(conf, namedOutput, false);\r\n    return conf.getClass(MO_PREFIX + namedOutput + VALUE, null, Object.class);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred\\lib",
  "methodName" : "addNamedOutput",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void addNamedOutput(JobConf conf, String namedOutput, Class<? extends OutputFormat> outputFormatClass, Class<?> keyClass, Class<?> valueClass)\n{\r\n    addNamedOutput(conf, namedOutput, false, outputFormatClass, keyClass, valueClass);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred\\lib",
  "methodName" : "addMultiNamedOutput",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void addMultiNamedOutput(JobConf conf, String namedOutput, Class<? extends OutputFormat> outputFormatClass, Class<?> keyClass, Class<?> valueClass)\n{\r\n    addNamedOutput(conf, namedOutput, true, outputFormatClass, keyClass, valueClass);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred\\lib",
  "methodName" : "addNamedOutput",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void addNamedOutput(JobConf conf, String namedOutput, boolean multi, Class<? extends OutputFormat> outputFormatClass, Class<?> keyClass, Class<?> valueClass)\n{\r\n    checkNamedOutputName(namedOutput);\r\n    checkNamedOutput(conf, namedOutput, true);\r\n    conf.set(NAMED_OUTPUTS, conf.get(NAMED_OUTPUTS, \"\") + \" \" + namedOutput);\r\n    conf.setClass(MO_PREFIX + namedOutput + FORMAT, outputFormatClass, OutputFormat.class);\r\n    conf.setClass(MO_PREFIX + namedOutput + KEY, keyClass, Object.class);\r\n    conf.setClass(MO_PREFIX + namedOutput + VALUE, valueClass, Object.class);\r\n    conf.setBoolean(MO_PREFIX + namedOutput + MULTI, multi);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred\\lib",
  "methodName" : "setCountersEnabled",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setCountersEnabled(JobConf conf, boolean enabled)\n{\r\n    conf.setBoolean(COUNTERS_ENABLED, enabled);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred\\lib",
  "methodName" : "getCountersEnabled",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "boolean getCountersEnabled(JobConf conf)\n{\r\n    return conf.getBoolean(COUNTERS_ENABLED, false);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred\\lib",
  "methodName" : "getNamedOutputs",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Iterator<String> getNamedOutputs()\n{\r\n    return namedOutputs.iterator();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred\\lib",
  "methodName" : "getRecordWriter",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "RecordWriter getRecordWriter(String namedOutput, String baseFileName, final Reporter reporter) throws IOException\n{\r\n    RecordWriter writer = recordWriters.get(baseFileName);\r\n    if (writer == null) {\r\n        if (countersEnabled && reporter == null) {\r\n            throw new IllegalArgumentException(\"Counters are enabled, Reporter cannot be NULL\");\r\n        }\r\n        JobConf jobConf = new JobConf(conf);\r\n        jobConf.set(InternalFileOutputFormat.CONFIG_NAMED_OUTPUT, namedOutput);\r\n        FileSystem fs = FileSystem.get(conf);\r\n        writer = outputFormat.getRecordWriter(fs, jobConf, baseFileName, reporter);\r\n        if (countersEnabled) {\r\n            if (reporter == null) {\r\n                throw new IllegalArgumentException(\"Counters are enabled, Reporter cannot be NULL\");\r\n            }\r\n            writer = new RecordWriterWithCounter(writer, baseFileName, reporter);\r\n        }\r\n        recordWriters.put(baseFileName, writer);\r\n    }\r\n    return writer;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred\\lib",
  "methodName" : "getCollector",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "OutputCollector getCollector(String namedOutput, Reporter reporter) throws IOException\n{\r\n    return getCollector(namedOutput, null, reporter);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred\\lib",
  "methodName" : "getCollector",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "OutputCollector getCollector(String namedOutput, String multiName, Reporter reporter) throws IOException\n{\r\n    checkNamedOutputName(namedOutput);\r\n    if (!namedOutputs.contains(namedOutput)) {\r\n        throw new IllegalArgumentException(\"Undefined named output '\" + namedOutput + \"'\");\r\n    }\r\n    boolean multi = isMultiNamedOutput(conf, namedOutput);\r\n    if (!multi && multiName != null) {\r\n        throw new IllegalArgumentException(\"Name output '\" + namedOutput + \"' has not been defined as multi\");\r\n    }\r\n    if (multi) {\r\n        checkTokenName(multiName);\r\n    }\r\n    String baseFileName = (multi) ? namedOutput + \"_\" + multiName : namedOutput;\r\n    final RecordWriter writer = getRecordWriter(namedOutput, baseFileName, reporter);\r\n    return new OutputCollector() {\r\n\r\n        @SuppressWarnings({ \"unchecked\" })\r\n        public void collect(Object key, Object value) throws IOException {\r\n            writer.write(key, value);\r\n        }\r\n    };\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred\\lib",
  "methodName" : "close",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void close() throws IOException\n{\r\n    for (RecordWriter writer : recordWriters.values()) {\r\n        writer.close(null);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\aggregate",
  "methodName" : "setup",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setup(Context context) throws IOException, InterruptedException\n{\r\n    ValueAggregatorJobBase.setup(context.getConfiguration());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\aggregate",
  "methodName" : "reduce",
  "errType" : null,
  "containingMethodsNum" : 9,
  "sourceCodeText" : "void reduce(Text key, Iterable<Text> values, Context context) throws IOException, InterruptedException\n{\r\n    String keyStr = key.toString();\r\n    int pos = keyStr.indexOf(ValueAggregatorDescriptor.TYPE_SEPARATOR);\r\n    String type = keyStr.substring(0, pos);\r\n    keyStr = keyStr.substring(pos + ValueAggregatorDescriptor.TYPE_SEPARATOR.length());\r\n    long uniqCount = context.getConfiguration().getLong(UniqValueCount.MAX_NUM_UNIQUE_VALUES, Long.MAX_VALUE);\r\n    ValueAggregator aggregator = ValueAggregatorBaseDescriptor.generateValueAggregator(type, uniqCount);\r\n    for (Text value : values) {\r\n        aggregator.addNextValue(value);\r\n    }\r\n    String val = aggregator.getReport();\r\n    key = new Text(keyStr);\r\n    context.write(key, new Text(val));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\partition",
  "methodName" : "setOffsets",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void setOffsets(Configuration conf, int left, int right)\n{\r\n    conf.setInt(LEFT_OFFSET_PROPERTY_NAME, left);\r\n    conf.setInt(RIGHT_OFFSET_PROPERTY_NAME, right);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\partition",
  "methodName" : "setLeftOffset",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setLeftOffset(Configuration conf, int offset)\n{\r\n    conf.setInt(LEFT_OFFSET_PROPERTY_NAME, offset);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\partition",
  "methodName" : "setRightOffset",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setRightOffset(Configuration conf, int offset)\n{\r\n    conf.setInt(RIGHT_OFFSET_PROPERTY_NAME, offset);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\partition",
  "methodName" : "setConf",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void setConf(Configuration conf)\n{\r\n    this.conf = conf;\r\n    leftOffset = conf.getInt(LEFT_OFFSET_PROPERTY_NAME, 0);\r\n    rightOffset = conf.getInt(RIGHT_OFFSET_PROPERTY_NAME, -1);\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\partition",
  "methodName" : "getConf",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Configuration getConf()\n{\r\n    return conf;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\partition",
  "methodName" : "getPartition",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "int getPartition(BinaryComparable key, V value, int numPartitions)\n{\r\n    int length = key.getLength();\r\n    int leftIndex = (leftOffset + length) % length;\r\n    int rightIndex = (rightOffset + length) % length;\r\n    int hash = WritableComparator.hashBytes(key.getBytes(), leftIndex, rightIndex - leftIndex + 1);\r\n    return (hash & Integer.MAX_VALUE) % numPartitions;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred\\join",
  "methodName" : "emit",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "V emit(TupleWritable dst) throws IOException",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred\\join",
  "methodName" : "combine",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean combine(Object[] srcs, TupleWritable dst)\n{\r\n    return true;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred\\join",
  "methodName" : "next",
  "errType" : null,
  "containingMethodsNum" : 13,
  "sourceCodeText" : "boolean next(K key, V value) throws IOException\n{\r\n    if (jc.flush(ivalue)) {\r\n        WritableUtils.cloneInto(key, jc.key());\r\n        WritableUtils.cloneInto(value, emit(ivalue));\r\n        return true;\r\n    }\r\n    jc.clear();\r\n    K iterkey = createKey();\r\n    final PriorityQueue<ComposableRecordReader<K, ?>> q = getRecordReaderQueue();\r\n    while (!q.isEmpty()) {\r\n        fillJoinCollector(iterkey);\r\n        jc.reset(iterkey);\r\n        if (jc.flush(ivalue)) {\r\n            WritableUtils.cloneInto(key, jc.key());\r\n            WritableUtils.cloneInto(value, emit(ivalue));\r\n            return true;\r\n        }\r\n        jc.clear();\r\n    }\r\n    return false;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred\\join",
  "methodName" : "createValue",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "V createValue()\n{\r\n    if (null == valueclass) {\r\n        final Class<?> cls = kids[0].createValue().getClass();\r\n        for (RecordReader<K, ? extends V> rr : kids) {\r\n            if (!cls.equals(rr.createValue().getClass())) {\r\n                throw new ClassCastException(\"Child value classes fail to agree\");\r\n            }\r\n        }\r\n        valueclass = cls.asSubclass(Writable.class);\r\n        ivalue = createInternalValue();\r\n    }\r\n    return (V) ReflectionUtils.newInstance(valueclass, null);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred\\join",
  "methodName" : "getDelegate",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "ResetableIterator<V> getDelegate()\n{\r\n    return new MultiFilterDelegationIterator();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\db",
  "methodName" : "executeQuery",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "ResultSet executeQuery(String query) throws SQLException\n{\r\n    this.statement = connection.prepareStatement(query, ResultSet.TYPE_FORWARD_ONLY, ResultSet.CONCUR_READ_ONLY);\r\n    return statement.executeQuery();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\db",
  "methodName" : "getSelectQuery",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 15,
  "sourceCodeText" : "String getSelectQuery()\n{\r\n    StringBuilder query = new StringBuilder();\r\n    if (dbConf.getInputQuery() == null) {\r\n        query.append(\"SELECT \");\r\n        for (int i = 0; i < fieldNames.length; i++) {\r\n            query.append(fieldNames[i]);\r\n            if (i != fieldNames.length - 1) {\r\n                query.append(\", \");\r\n            }\r\n        }\r\n        query.append(\" FROM \").append(tableName);\r\n        query.append(\" AS \").append(tableName);\r\n        if (conditions != null && conditions.length() > 0) {\r\n            query.append(\" WHERE (\").append(conditions).append(\")\");\r\n        }\r\n        String orderBy = dbConf.getInputOrderBy();\r\n        if (orderBy != null && orderBy.length() > 0) {\r\n            query.append(\" ORDER BY \").append(orderBy);\r\n        }\r\n    } else {\r\n        query.append(dbConf.getInputQuery());\r\n    }\r\n    try {\r\n        query.append(\" LIMIT \").append(split.getLength());\r\n        query.append(\" OFFSET \").append(split.getStart());\r\n    } catch (IOException ex) {\r\n    }\r\n    return query.toString();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\db",
  "methodName" : "close",
  "errType" : [ "SQLException" ],
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void close() throws IOException\n{\r\n    try {\r\n        if (null != results) {\r\n            results.close();\r\n        }\r\n        if (null != statement) {\r\n            statement.close();\r\n        }\r\n        if (null != connection) {\r\n            connection.commit();\r\n            connection.close();\r\n        }\r\n    } catch (SQLException e) {\r\n        throw new IOException(e.getMessage());\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\db",
  "methodName" : "initialize",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void initialize(InputSplit split, TaskAttemptContext context) throws IOException, InterruptedException\n{\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\db",
  "methodName" : "getCurrentKey",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "LongWritable getCurrentKey()\n{\r\n    return key;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\db",
  "methodName" : "getCurrentValue",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "T getCurrentValue()\n{\r\n    return value;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\db",
  "methodName" : "createValue",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "T createValue()\n{\r\n    return ReflectionUtils.newInstance(inputClass, conf);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\db",
  "methodName" : "getPos",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getPos() throws IOException\n{\r\n    return pos;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\db",
  "methodName" : "next",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "boolean next(LongWritable key, T value) throws IOException\n{\r\n    this.key = key;\r\n    this.value = value;\r\n    return nextKeyValue();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\db",
  "methodName" : "getProgress",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "float getProgress() throws IOException\n{\r\n    return pos / (float) split.getLength();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\db",
  "methodName" : "nextKeyValue",
  "errType" : [ "SQLException" ],
  "containingMethodsNum" : 5,
  "sourceCodeText" : "boolean nextKeyValue() throws IOException\n{\r\n    try {\r\n        if (key == null) {\r\n            key = new LongWritable();\r\n        }\r\n        if (value == null) {\r\n            value = createValue();\r\n        }\r\n        if (null == this.results) {\r\n            this.results = executeQuery(getSelectQuery());\r\n        }\r\n        if (!results.next())\r\n            return false;\r\n        key.set(pos + split.getStart());\r\n        value.readFields(results);\r\n        pos++;\r\n    } catch (SQLException e) {\r\n        throw new IOException(\"SQLException in nextKeyValue\", e);\r\n    }\r\n    return true;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\db",
  "methodName" : "getSplit",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "DBInputFormat.DBInputSplit getSplit()\n{\r\n    return split;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\db",
  "methodName" : "getFieldNames",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String[] getFieldNames()\n{\r\n    return fieldNames;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\db",
  "methodName" : "getTableName",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getTableName()\n{\r\n    return tableName;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\db",
  "methodName" : "getConditions",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getConditions()\n{\r\n    return conditions;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\db",
  "methodName" : "getDBConf",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "DBConfiguration getDBConf()\n{\r\n    return dbConf;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\db",
  "methodName" : "getConnection",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Connection getConnection()\n{\r\n    return connection;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\db",
  "methodName" : "getStatement",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "PreparedStatement getStatement()\n{\r\n    return statement;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\db",
  "methodName" : "setStatement",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setStatement(PreparedStatement stmt)\n{\r\n    this.statement = stmt;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\fieldsel",
  "methodName" : "extractFields",
  "errType" : null,
  "containingMethodsNum" : 12,
  "sourceCodeText" : "int extractFields(String[] fieldListSpec, List<Integer> fieldList)\n{\r\n    int allFieldsFrom = -1;\r\n    int i = 0;\r\n    int j = 0;\r\n    int pos = -1;\r\n    String fieldSpec = null;\r\n    for (i = 0; i < fieldListSpec.length; i++) {\r\n        fieldSpec = fieldListSpec[i];\r\n        if (fieldSpec.length() == 0) {\r\n            continue;\r\n        }\r\n        pos = fieldSpec.indexOf('-');\r\n        if (pos < 0) {\r\n            Integer fn = Integer.valueOf(fieldSpec);\r\n            fieldList.add(fn);\r\n        } else {\r\n            String start = fieldSpec.substring(0, pos);\r\n            String end = fieldSpec.substring(pos + 1);\r\n            if (start.length() == 0) {\r\n                start = \"0\";\r\n            }\r\n            if (end.length() == 0) {\r\n                allFieldsFrom = Integer.parseInt(start);\r\n                continue;\r\n            }\r\n            int startPos = Integer.parseInt(start);\r\n            int endPos = Integer.parseInt(end);\r\n            for (j = startPos; j <= endPos; j++) {\r\n                fieldList.add(j);\r\n            }\r\n        }\r\n    }\r\n    return allFieldsFrom;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\fieldsel",
  "methodName" : "selectFields",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "String selectFields(String[] fields, List<Integer> fieldList, int allFieldsFrom, String separator)\n{\r\n    String retv = null;\r\n    int i = 0;\r\n    StringBuffer sb = null;\r\n    if (fieldList != null && fieldList.size() > 0) {\r\n        if (sb == null) {\r\n            sb = new StringBuffer();\r\n        }\r\n        for (Integer index : fieldList) {\r\n            if (index < fields.length) {\r\n                sb.append(fields[index]);\r\n            }\r\n            sb.append(separator);\r\n        }\r\n    }\r\n    if (allFieldsFrom >= 0) {\r\n        if (sb == null) {\r\n            sb = new StringBuffer();\r\n        }\r\n        for (i = allFieldsFrom; i < fields.length; i++) {\r\n            sb.append(fields[i]).append(separator);\r\n        }\r\n    }\r\n    if (sb != null) {\r\n        retv = sb.toString();\r\n        if (retv.length() > 0) {\r\n            retv = retv.substring(0, retv.length() - 1);\r\n        }\r\n    }\r\n    return retv;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\fieldsel",
  "methodName" : "parseOutputKeyValueSpec",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "int parseOutputKeyValueSpec(String keyValueSpec, List<Integer> keyFieldList, List<Integer> valueFieldList)\n{\r\n    String[] keyValSpecs = keyValueSpec.split(\":\", -1);\r\n    String[] keySpec = keyValSpecs[0].split(\",\");\r\n    String[] valSpec = new String[0];\r\n    if (keyValSpecs.length > 1) {\r\n        valSpec = keyValSpecs[1].split(\",\");\r\n    }\r\n    FieldSelectionHelper.extractFields(keySpec, keyFieldList);\r\n    return FieldSelectionHelper.extractFields(valSpec, valueFieldList);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\fieldsel",
  "methodName" : "specToString",
  "errType" : null,
  "containingMethodsNum" : 11,
  "sourceCodeText" : "String specToString(String fieldSeparator, String keyValueSpec, int allValueFieldsFrom, List<Integer> keyFieldList, List<Integer> valueFieldList)\n{\r\n    StringBuffer sb = new StringBuffer();\r\n    sb.append(\"fieldSeparator: \").append(fieldSeparator).append(\"\\n\");\r\n    sb.append(\"keyValueSpec: \").append(keyValueSpec).append(\"\\n\");\r\n    sb.append(\"allValueFieldsFrom: \").append(allValueFieldsFrom);\r\n    sb.append(\"\\n\");\r\n    sb.append(\"keyFieldList.length: \").append(keyFieldList.size());\r\n    sb.append(\"\\n\");\r\n    for (Integer field : keyFieldList) {\r\n        sb.append(\"\\t\").append(field).append(\"\\n\");\r\n    }\r\n    sb.append(\"valueFieldList.length: \").append(valueFieldList.size());\r\n    sb.append(\"\\n\");\r\n    for (Integer field : valueFieldList) {\r\n        sb.append(\"\\t\").append(field).append(\"\\n\");\r\n    }\r\n    return sb.toString();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\fieldsel",
  "methodName" : "getKey",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Text getKey()\n{\r\n    return key;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\fieldsel",
  "methodName" : "getValue",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Text getValue()\n{\r\n    return value;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\fieldsel",
  "methodName" : "extractOutputKeyValue",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void extractOutputKeyValue(String key, String val, String fieldSep, List<Integer> keyFieldList, List<Integer> valFieldList, int allValueFieldsFrom, boolean ignoreKey, boolean isMap)\n{\r\n    if (!ignoreKey) {\r\n        val = key + val;\r\n    }\r\n    String[] fields = val.split(fieldSep);\r\n    String newKey = selectFields(fields, keyFieldList, -1, fieldSep);\r\n    String newVal = selectFields(fields, valFieldList, allValueFieldsFrom, fieldSep);\r\n    if (isMap && newKey == null) {\r\n        newKey = newVal;\r\n        newVal = null;\r\n    }\r\n    if (newKey != null) {\r\n        this.key = new Text(newKey);\r\n    }\r\n    if (newVal != null) {\r\n        this.value = new Text(newVal);\r\n    }\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\input",
  "methodName" : "getPath",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Path getPath()\n{\r\n    return file;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\input",
  "methodName" : "getStart",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getStart()\n{\r\n    return start;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\input",
  "methodName" : "getLength",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getLength()\n{\r\n    return length;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\input",
  "methodName" : "toString",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String toString()\n{\r\n    return file + \":\" + start + \"+\" + length;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\input",
  "methodName" : "write",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void write(DataOutput out) throws IOException\n{\r\n    Text.writeString(out, file.toString());\r\n    out.writeLong(start);\r\n    out.writeLong(length);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\input",
  "methodName" : "readFields",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void readFields(DataInput in) throws IOException\n{\r\n    file = new Path(Text.readString(in));\r\n    start = in.readLong();\r\n    length = in.readLong();\r\n    hosts = null;\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\input",
  "methodName" : "getLocations",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String[] getLocations() throws IOException\n{\r\n    if (this.hosts == null) {\r\n        return new String[] {};\r\n    } else {\r\n        return this.hosts;\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\input",
  "methodName" : "getLocationInfo",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "SplitLocationInfo[] getLocationInfo() throws IOException\n{\r\n    return hostInfos;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "merge",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "RawKeyValueIterator merge(Configuration conf, FileSystem fs, Class<K> keyClass, Class<V> valueClass, CompressionCodec codec, Path[] inputs, boolean deleteInputs, int mergeFactor, Path tmpDir, RawComparator<K> comparator, Progressable reporter, Counters.Counter readsCounter, Counters.Counter writesCounter, Progress mergePhase) throws IOException\n{\r\n    return new MergeQueue<K, V>(conf, fs, inputs, deleteInputs, codec, comparator, reporter, null, TaskType.REDUCE).merge(keyClass, valueClass, mergeFactor, tmpDir, readsCounter, writesCounter, mergePhase);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "merge",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "RawKeyValueIterator merge(Configuration conf, FileSystem fs, Class<K> keyClass, Class<V> valueClass, CompressionCodec codec, Path[] inputs, boolean deleteInputs, int mergeFactor, Path tmpDir, RawComparator<K> comparator, Progressable reporter, Counters.Counter readsCounter, Counters.Counter writesCounter, Counters.Counter mergedMapOutputsCounter, Progress mergePhase) throws IOException\n{\r\n    return new MergeQueue<K, V>(conf, fs, inputs, deleteInputs, codec, comparator, reporter, mergedMapOutputsCounter, TaskType.REDUCE).merge(keyClass, valueClass, mergeFactor, tmpDir, readsCounter, writesCounter, mergePhase);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "merge",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "RawKeyValueIterator merge(Configuration conf, FileSystem fs, Class<K> keyClass, Class<V> valueClass, List<Segment<K, V>> segments, int mergeFactor, Path tmpDir, RawComparator<K> comparator, Progressable reporter, Counters.Counter readsCounter, Counters.Counter writesCounter, Progress mergePhase) throws IOException\n{\r\n    return merge(conf, fs, keyClass, valueClass, segments, mergeFactor, tmpDir, comparator, reporter, false, readsCounter, writesCounter, mergePhase);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "merge",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "RawKeyValueIterator merge(Configuration conf, FileSystem fs, Class<K> keyClass, Class<V> valueClass, List<Segment<K, V>> segments, int mergeFactor, Path tmpDir, RawComparator<K> comparator, Progressable reporter, boolean sortSegments, Counters.Counter readsCounter, Counters.Counter writesCounter, Progress mergePhase) throws IOException\n{\r\n    return new MergeQueue<K, V>(conf, fs, segments, comparator, reporter, sortSegments, TaskType.REDUCE).merge(keyClass, valueClass, mergeFactor, tmpDir, readsCounter, writesCounter, mergePhase);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "merge",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "RawKeyValueIterator merge(Configuration conf, FileSystem fs, Class<K> keyClass, Class<V> valueClass, CompressionCodec codec, List<Segment<K, V>> segments, int mergeFactor, Path tmpDir, RawComparator<K> comparator, Progressable reporter, boolean sortSegments, Counters.Counter readsCounter, Counters.Counter writesCounter, Progress mergePhase, TaskType taskType) throws IOException\n{\r\n    return new MergeQueue<K, V>(conf, fs, segments, comparator, reporter, sortSegments, codec, taskType).merge(keyClass, valueClass, mergeFactor, tmpDir, readsCounter, writesCounter, mergePhase);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "merge",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "RawKeyValueIterator merge(Configuration conf, FileSystem fs, Class<K> keyClass, Class<V> valueClass, List<Segment<K, V>> segments, int mergeFactor, int inMemSegments, Path tmpDir, RawComparator<K> comparator, Progressable reporter, boolean sortSegments, Counters.Counter readsCounter, Counters.Counter writesCounter, Progress mergePhase) throws IOException\n{\r\n    return new MergeQueue<K, V>(conf, fs, segments, comparator, reporter, sortSegments, TaskType.REDUCE).merge(keyClass, valueClass, mergeFactor, inMemSegments, tmpDir, readsCounter, writesCounter, mergePhase);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "merge",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "RawKeyValueIterator merge(Configuration conf, FileSystem fs, Class<K> keyClass, Class<V> valueClass, CompressionCodec codec, List<Segment<K, V>> segments, int mergeFactor, int inMemSegments, Path tmpDir, RawComparator<K> comparator, Progressable reporter, boolean sortSegments, Counters.Counter readsCounter, Counters.Counter writesCounter, Progress mergePhase) throws IOException\n{\r\n    return new MergeQueue<K, V>(conf, fs, segments, comparator, reporter, sortSegments, codec, TaskType.REDUCE).merge(keyClass, valueClass, mergeFactor, inMemSegments, tmpDir, readsCounter, writesCounter, mergePhase);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "writeFile",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void writeFile(RawKeyValueIterator records, Writer<K, V> writer, Progressable progressable, Configuration conf) throws IOException\n{\r\n    long progressBar = conf.getLong(JobContext.RECORDS_BEFORE_PROGRESS, 10000);\r\n    long recordCtr = 0;\r\n    while (records.next()) {\r\n        writer.append(records.getKey(), records.getValue());\r\n        if (((recordCtr++) % progressBar) == 0) {\r\n            progressable.progress();\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\input",
  "methodName" : "setRecordLength",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setRecordLength(Configuration conf, int recordLength)\n{\r\n    conf.setInt(FIXED_RECORD_LENGTH, recordLength);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\input",
  "methodName" : "getRecordLength",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "int getRecordLength(Configuration conf)\n{\r\n    return conf.getInt(FIXED_RECORD_LENGTH, 0);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\input",
  "methodName" : "createRecordReader",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "RecordReader<LongWritable, BytesWritable> createRecordReader(InputSplit split, TaskAttemptContext context) throws IOException, InterruptedException\n{\r\n    int recordLength = getRecordLength(context.getConfiguration());\r\n    if (recordLength <= 0) {\r\n        throw new IOException(\"Fixed record length \" + recordLength + \" is invalid.  It should be set to a value greater than zero\");\r\n    }\r\n    return new FixedLengthRecordReader(recordLength);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\input",
  "methodName" : "isSplitable",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "boolean isSplitable(JobContext context, Path file)\n{\r\n    final CompressionCodec codec = new CompressionCodecFactory(context.getConfiguration()).getCodec(file);\r\n    return (null == codec);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "getDatum",
  "errType" : null,
  "containingMethodsNum" : 14,
  "sourceCodeText" : "Object getDatum()\n{\r\n    if (datum == null) {\r\n        datum = new TaskAttemptUnsuccessfulCompletion();\r\n        datum.setTaskid(new Utf8(attemptId.getTaskID().toString()));\r\n        datum.setTaskType(new Utf8(taskType.name()));\r\n        datum.setAttemptId(new Utf8(attemptId.toString()));\r\n        datum.setFinishTime(finishTime);\r\n        datum.setHostname(new Utf8(hostname));\r\n        if (rackName != null) {\r\n            datum.setRackname(new Utf8(rackName));\r\n        }\r\n        datum.setPort(port);\r\n        datum.setError(new Utf8(error));\r\n        datum.setStatus(new Utf8(status));\r\n        datum.setCounters(EventWriter.toAvro(counters));\r\n        datum.setClockSplits(AvroArrayUtils.toAvro(ProgressSplitsBlock.arrayGetWallclockTime(allSplits)));\r\n        datum.setCpuUsages(AvroArrayUtils.toAvro(ProgressSplitsBlock.arrayGetCPUTime(allSplits)));\r\n        datum.setVMemKbytes(AvroArrayUtils.toAvro(ProgressSplitsBlock.arrayGetVMemKbytes(allSplits)));\r\n        datum.setPhysMemKbytes(AvroArrayUtils.toAvro(ProgressSplitsBlock.arrayGetPhysMemKbytes(allSplits)));\r\n    }\r\n    return datum;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "setDatum",
  "errType" : null,
  "containingMethodsNum" : 13,
  "sourceCodeText" : "void setDatum(Object odatum)\n{\r\n    this.datum = (TaskAttemptUnsuccessfulCompletion) odatum;\r\n    this.attemptId = TaskAttemptID.forName(datum.getAttemptId().toString());\r\n    this.taskType = TaskType.valueOf(datum.getTaskType().toString());\r\n    this.finishTime = datum.getFinishTime();\r\n    this.hostname = datum.getHostname().toString();\r\n    this.rackName = datum.getRackname().toString();\r\n    this.port = datum.getPort();\r\n    this.status = datum.getStatus().toString();\r\n    this.error = datum.getError().toString();\r\n    this.counters = EventReader.fromAvro(datum.getCounters());\r\n    this.clockSplits = AvroArrayUtils.fromAvro(datum.getClockSplits());\r\n    this.cpuUsages = AvroArrayUtils.fromAvro(datum.getCpuUsages());\r\n    this.vMemKbytes = AvroArrayUtils.fromAvro(datum.getVMemKbytes());\r\n    this.physMemKbytes = AvroArrayUtils.fromAvro(datum.getPhysMemKbytes());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "getTaskId",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "TaskID getTaskId()\n{\r\n    return attemptId.getTaskID();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "getTaskType",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "TaskType getTaskType()\n{\r\n    return TaskType.valueOf(taskType.toString());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "getTaskAttemptId",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "TaskAttemptID getTaskAttemptId()\n{\r\n    return attemptId;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "getFinishTime",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getFinishTime()\n{\r\n    return finishTime;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "getStartTime",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getStartTime()\n{\r\n    return startTime;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "getHostname",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getHostname()\n{\r\n    return hostname;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "getPort",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int getPort()\n{\r\n    return port;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "getRackName",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String getRackName()\n{\r\n    return rackName == null ? null : rackName.toString();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "getError",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String getError()\n{\r\n    return error.toString();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "getTaskStatus",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String getTaskStatus()\n{\r\n    return status.toString();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "getCounters",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Counters getCounters()\n{\r\n    return counters;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "getEventType",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "EventType getEventType()\n{\r\n    boolean failed = TaskStatus.State.FAILED.toString().equals(getTaskStatus());\r\n    return getTaskId().getTaskType() == TaskType.MAP ? (failed ? EventType.MAP_ATTEMPT_FAILED : EventType.MAP_ATTEMPT_KILLED) : (failed ? EventType.REDUCE_ATTEMPT_FAILED : EventType.REDUCE_ATTEMPT_KILLED);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "getClockSplits",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int[] getClockSplits()\n{\r\n    return clockSplits;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "getCpuUsages",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int[] getCpuUsages()\n{\r\n    return cpuUsages;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "getVMemKbytes",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int[] getVMemKbytes()\n{\r\n    return vMemKbytes;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "getPhysMemKbytes",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int[] getPhysMemKbytes()\n{\r\n    return physMemKbytes;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "toTimelineEvent",
  "errType" : null,
  "containingMethodsNum" : 12,
  "sourceCodeText" : "TimelineEvent toTimelineEvent()\n{\r\n    TimelineEvent tEvent = new TimelineEvent();\r\n    tEvent.setId(StringUtils.toUpperCase(getEventType().name()));\r\n    tEvent.addInfo(\"TASK_TYPE\", getTaskType().toString());\r\n    tEvent.addInfo(\"TASK_ATTEMPT_ID\", getTaskAttemptId() == null ? \"\" : getTaskAttemptId().toString());\r\n    tEvent.addInfo(\"FINISH_TIME\", getFinishTime());\r\n    tEvent.addInfo(\"ERROR\", getError());\r\n    tEvent.addInfo(\"STATUS\", getTaskStatus());\r\n    tEvent.addInfo(\"HOSTNAME\", getHostname());\r\n    tEvent.addInfo(\"PORT\", getPort());\r\n    tEvent.addInfo(\"RACK_NAME\", getRackName());\r\n    tEvent.addInfo(\"SHUFFLE_FINISH_TIME\", getFinishTime());\r\n    tEvent.addInfo(\"SORT_FINISH_TIME\", getFinishTime());\r\n    tEvent.addInfo(\"MAP_FINISH_TIME\", getFinishTime());\r\n    return tEvent;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "getTimelineMetrics",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Set<TimelineMetric> getTimelineMetrics()\n{\r\n    Set<TimelineMetric> metrics = JobHistoryEventUtils.countersToTimelineMetric(getCounters(), finishTime);\r\n    return metrics;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getValues",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int[] getValues()\n{\r\n    return values;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "extendInternal",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void extendInternal(double newProgress, int newValue)",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "initializeInterval",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void initializeInterval()\n{\r\n    state.currentAccumulation = 0.0D;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "extend",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void extend(double newProgress, int newValue)\n{\r\n    if (state == null || newProgress < state.oldProgress) {\r\n        return;\r\n    }\r\n    int oldIndex = (int) (state.oldProgress * count);\r\n    int newIndex = (int) (newProgress * count);\r\n    int originalOldValue = state.oldValue;\r\n    double fullValueDistance = (double) newValue - state.oldValue;\r\n    double fullProgressDistance = newProgress - state.oldProgress;\r\n    double originalOldProgress = state.oldProgress;\r\n    for (int closee = oldIndex; closee < newIndex; ++closee) {\r\n        double interpolationProgress = (double) (closee + 1) / count;\r\n        interpolationProgress = Math.min(interpolationProgress, newProgress);\r\n        double progressLength = (interpolationProgress - originalOldProgress);\r\n        double interpolationProportion = progressLength / fullProgressDistance;\r\n        double interpolationValueDistance = fullValueDistance * interpolationProportion;\r\n        int interpolationValue = (int) interpolationValueDistance + originalOldValue;\r\n        extendInternal(interpolationProgress, interpolationValue);\r\n        advanceState(interpolationProgress, interpolationValue);\r\n        values[closee] = (int) state.currentAccumulation;\r\n        initializeInterval();\r\n    }\r\n    extendInternal(newProgress, newValue);\r\n    advanceState(newProgress, newValue);\r\n    if (newIndex == count) {\r\n        state = null;\r\n    }\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "advanceState",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void advanceState(double newProgress, int newValue)\n{\r\n    state.oldValue = newValue;\r\n    state.oldProgress = newProgress;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getCount",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int getCount()\n{\r\n    return count;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "get",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int get(int index)\n{\r\n    return values[index];\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "getJobID",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "JobID getJobID()\n{\r\n    return taskId.getJobID();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "getTaskID",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "TaskID getTaskID()\n{\r\n    return taskId;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "isMap",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "boolean isMap()\n{\r\n    return taskId.isMap();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "getTaskType",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "TaskType getTaskType()\n{\r\n    return taskId.getTaskType();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "equals",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "boolean equals(Object o)\n{\r\n    if (!super.equals(o))\r\n        return false;\r\n    TaskAttemptID that = (TaskAttemptID) o;\r\n    return this.taskId.equals(that.taskId);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "appendTo",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "StringBuilder appendTo(StringBuilder builder)\n{\r\n    return taskId.appendTo(builder).append(SEPARATOR).append(id);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "readFields",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void readFields(DataInput in) throws IOException\n{\r\n    super.readFields(in);\r\n    taskId.readFields(in);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "write",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void write(DataOutput out) throws IOException\n{\r\n    super.write(out);\r\n    taskId.write(out);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "hashCode",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "int hashCode()\n{\r\n    return taskId.hashCode() * 5 + id;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "compareTo",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "int compareTo(ID o)\n{\r\n    TaskAttemptID that = (TaskAttemptID) o;\r\n    int tipComp = this.taskId.compareTo(that.taskId);\r\n    if (tipComp == 0) {\r\n        return this.id - that.id;\r\n    } else\r\n        return tipComp;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "toString",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String toString()\n{\r\n    return appendTo(new StringBuilder(ATTEMPT)).toString();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "forName",
  "errType" : [ "Exception" ],
  "containingMethodsNum" : 6,
  "sourceCodeText" : "TaskAttemptID forName(String str) throws IllegalArgumentException\n{\r\n    if (str == null)\r\n        return null;\r\n    String exceptionMsg = null;\r\n    try {\r\n        String[] parts = str.split(Character.toString(SEPARATOR));\r\n        if (parts.length == 6) {\r\n            if (parts[0].equals(ATTEMPT)) {\r\n                String type = parts[3];\r\n                TaskType t = TaskID.getTaskType(type.charAt(0));\r\n                if (t != null) {\r\n                    return new org.apache.hadoop.mapred.TaskAttemptID(parts[1], Integer.parseInt(parts[2]), t, Integer.parseInt(parts[4]), Integer.parseInt(parts[5]));\r\n                } else\r\n                    exceptionMsg = \"Bad TaskType identifier. TaskAttemptId string : \" + str + \" is not properly formed.\";\r\n            }\r\n        }\r\n    } catch (Exception ex) {\r\n    }\r\n    if (exceptionMsg == null) {\r\n        exceptionMsg = \"TaskAttemptId string : \" + str + \" is not properly formed\";\r\n    }\r\n    throw new IllegalArgumentException(exceptionMsg);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getFileSystemCounterNames",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String[] getFileSystemCounterNames(String uriScheme)\n{\r\n    String scheme = StringUtils.toUpperCase(uriScheme);\r\n    return new String[] { scheme + \"_BYTES_READ\", scheme + \"_BYTES_WRITTEN\" };\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getOutputName",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String getOutputName(int partition)\n{\r\n    return \"part-\" + NUMBER_FORMAT.format(partition);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "setTaskDone",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setTaskDone()\n{\r\n    taskDone.set(true);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "setJobFile",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setJobFile(String jobFile)\n{\r\n    this.jobFile = jobFile;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getJobFile",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getJobFile()\n{\r\n    return jobFile;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getTaskID",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "TaskAttemptID getTaskID()\n{\r\n    return taskId;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getNumSlotsRequired",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int getNumSlotsRequired()\n{\r\n    return numSlotsRequired;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getCounters",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Counters getCounters()\n{\r\n    return counters;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getJobID",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "JobID getJobID()\n{\r\n    return taskId.getJobID();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "setJobTokenSecret",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setJobTokenSecret(SecretKey tokenSecret)\n{\r\n    this.tokenSecret = tokenSecret;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getEncryptedSpillKey",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "byte[] getEncryptedSpillKey()\n{\r\n    return encryptedSpillKey;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "setEncryptedSpillKey",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setEncryptedSpillKey(byte[] encryptedSpillKey)\n{\r\n    if (encryptedSpillKey != null) {\r\n        this.encryptedSpillKey = encryptedSpillKey;\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getJobTokenSecret",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "SecretKey getJobTokenSecret()\n{\r\n    return this.tokenSecret;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "setShuffleSecret",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setShuffleSecret(SecretKey shuffleSecret)\n{\r\n    this.shuffleSecret = shuffleSecret;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getShuffleSecret",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "SecretKey getShuffleSecret()\n{\r\n    return this.shuffleSecret;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getPartition",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int getPartition()\n{\r\n    return partition;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getPhase",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "TaskStatus.Phase getPhase()\n{\r\n    return this.taskStatus.getPhase();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "setPhase",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setPhase(TaskStatus.Phase phase)\n{\r\n    this.taskStatus.setPhase(phase);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "toWriteSkipRecs",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean toWriteSkipRecs()\n{\r\n    return writeSkipRecs;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "setWriteSkipRecs",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setWriteSkipRecs(boolean writeSkipRecs)\n{\r\n    this.writeSkipRecs = writeSkipRecs;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "reportFatalError",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void reportFatalError(TaskAttemptID id, Throwable throwable, String logMsg, boolean fastFail)\n{\r\n    LOG.error(logMsg);\r\n    if (ShutdownHookManager.get().isShutdownInProgress()) {\r\n        return;\r\n    }\r\n    Throwable tCause = throwable.getCause();\r\n    String cause = tCause == null ? StringUtils.stringifyException(throwable) : StringUtils.stringifyException(tCause);\r\n    try {\r\n        umbilical.fatalError(id, cause, fastFail);\r\n    } catch (IOException ioe) {\r\n        LOG.error(\"Failed to contact the tasktracker\", ioe);\r\n        System.exit(-1);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getFsStatistics",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "List<Statistics> getFsStatistics(Path path, Configuration conf) throws IOException\n{\r\n    List<Statistics> matchedStats = new ArrayList<FileSystem.Statistics>();\r\n    path = path.getFileSystem(conf).makeQualified(path);\r\n    String scheme = path.toUri().getScheme();\r\n    for (Statistics stats : FileSystem.getAllStatistics()) {\r\n        if (stats.getScheme().equals(scheme)) {\r\n            matchedStats.add(stats);\r\n        }\r\n    }\r\n    return matchedStats;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getSkipRanges",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "SortedRanges getSkipRanges()\n{\r\n    return skipRanges;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "setSkipRanges",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setSkipRanges(SortedRanges skipRanges)\n{\r\n    this.skipRanges = skipRanges;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "isSkipping",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean isSkipping()\n{\r\n    return skipping;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "setSkipping",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setSkipping(boolean skipping)\n{\r\n    this.skipping = skipping;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getState",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "TaskStatus.State getState()\n{\r\n    return this.taskStatus.getRunState();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "setState",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setState(TaskStatus.State state)\n{\r\n    this.taskStatus.setRunState(state);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "setTaskCleanupTask",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setTaskCleanupTask()\n{\r\n    taskCleanup = true;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "isTaskCleanupTask",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean isTaskCleanupTask()\n{\r\n    return taskCleanup;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "isJobCleanupTask",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean isJobCleanupTask()\n{\r\n    return jobCleanup;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "isJobAbortTask",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "boolean isJobAbortTask()\n{\r\n    return isJobCleanupTask() && (jobRunStateForCleanup == JobStatus.State.KILLED || jobRunStateForCleanup == JobStatus.State.FAILED);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "isJobSetupTask",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean isJobSetupTask()\n{\r\n    return jobSetup;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "setJobSetupTask",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setJobSetupTask()\n{\r\n    jobSetup = true;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "setJobCleanupTask",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setJobCleanupTask()\n{\r\n    jobCleanup = true;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "setJobCleanupTaskState",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setJobCleanupTaskState(JobStatus.State status)\n{\r\n    jobRunStateForCleanup = status;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "isMapOrReduce",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean isMapOrReduce()\n{\r\n    return !jobSetup && !jobCleanup && !taskCleanup;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getUser",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getUser()\n{\r\n    return user;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "setUser",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setUser(String user)\n{\r\n    this.user = user;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "write",
  "errType" : null,
  "containingMethodsNum" : 16,
  "sourceCodeText" : "void write(DataOutput out) throws IOException\n{\r\n    Text.writeString(out, jobFile);\r\n    taskId.write(out);\r\n    out.writeInt(partition);\r\n    out.writeInt(numSlotsRequired);\r\n    taskStatus.write(out);\r\n    skipRanges.write(out);\r\n    out.writeBoolean(skipping);\r\n    out.writeBoolean(jobCleanup);\r\n    if (jobCleanup) {\r\n        WritableUtils.writeEnum(out, jobRunStateForCleanup);\r\n    }\r\n    out.writeBoolean(jobSetup);\r\n    out.writeBoolean(writeSkipRecs);\r\n    out.writeBoolean(taskCleanup);\r\n    Text.writeString(out, user);\r\n    out.writeInt(encryptedSpillKey.length);\r\n    extraData.write(out);\r\n    out.write(encryptedSpillKey);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "readFields",
  "errType" : null,
  "containingMethodsNum" : 19,
  "sourceCodeText" : "void readFields(DataInput in) throws IOException\n{\r\n    jobFile = StringInterner.weakIntern(Text.readString(in));\r\n    taskId = TaskAttemptID.read(in);\r\n    partition = in.readInt();\r\n    numSlotsRequired = in.readInt();\r\n    taskStatus.readFields(in);\r\n    skipRanges.readFields(in);\r\n    currentRecIndexIterator = skipRanges.skipRangeIterator();\r\n    currentRecStartIndex = currentRecIndexIterator.next();\r\n    skipping = in.readBoolean();\r\n    jobCleanup = in.readBoolean();\r\n    if (jobCleanup) {\r\n        jobRunStateForCleanup = WritableUtils.readEnum(in, JobStatus.State.class);\r\n    }\r\n    jobSetup = in.readBoolean();\r\n    writeSkipRecs = in.readBoolean();\r\n    taskCleanup = in.readBoolean();\r\n    if (taskCleanup) {\r\n        setPhase(TaskStatus.Phase.CLEANUP);\r\n    }\r\n    user = StringInterner.weakIntern(Text.readString(in));\r\n    int len = in.readInt();\r\n    encryptedSpillKey = new byte[len];\r\n    extraData.readFields(in);\r\n    in.readFully(encryptedSpillKey);\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "toString",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String toString()\n{\r\n    return taskId.toString();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "localizeConfiguration",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void localizeConfiguration(JobConf conf) throws IOException\n{\r\n    conf.set(JobContext.TASK_ID, taskId.getTaskID().toString());\r\n    conf.set(JobContext.TASK_ATTEMPT_ID, taskId.toString());\r\n    conf.setBoolean(JobContext.TASK_ISMAP, isMapTask());\r\n    conf.setInt(JobContext.TASK_PARTITION, partition);\r\n    conf.set(JobContext.ID, taskId.getJobID().toString());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "run",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void run(JobConf job, TaskUmbilicalProtocol umbilical) throws IOException, ClassNotFoundException, InterruptedException",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "isMapTask",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean isMapTask()",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getProgress",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Progress getProgress()\n{\r\n    return taskProgress;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "initialize",
  "errType" : null,
  "containingMethodsNum" : 16,
  "sourceCodeText" : "void initialize(JobConf job, JobID id, Reporter reporter, boolean useNewApi) throws IOException, ClassNotFoundException, InterruptedException\n{\r\n    jobContext = new JobContextImpl(job, id, reporter);\r\n    taskContext = new TaskAttemptContextImpl(job, taskId, reporter);\r\n    if (getState() == TaskStatus.State.UNASSIGNED) {\r\n        setState(TaskStatus.State.RUNNING);\r\n    }\r\n    if (useNewApi) {\r\n        if (LOG.isDebugEnabled()) {\r\n            LOG.debug(\"using new api for output committer\");\r\n        }\r\n        outputFormat = ReflectionUtils.newInstance(taskContext.getOutputFormatClass(), job);\r\n        committer = outputFormat.getOutputCommitter(taskContext);\r\n    } else {\r\n        committer = conf.getOutputCommitter();\r\n    }\r\n    Path outputPath = FileOutputFormat.getOutputPath(conf);\r\n    if (outputPath != null) {\r\n        if ((committer instanceof FileOutputCommitter)) {\r\n            FileOutputFormat.setWorkOutputPath(conf, ((FileOutputCommitter) committer).getTaskAttemptPath(taskContext));\r\n        } else {\r\n            FileOutputFormat.setWorkOutputPath(conf, outputPath);\r\n        }\r\n    }\r\n    committer.setupTask(taskContext);\r\n    Class<? extends ResourceCalculatorProcessTree> clazz = conf.getClass(MRConfig.RESOURCE_CALCULATOR_PROCESS_TREE, null, ResourceCalculatorProcessTree.class);\r\n    pTree = ResourceCalculatorProcessTree.getResourceCalculatorProcessTree(System.getenv().get(\"JVM_PID\"), clazz, conf);\r\n    LOG.info(\" Using ResourceCalculatorProcessTree : \" + pTree);\r\n    if (pTree != null) {\r\n        pTree.updateProcessTree();\r\n        initCpuCumulativeTime = pTree.getCumulativeCpuTime();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "normalizeStatus",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "String normalizeStatus(String status, Configuration conf)\n{\r\n    int progressStatusLength = conf.getInt(MRConfig.PROGRESS_STATUS_LEN_LIMIT_KEY, MRConfig.PROGRESS_STATUS_LEN_LIMIT_DEFAULT);\r\n    if (status.length() > progressStatusLength) {\r\n        LOG.warn(\"Task status: \\\"\" + status + \"\\\" truncated to max limit (\" + progressStatusLength + \" characters)\");\r\n        status = status.substring(0, progressStatusLength);\r\n    }\r\n    return status;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "reportNextRecordRange",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void reportNextRecordRange(final TaskUmbilicalProtocol umbilical, long nextRecIndex) throws IOException\n{\r\n    long len = nextRecIndex - currentRecStartIndex + 1;\r\n    SortedRanges.Range range = new SortedRanges.Range(currentRecStartIndex, len);\r\n    taskStatus.setNextRecordRange(range);\r\n    if (LOG.isDebugEnabled()) {\r\n        LOG.debug(\"sending reportNextRecordRange \" + range);\r\n    }\r\n    umbilical.reportNextRecordRange(taskId, range);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "startReporter",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "TaskReporter startReporter(final TaskUmbilicalProtocol umbilical)\n{\r\n    TaskReporter reporter = new TaskReporter(getProgress(), umbilical);\r\n    reporter.startCommunicationThread();\r\n    return reporter;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "updateResourceCounters",
  "errType" : null,
  "containingMethodsNum" : 14,
  "sourceCodeText" : "void updateResourceCounters()\n{\r\n    updateHeapUsageCounter();\r\n    if (pTree == null) {\r\n        return;\r\n    }\r\n    pTree.updateProcessTree();\r\n    long cpuTime = pTree.getCumulativeCpuTime();\r\n    long pMem = pTree.getRssMemorySize();\r\n    long vMem = pTree.getVirtualMemorySize();\r\n    if (cpuTime != ResourceCalculatorProcessTree.UNAVAILABLE && initCpuCumulativeTime != ResourceCalculatorProcessTree.UNAVAILABLE) {\r\n        cpuTime -= initCpuCumulativeTime;\r\n    }\r\n    if (cpuTime != ResourceCalculatorProcessTree.UNAVAILABLE) {\r\n        counters.findCounter(TaskCounter.CPU_MILLISECONDS).setValue(cpuTime);\r\n    }\r\n    if (pMem != ResourceCalculatorProcessTree.UNAVAILABLE) {\r\n        counters.findCounter(TaskCounter.PHYSICAL_MEMORY_BYTES).setValue(pMem);\r\n    }\r\n    if (vMem != ResourceCalculatorProcessTree.UNAVAILABLE) {\r\n        counters.findCounter(TaskCounter.VIRTUAL_MEMORY_BYTES).setValue(vMem);\r\n    }\r\n    if (pMem != ResourceCalculatorProcessTree.UNAVAILABLE) {\r\n        TaskCounter counter = isMapTask() ? TaskCounter.MAP_PHYSICAL_MEMORY_BYTES_MAX : TaskCounter.REDUCE_PHYSICAL_MEMORY_BYTES_MAX;\r\n        Counters.Counter pMemCounter = counters.findCounter(counter);\r\n        pMemCounter.setValue(Math.max(pMemCounter.getValue(), pMem));\r\n    }\r\n    if (vMem != ResourceCalculatorProcessTree.UNAVAILABLE) {\r\n        TaskCounter counter = isMapTask() ? TaskCounter.MAP_VIRTUAL_MEMORY_BYTES_MAX : TaskCounter.REDUCE_VIRTUAL_MEMORY_BYTES_MAX;\r\n        Counters.Counter vMemCounter = counters.findCounter(counter);\r\n        vMemCounter.setValue(Math.max(vMemCounter.getValue(), vMem));\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "updateCounters",
  "errType" : null,
  "containingMethodsNum" : 15,
  "sourceCodeText" : "void updateCounters()\n{\r\n    Map<String, List<FileSystem.Statistics>> map = new HashMap<String, List<FileSystem.Statistics>>();\r\n    for (Statistics stat : FileSystem.getAllStatistics()) {\r\n        String uriScheme = stat.getScheme();\r\n        if (map.containsKey(uriScheme)) {\r\n            List<FileSystem.Statistics> list = map.get(uriScheme);\r\n            list.add(stat);\r\n        } else {\r\n            List<FileSystem.Statistics> list = new ArrayList<FileSystem.Statistics>();\r\n            list.add(stat);\r\n            map.put(uriScheme, list);\r\n        }\r\n    }\r\n    for (Map.Entry<String, List<FileSystem.Statistics>> entry : map.entrySet()) {\r\n        FileSystemStatisticUpdater updater = statisticUpdaters.get(entry.getKey());\r\n        if (updater == null) {\r\n            updater = new FileSystemStatisticUpdater(entry.getValue(), entry.getKey());\r\n            statisticUpdaters.put(entry.getKey(), updater);\r\n        }\r\n        updater.updateCounters();\r\n    }\r\n    gcUpdater.incrementGcCounter();\r\n    updateResourceCounters();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "updateHeapUsageCounter",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void updateHeapUsageCounter()\n{\r\n    long currentHeapUsage = Runtime.getRuntime().totalMemory();\r\n    counters.findCounter(TaskCounter.COMMITTED_HEAP_BYTES).setValue(currentHeapUsage);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "done",
  "errType" : [ "InterruptedException", "IOException" ],
  "containingMethodsNum" : 19,
  "sourceCodeText" : "void done(TaskUmbilicalProtocol umbilical, TaskReporter reporter) throws IOException, InterruptedException\n{\r\n    updateCounters();\r\n    if (taskStatus.getRunState() == TaskStatus.State.PREEMPTED) {\r\n        committer.commitTask(taskContext);\r\n        umbilical.preempted(taskId, taskStatus);\r\n        taskDone.set(true);\r\n        reporter.stopCommunicationThread();\r\n        return;\r\n    }\r\n    LOG.info(\"Task:\" + taskId + \" is done.\" + \" And is in the process of committing\");\r\n    boolean commitRequired = isCommitRequired();\r\n    if (commitRequired) {\r\n        int retries = MAX_RETRIES;\r\n        setState(TaskStatus.State.COMMIT_PENDING);\r\n        while (true) {\r\n            try {\r\n                umbilical.commitPending(taskId, taskStatus);\r\n                break;\r\n            } catch (InterruptedException ie) {\r\n            } catch (IOException ie) {\r\n                LOG.warn(\"Failure sending commit pending: \" + StringUtils.stringifyException(ie));\r\n                if (--retries == 0) {\r\n                    System.exit(67);\r\n                }\r\n            }\r\n        }\r\n        commit(umbilical, reporter, committer);\r\n    }\r\n    taskDone.set(true);\r\n    reporter.stopCommunicationThread();\r\n    updateCounters();\r\n    sendLastUpdate(umbilical);\r\n    sendDone(umbilical);\r\n    LOG.info(\"Final Counters for \" + taskId + \": \" + getCounters().toString());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "isCommitRequired",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "boolean isCommitRequired() throws IOException\n{\r\n    boolean commitRequired = false;\r\n    if (isMapOrReduce()) {\r\n        commitRequired = committer.needsTaskCommit(taskContext);\r\n    }\r\n    return commitRequired;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "statusUpdate",
  "errType" : [ "InterruptedException", "IOException" ],
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void statusUpdate(TaskUmbilicalProtocol umbilical) throws IOException\n{\r\n    int retries = MAX_RETRIES;\r\n    while (true) {\r\n        try {\r\n            if (!umbilical.statusUpdate(getTaskID(), taskStatus).getTaskFound()) {\r\n                if (uberized) {\r\n                    LOG.warn(\"Task no longer available: \" + taskId);\r\n                    break;\r\n                } else {\r\n                    LOG.warn(\"Parent died.  Exiting \" + taskId);\r\n                    ExitUtil.terminate(66);\r\n                }\r\n            }\r\n            taskStatus.clearStatus();\r\n            return;\r\n        } catch (InterruptedException ie) {\r\n            Thread.currentThread().interrupt();\r\n        } catch (IOException ie) {\r\n            LOG.warn(\"Failure sending status update: \" + StringUtils.stringifyException(ie));\r\n            if (--retries == 0) {\r\n                throw ie;\r\n            }\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "sendLastUpdate",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void sendLastUpdate(TaskUmbilicalProtocol umbilical) throws IOException\n{\r\n    taskStatus.setOutputSize(calculateOutputSize());\r\n    taskStatus.statusUpdate(taskProgress.get(), taskProgress.toString(), counters);\r\n    statusUpdate(umbilical);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "calculateOutputSize",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 7,
  "sourceCodeText" : "long calculateOutputSize() throws IOException\n{\r\n    if (!isMapOrReduce()) {\r\n        return -1;\r\n    }\r\n    if (isMapTask() && conf.getNumReduceTasks() > 0) {\r\n        try {\r\n            Path mapOutput = mapOutputFile.getOutputFile();\r\n            FileSystem localFS = FileSystem.getLocal(conf);\r\n            return localFS.getFileStatus(mapOutput).getLen();\r\n        } catch (IOException e) {\r\n            LOG.warn(\"Could not find output size \", e);\r\n        }\r\n    }\r\n    return -1;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "sendDone",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void sendDone(TaskUmbilicalProtocol umbilical) throws IOException\n{\r\n    int retries = MAX_RETRIES;\r\n    while (true) {\r\n        try {\r\n            umbilical.done(getTaskID());\r\n            LOG.info(\"Task '\" + taskId + \"' done.\");\r\n            return;\r\n        } catch (IOException ie) {\r\n            LOG.warn(\"Failure signalling completion: \" + StringUtils.stringifyException(ie));\r\n            if (--retries == 0) {\r\n                throw ie;\r\n            }\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "commit",
  "errType" : [ "IOException", "InterruptedException", "IOException" ],
  "containingMethodsNum" : 10,
  "sourceCodeText" : "void commit(TaskUmbilicalProtocol umbilical, TaskReporter reporter, org.apache.hadoop.mapreduce.OutputCommitter committer) throws IOException\n{\r\n    int retries = MAX_RETRIES;\r\n    while (true) {\r\n        try {\r\n            while (!umbilical.canCommit(taskId)) {\r\n                try {\r\n                    Thread.sleep(1000);\r\n                } catch (InterruptedException ie) {\r\n                }\r\n                reporter.setProgressFlag();\r\n            }\r\n            break;\r\n        } catch (IOException ie) {\r\n            LOG.warn(\"Failure asking whether task can commit: \" + StringUtils.stringifyException(ie));\r\n            if (--retries == 0) {\r\n                discardOutput(taskContext);\r\n                System.exit(68);\r\n            }\r\n        }\r\n    }\r\n    try {\r\n        LOG.info(\"Task \" + taskId + \" is allowed to commit now\");\r\n        committer.commitTask(taskContext);\r\n        return;\r\n    } catch (IOException iee) {\r\n        LOG.warn(\"Failure committing: \" + StringUtils.stringifyException(iee));\r\n        discardOutput(taskContext);\r\n        throw iee;\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 3,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "discardOutput",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void discardOutput(TaskAttemptContext taskContext)\n{\r\n    try {\r\n        committer.abortTask(taskContext);\r\n    } catch (IOException ioe) {\r\n        LOG.warn(\"Failure cleaning up: \" + StringUtils.stringifyException(ioe));\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "runTaskCleanupTask",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void runTaskCleanupTask(TaskUmbilicalProtocol umbilical, TaskReporter reporter) throws IOException, InterruptedException\n{\r\n    taskCleanup(umbilical);\r\n    done(umbilical, reporter);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "taskCleanup",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void taskCleanup(TaskUmbilicalProtocol umbilical) throws IOException\n{\r\n    setPhase(TaskStatus.Phase.CLEANUP);\r\n    getProgress().setStatus(\"cleanup\");\r\n    statusUpdate(umbilical);\r\n    LOG.info(\"Running cleanup for the task\");\r\n    committer.abortTask(taskContext);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "runJobCleanupTask",
  "errType" : null,
  "containingMethodsNum" : 16,
  "sourceCodeText" : "void runJobCleanupTask(TaskUmbilicalProtocol umbilical, TaskReporter reporter) throws IOException, InterruptedException\n{\r\n    setPhase(TaskStatus.Phase.CLEANUP);\r\n    getProgress().setStatus(\"cleanup\");\r\n    statusUpdate(umbilical);\r\n    LOG.info(\"Cleaning up job\");\r\n    if (jobRunStateForCleanup == JobStatus.State.FAILED || jobRunStateForCleanup == JobStatus.State.KILLED) {\r\n        LOG.info(\"Aborting job with runstate : \" + jobRunStateForCleanup.name());\r\n        if (conf.getUseNewMapper()) {\r\n            committer.abortJob(jobContext, jobRunStateForCleanup);\r\n        } else {\r\n            org.apache.hadoop.mapred.OutputCommitter oldCommitter = (org.apache.hadoop.mapred.OutputCommitter) committer;\r\n            oldCommitter.abortJob(jobContext, jobRunStateForCleanup);\r\n        }\r\n    } else if (jobRunStateForCleanup == JobStatus.State.SUCCEEDED) {\r\n        LOG.info(\"Committing job\");\r\n        committer.commitJob(jobContext);\r\n    } else {\r\n        throw new IOException(\"Invalid state of the job for cleanup. State found \" + jobRunStateForCleanup + \" expecting \" + JobStatus.State.SUCCEEDED + \", \" + JobStatus.State.FAILED + \" or \" + JobStatus.State.KILLED);\r\n    }\r\n    JobConf conf = new JobConf(jobContext.getConfiguration());\r\n    if (!keepTaskFiles(conf)) {\r\n        String jobTempDir = conf.get(MRJobConfig.MAPREDUCE_JOB_DIR);\r\n        Path jobTempDirPath = new Path(jobTempDir);\r\n        FileSystem fs = jobTempDirPath.getFileSystem(conf);\r\n        fs.delete(jobTempDirPath, true);\r\n    }\r\n    done(umbilical, reporter);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "keepTaskFiles",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "boolean keepTaskFiles(JobConf conf)\n{\r\n    return (conf.getKeepTaskFilesPattern() != null || conf.getKeepFailedTaskFiles());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "runJobSetupTask",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void runJobSetupTask(TaskUmbilicalProtocol umbilical, TaskReporter reporter) throws IOException, InterruptedException\n{\r\n    getProgress().setStatus(\"setup\");\r\n    committer.setupJob(jobContext);\r\n    done(umbilical, reporter);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "setConf",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void setConf(Configuration conf)\n{\r\n    if (conf instanceof JobConf) {\r\n        this.conf = (JobConf) conf;\r\n    } else {\r\n        this.conf = new JobConf(conf);\r\n    }\r\n    this.mapOutputFile = ReflectionUtils.newInstance(conf.getClass(MRConfig.TASK_LOCAL_OUTPUT_CLASS, MROutputFiles.class, MapOutputFile.class), conf);\r\n    this.lDirAlloc = new LocalDirAllocator(MRConfig.LOCAL_DIR);\r\n    String[] hostToResolved = conf.getStrings(MRConfig.STATIC_RESOLUTIONS);\r\n    if (hostToResolved != null) {\r\n        for (String str : hostToResolved) {\r\n            String name = str.substring(0, str.indexOf('='));\r\n            String resolvedName = str.substring(str.indexOf('=') + 1);\r\n            NetUtils.addStaticResolution(name, resolvedName);\r\n        }\r\n    }\r\n    uberized = conf.getBoolean(\"mapreduce.task.uberized\", false);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getConf",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Configuration getConf()\n{\r\n    return this.conf;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getMapOutputFile",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "MapOutputFile getMapOutputFile()\n{\r\n    return mapOutputFile;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "createReduceContext",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "org.apache.hadoop.mapreduce.Reducer<INKEY, INVALUE, OUTKEY, OUTVALUE>.Context createReduceContext(org.apache.hadoop.mapreduce.Reducer<INKEY, INVALUE, OUTKEY, OUTVALUE> reducer, Configuration job, org.apache.hadoop.mapreduce.TaskAttemptID taskId, RawKeyValueIterator rIter, org.apache.hadoop.mapreduce.Counter inputKeyCounter, org.apache.hadoop.mapreduce.Counter inputValueCounter, org.apache.hadoop.mapreduce.RecordWriter<OUTKEY, OUTVALUE> output, org.apache.hadoop.mapreduce.OutputCommitter committer, org.apache.hadoop.mapreduce.StatusReporter reporter, RawComparator<INKEY> comparator, Class<INKEY> keyClass, Class<INVALUE> valueClass) throws IOException, InterruptedException\n{\r\n    org.apache.hadoop.mapreduce.ReduceContext<INKEY, INVALUE, OUTKEY, OUTVALUE> reduceContext = new ReduceContextImpl<INKEY, INVALUE, OUTKEY, OUTVALUE>(job, taskId, rIter, inputKeyCounter, inputValueCounter, output, committer, reporter, comparator, keyClass, valueClass);\r\n    org.apache.hadoop.mapreduce.Reducer<INKEY, INVALUE, OUTKEY, OUTVALUE>.Context reducerContext = new WrappedReducer<INKEY, INVALUE, OUTKEY, OUTVALUE>().getReducerContext(reduceContext);\r\n    return reducerContext;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getExtraData",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "BytesWritable getExtraData()\n{\r\n    return extraData;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "setExtraData",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setExtraData(BytesWritable extraData)\n{\r\n    this.extraData = extraData;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "size",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "int size()\n{\r\n    return entries.capacity() / (MapTask.MAP_OUTPUT_INDEX_RECORD_LENGTH / 8);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getIndex",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "IndexRecord getIndex(int partition)\n{\r\n    final int pos = partition * MapTask.MAP_OUTPUT_INDEX_RECORD_LENGTH / 8;\r\n    return new IndexRecord(entries.get(pos), entries.get(pos + 1), entries.get(pos + 2));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "putIndex",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void putIndex(IndexRecord rec, int partition)\n{\r\n    final int pos = partition * MapTask.MAP_OUTPUT_INDEX_RECORD_LENGTH / 8;\r\n    entries.put(pos, rec.startOffset);\r\n    entries.put(pos + 1, rec.rawLength);\r\n    entries.put(pos + 2, rec.partLength);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "writeToFile",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void writeToFile(Path loc, JobConf job) throws IOException\n{\r\n    writeToFile(loc, job, new PureJavaCrc32());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "writeToFile",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void writeToFile(Path loc, JobConf job, Checksum crc) throws IOException\n{\r\n    final FileSystem rfs = FileSystem.getLocal(job).getRaw();\r\n    CheckedOutputStream chk = null;\r\n    final FSDataOutputStream out = rfs.create(loc);\r\n    try {\r\n        if (crc != null) {\r\n            crc.reset();\r\n            chk = new CheckedOutputStream(out, crc);\r\n            chk.write(buf.array());\r\n            out.writeLong(chk.getChecksum().getValue());\r\n        } else {\r\n            out.write(buf.array());\r\n        }\r\n    } finally {\r\n        if (chk != null) {\r\n            chk.close();\r\n        } else {\r\n            out.close();\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "close",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void close() throws IOException\n{\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "configure",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void configure(JobConf job)\n{\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\join",
  "methodName" : "emit",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "V emit(TupleWritable dst)\n{\r\n    return (V) dst.iterator().next();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\join",
  "methodName" : "createValue",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "V createValue()\n{\r\n    if (null == valueclass) {\r\n        Class<?> cls = kids[kids.length - 1].createValue().getClass();\r\n        for (int i = kids.length - 1; cls.equals(NullWritable.class); i--) {\r\n            cls = kids[i].createValue().getClass();\r\n        }\r\n        valueclass = cls.asSubclass(Writable.class);\r\n    }\r\n    if (valueclass.equals(NullWritable.class)) {\r\n        return (V) NullWritable.get();\r\n    }\r\n    return (V) ReflectionUtils.newInstance(valueclass, null);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\join",
  "methodName" : "fillJoinCollector",
  "errType" : null,
  "containingMethodsNum" : 17,
  "sourceCodeText" : "void fillJoinCollector(K iterkey) throws IOException, InterruptedException\n{\r\n    final PriorityQueue<ComposableRecordReader<K, ?>> q = getRecordReaderQueue();\r\n    if (q != null && !q.isEmpty()) {\r\n        int highpos = -1;\r\n        ArrayList<ComposableRecordReader<K, ?>> list = new ArrayList<ComposableRecordReader<K, ?>>(kids.length);\r\n        q.peek().key(iterkey);\r\n        final WritableComparator cmp = getComparator();\r\n        while (0 == cmp.compare(q.peek().key(), iterkey)) {\r\n            ComposableRecordReader<K, ?> t = q.poll();\r\n            if (-1 == highpos || list.get(highpos).id() < t.id()) {\r\n                highpos = list.size();\r\n            }\r\n            list.add(t);\r\n            if (q.isEmpty())\r\n                break;\r\n        }\r\n        ComposableRecordReader<K, ?> t = list.remove(highpos);\r\n        t.accept(jc, iterkey);\r\n        for (ComposableRecordReader<K, ?> rr : list) {\r\n            rr.skip(iterkey);\r\n        }\r\n        list.add(t);\r\n        for (ComposableRecordReader<K, ?> rr : list) {\r\n            if (rr.hasNext()) {\r\n                q.add(rr);\r\n            }\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\util",
  "methodName" : "redact",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void redact(final Configuration conf)\n{\r\n    for (String prop : conf.getTrimmedStringCollection(MRJobConfig.MR_JOB_REDACTED_PROPERTIES)) {\r\n        conf.set(prop, REDACTION_REPLACEMENT_VAL);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\util",
  "methodName" : "getTaskProgressReportInterval",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "long getTaskProgressReportInterval(final Configuration conf)\n{\r\n    long taskHeartbeatTimeOut = conf.getLong(MRJobConfig.TASK_TIMEOUT, MRJobConfig.DEFAULT_TASK_TIMEOUT_MILLIS);\r\n    return conf.getLong(MRJobConfig.TASK_PROGRESS_REPORT_INTERVAL, (long) (TASK_REPORT_INTERVAL_TO_TIMEOUT_RATIO * taskHeartbeatTimeOut));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\util",
  "methodName" : "setTaskLogProgressDeltaThresholds",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void setTaskLogProgressDeltaThresholds(final Configuration conf)\n{\r\n    if (progressMinDeltaThreshold == null) {\r\n        progressMinDeltaThreshold = new Double(PROGRESS_MIN_DELTA_FACTOR * conf.getDouble(MRJobConfig.TASK_LOG_PROGRESS_DELTA_THRESHOLD, MRJobConfig.TASK_LOG_PROGRESS_DELTA_THRESHOLD_DEFAULT));\r\n    }\r\n    if (progressMaxWaitDeltaTimeThreshold == null) {\r\n        progressMaxWaitDeltaTimeThreshold = TimeUnit.SECONDS.toMillis(conf.getLong(MRJobConfig.TASK_LOG_PROGRESS_WAIT_INTERVAL_SECONDS, MRJobConfig.TASK_LOG_PROGRESS_WAIT_INTERVAL_SECONDS_DEFAULT));\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\util",
  "methodName" : "getTaskProgressMinDeltaThreshold",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "double getTaskProgressMinDeltaThreshold()\n{\r\n    if (progressMinDeltaThreshold == null) {\r\n        return PROGRESS_MIN_DELTA_FACTOR * MRJobConfig.TASK_LOG_PROGRESS_DELTA_THRESHOLD_DEFAULT;\r\n    }\r\n    return progressMinDeltaThreshold.doubleValue();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\util",
  "methodName" : "getTaskProgressWaitDeltaTimeThreshold",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "long getTaskProgressWaitDeltaTimeThreshold()\n{\r\n    if (progressMaxWaitDeltaTimeThreshold == null) {\r\n        return TimeUnit.SECONDS.toMillis(MRJobConfig.TASK_LOG_PROGRESS_WAIT_INTERVAL_SECONDS_DEFAULT);\r\n    }\r\n    return progressMaxWaitDeltaTimeThreshold.longValue();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\util",
  "methodName" : "convertTaskProgressToFactor",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "double convertTaskProgressToFactor(final float progress)\n{\r\n    return Math.floor(progress * MRJobConfUtil.PROGRESS_MIN_DELTA_FACTOR);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\util",
  "methodName" : "initEncryptedIntermediateConfigsForTesting",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "Configuration initEncryptedIntermediateConfigsForTesting(Configuration conf)\n{\r\n    Configuration config = (conf == null) ? new Configuration() : conf;\r\n    final String childJVMOpts = TEST_JVM_SECURITY_EGD_OPT.concat(\" \").concat(config.get(\"mapred.child.java.opts\", \" \"));\r\n    config.set(\"yarn.app.mapreduce.am.admin-command-opts\", TEST_JVM_SECURITY_EGD_OPT);\r\n    config.set(\"mapred.child.java.opts\", childJVMOpts);\r\n    config.setBoolean(\"mapreduce.job.encrypted-intermediate-data\", true);\r\n    return config;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\util",
  "methodName" : "setLocalDirectoriesConfigForTesting",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "Configuration setLocalDirectoriesConfigForTesting(Configuration conf, File testRootDir)\n{\r\n    Configuration config = (conf == null) ? new Configuration() : conf;\r\n    final File hadoopLocalDir = new File(testRootDir, \"hadoop-dir\");\r\n    if (!hadoopLocalDir.getAbsoluteFile().mkdirs()) {\r\n        LOG.info(\"{} directory already exists\", hadoopLocalDir.getPath());\r\n    }\r\n    Path mapredHadoopTempDir = new Path(hadoopLocalDir.getPath());\r\n    Path mapredSystemDir = new Path(mapredHadoopTempDir, \"system\");\r\n    Path stagingDir = new Path(mapredHadoopTempDir, \"tmp/staging\");\r\n    config.set(\"mapreduce.jobtracker.staging.root.dir\", stagingDir.toString());\r\n    config.set(\"mapreduce.jobtracker.system.dir\", mapredSystemDir.toString());\r\n    config.set(\"mapreduce.cluster.temp.dir\", mapredHadoopTempDir.toString());\r\n    config.set(\"mapreduce.cluster.local.dir\", new Path(mapredHadoopTempDir, \"local\").toString());\r\n    return config;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "createKey",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "LongWritable createKey()\n{\r\n    return new LongWritable();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "createValue",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Text createValue()\n{\r\n    return new Text();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "isCompressedInput",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean isCompressedInput()\n{\r\n    return (codec != null);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "maxBytesToConsume",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "int maxBytesToConsume(long pos)\n{\r\n    return isCompressedInput() ? Integer.MAX_VALUE : (int) Math.max(Math.min(Integer.MAX_VALUE, end - pos), maxLineLength);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getFilePosition",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "long getFilePosition() throws IOException\n{\r\n    long retVal;\r\n    if (isCompressedInput() && null != filePosition) {\r\n        retVal = filePosition.getPos();\r\n    } else {\r\n        retVal = pos;\r\n    }\r\n    return retVal;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "skipUtfByteOrderMark",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "int skipUtfByteOrderMark(Text value) throws IOException\n{\r\n    int newMaxLineLength = (int) Math.min(3L + (long) maxLineLength, Integer.MAX_VALUE);\r\n    int newSize = in.readLine(value, newMaxLineLength, maxBytesToConsume(pos));\r\n    pos += newSize;\r\n    int textLength = value.getLength();\r\n    byte[] textBytes = value.getBytes();\r\n    if ((textLength >= 3) && (textBytes[0] == (byte) 0xEF) && (textBytes[1] == (byte) 0xBB) && (textBytes[2] == (byte) 0xBF)) {\r\n        LOG.info(\"Found UTF-8 BOM and skipped it\");\r\n        textLength -= 3;\r\n        newSize -= 3;\r\n        if (textLength > 0) {\r\n            textBytes = value.copyBytes();\r\n            value.set(textBytes, 3, textLength);\r\n        } else {\r\n            value.clear();\r\n        }\r\n    }\r\n    return newSize;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "next",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "boolean next(LongWritable key, Text value) throws IOException\n{\r\n    while (getFilePosition() <= end || in.needAdditionalRecordAfterSplit()) {\r\n        key.set(pos);\r\n        int newSize = 0;\r\n        if (pos == 0) {\r\n            newSize = skipUtfByteOrderMark(value);\r\n        } else {\r\n            newSize = in.readLine(value, maxLineLength, maxBytesToConsume(pos));\r\n            pos += newSize;\r\n        }\r\n        if (newSize == 0) {\r\n            return false;\r\n        }\r\n        if (newSize < maxLineLength) {\r\n            return true;\r\n        }\r\n        LOG.info(\"Skipped line of size \" + newSize + \" at pos \" + (pos - newSize));\r\n    }\r\n    return false;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getProgress",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "float getProgress() throws IOException\n{\r\n    if (start == end) {\r\n        return 0.0f;\r\n    } else {\r\n        return Math.min(1.0f, (getFilePosition() - start) / (float) (end - start));\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getPos",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getPos() throws IOException\n{\r\n    return pos;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "close",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void close() throws IOException\n{\r\n    try {\r\n        if (in != null) {\r\n            in.close();\r\n        }\r\n    } finally {\r\n        if (decompressor != null) {\r\n            CodecPool.returnDecompressor(decompressor);\r\n            decompressor = null;\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "copyAndConfigureFiles",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void copyAndConfigureFiles(Job job, Path jobSubmitDir) throws IOException\n{\r\n    Configuration conf = job.getConfiguration();\r\n    boolean useWildcards = conf.getBoolean(Job.USE_WILDCARD_FOR_LIBJARS, Job.DEFAULT_USE_WILDCARD_FOR_LIBJARS);\r\n    JobResourceUploader rUploader = new JobResourceUploader(jtFs, useWildcards);\r\n    rUploader.uploadResources(job, jobSubmitDir);\r\n    job.getWorkingDirectory();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "submitJobInternal",
  "errType" : [ "NoSuchAlgorithmException" ],
  "containingMethodsNum" : 48,
  "sourceCodeText" : "JobStatus submitJobInternal(Job job, Cluster cluster) throws ClassNotFoundException, InterruptedException, IOException\n{\r\n    checkSpecs(job);\r\n    Configuration conf = job.getConfiguration();\r\n    addMRFrameworkToDistributedCache(conf);\r\n    Path jobStagingArea = JobSubmissionFiles.getStagingDir(cluster, conf);\r\n    InetAddress ip = InetAddress.getLocalHost();\r\n    if (ip != null) {\r\n        submitHostAddress = ip.getHostAddress();\r\n        submitHostName = ip.getHostName();\r\n        conf.set(MRJobConfig.JOB_SUBMITHOST, submitHostName);\r\n        conf.set(MRJobConfig.JOB_SUBMITHOSTADDR, submitHostAddress);\r\n    }\r\n    JobID jobId = submitClient.getNewJobID();\r\n    job.setJobID(jobId);\r\n    Path submitJobDir = new Path(jobStagingArea, jobId.toString());\r\n    JobStatus status = null;\r\n    try {\r\n        conf.set(MRJobConfig.USER_NAME, UserGroupInformation.getCurrentUser().getShortUserName());\r\n        conf.set(\"hadoop.http.filter.initializers\", \"org.apache.hadoop.yarn.server.webproxy.amfilter.AmFilterInitializer\");\r\n        conf.set(MRJobConfig.MAPREDUCE_JOB_DIR, submitJobDir.toString());\r\n        LOG.debug(\"Configuring job \" + jobId + \" with \" + submitJobDir + \" as the submit dir\");\r\n        TokenCache.obtainTokensForNamenodes(job.getCredentials(), new Path[] { submitJobDir }, conf);\r\n        populateTokenCache(conf, job.getCredentials());\r\n        if (TokenCache.getShuffleSecretKey(job.getCredentials()) == null) {\r\n            KeyGenerator keyGen;\r\n            try {\r\n                keyGen = KeyGenerator.getInstance(SHUFFLE_KEYGEN_ALGORITHM);\r\n                keyGen.init(SHUFFLE_KEY_LENGTH);\r\n            } catch (NoSuchAlgorithmException e) {\r\n                throw new IOException(\"Error generating shuffle secret key\", e);\r\n            }\r\n            SecretKey shuffleKey = keyGen.generateKey();\r\n            TokenCache.setShuffleSecretKey(shuffleKey.getEncoded(), job.getCredentials());\r\n        }\r\n        if (CryptoUtils.isEncryptedSpillEnabled(conf)) {\r\n            conf.setInt(MRJobConfig.MR_AM_MAX_ATTEMPTS, 1);\r\n            LOG.warn(\"Max job attempts set to 1 since encrypted intermediate\" + \"data spill is enabled\");\r\n        }\r\n        copyAndConfigureFiles(job, submitJobDir);\r\n        Path submitJobFile = JobSubmissionFiles.getJobConfPath(submitJobDir);\r\n        LOG.debug(\"Creating splits at \" + jtFs.makeQualified(submitJobDir));\r\n        int maps = writeSplits(job, submitJobDir);\r\n        conf.setInt(MRJobConfig.NUM_MAPS, maps);\r\n        LOG.info(\"number of splits:\" + maps);\r\n        int maxMaps = conf.getInt(MRJobConfig.JOB_MAX_MAP, MRJobConfig.DEFAULT_JOB_MAX_MAP);\r\n        if (maxMaps >= 0 && maxMaps < maps) {\r\n            throw new IllegalArgumentException(\"The number of map tasks \" + maps + \" exceeded limit \" + maxMaps);\r\n        }\r\n        String queue = conf.get(MRJobConfig.QUEUE_NAME, JobConf.DEFAULT_QUEUE_NAME);\r\n        AccessControlList acl = submitClient.getQueueAdmins(queue);\r\n        conf.set(toFullPropertyName(queue, QueueACL.ADMINISTER_JOBS.getAclName()), acl.getAclString());\r\n        TokenCache.cleanUpTokenReferral(conf);\r\n        if (conf.getBoolean(MRJobConfig.JOB_TOKEN_TRACKING_IDS_ENABLED, MRJobConfig.DEFAULT_JOB_TOKEN_TRACKING_IDS_ENABLED)) {\r\n            ArrayList<String> trackingIds = new ArrayList<String>();\r\n            for (Token<? extends TokenIdentifier> t : job.getCredentials().getAllTokens()) {\r\n                trackingIds.add(t.decodeIdentifier().getTrackingId());\r\n            }\r\n            conf.setStrings(MRJobConfig.JOB_TOKEN_TRACKING_IDS, trackingIds.toArray(new String[trackingIds.size()]));\r\n        }\r\n        ReservationId reservationId = job.getReservationId();\r\n        if (reservationId != null) {\r\n            conf.set(MRJobConfig.RESERVATION_ID, reservationId.toString());\r\n        }\r\n        writeConf(conf, submitJobFile);\r\n        printTokens(jobId, job.getCredentials());\r\n        status = submitClient.submitJob(jobId, submitJobDir.toString(), job.getCredentials());\r\n        if (status != null) {\r\n            return status;\r\n        } else {\r\n            throw new IOException(\"Could not launch job\");\r\n        }\r\n    } finally {\r\n        if (status == null) {\r\n            LOG.info(\"Cleaning up the staging area \" + submitJobDir);\r\n            if (jtFs != null && submitJobDir != null)\r\n                jtFs.delete(submitJobDir, true);\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 2,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "checkSpecs",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void checkSpecs(Job job) throws ClassNotFoundException, InterruptedException, IOException\n{\r\n    JobConf jConf = (JobConf) job.getConfiguration();\r\n    if (jConf.getNumReduceTasks() == 0 ? jConf.getUseNewMapper() : jConf.getUseNewReducer()) {\r\n        org.apache.hadoop.mapreduce.OutputFormat<?, ?> output = ReflectionUtils.newInstance(job.getOutputFormatClass(), job.getConfiguration());\r\n        output.checkOutputSpecs(job);\r\n    } else {\r\n        jConf.getOutputFormat().checkOutputSpecs(jtFs, jConf);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "writeConf",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void writeConf(Configuration conf, Path jobFile) throws IOException\n{\r\n    FSDataOutputStream out = FileSystem.create(jtFs, jobFile, new FsPermission(JobSubmissionFiles.JOB_FILE_PERMISSION));\r\n    try {\r\n        conf.writeXml(out);\r\n    } finally {\r\n        out.close();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "printTokens",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void printTokens(JobID jobId, Credentials credentials) throws IOException\n{\r\n    LOG.info(\"Submitting tokens for job: \" + jobId);\r\n    LOG.info(\"Executing with tokens: {}\", credentials.getAllTokens());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "writeNewSplits",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "int writeNewSplits(JobContext job, Path jobSubmitDir) throws IOException, InterruptedException, ClassNotFoundException\n{\r\n    Configuration conf = job.getConfiguration();\r\n    InputFormat<?, ?> input = ReflectionUtils.newInstance(job.getInputFormatClass(), conf);\r\n    List<InputSplit> splits = input.getSplits(job);\r\n    T[] array = (T[]) splits.toArray(new InputSplit[splits.size()]);\r\n    Arrays.sort(array, new SplitComparator());\r\n    JobSplitWriter.createSplitFiles(jobSubmitDir, conf, jobSubmitDir.getFileSystem(conf), array);\r\n    return array.length;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "writeSplits",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "int writeSplits(org.apache.hadoop.mapreduce.JobContext job, Path jobSubmitDir) throws IOException, InterruptedException, ClassNotFoundException\n{\r\n    JobConf jConf = (JobConf) job.getConfiguration();\r\n    int maps;\r\n    if (jConf.getUseNewMapper()) {\r\n        maps = writeNewSplits(job, jobSubmitDir);\r\n    } else {\r\n        maps = writeOldSplits(jConf, jobSubmitDir);\r\n    }\r\n    return maps;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "writeOldSplits",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 3,
  "sourceCodeText" : "int writeOldSplits(JobConf job, Path jobSubmitDir) throws IOException\n{\r\n    org.apache.hadoop.mapred.InputSplit[] splits = job.getInputFormat().getSplits(job, job.getNumMapTasks());\r\n    Arrays.sort(splits, new Comparator<org.apache.hadoop.mapred.InputSplit>() {\r\n\r\n        public int compare(org.apache.hadoop.mapred.InputSplit a, org.apache.hadoop.mapred.InputSplit b) {\r\n            try {\r\n                long left = a.getLength();\r\n                long right = b.getLength();\r\n                if (left == right) {\r\n                    return 0;\r\n                } else if (left < right) {\r\n                    return 1;\r\n                } else {\r\n                    return -1;\r\n                }\r\n            } catch (IOException ie) {\r\n                throw new RuntimeException(\"Problem getting input split size\", ie);\r\n            }\r\n        }\r\n    });\r\n    JobSplitWriter.createSplitFiles(jobSubmitDir, job, jobSubmitDir.getFileSystem(job), splits);\r\n    return splits.length;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "readTokensFromFiles",
  "errType" : [ "JsonMappingException|JsonParseException" ],
  "containingMethodsNum" : 10,
  "sourceCodeText" : "void readTokensFromFiles(Configuration conf, Credentials credentials) throws IOException\n{\r\n    String binaryTokenFilename = conf.get(MRJobConfig.MAPREDUCE_JOB_CREDENTIALS_BINARY);\r\n    if (binaryTokenFilename != null) {\r\n        Credentials binary = Credentials.readTokenStorageFile(FileSystem.getLocal(conf).makeQualified(new Path(binaryTokenFilename)), conf);\r\n        credentials.addAll(binary);\r\n    }\r\n    String tokensFileName = conf.get(\"mapreduce.job.credentials.json\");\r\n    if (tokensFileName != null) {\r\n        LOG.info(\"loading user's secret keys from \" + tokensFileName);\r\n        String localFileName = new Path(tokensFileName).toUri().getPath();\r\n        try {\r\n            Map<String, String> nm = JsonSerialization.mapReader().readValue(new File(localFileName));\r\n            for (Map.Entry<String, String> ent : nm.entrySet()) {\r\n                credentials.addSecretKey(new Text(ent.getKey()), ent.getValue().getBytes(Charsets.UTF_8));\r\n            }\r\n        } catch (JsonMappingException | JsonParseException e) {\r\n            LOG.warn(\"couldn't parse Token Cache JSON file with user secret keys\");\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "populateTokenCache",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void populateTokenCache(Configuration conf, Credentials credentials) throws IOException\n{\r\n    readTokensFromFiles(conf, credentials);\r\n    String[] nameNodes = conf.getStrings(MRJobConfig.JOB_NAMENODES);\r\n    LOG.debug(\"adding the following namenodes' delegation tokens:\" + Arrays.toString(nameNodes));\r\n    if (nameNodes != null) {\r\n        Path[] ps = new Path[nameNodes.length];\r\n        for (int i = 0; i < nameNodes.length; i++) {\r\n            ps[i] = new Path(nameNodes[i]);\r\n        }\r\n        TokenCache.obtainTokensForNamenodes(credentials, ps, conf);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "addMRFrameworkToDistributedCache",
  "errType" : [ "URISyntaxException", "URISyntaxException" ],
  "containingMethodsNum" : 12,
  "sourceCodeText" : "void addMRFrameworkToDistributedCache(Configuration conf) throws IOException\n{\r\n    String framework = conf.get(MRJobConfig.MAPREDUCE_APPLICATION_FRAMEWORK_PATH, \"\");\r\n    if (!framework.isEmpty()) {\r\n        URI uri;\r\n        try {\r\n            uri = new URI(framework);\r\n        } catch (URISyntaxException e) {\r\n            throw new IllegalArgumentException(\"Unable to parse '\" + framework + \"' as a URI, check the setting for \" + MRJobConfig.MAPREDUCE_APPLICATION_FRAMEWORK_PATH, e);\r\n        }\r\n        String linkedName = uri.getFragment();\r\n        FileSystem fs = FileSystem.get(uri, conf);\r\n        Path frameworkPath = fs.makeQualified(new Path(uri.getScheme(), uri.getAuthority(), uri.getPath()));\r\n        FileContext fc = FileContext.getFileContext(frameworkPath.toUri(), conf);\r\n        frameworkPath = fc.resolvePath(frameworkPath);\r\n        uri = frameworkPath.toUri();\r\n        try {\r\n            uri = new URI(uri.getScheme(), uri.getAuthority(), uri.getPath(), null, linkedName);\r\n        } catch (URISyntaxException e) {\r\n            throw new IllegalArgumentException(e);\r\n        }\r\n        Job.addCacheArchive(uri, conf);\r\n    }\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 2,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\input",
  "methodName" : "getSplits",
  "errType" : [ "ClassNotFoundException" ],
  "containingMethodsNum" : 23,
  "sourceCodeText" : "List<InputSplit> getSplits(JobContext job) throws IOException, InterruptedException\n{\r\n    Configuration conf = job.getConfiguration();\r\n    Job jobCopy = Job.getInstance(conf);\r\n    List<InputSplit> splits = new ArrayList<InputSplit>();\r\n    Map<Path, InputFormat> formatMap = MultipleInputs.getInputFormatMap(job);\r\n    Map<Path, Class<? extends Mapper>> mapperMap = MultipleInputs.getMapperTypeMap(job);\r\n    Map<Class<? extends InputFormat>, List<Path>> formatPaths = new HashMap<Class<? extends InputFormat>, List<Path>>();\r\n    for (Entry<Path, InputFormat> entry : formatMap.entrySet()) {\r\n        if (!formatPaths.containsKey(entry.getValue().getClass())) {\r\n            formatPaths.put(entry.getValue().getClass(), new LinkedList<Path>());\r\n        }\r\n        formatPaths.get(entry.getValue().getClass()).add(entry.getKey());\r\n    }\r\n    for (Entry<Class<? extends InputFormat>, List<Path>> formatEntry : formatPaths.entrySet()) {\r\n        Class<? extends InputFormat> formatClass = formatEntry.getKey();\r\n        InputFormat format = (InputFormat) ReflectionUtils.newInstance(formatClass, conf);\r\n        List<Path> paths = formatEntry.getValue();\r\n        Map<Class<? extends Mapper>, List<Path>> mapperPaths = new HashMap<Class<? extends Mapper>, List<Path>>();\r\n        for (Path path : paths) {\r\n            Class<? extends Mapper> mapperClass = mapperMap.get(path);\r\n            if (!mapperPaths.containsKey(mapperClass)) {\r\n                mapperPaths.put(mapperClass, new LinkedList<Path>());\r\n            }\r\n            mapperPaths.get(mapperClass).add(path);\r\n        }\r\n        for (Entry<Class<? extends Mapper>, List<Path>> mapEntry : mapperPaths.entrySet()) {\r\n            paths = mapEntry.getValue();\r\n            Class<? extends Mapper> mapperClass = mapEntry.getKey();\r\n            if (mapperClass == null) {\r\n                try {\r\n                    mapperClass = job.getMapperClass();\r\n                } catch (ClassNotFoundException e) {\r\n                    throw new IOException(\"Mapper class is not found\", e);\r\n                }\r\n            }\r\n            FileInputFormat.setInputPaths(jobCopy, paths.toArray(new Path[paths.size()]));\r\n            List<InputSplit> pathSplits = format.getSplits(jobCopy);\r\n            for (InputSplit pathSplit : pathSplits) {\r\n                splits.add(new TaggedInputSplit(pathSplit, conf, format.getClass(), mapperClass));\r\n            }\r\n        }\r\n    }\r\n    return splits;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\input",
  "methodName" : "createRecordReader",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "RecordReader<K, V> createRecordReader(InputSplit split, TaskAttemptContext context) throws IOException, InterruptedException\n{\r\n    return new DelegatingRecordReader<K, V>(split, context);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\db",
  "methodName" : "getSelectQuery",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 23,
  "sourceCodeText" : "String getSelectQuery()\n{\r\n    StringBuilder query = new StringBuilder();\r\n    DBConfiguration dbConf = getDBConf();\r\n    String conditions = getConditions();\r\n    String tableName = getTableName();\r\n    String[] fieldNames = getFieldNames();\r\n    if (dbConf.getInputQuery() == null) {\r\n        query.append(\"SELECT \");\r\n        for (int i = 0; i < fieldNames.length; i++) {\r\n            query.append(fieldNames[i]);\r\n            if (i != fieldNames.length - 1) {\r\n                query.append(\", \");\r\n            }\r\n        }\r\n        query.append(\" FROM \").append(tableName);\r\n        if (conditions != null && conditions.length() > 0)\r\n            query.append(\" WHERE \").append(conditions);\r\n        String orderBy = dbConf.getInputOrderBy();\r\n        if (orderBy != null && orderBy.length() > 0) {\r\n            query.append(\" ORDER BY \").append(orderBy);\r\n        }\r\n    } else {\r\n        query.append(dbConf.getInputQuery());\r\n    }\r\n    try {\r\n        DBInputFormat.DBInputSplit split = getSplit();\r\n        if (split.getLength() > 0) {\r\n            String querystring = query.toString();\r\n            query = new StringBuilder();\r\n            query.append(\"SELECT * FROM (SELECT a.*,ROWNUM dbif_rno FROM ( \");\r\n            query.append(querystring);\r\n            query.append(\" ) a WHERE rownum <= \").append(split.getEnd());\r\n            query.append(\" ) WHERE dbif_rno > \").append(split.getStart());\r\n        }\r\n    } catch (IOException ex) {\r\n    }\r\n    return query.toString();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\db",
  "methodName" : "setSessionTimeZone",
  "errType" : [ "Exception", "Exception", "Exception" ],
  "containingMethodsNum" : 10,
  "sourceCodeText" : "void setSessionTimeZone(Configuration conf, Connection conn) throws SQLException\n{\r\n    Method method;\r\n    try {\r\n        method = conn.getClass().getMethod(\"setSessionTimeZone\", new Class[] { String.class });\r\n    } catch (Exception ex) {\r\n        LOG.error(\"Could not find method setSessionTimeZone in \" + conn.getClass().getName(), ex);\r\n        throw new SQLException(ex);\r\n    }\r\n    String clientTimeZone = conf.get(SESSION_TIMEZONE_KEY, \"GMT\");\r\n    try {\r\n        method.setAccessible(true);\r\n        method.invoke(conn, clientTimeZone);\r\n        LOG.info(\"Time zone has been set to \" + clientTimeZone);\r\n    } catch (Exception ex) {\r\n        LOG.warn(\"Time zone \" + clientTimeZone + \" could not be set on Oracle database.\");\r\n        LOG.warn(\"Setting default time zone: GMT\");\r\n        try {\r\n            method.invoke(conn, \"GMT\");\r\n        } catch (Exception ex2) {\r\n            LOG.error(\"Could not set time zone for oracle connection\", ex2);\r\n            throw new SQLException(ex);\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 3,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred\\join",
  "methodName" : "combine",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean combine(Object[] srcs, TupleWritable value)",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred\\join",
  "methodName" : "id",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int id()\n{\r\n    return id;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred\\join",
  "methodName" : "setConf",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setConf(Configuration conf)\n{\r\n    this.conf = conf;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred\\join",
  "methodName" : "getConf",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Configuration getConf()\n{\r\n    return conf;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred\\join",
  "methodName" : "getRecordReaderQueue",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "PriorityQueue<ComposableRecordReader<K, ?>> getRecordReaderQueue()\n{\r\n    return q;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred\\join",
  "methodName" : "getComparator",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "WritableComparator getComparator()\n{\r\n    return cmp;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred\\join",
  "methodName" : "add",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void add(ComposableRecordReader<K, ? extends V> rr) throws IOException\n{\r\n    kids[rr.id()] = rr;\r\n    if (null == q) {\r\n        cmp = WritableComparator.get(rr.createKey().getClass(), conf);\r\n        q = new PriorityQueue<ComposableRecordReader<K, ?>>(3, new Comparator<ComposableRecordReader<K, ?>>() {\r\n\r\n            public int compare(ComposableRecordReader<K, ?> o1, ComposableRecordReader<K, ?> o2) {\r\n                return cmp.compare(o1.key(), o2.key());\r\n            }\r\n        });\r\n    }\r\n    if (rr.hasNext()) {\r\n        q.add(rr);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred\\join",
  "methodName" : "key",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "K key()\n{\r\n    if (jc.hasNext()) {\r\n        return jc.key();\r\n    }\r\n    if (!q.isEmpty()) {\r\n        return q.peek().key();\r\n    }\r\n    return null;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred\\join",
  "methodName" : "key",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void key(K key) throws IOException\n{\r\n    WritableUtils.cloneInto(key, key());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred\\join",
  "methodName" : "hasNext",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "boolean hasNext()\n{\r\n    return jc.hasNext() || !q.isEmpty();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred\\join",
  "methodName" : "skip",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void skip(K key) throws IOException\n{\r\n    ArrayList<ComposableRecordReader<K, ?>> tmp = new ArrayList<ComposableRecordReader<K, ?>>();\r\n    while (!q.isEmpty() && cmp.compare(q.peek().key(), key) <= 0) {\r\n        tmp.add(q.poll());\r\n    }\r\n    for (ComposableRecordReader<K, ?> rr : tmp) {\r\n        rr.skip(key);\r\n        if (rr.hasNext()) {\r\n            q.add(rr);\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred\\join",
  "methodName" : "getDelegate",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "ResetableIterator<X> getDelegate()",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred\\join",
  "methodName" : "accept",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void accept(CompositeRecordReader.JoinCollector jc, K key) throws IOException\n{\r\n    if (hasNext() && 0 == cmp.compare(key, key())) {\r\n        fillJoinCollector(createKey());\r\n        jc.add(id, getDelegate());\r\n        return;\r\n    }\r\n    jc.add(id, EMPTY);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred\\join",
  "methodName" : "fillJoinCollector",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void fillJoinCollector(K iterkey) throws IOException\n{\r\n    if (!q.isEmpty()) {\r\n        q.peek().key(iterkey);\r\n        while (0 == cmp.compare(q.peek().key(), iterkey)) {\r\n            ComposableRecordReader<K, ?> t = q.poll();\r\n            t.accept(jc, iterkey);\r\n            if (t.hasNext()) {\r\n                q.add(t);\r\n            } else if (q.isEmpty()) {\r\n                return;\r\n            }\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred\\join",
  "methodName" : "compareTo",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "int compareTo(ComposableRecordReader<K, ?> other)\n{\r\n    return cmp.compare(key(), other.key());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred\\join",
  "methodName" : "createKey",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "K createKey()\n{\r\n    if (null == keyclass) {\r\n        final Class<?> cls = kids[0].createKey().getClass();\r\n        for (RecordReader<K, ? extends Writable> rr : kids) {\r\n            if (!cls.equals(rr.createKey().getClass())) {\r\n                throw new ClassCastException(\"Child key classes fail to agree\");\r\n            }\r\n        }\r\n        keyclass = cls.asSubclass(WritableComparable.class);\r\n    }\r\n    return (K) ReflectionUtils.newInstance(keyclass, getConf());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred\\join",
  "methodName" : "createInternalValue",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "TupleWritable createInternalValue()\n{\r\n    Writable[] vals = new Writable[kids.length];\r\n    for (int i = 0; i < vals.length; ++i) {\r\n        vals[i] = kids[i].createValue();\r\n    }\r\n    return new TupleWritable(vals);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred\\join",
  "methodName" : "getPos",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getPos() throws IOException\n{\r\n    return 0;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred\\join",
  "methodName" : "close",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void close() throws IOException\n{\r\n    if (kids != null) {\r\n        for (RecordReader<K, ? extends Writable> rr : kids) {\r\n            rr.close();\r\n        }\r\n    }\r\n    if (jc != null) {\r\n        jc.close();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred\\join",
  "methodName" : "getProgress",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "float getProgress() throws IOException\n{\r\n    float ret = 1.0f;\r\n    for (RecordReader<K, ? extends Writable> rr : kids) {\r\n        ret = Math.min(ret, rr.getProgress());\r\n    }\r\n    return ret;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred\\lib",
  "methodName" : "getRecordReader",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "RecordReader<LongWritable, Text> getRecordReader(InputSplit split, JobConf conf, Reporter reporter) throws IOException\n{\r\n    return new CombineFileRecordReader(conf, (CombineFileSplit) split, reporter, TextRecordReaderWrapper.class);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "getDatum",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "Object getDatum()\n{\r\n    if (datum == null) {\r\n        datum = new TaskFinished();\r\n        datum.setTaskid(new Utf8(taskid.toString()));\r\n        if (successfulAttemptId != null) {\r\n            datum.setSuccessfulAttemptId(new Utf8(successfulAttemptId.toString()));\r\n        }\r\n        datum.setFinishTime(finishTime);\r\n        datum.setCounters(EventWriter.toAvro(counters));\r\n        datum.setTaskType(new Utf8(taskType.name()));\r\n        datum.setStatus(new Utf8(status));\r\n    }\r\n    return datum;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "setDatum",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void setDatum(Object oDatum)\n{\r\n    this.datum = (TaskFinished) oDatum;\r\n    this.taskid = TaskID.forName(datum.getTaskid().toString());\r\n    if (datum.getSuccessfulAttemptId() != null) {\r\n        this.successfulAttemptId = TaskAttemptID.forName(datum.getSuccessfulAttemptId().toString());\r\n    }\r\n    this.finishTime = datum.getFinishTime();\r\n    this.taskType = TaskType.valueOf(datum.getTaskType().toString());\r\n    this.status = datum.getStatus().toString();\r\n    this.counters = EventReader.fromAvro(datum.getCounters());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "getTaskId",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "TaskID getTaskId()\n{\r\n    return taskid;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "getSuccessfulTaskAttemptId",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "TaskAttemptID getSuccessfulTaskAttemptId()\n{\r\n    return successfulAttemptId;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "getFinishTime",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getFinishTime()\n{\r\n    return finishTime;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "getStartTime",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getStartTime()\n{\r\n    return startTime;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "getCounters",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Counters getCounters()\n{\r\n    return counters;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "getTaskType",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "TaskType getTaskType()\n{\r\n    return taskType;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "getTaskStatus",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String getTaskStatus()\n{\r\n    return status.toString();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "getEventType",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "EventType getEventType()\n{\r\n    return EventType.TASK_FINISHED;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "toTimelineEvent",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "TimelineEvent toTimelineEvent()\n{\r\n    TimelineEvent tEvent = new TimelineEvent();\r\n    tEvent.setId(StringUtils.toUpperCase(getEventType().name()));\r\n    tEvent.addInfo(\"TASK_TYPE\", getTaskType().toString());\r\n    tEvent.addInfo(\"FINISH_TIME\", getFinishTime());\r\n    tEvent.addInfo(\"STATUS\", TaskStatus.State.SUCCEEDED.toString());\r\n    tEvent.addInfo(\"SUCCESSFUL_TASK_ATTEMPT_ID\", getSuccessfulTaskAttemptId() == null ? \"\" : getSuccessfulTaskAttemptId().toString());\r\n    return tEvent;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "getTimelineMetrics",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Set<TimelineMetric> getTimelineMetrics()\n{\r\n    Set<TimelineMetric> jobMetrics = JobHistoryEventUtils.countersToTimelineMetric(getCounters(), finishTime);\r\n    return jobMetrics;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "getJobID",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "JobID getJobID()\n{\r\n    return jobId;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "isMap",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean isMap()\n{\r\n    return type == TaskType.MAP;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "getTaskType",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "TaskType getTaskType()\n{\r\n    return type;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "equals",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "boolean equals(Object o)\n{\r\n    if (!super.equals(o))\r\n        return false;\r\n    TaskID that = (TaskID) o;\r\n    return this.type == that.type && this.jobId.equals(that.jobId);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "compareTo",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "int compareTo(ID o)\n{\r\n    TaskID that = (TaskID) o;\r\n    int jobComp = this.jobId.compareTo(that.jobId);\r\n    if (jobComp == 0) {\r\n        if (this.type == that.type) {\r\n            return this.id - that.id;\r\n        } else {\r\n            return this.type.compareTo(that.type);\r\n        }\r\n    } else\r\n        return jobComp;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "toString",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String toString()\n{\r\n    return appendTo(new StringBuilder(TASK)).toString();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "appendTo",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "StringBuilder appendTo(StringBuilder builder)\n{\r\n    return jobId.appendTo(builder).append(SEPARATOR).append(CharTaskTypeMaps.getRepresentingCharacter(type)).append(SEPARATOR).append(idFormat.format(id));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "hashCode",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "int hashCode()\n{\r\n    return jobId.hashCode() * 524287 + id;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "readFields",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void readFields(DataInput in) throws IOException\n{\r\n    super.readFields(in);\r\n    jobId.readFields(in);\r\n    type = WritableUtils.readEnum(in, TaskType.class);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "write",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void write(DataOutput out) throws IOException\n{\r\n    super.write(out);\r\n    jobId.write(out);\r\n    WritableUtils.writeEnum(out, type);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "forName",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "TaskID forName(String str) throws IllegalArgumentException\n{\r\n    if (str == null)\r\n        return null;\r\n    Matcher m = taskIdPattern.matcher(str);\r\n    if (m.matches()) {\r\n        return new org.apache.hadoop.mapred.TaskID(m.group(1), Integer.parseInt(m.group(2)), CharTaskTypeMaps.getTaskType(m.group(3).charAt(0)), Integer.parseInt(m.group(4)));\r\n    }\r\n    String exceptionMsg = \"TaskId string : \" + str + \" is not properly formed\" + \"\\nReason: \" + m.toString();\r\n    throw new IllegalArgumentException(exceptionMsg);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "getRepresentingCharacter",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "char getRepresentingCharacter(TaskType type)\n{\r\n    return CharTaskTypeMaps.getRepresentingCharacter(type);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "getTaskType",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "TaskType getTaskType(char c)\n{\r\n    return CharTaskTypeMaps.getTaskType(c);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "getAllTaskTypes",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getAllTaskTypes()\n{\r\n    return CharTaskTypeMaps.allTaskTypes;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\security\\token",
  "methodName" : "selectToken",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "Token<JobTokenIdentifier> selectToken(Text service, Collection<Token<? extends TokenIdentifier>> tokens)\n{\r\n    if (service == null) {\r\n        return null;\r\n    }\r\n    for (Token<? extends TokenIdentifier> token : tokens) {\r\n        if (JobTokenIdentifier.KIND_NAME.equals(token.getKind()) && service.equals(token.getService())) {\r\n            return (Token<JobTokenIdentifier>) token;\r\n        }\r\n    }\r\n    return null;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getRecordReader",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "RecordReader<K, V> getRecordReader(InputSplit split, JobConf job, Reporter reporter) throws IOException\n{\r\n    reporter.setStatus(split.toString());\r\n    return new FilterRecordReader<K, V>(job, (FileSplit) split);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "setFilterClass",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setFilterClass(Configuration conf, Class filterClass)\n{\r\n    conf.set(FILTER_CLASS, filterClass.getName());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred\\pipes",
  "methodName" : "configure",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void configure(JobConf job)\n{\r\n    this.job = job;\r\n    SkipBadRecords.setAutoIncrMapperProcCount(job, false);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred\\pipes",
  "methodName" : "run",
  "errType" : [ "InterruptedException", "Throwable" ],
  "containingMethodsNum" : 18,
  "sourceCodeText" : "void run(RecordReader<K1, V1> input, OutputCollector<K2, V2> output, Reporter reporter) throws IOException\n{\r\n    Application<K1, V1, K2, V2> application = null;\r\n    try {\r\n        RecordReader<FloatWritable, NullWritable> fakeInput = (!Submitter.getIsJavaRecordReader(job) && !Submitter.getIsJavaMapper(job)) ? (RecordReader<FloatWritable, NullWritable>) input : null;\r\n        application = new Application<K1, V1, K2, V2>(job, fakeInput, output, reporter, (Class<? extends K2>) job.getOutputKeyClass(), (Class<? extends V2>) job.getOutputValueClass());\r\n    } catch (InterruptedException ie) {\r\n        throw new RuntimeException(\"interrupted\", ie);\r\n    }\r\n    DownwardProtocol<K1, V1> downlink = application.getDownlink();\r\n    boolean isJavaInput = Submitter.getIsJavaRecordReader(job);\r\n    downlink.runMap(reporter.getInputSplit(), job.getNumReduceTasks(), isJavaInput);\r\n    boolean skipping = job.getBoolean(MRJobConfig.SKIP_RECORDS, false);\r\n    try {\r\n        if (isJavaInput) {\r\n            K1 key = input.createKey();\r\n            V1 value = input.createValue();\r\n            downlink.setInputTypes(key.getClass().getName(), value.getClass().getName());\r\n            while (input.next(key, value)) {\r\n                downlink.mapItem(key, value);\r\n                if (skipping) {\r\n                    downlink.flush();\r\n                }\r\n            }\r\n            downlink.endOfInput();\r\n        }\r\n        application.waitForFinish();\r\n    } catch (Throwable t) {\r\n        application.abort(t);\r\n    } finally {\r\n        application.cleanup();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 2,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getTaskAttemptID",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "TaskAttemptID getTaskAttemptID()\n{\r\n    return (TaskAttemptID) super.getTaskAttemptID();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getProgressible",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Progressable getProgressible()\n{\r\n    return reporter;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getJobConf",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "JobConf getJobConf()\n{\r\n    return (JobConf) getConfiguration();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getProgress",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "float getProgress()\n{\r\n    return reporter.getProgress();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getCounter",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Counter getCounter(Enum<?> counterName)\n{\r\n    return reporter.getCounter(counterName);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getCounter",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Counter getCounter(String groupName, String counterName)\n{\r\n    return reporter.getCounter(groupName, counterName);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "progress",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void progress()\n{\r\n    reporter.progress();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "setStatus",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void setStatus(String status)\n{\r\n    setStatusString(status);\r\n    reporter.setStatus(status);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "getJtIdentifier",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String getJtIdentifier()\n{\r\n    return jtIdentifier.toString();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "equals",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "boolean equals(Object o)\n{\r\n    if (!super.equals(o))\r\n        return false;\r\n    JobID that = (JobID) o;\r\n    return this.jtIdentifier.equals(that.jtIdentifier);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "compareTo",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "int compareTo(ID o)\n{\r\n    JobID that = (JobID) o;\r\n    int jtComp = this.jtIdentifier.compareTo(that.jtIdentifier);\r\n    if (jtComp == 0) {\r\n        return this.id - that.id;\r\n    } else\r\n        return jtComp;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "appendTo",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "StringBuilder appendTo(StringBuilder builder)\n{\r\n    builder.append(SEPARATOR);\r\n    builder.append(jtIdentifier);\r\n    builder.append(SEPARATOR);\r\n    builder.append(idFormat.format(id));\r\n    return builder;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "hashCode",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "int hashCode()\n{\r\n    return jtIdentifier.hashCode() + id;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "toString",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String toString()\n{\r\n    return appendTo(new StringBuilder(JOB)).toString();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "readFields",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void readFields(DataInput in) throws IOException\n{\r\n    super.readFields(in);\r\n    this.jtIdentifier.readFields(in);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "write",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void write(DataOutput out) throws IOException\n{\r\n    super.write(out);\r\n    jtIdentifier.write(out);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "forName",
  "errType" : [ "Exception" ],
  "containingMethodsNum" : 3,
  "sourceCodeText" : "JobID forName(String str) throws IllegalArgumentException\n{\r\n    if (str == null)\r\n        return null;\r\n    try {\r\n        String[] parts = str.split(\"_\");\r\n        if (parts.length == 3) {\r\n            if (parts[0].equals(JOB)) {\r\n                return new org.apache.hadoop.mapred.JobID(parts[1], Integer.parseInt(parts[2]));\r\n            }\r\n        }\r\n    } catch (Exception ex) {\r\n    }\r\n    throw new IllegalArgumentException(\"JobId string : \" + str + \" is not properly formed\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest\\files",
  "methodName" : "run",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "int run(String[] args) throws Exception\n{\r\n    if (args.length != 1) {\r\n        printUsage();\r\n        return -1;\r\n    }\r\n    Path path = new Path(args[0]);\r\n    loadAndPrintManifest(path.getFileSystem(getConf()), path);\r\n    return 0;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest\\files",
  "methodName" : "loadAndPrintManifest",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "ManifestSuccessData loadAndPrintManifest(FileSystem fs, Path path) throws IOException\n{\r\n    println(\"Manifest file: %s\", path);\r\n    final ManifestSuccessData success = ManifestSuccessData.load(fs, path);\r\n    printManifest(success);\r\n    return success;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest\\files",
  "methodName" : "printManifest",
  "errType" : null,
  "containingMethodsNum" : 11,
  "sourceCodeText" : "void printManifest(ManifestSuccessData success)\n{\r\n    field(\"succeeded\", success.getSuccess());\r\n    field(\"created\", success.getDate());\r\n    field(\"committer\", success.getCommitter());\r\n    field(\"hostname\", success.getHostname());\r\n    field(\"description\", success.getDescription());\r\n    field(\"jobId\", success.getJobId());\r\n    field(\"jobIdSource\", success.getJobIdSource());\r\n    field(\"stage\", success.getStage());\r\n    println(\"Diagnostics\\n%s\", success.dumpDiagnostics(\"  \", \" = \", \"\\n\"));\r\n    println(\"Statistics:\\n%s\", ioStatisticsToPrettyString(success.getIOStatistics()));\r\n    out.flush();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest\\files",
  "methodName" : "printUsage",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void printUsage()\n{\r\n    println(USAGE);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest\\files",
  "methodName" : "println",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void println(String format, Object... args)\n{\r\n    out.format(format, args);\r\n    out.println();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest\\files",
  "methodName" : "field",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void field(String name, Object value)\n{\r\n    if (value != null) {\r\n        println(\"%s: %s\", name, value);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest\\files",
  "methodName" : "main",
  "errType" : [ "ExitUtil.ExitException" ],
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void main(String[] argv) throws Exception\n{\r\n    try {\r\n        int res = ToolRunner.run(new ManifestPrinter(), argv);\r\n        System.exit(res);\r\n    } catch (ExitUtil.ExitException e) {\r\n        ExitUtil.terminate(e);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\input",
  "methodName" : "createRecordReader",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "RecordReader<K, V> createRecordReader(InputSplit split, TaskAttemptContext context) throws IOException\n{\r\n    return new SequenceFileRecordReader<K, V>();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\input",
  "methodName" : "getFormatMinSplitSize",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getFormatMinSplitSize()\n{\r\n    return SequenceFile.SYNC_INTERVAL;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\input",
  "methodName" : "listStatus",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "List<FileStatus> listStatus(JobContext job) throws IOException\n{\r\n    List<FileStatus> files = super.listStatus(job);\r\n    int len = files.size();\r\n    for (int i = 0; i < len; ++i) {\r\n        FileStatus file = files.get(i);\r\n        if (file.isDirectory()) {\r\n            Path p = file.getPath();\r\n            FileSystem fs = p.getFileSystem(job.getConfiguration());\r\n            files.set(i, fs.getFileStatus(new Path(p, MapFile.DATA_FILE_NAME)));\r\n        }\r\n    }\r\n    return files;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\task\\reduce",
  "methodName" : "run",
  "errType" : [ "InterruptedException", "Throwable" ],
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void run()\n{\r\n    Set<TaskAttemptID> maps = new HashSet<TaskAttemptID>();\r\n    for (TaskAttemptID map : localMapFiles.keySet()) {\r\n        maps.add(map);\r\n    }\r\n    while (maps.size() > 0) {\r\n        try {\r\n            merger.waitForResource();\r\n            metrics.threadBusy();\r\n            doCopy(maps);\r\n            metrics.threadFree();\r\n        } catch (InterruptedException ie) {\r\n        } catch (Throwable t) {\r\n            exceptionReporter.reportException(t);\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\task\\reduce",
  "methodName" : "doCopy",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void doCopy(Set<TaskAttemptID> maps) throws IOException\n{\r\n    Iterator<TaskAttemptID> iter = maps.iterator();\r\n    while (iter.hasNext()) {\r\n        TaskAttemptID map = iter.next();\r\n        LOG.debug(\"LocalFetcher \" + id + \" going to fetch: \" + map);\r\n        if (copyMapOutput(map)) {\r\n            iter.remove();\r\n        } else {\r\n            break;\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\task\\reduce",
  "methodName" : "copyMapOutput",
  "errType" : null,
  "containingMethodsNum" : 15,
  "sourceCodeText" : "boolean copyMapOutput(TaskAttemptID mapTaskId) throws IOException\n{\r\n    Path mapOutputFileName = localMapFiles.get(mapTaskId).getOutputFile();\r\n    Path indexFileName = mapOutputFileName.suffix(\".index\");\r\n    SpillRecord sr = new SpillRecord(indexFileName, job);\r\n    IndexRecord ir = sr.getIndex(reduce);\r\n    long compressedLength = ir.partLength;\r\n    long decompressedLength = ir.rawLength;\r\n    compressedLength -= CryptoUtils.cryptoPadding(job);\r\n    decompressedLength -= CryptoUtils.cryptoPadding(job);\r\n    MapOutput<K, V> mapOutput = merger.reserve(mapTaskId, decompressedLength, id);\r\n    if (mapOutput == null) {\r\n        LOG.info(\"fetcher#\" + id + \" - MergeManager returned Status.WAIT ...\");\r\n        return false;\r\n    }\r\n    LOG.info(\"localfetcher#\" + id + \" about to shuffle output of map \" + mapOutput.getMapId() + \" decomp: \" + decompressedLength + \" len: \" + compressedLength + \" to \" + mapOutput.getDescription());\r\n    FileSystem localFs = FileSystem.getLocal(job).getRaw();\r\n    FSDataInputStream inStream = localFs.open(mapOutputFileName);\r\n    try {\r\n        inStream.seek(ir.startOffset);\r\n        inStream = IntermediateEncryptedStream.wrapIfNecessary(job, inStream, mapOutputFileName);\r\n        mapOutput.shuffle(LOCALHOST, inStream, compressedLength, decompressedLength, metrics, reporter);\r\n    } finally {\r\n        IOUtils.cleanupWithLogger(LOG, inStream);\r\n    }\r\n    scheduler.copySucceeded(mapTaskId, LOCALHOST, compressedLength, 0, 0, mapOutput);\r\n    return true;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "getDatum",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "Object getDatum()\n{\r\n    if (datum == null) {\r\n        datum = new TaskFailed();\r\n        datum.setTaskid(new Utf8(id.toString()));\r\n        datum.setError(new Utf8(error));\r\n        datum.setFinishTime(finishTime);\r\n        datum.setTaskType(new Utf8(taskType.name()));\r\n        datum.setFailedDueToAttempt(failedDueToAttempt == null ? null : new Utf8(failedDueToAttempt.toString()));\r\n        datum.setStatus(new Utf8(status));\r\n        datum.setCounters(EventWriter.toAvro(counters));\r\n    }\r\n    return datum;\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "setDatum",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void setDatum(Object odatum)\n{\r\n    this.datum = (TaskFailed) odatum;\r\n    this.id = TaskID.forName(datum.getTaskid().toString());\r\n    this.taskType = TaskType.valueOf(datum.getTaskType().toString());\r\n    this.finishTime = datum.getFinishTime();\r\n    this.error = datum.getError().toString();\r\n    this.failedDueToAttempt = datum.getFailedDueToAttempt() == null ? null : TaskAttemptID.forName(datum.getFailedDueToAttempt().toString());\r\n    this.status = datum.getStatus().toString();\r\n    this.counters = EventReader.fromAvro(datum.getCounters());\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "getTaskId",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "TaskID getTaskId()\n{\r\n    return id;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "getError",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getError()\n{\r\n    return error;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "getFinishTime",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getFinishTime()\n{\r\n    return finishTime;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "getStartTime",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getStartTime()\n{\r\n    return startTime;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "getTaskType",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "TaskType getTaskType()\n{\r\n    return taskType;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "getFailedAttemptID",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "TaskAttemptID getFailedAttemptID()\n{\r\n    return failedDueToAttempt;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "getTaskStatus",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getTaskStatus()\n{\r\n    return status;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "getCounters",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Counters getCounters()\n{\r\n    return counters;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "getEventType",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "EventType getEventType()\n{\r\n    return EventType.TASK_FAILED;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "toTimelineEvent",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "TimelineEvent toTimelineEvent()\n{\r\n    TimelineEvent tEvent = new TimelineEvent();\r\n    tEvent.setId(StringUtils.toUpperCase(getEventType().name()));\r\n    tEvent.addInfo(\"TASK_TYPE\", getTaskType().toString());\r\n    tEvent.addInfo(\"STATUS\", TaskStatus.State.FAILED.toString());\r\n    tEvent.addInfo(\"FINISH_TIME\", getFinishTime());\r\n    tEvent.addInfo(\"ERROR\", getError());\r\n    tEvent.addInfo(\"FAILED_ATTEMPT_ID\", getFailedAttemptID() == null ? \"\" : getFailedAttemptID().toString());\r\n    return tEvent;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "getTimelineMetrics",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Set<TimelineMetric> getTimelineMetrics()\n{\r\n    Set<TimelineMetric> metrics = JobHistoryEventUtils.countersToTimelineMetric(getCounters(), finishTime);\r\n    return metrics;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred\\lib",
  "methodName" : "configure",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void configure(JobConf job)\n{\r\n    pattern = Pattern.compile(job.get(org.apache.hadoop.mapreduce.lib.map.RegexMapper.PATTERN));\r\n    group = job.getInt(org.apache.hadoop.mapreduce.lib.map.RegexMapper.GROUP, 0);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred\\lib",
  "methodName" : "map",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void map(K key, Text value, OutputCollector<Text, LongWritable> output, Reporter reporter) throws IOException\n{\r\n    String text = value.toString();\r\n    Matcher matcher = pattern.matcher(text);\r\n    while (matcher.find()) {\r\n        output.collect(new Text(matcher.group(group)), new LongWritable(1));\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\checkpoint",
  "methodName" : "getPath",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Path getPath()\n{\r\n    return path;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\checkpoint",
  "methodName" : "toString",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String toString()\n{\r\n    return path.toString();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\checkpoint",
  "methodName" : "write",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void write(DataOutput out) throws IOException\n{\r\n    Text.writeString(out, path.toString());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\checkpoint",
  "methodName" : "readFields",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void readFields(DataInput in) throws IOException\n{\r\n    this.path = new Path(Text.readString(in));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\checkpoint",
  "methodName" : "equals",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "boolean equals(Object other)\n{\r\n    return other instanceof FSCheckpointID && path.equals(((FSCheckpointID) other).path);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\checkpoint",
  "methodName" : "hashCode",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "int hashCode()\n{\r\n    return path.hashCode();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "setupJob",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setupJob(JobContext jobContext) throws IOException",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "cleanupJob",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void cleanupJob(JobContext jobContext) throws IOException\n{\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "commitJob",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void commitJob(JobContext jobContext) throws IOException\n{\r\n    cleanupJob(jobContext);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "abortJob",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void abortJob(JobContext jobContext, int status) throws IOException\n{\r\n    cleanupJob(jobContext);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "setupTask",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setupTask(TaskAttemptContext taskContext) throws IOException",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "needsTaskCommit",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean needsTaskCommit(TaskAttemptContext taskContext) throws IOException",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "commitTask",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void commitTask(TaskAttemptContext taskContext) throws IOException",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "abortTask",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void abortTask(TaskAttemptContext taskContext) throws IOException",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "isRecoverySupported",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean isRecoverySupported()\n{\r\n    return false;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "isRecoverySupported",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "boolean isRecoverySupported(JobContext jobContext) throws IOException\n{\r\n    return isRecoverySupported();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "isCommitJobRepeatable",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean isCommitJobRepeatable(JobContext jobContext) throws IOException\n{\r\n    return false;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "isCommitJobRepeatable",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "boolean isCommitJobRepeatable(org.apache.hadoop.mapreduce.JobContext jobContext) throws IOException\n{\r\n    return isCommitJobRepeatable((JobContext) jobContext);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "recoverTask",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void recoverTask(TaskAttemptContext taskContext) throws IOException\n{\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "setupJob",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setupJob(org.apache.hadoop.mapreduce.JobContext jobContext) throws IOException\n{\r\n    setupJob((JobContext) jobContext);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "cleanupJob",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void cleanupJob(org.apache.hadoop.mapreduce.JobContext context) throws IOException\n{\r\n    cleanupJob((JobContext) context);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "commitJob",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void commitJob(org.apache.hadoop.mapreduce.JobContext context) throws IOException\n{\r\n    commitJob((JobContext) context);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "abortJob",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void abortJob(org.apache.hadoop.mapreduce.JobContext context, org.apache.hadoop.mapreduce.JobStatus.State runState) throws IOException\n{\r\n    int state = JobStatus.getOldNewJobRunState(runState);\r\n    if (state != JobStatus.FAILED && state != JobStatus.KILLED) {\r\n        throw new IOException(\"Invalid job run state : \" + runState.name());\r\n    }\r\n    abortJob((JobContext) context, state);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "setupTask",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setupTask(org.apache.hadoop.mapreduce.TaskAttemptContext taskContext) throws IOException\n{\r\n    setupTask((TaskAttemptContext) taskContext);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "needsTaskCommit",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "boolean needsTaskCommit(org.apache.hadoop.mapreduce.TaskAttemptContext taskContext) throws IOException\n{\r\n    return needsTaskCommit((TaskAttemptContext) taskContext);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "commitTask",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void commitTask(org.apache.hadoop.mapreduce.TaskAttemptContext taskContext) throws IOException\n{\r\n    commitTask((TaskAttemptContext) taskContext);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "abortTask",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void abortTask(org.apache.hadoop.mapreduce.TaskAttemptContext taskContext) throws IOException\n{\r\n    abortTask((TaskAttemptContext) taskContext);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "recoverTask",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void recoverTask(org.apache.hadoop.mapreduce.TaskAttemptContext taskContext) throws IOException\n{\r\n    recoverTask((TaskAttemptContext) taskContext);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "isRecoverySupported",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "boolean isRecoverySupported(org.apache.hadoop.mapreduce.JobContext context) throws IOException\n{\r\n    return isRecoverySupported((JobContext) context);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\task\\reduce",
  "methodName" : "init",
  "errType" : null,
  "containingMethodsNum" : 13,
  "sourceCodeText" : "void init(ShuffleConsumerPlugin.Context context)\n{\r\n    this.context = context;\r\n    this.reduceId = context.getReduceId();\r\n    this.jobConf = context.getJobConf();\r\n    this.umbilical = context.getUmbilical();\r\n    this.reporter = context.getReporter();\r\n    this.metrics = ShuffleClientMetrics.create(context.getReduceId(), this.jobConf);\r\n    this.copyPhase = context.getCopyPhase();\r\n    this.taskStatus = context.getStatus();\r\n    this.reduceTask = context.getReduceTask();\r\n    this.localMapFiles = context.getLocalMapFiles();\r\n    scheduler = new ShuffleSchedulerImpl<K, V>(jobConf, taskStatus, reduceId, this, copyPhase, context.getShuffledMapsCounter(), context.getReduceShuffleBytes(), context.getFailedShuffleCounter());\r\n    merger = createMergeManager(context);\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\task\\reduce",
  "methodName" : "createMergeManager",
  "errType" : null,
  "containingMethodsNum" : 10,
  "sourceCodeText" : "MergeManager<K, V> createMergeManager(ShuffleConsumerPlugin.Context context)\n{\r\n    return new MergeManagerImpl<K, V>(reduceId, jobConf, context.getLocalFS(), context.getLocalDirAllocator(), reporter, context.getCodec(), context.getCombinerClass(), context.getCombineCollector(), context.getSpilledRecordsCounter(), context.getReduceCombineInputCounter(), context.getMergedMapOutputsCounter(), this, context.getMergePhase(), context.getMapOutputFile());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\task\\reduce",
  "methodName" : "run",
  "errType" : [ "Throwable" ],
  "containingMethodsNum" : 17,
  "sourceCodeText" : "RawKeyValueIterator run() throws IOException, InterruptedException\n{\r\n    int eventsPerReducer = Math.max(MIN_EVENTS_TO_FETCH, MAX_RPC_OUTSTANDING_EVENTS / jobConf.getNumReduceTasks());\r\n    int maxEventsToFetch = Math.min(MAX_EVENTS_TO_FETCH, eventsPerReducer);\r\n    final EventFetcher<K, V> eventFetcher = new EventFetcher<K, V>(reduceId, umbilical, scheduler, this, maxEventsToFetch);\r\n    eventFetcher.start();\r\n    boolean isLocal = localMapFiles != null;\r\n    final int numFetchers = isLocal ? 1 : jobConf.getInt(MRJobConfig.SHUFFLE_PARALLEL_COPIES, 5);\r\n    Fetcher<K, V>[] fetchers = new Fetcher[numFetchers];\r\n    if (isLocal) {\r\n        fetchers[0] = new LocalFetcher<K, V>(jobConf, reduceId, scheduler, merger, reporter, metrics, this, reduceTask.getShuffleSecret(), localMapFiles);\r\n        fetchers[0].start();\r\n    } else {\r\n        for (int i = 0; i < numFetchers; ++i) {\r\n            fetchers[i] = new Fetcher<K, V>(jobConf, reduceId, scheduler, merger, reporter, metrics, this, reduceTask.getShuffleSecret());\r\n            fetchers[i].start();\r\n        }\r\n    }\r\n    while (!scheduler.waitUntilDone(PROGRESS_FREQUENCY)) {\r\n        reporter.progress();\r\n        synchronized (this) {\r\n            if (throwable != null) {\r\n                throw new ShuffleError(\"error in shuffle in \" + throwingThreadName, throwable);\r\n            }\r\n        }\r\n    }\r\n    eventFetcher.shutDown();\r\n    for (Fetcher<K, V> fetcher : fetchers) {\r\n        fetcher.shutDown();\r\n    }\r\n    scheduler.close();\r\n    copyPhase.complete();\r\n    taskStatus.setPhase(TaskStatus.Phase.SORT);\r\n    reduceTask.statusUpdate(umbilical);\r\n    RawKeyValueIterator kvIter = null;\r\n    try {\r\n        kvIter = merger.close();\r\n    } catch (Throwable e) {\r\n        throw new ShuffleError(\"Error while doing final merge \", e);\r\n    }\r\n    synchronized (this) {\r\n        if (throwable != null) {\r\n            throw new ShuffleError(\"error in shuffle in \" + throwingThreadName, throwable);\r\n        }\r\n    }\r\n    return kvIter;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\task\\reduce",
  "methodName" : "close",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void close()\n{\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\task\\reduce",
  "methodName" : "reportException",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void reportException(Throwable t)\n{\r\n    if (throwable == null) {\r\n        throwable = t;\r\n        throwingThreadName = Thread.currentThread().getName();\r\n        synchronized (scheduler) {\r\n            scheduler.notifyAll();\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred\\lib",
  "methodName" : "getRecordWriter",
  "errType" : null,
  "containingMethodsNum" : 14,
  "sourceCodeText" : "RecordWriter<K, V> getRecordWriter(FileSystem fs, JobConf job, String name, Progressable arg3) throws IOException\n{\r\n    final FileSystem myFS = fs;\r\n    final String myName = generateLeafFileName(name);\r\n    final JobConf myJob = job;\r\n    final Progressable myProgressable = arg3;\r\n    return new RecordWriter<K, V>() {\r\n\r\n        TreeMap<String, RecordWriter<K, V>> recordWriters = new TreeMap<String, RecordWriter<K, V>>();\r\n\r\n        public void write(K key, V value) throws IOException {\r\n            String keyBasedPath = generateFileNameForKeyValue(key, value, myName);\r\n            String finalPath = getInputFileBasedOutputFileName(myJob, keyBasedPath);\r\n            K actualKey = generateActualKey(key, value);\r\n            V actualValue = generateActualValue(key, value);\r\n            RecordWriter<K, V> rw = this.recordWriters.get(finalPath);\r\n            if (rw == null) {\r\n                rw = getBaseRecordWriter(myFS, myJob, finalPath, myProgressable);\r\n                this.recordWriters.put(finalPath, rw);\r\n            }\r\n            rw.write(actualKey, actualValue);\r\n        }\r\n\r\n        public void close(Reporter reporter) throws IOException {\r\n            Iterator<String> keys = this.recordWriters.keySet().iterator();\r\n            while (keys.hasNext()) {\r\n                RecordWriter<K, V> rw = this.recordWriters.get(keys.next());\r\n                rw.close(reporter);\r\n            }\r\n            this.recordWriters.clear();\r\n        }\r\n    };\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred\\lib",
  "methodName" : "generateLeafFileName",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String generateLeafFileName(String name)\n{\r\n    return name;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred\\lib",
  "methodName" : "generateFileNameForKeyValue",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String generateFileNameForKeyValue(K key, V value, String name)\n{\r\n    return name;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred\\lib",
  "methodName" : "generateActualKey",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "K generateActualKey(K key, V value)\n{\r\n    return key;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred\\lib",
  "methodName" : "generateActualValue",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "V generateActualValue(K key, V value)\n{\r\n    return value;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred\\lib",
  "methodName" : "getInputFileBasedOutputFileName",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "String getInputFileBasedOutputFileName(JobConf job, String name)\n{\r\n    String infilepath = job.get(MRJobConfig.MAP_INPUT_FILE);\r\n    if (infilepath == null) {\r\n        return name;\r\n    }\r\n    int numOfTrailingLegsToUse = job.getInt(\"mapred.outputformat.numOfTrailingLegs\", 0);\r\n    if (numOfTrailingLegsToUse <= 0) {\r\n        return name;\r\n    }\r\n    Path infile = new Path(infilepath);\r\n    Path parent = infile.getParent();\r\n    String midName = infile.getName();\r\n    Path outPath = new Path(midName);\r\n    for (int i = 1; i < numOfTrailingLegsToUse; i++) {\r\n        if (parent == null)\r\n            break;\r\n        midName = parent.getName();\r\n        if (midName.length() == 0)\r\n            break;\r\n        parent = parent.getParent();\r\n        outPath = new Path(midName, outPath);\r\n    }\r\n    return outPath.toString();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred\\lib",
  "methodName" : "getBaseRecordWriter",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "RecordWriter<K, V> getBaseRecordWriter(FileSystem fs, JobConf job, String name, Progressable arg3) throws IOException",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "getId",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int getId()\n{\r\n    return id;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "toString",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String toString()\n{\r\n    return String.valueOf(id);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "hashCode",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int hashCode()\n{\r\n    return id;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "equals",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "boolean equals(Object o)\n{\r\n    if (this == o)\r\n        return true;\r\n    if (o == null)\r\n        return false;\r\n    if (o.getClass() == this.getClass()) {\r\n        ID that = (ID) o;\r\n        return this.id == that.id;\r\n    } else\r\n        return false;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "compareTo",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int compareTo(ID that)\n{\r\n    return this.id - that.id;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "readFields",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void readFields(DataInput in) throws IOException\n{\r\n    this.id = in.readInt();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "write",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void write(DataOutput out) throws IOException\n{\r\n    out.writeInt(id);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "addToQueue",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void addToQueue(PathDeletionContext... contexts)\n{\r\n    cleanupThread.addToQueue(contexts);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "deletePath",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "boolean deletePath(PathDeletionContext context) throws IOException\n{\r\n    context.enablePathForCleanup();\r\n    if (LOG.isDebugEnabled()) {\r\n        LOG.debug(\"Trying to delete \" + context.fullPath);\r\n    }\r\n    if (context.fs.exists(new Path(context.fullPath))) {\r\n        return context.fs.delete(new Path(context.fullPath), true);\r\n    }\r\n    return true;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "isQueueEmpty",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "boolean isQueueEmpty()\n{\r\n    return (cleanupThread.queue.size() == 0);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest\\stages",
  "methodName" : "executeStage",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "Path executeStage(final TaskManifest manifest) throws IOException\n{\r\n    final Path manifestDir = getTaskManifestDir();\r\n    Path manifestFile = manifestPathForTask(manifestDir, getRequiredTaskId());\r\n    Path manifestTempFile = manifestTempPathForTaskAttempt(manifestDir, getRequiredTaskAttemptId());\r\n    LOG.info(\"{}: Saving manifest file to {}\", getName(), manifestFile);\r\n    save(manifest, manifestTempFile, manifestFile);\r\n    return manifestFile;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred\\lib",
  "methodName" : "getBaseRecordWriter",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "RecordWriter<K, V> getBaseRecordWriter(FileSystem fs, JobConf job, String name, Progressable arg3) throws IOException\n{\r\n    if (theTextOutputFormat == null) {\r\n        theTextOutputFormat = new TextOutputFormat<K, V>();\r\n    }\r\n    return theTextOutputFormat.getRecordWriter(fs, job, name, arg3);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\join",
  "methodName" : "combine",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean combine(Object[] srcs, TupleWritable value)",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\join",
  "methodName" : "initialize",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void initialize(InputSplit split, TaskAttemptContext context) throws IOException, InterruptedException\n{\r\n    if (kids != null) {\r\n        for (int i = 0; i < kids.length; ++i) {\r\n            kids[i].initialize(((CompositeInputSplit) split).get(i), context);\r\n            if (kids[i].key() == null) {\r\n                continue;\r\n            }\r\n            if (keyclass == null) {\r\n                keyclass = kids[i].createKey().getClass().asSubclass(WritableComparable.class);\r\n            }\r\n            if (null == q) {\r\n                cmp = WritableComparator.get(keyclass, conf);\r\n                q = new PriorityQueue<ComposableRecordReader<K, ?>>(3, new Comparator<ComposableRecordReader<K, ?>>() {\r\n\r\n                    public int compare(ComposableRecordReader<K, ?> o1, ComposableRecordReader<K, ?> o2) {\r\n                        return cmp.compare(o1.key(), o2.key());\r\n                    }\r\n                });\r\n            }\r\n            if (!keyclass.equals(kids[i].key().getClass())) {\r\n                throw new ClassCastException(\"Child key classes fail to agree\");\r\n            }\r\n            if (kids[i].hasNext()) {\r\n                q.add(kids[i]);\r\n            }\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\join",
  "methodName" : "id",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int id()\n{\r\n    return id;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\join",
  "methodName" : "setConf",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setConf(Configuration conf)\n{\r\n    this.conf = conf;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\join",
  "methodName" : "getConf",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Configuration getConf()\n{\r\n    return conf;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\join",
  "methodName" : "getRecordReaderQueue",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "PriorityQueue<ComposableRecordReader<K, ?>> getRecordReaderQueue()\n{\r\n    return q;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\join",
  "methodName" : "getComparator",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "WritableComparator getComparator()\n{\r\n    return cmp;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\join",
  "methodName" : "add",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void add(ComposableRecordReader<K, ? extends V> rr) throws IOException, InterruptedException\n{\r\n    kids[rr.id()] = rr;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\join",
  "methodName" : "key",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "K key()\n{\r\n    if (jc.hasNext()) {\r\n        return jc.key();\r\n    }\r\n    if (!q.isEmpty()) {\r\n        return q.peek().key();\r\n    }\r\n    return null;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\join",
  "methodName" : "key",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void key(K key) throws IOException\n{\r\n    ReflectionUtils.copy(conf, key(), key);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\join",
  "methodName" : "getCurrentKey",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "K getCurrentKey()\n{\r\n    return key;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\join",
  "methodName" : "hasNext",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "boolean hasNext()\n{\r\n    return jc.hasNext() || !q.isEmpty();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\join",
  "methodName" : "skip",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void skip(K key) throws IOException, InterruptedException\n{\r\n    ArrayList<ComposableRecordReader<K, ?>> tmp = new ArrayList<ComposableRecordReader<K, ?>>();\r\n    while (!q.isEmpty() && cmp.compare(q.peek().key(), key) <= 0) {\r\n        tmp.add(q.poll());\r\n    }\r\n    for (ComposableRecordReader<K, ?> rr : tmp) {\r\n        rr.skip(key);\r\n        if (rr.hasNext()) {\r\n            q.add(rr);\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\join",
  "methodName" : "getDelegate",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "ResetableIterator<X> getDelegate()",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\join",
  "methodName" : "accept",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void accept(CompositeRecordReader.JoinCollector jc, K key) throws IOException, InterruptedException\n{\r\n    if (hasNext() && 0 == cmp.compare(key, key())) {\r\n        fillJoinCollector(createKey());\r\n        jc.add(id, getDelegate());\r\n        return;\r\n    }\r\n    jc.add(id, EMPTY);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\join",
  "methodName" : "fillJoinCollector",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void fillJoinCollector(K iterkey) throws IOException, InterruptedException\n{\r\n    if (!q.isEmpty()) {\r\n        q.peek().key(iterkey);\r\n        while (0 == cmp.compare(q.peek().key(), iterkey)) {\r\n            ComposableRecordReader<K, ?> t = q.poll();\r\n            t.accept(jc, iterkey);\r\n            if (t.hasNext()) {\r\n                q.add(t);\r\n            } else if (q.isEmpty()) {\r\n                return;\r\n            }\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\join",
  "methodName" : "compareTo",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "int compareTo(ComposableRecordReader<K, ?> other)\n{\r\n    return cmp.compare(key(), other.key());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\join",
  "methodName" : "createKey",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "K createKey()\n{\r\n    if (keyclass == null || keyclass.equals(NullWritable.class)) {\r\n        return (K) NullWritable.get();\r\n    }\r\n    return (K) ReflectionUtils.newInstance(keyclass, getConf());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\join",
  "methodName" : "createTupleWritable",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "TupleWritable createTupleWritable()\n{\r\n    Writable[] vals = new Writable[kids.length];\r\n    for (int i = 0; i < vals.length; ++i) {\r\n        vals[i] = kids[i].createValue();\r\n    }\r\n    return new TupleWritable(vals);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\join",
  "methodName" : "getCurrentValue",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "X getCurrentValue() throws IOException, InterruptedException\n{\r\n    return value;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\join",
  "methodName" : "close",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void close() throws IOException\n{\r\n    if (kids != null) {\r\n        for (RecordReader<K, ? extends Writable> rr : kids) {\r\n            rr.close();\r\n        }\r\n    }\r\n    if (jc != null) {\r\n        jc.close();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\join",
  "methodName" : "getProgress",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "float getProgress() throws IOException, InterruptedException\n{\r\n    float ret = 1.0f;\r\n    for (RecordReader<K, ? extends Writable> rr : kids) {\r\n        ret = Math.min(ret, rr.getProgress());\r\n    }\r\n    return ret;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "getDatum",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Object getDatum()\n{\r\n    return datum;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "setDatum",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setDatum(Object datum)\n{\r\n    this.datum = (JobStatusChanged) datum;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "getJobId",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "JobID getJobId()\n{\r\n    return JobID.forName(datum.getJobid().toString());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "getStatus",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String getStatus()\n{\r\n    return datum.getJobStatus().toString();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "getEventType",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "EventType getEventType()\n{\r\n    return EventType.JOB_STATUS_CHANGED;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "toTimelineEvent",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "TimelineEvent toTimelineEvent()\n{\r\n    TimelineEvent tEvent = new TimelineEvent();\r\n    tEvent.setId(StringUtils.toUpperCase(getEventType().name()));\r\n    tEvent.addInfo(\"STATUS\", getStatus());\r\n    return tEvent;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "getTimelineMetrics",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Set<TimelineMetric> getTimelineMetrics()\n{\r\n    return null;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\input",
  "methodName" : "initialize",
  "errType" : null,
  "containingMethodsNum" : 18,
  "sourceCodeText" : "void initialize(InputSplit genericSplit, TaskAttemptContext context) throws IOException\n{\r\n    FileSplit split = (FileSplit) genericSplit;\r\n    Configuration job = context.getConfiguration();\r\n    this.maxLineLength = job.getInt(MAX_LINE_LENGTH, Integer.MAX_VALUE);\r\n    start = split.getStart();\r\n    end = start + split.getLength();\r\n    final Path file = split.getPath();\r\n    final FutureDataInputStreamBuilder builder = file.getFileSystem(job).openFile(file);\r\n    FutureIOSupport.propagateOptions(builder, job, MRJobConfig.INPUT_FILE_OPTION_PREFIX, MRJobConfig.INPUT_FILE_MANDATORY_PREFIX);\r\n    fileIn = FutureIOSupport.awaitFuture(builder.build());\r\n    CompressionCodec codec = new CompressionCodecFactory(job).getCodec(file);\r\n    if (null != codec) {\r\n        isCompressedInput = true;\r\n        decompressor = CodecPool.getDecompressor(codec);\r\n        if (codec instanceof SplittableCompressionCodec) {\r\n            final SplitCompressionInputStream cIn = ((SplittableCompressionCodec) codec).createInputStream(fileIn, decompressor, start, end, SplittableCompressionCodec.READ_MODE.BYBLOCK);\r\n            in = new CompressedSplitLineReader(cIn, job, this.recordDelimiterBytes);\r\n            start = cIn.getAdjustedStart();\r\n            end = cIn.getAdjustedEnd();\r\n            filePosition = cIn;\r\n        } else {\r\n            if (start != 0) {\r\n                throw new IOException(\"Cannot seek in \" + codec.getClass().getSimpleName() + \" compressed stream\");\r\n            }\r\n            in = new SplitLineReader(codec.createInputStream(fileIn, decompressor), job, this.recordDelimiterBytes);\r\n            filePosition = fileIn;\r\n        }\r\n    } else {\r\n        fileIn.seek(start);\r\n        in = new UncompressedSplitLineReader(fileIn, job, this.recordDelimiterBytes, split.getLength());\r\n        filePosition = fileIn;\r\n    }\r\n    if (start != 0) {\r\n        start += in.readLine(new Text(), 0, maxBytesToConsume(start));\r\n    }\r\n    this.pos = start;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\input",
  "methodName" : "maxBytesToConsume",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "int maxBytesToConsume(long pos)\n{\r\n    return isCompressedInput ? Integer.MAX_VALUE : (int) Math.max(Math.min(Integer.MAX_VALUE, end - pos), maxLineLength);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\input",
  "methodName" : "getFilePosition",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "long getFilePosition() throws IOException\n{\r\n    long retVal;\r\n    if (isCompressedInput && null != filePosition) {\r\n        retVal = filePosition.getPos();\r\n    } else {\r\n        retVal = pos;\r\n    }\r\n    return retVal;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\input",
  "methodName" : "skipUtfByteOrderMark",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "int skipUtfByteOrderMark() throws IOException\n{\r\n    int newMaxLineLength = (int) Math.min(3L + (long) maxLineLength, Integer.MAX_VALUE);\r\n    int newSize = in.readLine(value, newMaxLineLength, maxBytesToConsume(pos));\r\n    pos += newSize;\r\n    int textLength = value.getLength();\r\n    byte[] textBytes = value.getBytes();\r\n    if ((textLength >= 3) && (textBytes[0] == (byte) 0xEF) && (textBytes[1] == (byte) 0xBB) && (textBytes[2] == (byte) 0xBF)) {\r\n        LOG.info(\"Found UTF-8 BOM and skipped it\");\r\n        textLength -= 3;\r\n        newSize -= 3;\r\n        if (textLength > 0) {\r\n            textBytes = value.copyBytes();\r\n            value.set(textBytes, 3, textLength);\r\n        } else {\r\n            value.clear();\r\n        }\r\n    }\r\n    return newSize;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\input",
  "methodName" : "nextKeyValue",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "boolean nextKeyValue() throws IOException\n{\r\n    if (key == null) {\r\n        key = new LongWritable();\r\n    }\r\n    key.set(pos);\r\n    if (value == null) {\r\n        value = new Text();\r\n    }\r\n    int newSize = 0;\r\n    while (getFilePosition() <= end || in.needAdditionalRecordAfterSplit()) {\r\n        if (pos == 0) {\r\n            newSize = skipUtfByteOrderMark();\r\n        } else {\r\n            newSize = in.readLine(value, maxLineLength, maxBytesToConsume(pos));\r\n            pos += newSize;\r\n        }\r\n        if ((newSize == 0) || (newSize < maxLineLength)) {\r\n            break;\r\n        }\r\n        LOG.info(\"Skipped line of size \" + newSize + \" at pos \" + (pos - newSize));\r\n    }\r\n    if (newSize == 0) {\r\n        key = null;\r\n        value = null;\r\n        return false;\r\n    } else {\r\n        return true;\r\n    }\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\input",
  "methodName" : "getCurrentKey",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "LongWritable getCurrentKey()\n{\r\n    return key;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\input",
  "methodName" : "getCurrentValue",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Text getCurrentValue()\n{\r\n    return value;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\input",
  "methodName" : "getProgress",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "float getProgress() throws IOException\n{\r\n    if (start == end) {\r\n        return 0.0f;\r\n    } else {\r\n        return Math.min(1.0f, (getFilePosition() - start) / (float) (end - start));\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\input",
  "methodName" : "close",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void close() throws IOException\n{\r\n    try {\r\n        if (in != null) {\r\n            in.close();\r\n        }\r\n    } finally {\r\n        if (decompressor != null) {\r\n            CodecPool.returnDecompressor(decompressor);\r\n            decompressor = null;\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "downgrade",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "QueueAclsInfo downgrade(org.apache.hadoop.mapreduce.QueueAclsInfo acl)\n{\r\n    return new QueueAclsInfo(acl.getQueueName(), acl.getOperations());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred\\lib",
  "methodName" : "configure",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void configure(JobConf job)\n{\r\n    super.setConf(job);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\aggregate",
  "methodName" : "setup",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setup(Context context) throws IOException, InterruptedException\n{\r\n    ValueAggregatorJobBase.setup(context.getConfiguration());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\aggregate",
  "methodName" : "map",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void map(K1 key, V1 value, Context context) throws IOException, InterruptedException\n{\r\n    Iterator<?> iter = ValueAggregatorJobBase.aggregatorDescriptorList.iterator();\r\n    while (iter.hasNext()) {\r\n        ValueAggregatorDescriptor ad = (ValueAggregatorDescriptor) iter.next();\r\n        Iterator<Entry<Text, Text>> ens = ad.generateKeyValPairs(key, value).iterator();\r\n        while (ens.hasNext()) {\r\n            Entry<Text, Text> en = ens.next();\r\n            context.write(en.getKey(), en.getValue());\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "extendInternal",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void extendInternal(double newProgress, int newValue)\n{\r\n    if (state == null) {\r\n        return;\r\n    }\r\n    state.currentAccumulation += (double) (newValue - previousValue);\r\n    previousValue = newValue;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\db",
  "methodName" : "executeQuery",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "ResultSet executeQuery(String query) throws SQLException\n{\r\n    statement = getConnection().prepareStatement(query, ResultSet.TYPE_FORWARD_ONLY, ResultSet.CONCUR_READ_ONLY);\r\n    statement.setFetchSize(Integer.MIN_VALUE);\r\n    return statement.executeQuery();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "isOnDisk",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean isOnDisk()\n{\r\n    return true;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "isInMemory",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean isInMemory()\n{\r\n    return inMemory;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getLocation",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getLocation()\n{\r\n    return location;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output",
  "methodName" : "getOutputPath",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Path getOutputPath()\n{\r\n    return committer.getOutputPath();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output",
  "methodName" : "getWorkPath",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Path getWorkPath() throws IOException\n{\r\n    return committer.getWorkPath();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output",
  "methodName" : "setupJob",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setupJob(JobContext jobContext) throws IOException\n{\r\n    committer.setupJob(jobContext);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output",
  "methodName" : "setupTask",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setupTask(TaskAttemptContext taskContext) throws IOException\n{\r\n    committer.setupTask(taskContext);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output",
  "methodName" : "needsTaskCommit",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "boolean needsTaskCommit(TaskAttemptContext taskContext) throws IOException\n{\r\n    return committer.needsTaskCommit(taskContext);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output",
  "methodName" : "commitTask",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void commitTask(TaskAttemptContext taskContext) throws IOException\n{\r\n    committer.commitTask(taskContext);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output",
  "methodName" : "abortTask",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void abortTask(TaskAttemptContext taskContext) throws IOException\n{\r\n    committer.abortTask(taskContext);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output",
  "methodName" : "cleanupJob",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void cleanupJob(JobContext jobContext) throws IOException\n{\r\n    super.cleanupJob(jobContext);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output",
  "methodName" : "commitJob",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void commitJob(JobContext jobContext) throws IOException\n{\r\n    committer.commitJob(jobContext);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output",
  "methodName" : "abortJob",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void abortJob(JobContext jobContext, JobStatus.State state) throws IOException\n{\r\n    committer.abortJob(jobContext, state);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output",
  "methodName" : "isRecoverySupported",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "boolean isRecoverySupported()\n{\r\n    return committer.isRecoverySupported();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output",
  "methodName" : "isCommitJobRepeatable",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "boolean isCommitJobRepeatable(JobContext jobContext) throws IOException\n{\r\n    return committer.isCommitJobRepeatable(jobContext);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output",
  "methodName" : "isRecoverySupported",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "boolean isRecoverySupported(JobContext jobContext) throws IOException\n{\r\n    return committer.isRecoverySupported(jobContext);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output",
  "methodName" : "recoverTask",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void recoverTask(TaskAttemptContext taskContext) throws IOException\n{\r\n    committer.recoverTask(taskContext);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output",
  "methodName" : "hasOutputPath",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "boolean hasOutputPath()\n{\r\n    return committer.hasOutputPath();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output",
  "methodName" : "toString",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String toString()\n{\r\n    return \"BindingPathOutputCommitter{\" + \"committer=\" + committer + '}';\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output",
  "methodName" : "getCommitter",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "PathOutputCommitter getCommitter()\n{\r\n    return committer;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred\\lib",
  "methodName" : "map",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void map(K key, Text value, OutputCollector<Text, LongWritable> output, Reporter reporter) throws IOException\n{\r\n    String text = value.toString();\r\n    StringTokenizer st = new StringTokenizer(text);\r\n    while (st.hasMoreTokens()) {\r\n        output.collect(new Text(st.nextToken()), new LongWritable(1));\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "getTaskTrackerName",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getTaskTrackerName()\n{\r\n    return name;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "isBlacklisted",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean isBlacklisted()\n{\r\n    return isBlacklisted;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "getReasonForBlacklist",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getReasonForBlacklist()\n{\r\n    return reasonForBlacklist;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "getBlacklistReport",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getBlacklistReport()\n{\r\n    return blacklistReport;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "readFields",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void readFields(DataInput in) throws IOException\n{\r\n    name = Text.readString(in);\r\n    isBlacklisted = in.readBoolean();\r\n    reasonForBlacklist = Text.readString(in);\r\n    blacklistReport = Text.readString(in);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "write",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void write(DataOutput out) throws IOException\n{\r\n    Text.writeString(out, name);\r\n    out.writeBoolean(isBlacklisted);\r\n    Text.writeString(out, reasonForBlacklist);\r\n    Text.writeString(out, blacklistReport);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest\\stages",
  "methodName" : "getStageStatisticName",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getStageStatisticName(Arguments arguments)\n{\r\n    return arguments.statisticName;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest\\stages",
  "methodName" : "executeStage",
  "errType" : [ "FileNotFoundException", "IOException" ],
  "containingMethodsNum" : 18,
  "sourceCodeText" : "Result executeStage(final Arguments args) throws IOException\n{\r\n    stageName = getStageName(args);\r\n    final Path baseDir = requireNonNull(getStageConfig().getOutputTempSubDir());\r\n    LOG.debug(\"{}: Cleaup of directory {} with {}\", getName(), baseDir, args);\r\n    if (!args.enabled) {\r\n        LOG.info(\"{}: Cleanup of {} disabled\", getName(), baseDir);\r\n        return new Result(Outcome.DISABLED, baseDir, 0, null);\r\n    }\r\n    if (getFileStatusOrNull(baseDir) == null) {\r\n        return new Result(Outcome.NOTHING_TO_CLEAN_UP, baseDir, 0, null);\r\n    }\r\n    Outcome outcome = null;\r\n    IOException exception;\r\n    LOG.info(\"{}: Deleting job directory {}\", getName(), baseDir);\r\n    if (args.deleteTaskAttemptDirsInParallel) {\r\n        Path taskSubDir = getStageConfig().getJobAttemptTaskSubDir();\r\n        try (DurationInfo info = new DurationInfo(LOG, \"parallel deletion of task attempts in %s\", taskSubDir)) {\r\n            RemoteIterator<FileStatus> dirs = RemoteIterators.filteringRemoteIterator(listStatusIterator(taskSubDir), FileStatus::isDirectory);\r\n            TaskPool.foreach(dirs).executeWith(getIOProcessors()).stopOnFailure().suppressExceptions(false).run(this::rmTaskAttemptDir);\r\n            getIOStatistics().aggregate((retrieveIOStatistics(dirs)));\r\n            if (getLastDeleteException() != null) {\r\n                throw getLastDeleteException();\r\n            }\r\n            outcome = Outcome.PARALLEL_DELETE;\r\n        } catch (FileNotFoundException ex) {\r\n            LOG.debug(\"{}: Task attempt dir {} not found\", getName(), taskSubDir);\r\n            outcome = Outcome.DELETED;\r\n        } catch (IOException ex) {\r\n            LOG.info(\"{}: Exception while listing/deleting task attempts under {}; continuing\", getName(), taskSubDir, ex);\r\n            outcome = Outcome.DELETED;\r\n        }\r\n    }\r\n    exception = deleteOneDir(baseDir);\r\n    if (exception != null) {\r\n        outcome = Outcome.FAILURE;\r\n    } else {\r\n        if (outcome == null) {\r\n            outcome = Outcome.DELETED;\r\n        }\r\n    }\r\n    Result result = new Result(outcome, baseDir, deleteDirCount.get(), exception);\r\n    if (!result.succeeded() && !args.suppressExceptions) {\r\n        result.maybeRethrowException();\r\n    }\r\n    return result;\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest\\stages",
  "methodName" : "rmTaskAttemptDir",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void rmTaskAttemptDir(FileStatus status) throws IOException\n{\r\n    updateAuditContext(stageName);\r\n    progress();\r\n    deleteOneDir(status.getPath());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest\\stages",
  "methodName" : "deleteOneDir",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "IOException deleteOneDir(final Path dir) throws IOException\n{\r\n    deleteDirCount.incrementAndGet();\r\n    IOException ex = deleteDir(dir, true);\r\n    if (ex != null) {\r\n        deleteFailure(ex);\r\n    }\r\n    return ex;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest\\stages",
  "methodName" : "deleteFailure",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void deleteFailure(IOException ex)\n{\r\n    deleteFailureCount.incrementAndGet();\r\n    lastDeleteException = ex;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest\\stages",
  "methodName" : "getLastDeleteException",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "IOException getLastDeleteException()\n{\r\n    return lastDeleteException;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest\\stages",
  "methodName" : "cleanupStageOptionsFromConfig",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "Arguments cleanupStageOptionsFromConfig(String statisticName, Configuration conf)\n{\r\n    boolean enabled = !conf.getBoolean(FILEOUTPUTCOMMITTER_CLEANUP_SKIPPED, FILEOUTPUTCOMMITTER_CLEANUP_SKIPPED_DEFAULT);\r\n    boolean suppressExceptions = conf.getBoolean(FILEOUTPUTCOMMITTER_CLEANUP_FAILURES_IGNORED, FILEOUTPUTCOMMITTER_CLEANUP_FAILURES_IGNORED_DEFAULT);\r\n    boolean deleteTaskAttemptDirsInParallel = conf.getBoolean(OPT_CLEANUP_PARALLEL_DELETE, OPT_CLEANUP_PARALLEL_DELETE_DIRS_DEFAULT);\r\n    return new Arguments(statisticName, enabled, deleteTaskAttemptDirsInParallel, suppressExceptions);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "getLength",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getLength() throws IOException, InterruptedException",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "getLocations",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String[] getLocations() throws IOException, InterruptedException",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "getLocationInfo",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "SplitLocationInfo[] getLocationInfo() throws IOException\n{\r\n    return null;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\join",
  "methodName" : "createRecordReader",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "ComposableRecordReader<K, V> createRecordReader(InputSplit split, TaskAttemptContext context) throws IOException, InterruptedException",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\input",
  "methodName" : "createRecordReader",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "RecordReader<LongWritable, Text> createRecordReader(InputSplit genericSplit, TaskAttemptContext context) throws IOException\n{\r\n    context.setStatus(genericSplit.toString());\r\n    return new LineRecordReader();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\input",
  "methodName" : "getSplits",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "List<InputSplit> getSplits(JobContext job) throws IOException\n{\r\n    List<InputSplit> splits = new ArrayList<InputSplit>();\r\n    int numLinesPerSplit = getNumLinesPerSplit(job);\r\n    for (FileStatus status : listStatus(job)) {\r\n        splits.addAll(getSplitsForFile(status, job.getConfiguration(), numLinesPerSplit));\r\n    }\r\n    return splits;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\input",
  "methodName" : "getSplitsForFile",
  "errType" : null,
  "containingMethodsNum" : 9,
  "sourceCodeText" : "List<FileSplit> getSplitsForFile(FileStatus status, Configuration conf, int numLinesPerSplit) throws IOException\n{\r\n    List<FileSplit> splits = new ArrayList<FileSplit>();\r\n    Path fileName = status.getPath();\r\n    if (status.isDirectory()) {\r\n        throw new IOException(\"Not a file: \" + fileName);\r\n    }\r\n    LineReader lr = null;\r\n    try {\r\n        final FutureDataInputStreamBuilder builder = fileName.getFileSystem(conf).openFile(fileName);\r\n        FutureIOSupport.propagateOptions(builder, conf, MRJobConfig.INPUT_FILE_OPTION_PREFIX, MRJobConfig.INPUT_FILE_MANDATORY_PREFIX);\r\n        FSDataInputStream in = FutureIOSupport.awaitFuture(builder.build());\r\n        lr = new LineReader(in, conf);\r\n        Text line = new Text();\r\n        int numLines = 0;\r\n        long begin = 0;\r\n        long length = 0;\r\n        int num = -1;\r\n        while ((num = lr.readLine(line)) > 0) {\r\n            numLines++;\r\n            length += num;\r\n            if (numLines == numLinesPerSplit) {\r\n                splits.add(createFileSplit(fileName, begin, length));\r\n                begin += length;\r\n                length = 0;\r\n                numLines = 0;\r\n            }\r\n        }\r\n        if (numLines != 0) {\r\n            splits.add(createFileSplit(fileName, begin, length));\r\n        }\r\n    } finally {\r\n        if (lr != null) {\r\n            lr.close();\r\n        }\r\n    }\r\n    return splits;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\input",
  "methodName" : "createFileSplit",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "FileSplit createFileSplit(Path fileName, long begin, long length)\n{\r\n    return (begin == 0) ? new FileSplit(fileName, begin, length - 1, new String[] {}) : new FileSplit(fileName, begin - 1, length, new String[] {});\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\input",
  "methodName" : "setNumLinesPerSplit",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setNumLinesPerSplit(Job job, int numLines)\n{\r\n    job.getConfiguration().setInt(LINES_PER_MAP, numLines);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\input",
  "methodName" : "getNumLinesPerSplit",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "int getNumLinesPerSplit(JobContext job)\n{\r\n    return job.getConfiguration().getInt(LINES_PER_MAP, 1);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\db",
  "methodName" : "executeQuery",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "ResultSet executeQuery(String query) throws SQLException\n{\r\n    statement = getConnection().prepareStatement(query, ResultSet.TYPE_FORWARD_ONLY, ResultSet.CONCUR_READ_ONLY);\r\n    statement.setFetchSize(Integer.MIN_VALUE);\r\n    return statement.executeQuery();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred\\join",
  "methodName" : "reduce",
  "errType" : null,
  "containingMethodsNum" : 9,
  "sourceCodeText" : "Token reduce(Stack<Token> st, JobConf job) throws IOException\n{\r\n    LinkedList<Token> args = new LinkedList<Token>();\r\n    while (!st.isEmpty() && !TType.LPAREN.equals(st.peek().getType())) {\r\n        args.addFirst(st.pop());\r\n    }\r\n    if (st.isEmpty()) {\r\n        throw new IOException(\"Unmatched ')'\");\r\n    }\r\n    st.pop();\r\n    if (st.isEmpty() || !TType.IDENT.equals(st.peek().getType())) {\r\n        throw new IOException(\"Identifier expected\");\r\n    }\r\n    Node n = Node.forIdent(st.pop().getStr());\r\n    n.parse(args, job);\r\n    return new NodeToken(n);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred\\join",
  "methodName" : "parse",
  "errType" : null,
  "containingMethodsNum" : 9,
  "sourceCodeText" : "Node parse(String expr, JobConf job) throws IOException\n{\r\n    if (null == expr) {\r\n        throw new IOException(\"Expression is null\");\r\n    }\r\n    Class<? extends WritableComparator> cmpcl = job.getClass(\"mapred.join.keycomparator\", null, WritableComparator.class);\r\n    Lexer lex = new Lexer(expr);\r\n    Stack<Token> st = new Stack<Token>();\r\n    Token tok;\r\n    while ((tok = lex.next()) != null) {\r\n        if (TType.RPAREN.equals(tok.getType())) {\r\n            st.push(reduce(st, job));\r\n        } else {\r\n            st.push(tok);\r\n        }\r\n    }\r\n    if (st.size() == 1 && TType.CIF.equals(st.peek().getType())) {\r\n        Node ret = st.pop().getNode();\r\n        if (cmpcl != null) {\r\n            ret.setKeyComparator(cmpcl);\r\n        }\r\n        return ret;\r\n    }\r\n    throw new IOException(\"Missing ')'\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest",
  "methodName" : "enterCommitter",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "ManifestCommitterConfig enterCommitter(boolean isTask, JobContext context)\n{\r\n    ManifestCommitterConfig committerConfig = new ManifestCommitterConfig(getOutputPath(), isTask ? TASK_COMMITTER : JOB_COMMITTER, context, iostatistics, this);\r\n    updateCommonContextOnCommitterEntry(committerConfig);\r\n    return committerConfig;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest",
  "methodName" : "setupJob",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void setupJob(final JobContext jobContext) throws IOException\n{\r\n    ManifestCommitterConfig committerConfig = enterCommitter(false, jobContext);\r\n    StageConfig stageConfig = committerConfig.createStageConfig().withOperations(createManifestStoreOperations()).build();\r\n    new SetupJobStage(stageConfig).apply(committerConfig.getCreateJobMarker());\r\n    logCommitterStatisticsAtDebug();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest",
  "methodName" : "setupTask",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void setupTask(final TaskAttemptContext context) throws IOException\n{\r\n    ManifestCommitterConfig committerConfig = enterCommitter(true, context);\r\n    StageConfig stageConfig = committerConfig.createStageConfig().withOperations(createManifestStoreOperations()).build();\r\n    new SetupTaskStage(stageConfig).apply(\"\");\r\n    logCommitterStatisticsAtDebug();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest",
  "methodName" : "needsTaskCommit",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "boolean needsTaskCommit(final TaskAttemptContext context) throws IOException\n{\r\n    LOG.info(\"Probe for needsTaskCommit({})\", context.getTaskAttemptID());\r\n    return true;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest",
  "methodName" : "isCommitJobRepeatable",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "boolean isCommitJobRepeatable(final JobContext jobContext) throws IOException\n{\r\n    LOG.info(\"Probe for isCommitJobRepeatable({}): returning false\", jobContext.getJobID());\r\n    return false;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest",
  "methodName" : "isRecoverySupported",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "boolean isRecoverySupported(final JobContext jobContext) throws IOException\n{\r\n    LOG.info(\"Probe for isRecoverySupported({}): returning false\", jobContext.getJobID());\r\n    return false;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest",
  "methodName" : "recoverTask",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void recoverTask(final TaskAttemptContext taskContext) throws IOException\n{\r\n    LOG.warn(\"Rejecting recoverTask({}) call\", taskContext.getTaskAttemptID());\r\n    throw new IOException(\"Cannot recover task \" + taskContext.getTaskAttemptID());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest",
  "methodName" : "commitTask",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void commitTask(final TaskAttemptContext context) throws IOException\n{\r\n    ManifestCommitterConfig committerConfig = enterCommitter(true, context);\r\n    try {\r\n        StageConfig stageConfig = committerConfig.createStageConfig().withOperations(createManifestStoreOperations()).build();\r\n        taskAttemptCommittedManifest = new CommitTaskStage(stageConfig).apply(null).getTaskManifest();\r\n        iostatistics.incrementCounter(COMMITTER_TASKS_COMPLETED_COUNT, 1);\r\n    } catch (IOException e) {\r\n        iostatistics.incrementCounter(COMMITTER_TASKS_FAILED_COUNT, 1);\r\n        throw e;\r\n    } finally {\r\n        logCommitterStatisticsAtDebug();\r\n        updateCommonContextOnCommitterExit();\r\n    }\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest",
  "methodName" : "abortTask",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void abortTask(final TaskAttemptContext context) throws IOException\n{\r\n    ManifestCommitterConfig committerConfig = enterCommitter(true, context);\r\n    try {\r\n        new AbortTaskStage(committerConfig.createStageConfig().withOperations(createManifestStoreOperations()).build()).apply(false);\r\n    } finally {\r\n        logCommitterStatisticsAtDebug();\r\n        updateCommonContextOnCommitterExit();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest",
  "methodName" : "getOrCreateSuccessData",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "ManifestSuccessData getOrCreateSuccessData(ManifestCommitterConfig committerConfig)\n{\r\n    if (successReport == null) {\r\n        successReport = createManifestOutcome(committerConfig.createStageConfig(), activeStage);\r\n    }\r\n    return successReport;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest",
  "methodName" : "commitJob",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 14,
  "sourceCodeText" : "void commitJob(final JobContext jobContext) throws IOException\n{\r\n    ManifestCommitterConfig committerConfig = enterCommitter(false, jobContext);\r\n    ManifestSuccessData marker = getOrCreateSuccessData(committerConfig);\r\n    IOException failure = null;\r\n    try (CloseableTaskPoolSubmitter ioProcs = committerConfig.createSubmitter();\r\n        ManifestStoreOperations storeOperations = createManifestStoreOperations()) {\r\n        StageConfig stageConfig = committerConfig.createStageConfig().withOperations(storeOperations).withIOProcessors(ioProcs).build();\r\n        final Configuration conf = jobContext.getConfiguration();\r\n        CommitJobStage.Result result = new CommitJobStage(stageConfig).apply(new CommitJobStage.Arguments(committerConfig.getCreateJobMarker(), committerConfig.getValidateOutput(), conf.getTrimmed(OPT_DIAGNOSTICS_MANIFEST_DIR, \"\"), cleanupStageOptionsFromConfig(OP_STAGE_JOB_CLEANUP, conf)));\r\n        marker = result.getJobSuccessData();\r\n        setSuccessReport(marker);\r\n    } catch (IOException e) {\r\n        failure = e;\r\n        throw e;\r\n    } finally {\r\n        maybeSaveSummary(activeStage, committerConfig, marker, failure, true, true);\r\n        LOG.info(\"{}: Job Commit statistics {}\", committerConfig.getName(), ioStatisticsToPrettyString(iostatistics));\r\n        final Long recoveries = iostatistics.counters().get(OP_COMMIT_FILE_RENAME_RECOVERED);\r\n        if (recoveries != null && recoveries > 0) {\r\n            LOG.warn(\"{}: rename failures were recovered from. Number of recoveries: {}\", committerConfig.getName(), recoveries);\r\n        }\r\n        updateCommonContextOnCommitterExit();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest",
  "methodName" : "abortJob",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void abortJob(final JobContext jobContext, final JobStatus.State state) throws IOException\n{\r\n    LOG.info(\"Aborting Job {} in state {}\", jobContext.getJobID(), state);\r\n    ManifestCommitterConfig committerConfig = enterCommitter(false, jobContext);\r\n    ManifestSuccessData report = getOrCreateSuccessData(committerConfig);\r\n    IOException failure = null;\r\n    try {\r\n        executeCleanup(OP_STAGE_JOB_ABORT, jobContext, committerConfig);\r\n    } catch (IOException e) {\r\n        failure = e;\r\n    }\r\n    report.setSuccess(false);\r\n    maybeSaveSummary(activeStage, committerConfig, report, failure, true, false);\r\n    LOG.info(\"Job Abort statistics {}\", ioStatisticsToPrettyString(iostatistics));\r\n    updateCommonContextOnCommitterExit();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest",
  "methodName" : "cleanupJob",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void cleanupJob(final JobContext jobContext) throws IOException\n{\r\n    ManifestCommitterConfig committerConfig = enterCommitter(false, jobContext);\r\n    try {\r\n        executeCleanup(OP_STAGE_JOB_CLEANUP, jobContext, committerConfig);\r\n    } finally {\r\n        logCommitterStatisticsAtDebug();\r\n        updateCommonContextOnCommitterExit();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest",
  "methodName" : "executeCleanup",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "CleanupJobStage.Result executeCleanup(final String statisticName, final JobContext jobContext, final ManifestCommitterConfig committerConfig) throws IOException\n{\r\n    try (CloseableTaskPoolSubmitter ioProcs = committerConfig.createSubmitter()) {\r\n        return new CleanupJobStage(committerConfig.createStageConfig().withOperations(createManifestStoreOperations()).withIOProcessors(ioProcs).build()).apply(cleanupStageOptionsFromConfig(statisticName, jobContext.getConfiguration()));\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest",
  "methodName" : "getOutputPath",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Path getOutputPath()\n{\r\n    return getDestinationDir();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest",
  "methodName" : "getWorkPath",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Path getWorkPath()\n{\r\n    return getTaskAttemptDir();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest",
  "methodName" : "getDestinationDir",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Path getDestinationDir()\n{\r\n    return destinationDir;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest",
  "methodName" : "getTaskAttemptDir",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Path getTaskAttemptDir()\n{\r\n    return taskAttemptDir;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest",
  "methodName" : "enterStage",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void enterStage(String stage)\n{\r\n    activeStage = stage;\r\n    AuditingIntegration.enterStage(stage);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest",
  "methodName" : "exitStage",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void exitStage(String stage)\n{\r\n    AuditingIntegration.exitStage();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest",
  "methodName" : "getJobUniqueId",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String getJobUniqueId()\n{\r\n    return baseConfig.getJobUniqueId();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest",
  "methodName" : "getConf",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Configuration getConf()\n{\r\n    return baseConfig.getConf();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest",
  "methodName" : "getSuccessReport",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "ManifestSuccessData getSuccessReport()\n{\r\n    return successReport;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest",
  "methodName" : "setSuccessReport",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setSuccessReport(ManifestSuccessData successReport)\n{\r\n    this.successReport = successReport;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest",
  "methodName" : "getTaskAttemptCommittedManifest",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "TaskManifest getTaskAttemptCommittedManifest()\n{\r\n    return taskAttemptCommittedManifest;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest",
  "methodName" : "getTaskAttemptPath",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Path getTaskAttemptPath(TaskAttemptContext context)\n{\r\n    return enterCommitter(false, context).getTaskAttemptDir();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest",
  "methodName" : "getTaskManifestPath",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "Path getTaskManifestPath(TaskAttemptContext context)\n{\r\n    final Path dir = enterCommitter(false, context).getTaskManifestDir();\r\n    return manifestPathForTask(dir, context.getTaskAttemptID().getTaskID().toString());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest",
  "methodName" : "getJobAttemptPath",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Path getJobAttemptPath(JobContext context)\n{\r\n    return enterCommitter(false, context).getJobAttemptDir();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest",
  "methodName" : "resolveDestinationDirectory",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Path resolveDestinationDirectory(Path outputPath, Configuration conf) throws IOException\n{\r\n    return FileSystem.get(outputPath.toUri(), conf).makeQualified(outputPath);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest",
  "methodName" : "createManifestStoreOperations",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "ManifestStoreOperations createManifestStoreOperations() throws IOException\n{\r\n    return ManifestCommitterSupport.createManifestStoreOperations(baseConfig.getConf(), baseConfig.getDestinationFileSystem(), baseConfig.getDestinationDir());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest",
  "methodName" : "logCommitterStatisticsAtDebug",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void logCommitterStatisticsAtDebug()\n{\r\n    logIOStatisticsAtDebug(LOG, \"Committer Statistics\", this);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest",
  "methodName" : "toString",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "String toString()\n{\r\n    final StringBuilder sb = new StringBuilder(\"ManifestCommitter{\");\r\n    sb.append(baseConfig);\r\n    sb.append(\", iostatistics=\").append(ioStatisticsToPrettyString(iostatistics));\r\n    sb.append('}');\r\n    return sb.toString();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest",
  "methodName" : "maybeSaveSummary",
  "errType" : [ "IOException", "FileNotFoundException" ],
  "containingMethodsNum" : 15,
  "sourceCodeText" : "Path maybeSaveSummary(String activeStage, ManifestCommitterConfig config, ManifestSuccessData report, Throwable thrown, boolean quiet, boolean overwrite) throws IOException\n{\r\n    Configuration conf = config.getConf();\r\n    String reportDir = conf.getTrimmed(OPT_SUMMARY_REPORT_DIR, \"\");\r\n    if (reportDir.isEmpty()) {\r\n        LOG.debug(\"No summary directory set in \" + OPT_SUMMARY_REPORT_DIR);\r\n        return null;\r\n    }\r\n    LOG.debug(\"Summary directory set in to {}\" + OPT_SUMMARY_REPORT_DIR, reportDir);\r\n    report.snapshotIOStatistics(config.getIOStatistics());\r\n    Path reportDirPath = new Path(reportDir);\r\n    Path path = new Path(reportDirPath, createJobSummaryFilename(config.getJobUniqueId()));\r\n    if (thrown != null) {\r\n        report.recordJobFailure(thrown);\r\n    }\r\n    report.putDiagnostic(STAGE, activeStage);\r\n    final FileSystem fs = path.getFileSystem(conf);\r\n    try (ManifestStoreOperations operations = new ManifestStoreOperationsThroughFileSystem(fs)) {\r\n        if (!overwrite) {\r\n            try {\r\n                FileStatus st = operations.getFileStatus(path);\r\n                LOG.debug(\"Report already exists: {}\", st);\r\n                return null;\r\n            } catch (FileNotFoundException ignored) {\r\n            }\r\n        }\r\n        operations.save(report, path, overwrite);\r\n        LOG.info(\"Job summary saved to {}\", path);\r\n        return path;\r\n    } catch (IOException e) {\r\n        LOG.debug(\"Failed to save summary to {}\", path, e);\r\n        if (quiet) {\r\n            return null;\r\n        } else {\r\n            throw e;\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 2,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest",
  "methodName" : "getIOStatistics",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "IOStatisticsStore getIOStatistics()\n{\r\n    return iostatistics;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "getEventType",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "EventType getEventType()\n{\r\n    return EventType.JOB_QUEUE_CHANGED;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "getDatum",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Object getDatum()\n{\r\n    return datum;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "setDatum",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setDatum(Object datum)\n{\r\n    this.datum = (JobQueueChange) datum;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "getJobId",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "JobID getJobId()\n{\r\n    return JobID.forName(datum.jobid.toString());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "getJobQueueName",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String getJobQueueName()\n{\r\n    if (datum.jobQueueName != null) {\r\n        return datum.jobQueueName.toString();\r\n    }\r\n    return null;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "toTimelineEvent",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "TimelineEvent toTimelineEvent()\n{\r\n    TimelineEvent tEvent = new TimelineEvent();\r\n    tEvent.setId(StringUtils.toUpperCase(getEventType().name()));\r\n    tEvent.addInfo(\"QUEUE_NAMES\", getJobQueueName());\r\n    return tEvent;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "getTimelineMetrics",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Set<TimelineMetric> getTimelineMetrics()\n{\r\n    return null;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\task\\reduce",
  "methodName" : "getMerger",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "MergeManagerImpl<K, V> getMerger()\n{\r\n    return merger;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\task\\reduce",
  "methodName" : "doShuffle",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void doShuffle(MapHost host, IFileInputStream iFileInputStream, long compressedLength, long decompressedLength, ShuffleClientMetrics metrics, Reporter reporter) throws IOException",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\task\\reduce",
  "methodName" : "shuffle",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void shuffle(MapHost host, InputStream input, long compressedLength, long decompressedLength, ShuffleClientMetrics metrics, Reporter reporter) throws IOException\n{\r\n    doShuffle(host, new IFileInputStream(input, compressedLength, conf), compressedLength, decompressedLength, metrics, reporter);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "getDatum",
  "errType" : null,
  "containingMethodsNum" : 9,
  "sourceCodeText" : "Object getDatum()\n{\r\n    if (datum == null) {\r\n        datum = new TaskAttemptFinished();\r\n        datum.setTaskid(new Utf8(attemptId.getTaskID().toString()));\r\n        datum.setAttemptId(new Utf8(attemptId.toString()));\r\n        datum.setTaskType(new Utf8(taskType.name()));\r\n        datum.setTaskStatus(new Utf8(taskStatus));\r\n        datum.setFinishTime(finishTime);\r\n        if (rackName != null) {\r\n            datum.setRackname(new Utf8(rackName));\r\n        }\r\n        datum.setHostname(new Utf8(hostname));\r\n        datum.setState(new Utf8(state));\r\n        datum.setCounters(EventWriter.toAvro(counters));\r\n    }\r\n    return datum;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "setDatum",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void setDatum(Object oDatum)\n{\r\n    this.datum = (TaskAttemptFinished) oDatum;\r\n    this.attemptId = TaskAttemptID.forName(datum.getAttemptId().toString());\r\n    this.taskType = TaskType.valueOf(datum.getTaskType().toString());\r\n    this.taskStatus = datum.getTaskStatus().toString();\r\n    this.finishTime = datum.getFinishTime();\r\n    this.rackName = datum.getRackname().toString();\r\n    this.hostname = datum.getHostname().toString();\r\n    this.state = datum.getState().toString();\r\n    this.counters = EventReader.fromAvro(datum.getCounters());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "getTaskId",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "TaskID getTaskId()\n{\r\n    return attemptId.getTaskID();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "getAttemptId",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "TaskAttemptID getAttemptId()\n{\r\n    return attemptId;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "getTaskType",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "TaskType getTaskType()\n{\r\n    return taskType;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "getTaskStatus",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String getTaskStatus()\n{\r\n    return taskStatus.toString();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "getFinishTime",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getFinishTime()\n{\r\n    return finishTime;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "getStartTime",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getStartTime()\n{\r\n    return startTime;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "getHostname",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String getHostname()\n{\r\n    return hostname.toString();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "getRackName",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String getRackName()\n{\r\n    return rackName == null ? null : rackName.toString();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "getState",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String getState()\n{\r\n    return state.toString();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "getCounters",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Counters getCounters()\n{\r\n    return counters;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "getEventType",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "EventType getEventType()\n{\r\n    return getTaskId().getTaskType() == TaskType.MAP ? EventType.MAP_ATTEMPT_FINISHED : EventType.REDUCE_ATTEMPT_FINISHED;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "toTimelineEvent",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "TimelineEvent toTimelineEvent()\n{\r\n    TimelineEvent tEvent = new TimelineEvent();\r\n    tEvent.setId(StringUtils.toUpperCase(getEventType().name()));\r\n    tEvent.addInfo(\"TASK_TYPE\", getTaskType().toString());\r\n    tEvent.addInfo(\"ATTEMPT_ID\", getAttemptId() == null ? \"\" : getAttemptId().toString());\r\n    tEvent.addInfo(\"FINISH_TIME\", getFinishTime());\r\n    tEvent.addInfo(\"STATUS\", getTaskStatus());\r\n    tEvent.addInfo(\"STATE\", getState());\r\n    tEvent.addInfo(\"HOSTNAME\", getHostname());\r\n    return tEvent;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "getTimelineMetrics",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Set<TimelineMetric> getTimelineMetrics()\n{\r\n    Set<TimelineMetric> metrics = JobHistoryEventUtils.countersToTimelineMetric(getCounters(), finishTime);\r\n    return metrics;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\input",
  "methodName" : "close",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void close() throws IOException\n{\r\n    originalRR.close();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\input",
  "methodName" : "getCurrentKey",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "K getCurrentKey() throws IOException, InterruptedException\n{\r\n    return originalRR.getCurrentKey();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\input",
  "methodName" : "getCurrentValue",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "V getCurrentValue() throws IOException, InterruptedException\n{\r\n    return originalRR.getCurrentValue();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\input",
  "methodName" : "getProgress",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "float getProgress() throws IOException, InterruptedException\n{\r\n    return originalRR.getProgress();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\input",
  "methodName" : "initialize",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void initialize(InputSplit split, TaskAttemptContext context) throws IOException, InterruptedException\n{\r\n    originalRR.initialize(((TaggedInputSplit) split).getInputSplit(), context);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\input",
  "methodName" : "nextKeyValue",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "boolean nextKeyValue() throws IOException, InterruptedException\n{\r\n    return originalRR.nextKeyValue();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "getDatum",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Object getDatum()\n{\r\n    return datum;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "setDatum",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setDatum(Object datum)\n{\r\n    this.datum = (JobInited) datum;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "getJobId",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "JobID getJobId()\n{\r\n    return JobID.forName(datum.getJobid().toString());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "getLaunchTime",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "long getLaunchTime()\n{\r\n    return datum.getLaunchTime();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "getTotalMaps",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "int getTotalMaps()\n{\r\n    return datum.getTotalMaps();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "getTotalReduces",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "int getTotalReduces()\n{\r\n    return datum.getTotalReduces();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "getStatus",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String getStatus()\n{\r\n    return datum.getJobStatus().toString();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "getEventType",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "EventType getEventType()\n{\r\n    return EventType.JOB_INITED;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "getUberized",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "boolean getUberized()\n{\r\n    return datum.getUberized();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "toTimelineEvent",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "TimelineEvent toTimelineEvent()\n{\r\n    TimelineEvent tEvent = new TimelineEvent();\r\n    tEvent.setId(StringUtils.toUpperCase(getEventType().name()));\r\n    tEvent.addInfo(\"START_TIME\", getLaunchTime());\r\n    tEvent.addInfo(\"STATUS\", getStatus());\r\n    tEvent.addInfo(\"TOTAL_MAPS\", getTotalMaps());\r\n    tEvent.addInfo(\"TOTAL_REDUCES\", getTotalReduces());\r\n    tEvent.addInfo(\"UBERIZED\", getUberized());\r\n    return tEvent;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "getTimelineMetrics",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Set<TimelineMetric> getTimelineMetrics()\n{\r\n    return null;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\join",
  "methodName" : "reduce",
  "errType" : null,
  "containingMethodsNum" : 9,
  "sourceCodeText" : "Token reduce(Stack<Token> st, Configuration conf) throws IOException\n{\r\n    LinkedList<Token> args = new LinkedList<Token>();\r\n    while (!st.isEmpty() && !TType.LPAREN.equals(st.peek().getType())) {\r\n        args.addFirst(st.pop());\r\n    }\r\n    if (st.isEmpty()) {\r\n        throw new IOException(\"Unmatched ')'\");\r\n    }\r\n    st.pop();\r\n    if (st.isEmpty() || !TType.IDENT.equals(st.peek().getType())) {\r\n        throw new IOException(\"Identifier expected\");\r\n    }\r\n    Node n = Node.forIdent(st.pop().getStr());\r\n    n.parse(args, conf);\r\n    return new NodeToken(n);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\join",
  "methodName" : "parse",
  "errType" : null,
  "containingMethodsNum" : 9,
  "sourceCodeText" : "Node parse(String expr, Configuration conf) throws IOException\n{\r\n    if (null == expr) {\r\n        throw new IOException(\"Expression is null\");\r\n    }\r\n    Class<? extends WritableComparator> cmpcl = conf.getClass(CompositeInputFormat.JOIN_COMPARATOR, null, WritableComparator.class);\r\n    Lexer lex = new Lexer(expr);\r\n    Stack<Token> st = new Stack<Token>();\r\n    Token tok;\r\n    while ((tok = lex.next()) != null) {\r\n        if (TType.RPAREN.equals(tok.getType())) {\r\n            st.push(reduce(st, conf));\r\n        } else {\r\n            st.push(tok);\r\n        }\r\n    }\r\n    if (st.size() == 1 && TType.CIF.equals(st.peek().getType())) {\r\n        Node ret = st.pop().getNode();\r\n        if (cmpcl != null) {\r\n            ret.setKeyComparator(cmpcl);\r\n        }\r\n        return ret;\r\n    }\r\n    throw new IOException(\"Missing ')'\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\db",
  "methodName" : "getSplitter",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "DBSplitter getSplitter(int sqlDataType)\n{\r\n    switch(sqlDataType) {\r\n        case Types.NUMERIC:\r\n        case Types.DECIMAL:\r\n            return new BigDecimalSplitter();\r\n        case Types.BIT:\r\n        case Types.BOOLEAN:\r\n            return new BooleanSplitter();\r\n        case Types.INTEGER:\r\n        case Types.TINYINT:\r\n        case Types.SMALLINT:\r\n        case Types.BIGINT:\r\n            return new IntegerSplitter();\r\n        case Types.REAL:\r\n        case Types.FLOAT:\r\n        case Types.DOUBLE:\r\n            return new FloatSplitter();\r\n        case Types.CHAR:\r\n        case Types.VARCHAR:\r\n        case Types.LONGVARCHAR:\r\n            return new TextSplitter();\r\n        case Types.DATE:\r\n        case Types.TIME:\r\n        case Types.TIMESTAMP:\r\n            return new DateSplitter();\r\n        default:\r\n            return null;\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\db",
  "methodName" : "getSplits",
  "errType" : [ "SQLException", "SQLException", "SQLException", "SQLException" ],
  "containingMethodsNum" : 16,
  "sourceCodeText" : "List<InputSplit> getSplits(JobContext job) throws IOException\n{\r\n    int targetNumTasks = job.getConfiguration().getInt(MRJobConfig.NUM_MAPS, 1);\r\n    if (1 == targetNumTasks) {\r\n        List<InputSplit> singletonSplit = new ArrayList<InputSplit>();\r\n        singletonSplit.add(new DataDrivenDBInputSplit(\"1=1\", \"1=1\"));\r\n        return singletonSplit;\r\n    }\r\n    ResultSet results = null;\r\n    Statement statement = null;\r\n    try {\r\n        statement = connection.createStatement();\r\n        results = statement.executeQuery(getBoundingValsQuery());\r\n        results.next();\r\n        int sqlDataType = results.getMetaData().getColumnType(1);\r\n        DBSplitter splitter = getSplitter(sqlDataType);\r\n        if (null == splitter) {\r\n            throw new IOException(\"Unknown SQL data type: \" + sqlDataType);\r\n        }\r\n        return splitter.split(job.getConfiguration(), results, getDBConf().getInputOrderBy());\r\n    } catch (SQLException e) {\r\n        throw new IOException(e.getMessage());\r\n    } finally {\r\n        try {\r\n            if (null != results) {\r\n                results.close();\r\n            }\r\n        } catch (SQLException se) {\r\n            LOG.debug(\"SQLException closing resultset: \" + se.toString());\r\n        }\r\n        try {\r\n            if (null != statement) {\r\n                statement.close();\r\n            }\r\n        } catch (SQLException se) {\r\n            LOG.debug(\"SQLException closing statement: \" + se.toString());\r\n        }\r\n        try {\r\n            connection.commit();\r\n            closeConnection();\r\n        } catch (SQLException se) {\r\n            LOG.debug(\"SQLException committing split transaction: \" + se.toString());\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 4,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\db",
  "methodName" : "getBoundingValsQuery",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "String getBoundingValsQuery()\n{\r\n    String userQuery = getDBConf().getInputBoundingQuery();\r\n    if (null != userQuery) {\r\n        return userQuery;\r\n    }\r\n    StringBuilder query = new StringBuilder();\r\n    String splitCol = getDBConf().getInputOrderBy();\r\n    query.append(\"SELECT MIN(\").append(splitCol).append(\"), \");\r\n    query.append(\"MAX(\").append(splitCol).append(\") FROM \");\r\n    query.append(getDBConf().getInputTableName());\r\n    String conditions = getDBConf().getInputConditions();\r\n    if (null != conditions) {\r\n        query.append(\" WHERE ( \" + conditions + \" )\");\r\n    }\r\n    return query.toString();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\db",
  "methodName" : "setBoundingQuery",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void setBoundingQuery(Configuration conf, String query)\n{\r\n    if (null != query) {\r\n        if (query.indexOf(SUBSTITUTE_TOKEN) == -1) {\r\n            LOG.warn(\"Could not find \" + SUBSTITUTE_TOKEN + \" token in query: \" + query + \"; splits may not partition data.\");\r\n        }\r\n    }\r\n    conf.set(DBConfiguration.INPUT_BOUNDING_QUERY, query);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\db",
  "methodName" : "createDBRecordReader",
  "errType" : [ "SQLException" ],
  "containingMethodsNum" : 14,
  "sourceCodeText" : "RecordReader<LongWritable, T> createDBRecordReader(DBInputSplit split, Configuration conf) throws IOException\n{\r\n    DBConfiguration dbConf = getDBConf();\r\n    @SuppressWarnings(\"unchecked\")\r\n    Class<T> inputClass = (Class<T>) (dbConf.getInputClass());\r\n    String dbProductName = getDBProductName();\r\n    LOG.debug(\"Creating db record reader for db product: \" + dbProductName);\r\n    try {\r\n        if (dbProductName.startsWith(\"MYSQL\")) {\r\n            return new MySQLDataDrivenDBRecordReader<T>(split, inputClass, conf, createConnection(), dbConf, dbConf.getInputConditions(), dbConf.getInputFieldNames(), dbConf.getInputTableName());\r\n        } else {\r\n            return new DataDrivenDBRecordReader<T>(split, inputClass, conf, createConnection(), dbConf, dbConf.getInputConditions(), dbConf.getInputFieldNames(), dbConf.getInputTableName(), dbProductName);\r\n        }\r\n    } catch (SQLException ex) {\r\n        throw new IOException(ex.getMessage());\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\db",
  "methodName" : "setInput",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void setInput(Job job, Class<? extends DBWritable> inputClass, String tableName, String conditions, String splitBy, String... fieldNames)\n{\r\n    DBInputFormat.setInput(job, inputClass, tableName, conditions, splitBy, fieldNames);\r\n    job.setInputFormatClass(DataDrivenDBInputFormat.class);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\db",
  "methodName" : "setInput",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void setInput(Job job, Class<? extends DBWritable> inputClass, String inputQuery, String inputBoundingQuery)\n{\r\n    DBInputFormat.setInput(job, inputClass, inputQuery, \"\");\r\n    job.getConfiguration().set(DBConfiguration.INPUT_BOUNDING_QUERY, inputBoundingQuery);\r\n    job.setInputFormatClass(DataDrivenDBInputFormat.class);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred\\join",
  "methodName" : "add",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void add(InputSplit s) throws IOException\n{\r\n    if (null == splits) {\r\n        throw new IOException(\"Uninitialized InputSplit\");\r\n    }\r\n    if (fill == splits.length) {\r\n        throw new IOException(\"Too many splits\");\r\n    }\r\n    splits[fill++] = s;\r\n    totsize += s.getLength();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred\\join",
  "methodName" : "get",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "InputSplit get(int i)\n{\r\n    return splits[i];\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred\\join",
  "methodName" : "getLength",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getLength() throws IOException\n{\r\n    return totsize;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred\\join",
  "methodName" : "getLength",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "long getLength(int i) throws IOException\n{\r\n    return splits[i].getLength();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred\\join",
  "methodName" : "getLocations",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "String[] getLocations() throws IOException\n{\r\n    HashSet<String> hosts = new HashSet<String>();\r\n    for (InputSplit s : splits) {\r\n        String[] hints = s.getLocations();\r\n        if (hints != null && hints.length > 0) {\r\n            for (String host : hints) {\r\n                hosts.add(host);\r\n            }\r\n        }\r\n    }\r\n    return hosts.toArray(new String[hosts.size()]);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred\\join",
  "methodName" : "getLocation",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String[] getLocation(int i) throws IOException\n{\r\n    return splits[i].getLocations();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred\\join",
  "methodName" : "write",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void write(DataOutput out) throws IOException\n{\r\n    WritableUtils.writeVInt(out, splits.length);\r\n    for (InputSplit s : splits) {\r\n        Text.writeString(out, s.getClass().getName());\r\n    }\r\n    for (InputSplit s : splits) {\r\n        s.write(out);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred\\join",
  "methodName" : "readFields",
  "errType" : [ "ClassNotFoundException" ],
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void readFields(DataInput in) throws IOException\n{\r\n    int card = WritableUtils.readVInt(in);\r\n    if (splits == null || splits.length != card) {\r\n        splits = new InputSplit[card];\r\n    }\r\n    Class<? extends InputSplit>[] cls = new Class[card];\r\n    try {\r\n        for (int i = 0; i < card; ++i) {\r\n            cls[i] = Class.forName(Text.readString(in)).asSubclass(InputSplit.class);\r\n        }\r\n        for (int i = 0; i < card; ++i) {\r\n            splits[i] = ReflectionUtils.newInstance(cls[i], null);\r\n            splits[i].readFields(in);\r\n        }\r\n    } catch (ClassNotFoundException e) {\r\n        throw (IOException) new IOException(\"Failed split init\").initCause(e);\r\n    }\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred\\lib\\db",
  "methodName" : "checkOutputSpecs",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void checkOutputSpecs(FileSystem filesystem, JobConf job) throws IOException\n{\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred\\lib\\db",
  "methodName" : "getRecordWriter",
  "errType" : [ "SQLException" ],
  "containingMethodsNum" : 3,
  "sourceCodeText" : "RecordWriter<K, V> getRecordWriter(FileSystem filesystem, JobConf job, String name, Progressable progress) throws IOException\n{\r\n    org.apache.hadoop.mapreduce.RecordWriter<K, V> w = super.getRecordWriter(new TaskAttemptContextImpl(job, TaskAttemptID.forName(job.get(MRJobConfig.TASK_ATTEMPT_ID))));\r\n    org.apache.hadoop.mapreduce.lib.db.DBOutputFormat.DBRecordWriter writer = (org.apache.hadoop.mapreduce.lib.db.DBOutputFormat.DBRecordWriter) w;\r\n    try {\r\n        return new DBRecordWriter(writer.getConnection(), writer.getStatement());\r\n    } catch (SQLException se) {\r\n        throw new IOException(se);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred\\lib\\db",
  "methodName" : "setOutput",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void setOutput(JobConf job, String tableName, String... fieldNames)\n{\r\n    if (fieldNames.length > 0 && fieldNames[0] != null) {\r\n        DBConfiguration dbConf = setOutput(job, tableName);\r\n        dbConf.setOutputFieldNames(fieldNames);\r\n    } else {\r\n        if (fieldNames.length > 0)\r\n            setOutput(job, tableName, fieldNames.length);\r\n        else\r\n            throw new IllegalArgumentException(\"Field names must be greater than 0\");\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred\\lib\\db",
  "methodName" : "setOutput",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void setOutput(JobConf job, String tableName, int fieldCount)\n{\r\n    DBConfiguration dbConf = setOutput(job, tableName);\r\n    dbConf.setOutputFieldCount(fieldCount);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred\\lib\\db",
  "methodName" : "setOutput",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "DBConfiguration setOutput(JobConf job, String tableName)\n{\r\n    job.setOutputFormat(DBOutputFormat.class);\r\n    job.setReduceSpeculativeExecution(false);\r\n    DBConfiguration dbConf = new DBConfiguration(job);\r\n    dbConf.setOutputTableName(tableName);\r\n    return dbConf;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "setSequenceFileOutputKeyClass",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setSequenceFileOutputKeyClass(JobConf conf, Class<?> theClass)\n{\r\n    conf.setClass(org.apache.hadoop.mapreduce.lib.output.SequenceFileAsBinaryOutputFormat.KEY_CLASS, theClass, Object.class);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "setSequenceFileOutputValueClass",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setSequenceFileOutputValueClass(JobConf conf, Class<?> theClass)\n{\r\n    conf.setClass(org.apache.hadoop.mapreduce.lib.output.SequenceFileAsBinaryOutputFormat.VALUE_CLASS, theClass, Object.class);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getSequenceFileOutputKeyClass",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Class<? extends WritableComparable> getSequenceFileOutputKeyClass(JobConf conf)\n{\r\n    return conf.getClass(org.apache.hadoop.mapreduce.lib.output.SequenceFileAsBinaryOutputFormat.KEY_CLASS, conf.getOutputKeyClass().asSubclass(WritableComparable.class), WritableComparable.class);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getSequenceFileOutputValueClass",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Class<? extends Writable> getSequenceFileOutputValueClass(JobConf conf)\n{\r\n    return conf.getClass(org.apache.hadoop.mapreduce.lib.output.SequenceFileAsBinaryOutputFormat.VALUE_CLASS, conf.getOutputValueClass().asSubclass(Writable.class), Writable.class);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getRecordWriter",
  "errType" : null,
  "containingMethodsNum" : 11,
  "sourceCodeText" : "RecordWriter<BytesWritable, BytesWritable> getRecordWriter(FileSystem ignored, JobConf job, String name, Progressable progress) throws IOException\n{\r\n    Path file = FileOutputFormat.getTaskOutputPath(job, name);\r\n    FileSystem fs = file.getFileSystem(job);\r\n    CompressionCodec codec = null;\r\n    CompressionType compressionType = CompressionType.NONE;\r\n    if (getCompressOutput(job)) {\r\n        compressionType = getOutputCompressionType(job);\r\n        Class<? extends CompressionCodec> codecClass = getOutputCompressorClass(job, DefaultCodec.class);\r\n        codec = ReflectionUtils.newInstance(codecClass, job);\r\n    }\r\n    final SequenceFile.Writer out = SequenceFile.createWriter(fs, job, file, getSequenceFileOutputKeyClass(job), getSequenceFileOutputValueClass(job), compressionType, codec, progress);\r\n    return new RecordWriter<BytesWritable, BytesWritable>() {\r\n\r\n        private WritableValueBytes wvaluebytes = new WritableValueBytes();\r\n\r\n        public void write(BytesWritable bkey, BytesWritable bvalue) throws IOException {\r\n            wvaluebytes.reset(bvalue);\r\n            out.appendRaw(bkey.getBytes(), 0, bkey.getLength(), wvaluebytes);\r\n            wvaluebytes.reset(null);\r\n        }\r\n\r\n        public void close(Reporter reporter) throws IOException {\r\n            out.close();\r\n        }\r\n    };\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "checkOutputSpecs",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void checkOutputSpecs(FileSystem ignored, JobConf job) throws IOException\n{\r\n    super.checkOutputSpecs(ignored, job);\r\n    if (getCompressOutput(job) && getOutputCompressionType(job) == CompressionType.RECORD) {\r\n        throw new InvalidJobConfException(\"SequenceFileAsBinaryOutputFormat \" + \"doesn't support Record Compression\");\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\db",
  "methodName" : "dateToString",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String dateToString(Date d)\n{\r\n    return \"TO_TIMESTAMP('\" + d.toString() + \"', 'YYYY-MM-DD HH24:MI:SS.FF')\";\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output",
  "methodName" : "getRecordWriter",
  "errType" : null,
  "containingMethodsNum" : 10,
  "sourceCodeText" : "RecordWriter<K, V> getRecordWriter(TaskAttemptContext job) throws IOException, InterruptedException\n{\r\n    Configuration conf = job.getConfiguration();\r\n    boolean isCompressed = getCompressOutput(job);\r\n    String keyValueSeparator = conf.get(SEPARATOR, \"\\t\");\r\n    CompressionCodec codec = null;\r\n    String extension = \"\";\r\n    if (isCompressed) {\r\n        Class<? extends CompressionCodec> codecClass = getOutputCompressorClass(job, GzipCodec.class);\r\n        codec = ReflectionUtils.newInstance(codecClass, conf);\r\n        extension = codec.getDefaultExtension();\r\n    }\r\n    Path file = getDefaultWorkFile(job, extension);\r\n    FileSystem fs = file.getFileSystem(conf);\r\n    FSDataOutputStream fileOut = fs.create(file, false);\r\n    if (isCompressed) {\r\n        return new LineRecordWriter<>(new DataOutputStream(codec.createOutputStream(fileOut)), keyValueSeparator);\r\n    } else {\r\n        return new LineRecordWriter<>(fileOut, keyValueSeparator);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "readFields",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void readFields(DataInput in) throws IOException\n{\r\n    jvmId.readFields(in);\r\n    this.pid = Text.readString(in);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "write",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void write(DataOutput out) throws IOException\n{\r\n    jvmId.write(out);\r\n    Text.writeString(out, pid);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "print",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void print() throws IOException\n{\r\n    print(System.out);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "print",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void print(PrintStream ps) throws IOException\n{\r\n    jhvp.print(ps);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "getTaskLogsUrl",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "String getTaskLogsUrl(String scheme, JobHistoryParser.TaskAttemptInfo attempt)\n{\r\n    if (attempt.getHttpPort() == -1 || attempt.getTrackerName().equals(\"\") || attempt.getAttemptId() == null) {\r\n        return null;\r\n    }\r\n    String taskTrackerName = HostUtil.convertTrackerNameToHostName(attempt.getTrackerName());\r\n    return HostUtil.getTaskLogUrl(scheme, taskTrackerName, Integer.toString(attempt.getHttpPort()), attempt.getAttemptId().toString());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest\\stages",
  "methodName" : "executeStage",
  "errType" : [ "IOException|IllegalArgumentException" ],
  "containingMethodsNum" : 31,
  "sourceCodeText" : "CommitJobStage.Result executeStage(final CommitJobStage.Arguments arguments) throws IOException\n{\r\n    LOG.info(\"{}: Committing job \\\"{}\\\". resilient commit supported = {}\", getName(), getJobId(), storeSupportsResilientCommit());\r\n    boolean createMarker = arguments.isCreateMarker();\r\n    final StageConfig stageConfig = getStageConfig();\r\n    LoadManifestsStage.Result result = new LoadManifestsStage(stageConfig).apply(true);\r\n    List<TaskManifest> manifests = result.getManifests();\r\n    LoadManifestsStage.SummaryInfo summary = result.getSummary();\r\n    LOG.debug(\"{}: Job Summary {}\", getName(), summary);\r\n    LOG.info(\"{}: Committing job with file count: {}; total size {} bytes\", getName(), summary.getFileCount(), byteCountToDisplaySize(summary.getTotalFileSize()));\r\n    IOStatisticsStore iostats = getIOStatistics();\r\n    iostats.aggregate(summary.getIOStatistics());\r\n    final CreateOutputDirectoriesStage.Result dirStageResults = new CreateOutputDirectoriesStage(stageConfig).apply(manifests);\r\n    ManifestSuccessData successData;\r\n    successData = new RenameFilesStage(stageConfig).apply(Pair.of(manifests, dirStageResults.getCreatedDirectories()));\r\n    if (LOG.isDebugEnabled()) {\r\n        LOG.debug(\"{}: _SUCCESS file summary {}\", getName(), successData.toJson());\r\n    }\r\n    iostats.setCounter(COMMITTER_FILES_COMMITTED_COUNT, summary.getFileCount());\r\n    iostats.setCounter(COMMITTER_BYTES_COMMITTED_COUNT, summary.getTotalFileSize());\r\n    successData.snapshotIOStatistics(iostats);\r\n    final String manifestRenameDir = arguments.getManifestRenameDir();\r\n    if (isNotBlank(manifestRenameDir)) {\r\n        Path manifestRenamePath = new Path(new Path(manifestRenameDir), getJobId());\r\n        LOG.info(\"{}: Renaming manifests to {}\", getName(), manifestRenamePath);\r\n        try {\r\n            renameDir(getTaskManifestDir(), manifestRenamePath);\r\n            successData.getDiagnostics().put(MANIFESTS, manifestRenamePath.toUri().toString());\r\n        } catch (IOException | IllegalArgumentException e) {\r\n            LOG.warn(\"{}: Failed to rename manifests to {}\", getName(), manifestRenamePath, e);\r\n        }\r\n    }\r\n    Path successPath = null;\r\n    if (createMarker) {\r\n        successPath = new SaveSuccessFileStage(stageConfig).apply(successData);\r\n        LOG.debug(\"{}: Saving _SUCCESS file to {}\", getName(), successPath);\r\n    }\r\n    new CleanupJobStage(stageConfig).apply(arguments.getCleanupArguments());\r\n    if (arguments.isValidateOutput()) {\r\n        LOG.info(\"{}: Validating output.\", getName());\r\n        new ValidateRenamedFilesStage(stageConfig).apply(result.getManifests());\r\n    }\r\n    stageConfig.enterStage(getStageName(arguments));\r\n    return new CommitJobStage.Result(successPath, successData);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "configure",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void configure(JobConf job)\n{\r\n    this.mapper = ReflectionUtils.newInstance(job.getMapperClass(), job);\r\n    this.incrProcCount = SkipBadRecords.getMapperMaxSkipRecords(job) > 0 && SkipBadRecords.getAutoIncrMapperProcCount(job);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "run",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void run(RecordReader<K1, V1> input, OutputCollector<K2, V2> output, Reporter reporter) throws IOException\n{\r\n    try {\r\n        K1 key = input.createKey();\r\n        V1 value = input.createValue();\r\n        while (input.next(key, value)) {\r\n            mapper.map(key, value, output, reporter);\r\n            if (incrProcCount) {\r\n                reporter.incrCounter(SkipBadRecords.COUNTER_GROUP, SkipBadRecords.COUNTER_MAP_PROCESSED_RECORDS, 1);\r\n            }\r\n        }\r\n    } finally {\r\n        mapper.close();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getMapper",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Mapper<K1, V1, K2, V2> getMapper()\n{\r\n    return mapper;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getKeyClass",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Class getKeyClass()\n{\r\n    return Text.class;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "createKey",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Text createKey()\n{\r\n    return new Text();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "createValue",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Text createValue()\n{\r\n    return new Text();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "findSeparator",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "int findSeparator(byte[] utf, int start, int length, byte sep)\n{\r\n    return org.apache.hadoop.mapreduce.lib.input.KeyValueLineRecordReader.findSeparator(utf, start, length, sep);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "next",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "boolean next(Text key, Text value) throws IOException\n{\r\n    byte[] line = null;\r\n    int lineLen = -1;\r\n    if (lineRecordReader.next(dummyKey, innerValue)) {\r\n        line = innerValue.getBytes();\r\n        lineLen = innerValue.getLength();\r\n    } else {\r\n        return false;\r\n    }\r\n    if (line == null)\r\n        return false;\r\n    int pos = findSeparator(line, 0, lineLen, this.separator);\r\n    org.apache.hadoop.mapreduce.lib.input.KeyValueLineRecordReader.setKeyValue(key, value, line, lineLen, pos);\r\n    return true;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getProgress",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "float getProgress() throws IOException\n{\r\n    return lineRecordReader.getProgress();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getPos",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "long getPos() throws IOException\n{\r\n    return lineRecordReader.getPos();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "close",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void close() throws IOException\n{\r\n    lineRecordReader.close();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\join",
  "methodName" : "hasNext",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "boolean hasNext()\n{\r\n    return iter.hasNext();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\join",
  "methodName" : "next",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "boolean next(X val) throws IOException\n{\r\n    if (iter.hasNext()) {\r\n        ReflectionUtils.copy(conf, iter.next(), val);\r\n        if (null == hold) {\r\n            hold = WritableUtils.clone(val, null);\r\n        } else {\r\n            ReflectionUtils.copy(conf, val, hold);\r\n        }\r\n        return true;\r\n    }\r\n    return false;\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\join",
  "methodName" : "replay",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "boolean replay(X val) throws IOException\n{\r\n    ReflectionUtils.copy(conf, hold, val);\r\n    return true;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\join",
  "methodName" : "reset",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void reset()\n{\r\n    iter = data.iterator();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\join",
  "methodName" : "add",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void add(X item) throws IOException\n{\r\n    data.add(WritableUtils.clone(item, null));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\join",
  "methodName" : "close",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void close() throws IOException\n{\r\n    iter = null;\r\n    data = null;\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\join",
  "methodName" : "clear",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void clear()\n{\r\n    data.clear();\r\n    reset();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest",
  "methodName" : "createOutputCommitter",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "ManifestCommitter createOutputCommitter(final Path outputPath, final TaskAttemptContext context) throws IOException\n{\r\n    final String scheme = outputPath.toUri().getScheme();\r\n    if (UNSUPPORTED_FS_SCHEMAS.contains(scheme)) {\r\n        throw new PathIOException(outputPath.toString(), \"This committer does not work with the filesystem of type \" + scheme);\r\n    }\r\n    return new ManifestCommitter(outputPath, context);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "getDatum",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Object getDatum()\n{\r\n    return datum;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "setDatum",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setDatum(Object datum)\n{\r\n    this.datum = (TaskStarted) datum;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "getTaskId",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "TaskID getTaskId()\n{\r\n    return TaskID.forName(datum.getTaskid().toString());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "getSplitLocations",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String getSplitLocations()\n{\r\n    return datum.getSplitLocations().toString();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "getStartTime",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "long getStartTime()\n{\r\n    return datum.getStartTime();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "getTaskType",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "TaskType getTaskType()\n{\r\n    return TaskType.valueOf(datum.getTaskType().toString());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "getEventType",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "EventType getEventType()\n{\r\n    return EventType.TASK_STARTED;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "toTimelineEvent",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "TimelineEvent toTimelineEvent()\n{\r\n    TimelineEvent tEvent = new TimelineEvent();\r\n    tEvent.setId(StringUtils.toUpperCase(getEventType().name()));\r\n    tEvent.addInfo(\"TASK_TYPE\", getTaskType().toString());\r\n    tEvent.addInfo(\"START_TIME\", getStartTime());\r\n    tEvent.addInfo(\"SPLIT_LOCATIONS\", getSplitLocations());\r\n    return tEvent;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "getTimelineMetrics",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Set<TimelineMetric> getTimelineMetrics()\n{\r\n    return null;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\input",
  "methodName" : "createRecordReader",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "RecordReader<LongWritable, Text> createRecordReader(InputSplit split, TaskAttemptContext context) throws IOException\n{\r\n    return new CombineFileRecordReader<LongWritable, Text>((CombineFileSplit) split, context, TextRecordReaderWrapper.class);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\task\\reduce",
  "methodName" : "create",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "ShuffleClientMetrics create(TaskAttemptID reduceId, JobConf jobConf)\n{\r\n    MetricsSystem ms = DefaultMetricsSystem.initialize(\"JobTracker\");\r\n    ShuffleClientMetrics shuffleClientMetrics = new ShuffleClientMetrics();\r\n    shuffleClientMetrics.addTags(reduceId, jobConf);\r\n    return ms.register(\"ShuffleClientMetrics-\" + ThreadLocalRandom.current().nextInt(), null, shuffleClientMetrics);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\task\\reduce",
  "methodName" : "inputBytes",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void inputBytes(long bytes)\n{\r\n    numBytes.incr(bytes);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\task\\reduce",
  "methodName" : "failedFetch",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void failedFetch()\n{\r\n    numFailedFetches.incr();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\task\\reduce",
  "methodName" : "successFetch",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void successFetch()\n{\r\n    numSuccessFetches.incr();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\task\\reduce",
  "methodName" : "threadBusy",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void threadBusy()\n{\r\n    numThreadsBusy.incr();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\task\\reduce",
  "methodName" : "threadFree",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void threadFree()\n{\r\n    numThreadsBusy.decr();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\task\\reduce",
  "methodName" : "addTags",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void addTags(TaskAttemptID reduceId, JobConf jobConf)\n{\r\n    metricsRegistry.tag(\"user\", \"\", jobConf.getUser()).tag(\"jobName\", \"\", jobConf.getJobName()).tag(\"jobId\", \"\", reduceId.getJobID().toString()).tag(\"taskId\", \"\", reduceId.toString());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\task\\reduce",
  "methodName" : "getMetricsRegistry",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "MetricsRegistry getMetricsRegistry()\n{\r\n    return metricsRegistry;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred\\join",
  "methodName" : "next",
  "errType" : null,
  "containingMethodsNum" : 11,
  "sourceCodeText" : "boolean next(K key, TupleWritable value) throws IOException\n{\r\n    if (jc.flush(value)) {\r\n        WritableUtils.cloneInto(key, jc.key());\r\n        return true;\r\n    }\r\n    jc.clear();\r\n    K iterkey = createKey();\r\n    final PriorityQueue<ComposableRecordReader<K, ?>> q = getRecordReaderQueue();\r\n    while (!q.isEmpty()) {\r\n        fillJoinCollector(iterkey);\r\n        jc.reset(iterkey);\r\n        if (jc.flush(value)) {\r\n            WritableUtils.cloneInto(key, jc.key());\r\n            return true;\r\n        }\r\n        jc.clear();\r\n    }\r\n    return false;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred\\join",
  "methodName" : "createValue",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "TupleWritable createValue()\n{\r\n    return createInternalValue();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred\\join",
  "methodName" : "getDelegate",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "ResetableIterator<TupleWritable> getDelegate()\n{\r\n    return new JoinDelegationIterator();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\util",
  "methodName" : "countersToJSON",
  "errType" : null,
  "containingMethodsNum" : 9,
  "sourceCodeText" : "JsonNode countersToJSON(Counters counters)\n{\r\n    ObjectMapper mapper = new ObjectMapper();\r\n    ArrayNode nodes = mapper.createArrayNode();\r\n    if (counters != null) {\r\n        for (CounterGroup counterGroup : counters) {\r\n            ObjectNode groupNode = nodes.addObject();\r\n            groupNode.put(\"NAME\", counterGroup.getName());\r\n            groupNode.put(\"DISPLAY_NAME\", counterGroup.getDisplayName());\r\n            ArrayNode countersNode = groupNode.putArray(\"COUNTERS\");\r\n            for (Counter counter : counterGroup) {\r\n                ObjectNode counterNode = countersNode.addObject();\r\n                counterNode.put(\"NAME\", counter.getName());\r\n                counterNode.put(\"DISPLAY_NAME\", counter.getDisplayName());\r\n                counterNode.put(\"VALUE\", counter.getValue());\r\n            }\r\n        }\r\n    }\r\n    return nodes;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\util",
  "methodName" : "countersToTimelineMetric",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Set<TimelineMetric> countersToTimelineMetric(Counters counters, long timestamp)\n{\r\n    return countersToTimelineMetric(counters, timestamp, \"\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\util",
  "methodName" : "countersToTimelineMetric",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "Set<TimelineMetric> countersToTimelineMetric(Counters counters, long timestamp, String groupNamePrefix)\n{\r\n    Set<TimelineMetric> entityMetrics = new HashSet<TimelineMetric>();\r\n    for (CounterGroup g : counters) {\r\n        String groupName = g.getName();\r\n        for (Counter c : g) {\r\n            String name = groupNamePrefix + groupName + \":\" + c.getName();\r\n            TimelineMetric metric = new TimelineMetric();\r\n            metric.setId(name);\r\n            metric.addValue(timestamp, c.getValue());\r\n            entityMetrics.add(metric);\r\n        }\r\n    }\r\n    return entityMetrics;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getMaxStringSize",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int getMaxStringSize()\n{\r\n    return MAX_STRING_SIZE;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getTaskID",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "TaskAttemptID getTaskID()\n{\r\n    return taskid;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getIsMap",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean getIsMap()",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getNumSlots",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int getNumSlots()\n{\r\n    return numSlots;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getProgress",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "float getProgress()\n{\r\n    return progress;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "setProgress",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setProgress(float progress)\n{\r\n    this.progress = progress;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getRunState",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "State getRunState()\n{\r\n    return runState;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getTaskTracker",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getTaskTracker()\n{\r\n    return taskTracker;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "setTaskTracker",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setTaskTracker(String tracker)\n{\r\n    this.taskTracker = tracker;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "setRunState",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setRunState(State runState)\n{\r\n    this.runState = runState;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getDiagnosticInfo",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getDiagnosticInfo()\n{\r\n    return diagnosticInfo;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "setDiagnosticInfo",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void setDiagnosticInfo(String info)\n{\r\n    if (diagnosticInfo != null && diagnosticInfo.length() == getMaxStringSize()) {\r\n        LOG.info(\"task-diagnostic-info for task \" + taskid + \" : \" + info);\r\n        return;\r\n    }\r\n    diagnosticInfo = ((diagnosticInfo == null) ? info : diagnosticInfo.concat(info));\r\n    if (diagnosticInfo != null && diagnosticInfo.length() > getMaxStringSize()) {\r\n        LOG.info(\"task-diagnostic-info for task \" + taskid + \" : \" + diagnosticInfo);\r\n        diagnosticInfo = diagnosticInfo.substring(0, getMaxStringSize());\r\n    }\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getStateString",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getStateString()\n{\r\n    return stateString;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "setStateString",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void setStateString(String stateString)\n{\r\n    if (stateString != null) {\r\n        if (stateString.length() <= getMaxStringSize()) {\r\n            this.stateString = stateString;\r\n        } else {\r\n            LOG.info(\"state-string for task \" + taskid + \" : \" + stateString);\r\n            this.stateString = stateString.substring(0, getMaxStringSize());\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getNextRecordRange",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "SortedRanges.Range getNextRecordRange()\n{\r\n    return nextRecordRange;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "setNextRecordRange",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setNextRecordRange(SortedRanges.Range nextRecordRange)\n{\r\n    this.nextRecordRange = nextRecordRange;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getFinishTime",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getFinishTime()\n{\r\n    return finishTime;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "setFinishTime",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void setFinishTime(long finishTime)\n{\r\n    if (this.getStartTime() > 0 && finishTime > 0) {\r\n        this.finishTime = finishTime;\r\n    } else {\r\n        LOG.error(\"Trying to set finish time for task \" + taskid + \" when no start time is set, stackTrace is : \" + StringUtils.stringifyException(new Exception()));\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getShuffleFinishTime",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getShuffleFinishTime()\n{\r\n    return 0;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "setShuffleFinishTime",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setShuffleFinishTime(long shuffleFinishTime)\n{\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getMapFinishTime",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getMapFinishTime()\n{\r\n    return 0;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "setMapFinishTime",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setMapFinishTime(long mapFinishTime)\n{\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getSortFinishTime",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getSortFinishTime()\n{\r\n    return 0;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "setSortFinishTime",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setSortFinishTime(long sortFinishTime)\n{\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getStartTime",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getStartTime()\n{\r\n    return startTime;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "setStartTime",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setStartTime(long startTime)\n{\r\n    if (startTime > 0) {\r\n        this.startTime = startTime;\r\n    } else {\r\n        LOG.error(\"Trying to set illegal startTime for task : \" + taskid + \".Stack trace is : \" + StringUtils.stringifyException(new Exception()));\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getPhase",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Phase getPhase()\n{\r\n    return this.phase;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "setPhase",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void setPhase(Phase phase)\n{\r\n    TaskStatus.Phase oldPhase = getPhase();\r\n    if (oldPhase != phase) {\r\n        if (phase == TaskStatus.Phase.SORT) {\r\n            if (oldPhase == TaskStatus.Phase.MAP) {\r\n                setMapFinishTime(System.currentTimeMillis());\r\n            } else {\r\n                setShuffleFinishTime(System.currentTimeMillis());\r\n            }\r\n        } else if (phase == TaskStatus.Phase.REDUCE) {\r\n            setSortFinishTime(System.currentTimeMillis());\r\n        }\r\n        this.phase = phase;\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "inTaskCleanupPhase",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean inTaskCleanupPhase()\n{\r\n    return (this.phase == TaskStatus.Phase.CLEANUP && (this.runState == TaskStatus.State.FAILED_UNCLEAN || this.runState == TaskStatus.State.KILLED_UNCLEAN));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getIncludeAllCounters",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean getIncludeAllCounters()\n{\r\n    return includeAllCounters;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "setIncludeAllCounters",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setIncludeAllCounters(boolean send)\n{\r\n    includeAllCounters = send;\r\n    counters.setWriteAllCounters(send);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getCounters",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Counters getCounters()\n{\r\n    return counters;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "setCounters",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setCounters(Counters counters)\n{\r\n    this.counters = counters;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getOutputSize",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getOutputSize()\n{\r\n    return outputSize;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "setOutputSize",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setOutputSize(long l)\n{\r\n    outputSize = l;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getFetchFailedMaps",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "List<TaskAttemptID> getFetchFailedMaps()\n{\r\n    return null;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "addFetchFailedMap",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void addFetchFailedMap(TaskAttemptID mapTaskId)",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "statusUpdate",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void statusUpdate(float progress, String state, Counters counters)\n{\r\n    setProgress(progress);\r\n    setStateString(state);\r\n    setCounters(counters);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "statusUpdate",
  "errType" : null,
  "containingMethodsNum" : 11,
  "sourceCodeText" : "void statusUpdate(TaskStatus status)\n{\r\n    setProgress(status.getProgress());\r\n    this.runState = status.getRunState();\r\n    setStateString(status.getStateString());\r\n    this.nextRecordRange = status.getNextRecordRange();\r\n    setDiagnosticInfo(status.getDiagnosticInfo());\r\n    if (status.getStartTime() > 0) {\r\n        this.setStartTime(status.getStartTime());\r\n    }\r\n    if (status.getFinishTime() > 0) {\r\n        this.setFinishTime(status.getFinishTime());\r\n    }\r\n    this.phase = status.getPhase();\r\n    this.counters = status.getCounters();\r\n    this.outputSize = status.outputSize;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "statusUpdate",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void statusUpdate(State runState, float progress, String state, Phase phase, long finishTime)\n{\r\n    setRunState(runState);\r\n    setProgress(progress);\r\n    setStateString(state);\r\n    setPhase(phase);\r\n    if (finishTime > 0) {\r\n        setFinishTime(finishTime);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "clearStatus",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void clearStatus()\n{\r\n    diagnosticInfo = \"\";\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "clone",
  "errType" : [ "CloneNotSupportedException" ],
  "containingMethodsNum" : 2,
  "sourceCodeText" : "Object clone()\n{\r\n    try {\r\n        return super.clone();\r\n    } catch (CloneNotSupportedException cnse) {\r\n        throw new InternalError(cnse.toString());\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "write",
  "errType" : null,
  "containingMethodsNum" : 13,
  "sourceCodeText" : "void write(DataOutput out) throws IOException\n{\r\n    taskid.write(out);\r\n    out.writeFloat(progress);\r\n    out.writeInt(numSlots);\r\n    WritableUtils.writeEnum(out, runState);\r\n    Text.writeString(out, diagnosticInfo);\r\n    Text.writeString(out, stateString);\r\n    WritableUtils.writeEnum(out, phase);\r\n    out.writeLong(startTime);\r\n    out.writeLong(finishTime);\r\n    out.writeBoolean(includeAllCounters);\r\n    out.writeLong(outputSize);\r\n    counters.write(out);\r\n    nextRecordRange.write(out);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "readFields",
  "errType" : null,
  "containingMethodsNum" : 13,
  "sourceCodeText" : "void readFields(DataInput in) throws IOException\n{\r\n    this.taskid.readFields(in);\r\n    setProgress(in.readFloat());\r\n    this.numSlots = in.readInt();\r\n    this.runState = WritableUtils.readEnum(in, State.class);\r\n    setDiagnosticInfo(StringInterner.weakIntern(Text.readString(in)));\r\n    setStateString(StringInterner.weakIntern(Text.readString(in)));\r\n    this.phase = WritableUtils.readEnum(in, Phase.class);\r\n    this.startTime = in.readLong();\r\n    this.finishTime = in.readLong();\r\n    counters = new Counters();\r\n    this.includeAllCounters = in.readBoolean();\r\n    this.outputSize = in.readLong();\r\n    counters.readFields(in);\r\n    nextRecordRange.readFields(in);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "createTaskStatus",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "TaskStatus createTaskStatus(DataInput in, TaskAttemptID taskId, float progress, int numSlots, State runState, String diagnosticInfo, String stateString, String taskTracker, Phase phase, Counters counters) throws IOException\n{\r\n    boolean isMap = in.readBoolean();\r\n    return createTaskStatus(isMap, taskId, progress, numSlots, runState, diagnosticInfo, stateString, taskTracker, phase, counters);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "createTaskStatus",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "TaskStatus createTaskStatus(boolean isMap, TaskAttemptID taskId, float progress, int numSlots, State runState, String diagnosticInfo, String stateString, String taskTracker, Phase phase, Counters counters)\n{\r\n    return (isMap) ? new MapTaskStatus(taskId, progress, numSlots, runState, diagnosticInfo, stateString, taskTracker, phase, counters) : new ReduceTaskStatus(taskId, progress, numSlots, runState, diagnosticInfo, stateString, taskTracker, phase, counters);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "createTaskStatus",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "TaskStatus createTaskStatus(boolean isMap)\n{\r\n    return (isMap) ? new MapTaskStatus() : new ReduceTaskStatus();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\input",
  "methodName" : "isSplitable",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "boolean isSplitable(JobContext context, Path file)\n{\r\n    final CompressionCodec codec = new CompressionCodecFactory(context.getConfiguration()).getCodec(file);\r\n    if (null == codec) {\r\n        return true;\r\n    }\r\n    return codec instanceof SplittableCompressionCodec;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\input",
  "methodName" : "createRecordReader",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "RecordReader<Text, Text> createRecordReader(InputSplit genericSplit, TaskAttemptContext context) throws IOException\n{\r\n    context.setStatus(genericSplit.toString());\r\n    return new KeyValueLineRecordReader(context.getConfiguration());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest\\files",
  "methodName" : "getType",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getType()\n{\r\n    return type;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest\\files",
  "methodName" : "setType",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setType(String type)\n{\r\n    this.type = type;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest\\files",
  "methodName" : "getVersion",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int getVersion()\n{\r\n    return version;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest\\files",
  "methodName" : "setVersion",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setVersion(int version)\n{\r\n    this.version = version;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest\\files",
  "methodName" : "getIOStatistics",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "IOStatisticsSnapshot getIOStatistics()\n{\r\n    return iostatistics;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest\\files",
  "methodName" : "setIOStatistics",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setIOStatistics(@Nullable final IOStatisticsSnapshot ioStatistics)\n{\r\n    this.iostatistics = ioStatistics;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest\\files",
  "methodName" : "getJobId",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getJobId()\n{\r\n    return jobId;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest\\files",
  "methodName" : "setJobId",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setJobId(final String jobId)\n{\r\n    this.jobId = jobId;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest\\files",
  "methodName" : "getJobAttemptNumber",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int getJobAttemptNumber()\n{\r\n    return jobAttemptNumber;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest\\files",
  "methodName" : "setJobAttemptNumber",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setJobAttemptNumber(final int jobAttemptNumber)\n{\r\n    this.jobAttemptNumber = jobAttemptNumber;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest\\files",
  "methodName" : "getTaskID",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getTaskID()\n{\r\n    return taskID;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest\\files",
  "methodName" : "setTaskID",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setTaskID(final String taskID)\n{\r\n    this.taskID = taskID;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest\\files",
  "methodName" : "getTaskAttemptID",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getTaskAttemptID()\n{\r\n    return taskAttemptID;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest\\files",
  "methodName" : "setTaskAttemptID",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setTaskAttemptID(final String taskAttemptID)\n{\r\n    this.taskAttemptID = taskAttemptID;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest\\files",
  "methodName" : "getTaskAttemptDir",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getTaskAttemptDir()\n{\r\n    return taskAttemptDir;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest\\files",
  "methodName" : "setTaskAttemptDir",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setTaskAttemptDir(final String taskAttemptDir)\n{\r\n    this.taskAttemptDir = taskAttemptDir;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest\\files",
  "methodName" : "addFileToCommit",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void addFileToCommit(FileEntry entry)\n{\r\n    filesToCommit.add(entry);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest\\files",
  "methodName" : "getFilesToCommit",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "List<FileEntry> getFilesToCommit()\n{\r\n    return filesToCommit;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest\\files",
  "methodName" : "getTotalFileSize",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "long getTotalFileSize()\n{\r\n    return filesToCommit.stream().mapToLong(FileEntry::getSize).sum();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest\\files",
  "methodName" : "getDestDirectories",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "List<DirEntry> getDestDirectories()\n{\r\n    return destDirectories;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest\\files",
  "methodName" : "addDirectory",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void addDirectory(DirEntry entry)\n{\r\n    destDirectories.add(entry);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest\\files",
  "methodName" : "getExtraData",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Map<String, String> getExtraData()\n{\r\n    return extraData;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest\\files",
  "methodName" : "toBytes",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "byte[] toBytes() throws IOException\n{\r\n    return serializer().toBytes(this);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest\\files",
  "methodName" : "toJson",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String toJson() throws IOException\n{\r\n    return serializer().toJson(this);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest\\files",
  "methodName" : "save",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void save(FileSystem fs, Path path, boolean overwrite) throws IOException\n{\r\n    serializer().save(fs, path, this, overwrite);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest\\files",
  "methodName" : "validate",
  "errType" : null,
  "containingMethodsNum" : 9,
  "sourceCodeText" : "TaskManifest validate() throws IOException\n{\r\n    verify(TYPE.equals(type), \"Wrong type: %s\", type);\r\n    verify(version == VERSION, \"Wrong version: %s\", version);\r\n    validateCollectionClass(extraData.keySet(), String.class);\r\n    validateCollectionClass(extraData.values(), String.class);\r\n    Set<String> destinations = new HashSet<>(filesToCommit.size());\r\n    validateCollectionClass(filesToCommit, FileEntry.class);\r\n    for (FileEntry c : filesToCommit) {\r\n        c.validate();\r\n        verify(!destinations.contains(c.getDest()), \"Destination %s is written to by more than one pending commit\", c.getDest());\r\n        destinations.add(c.getDest());\r\n    }\r\n    return this;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest\\files",
  "methodName" : "createSerializer",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "JsonSerialization<TaskManifest> createSerializer()\n{\r\n    return serializer();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest\\files",
  "methodName" : "serializer",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "JsonSerialization<TaskManifest> serializer()\n{\r\n    return new JsonSerialization<>(TaskManifest.class, false, true);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest\\files",
  "methodName" : "load",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "TaskManifest load(FileSystem fs, Path path) throws IOException\n{\r\n    LOG.debug(\"Reading Manifest in file {}\", path);\r\n    return serializer().load(fs, path).validate();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest\\files",
  "methodName" : "load",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "TaskManifest load(JsonSerialization<TaskManifest> serializer, FileSystem fs, Path path, FileStatus status) throws IOException\n{\r\n    LOG.debug(\"Reading Manifest in file {}\", path);\r\n    return serializer.load(fs, path, status).validate();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\task",
  "methodName" : "nextKey",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "boolean nextKey() throws IOException, InterruptedException\n{\r\n    while (hasMore && nextKeyIsSame) {\r\n        nextKeyValue();\r\n    }\r\n    if (hasMore) {\r\n        if (inputKeyCounter != null) {\r\n            inputKeyCounter.increment(1);\r\n        }\r\n        return nextKeyValue();\r\n    } else {\r\n        return false;\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\task",
  "methodName" : "nextKeyValue",
  "errType" : null,
  "containingMethodsNum" : 16,
  "sourceCodeText" : "boolean nextKeyValue() throws IOException, InterruptedException\n{\r\n    if (!hasMore) {\r\n        key = null;\r\n        value = null;\r\n        return false;\r\n    }\r\n    firstValue = !nextKeyIsSame;\r\n    DataInputBuffer nextKey = input.getKey();\r\n    currentRawKey.set(nextKey.getData(), nextKey.getPosition(), nextKey.getLength() - nextKey.getPosition());\r\n    buffer.reset(currentRawKey.getBytes(), 0, currentRawKey.getLength());\r\n    key = keyDeserializer.deserialize(key);\r\n    DataInputBuffer nextVal = input.getValue();\r\n    buffer.reset(nextVal.getData(), nextVal.getPosition(), nextVal.getLength() - nextVal.getPosition());\r\n    value = valueDeserializer.deserialize(value);\r\n    currentKeyLength = nextKey.getLength() - nextKey.getPosition();\r\n    currentValueLength = nextVal.getLength() - nextVal.getPosition();\r\n    if (isMarked) {\r\n        backupStore.write(nextKey, nextVal);\r\n    }\r\n    hasMore = input.next();\r\n    if (hasMore) {\r\n        nextKey = input.getKey();\r\n        nextKeyIsSame = comparator.compare(currentRawKey.getBytes(), 0, currentRawKey.getLength(), nextKey.getData(), nextKey.getPosition(), nextKey.getLength() - nextKey.getPosition()) == 0;\r\n    } else {\r\n        nextKeyIsSame = false;\r\n    }\r\n    inputValueCounter.increment(1);\r\n    return true;\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\task",
  "methodName" : "getCurrentKey",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "KEYIN getCurrentKey()\n{\r\n    return key;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\task",
  "methodName" : "getCurrentValue",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "VALUEIN getCurrentValue()\n{\r\n    return value;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\task",
  "methodName" : "getBackupStore",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "BackupStore<KEYIN, VALUEIN> getBackupStore()\n{\r\n    return backupStore;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\task",
  "methodName" : "getValues",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Iterable<VALUEIN> getValues() throws IOException, InterruptedException\n{\r\n    return iterable;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "getTaskType",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "TaskType getTaskType()\n{\r\n    return this.taskType;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "getMemory",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getMemory()\n{\r\n    return this.memory;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "getEventType",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "EventType getEventType()\n{\r\n    return EventType.NORMALIZED_RESOURCE;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "getDatum",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Object getDatum()\n{\r\n    throw new UnsupportedOperationException(\"Not a seriable object\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "setDatum",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setDatum(Object datum)\n{\r\n    throw new UnsupportedOperationException(\"Not a seriable object\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "toTimelineEvent",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "TimelineEvent toTimelineEvent()\n{\r\n    TimelineEvent tEvent = new TimelineEvent();\r\n    tEvent.setId(StringUtils.toUpperCase(getEventType().name()));\r\n    tEvent.addInfo(\"MEMORY\", \"\" + getMemory());\r\n    tEvent.addInfo(\"TASK_TYPE\", getTaskType());\r\n    return tEvent;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "getTimelineMetrics",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Set<TimelineMetric> getTimelineMetrics()\n{\r\n    return null;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred\\lib",
  "methodName" : "setReducer",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void setReducer(JobConf job, Class<? extends Reducer<K1, V1, K2, V2>> klass, Class<? extends K1> inputKeyClass, Class<? extends V1> inputValueClass, Class<? extends K2> outputKeyClass, Class<? extends V2> outputValueClass, boolean byValue, JobConf reducerConf)\n{\r\n    job.setReducerClass(ChainReducer.class);\r\n    job.setOutputKeyClass(outputKeyClass);\r\n    job.setOutputValueClass(outputValueClass);\r\n    Chain.setReducer(job, klass, inputKeyClass, inputValueClass, outputKeyClass, outputValueClass, byValue, reducerConf);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred\\lib",
  "methodName" : "addMapper",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void addMapper(JobConf job, Class<? extends Mapper<K1, V1, K2, V2>> klass, Class<? extends K1> inputKeyClass, Class<? extends V1> inputValueClass, Class<? extends K2> outputKeyClass, Class<? extends V2> outputValueClass, boolean byValue, JobConf mapperConf)\n{\r\n    job.setOutputKeyClass(outputKeyClass);\r\n    job.setOutputValueClass(outputValueClass);\r\n    Chain.addMapper(false, job, klass, inputKeyClass, inputValueClass, outputKeyClass, outputValueClass, byValue, mapperConf);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred\\lib",
  "methodName" : "configure",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void configure(JobConf job)\n{\r\n    chain.configure(job);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred\\lib",
  "methodName" : "reduce",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void reduce(Object key, Iterator values, OutputCollector output, Reporter reporter) throws IOException\n{\r\n    Reducer reducer = chain.getReducer();\r\n    if (reducer != null) {\r\n        reducer.reduce(key, values, chain.getReducerCollector(output, reporter), reporter);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred\\lib",
  "methodName" : "close",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void close() throws IOException\n{\r\n    chain.close();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getTaskTrackers",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int getTaskTrackers()\n{\r\n    return numActiveTrackers;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getActiveTrackerNames",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Collection<String> getActiveTrackerNames()\n{\r\n    return activeTrackers;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getBlacklistedTrackerNames",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Collection<String> getBlacklistedTrackerNames()\n{\r\n    ArrayList<String> blacklistedTrackers = new ArrayList<String>();\r\n    for (BlackListInfo bi : blacklistedTrackersInfo) {\r\n        blacklistedTrackers.add(bi.getTrackerName());\r\n    }\r\n    return blacklistedTrackers;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getGraylistedTrackerNames",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Collection<String> getGraylistedTrackerNames()\n{\r\n    return Collections.emptySet();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getGraylistedTrackers",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int getGraylistedTrackers()\n{\r\n    return grayListedTrackers;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getBlacklistedTrackers",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int getBlacklistedTrackers()\n{\r\n    return numBlacklistedTrackers;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getNumExcludedNodes",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int getNumExcludedNodes()\n{\r\n    return numExcludedNodes;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getTTExpiryInterval",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getTTExpiryInterval()\n{\r\n    return ttExpiryInterval;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getMapTasks",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int getMapTasks()\n{\r\n    return map_tasks;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getReduceTasks",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int getReduceTasks()\n{\r\n    return reduce_tasks;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getMaxMapTasks",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int getMaxMapTasks()\n{\r\n    return max_map_tasks;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getMaxReduceTasks",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int getMaxReduceTasks()\n{\r\n    return max_reduce_tasks;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getJobTrackerStatus",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "JobTrackerStatus getJobTrackerStatus()\n{\r\n    return status;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getMaxMemory",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getMaxMemory()\n{\r\n    return UNINITIALIZED_MEMORY_VALUE;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getUsedMemory",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getUsedMemory()\n{\r\n    return UNINITIALIZED_MEMORY_VALUE;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getBlackListedTrackersInfo",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Collection<BlackListInfo> getBlackListedTrackersInfo()\n{\r\n    return blacklistedTrackersInfo;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getJobTrackerState",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "JobTracker.State getJobTrackerState()\n{\r\n    return JobTracker.State.RUNNING;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "write",
  "errType" : null,
  "containingMethodsNum" : 20,
  "sourceCodeText" : "void write(DataOutput out) throws IOException\n{\r\n    if (activeTrackers.size() == 0) {\r\n        out.writeInt(numActiveTrackers);\r\n        out.writeInt(0);\r\n    } else {\r\n        out.writeInt(activeTrackers.size());\r\n        out.writeInt(activeTrackers.size());\r\n        for (String tracker : activeTrackers) {\r\n            Text.writeString(out, tracker);\r\n        }\r\n    }\r\n    if (blacklistedTrackersInfo.size() == 0) {\r\n        out.writeInt(numBlacklistedTrackers);\r\n        out.writeInt(blacklistedTrackersInfo.size());\r\n    } else {\r\n        out.writeInt(blacklistedTrackersInfo.size());\r\n        out.writeInt(blacklistedTrackersInfo.size());\r\n        for (BlackListInfo tracker : blacklistedTrackersInfo) {\r\n            tracker.write(out);\r\n        }\r\n    }\r\n    out.writeInt(numExcludedNodes);\r\n    out.writeLong(ttExpiryInterval);\r\n    out.writeInt(map_tasks);\r\n    out.writeInt(reduce_tasks);\r\n    out.writeInt(max_map_tasks);\r\n    out.writeInt(max_reduce_tasks);\r\n    WritableUtils.writeEnum(out, status);\r\n    out.writeInt(grayListedTrackers);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "readFields",
  "errType" : null,
  "containingMethodsNum" : 16,
  "sourceCodeText" : "void readFields(DataInput in) throws IOException\n{\r\n    numActiveTrackers = in.readInt();\r\n    int numTrackerNames = in.readInt();\r\n    if (numTrackerNames > 0) {\r\n        for (int i = 0; i < numTrackerNames; i++) {\r\n            String name = StringInterner.weakIntern(Text.readString(in));\r\n            activeTrackers.add(name);\r\n        }\r\n    }\r\n    numBlacklistedTrackers = in.readInt();\r\n    int blackListTrackerInfoSize = in.readInt();\r\n    if (blackListTrackerInfoSize > 0) {\r\n        for (int i = 0; i < blackListTrackerInfoSize; i++) {\r\n            BlackListInfo info = new BlackListInfo();\r\n            info.readFields(in);\r\n            blacklistedTrackersInfo.add(info);\r\n        }\r\n    }\r\n    numExcludedNodes = in.readInt();\r\n    ttExpiryInterval = in.readLong();\r\n    map_tasks = in.readInt();\r\n    reduce_tasks = in.readInt();\r\n    max_map_tasks = in.readInt();\r\n    max_reduce_tasks = in.readInt();\r\n    status = WritableUtils.readEnum(in, JobTrackerStatus.class);\r\n    grayListedTrackers = in.readInt();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "close",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void close() throws IOException\n{\r\n    if (closed) {\r\n        return;\r\n    }\r\n    closed = true;\r\n    try {\r\n        finish();\r\n    } finally {\r\n        IOUtils.closeStream(out);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "finish",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void finish() throws IOException\n{\r\n    if (finished) {\r\n        return;\r\n    }\r\n    finished = true;\r\n    sum.writeValue(barray, 0, false);\r\n    out.write(barray, 0, sum.getChecksumSize());\r\n    out.flush();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "write",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void write(byte[] b, int off, int len) throws IOException\n{\r\n    sum.update(b, off, len);\r\n    out.write(b, off, len);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "write",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void write(int b) throws IOException\n{\r\n    barray[0] = (byte) (b & 0xFF);\r\n    write(barray, 0, 1);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "createKey",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Text createKey()\n{\r\n    return new Text();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "createValue",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Text createValue()\n{\r\n    return new Text();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "next",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "boolean next(Text key, Text value) throws IOException\n{\r\n    Text tKey = key;\r\n    Text tValue = value;\r\n    if (!sequenceFileRecordReader.next(innerKey, innerValue)) {\r\n        return false;\r\n    }\r\n    tKey.set(innerKey.toString());\r\n    tValue.set(innerValue.toString());\r\n    return true;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getProgress",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "float getProgress() throws IOException\n{\r\n    return sequenceFileRecordReader.getProgress();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getPos",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "long getPos() throws IOException\n{\r\n    return sequenceFileRecordReader.getPos();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "close",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void close() throws IOException\n{\r\n    sequenceFileRecordReader.close();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\db",
  "methodName" : "split",
  "errType" : null,
  "containingMethodsNum" : 12,
  "sourceCodeText" : "List<InputSplit> split(Configuration conf, ResultSet results, String colName) throws SQLException\n{\r\n    BigDecimal minVal = results.getBigDecimal(1);\r\n    BigDecimal maxVal = results.getBigDecimal(2);\r\n    String lowClausePrefix = colName + \" >= \";\r\n    String highClausePrefix = colName + \" < \";\r\n    BigDecimal numSplits = new BigDecimal(conf.getInt(MRJobConfig.NUM_MAPS, 1));\r\n    if (minVal == null && maxVal == null) {\r\n        List<InputSplit> splits = new ArrayList<InputSplit>();\r\n        splits.add(new DataDrivenDBInputFormat.DataDrivenDBInputSplit(colName + \" IS NULL\", colName + \" IS NULL\"));\r\n        return splits;\r\n    }\r\n    if (minVal == null || maxVal == null) {\r\n        LOG.error(\"Cannot find a range for NUMERIC or DECIMAL fields with one end NULL.\");\r\n        return null;\r\n    }\r\n    List<BigDecimal> splitPoints = split(numSplits, minVal, maxVal);\r\n    List<InputSplit> splits = new ArrayList<InputSplit>();\r\n    BigDecimal start = splitPoints.get(0);\r\n    for (int i = 1; i < splitPoints.size(); i++) {\r\n        BigDecimal end = splitPoints.get(i);\r\n        if (i == splitPoints.size() - 1) {\r\n            splits.add(new DataDrivenDBInputFormat.DataDrivenDBInputSplit(lowClausePrefix + start.toString(), colName + \" <= \" + end.toString()));\r\n        } else {\r\n            splits.add(new DataDrivenDBInputFormat.DataDrivenDBInputSplit(lowClausePrefix + start.toString(), highClausePrefix + end.toString()));\r\n        }\r\n        start = end;\r\n    }\r\n    return splits;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\db",
  "methodName" : "tryDivide",
  "errType" : [ "ArithmeticException" ],
  "containingMethodsNum" : 2,
  "sourceCodeText" : "BigDecimal tryDivide(BigDecimal numerator, BigDecimal denominator)\n{\r\n    try {\r\n        return numerator.divide(denominator);\r\n    } catch (ArithmeticException ae) {\r\n        return numerator.divide(denominator, BigDecimal.ROUND_HALF_UP);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\db",
  "methodName" : "split",
  "errType" : null,
  "containingMethodsNum" : 9,
  "sourceCodeText" : "List<BigDecimal> split(BigDecimal numSplits, BigDecimal minVal, BigDecimal maxVal) throws SQLException\n{\r\n    List<BigDecimal> splits = new ArrayList<BigDecimal>();\r\n    BigDecimal splitSize = tryDivide(maxVal.subtract(minVal), (numSplits));\r\n    if (splitSize.compareTo(MIN_INCREMENT) < 0) {\r\n        splitSize = MIN_INCREMENT;\r\n        LOG.warn(\"Set BigDecimal splitSize to MIN_INCREMENT\");\r\n    }\r\n    BigDecimal curVal = minVal;\r\n    while (curVal.compareTo(maxVal) <= 0) {\r\n        splits.add(curVal);\r\n        curVal = curVal.add(splitSize);\r\n    }\r\n    if (splits.get(splits.size() - 1).compareTo(maxVal) != 0 || splits.size() == 1) {\r\n        splits.add(maxVal);\r\n    }\r\n    return splits;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\counters",
  "methodName" : "setDisplayName",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setDisplayName(String displayName)\n{\r\n    this.displayName = displayName;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\counters",
  "methodName" : "readFields",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void readFields(DataInput in) throws IOException\n{\r\n    name = StringInterner.weakIntern(Text.readString(in));\r\n    displayName = in.readBoolean() ? StringInterner.weakIntern(Text.readString(in)) : name;\r\n    value = WritableUtils.readVLong(in);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\counters",
  "methodName" : "write",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void write(DataOutput out) throws IOException\n{\r\n    Text.writeString(out, name);\r\n    boolean distinctDisplayName = !name.equals(displayName);\r\n    out.writeBoolean(distinctDisplayName);\r\n    if (distinctDisplayName) {\r\n        Text.writeString(out, displayName);\r\n    }\r\n    WritableUtils.writeVLong(out, value);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\counters",
  "methodName" : "getName",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getName()\n{\r\n    return name;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\counters",
  "methodName" : "getDisplayName",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getDisplayName()\n{\r\n    return displayName;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\counters",
  "methodName" : "getValue",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getValue()\n{\r\n    return value;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\counters",
  "methodName" : "setValue",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setValue(long value)\n{\r\n    this.value = value;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\counters",
  "methodName" : "increment",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void increment(long incr)\n{\r\n    value += incr;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\counters",
  "methodName" : "getUnderlyingCounter",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Counter getUnderlyingCounter()\n{\r\n    return this;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "initSharedCache",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void initSharedCache(JobID jobid, Configuration conf)\n{\r\n    this.scConfig.init(conf);\r\n    if (this.scConfig.isSharedCacheEnabled()) {\r\n        this.scClient = createSharedCacheClient(conf);\r\n        appId = jobIDToAppId(jobid);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "jobIDToAppId",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "ApplicationId jobIDToAppId(JobID jobId)\n{\r\n    return ApplicationId.newInstance(Long.parseLong(jobId.getJtIdentifier()), jobId.getId());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "stopSharedCache",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void stopSharedCache()\n{\r\n    if (scClient != null) {\r\n        scClient.stop();\r\n        scClient = null;\r\n    }\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "createSharedCacheClient",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "SharedCacheClient createSharedCacheClient(Configuration conf)\n{\r\n    SharedCacheClient scc = SharedCacheClient.createSharedCacheClient();\r\n    scc.init(conf);\r\n    scc.start();\r\n    return scc;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "uploadResources",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void uploadResources(Job job, Path submitJobDir) throws IOException\n{\r\n    try {\r\n        initSharedCache(job.getJobID(), job.getConfiguration());\r\n        uploadResourcesInternal(job, submitJobDir);\r\n    } finally {\r\n        stopSharedCache();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "uploadResourcesInternal",
  "errType" : null,
  "containingMethodsNum" : 28,
  "sourceCodeText" : "void uploadResourcesInternal(Job job, Path submitJobDir) throws IOException\n{\r\n    Configuration conf = job.getConfiguration();\r\n    short replication = (short) conf.getInt(Job.SUBMIT_REPLICATION, Job.DEFAULT_SUBMIT_REPLICATION);\r\n    if (!(conf.getBoolean(Job.USED_GENERIC_PARSER, false))) {\r\n        LOG.warn(\"Hadoop command-line option parsing not performed. \" + \"Implement the Tool interface and execute your application \" + \"with ToolRunner to remedy this.\");\r\n    }\r\n    LOG.debug(\"default FileSystem: \" + jtFs.getUri());\r\n    if (jtFs.exists(submitJobDir)) {\r\n        throw new IOException(\"Not submitting job. Job directory \" + submitJobDir + \" already exists!! This is unexpected.Please check what's there in\" + \" that directory\");\r\n    }\r\n    submitJobDir = jtFs.makeQualified(submitJobDir);\r\n    submitJobDir = new Path(submitJobDir.toUri().getPath());\r\n    FsPermission mapredSysPerms = new FsPermission(JobSubmissionFiles.JOB_DIR_PERMISSION);\r\n    mkdirs(jtFs, submitJobDir, mapredSysPerms);\r\n    if (!conf.getBoolean(MRJobConfig.MR_AM_STAGING_DIR_ERASURECODING_ENABLED, MRJobConfig.DEFAULT_MR_AM_STAGING_ERASURECODING_ENABLED)) {\r\n        disableErasureCodingForPath(submitJobDir);\r\n    }\r\n    Collection<String> files = conf.getStringCollection(\"tmpfiles\");\r\n    Collection<String> libjars = conf.getStringCollection(\"tmpjars\");\r\n    Collection<String> archives = conf.getStringCollection(\"tmparchives\");\r\n    String jobJar = job.getJar();\r\n    files.addAll(conf.getStringCollection(MRJobConfig.FILES_FOR_SHARED_CACHE));\r\n    libjars.addAll(conf.getStringCollection(MRJobConfig.FILES_FOR_CLASSPATH_AND_SHARED_CACHE));\r\n    archives.addAll(conf.getStringCollection(MRJobConfig.ARCHIVES_FOR_SHARED_CACHE));\r\n    Map<URI, FileStatus> statCache = new HashMap<URI, FileStatus>();\r\n    checkLocalizationLimits(conf, files, libjars, archives, jobJar, statCache);\r\n    Map<String, Boolean> fileSCUploadPolicies = new LinkedHashMap<String, Boolean>();\r\n    Map<String, Boolean> archiveSCUploadPolicies = new LinkedHashMap<String, Boolean>();\r\n    uploadFiles(job, files, submitJobDir, mapredSysPerms, replication, fileSCUploadPolicies, statCache);\r\n    uploadLibJars(job, libjars, submitJobDir, mapredSysPerms, replication, fileSCUploadPolicies, statCache);\r\n    uploadArchives(job, archives, submitJobDir, mapredSysPerms, replication, archiveSCUploadPolicies, statCache);\r\n    uploadJobJar(job, jobJar, submitJobDir, replication, statCache);\r\n    addLog4jToDistributedCache(job, submitJobDir);\r\n    Job.setFileSharedCacheUploadPolicies(conf, fileSCUploadPolicies);\r\n    Job.setArchiveSharedCacheUploadPolicies(conf, archiveSCUploadPolicies);\r\n    ClientDistributedCacheManager.determineTimestampsAndCacheVisibilities(conf, statCache);\r\n    ClientDistributedCacheManager.getDelegationTokens(conf, job.getCredentials());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "uploadFiles",
  "errType" : [ "URISyntaxException", "URISyntaxException" ],
  "containingMethodsNum" : 11,
  "sourceCodeText" : "void uploadFiles(Job job, Collection<String> files, Path submitJobDir, FsPermission mapredSysPerms, short submitReplication, Map<String, Boolean> fileSCUploadPolicies, Map<URI, FileStatus> statCache) throws IOException\n{\r\n    Configuration conf = job.getConfiguration();\r\n    Path filesDir = JobSubmissionFiles.getJobDistCacheFiles(submitJobDir);\r\n    if (!files.isEmpty()) {\r\n        mkdirs(jtFs, filesDir, mapredSysPerms);\r\n        for (String tmpFile : files) {\r\n            URI tmpURI = null;\r\n            try {\r\n                tmpURI = new URI(tmpFile);\r\n            } catch (URISyntaxException e) {\r\n                throw new IllegalArgumentException(\"Error parsing files argument.\" + \" Argument must be a valid URI: \" + tmpFile, e);\r\n            }\r\n            Path tmp = new Path(tmpURI);\r\n            URI newURI = null;\r\n            boolean uploadToSharedCache = false;\r\n            if (scConfig.isSharedCacheFilesEnabled()) {\r\n                newURI = useSharedCache(tmpURI, tmp.getName(), statCache, conf, true);\r\n                if (newURI == null) {\r\n                    uploadToSharedCache = true;\r\n                }\r\n            }\r\n            if (newURI == null) {\r\n                Path newPath = copyRemoteFiles(filesDir, tmp, conf, submitReplication);\r\n                try {\r\n                    newURI = getPathURI(newPath, tmpURI.getFragment());\r\n                } catch (URISyntaxException ue) {\r\n                    throw new IOException(\"Failed to create a URI (URISyntaxException) for the\" + \" remote path \" + newPath + \". This was based on the files parameter: \" + tmpFile, ue);\r\n                }\r\n            }\r\n            job.addCacheFile(newURI);\r\n            if (scConfig.isSharedCacheFilesEnabled()) {\r\n                fileSCUploadPolicies.put(newURI.toString(), uploadToSharedCache);\r\n            }\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 2,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "uploadLibJars",
  "errType" : [ "URISyntaxException", "URISyntaxException" ],
  "containingMethodsNum" : 17,
  "sourceCodeText" : "void uploadLibJars(Job job, Collection<String> libjars, Path submitJobDir, FsPermission mapredSysPerms, short submitReplication, Map<String, Boolean> fileSCUploadPolicies, Map<URI, FileStatus> statCache) throws IOException\n{\r\n    Configuration conf = job.getConfiguration();\r\n    Path libjarsDir = JobSubmissionFiles.getJobDistCacheLibjars(submitJobDir);\r\n    if (!libjars.isEmpty()) {\r\n        mkdirs(jtFs, libjarsDir, mapredSysPerms);\r\n        Collection<URI> libjarURIs = new LinkedList<>();\r\n        boolean foundFragment = false;\r\n        for (String tmpjars : libjars) {\r\n            URI tmpURI = null;\r\n            try {\r\n                tmpURI = new URI(tmpjars);\r\n            } catch (URISyntaxException e) {\r\n                throw new IllegalArgumentException(\"Error parsing libjars argument.\" + \" Argument must be a valid URI: \" + tmpjars, e);\r\n            }\r\n            Path tmp = new Path(tmpURI);\r\n            URI newURI = null;\r\n            boolean uploadToSharedCache = false;\r\n            boolean fromSharedCache = false;\r\n            if (scConfig.isSharedCacheLibjarsEnabled()) {\r\n                newURI = useSharedCache(tmpURI, tmp.getName(), statCache, conf, true);\r\n                if (newURI == null) {\r\n                    uploadToSharedCache = true;\r\n                } else {\r\n                    fromSharedCache = true;\r\n                }\r\n            }\r\n            if (newURI == null) {\r\n                Path newPath = copyRemoteFiles(libjarsDir, tmp, conf, submitReplication);\r\n                try {\r\n                    newURI = getPathURI(newPath, tmpURI.getFragment());\r\n                } catch (URISyntaxException ue) {\r\n                    throw new IOException(\"Failed to create a URI (URISyntaxException) for the\" + \" remote path \" + newPath + \". This was based on the libjar parameter: \" + tmpjars, ue);\r\n                }\r\n            }\r\n            if (!foundFragment) {\r\n                foundFragment = (newURI.getFragment() != null) && !fromSharedCache;\r\n            }\r\n            Job.addFileToClassPath(new Path(newURI.getPath()), conf, jtFs, false);\r\n            if (fromSharedCache) {\r\n                Job.addCacheFile(newURI, conf);\r\n            } else {\r\n                libjarURIs.add(newURI);\r\n            }\r\n            if (scConfig.isSharedCacheLibjarsEnabled()) {\r\n                fileSCUploadPolicies.put(newURI.toString(), uploadToSharedCache);\r\n            }\r\n        }\r\n        if (useWildcard && !foundFragment) {\r\n            Path libJarsDirWildcard = jtFs.makeQualified(new Path(libjarsDir, DistributedCache.WILDCARD));\r\n            Job.addCacheFile(libJarsDirWildcard.toUri(), conf);\r\n        } else {\r\n            for (URI uri : libjarURIs) {\r\n                Job.addCacheFile(uri, conf);\r\n            }\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 2,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "uploadArchives",
  "errType" : [ "URISyntaxException", "URISyntaxException" ],
  "containingMethodsNum" : 11,
  "sourceCodeText" : "void uploadArchives(Job job, Collection<String> archives, Path submitJobDir, FsPermission mapredSysPerms, short submitReplication, Map<String, Boolean> archiveSCUploadPolicies, Map<URI, FileStatus> statCache) throws IOException\n{\r\n    Configuration conf = job.getConfiguration();\r\n    Path archivesDir = JobSubmissionFiles.getJobDistCacheArchives(submitJobDir);\r\n    if (!archives.isEmpty()) {\r\n        mkdirs(jtFs, archivesDir, mapredSysPerms);\r\n        for (String tmpArchives : archives) {\r\n            URI tmpURI;\r\n            try {\r\n                tmpURI = new URI(tmpArchives);\r\n            } catch (URISyntaxException e) {\r\n                throw new IllegalArgumentException(\"Error parsing archives argument.\" + \" Argument must be a valid URI: \" + tmpArchives, e);\r\n            }\r\n            Path tmp = new Path(tmpURI);\r\n            URI newURI = null;\r\n            boolean uploadToSharedCache = false;\r\n            if (scConfig.isSharedCacheArchivesEnabled()) {\r\n                newURI = useSharedCache(tmpURI, tmp.getName(), statCache, conf, true);\r\n                if (newURI == null) {\r\n                    uploadToSharedCache = true;\r\n                }\r\n            }\r\n            if (newURI == null) {\r\n                Path newPath = copyRemoteFiles(archivesDir, tmp, conf, submitReplication);\r\n                try {\r\n                    newURI = getPathURI(newPath, tmpURI.getFragment());\r\n                } catch (URISyntaxException ue) {\r\n                    throw new IOException(\"Failed to create a URI (URISyntaxException) for the\" + \" remote path \" + newPath + \". This was based on the archive parameter: \" + tmpArchives, ue);\r\n                }\r\n            }\r\n            job.addCacheArchive(newURI);\r\n            if (scConfig.isSharedCacheArchivesEnabled()) {\r\n                archiveSCUploadPolicies.put(newURI.toString(), uploadToSharedCache);\r\n            }\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 2,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "uploadJobJar",
  "errType" : null,
  "containingMethodsNum" : 21,
  "sourceCodeText" : "void uploadJobJar(Job job, String jobJar, Path submitJobDir, short submitReplication, Map<URI, FileStatus> statCache) throws IOException\n{\r\n    Configuration conf = job.getConfiguration();\r\n    if (jobJar != null) {\r\n        if (\"\".equals(job.getJobName())) {\r\n            job.setJobName(new Path(jobJar).getName());\r\n        }\r\n        Path jobJarPath = new Path(jobJar);\r\n        URI jobJarURI = jobJarPath.toUri();\r\n        Path newJarPath = null;\r\n        boolean uploadToSharedCache = false;\r\n        if (jobJarURI.getScheme() == null || jobJarURI.getScheme().equals(\"file\")) {\r\n            if (scConfig.isSharedCacheJobjarEnabled()) {\r\n                jobJarPath = FileSystem.getLocal(conf).makeQualified(jobJarPath);\r\n                URI newURI = useSharedCache(jobJarPath.toUri(), null, statCache, conf, false);\r\n                if (newURI == null) {\r\n                    uploadToSharedCache = true;\r\n                } else {\r\n                    newJarPath = stringToPath(newURI.toString());\r\n                    conf.setBoolean(MRJobConfig.JOBJAR_VISIBILITY, true);\r\n                }\r\n            }\r\n            if (newJarPath == null) {\r\n                newJarPath = JobSubmissionFiles.getJobJar(submitJobDir);\r\n                copyJar(jobJarPath, newJarPath, submitReplication);\r\n            }\r\n        } else {\r\n            if (scConfig.isSharedCacheJobjarEnabled()) {\r\n                URI newURI = useSharedCache(jobJarURI, null, statCache, conf, false);\r\n                if (newURI == null) {\r\n                    uploadToSharedCache = true;\r\n                    newJarPath = jobJarPath;\r\n                } else {\r\n                    newJarPath = stringToPath(newURI.toString());\r\n                    conf.setBoolean(MRJobConfig.JOBJAR_VISIBILITY, true);\r\n                }\r\n            } else {\r\n                newJarPath = jobJarPath;\r\n            }\r\n        }\r\n        job.setJar(newJarPath.toString());\r\n        if (scConfig.isSharedCacheJobjarEnabled()) {\r\n            conf.setBoolean(MRJobConfig.JOBJAR_SHARED_CACHE_UPLOAD_POLICY, uploadToSharedCache);\r\n        }\r\n    } else {\r\n        LOG.warn(\"No job jar file set.  User classes may not be found. \" + \"See Job or Job#setJar(String).\");\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "checkLocalizationLimits",
  "errType" : null,
  "containingMethodsNum" : 9,
  "sourceCodeText" : "void checkLocalizationLimits(Configuration conf, Collection<String> files, Collection<String> libjars, Collection<String> archives, String jobJar, Map<URI, FileStatus> statCache) throws IOException\n{\r\n    LimitChecker limitChecker = new LimitChecker(conf);\r\n    if (!limitChecker.hasLimits()) {\r\n        return;\r\n    }\r\n    Collection<String> dcFiles = conf.getStringCollection(MRJobConfig.CACHE_FILES);\r\n    Collection<String> dcArchives = conf.getStringCollection(MRJobConfig.CACHE_ARCHIVES);\r\n    for (String uri : dcFiles) {\r\n        explorePath(conf, stringToPath(uri), limitChecker, statCache);\r\n    }\r\n    for (String uri : dcArchives) {\r\n        explorePath(conf, stringToPath(uri), limitChecker, statCache);\r\n    }\r\n    for (String uri : files) {\r\n        explorePath(conf, stringToPath(uri), limitChecker, statCache);\r\n    }\r\n    for (String uri : libjars) {\r\n        explorePath(conf, stringToPath(uri), limitChecker, statCache);\r\n    }\r\n    for (String uri : archives) {\r\n        explorePath(conf, stringToPath(uri), limitChecker, statCache);\r\n    }\r\n    if (jobJar != null) {\r\n        explorePath(conf, stringToPath(jobJar), limitChecker, statCache);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "stringToPath",
  "errType" : [ "URISyntaxException" ],
  "containingMethodsNum" : 3,
  "sourceCodeText" : "Path stringToPath(String s)\n{\r\n    try {\r\n        URI uri = new URI(s);\r\n        return new Path(uri.getScheme(), uri.getAuthority(), uri.getPath());\r\n    } catch (URISyntaxException e) {\r\n        throw new IllegalArgumentException(\"Error parsing argument.\" + \" Argument must be a valid URI: \" + s, e);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "explorePath",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void explorePath(Configuration job, Path p, LimitChecker limitChecker, Map<URI, FileStatus> statCache) throws IOException\n{\r\n    Path pathWithScheme = p;\r\n    if (!pathWithScheme.toUri().isAbsolute()) {\r\n        FileSystem localFs = FileSystem.getLocal(job);\r\n        pathWithScheme = localFs.makeQualified(p);\r\n    }\r\n    FileStatus status = getFileStatus(statCache, job, pathWithScheme);\r\n    if (status.isDirectory()) {\r\n        FileStatus[] statusArray = pathWithScheme.getFileSystem(job).listStatus(pathWithScheme);\r\n        for (FileStatus s : statusArray) {\r\n            explorePath(job, s.getPath(), limitChecker, statCache);\r\n        }\r\n    } else {\r\n        limitChecker.addFile(pathWithScheme, status.getLen());\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "getFileStatus",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "FileStatus getFileStatus(Map<URI, FileStatus> statCache, Configuration job, Path p) throws IOException\n{\r\n    URI u = p.toUri();\r\n    FileStatus status = statCache.get(u);\r\n    if (status == null) {\r\n        status = p.getFileSystem(job).getFileStatus(p);\r\n        statCache.put(u, status);\r\n    }\r\n    return status;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "mkdirs",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "boolean mkdirs(FileSystem fs, Path dir, FsPermission permission) throws IOException\n{\r\n    return FileSystem.mkdirs(fs, dir, permission);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "copyRemoteFiles",
  "errType" : [ "URISyntaxException" ],
  "containingMethodsNum" : 10,
  "sourceCodeText" : "Path copyRemoteFiles(Path parentDir, Path originalPath, Configuration conf, short replication) throws IOException\n{\r\n    FileSystem remoteFs = null;\r\n    remoteFs = originalPath.getFileSystem(conf);\r\n    if (FileUtil.compareFs(remoteFs, jtFs)) {\r\n        return originalPath;\r\n    }\r\n    boolean root = false;\r\n    if (ROOT_PATH.equals(originalPath.toUri().getPath())) {\r\n        root = true;\r\n    } else {\r\n        String uriString = originalPath.toUri().toString();\r\n        if (uriString.endsWith(\"/\")) {\r\n            try {\r\n                URI strippedURI = new URI(uriString.substring(0, uriString.length() - 1));\r\n                originalPath = new Path(strippedURI);\r\n            } catch (URISyntaxException e) {\r\n                throw new IllegalArgumentException(\"Error processing URI\", e);\r\n            }\r\n        }\r\n    }\r\n    Path newPath = root ? parentDir : new Path(parentDir, originalPath.getName());\r\n    FileUtil.copy(remoteFs, originalPath, jtFs, newPath, false, conf);\r\n    jtFs.setReplication(newPath, replication);\r\n    jtFs.makeQualified(newPath);\r\n    return newPath;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "useSharedCache",
  "errType" : [ "YarnException", "URISyntaxException" ],
  "containingMethodsNum" : 21,
  "sourceCodeText" : "URI useSharedCache(URI sourceFile, String resourceName, Map<URI, FileStatus> statCache, Configuration conf, boolean honorFragment) throws IOException\n{\r\n    if (scClient == null) {\r\n        return null;\r\n    }\r\n    Path filePath = new Path(sourceFile);\r\n    if (getFileStatus(statCache, conf, filePath).isDirectory()) {\r\n        LOG.warn(\"Shared cache does not support directories\" + \" (see YARN-6097).\" + \" Will not upload \" + filePath + \" to the shared cache.\");\r\n        return null;\r\n    }\r\n    String rn = resourceName;\r\n    if (honorFragment) {\r\n        if (sourceFile.getFragment() != null) {\r\n            rn = sourceFile.getFragment();\r\n        }\r\n    }\r\n    String checksum = scClient.getFileChecksum(filePath);\r\n    URL url = null;\r\n    try {\r\n        url = scClient.use(this.appId, checksum);\r\n    } catch (YarnException e) {\r\n        LOG.warn(\"Error trying to contact the shared cache manager,\" + \" disabling the SCMClient for the rest of this job submission\", e);\r\n        stopSharedCache();\r\n    }\r\n    if (url != null) {\r\n        URI uri = null;\r\n        try {\r\n            String name = new Path(url.getFile()).getName();\r\n            if (rn != null && !name.equals(rn)) {\r\n                uri = new URI(url.getScheme(), url.getUserInfo(), url.getHost(), url.getPort(), url.getFile(), null, rn);\r\n            } else {\r\n                uri = new URI(url.getScheme(), url.getUserInfo(), url.getHost(), url.getPort(), url.getFile(), null, null);\r\n            }\r\n            return uri;\r\n        } catch (URISyntaxException e) {\r\n            LOG.warn(\"Error trying to convert URL received from shared cache to\" + \" a URI: \" + url.toString());\r\n            return null;\r\n        }\r\n    } else {\r\n        return null;\r\n    }\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 2,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "copyJar",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void copyJar(Path originalJarPath, Path submitJarFile, short replication) throws IOException\n{\r\n    jtFs.copyFromLocalFile(originalJarPath, submitJarFile);\r\n    jtFs.setReplication(submitJarFile, replication);\r\n    jtFs.setPermission(submitJarFile, new FsPermission(JobSubmissionFiles.JOB_FILE_PERMISSION));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "addLog4jToDistributedCache",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void addLog4jToDistributedCache(Job job, Path jobSubmitDir) throws IOException\n{\r\n    Configuration conf = job.getConfiguration();\r\n    String log4jPropertyFile = conf.get(MRJobConfig.MAPREDUCE_JOB_LOG4J_PROPERTIES_FILE, \"\");\r\n    if (!log4jPropertyFile.isEmpty()) {\r\n        short replication = (short) conf.getInt(Job.SUBMIT_REPLICATION, 10);\r\n        copyLog4jPropertyFile(job, jobSubmitDir, replication);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "getPathURI",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "URI getPathURI(Path destPath, String fragment) throws URISyntaxException\n{\r\n    URI pathURI = destPath.toUri();\r\n    if (pathURI.getFragment() == null) {\r\n        if (fragment == null) {\r\n        } else {\r\n            pathURI = new URI(pathURI.toString() + \"#\" + fragment);\r\n        }\r\n    }\r\n    return pathURI;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "copyLog4jPropertyFile",
  "errType" : [ "FileNotFoundException", "URISyntaxException" ],
  "containingMethodsNum" : 9,
  "sourceCodeText" : "void copyLog4jPropertyFile(Job job, Path submitJobDir, short replication) throws IOException\n{\r\n    Configuration conf = job.getConfiguration();\r\n    String file = validateFilePath(conf.get(MRJobConfig.MAPREDUCE_JOB_LOG4J_PROPERTIES_FILE), conf);\r\n    LOG.debug(\"default FileSystem: \" + jtFs.getUri());\r\n    FsPermission mapredSysPerms = new FsPermission(JobSubmissionFiles.JOB_DIR_PERMISSION);\r\n    try {\r\n        jtFs.getFileStatus(submitJobDir);\r\n    } catch (FileNotFoundException e) {\r\n        throw new IOException(\"Cannot find job submission directory! \" + \"It should just be created, so something wrong here.\", e);\r\n    }\r\n    Path fileDir = JobSubmissionFiles.getJobLog4jFile(submitJobDir);\r\n    if (file != null) {\r\n        FileSystem.mkdirs(jtFs, fileDir, mapredSysPerms);\r\n        URI tmpURI = null;\r\n        try {\r\n            tmpURI = new URI(file);\r\n        } catch (URISyntaxException e) {\r\n            throw new IllegalArgumentException(e);\r\n        }\r\n        Path tmp = new Path(tmpURI);\r\n        Path newPath = copyRemoteFiles(fileDir, tmp, conf, replication);\r\n        Path path = new Path(newPath.toUri().getPath());\r\n        Job.addFileToClassPath(path, conf, path.getFileSystem(conf));\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 2,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "validateFilePath",
  "errType" : [ "URISyntaxException" ],
  "containingMethodsNum" : 8,
  "sourceCodeText" : "String validateFilePath(String file, Configuration conf) throws IOException\n{\r\n    if (file == null) {\r\n        return null;\r\n    }\r\n    if (file.isEmpty()) {\r\n        throw new IllegalArgumentException(\"File name can't be empty string\");\r\n    }\r\n    String finalPath;\r\n    URI pathURI;\r\n    try {\r\n        pathURI = new URI(file);\r\n    } catch (URISyntaxException e) {\r\n        throw new IllegalArgumentException(e);\r\n    }\r\n    Path path = new Path(pathURI);\r\n    if (pathURI.getScheme() == null) {\r\n        FileSystem localFs = FileSystem.getLocal(conf);\r\n        localFs.getFileStatus(path);\r\n        finalPath = path.makeQualified(localFs.getUri(), localFs.getWorkingDirectory()).toString();\r\n    } else {\r\n        FileSystem fs = path.getFileSystem(conf);\r\n        fs.getFileStatus(path);\r\n        finalPath = path.makeQualified(fs.getUri(), fs.getWorkingDirectory()).toString();\r\n    }\r\n    return finalPath;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "disableErasureCodingForPath",
  "errType" : [ "RemoteException" ],
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void disableErasureCodingForPath(Path path) throws IOException\n{\r\n    try {\r\n        if (jtFs instanceof DistributedFileSystem) {\r\n            LOG.info(\"Disabling Erasure Coding for path: \" + path);\r\n            DistributedFileSystem dfs = (DistributedFileSystem) jtFs;\r\n            dfs.setErasureCodingPolicy(path, SystemErasureCodingPolicies.getReplicationPolicy().getName());\r\n        }\r\n    } catch (RemoteException e) {\r\n        if (!RpcNoSuchMethodException.class.getName().equals(e.getClassName())) {\r\n            throw e;\r\n        } else {\r\n            if (LOG.isDebugEnabled()) {\r\n                LOG.debug(\"Ignore disabling erasure coding for path {} because method \" + \"disableErasureCodingForPath doesn't exist, probably \" + \"talking to a lower version HDFS.\", path.toString(), e);\r\n            }\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred\\lib",
  "methodName" : "map",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void map(K1 key, V1 value, OutputCollector<K2, V2> outputCollector, Reporter reporter) throws IOException\n{\r\n    if (mapper == null) {\r\n        TaggedInputSplit inputSplit = (TaggedInputSplit) reporter.getInputSplit();\r\n        mapper = (Mapper<K1, V1, K2, V2>) ReflectionUtils.newInstance(inputSplit.getMapperClass(), conf);\r\n    }\r\n    mapper.map(key, value, outputCollector, reporter);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred\\lib",
  "methodName" : "configure",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void configure(JobConf conf)\n{\r\n    this.conf = conf;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred\\lib",
  "methodName" : "close",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void close() throws IOException\n{\r\n    if (mapper != null) {\r\n        mapper.close();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\task\\reduce",
  "methodName" : "close",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void close() throws InterruptedException\n{\r\n    closed = true;\r\n    waitForMerge();\r\n    interrupt();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\task\\reduce",
  "methodName" : "startMerge",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void startMerge(Set<T> inputs)\n{\r\n    if (!closed) {\r\n        numPending.incrementAndGet();\r\n        List<T> toMergeInputs = new ArrayList<T>();\r\n        Iterator<T> iter = inputs.iterator();\r\n        for (int ctr = 0; iter.hasNext() && ctr < mergeFactor; ++ctr) {\r\n            toMergeInputs.add(iter.next());\r\n            iter.remove();\r\n        }\r\n        LOG.info(getName() + \": Starting merge with \" + toMergeInputs.size() + \" segments, while ignoring \" + inputs.size() + \" segments\");\r\n        synchronized (pendingToBeMerged) {\r\n            pendingToBeMerged.addLast(toMergeInputs);\r\n            pendingToBeMerged.notifyAll();\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\task\\reduce",
  "methodName" : "waitForMerge",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void waitForMerge() throws InterruptedException\n{\r\n    while (numPending.get() > 0) {\r\n        wait();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\task\\reduce",
  "methodName" : "run",
  "errType" : [ "InterruptedException", "Throwable" ],
  "containingMethodsNum" : 9,
  "sourceCodeText" : "void run()\n{\r\n    while (true) {\r\n        List<T> inputs = null;\r\n        try {\r\n            synchronized (pendingToBeMerged) {\r\n                while (pendingToBeMerged.size() <= 0) {\r\n                    pendingToBeMerged.wait();\r\n                }\r\n                inputs = pendingToBeMerged.removeFirst();\r\n            }\r\n            merge(inputs);\r\n        } catch (InterruptedException ie) {\r\n            numPending.set(0);\r\n            return;\r\n        } catch (Throwable t) {\r\n            numPending.set(0);\r\n            reporter.reportException(t);\r\n            return;\r\n        } finally {\r\n            synchronized (this) {\r\n                numPending.decrementAndGet();\r\n                notifyAll();\r\n            }\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\task\\reduce",
  "methodName" : "merge",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void merge(List<T> inputs) throws IOException",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\counters",
  "methodName" : "addFrameworkGroup",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void addFrameworkGroup(final Class<T> cls)\n{\r\n    updateFrameworkGroupMapping(cls);\r\n    fmap.put(cls.getName(), newFrameworkGroupFactory(cls));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\counters",
  "methodName" : "updateFrameworkGroupMapping",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void updateFrameworkGroupMapping(Class<?> cls)\n{\r\n    String name = cls.getName();\r\n    Integer i = s2i.get(name);\r\n    if (i != null)\r\n        return;\r\n    i2s.add(name);\r\n    s2i.put(name, i2s.size() - 1);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\counters",
  "methodName" : "newFrameworkGroupFactory",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "FrameworkGroupFactory<G> newFrameworkGroupFactory(Class<T> cls)",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\counters",
  "methodName" : "newGroup",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "G newGroup(String name, Limits limits)\n{\r\n    return newGroup(name, ResourceBundles.getCounterGroupName(name, name), limits);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\counters",
  "methodName" : "newGroup",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "G newGroup(String name, String displayName, Limits limits)\n{\r\n    FrameworkGroupFactory<G> gf = fmap.get(name);\r\n    if (gf != null)\r\n        return gf.newGroup(name);\r\n    if (name.equals(FS_GROUP_NAME)) {\r\n        return newFileSystemGroup();\r\n    } else if (s2i.get(name) != null) {\r\n        return newFrameworkGroup(s2i.get(name));\r\n    }\r\n    return newGenericGroup(name, displayName, limits);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\counters",
  "methodName" : "newFrameworkGroup",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "G newFrameworkGroup(int id)\n{\r\n    String name;\r\n    synchronized (CounterGroupFactory.class) {\r\n        if (id < 0 || id >= i2s.size())\r\n            throwBadFrameGroupIdException(id);\r\n        name = i2s.get(id);\r\n    }\r\n    FrameworkGroupFactory<G> gf = fmap.get(name);\r\n    if (gf == null)\r\n        throwBadFrameGroupIdException(id);\r\n    return gf.newGroup(name);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\counters",
  "methodName" : "getFrameworkGroupId",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "int getFrameworkGroupId(String name)\n{\r\n    Integer i = s2i.get(name);\r\n    if (i == null)\r\n        throwBadFrameworkGroupNameException(name);\r\n    return i;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\counters",
  "methodName" : "version",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int version()\n{\r\n    return VERSION;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\counters",
  "methodName" : "isFrameworkGroup",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "boolean isFrameworkGroup(String name)\n{\r\n    return s2i.get(name) != null || name.equals(FS_GROUP_NAME);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\counters",
  "methodName" : "throwBadFrameGroupIdException",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void throwBadFrameGroupIdException(int id)\n{\r\n    throw new IllegalArgumentException(\"bad framework group id: \" + id);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\counters",
  "methodName" : "throwBadFrameworkGroupNameException",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void throwBadFrameworkGroupNameException(String name)\n{\r\n    throw new IllegalArgumentException(\"bad framework group name: \" + name);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\counters",
  "methodName" : "newGenericGroup",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "G newGenericGroup(String name, String displayName, Limits limits)",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\counters",
  "methodName" : "newFileSystemGroup",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "G newFileSystemGroup()",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\input",
  "methodName" : "setup",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void setup(Context context) throws IOException, InterruptedException\n{\r\n    TaggedInputSplit inputSplit = (TaggedInputSplit) context.getInputSplit();\r\n    mapper = (Mapper<K1, V1, K2, V2>) ReflectionUtils.newInstance(inputSplit.getMapperClass(), context.getConfiguration());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\input",
  "methodName" : "run",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void run(Context context) throws IOException, InterruptedException\n{\r\n    setup(context);\r\n    mapper.run(context);\r\n    cleanup(context);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\aggregate",
  "methodName" : "setup",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void setup(Configuration job)\n{\r\n    initializeMySpec(job);\r\n    logSpec();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\aggregate",
  "methodName" : "getValueAggregatorDescriptor",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "ValueAggregatorDescriptor getValueAggregatorDescriptor(String spec, Configuration conf)\n{\r\n    if (spec == null)\r\n        return null;\r\n    String[] segments = spec.split(\",\", -1);\r\n    String type = segments[0];\r\n    if (type.compareToIgnoreCase(\"UserDefined\") == 0) {\r\n        String className = segments[1];\r\n        return new UserDefinedValueAggregatorDescriptor(className, conf);\r\n    }\r\n    return null;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\aggregate",
  "methodName" : "getAggregatorDescriptors",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "ArrayList<ValueAggregatorDescriptor> getAggregatorDescriptors(Configuration conf)\n{\r\n    int num = conf.getInt(DESCRIPTOR_NUM, 0);\r\n    ArrayList<ValueAggregatorDescriptor> retv = new ArrayList<ValueAggregatorDescriptor>(num);\r\n    for (int i = 0; i < num; i++) {\r\n        String spec = conf.get(DESCRIPTOR + \".\" + i);\r\n        ValueAggregatorDescriptor ad = getValueAggregatorDescriptor(spec, conf);\r\n        if (ad != null) {\r\n            retv.add(ad);\r\n        }\r\n    }\r\n    return retv;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\aggregate",
  "methodName" : "initializeMySpec",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void initializeMySpec(Configuration conf)\n{\r\n    aggregatorDescriptorList = getAggregatorDescriptors(conf);\r\n    if (aggregatorDescriptorList.size() == 0) {\r\n        aggregatorDescriptorList.add(new UserDefinedValueAggregatorDescriptor(ValueAggregatorBaseDescriptor.class.getCanonicalName(), conf));\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\aggregate",
  "methodName" : "logSpec",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void logSpec()\n{\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest\\stages",
  "methodName" : "checkOpen",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void checkOpen()\n{\r\n    Preconditions.checkState(!frozen, \"StageConfig is now read-only\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest\\stages",
  "methodName" : "build",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "StageConfig build()\n{\r\n    frozen = true;\r\n    return this;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest\\stages",
  "methodName" : "withDestinationDir",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "StageConfig withDestinationDir(final Path dir)\n{\r\n    destinationDir = dir;\r\n    return this;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest\\stages",
  "methodName" : "withIOStatistics",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "StageConfig withIOStatistics(final IOStatisticsStore store)\n{\r\n    checkOpen();\r\n    iostatistics = store;\r\n    return this;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest\\stages",
  "methodName" : "withIOProcessors",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "StageConfig withIOProcessors(final TaskPool.Submitter value)\n{\r\n    checkOpen();\r\n    ioProcessors = value;\r\n    return this;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest\\stages",
  "methodName" : "withJobAttemptDir",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "StageConfig withJobAttemptDir(final Path dir)\n{\r\n    checkOpen();\r\n    jobAttemptDir = dir;\r\n    return this;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest\\stages",
  "methodName" : "getTaskManifestDir",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Path getTaskManifestDir()\n{\r\n    return taskManifestDir;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest\\stages",
  "methodName" : "withTaskManifestDir",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "StageConfig withTaskManifestDir(Path value)\n{\r\n    checkOpen();\r\n    taskManifestDir = value;\r\n    return this;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest\\stages",
  "methodName" : "withJobAttemptTaskSubDir",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "StageConfig withJobAttemptTaskSubDir(Path value)\n{\r\n    jobAttemptTaskSubDir = value;\r\n    return this;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest\\stages",
  "methodName" : "getJobAttemptTaskSubDir",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Path getJobAttemptTaskSubDir()\n{\r\n    return jobAttemptTaskSubDir;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest\\stages",
  "methodName" : "withJobDirectories",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "StageConfig withJobDirectories(final ManifestCommitterSupport.AttemptDirectories dirs)\n{\r\n    checkOpen();\r\n    withJobAttemptDir(dirs.getJobAttemptDir()).withJobAttemptTaskSubDir(dirs.getJobAttemptTaskSubDir()).withDestinationDir(dirs.getOutputPath()).withOutputTempSubDir(dirs.getOutputTempSubDir()).withTaskManifestDir(dirs.getTaskManifestDir());\r\n    return this;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest\\stages",
  "methodName" : "withJobId",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "StageConfig withJobId(final String value)\n{\r\n    checkOpen();\r\n    jobId = value;\r\n    return this;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest\\stages",
  "methodName" : "getOutputTempSubDir",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Path getOutputTempSubDir()\n{\r\n    return outputTempSubDir;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest\\stages",
  "methodName" : "withOutputTempSubDir",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "StageConfig withOutputTempSubDir(final Path value)\n{\r\n    checkOpen();\r\n    outputTempSubDir = value;\r\n    return this;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest\\stages",
  "methodName" : "withOperations",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "StageConfig withOperations(final ManifestStoreOperations value)\n{\r\n    checkOpen();\r\n    operations = value;\r\n    return this;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest\\stages",
  "methodName" : "withTaskAttemptId",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "StageConfig withTaskAttemptId(final String value)\n{\r\n    checkOpen();\r\n    taskAttemptId = value;\r\n    return this;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest\\stages",
  "methodName" : "withTaskId",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "StageConfig withTaskId(final String value)\n{\r\n    checkOpen();\r\n    taskId = value;\r\n    return this;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest\\stages",
  "methodName" : "withStageEventCallbacks",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "StageConfig withStageEventCallbacks(StageEventCallbacks value)\n{\r\n    checkOpen();\r\n    enterStageEventHandler = value;\r\n    return this;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest\\stages",
  "methodName" : "withProgressable",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "StageConfig withProgressable(final Progressable value)\n{\r\n    checkOpen();\r\n    progressable = value;\r\n    return this;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest\\stages",
  "methodName" : "withTaskAttemptDir",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "StageConfig withTaskAttemptDir(final Path value)\n{\r\n    checkOpen();\r\n    taskAttemptDir = value;\r\n    return this;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest\\stages",
  "methodName" : "withJobAttemptNumber",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "StageConfig withJobAttemptNumber(final int value)\n{\r\n    checkOpen();\r\n    jobAttemptNumber = value;\r\n    return this;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest\\stages",
  "methodName" : "withJobIdSource",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "StageConfig withJobIdSource(final String value)\n{\r\n    checkOpen();\r\n    jobIdSource = value;\r\n    return this;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest\\stages",
  "methodName" : "withName",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "StageConfig withName(String value)\n{\r\n    name = value;\r\n    return this;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest\\stages",
  "methodName" : "getName",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getName()\n{\r\n    return name;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest\\stages",
  "methodName" : "getEnterStageEventHandler",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "StageEventCallbacks getEnterStageEventHandler()\n{\r\n    return enterStageEventHandler;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest\\stages",
  "methodName" : "getIOStatistics",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "IOStatisticsStore getIOStatistics()\n{\r\n    return iostatistics;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest\\stages",
  "methodName" : "getJobId",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getJobId()\n{\r\n    return jobId;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest\\stages",
  "methodName" : "getTaskId",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getTaskId()\n{\r\n    return taskId;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest\\stages",
  "methodName" : "getTaskAttemptId",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getTaskAttemptId()\n{\r\n    return taskAttemptId;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest\\stages",
  "methodName" : "getJobAttemptDir",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Path getJobAttemptDir()\n{\r\n    return jobAttemptDir;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest\\stages",
  "methodName" : "getDestinationDir",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Path getDestinationDir()\n{\r\n    return destinationDir;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest\\stages",
  "methodName" : "getJobSuccessMarkerPath",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Path getJobSuccessMarkerPath()\n{\r\n    return new Path(destinationDir, SUCCESS_MARKER);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest\\stages",
  "methodName" : "getOperations",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "ManifestStoreOperations getOperations()\n{\r\n    return operations;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest\\stages",
  "methodName" : "getIoProcessors",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "TaskPool.Submitter getIoProcessors()\n{\r\n    return ioProcessors;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest\\stages",
  "methodName" : "getProgressable",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Progressable getProgressable()\n{\r\n    return progressable;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest\\stages",
  "methodName" : "getTaskAttemptDir",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Path getTaskAttemptDir()\n{\r\n    return taskAttemptDir;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest\\stages",
  "methodName" : "getJobAttemptNumber",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int getJobAttemptNumber()\n{\r\n    return jobAttemptNumber;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest\\stages",
  "methodName" : "getJobIdSource",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getJobIdSource()\n{\r\n    return jobIdSource;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest\\stages",
  "methodName" : "currentManifestSerializer",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "JsonSerialization<TaskManifest> currentManifestSerializer()\n{\r\n    return threadLocalSerializer.get();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest\\stages",
  "methodName" : "withDeleteTargetPaths",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "StageConfig withDeleteTargetPaths(boolean value)\n{\r\n    checkOpen();\r\n    deleteTargetPaths = value;\r\n    return this;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest\\stages",
  "methodName" : "getDeleteTargetPaths",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean getDeleteTargetPaths()\n{\r\n    return deleteTargetPaths;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest\\stages",
  "methodName" : "enterStage",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void enterStage(String stage)\n{\r\n    if (enterStageEventHandler != null) {\r\n        enterStageEventHandler.enterStage(stage);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest\\stages",
  "methodName" : "exitStage",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void exitStage(String stage)\n{\r\n    if (enterStageEventHandler != null) {\r\n        enterStageEventHandler.exitStage(stage);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred\\lib",
  "methodName" : "getRecordReader",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "RecordReader<LongWritable, Text> getRecordReader(InputSplit genericSplit, JobConf job, Reporter reporter) throws IOException\n{\r\n    reporter.setStatus(genericSplit.toString());\r\n    return new LineRecordReader(job, (FileSplit) genericSplit);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred\\lib",
  "methodName" : "getSplits",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "InputSplit[] getSplits(JobConf job, int numSplits) throws IOException\n{\r\n    ArrayList<FileSplit> splits = new ArrayList<FileSplit>();\r\n    for (FileStatus status : listStatus(job)) {\r\n        for (org.apache.hadoop.mapreduce.lib.input.FileSplit split : org.apache.hadoop.mapreduce.lib.input.NLineInputFormat.getSplitsForFile(status, job, N)) {\r\n            splits.add(new FileSplit(split));\r\n        }\r\n    }\r\n    return splits.toArray(new FileSplit[splits.size()]);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred\\lib",
  "methodName" : "configure",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void configure(JobConf conf)\n{\r\n    N = conf.getInt(\"mapreduce.input.lineinputformat.linespermap\", 1);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred\\lib",
  "methodName" : "createFileSplit",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "FileSplit createFileSplit(Path fileName, long begin, long length)\n{\r\n    return (begin == 0) ? new FileSplit(fileName, begin, length - 1, new String[] {}) : new FileSplit(fileName, begin - 1, length, new String[] {});\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\security",
  "methodName" : "generateHash",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String generateHash(byte[] msg, SecretKey key)\n{\r\n    return new String(Base64.encodeBase64(generateByteHash(msg, key)), Charsets.UTF_8);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\security",
  "methodName" : "generateByteHash",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "byte[] generateByteHash(byte[] msg, SecretKey key)\n{\r\n    return JobTokenSecretManager.computeHash(msg, key);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\security",
  "methodName" : "verifyHash",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "boolean verifyHash(byte[] hash, byte[] msg, SecretKey key)\n{\r\n    byte[] msg_hash = generateByteHash(msg, key);\r\n    return WritableComparator.compareBytes(msg_hash, 0, msg_hash.length, hash, 0, hash.length) == 0;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\security",
  "methodName" : "hashFromString",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String hashFromString(String enc_str, SecretKey key) throws IOException\n{\r\n    return generateHash(enc_str.getBytes(Charsets.UTF_8), key);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\security",
  "methodName" : "verifyReply",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void verifyReply(String base64Hash, String msg, SecretKey key) throws IOException\n{\r\n    byte[] hash = Base64.decodeBase64(base64Hash.getBytes(Charsets.UTF_8));\r\n    boolean res = verifyHash(hash, msg.getBytes(Charsets.UTF_8), key);\r\n    if (res != true) {\r\n        throw new IOException(\"Verification of the hashReply failed\");\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\security",
  "methodName" : "buildMsgFrom",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String buildMsgFrom(URL url)\n{\r\n    return buildMsgFrom(url.getPath(), url.getQuery(), url.getPort());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\security",
  "methodName" : "buildMsgFrom",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String buildMsgFrom(HttpServletRequest request)\n{\r\n    return buildMsgFrom(request.getRequestURI(), request.getQueryString(), request.getLocalPort());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\security",
  "methodName" : "buildMsgFrom",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String buildMsgFrom(String uri_path, String uri_query, int port)\n{\r\n    return String.valueOf(port) + uri_path + \"?\" + uri_query;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\security",
  "methodName" : "toHex",
  "errType" : [ "UnsupportedEncodingException" ],
  "containingMethodsNum" : 2,
  "sourceCodeText" : "String toHex(byte[] ba)\n{\r\n    ByteArrayOutputStream baos = new ByteArrayOutputStream();\r\n    String strHex = \"\";\r\n    try {\r\n        PrintStream ps = new PrintStream(baos, false, \"UTF-8\");\r\n        for (byte b : ba) {\r\n            ps.printf(\"%x\", b);\r\n        }\r\n        strHex = baos.toString(\"UTF-8\");\r\n    } catch (UnsupportedEncodingException e) {\r\n    }\r\n    return strHex;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\db",
  "methodName" : "split",
  "errType" : null,
  "containingMethodsNum" : 10,
  "sourceCodeText" : "List<InputSplit> split(Configuration conf, ResultSet results, String colName) throws SQLException\n{\r\n    List<InputSplit> splits = new ArrayList<InputSplit>();\r\n    if (results.getString(1) == null && results.getString(2) == null) {\r\n        splits.add(new DataDrivenDBInputFormat.DataDrivenDBInputSplit(colName + \" IS NULL\", colName + \" IS NULL\"));\r\n        return splits;\r\n    }\r\n    boolean minVal = results.getBoolean(1);\r\n    boolean maxVal = results.getBoolean(2);\r\n    if (!minVal) {\r\n        splits.add(new DataDrivenDBInputFormat.DataDrivenDBInputSplit(colName + \" = FALSE\", colName + \" = FALSE\"));\r\n    }\r\n    if (maxVal) {\r\n        splits.add(new DataDrivenDBInputFormat.DataDrivenDBInputSplit(colName + \" = TRUE\", colName + \" = TRUE\"));\r\n    }\r\n    if (results.getString(1) == null || results.getString(2) == null) {\r\n        splits.add(new DataDrivenDBInputFormat.DataDrivenDBInputSplit(colName + \" IS NULL\", colName + \" IS NULL\"));\r\n    }\r\n    return splits;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred\\join",
  "methodName" : "combine",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "boolean combine(Object[] srcs, TupleWritable dst)\n{\r\n    assert srcs.length == dst.size();\r\n    for (int i = 0; i < srcs.length; ++i) {\r\n        if (!dst.has(i)) {\r\n            return false;\r\n        }\r\n    }\r\n    return true;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "getDatum",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Object getDatum()\n{\r\n    return datum;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "setDatum",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setDatum(Object datum)\n{\r\n    this.datum = (TaskUpdated) datum;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "getTaskId",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "TaskID getTaskId()\n{\r\n    return TaskID.forName(datum.getTaskid().toString());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "getFinishTime",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "long getFinishTime()\n{\r\n    return datum.getFinishTime();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "getEventType",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "EventType getEventType()\n{\r\n    return EventType.TASK_UPDATED;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "toTimelineEvent",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "TimelineEvent toTimelineEvent()\n{\r\n    TimelineEvent tEvent = new TimelineEvent();\r\n    tEvent.setId(StringUtils.toUpperCase(getEventType().name()));\r\n    tEvent.addInfo(\"FINISH_TIME\", getFinishTime());\r\n    return tEvent;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "getTimelineMetrics",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Set<TimelineMetric> getTimelineMetrics()\n{\r\n    return null;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "mark",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void mark() throws IOException\n{\r\n    baseIterator.mark();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "reset",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void reset() throws IOException\n{\r\n    baseIterator.reset();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "clearMark",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void clearMark() throws IOException\n{\r\n    baseIterator.clearMark();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "hasNext",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "boolean hasNext()\n{\r\n    return baseIterator.hasNext();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "next",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "VALUE next()\n{\r\n    return baseIterator.next();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "remove",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void remove()\n{\r\n    throw new UnsupportedOperationException(\"Remove Not Implemented\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getCredentials",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Credentials getCredentials()\n{\r\n    return credentials;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "setCredentials",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setCredentials(Credentials credentials)\n{\r\n    this.credentials = credentials;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getJar",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String getJar()\n{\r\n    return get(JobContext.JAR);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "setJar",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setJar(String jar)\n{\r\n    set(JobContext.JAR, jar);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getJarUnpackPattern",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Pattern getJarUnpackPattern()\n{\r\n    return getPattern(JobContext.JAR_UNPACK_PATTERN, UNPACK_JAR_PATTERN_DEFAULT);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "setJarByClass",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void setJarByClass(Class cls)\n{\r\n    String jar = ClassUtil.findContainingJar(cls);\r\n    if (jar != null) {\r\n        setJar(jar);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getLocalDirs",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String[] getLocalDirs() throws IOException\n{\r\n    return getTrimmedStrings(MRConfig.LOCAL_DIR);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "deleteLocalFiles",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void deleteLocalFiles() throws IOException\n{\r\n    String[] localDirs = getLocalDirs();\r\n    for (int i = 0; i < localDirs.length; i++) {\r\n        FileSystem.getLocal(this).delete(new Path(localDirs[i]), true);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "deleteLocalFiles",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void deleteLocalFiles(String subdir) throws IOException\n{\r\n    String[] localDirs = getLocalDirs();\r\n    for (int i = 0; i < localDirs.length; i++) {\r\n        FileSystem.getLocal(this).delete(new Path(localDirs[i], subdir), true);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getLocalPath",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Path getLocalPath(String pathString) throws IOException\n{\r\n    return getLocalPath(MRConfig.LOCAL_DIR, pathString);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getUser",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String getUser()\n{\r\n    return get(JobContext.USER_NAME);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "setUser",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setUser(String user)\n{\r\n    set(JobContext.USER_NAME, user);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "setKeepFailedTaskFiles",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setKeepFailedTaskFiles(boolean keep)\n{\r\n    setBoolean(JobContext.PRESERVE_FAILED_TASK_FILES, keep);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getKeepFailedTaskFiles",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "boolean getKeepFailedTaskFiles()\n{\r\n    return getBoolean(JobContext.PRESERVE_FAILED_TASK_FILES, false);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "setKeepTaskFilesPattern",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setKeepTaskFilesPattern(String pattern)\n{\r\n    set(JobContext.PRESERVE_FILES_PATTERN, pattern);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getKeepTaskFilesPattern",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String getKeepTaskFilesPattern()\n{\r\n    return get(JobContext.PRESERVE_FILES_PATTERN);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "setWorkingDirectory",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void setWorkingDirectory(Path dir)\n{\r\n    dir = new Path(getWorkingDirectory(), dir);\r\n    set(JobContext.WORKING_DIR, dir.toString());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getWorkingDirectory",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 3,
  "sourceCodeText" : "Path getWorkingDirectory()\n{\r\n    String name = get(JobContext.WORKING_DIR);\r\n    if (name != null) {\r\n        return new Path(name);\r\n    } else {\r\n        try {\r\n            Path dir = FileSystem.get(this).getWorkingDirectory();\r\n            set(JobContext.WORKING_DIR, dir.toString());\r\n            return dir;\r\n        } catch (IOException e) {\r\n            throw new RuntimeException(e);\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "setNumTasksToExecutePerJvm",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setNumTasksToExecutePerJvm(int numTasks)\n{\r\n    setInt(JobContext.JVM_NUMTASKS_TORUN, numTasks);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getNumTasksToExecutePerJvm",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "int getNumTasksToExecutePerJvm()\n{\r\n    return getInt(JobContext.JVM_NUMTASKS_TORUN, 1);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getInputFormat",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "InputFormat getInputFormat()\n{\r\n    return ReflectionUtils.newInstance(getClass(\"mapred.input.format.class\", TextInputFormat.class, InputFormat.class), this);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "setInputFormat",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setInputFormat(Class<? extends InputFormat> theClass)\n{\r\n    setClass(\"mapred.input.format.class\", theClass, InputFormat.class);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getOutputFormat",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "OutputFormat getOutputFormat()\n{\r\n    return ReflectionUtils.newInstance(getClass(\"mapred.output.format.class\", TextOutputFormat.class, OutputFormat.class), this);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getOutputCommitter",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "OutputCommitter getOutputCommitter()\n{\r\n    return (OutputCommitter) ReflectionUtils.newInstance(getClass(\"mapred.output.committer.class\", FileOutputCommitter.class, OutputCommitter.class), this);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "setOutputCommitter",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setOutputCommitter(Class<? extends OutputCommitter> theClass)\n{\r\n    setClass(\"mapred.output.committer.class\", theClass, OutputCommitter.class);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "setOutputFormat",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setOutputFormat(Class<? extends OutputFormat> theClass)\n{\r\n    setClass(\"mapred.output.format.class\", theClass, OutputFormat.class);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "setCompressMapOutput",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setCompressMapOutput(boolean compress)\n{\r\n    setBoolean(JobContext.MAP_OUTPUT_COMPRESS, compress);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getCompressMapOutput",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "boolean getCompressMapOutput()\n{\r\n    return getBoolean(JobContext.MAP_OUTPUT_COMPRESS, false);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "setMapOutputCompressorClass",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void setMapOutputCompressorClass(Class<? extends CompressionCodec> codecClass)\n{\r\n    setCompressMapOutput(true);\r\n    setClass(JobContext.MAP_OUTPUT_COMPRESS_CODEC, codecClass, CompressionCodec.class);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getMapOutputCompressorClass",
  "errType" : [ "ClassNotFoundException" ],
  "containingMethodsNum" : 2,
  "sourceCodeText" : "Class<? extends CompressionCodec> getMapOutputCompressorClass(Class<? extends CompressionCodec> defaultValue)\n{\r\n    Class<? extends CompressionCodec> codecClass = defaultValue;\r\n    String name = get(JobContext.MAP_OUTPUT_COMPRESS_CODEC);\r\n    if (name != null) {\r\n        try {\r\n            codecClass = getClassByName(name).asSubclass(CompressionCodec.class);\r\n        } catch (ClassNotFoundException e) {\r\n            throw new IllegalArgumentException(\"Compression codec \" + name + \" was not found.\", e);\r\n        }\r\n    }\r\n    return codecClass;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getMapOutputKeyClass",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "Class<?> getMapOutputKeyClass()\n{\r\n    Class<?> retv = getClass(JobContext.MAP_OUTPUT_KEY_CLASS, null, Object.class);\r\n    if (retv == null) {\r\n        retv = getOutputKeyClass();\r\n    }\r\n    return retv;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "setMapOutputKeyClass",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setMapOutputKeyClass(Class<?> theClass)\n{\r\n    setClass(JobContext.MAP_OUTPUT_KEY_CLASS, theClass, Object.class);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getMapOutputValueClass",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "Class<?> getMapOutputValueClass()\n{\r\n    Class<?> retv = getClass(JobContext.MAP_OUTPUT_VALUE_CLASS, null, Object.class);\r\n    if (retv == null) {\r\n        retv = getOutputValueClass();\r\n    }\r\n    return retv;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "setMapOutputValueClass",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setMapOutputValueClass(Class<?> theClass)\n{\r\n    setClass(JobContext.MAP_OUTPUT_VALUE_CLASS, theClass, Object.class);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getOutputKeyClass",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Class<?> getOutputKeyClass()\n{\r\n    return getClass(JobContext.OUTPUT_KEY_CLASS, LongWritable.class, Object.class);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "setOutputKeyClass",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setOutputKeyClass(Class<?> theClass)\n{\r\n    setClass(JobContext.OUTPUT_KEY_CLASS, theClass, Object.class);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getOutputKeyComparator",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "RawComparator getOutputKeyComparator()\n{\r\n    Class<? extends RawComparator> theClass = getClass(JobContext.KEY_COMPARATOR, null, RawComparator.class);\r\n    if (theClass != null)\r\n        return ReflectionUtils.newInstance(theClass, this);\r\n    return WritableComparator.get(getMapOutputKeyClass().asSubclass(WritableComparable.class), this);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "setOutputKeyComparatorClass",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setOutputKeyComparatorClass(Class<? extends RawComparator> theClass)\n{\r\n    setClass(JobContext.KEY_COMPARATOR, theClass, RawComparator.class);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "setKeyFieldComparatorOptions",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void setKeyFieldComparatorOptions(String keySpec)\n{\r\n    setOutputKeyComparatorClass(KeyFieldBasedComparator.class);\r\n    set(KeyFieldBasedComparator.COMPARATOR_OPTIONS, keySpec);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getKeyFieldComparatorOption",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String getKeyFieldComparatorOption()\n{\r\n    return get(KeyFieldBasedComparator.COMPARATOR_OPTIONS);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "setKeyFieldPartitionerOptions",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void setKeyFieldPartitionerOptions(String keySpec)\n{\r\n    setPartitionerClass(KeyFieldBasedPartitioner.class);\r\n    set(KeyFieldBasedPartitioner.PARTITIONER_OPTIONS, keySpec);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getKeyFieldPartitionerOption",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String getKeyFieldPartitionerOption()\n{\r\n    return get(KeyFieldBasedPartitioner.PARTITIONER_OPTIONS);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getCombinerKeyGroupingComparator",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "RawComparator getCombinerKeyGroupingComparator()\n{\r\n    Class<? extends RawComparator> theClass = getClass(JobContext.COMBINER_GROUP_COMPARATOR_CLASS, null, RawComparator.class);\r\n    if (theClass == null) {\r\n        return getOutputKeyComparator();\r\n    }\r\n    return ReflectionUtils.newInstance(theClass, this);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getOutputValueGroupingComparator",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "RawComparator getOutputValueGroupingComparator()\n{\r\n    Class<? extends RawComparator> theClass = getClass(JobContext.GROUP_COMPARATOR_CLASS, null, RawComparator.class);\r\n    if (theClass == null) {\r\n        return getOutputKeyComparator();\r\n    }\r\n    return ReflectionUtils.newInstance(theClass, this);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "setCombinerKeyGroupingComparator",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setCombinerKeyGroupingComparator(Class<? extends RawComparator> theClass)\n{\r\n    setClass(JobContext.COMBINER_GROUP_COMPARATOR_CLASS, theClass, RawComparator.class);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "setOutputValueGroupingComparator",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setOutputValueGroupingComparator(Class<? extends RawComparator> theClass)\n{\r\n    setClass(JobContext.GROUP_COMPARATOR_CLASS, theClass, RawComparator.class);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getUseNewMapper",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "boolean getUseNewMapper()\n{\r\n    return getBoolean(\"mapred.mapper.new-api\", false);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "setUseNewMapper",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setUseNewMapper(boolean flag)\n{\r\n    setBoolean(\"mapred.mapper.new-api\", flag);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getUseNewReducer",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "boolean getUseNewReducer()\n{\r\n    return getBoolean(\"mapred.reducer.new-api\", false);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "setUseNewReducer",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setUseNewReducer(boolean flag)\n{\r\n    setBoolean(\"mapred.reducer.new-api\", flag);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getOutputValueClass",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Class<?> getOutputValueClass()\n{\r\n    return getClass(JobContext.OUTPUT_VALUE_CLASS, Text.class, Object.class);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "setOutputValueClass",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setOutputValueClass(Class<?> theClass)\n{\r\n    setClass(JobContext.OUTPUT_VALUE_CLASS, theClass, Object.class);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getMapperClass",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Class<? extends Mapper> getMapperClass()\n{\r\n    return getClass(\"mapred.mapper.class\", IdentityMapper.class, Mapper.class);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "setMapperClass",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setMapperClass(Class<? extends Mapper> theClass)\n{\r\n    setClass(\"mapred.mapper.class\", theClass, Mapper.class);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getMapRunnerClass",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Class<? extends MapRunnable> getMapRunnerClass()\n{\r\n    return getClass(\"mapred.map.runner.class\", MapRunner.class, MapRunnable.class);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "setMapRunnerClass",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setMapRunnerClass(Class<? extends MapRunnable> theClass)\n{\r\n    setClass(\"mapred.map.runner.class\", theClass, MapRunnable.class);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getPartitionerClass",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Class<? extends Partitioner> getPartitionerClass()\n{\r\n    return getClass(\"mapred.partitioner.class\", HashPartitioner.class, Partitioner.class);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "setPartitionerClass",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setPartitionerClass(Class<? extends Partitioner> theClass)\n{\r\n    setClass(\"mapred.partitioner.class\", theClass, Partitioner.class);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getReducerClass",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Class<? extends Reducer> getReducerClass()\n{\r\n    return getClass(\"mapred.reducer.class\", IdentityReducer.class, Reducer.class);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "setReducerClass",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setReducerClass(Class<? extends Reducer> theClass)\n{\r\n    setClass(\"mapred.reducer.class\", theClass, Reducer.class);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getCombinerClass",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Class<? extends Reducer> getCombinerClass()\n{\r\n    return getClass(\"mapred.combiner.class\", null, Reducer.class);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "setCombinerClass",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setCombinerClass(Class<? extends Reducer> theClass)\n{\r\n    setClass(\"mapred.combiner.class\", theClass, Reducer.class);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getSpeculativeExecution",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "boolean getSpeculativeExecution()\n{\r\n    return (getMapSpeculativeExecution() || getReduceSpeculativeExecution());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "setSpeculativeExecution",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void setSpeculativeExecution(boolean speculativeExecution)\n{\r\n    setMapSpeculativeExecution(speculativeExecution);\r\n    setReduceSpeculativeExecution(speculativeExecution);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getMapSpeculativeExecution",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "boolean getMapSpeculativeExecution()\n{\r\n    return getBoolean(JobContext.MAP_SPECULATIVE, true);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "setMapSpeculativeExecution",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setMapSpeculativeExecution(boolean speculativeExecution)\n{\r\n    setBoolean(JobContext.MAP_SPECULATIVE, speculativeExecution);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getReduceSpeculativeExecution",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "boolean getReduceSpeculativeExecution()\n{\r\n    return getBoolean(JobContext.REDUCE_SPECULATIVE, true);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "setReduceSpeculativeExecution",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setReduceSpeculativeExecution(boolean speculativeExecution)\n{\r\n    setBoolean(JobContext.REDUCE_SPECULATIVE, speculativeExecution);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getNumMapTasks",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "int getNumMapTasks()\n{\r\n    return getInt(JobContext.NUM_MAPS, 1);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "setNumMapTasks",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setNumMapTasks(int n)\n{\r\n    setInt(JobContext.NUM_MAPS, n);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getNumReduceTasks",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "int getNumReduceTasks()\n{\r\n    return getInt(JobContext.NUM_REDUCES, 1);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "setNumReduceTasks",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setNumReduceTasks(int n)\n{\r\n    setInt(JobContext.NUM_REDUCES, n);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getMaxMapAttempts",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "int getMaxMapAttempts()\n{\r\n    return getInt(JobContext.MAP_MAX_ATTEMPTS, 4);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "setMaxMapAttempts",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setMaxMapAttempts(int n)\n{\r\n    setInt(JobContext.MAP_MAX_ATTEMPTS, n);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getMaxReduceAttempts",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "int getMaxReduceAttempts()\n{\r\n    return getInt(JobContext.REDUCE_MAX_ATTEMPTS, 4);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "setMaxReduceAttempts",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setMaxReduceAttempts(int n)\n{\r\n    setInt(JobContext.REDUCE_MAX_ATTEMPTS, n);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getJobName",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String getJobName()\n{\r\n    return get(JobContext.JOB_NAME, \"\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "setJobName",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setJobName(String name)\n{\r\n    set(JobContext.JOB_NAME, name);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getSessionId",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String getSessionId()\n{\r\n    return get(\"session.id\", \"\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "setSessionId",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setSessionId(String sessionId)\n{\r\n    set(\"session.id\", sessionId);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "setMaxTaskFailuresPerTracker",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setMaxTaskFailuresPerTracker(int noFailures)\n{\r\n    setInt(JobContext.MAX_TASK_FAILURES_PER_TRACKER, noFailures);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getMaxTaskFailuresPerTracker",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "int getMaxTaskFailuresPerTracker()\n{\r\n    return getInt(JobContext.MAX_TASK_FAILURES_PER_TRACKER, 3);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getMaxMapTaskFailuresPercent",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "int getMaxMapTaskFailuresPercent()\n{\r\n    return getInt(JobContext.MAP_FAILURES_MAX_PERCENT, 0);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "setMaxMapTaskFailuresPercent",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setMaxMapTaskFailuresPercent(int percent)\n{\r\n    setInt(JobContext.MAP_FAILURES_MAX_PERCENT, percent);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getMaxReduceTaskFailuresPercent",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "int getMaxReduceTaskFailuresPercent()\n{\r\n    return getInt(JobContext.REDUCE_FAILURES_MAXPERCENT, 0);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "setMaxReduceTaskFailuresPercent",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setMaxReduceTaskFailuresPercent(int percent)\n{\r\n    setInt(JobContext.REDUCE_FAILURES_MAXPERCENT, percent);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "setJobPriority",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setJobPriority(JobPriority prio)\n{\r\n    set(JobContext.PRIORITY, prio.toString());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "setJobPriorityAsInteger",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setJobPriorityAsInteger(int prio)\n{\r\n    set(JobContext.PRIORITY, Integer.toString(prio));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getJobPriority",
  "errType" : [ "IllegalArgumentException" ],
  "containingMethodsNum" : 3,
  "sourceCodeText" : "JobPriority getJobPriority()\n{\r\n    String prio = get(JobContext.PRIORITY);\r\n    if (prio == null) {\r\n        return JobPriority.DEFAULT;\r\n    }\r\n    JobPriority priority = JobPriority.DEFAULT;\r\n    try {\r\n        priority = JobPriority.valueOf(prio);\r\n    } catch (IllegalArgumentException e) {\r\n        return convertToJobPriority(Integer.parseInt(prio));\r\n    }\r\n    return priority;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getJobPriorityAsInteger",
  "errType" : [ "IllegalArgumentException" ],
  "containingMethodsNum" : 3,
  "sourceCodeText" : "int getJobPriorityAsInteger()\n{\r\n    String priority = get(JobContext.PRIORITY);\r\n    if (priority == null) {\r\n        return 0;\r\n    }\r\n    int jobPriority = 0;\r\n    try {\r\n        jobPriority = convertPriorityToInteger(priority);\r\n    } catch (IllegalArgumentException e) {\r\n        return Integer.parseInt(priority);\r\n    }\r\n    return jobPriority;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "convertPriorityToInteger",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "int convertPriorityToInteger(String priority)\n{\r\n    JobPriority jobPriority = JobPriority.valueOf(priority);\r\n    switch(jobPriority) {\r\n        case VERY_HIGH:\r\n            return 5;\r\n        case HIGH:\r\n            return 4;\r\n        case NORMAL:\r\n            return 3;\r\n        case LOW:\r\n            return 2;\r\n        case VERY_LOW:\r\n            return 1;\r\n        case DEFAULT:\r\n            return 0;\r\n        default:\r\n            break;\r\n    }\r\n    return 0;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "convertToJobPriority",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "JobPriority convertToJobPriority(int priority)\n{\r\n    switch(priority) {\r\n        case 5:\r\n            return JobPriority.VERY_HIGH;\r\n        case 4:\r\n            return JobPriority.HIGH;\r\n        case 3:\r\n            return JobPriority.NORMAL;\r\n        case 2:\r\n            return JobPriority.LOW;\r\n        case 1:\r\n            return JobPriority.VERY_LOW;\r\n        case 0:\r\n            return JobPriority.DEFAULT;\r\n        default:\r\n            break;\r\n    }\r\n    return JobPriority.UNDEFINED_PRIORITY;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "setJobSubmitHostName",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setJobSubmitHostName(String hostname)\n{\r\n    set(MRJobConfig.JOB_SUBMITHOST, hostname);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getJobSubmitHostName",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String getJobSubmitHostName()\n{\r\n    String hostname = get(MRJobConfig.JOB_SUBMITHOST);\r\n    return hostname;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "setJobSubmitHostAddress",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setJobSubmitHostAddress(String hostadd)\n{\r\n    set(MRJobConfig.JOB_SUBMITHOSTADDR, hostadd);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getJobSubmitHostAddress",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String getJobSubmitHostAddress()\n{\r\n    String hostadd = get(MRJobConfig.JOB_SUBMITHOSTADDR);\r\n    return hostadd;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getProfileEnabled",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "boolean getProfileEnabled()\n{\r\n    return getBoolean(JobContext.TASK_PROFILE, false);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "setProfileEnabled",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setProfileEnabled(boolean newValue)\n{\r\n    setBoolean(JobContext.TASK_PROFILE, newValue);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getProfileParams",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String getProfileParams()\n{\r\n    return get(JobContext.TASK_PROFILE_PARAMS, MRJobConfig.DEFAULT_TASK_PROFILE_PARAMS);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "setProfileParams",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setProfileParams(String value)\n{\r\n    set(JobContext.TASK_PROFILE_PARAMS, value);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getProfileTaskRange",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "IntegerRanges getProfileTaskRange(boolean isMap)\n{\r\n    return getRange((isMap ? JobContext.NUM_MAP_PROFILES : JobContext.NUM_REDUCE_PROFILES), \"0-2\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "setProfileTaskRange",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setProfileTaskRange(boolean isMap, String newValue)\n{\r\n    new Configuration.IntegerRanges(newValue);\r\n    set((isMap ? JobContext.NUM_MAP_PROFILES : JobContext.NUM_REDUCE_PROFILES), newValue);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "setMapDebugScript",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setMapDebugScript(String mDbgScript)\n{\r\n    set(JobContext.MAP_DEBUG_SCRIPT, mDbgScript);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getMapDebugScript",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String getMapDebugScript()\n{\r\n    return get(JobContext.MAP_DEBUG_SCRIPT);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "setReduceDebugScript",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setReduceDebugScript(String rDbgScript)\n{\r\n    set(JobContext.REDUCE_DEBUG_SCRIPT, rDbgScript);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getReduceDebugScript",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String getReduceDebugScript()\n{\r\n    return get(JobContext.REDUCE_DEBUG_SCRIPT);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getJobEndNotificationURI",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String getJobEndNotificationURI()\n{\r\n    return get(JobContext.MR_JOB_END_NOTIFICATION_URL);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "setJobEndNotificationURI",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setJobEndNotificationURI(String uri)\n{\r\n    set(JobContext.MR_JOB_END_NOTIFICATION_URL, uri);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getJobEndNotificationCustomNotifierClass",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String getJobEndNotificationCustomNotifierClass()\n{\r\n    return get(JobContext.MR_JOB_END_NOTIFICATION_CUSTOM_NOTIFIER_CLASS);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "setJobEndNotificationCustomNotifierClass",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setJobEndNotificationCustomNotifierClass(String customNotifierClassName)\n{\r\n    set(JobContext.MR_JOB_END_NOTIFICATION_CUSTOM_NOTIFIER_CLASS, customNotifierClassName);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getJobLocalDir",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String getJobLocalDir()\n{\r\n    return get(JobContext.JOB_LOCAL_DIR);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getMemoryForMapTask",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "long getMemoryForMapTask()\n{\r\n    long value = getDeprecatedMemoryValue();\r\n    if (value < 0) {\r\n        return getMemoryRequired(TaskType.MAP);\r\n    }\r\n    return value;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "setMemoryForMapTask",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void setMemoryForMapTask(long mem)\n{\r\n    setLong(JobConf.MAPREDUCE_JOB_MAP_MEMORY_MB_PROPERTY, mem);\r\n    setLong(JobConf.MAPRED_JOB_MAP_MEMORY_MB_PROPERTY, mem);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getMemoryForReduceTask",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "long getMemoryForReduceTask()\n{\r\n    long value = getDeprecatedMemoryValue();\r\n    if (value < 0) {\r\n        return getMemoryRequired(TaskType.REDUCE);\r\n    }\r\n    return value;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getDeprecatedMemoryValue",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "long getDeprecatedMemoryValue()\n{\r\n    long oldValue = getLong(MAPRED_TASK_MAXVMEM_PROPERTY, DISABLED_MEMORY_LIMIT);\r\n    if (oldValue > 0) {\r\n        oldValue /= (1024 * 1024);\r\n    }\r\n    return oldValue;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "setMemoryForReduceTask",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void setMemoryForReduceTask(long mem)\n{\r\n    setLong(JobConf.MAPREDUCE_JOB_REDUCE_MEMORY_MB_PROPERTY, mem);\r\n    setLong(JobConf.MAPRED_JOB_REDUCE_MEMORY_MB_PROPERTY, mem);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getQueueName",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String getQueueName()\n{\r\n    return get(JobContext.QUEUE_NAME, DEFAULT_QUEUE_NAME);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "setQueueName",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setQueueName(String queueName)\n{\r\n    set(JobContext.QUEUE_NAME, queueName);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "normalizeMemoryConfigValue",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long normalizeMemoryConfigValue(long val)\n{\r\n    if (val < 0) {\r\n        val = DISABLED_MEMORY_LIMIT;\r\n    }\r\n    return val;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "findContainingJar",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String findContainingJar(Class my_class)\n{\r\n    return ClassUtil.findContainingJar(my_class);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getMaxVirtualMemoryForTask",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "long getMaxVirtualMemoryForTask()\n{\r\n    LOG.warn(\"getMaxVirtualMemoryForTask() is deprecated. \" + \"Instead use getMemoryForMapTask() and getMemoryForReduceTask()\");\r\n    long value = getLong(MAPRED_TASK_MAXVMEM_PROPERTY, Math.max(getMemoryForMapTask(), getMemoryForReduceTask()) * 1024 * 1024);\r\n    return value;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "setMaxVirtualMemoryForTask",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void setMaxVirtualMemoryForTask(long vmem)\n{\r\n    LOG.warn(\"setMaxVirtualMemoryForTask() is deprecated.\" + \"Instead use setMemoryForMapTask() and setMemoryForReduceTask()\");\r\n    if (vmem < 0) {\r\n        throw new IllegalArgumentException(\"Task memory allocation may not be < 0\");\r\n    }\r\n    if (get(JobConf.MAPRED_TASK_MAXVMEM_PROPERTY) == null) {\r\n        setMemoryForMapTask(vmem / (1024 * 1024));\r\n        setMemoryForReduceTask(vmem / (1024 * 1024));\r\n    } else {\r\n        this.setLong(JobConf.MAPRED_TASK_MAXVMEM_PROPERTY, vmem);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getMaxPhysicalMemoryForTask",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "long getMaxPhysicalMemoryForTask()\n{\r\n    LOG.warn(\"The API getMaxPhysicalMemoryForTask() is deprecated.\" + \" Refer to the APIs getMemoryForMapTask() and\" + \" getMemoryForReduceTask() for details.\");\r\n    return -1;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "setMaxPhysicalMemoryForTask",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setMaxPhysicalMemoryForTask(long mem)\n{\r\n    LOG.warn(\"The API setMaxPhysicalMemoryForTask() is deprecated.\" + \" The value set is ignored. Refer to \" + \" setMemoryForMapTask() and setMemoryForReduceTask() for details.\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "deprecatedString",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String deprecatedString(String key)\n{\r\n    return \"The variable \" + key + \" is no longer used.\";\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "checkAndWarnDeprecation",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void checkAndWarnDeprecation()\n{\r\n    if (get(JobConf.MAPRED_TASK_MAXVMEM_PROPERTY) != null) {\r\n        LOG.warn(JobConf.deprecatedString(JobConf.MAPRED_TASK_MAXVMEM_PROPERTY) + \" Instead use \" + JobConf.MAPREDUCE_JOB_MAP_MEMORY_MB_PROPERTY + \" and \" + JobConf.MAPREDUCE_JOB_REDUCE_MEMORY_MB_PROPERTY);\r\n    }\r\n    if (get(JobConf.MAPRED_TASK_ULIMIT) != null) {\r\n        LOG.warn(JobConf.deprecatedString(JobConf.MAPRED_TASK_ULIMIT));\r\n    }\r\n    if (get(JobConf.MAPRED_MAP_TASK_ULIMIT) != null) {\r\n        LOG.warn(JobConf.deprecatedString(JobConf.MAPRED_MAP_TASK_ULIMIT));\r\n    }\r\n    if (get(JobConf.MAPRED_REDUCE_TASK_ULIMIT) != null) {\r\n        LOG.warn(JobConf.deprecatedString(JobConf.MAPRED_REDUCE_TASK_ULIMIT));\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getConfiguredTaskJavaOpts",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "String getConfiguredTaskJavaOpts(TaskType taskType)\n{\r\n    String userClasspath = \"\";\r\n    String adminClasspath = \"\";\r\n    if (taskType == TaskType.MAP) {\r\n        userClasspath = get(MAPRED_MAP_TASK_JAVA_OPTS, get(MAPRED_TASK_JAVA_OPTS, DEFAULT_MAPRED_TASK_JAVA_OPTS));\r\n        adminClasspath = get(MRJobConfig.MAPRED_MAP_ADMIN_JAVA_OPTS, MRJobConfig.DEFAULT_MAPRED_ADMIN_JAVA_OPTS);\r\n    } else {\r\n        userClasspath = get(MAPRED_REDUCE_TASK_JAVA_OPTS, get(MAPRED_TASK_JAVA_OPTS, DEFAULT_MAPRED_TASK_JAVA_OPTS));\r\n        adminClasspath = get(MRJobConfig.MAPRED_REDUCE_ADMIN_JAVA_OPTS, MRJobConfig.DEFAULT_MAPRED_ADMIN_JAVA_OPTS);\r\n    }\r\n    return adminClasspath + \" \" + userClasspath;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getTaskJavaOpts",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "String getTaskJavaOpts(TaskType taskType)\n{\r\n    String javaOpts = getConfiguredTaskJavaOpts(taskType);\r\n    if (!javaOpts.contains(\"-Xmx\")) {\r\n        float heapRatio = getFloat(MRJobConfig.HEAP_MEMORY_MB_RATIO, MRJobConfig.DEFAULT_HEAP_MEMORY_MB_RATIO);\r\n        if (heapRatio > 1.0f || heapRatio < 0) {\r\n            LOG.warn(\"Invalid value for \" + MRJobConfig.HEAP_MEMORY_MB_RATIO + \", using the default.\");\r\n            heapRatio = MRJobConfig.DEFAULT_HEAP_MEMORY_MB_RATIO;\r\n        }\r\n        int taskContainerMb = getMemoryRequired(taskType);\r\n        int taskHeapSize = (int) Math.ceil(taskContainerMb * heapRatio);\r\n        String xmxArg = String.format(\"-Xmx%dm\", taskHeapSize);\r\n        LOG.info(\"Task java-opts do not specify heap size. Setting task attempt\" + \" jvm max heap size to \" + xmxArg);\r\n        javaOpts += \" \" + xmxArg;\r\n    }\r\n    return javaOpts;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "parseMaximumHeapSizeMB",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "int parseMaximumHeapSizeMB(String javaOpts)\n{\r\n    Matcher m = JAVA_OPTS_XMX_PATTERN.matcher(javaOpts);\r\n    if (m.matches()) {\r\n        long size = Long.parseLong(m.group(1));\r\n        if (size <= 0) {\r\n            return -1;\r\n        }\r\n        if (m.group(2).isEmpty()) {\r\n            return (int) (size / (1024 * 1024));\r\n        }\r\n        char unit = m.group(2).charAt(0);\r\n        switch(unit) {\r\n            case 'g':\r\n            case 'G':\r\n                return (int) (size * 1024);\r\n            case 'm':\r\n            case 'M':\r\n                return (int) size;\r\n            case 'k':\r\n            case 'K':\r\n                return (int) (size / 1024);\r\n        }\r\n    }\r\n    return -1;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getMemoryRequiredHelper",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "int getMemoryRequiredHelper(String configName, int defaultValue, int heapSize, float heapRatio)\n{\r\n    int memory = getInt(configName, -1);\r\n    if (memory <= 0) {\r\n        if (heapSize > 0) {\r\n            memory = (int) Math.ceil(heapSize / heapRatio);\r\n            LOG.info(\"Figured value for \" + configName + \" from javaOpts\");\r\n        } else {\r\n            memory = defaultValue;\r\n        }\r\n    }\r\n    return memory;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getMemoryRequired",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "int getMemoryRequired(TaskType taskType)\n{\r\n    int memory = 1024;\r\n    int heapSize = parseMaximumHeapSizeMB(getConfiguredTaskJavaOpts(taskType));\r\n    float heapRatio = getFloat(MRJobConfig.HEAP_MEMORY_MB_RATIO, MRJobConfig.DEFAULT_HEAP_MEMORY_MB_RATIO);\r\n    if (taskType == TaskType.MAP) {\r\n        return getMemoryRequiredHelper(MRJobConfig.MAP_MEMORY_MB, MRJobConfig.DEFAULT_MAP_MEMORY_MB, heapSize, heapRatio);\r\n    } else if (taskType == TaskType.REDUCE) {\r\n        return getMemoryRequiredHelper(MRJobConfig.REDUCE_MEMORY_MB, MRJobConfig.DEFAULT_REDUCE_MEMORY_MB, heapSize, heapRatio);\r\n    } else {\r\n        return memory;\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "main",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void main(String[] args) throws Exception\n{\r\n    new JobConf(new Configuration()).writeXml(System.out);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "setQueueName",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setQueueName(String queueName)\n{\r\n    super.setQueueName(queueName);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "setSchedulingInfo",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setSchedulingInfo(String schedulingInfo)\n{\r\n    super.setSchedulingInfo(schedulingInfo);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "setQueueState",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setQueueState(String state)\n{\r\n    super.setState(QueueState.getState(state));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getQueueState",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String getQueueState()\n{\r\n    return super.getState().toString();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "setChildren",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void setChildren(List<JobQueueInfo> children)\n{\r\n    List<QueueInfo> list = new ArrayList<QueueInfo>();\r\n    for (JobQueueInfo q : children) {\r\n        list.add(q);\r\n    }\r\n    super.setQueueChildren(list);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getChildren",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "List<JobQueueInfo> getChildren()\n{\r\n    List<JobQueueInfo> list = new ArrayList<JobQueueInfo>();\r\n    for (QueueInfo q : super.getQueueChildren()) {\r\n        list.add((JobQueueInfo) q);\r\n    }\r\n    return list;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "setProperties",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setProperties(Properties props)\n{\r\n    super.setProperties(props);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "addChild",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void addChild(JobQueueInfo child)\n{\r\n    List<JobQueueInfo> children = getChildren();\r\n    children.add(child);\r\n    setChildren(children);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "removeChild",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void removeChild(JobQueueInfo child)\n{\r\n    List<JobQueueInfo> children = getChildren();\r\n    children.remove(child);\r\n    setChildren(children);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "setJobStatuses",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setJobStatuses(org.apache.hadoop.mapreduce.JobStatus[] stats)\n{\r\n    super.setJobStatuses(stats);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\join",
  "methodName" : "setFormat",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void setFormat(Configuration conf) throws IOException\n{\r\n    addDefaults();\r\n    addUserIdentifiers(conf);\r\n    root = Parser.parse(conf.get(JOIN_EXPR, null), conf);\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\join",
  "methodName" : "addDefaults",
  "errType" : [ "NoSuchMethodException" ],
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void addDefaults()\n{\r\n    try {\r\n        Parser.CNode.addIdentifier(\"inner\", InnerJoinRecordReader.class);\r\n        Parser.CNode.addIdentifier(\"outer\", OuterJoinRecordReader.class);\r\n        Parser.CNode.addIdentifier(\"override\", OverrideRecordReader.class);\r\n        Parser.WNode.addIdentifier(\"tbl\", WrappedRecordReader.class);\r\n    } catch (NoSuchMethodException e) {\r\n        throw new RuntimeException(\"FATAL: Failed to init defaults\", e);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\join",
  "methodName" : "addUserIdentifiers",
  "errType" : [ "NoSuchMethodException" ],
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void addUserIdentifiers(Configuration conf) throws IOException\n{\r\n    Pattern x = Pattern.compile(\"^mapreduce\\\\.join\\\\.define\\\\.(\\\\w+)$\");\r\n    for (Map.Entry<String, String> kv : conf) {\r\n        Matcher m = x.matcher(kv.getKey());\r\n        if (m.matches()) {\r\n            try {\r\n                Parser.CNode.addIdentifier(m.group(1), conf.getClass(m.group(0), null, ComposableRecordReader.class));\r\n            } catch (NoSuchMethodException e) {\r\n                throw new IOException(\"Invalid define for \" + m.group(1), e);\r\n            }\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\join",
  "methodName" : "getSplits",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "List<InputSplit> getSplits(JobContext job) throws IOException, InterruptedException\n{\r\n    setFormat(job.getConfiguration());\r\n    job.getConfiguration().setLong(\"mapreduce.input.fileinputformat.split.minsize\", Long.MAX_VALUE);\r\n    return root.getSplits(job);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\join",
  "methodName" : "createRecordReader",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "RecordReader<K, TupleWritable> createRecordReader(InputSplit split, TaskAttemptContext taskContext) throws IOException, InterruptedException\n{\r\n    setFormat(taskContext.getConfiguration());\r\n    return root.createRecordReader(split, taskContext);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\join",
  "methodName" : "compose",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String compose(Class<? extends InputFormat> inf, String path)\n{\r\n    return compose(inf.getName().intern(), path, new StringBuffer()).toString();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\join",
  "methodName" : "compose",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "String compose(String op, Class<? extends InputFormat> inf, String... path)\n{\r\n    final String infname = inf.getName();\r\n    StringBuffer ret = new StringBuffer(op + '(');\r\n    for (String p : path) {\r\n        compose(infname, p, ret);\r\n        ret.append(',');\r\n    }\r\n    ret.setCharAt(ret.length() - 1, ')');\r\n    return ret.toString();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\join",
  "methodName" : "compose",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "String compose(String op, Class<? extends InputFormat> inf, Path... path)\n{\r\n    ArrayList<String> tmp = new ArrayList<String>(path.length);\r\n    for (Path p : path) {\r\n        tmp.add(p.toString());\r\n    }\r\n    return compose(op, inf, tmp.toArray(new String[0]));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\join",
  "methodName" : "compose",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "StringBuffer compose(String inf, String path, StringBuffer sb)\n{\r\n    sb.append(\"tbl(\" + inf + \",\\\"\");\r\n    sb.append(path);\r\n    sb.append(\"\\\")\");\r\n    return sb;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\task",
  "methodName" : "getTaskAttemptID",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "TaskAttemptID getTaskAttemptID()\n{\r\n    return taskId;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\task",
  "methodName" : "getStatus",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getStatus()\n{\r\n    return status;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\task",
  "methodName" : "getCounter",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Counter getCounter(Enum<?> counterName)\n{\r\n    return reporter.getCounter(counterName);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\task",
  "methodName" : "getCounter",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Counter getCounter(String groupName, String counterName)\n{\r\n    return reporter.getCounter(groupName, counterName);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\task",
  "methodName" : "progress",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void progress()\n{\r\n    reporter.progress();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\task",
  "methodName" : "setStatusString",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setStatusString(String status)\n{\r\n    this.status = status;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\task",
  "methodName" : "setStatus",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void setStatus(String status)\n{\r\n    String normalizedStatus = Task.normalizeStatus(status, conf);\r\n    setStatusString(normalizedStatus);\r\n    reporter.setStatus(normalizedStatus);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\task",
  "methodName" : "getProgress",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "float getProgress()\n{\r\n    return reporter.getProgress();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\task",
  "methodName" : "toString",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "String toString()\n{\r\n    final StringBuilder sb = new StringBuilder(\"TaskAttemptContextImpl{\");\r\n    sb.append(super.toString());\r\n    sb.append(\"; taskId=\").append(taskId);\r\n    sb.append(\", status='\").append(status).append('\\'');\r\n    sb.append('}');\r\n    return sb.toString();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\checkpoint",
  "methodName" : "getNewName",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getNewName()\n{\r\n    return \"checkpoint_\" + name;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "createNotification",
  "errType" : null,
  "containingMethodsNum" : 10,
  "sourceCodeText" : "JobEndStatusInfo createNotification(JobConf conf, JobStatus status)\n{\r\n    JobEndStatusInfo notification = null;\r\n    String uri = conf.getJobEndNotificationURI();\r\n    if (uri != null) {\r\n        int retryAttempts = conf.getInt(JobContext.MR_JOB_END_RETRY_ATTEMPTS, 0);\r\n        long retryInterval = conf.getInt(JobContext.MR_JOB_END_RETRY_INTERVAL, 30000);\r\n        int timeout = conf.getInt(JobContext.MR_JOB_END_NOTIFICATION_TIMEOUT, JobContext.DEFAULT_MR_JOB_END_NOTIFICATION_TIMEOUT);\r\n        if (uri.contains(\"$jobId\")) {\r\n            uri = uri.replace(\"$jobId\", status.getJobID().toString());\r\n        }\r\n        if (uri.contains(\"$jobStatus\")) {\r\n            String statusStr = (status.getRunState() == JobStatus.SUCCEEDED) ? \"SUCCEEDED\" : (status.getRunState() == JobStatus.FAILED) ? \"FAILED\" : \"KILLED\";\r\n            uri = uri.replace(\"$jobStatus\", statusStr);\r\n        }\r\n        notification = new JobEndStatusInfo(uri, retryAttempts, retryInterval, timeout);\r\n    }\r\n    return notification;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "httpNotification",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "int httpNotification(String uri, int timeout) throws IOException, URISyntaxException\n{\r\n    DefaultHttpClient client = new DefaultHttpClient();\r\n    client.getParams().setIntParameter(CoreConnectionPNames.SO_TIMEOUT, timeout).setLongParameter(ClientPNames.CONN_MANAGER_TIMEOUT, (long) timeout);\r\n    HttpGet httpGet = new HttpGet(new URI(uri));\r\n    httpGet.setHeader(\"Accept\", \"*/*\");\r\n    return client.execute(httpGet).getStatusLine().getStatusCode();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "localRunnerNotification",
  "errType" : [ "IOException", "Exception", "InterruptedException" ],
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void localRunnerNotification(JobConf conf, JobStatus status)\n{\r\n    JobEndStatusInfo notification = createNotification(conf, status);\r\n    if (notification != null) {\r\n        do {\r\n            try {\r\n                int code = httpNotification(notification.getUri(), notification.getTimeout());\r\n                if (code != 200) {\r\n                    throw new IOException(\"Invalid response status code: \" + code);\r\n                } else {\r\n                    break;\r\n                }\r\n            } catch (IOException ioex) {\r\n                LOG.error(\"Notification error [\" + notification.getUri() + \"]\", ioex);\r\n            } catch (Exception ex) {\r\n                LOG.error(\"Notification error [\" + notification.getUri() + \"]\", ex);\r\n            }\r\n            try {\r\n                Thread.sleep(notification.getRetryInterval());\r\n            } catch (InterruptedException iex) {\r\n                LOG.error(\"Notification retry error [\" + notification + \"]\", iex);\r\n            }\r\n        } while (notification.configureForRetry());\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 2,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\filecache",
  "methodName" : "setCacheArchives",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setCacheArchives(URI[] archives, Configuration conf)\n{\r\n    Job.setCacheArchives(archives, conf);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\filecache",
  "methodName" : "setCacheFiles",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setCacheFiles(URI[] files, Configuration conf)\n{\r\n    Job.setCacheFiles(files, conf);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\filecache",
  "methodName" : "getCacheArchives",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "URI[] getCacheArchives(Configuration conf) throws IOException\n{\r\n    return JobContextImpl.getCacheArchives(conf);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\filecache",
  "methodName" : "getCacheFiles",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "URI[] getCacheFiles(Configuration conf) throws IOException\n{\r\n    return JobContextImpl.getCacheFiles(conf);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\filecache",
  "methodName" : "getLocalCacheArchives",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Path[] getLocalCacheArchives(Configuration conf) throws IOException\n{\r\n    return JobContextImpl.getLocalCacheArchives(conf);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\filecache",
  "methodName" : "getLocalCacheFiles",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Path[] getLocalCacheFiles(Configuration conf) throws IOException\n{\r\n    return JobContextImpl.getLocalCacheFiles(conf);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\filecache",
  "methodName" : "getArchiveTimestamps",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "long[] getArchiveTimestamps(Configuration conf)\n{\r\n    return JobContextImpl.getArchiveTimestamps(conf);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\filecache",
  "methodName" : "getFileTimestamps",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "long[] getFileTimestamps(Configuration conf)\n{\r\n    return JobContextImpl.getFileTimestamps(conf);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\filecache",
  "methodName" : "addCacheArchive",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void addCacheArchive(URI uri, Configuration conf)\n{\r\n    Job.addCacheArchive(uri, conf);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\filecache",
  "methodName" : "addCacheFile",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void addCacheFile(URI uri, Configuration conf)\n{\r\n    Job.addCacheFile(uri, conf);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\filecache",
  "methodName" : "addFileToClassPath",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void addFileToClassPath(Path file, Configuration conf) throws IOException\n{\r\n    Job.addFileToClassPath(file, conf, file.getFileSystem(conf));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\filecache",
  "methodName" : "addFileToClassPath",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void addFileToClassPath(Path file, Configuration conf, FileSystem fs)\n{\r\n    Job.addFileToClassPath(file, conf, fs, true);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\filecache",
  "methodName" : "addFileToClassPath",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void addFileToClassPath(Path file, Configuration conf, FileSystem fs, boolean addToCache)\n{\r\n    Job.addFileToClassPath(file, conf, fs, addToCache);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\filecache",
  "methodName" : "getFileClassPaths",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Path[] getFileClassPaths(Configuration conf)\n{\r\n    return JobContextImpl.getFileClassPaths(conf);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\filecache",
  "methodName" : "addArchiveToClassPath",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void addArchiveToClassPath(Path archive, Configuration conf) throws IOException\n{\r\n    Job.addArchiveToClassPath(archive, conf, archive.getFileSystem(conf));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\filecache",
  "methodName" : "addArchiveToClassPath",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void addArchiveToClassPath(Path archive, Configuration conf, FileSystem fs) throws IOException\n{\r\n    Job.addArchiveToClassPath(archive, conf, fs);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\filecache",
  "methodName" : "getArchiveClassPaths",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Path[] getArchiveClassPaths(Configuration conf)\n{\r\n    return JobContextImpl.getArchiveClassPaths(conf);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\filecache",
  "methodName" : "createSymlink",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void createSymlink(Configuration conf)\n{\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\filecache",
  "methodName" : "getSymlink",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean getSymlink(Configuration conf)\n{\r\n    return true;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\filecache",
  "methodName" : "parseBooleans",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "boolean[] parseBooleans(String[] strs)\n{\r\n    if (null == strs) {\r\n        return null;\r\n    }\r\n    boolean[] result = new boolean[strs.length];\r\n    for (int i = 0; i < strs.length; ++i) {\r\n        result[i] = Boolean.parseBoolean(strs[i]);\r\n    }\r\n    return result;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\filecache",
  "methodName" : "getFileVisibilities",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "boolean[] getFileVisibilities(Configuration conf)\n{\r\n    return parseBooleans(conf.getStrings(MRJobConfig.CACHE_FILE_VISIBILITIES));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\filecache",
  "methodName" : "getArchiveVisibilities",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "boolean[] getArchiveVisibilities(Configuration conf)\n{\r\n    return parseBooleans(conf.getStrings(MRJobConfig.CACHE_ARCHIVES_VISIBILITIES));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\filecache",
  "methodName" : "checkURIs",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "boolean checkURIs(URI[] uriFiles, URI[] uriArchives)\n{\r\n    if ((uriFiles == null) && (uriArchives == null)) {\r\n        return true;\r\n    }\r\n    Set<String> fragments = new HashSet<String>();\r\n    if (uriFiles != null) {\r\n        for (int i = 0; i < uriFiles.length; i++) {\r\n            String fragment = uriFiles[i].getFragment();\r\n            if (fragment == null) {\r\n                return false;\r\n            }\r\n            String lowerCaseFragment = StringUtils.toLowerCase(fragment);\r\n            if (fragments.contains(lowerCaseFragment)) {\r\n                return false;\r\n            }\r\n            fragments.add(lowerCaseFragment);\r\n        }\r\n    }\r\n    if (uriArchives != null) {\r\n        for (int i = 0; i < uriArchives.length; i++) {\r\n            String fragment = uriArchives[i].getFragment();\r\n            if (fragment == null) {\r\n                return false;\r\n            }\r\n            String lowerCaseFragment = StringUtils.toLowerCase(fragment);\r\n            if (fragments.contains(lowerCaseFragment)) {\r\n                return false;\r\n            }\r\n            fragments.add(lowerCaseFragment);\r\n        }\r\n    }\r\n    return true;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "configure",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void configure(JobConf conf)\n{\r\n    compressionCodecs = new CompressionCodecFactory(conf);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "isSplitable",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "boolean isSplitable(FileSystem fs, Path file)\n{\r\n    final CompressionCodec codec = compressionCodecs.getCodec(file);\r\n    if (null == codec) {\r\n        return true;\r\n    }\r\n    return codec instanceof SplittableCompressionCodec;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getRecordReader",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "RecordReader<Text, Text> getRecordReader(InputSplit genericSplit, JobConf job, Reporter reporter) throws IOException\n{\r\n    reporter.setStatus(genericSplit.toString());\r\n    return new KeyValueLineRecordReader(job, (FileSplit) genericSplit);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\counters",
  "methodName" : "getName",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String getName()\n{\r\n    return FileSystemCounter.class.getName();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\counters",
  "methodName" : "getDisplayName",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String getDisplayName()\n{\r\n    if (displayName == null) {\r\n        displayName = ResourceBundles.getCounterGroupName(getName(), \"File System Counters\");\r\n    }\r\n    return displayName;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\counters",
  "methodName" : "setDisplayName",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setDisplayName(String displayName)\n{\r\n    this.displayName = displayName;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\counters",
  "methodName" : "addCounter",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void addCounter(C counter)\n{\r\n    C ours;\r\n    if (counter instanceof FileSystemCounterGroup.FSCounter) {\r\n        FSCounter c = (FSCounter) counter;\r\n        ours = findCounter(c.scheme, c.key);\r\n    } else {\r\n        ours = findCounter(counter.getName());\r\n    }\r\n    if (ours != null) {\r\n        ours.setValue(counter.getValue());\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\counters",
  "methodName" : "addCounter",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "C addCounter(String name, String displayName, long value)\n{\r\n    C counter = findCounter(name);\r\n    if (counter != null) {\r\n        counter.setValue(value);\r\n    }\r\n    return counter;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\counters",
  "methodName" : "parseCounterName",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "String[] parseCounterName(String counterName)\n{\r\n    int schemeEnd = counterName.indexOf('_');\r\n    if (schemeEnd < 0) {\r\n        throw new IllegalArgumentException(\"bad fs counter name\");\r\n    }\r\n    return new String[] { counterName.substring(0, schemeEnd), counterName.substring(schemeEnd + 1) };\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\counters",
  "methodName" : "findCounter",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "C findCounter(String counterName, String displayName)\n{\r\n    return findCounter(counterName);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\counters",
  "methodName" : "findCounter",
  "errType" : [ "Exception" ],
  "containingMethodsNum" : 3,
  "sourceCodeText" : "C findCounter(String counterName, boolean create)\n{\r\n    try {\r\n        String[] pair = parseCounterName(counterName);\r\n        return findCounter(pair[0], FileSystemCounter.valueOf(pair[1]));\r\n    } catch (Exception e) {\r\n        if (create)\r\n            throw new IllegalArgumentException(e);\r\n        LOG.warn(counterName + \" is not a recognized counter.\");\r\n        return null;\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\counters",
  "methodName" : "findCounter",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "C findCounter(String counterName)\n{\r\n    return findCounter(counterName, false);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\counters",
  "methodName" : "findCounter",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "C findCounter(String scheme, FileSystemCounter key)\n{\r\n    final String canonicalScheme = checkScheme(scheme);\r\n    if (map == null) {\r\n        map = new ConcurrentSkipListMap<>();\r\n    }\r\n    Object[] counters = map.get(canonicalScheme);\r\n    int ord = key.ordinal();\r\n    if (counters == null) {\r\n        counters = new Object[FileSystemCounter.values().length];\r\n        map.put(canonicalScheme, counters);\r\n        counters[ord] = newCounter(canonicalScheme, key);\r\n    } else if (counters[ord] == null) {\r\n        counters[ord] = newCounter(canonicalScheme, key);\r\n    }\r\n    return (C) counters[ord];\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\counters",
  "methodName" : "checkScheme",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "String checkScheme(String scheme)\n{\r\n    String fixed = StringUtils.toUpperCase(scheme);\r\n    String interned = schemes.putIfAbsent(fixed, fixed);\r\n    if (schemes.size() > MAX_NUM_SCHEMES) {\r\n        throw new IllegalArgumentException(\"too many schemes? \" + schemes.size() + \" when process scheme: \" + scheme);\r\n    }\r\n    return interned == null ? fixed : interned;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\counters",
  "methodName" : "newCounter",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "C newCounter(String scheme, FileSystemCounter key)",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\counters",
  "methodName" : "size",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "int size()\n{\r\n    int n = 0;\r\n    if (map != null) {\r\n        for (Object[] counters : map.values()) {\r\n            n += numSetCounters(counters);\r\n        }\r\n    }\r\n    return n;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\counters",
  "methodName" : "incrAllCounters",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void incrAllCounters(CounterGroupBase<C> other)\n{\r\n    if (checkNotNull(other.getUnderlyingGroup(), \"other group\") instanceof FileSystemCounterGroup<?>) {\r\n        for (Counter counter : other) {\r\n            FSCounter c = (FSCounter) ((Counter) counter).getUnderlyingCounter();\r\n            findCounter(c.scheme, c.key).increment(counter.getValue());\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\counters",
  "methodName" : "write",
  "errType" : null,
  "containingMethodsNum" : 9,
  "sourceCodeText" : "void write(DataOutput out) throws IOException\n{\r\n    if (map != null) {\r\n        WritableUtils.writeVInt(out, map.size());\r\n        for (Map.Entry<String, Object[]> entry : map.entrySet()) {\r\n            WritableUtils.writeString(out, entry.getKey());\r\n            WritableUtils.writeVInt(out, numSetCounters(entry.getValue()));\r\n            for (Object counter : entry.getValue()) {\r\n                if (counter == null)\r\n                    continue;\r\n                @SuppressWarnings(\"unchecked\")\r\n                FSCounter c = (FSCounter) ((Counter) counter).getUnderlyingCounter();\r\n                WritableUtils.writeVInt(out, c.key.ordinal());\r\n                WritableUtils.writeVLong(out, c.getValue());\r\n            }\r\n        }\r\n    } else {\r\n        WritableUtils.writeVInt(out, 0);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\counters",
  "methodName" : "numSetCounters",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int numSetCounters(Object[] counters)\n{\r\n    int n = 0;\r\n    for (Object counter : counters) if (counter != null)\r\n        ++n;\r\n    return n;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\counters",
  "methodName" : "readFields",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void readFields(DataInput in) throws IOException\n{\r\n    int numSchemes = WritableUtils.readVInt(in);\r\n    FileSystemCounter[] enums = FileSystemCounter.values();\r\n    for (int i = 0; i < numSchemes; ++i) {\r\n        String scheme = WritableUtils.readString(in);\r\n        int numCounters = WritableUtils.readVInt(in);\r\n        for (int j = 0; j < numCounters; ++j) {\r\n            findCounter(scheme, enums[WritableUtils.readVInt(in)]).setValue(WritableUtils.readVLong(in));\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\counters",
  "methodName" : "iterator",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "Iterator<C> iterator()\n{\r\n    return new AbstractIterator<C>() {\r\n\r\n        Iterator<Object[]> it = map != null ? map.values().iterator() : null;\r\n\r\n        Object[] counters = (it != null && it.hasNext()) ? it.next() : null;\r\n\r\n        int i = 0;\r\n\r\n        @Override\r\n        protected C computeNext() {\r\n            while (counters != null) {\r\n                while (i < counters.length) {\r\n                    @SuppressWarnings(\"unchecked\")\r\n                    C counter = (C) counters[i++];\r\n                    if (counter != null)\r\n                        return counter;\r\n                }\r\n                i = 0;\r\n                counters = (it != null && it.hasNext()) ? it.next() : null;\r\n            }\r\n            return endOfData();\r\n        }\r\n    };\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\counters",
  "methodName" : "equals",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "boolean equals(Object genericRight)\n{\r\n    if (genericRight instanceof CounterGroupBase<?>) {\r\n        @SuppressWarnings(\"unchecked\")\r\n        CounterGroupBase<C> right = (CounterGroupBase<C>) genericRight;\r\n        return Iterators.elementsEqual(iterator(), right.iterator());\r\n    }\r\n    return false;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\counters",
  "methodName" : "hashCode",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "int hashCode()\n{\r\n    int hash = FileSystemCounter.class.hashCode();\r\n    if (map != null) {\r\n        for (Object[] counters : map.values()) {\r\n            if (counters != null)\r\n                hash ^= Arrays.hashCode(counters);\r\n        }\r\n    }\r\n    return hash;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "getDatum",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Object getDatum()\n{\r\n    return datum;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "setDatum",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setDatum(Object datum)\n{\r\n    this.datum = (JobUnsuccessfulCompletion) datum;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "getJobId",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "JobID getJobId()\n{\r\n    return JobID.forName(datum.getJobid().toString());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "getFinishTime",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "long getFinishTime()\n{\r\n    return datum.getFinishTime();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "getSucceededMaps",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "int getSucceededMaps()\n{\r\n    return datum.getFinishedMaps();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "getSucceededReduces",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "int getSucceededReduces()\n{\r\n    return datum.getFinishedReduces();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "getFailedMaps",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "int getFailedMaps()\n{\r\n    return datum.getFailedMaps();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "getFailedReduces",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "int getFailedReduces()\n{\r\n    return datum.getFailedReduces();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "getKilledMaps",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "int getKilledMaps()\n{\r\n    return datum.getKilledMaps();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "getKilledReduces",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "int getKilledReduces()\n{\r\n    return datum.getKilledReduces();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "getStatus",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String getStatus()\n{\r\n    return datum.getJobStatus().toString();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "getEventType",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "EventType getEventType()\n{\r\n    if (\"FAILED\".equals(getStatus())) {\r\n        return EventType.JOB_FAILED;\r\n    } else if (\"ERROR\".equals(getStatus())) {\r\n        return EventType.JOB_ERROR;\r\n    } else\r\n        return EventType.JOB_KILLED;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "getDiagnostics",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "String getDiagnostics()\n{\r\n    final CharSequence diagnostics = datum.getDiagnostics();\r\n    return diagnostics == null ? NODIAGS : diagnostics.toString();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "toTimelineEvent",
  "errType" : null,
  "containingMethodsNum" : 12,
  "sourceCodeText" : "TimelineEvent toTimelineEvent()\n{\r\n    TimelineEvent tEvent = new TimelineEvent();\r\n    tEvent.setId(StringUtils.toUpperCase(getEventType().name()));\r\n    tEvent.addInfo(\"FINISH_TIME\", getFinishTime());\r\n    tEvent.addInfo(\"NUM_MAPS\", getSucceededMaps() + getFailedMaps() + getKilledMaps());\r\n    tEvent.addInfo(\"NUM_REDUCES\", getSucceededReduces() + getFailedReduces() + getKilledReduces());\r\n    tEvent.addInfo(\"JOB_STATUS\", getStatus());\r\n    tEvent.addInfo(\"DIAGNOSTICS\", getDiagnostics());\r\n    tEvent.addInfo(\"SUCCESSFUL_MAPS\", getSucceededMaps());\r\n    tEvent.addInfo(\"SUCCESSFUL_REDUCES\", getSucceededReduces());\r\n    tEvent.addInfo(\"FAILED_MAPS\", getFailedMaps());\r\n    tEvent.addInfo(\"FAILED_REDUCES\", getFailedReduces());\r\n    tEvent.addInfo(\"KILLED_MAPS\", getKilledMaps());\r\n    tEvent.addInfo(\"KILLED_REDUCES\", getKilledReduces());\r\n    return tEvent;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "getTimelineMetrics",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Set<TimelineMetric> getTimelineMetrics()\n{\r\n    return null;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\task",
  "methodName" : "getConfiguration",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Configuration getConfiguration()\n{\r\n    return conf;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\task",
  "methodName" : "getJobID",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "JobID getJobID()\n{\r\n    return jobId;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\task",
  "methodName" : "setJobID",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setJobID(JobID jobId)\n{\r\n    this.jobId = jobId;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\task",
  "methodName" : "getNumReduceTasks",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "int getNumReduceTasks()\n{\r\n    return conf.getNumReduceTasks();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\task",
  "methodName" : "getWorkingDirectory",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Path getWorkingDirectory() throws IOException\n{\r\n    return conf.getWorkingDirectory();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\task",
  "methodName" : "getOutputKeyClass",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Class<?> getOutputKeyClass()\n{\r\n    return conf.getOutputKeyClass();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\task",
  "methodName" : "getOutputValueClass",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Class<?> getOutputValueClass()\n{\r\n    return conf.getOutputValueClass();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\task",
  "methodName" : "getMapOutputKeyClass",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Class<?> getMapOutputKeyClass()\n{\r\n    return conf.getMapOutputKeyClass();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\task",
  "methodName" : "getMapOutputValueClass",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Class<?> getMapOutputValueClass()\n{\r\n    return conf.getMapOutputValueClass();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\task",
  "methodName" : "getJobName",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String getJobName()\n{\r\n    return conf.getJobName();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\task",
  "methodName" : "getInputFormatClass",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Class<? extends InputFormat<?, ?>> getInputFormatClass() throws ClassNotFoundException\n{\r\n    return (Class<? extends InputFormat<?, ?>>) conf.getClass(INPUT_FORMAT_CLASS_ATTR, TextInputFormat.class);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\task",
  "methodName" : "getMapperClass",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Class<? extends Mapper<?, ?, ?, ?>> getMapperClass() throws ClassNotFoundException\n{\r\n    return (Class<? extends Mapper<?, ?, ?, ?>>) conf.getClass(MAP_CLASS_ATTR, Mapper.class);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\task",
  "methodName" : "getCombinerClass",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Class<? extends Reducer<?, ?, ?, ?>> getCombinerClass() throws ClassNotFoundException\n{\r\n    return (Class<? extends Reducer<?, ?, ?, ?>>) conf.getClass(COMBINE_CLASS_ATTR, null);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\task",
  "methodName" : "getReducerClass",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Class<? extends Reducer<?, ?, ?, ?>> getReducerClass() throws ClassNotFoundException\n{\r\n    return (Class<? extends Reducer<?, ?, ?, ?>>) conf.getClass(REDUCE_CLASS_ATTR, Reducer.class);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\task",
  "methodName" : "getOutputFormatClass",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Class<? extends OutputFormat<?, ?>> getOutputFormatClass() throws ClassNotFoundException\n{\r\n    return (Class<? extends OutputFormat<?, ?>>) conf.getClass(OUTPUT_FORMAT_CLASS_ATTR, TextOutputFormat.class);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\task",
  "methodName" : "getPartitionerClass",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Class<? extends Partitioner<?, ?>> getPartitionerClass() throws ClassNotFoundException\n{\r\n    return (Class<? extends Partitioner<?, ?>>) conf.getClass(PARTITIONER_CLASS_ATTR, HashPartitioner.class);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\task",
  "methodName" : "getSortComparator",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "RawComparator<?> getSortComparator()\n{\r\n    return conf.getOutputKeyComparator();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\task",
  "methodName" : "getJar",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String getJar()\n{\r\n    return conf.getJar();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\task",
  "methodName" : "getCombinerKeyGroupingComparator",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "RawComparator<?> getCombinerKeyGroupingComparator()\n{\r\n    return conf.getCombinerKeyGroupingComparator();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\task",
  "methodName" : "getGroupingComparator",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "RawComparator<?> getGroupingComparator()\n{\r\n    return conf.getOutputValueGroupingComparator();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\task",
  "methodName" : "getJobSetupCleanupNeeded",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "boolean getJobSetupCleanupNeeded()\n{\r\n    return conf.getBoolean(MRJobConfig.SETUP_CLEANUP_NEEDED, true);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\task",
  "methodName" : "getTaskCleanupNeeded",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "boolean getTaskCleanupNeeded()\n{\r\n    return conf.getBoolean(MRJobConfig.TASK_CLEANUP_NEEDED, true);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\task",
  "methodName" : "getSymlink",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "boolean getSymlink()\n{\r\n    return DistributedCache.getSymlink(conf);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\task",
  "methodName" : "getArchiveClassPaths",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Path[] getArchiveClassPaths()\n{\r\n    return getArchiveClassPaths(conf);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\task",
  "methodName" : "getArchiveClassPaths",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "Path[] getArchiveClassPaths(Configuration conf)\n{\r\n    ArrayList<String> list = (ArrayList<String>) conf.getStringCollection(MRJobConfig.CLASSPATH_ARCHIVES);\r\n    if (list.size() == 0) {\r\n        return null;\r\n    }\r\n    Path[] paths = new Path[list.size()];\r\n    for (int i = 0; i < list.size(); i++) {\r\n        paths[i] = new Path(list.get(i));\r\n    }\r\n    return paths;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\task",
  "methodName" : "getCacheArchives",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "URI[] getCacheArchives() throws IOException\n{\r\n    return getCacheArchives(conf);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\task",
  "methodName" : "getCacheArchives",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "URI[] getCacheArchives(Configuration conf)\n{\r\n    return StringUtils.stringToURI(conf.getStrings(MRJobConfig.CACHE_ARCHIVES));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\task",
  "methodName" : "getCacheFiles",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "URI[] getCacheFiles() throws IOException\n{\r\n    return getCacheFiles(conf);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\task",
  "methodName" : "getCacheFiles",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "URI[] getCacheFiles(Configuration conf)\n{\r\n    return StringUtils.stringToURI(conf.getStrings(MRJobConfig.CACHE_FILES));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\task",
  "methodName" : "getLocalCacheArchives",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Path[] getLocalCacheArchives() throws IOException\n{\r\n    return getLocalCacheArchives(conf);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\task",
  "methodName" : "getLocalCacheArchives",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Path[] getLocalCacheArchives(Configuration conf)\n{\r\n    return StringUtils.stringToPath(conf.getStrings(MRJobConfig.CACHE_LOCALARCHIVES));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\task",
  "methodName" : "getLocalCacheFiles",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Path[] getLocalCacheFiles() throws IOException\n{\r\n    return getLocalCacheFiles(conf);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\task",
  "methodName" : "getLocalCacheFiles",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Path[] getLocalCacheFiles(Configuration conf)\n{\r\n    return StringUtils.stringToPath(conf.getStrings(MRJobConfig.CACHE_LOCALFILES));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\task",
  "methodName" : "parseTimestamps",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "long[] parseTimestamps(String[] strs)\n{\r\n    if (strs == null) {\r\n        return null;\r\n    }\r\n    long[] result = new long[strs.length];\r\n    for (int i = 0; i < strs.length; ++i) {\r\n        result[i] = Long.parseLong(strs[i]);\r\n    }\r\n    return result;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\task",
  "methodName" : "getArchiveTimestamps",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "long[] getArchiveTimestamps(Configuration conf)\n{\r\n    return parseTimestamps(conf.getStrings(MRJobConfig.CACHE_ARCHIVES_TIMESTAMPS));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\task",
  "methodName" : "getFileTimestamps",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "long[] getFileTimestamps(Configuration conf)\n{\r\n    return parseTimestamps(conf.getStrings(MRJobConfig.CACHE_FILE_TIMESTAMPS));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\task",
  "methodName" : "getFileClassPaths",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Path[] getFileClassPaths()\n{\r\n    return getFileClassPaths(conf);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\task",
  "methodName" : "getFileClassPaths",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "Path[] getFileClassPaths(Configuration conf)\n{\r\n    ArrayList<String> list = (ArrayList<String>) conf.getStringCollection(MRJobConfig.CLASSPATH_FILES);\r\n    if (list.size() == 0) {\r\n        return null;\r\n    }\r\n    Path[] paths = new Path[list.size()];\r\n    for (int i = 0; i < list.size(); i++) {\r\n        paths[i] = new Path(list.get(i));\r\n    }\r\n    return paths;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\task",
  "methodName" : "toTimestampStrs",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String[] toTimestampStrs(long[] timestamps)\n{\r\n    if (timestamps == null) {\r\n        return null;\r\n    }\r\n    String[] result = new String[timestamps.length];\r\n    for (int i = 0; i < timestamps.length; ++i) {\r\n        result[i] = Long.toString(timestamps[i]);\r\n    }\r\n    return result;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\task",
  "methodName" : "getArchiveTimestamps",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String[] getArchiveTimestamps()\n{\r\n    return toTimestampStrs(getArchiveTimestamps(conf));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\task",
  "methodName" : "getFileTimestamps",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String[] getFileTimestamps()\n{\r\n    return toTimestampStrs(getFileTimestamps(conf));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\task",
  "methodName" : "getMaxMapAttempts",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "int getMaxMapAttempts()\n{\r\n    return conf.getMaxMapAttempts();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\task",
  "methodName" : "getMaxReduceAttempts",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "int getMaxReduceAttempts()\n{\r\n    return conf.getMaxReduceAttempts();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\task",
  "methodName" : "getProfileEnabled",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "boolean getProfileEnabled()\n{\r\n    return conf.getProfileEnabled();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\task",
  "methodName" : "getProfileParams",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String getProfileParams()\n{\r\n    return conf.getProfileParams();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\task",
  "methodName" : "getProfileTaskRange",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "IntegerRanges getProfileTaskRange(boolean isMap)\n{\r\n    return conf.getProfileTaskRange(isMap);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\task",
  "methodName" : "getUser",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String getUser()\n{\r\n    return conf.getUser();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\task",
  "methodName" : "getCredentials",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Credentials getCredentials()\n{\r\n    return credentials;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\task",
  "methodName" : "toString",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "String toString()\n{\r\n    final StringBuilder sb = new StringBuilder(\"JobContextImpl{\");\r\n    sb.append(\"jobId=\").append(jobId);\r\n    sb.append('}');\r\n    return sb.toString();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "burst",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "int[][] burst()\n{\r\n    int[][] result = new int[4][];\r\n    result[WALLCLOCK_TIME_INDEX] = progressWallclockTime.getValues();\r\n    result[CPU_TIME_INDEX] = progressCPUTime.getValues();\r\n    result[VIRTUAL_MEMORY_KBYTES_INDEX] = progressVirtualMemoryKbytes.getValues();\r\n    result[PHYSICAL_MEMORY_KBYTES_INDEX] = progressPhysicalMemoryKbytes.getValues();\r\n    return result;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "arrayGet",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int[] arrayGet(int[][] burstedBlock, int index)\n{\r\n    return burstedBlock == null ? NULL_ARRAY : burstedBlock[index];\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "arrayGetWallclockTime",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "int[] arrayGetWallclockTime(int[][] burstedBlock)\n{\r\n    return arrayGet(burstedBlock, WALLCLOCK_TIME_INDEX);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "arrayGetCPUTime",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "int[] arrayGetCPUTime(int[][] burstedBlock)\n{\r\n    return arrayGet(burstedBlock, CPU_TIME_INDEX);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "arrayGetVMemKbytes",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "int[] arrayGetVMemKbytes(int[][] burstedBlock)\n{\r\n    return arrayGet(burstedBlock, VIRTUAL_MEMORY_KBYTES_INDEX);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "arrayGetPhysMemKbytes",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "int[] arrayGetPhysMemKbytes(int[][] burstedBlock)\n{\r\n    return arrayGet(burstedBlock, PHYSICAL_MEMORY_KBYTES_INDEX);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "listStatus",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "FileStatus[] listStatus(JobConf job) throws IOException\n{\r\n    FileStatus[] files = super.listStatus(job);\r\n    for (int i = 0; i < files.length; i++) {\r\n        FileStatus file = files[i];\r\n        if (file.isDirectory()) {\r\n            Path dataFile = new Path(file.getPath(), MapFile.DATA_FILE_NAME);\r\n            FileSystem fs = file.getPath().getFileSystem(job);\r\n            files[i] = fs.getFileStatus(dataFile);\r\n        }\r\n    }\r\n    return files;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getRecordReader",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "RecordReader<K, V> getRecordReader(InputSplit split, JobConf job, Reporter reporter) throws IOException\n{\r\n    reporter.setStatus(split.toString());\r\n    return new SequenceFileRecordReader<K, V>(job, (FileSplit) split);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\db",
  "methodName" : "checkOutputSpecs",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void checkOutputSpecs(JobContext context) throws IOException, InterruptedException\n{\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\db",
  "methodName" : "getOutputCommitter",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "OutputCommitter getOutputCommitter(TaskAttemptContext context) throws IOException, InterruptedException\n{\r\n    return new FileOutputCommitter(FileOutputFormat.getOutputPath(context), context);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\db",
  "methodName" : "constructQuery",
  "errType" : null,
  "containingMethodsNum" : 13,
  "sourceCodeText" : "String constructQuery(String table, String[] fieldNames)\n{\r\n    if (fieldNames == null) {\r\n        throw new IllegalArgumentException(\"Field names may not be null\");\r\n    }\r\n    StringBuilder query = new StringBuilder();\r\n    query.append(\"INSERT INTO \").append(table);\r\n    if (fieldNames.length > 0 && fieldNames[0] != null) {\r\n        query.append(\" (\");\r\n        for (int i = 0; i < fieldNames.length; i++) {\r\n            query.append(fieldNames[i]);\r\n            if (i != fieldNames.length - 1) {\r\n                query.append(\",\");\r\n            }\r\n        }\r\n        query.append(\")\");\r\n    }\r\n    query.append(\" VALUES (\");\r\n    for (int i = 0; i < fieldNames.length; i++) {\r\n        query.append(\"?\");\r\n        if (i != fieldNames.length - 1) {\r\n            query.append(\",\");\r\n        }\r\n    }\r\n    if (dbProductName.startsWith(\"DB2\") || dbProductName.startsWith(\"ORACLE\")) {\r\n        query.append(\")\");\r\n    } else {\r\n        query.append(\");\");\r\n    }\r\n    return query.toString();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\db",
  "methodName" : "getRecordWriter",
  "errType" : [ "Exception" ],
  "containingMethodsNum" : 9,
  "sourceCodeText" : "RecordWriter<K, V> getRecordWriter(TaskAttemptContext context) throws IOException\n{\r\n    DBConfiguration dbConf = new DBConfiguration(context.getConfiguration());\r\n    String tableName = dbConf.getOutputTableName();\r\n    String[] fieldNames = dbConf.getOutputFieldNames();\r\n    if (fieldNames == null) {\r\n        fieldNames = new String[dbConf.getOutputFieldCount()];\r\n    }\r\n    try {\r\n        Connection connection = dbConf.getConnection();\r\n        PreparedStatement statement = null;\r\n        DatabaseMetaData dbMeta = connection.getMetaData();\r\n        this.dbProductName = dbMeta.getDatabaseProductName().toUpperCase();\r\n        statement = connection.prepareStatement(constructQuery(tableName, fieldNames));\r\n        return new DBRecordWriter(connection, statement);\r\n    } catch (Exception ex) {\r\n        throw new IOException(ex.getMessage());\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\db",
  "methodName" : "setOutput",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void setOutput(Job job, String tableName, String... fieldNames) throws IOException\n{\r\n    if (fieldNames.length > 0 && fieldNames[0] != null) {\r\n        DBConfiguration dbConf = setOutput(job, tableName);\r\n        dbConf.setOutputFieldNames(fieldNames);\r\n    } else {\r\n        if (fieldNames.length > 0) {\r\n            setOutput(job, tableName, fieldNames.length);\r\n        } else {\r\n            throw new IllegalArgumentException(\"Field names must be greater than 0\");\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\db",
  "methodName" : "setOutput",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void setOutput(Job job, String tableName, int fieldCount) throws IOException\n{\r\n    DBConfiguration dbConf = setOutput(job, tableName);\r\n    dbConf.setOutputFieldCount(fieldCount);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\db",
  "methodName" : "setOutput",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "DBConfiguration setOutput(Job job, String tableName) throws IOException\n{\r\n    job.setOutputFormatClass(DBOutputFormat.class);\r\n    job.setReduceSpeculativeExecution(false);\r\n    DBConfiguration dbConf = new DBConfiguration(job.getConfiguration());\r\n    dbConf.setOutputTableName(tableName);\r\n    return dbConf;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getMRv2LogDir",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String getMRv2LogDir()\n{\r\n    return System.getProperty(YarnConfiguration.YARN_APP_CONTAINER_LOG_DIR);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getTaskLogFile",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "File getTaskLogFile(TaskAttemptID taskid, boolean isCleanup, LogName filter)\n{\r\n    if (getMRv2LogDir() != null) {\r\n        return new File(getMRv2LogDir(), filter.toString());\r\n    } else {\r\n        return new File(getAttemptDir(taskid, isCleanup), filter.toString());\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getRealTaskLogFileLocation",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 3,
  "sourceCodeText" : "File getRealTaskLogFileLocation(TaskAttemptID taskid, boolean isCleanup, LogName filter)\n{\r\n    LogFileDetail l;\r\n    try {\r\n        l = getLogFileDetail(taskid, filter, isCleanup);\r\n    } catch (IOException ie) {\r\n        LOG.error(\"getTaskLogFileDetail threw an exception \" + ie);\r\n        return null;\r\n    }\r\n    return new File(l.location, filter.toString());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getLogFileDetail",
  "errType" : null,
  "containingMethodsNum" : 17,
  "sourceCodeText" : "LogFileDetail getLogFileDetail(TaskAttemptID taskid, LogName filter, boolean isCleanup) throws IOException\n{\r\n    File indexFile = getIndexFile(taskid, isCleanup);\r\n    BufferedReader fis = new BufferedReader(new InputStreamReader(SecureIOUtils.openForRead(indexFile, obtainLogDirOwner(taskid), null), Charsets.UTF_8));\r\n    LogFileDetail l = new LogFileDetail();\r\n    String str = null;\r\n    try {\r\n        str = fis.readLine();\r\n        if (str == null) {\r\n            throw new IOException(\"Index file for the log of \" + taskid + \" doesn't exist.\");\r\n        }\r\n        l.location = str.substring(str.indexOf(LogFileDetail.LOCATION) + LogFileDetail.LOCATION.length());\r\n        if (filter.equals(LogName.DEBUGOUT) || filter.equals(LogName.PROFILE)) {\r\n            l.length = new File(l.location, filter.toString()).length();\r\n            l.start = 0;\r\n            fis.close();\r\n            return l;\r\n        }\r\n        str = fis.readLine();\r\n        while (str != null) {\r\n            if (str.contains(filter.toString())) {\r\n                str = str.substring(filter.toString().length() + 1);\r\n                String[] startAndLen = str.split(\" \");\r\n                l.start = Long.parseLong(startAndLen[0]);\r\n                l.length = Long.parseLong(startAndLen[1]);\r\n                break;\r\n            }\r\n            str = fis.readLine();\r\n        }\r\n        fis.close();\r\n        fis = null;\r\n    } finally {\r\n        IOUtils.cleanupWithLogger(LOG, fis);\r\n    }\r\n    return l;\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getTmpIndexFile",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "File getTmpIndexFile(TaskAttemptID taskid, boolean isCleanup)\n{\r\n    return new File(getAttemptDir(taskid, isCleanup), \"log.tmp\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getIndexFile",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "File getIndexFile(TaskAttemptID taskid, boolean isCleanup)\n{\r\n    return new File(getAttemptDir(taskid, isCleanup), \"log.index\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "obtainLogDirOwner",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "String obtainLogDirOwner(TaskAttemptID taskid) throws IOException\n{\r\n    Configuration conf = new Configuration();\r\n    FileSystem raw = FileSystem.getLocal(conf).getRaw();\r\n    Path jobLogDir = new Path(getJobDir(taskid.getJobID()).getAbsolutePath());\r\n    FileStatus jobStat = raw.getFileStatus(jobLogDir);\r\n    return jobStat.getOwner();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getBaseLogDir",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String getBaseLogDir()\n{\r\n    return System.getProperty(\"hadoop.log.dir\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getAttemptDir",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "File getAttemptDir(TaskAttemptID taskid, boolean isCleanup)\n{\r\n    String cleanupSuffix = isCleanup ? \".cleanup\" : \"\";\r\n    return new File(getJobDir(taskid.getJobID()), taskid + cleanupSuffix);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "writeToIndexFile",
  "errType" : null,
  "containingMethodsNum" : 17,
  "sourceCodeText" : "void writeToIndexFile(String logLocation, boolean isCleanup) throws IOException\n{\r\n    File tmpIndexFile = getTmpIndexFile(currentTaskid, isCleanup);\r\n    BufferedOutputStream bos = null;\r\n    DataOutputStream dos = null;\r\n    try {\r\n        bos = new BufferedOutputStream(SecureIOUtils.createForWrite(tmpIndexFile, 0644));\r\n        dos = new DataOutputStream(bos);\r\n        dos.writeBytes(LogFileDetail.LOCATION + logLocation + \"\\n\" + LogName.STDOUT.toString() + \":\");\r\n        dos.writeBytes(Long.toString(prevOutLength) + \" \");\r\n        dos.writeBytes(Long.toString(new File(logLocation, LogName.STDOUT.toString()).length() - prevOutLength) + \"\\n\" + LogName.STDERR + \":\");\r\n        dos.writeBytes(Long.toString(prevErrLength) + \" \");\r\n        dos.writeBytes(Long.toString(new File(logLocation, LogName.STDERR.toString()).length() - prevErrLength) + \"\\n\" + LogName.SYSLOG.toString() + \":\");\r\n        dos.writeBytes(Long.toString(prevLogLength) + \" \");\r\n        dos.writeBytes(Long.toString(new File(logLocation, LogName.SYSLOG.toString()).length() - prevLogLength) + \"\\n\");\r\n        dos.close();\r\n        dos = null;\r\n        bos.close();\r\n        bos = null;\r\n    } finally {\r\n        IOUtils.cleanupWithLogger(LOG, dos, bos);\r\n    }\r\n    File indexFile = getIndexFile(currentTaskid, isCleanup);\r\n    Path indexFilePath = new Path(indexFile.getAbsolutePath());\r\n    Path tmpIndexFilePath = new Path(tmpIndexFile.getAbsolutePath());\r\n    if (localFS == null) {\r\n        localFS = FileSystem.getLocal(new Configuration());\r\n    }\r\n    localFS.rename(tmpIndexFilePath, indexFilePath);\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "resetPrevLengths",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void resetPrevLengths(String logLocation)\n{\r\n    prevOutLength = new File(logLocation, LogName.STDOUT.toString()).length();\r\n    prevErrLength = new File(logLocation, LogName.STDERR.toString()).length();\r\n    prevLogLength = new File(logLocation, LogName.SYSLOG.toString()).length();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "syncLogs",
  "errType" : null,
  "containingMethodsNum" : 11,
  "sourceCodeText" : "void syncLogs(String logLocation, TaskAttemptID taskid, boolean isCleanup) throws IOException\n{\r\n    System.out.flush();\r\n    System.err.flush();\r\n    Enumeration<Logger> allLoggers = LogManager.getCurrentLoggers();\r\n    while (allLoggers.hasMoreElements()) {\r\n        Logger l = allLoggers.nextElement();\r\n        Enumeration<Appender> allAppenders = l.getAllAppenders();\r\n        while (allAppenders.hasMoreElements()) {\r\n            Appender a = allAppenders.nextElement();\r\n            if (a instanceof TaskLogAppender) {\r\n                ((TaskLogAppender) a).flush();\r\n            }\r\n        }\r\n    }\r\n    if (currentTaskid != taskid) {\r\n        currentTaskid = taskid;\r\n        resetPrevLengths(logLocation);\r\n    }\r\n    writeToIndexFile(logLocation, isCleanup);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "syncLogsShutdown",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void syncLogsShutdown(ScheduledExecutorService scheduler)\n{\r\n    System.out.flush();\r\n    System.err.flush();\r\n    if (scheduler != null) {\r\n        scheduler.shutdownNow();\r\n    }\r\n    LogManager.shutdown();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "syncLogs",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void syncLogs()\n{\r\n    System.out.flush();\r\n    System.err.flush();\r\n    final Logger rootLogger = Logger.getRootLogger();\r\n    flushAppenders(rootLogger);\r\n    final Enumeration<Logger> allLoggers = rootLogger.getLoggerRepository().getCurrentLoggers();\r\n    while (allLoggers.hasMoreElements()) {\r\n        final Logger l = allLoggers.nextElement();\r\n        flushAppenders(l);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "flushAppenders",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void flushAppenders(Logger l)\n{\r\n    final Enumeration<Appender> allAppenders = l.getAllAppenders();\r\n    while (allAppenders.hasMoreElements()) {\r\n        final Appender a = allAppenders.nextElement();\r\n        if (a instanceof Flushable) {\r\n            try {\r\n                ((Flushable) a).flush();\r\n            } catch (IOException ioe) {\r\n                System.err.println(a + \": Failed to flush!\" + StringUtils.stringifyException(ioe));\r\n            }\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "createLogSyncer",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "ScheduledExecutorService createLogSyncer()\n{\r\n    final ScheduledExecutorService scheduler = HadoopExecutors.newSingleThreadScheduledExecutor(new ThreadFactory() {\r\n\r\n        @Override\r\n        public Thread newThread(Runnable r) {\r\n            final Thread t = Executors.defaultThreadFactory().newThread(r);\r\n            t.setDaemon(true);\r\n            t.setName(\"Thread for syncLogs\");\r\n            return t;\r\n        }\r\n    });\r\n    ShutdownHookManager.get().addShutdownHook(new Runnable() {\r\n\r\n        @Override\r\n        public void run() {\r\n            TaskLog.syncLogsShutdown(scheduler);\r\n        }\r\n    }, 50);\r\n    scheduler.scheduleWithFixedDelay(new Runnable() {\r\n\r\n        @Override\r\n        public void run() {\r\n            TaskLog.syncLogs();\r\n        }\r\n    }, 0L, 5L, TimeUnit.SECONDS);\r\n    return scheduler;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getTaskLogLength",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "long getTaskLogLength(JobConf conf)\n{\r\n    return getTaskLogLimitBytes(conf);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getTaskLogLimitBytes",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "long getTaskLogLimitBytes(Configuration conf)\n{\r\n    return conf.getLong(JobContext.TASK_USERLOG_LIMIT, 0) * 1024;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "captureOutAndError",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "List<String> captureOutAndError(List<String> setup, List<String> cmd, File stdoutFilename, File stderrFilename, long tailLength, boolean useSetsid) throws IOException\n{\r\n    List<String> result = new ArrayList<String>(3);\r\n    result.add(bashCommand);\r\n    result.add(\"-c\");\r\n    String mergedCmd = buildCommandLine(setup, cmd, stdoutFilename, stderrFilename, tailLength, useSetsid);\r\n    result.add(mergedCmd);\r\n    return result;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "buildCommandLine",
  "errType" : null,
  "containingMethodsNum" : 29,
  "sourceCodeText" : "String buildCommandLine(List<String> setup, List<String> cmd, File stdoutFilename, File stderrFilename, long tailLength, boolean useSetsid) throws IOException\n{\r\n    String stdout = FileUtil.makeShellPath(stdoutFilename);\r\n    String stderr = FileUtil.makeShellPath(stderrFilename);\r\n    StringBuffer mergedCmd = new StringBuffer();\r\n    if (!Shell.WINDOWS) {\r\n        mergedCmd.append(\" export JVM_PID=`echo $$` ; \");\r\n    }\r\n    if (setup != null && setup.size() > 0) {\r\n        mergedCmd.append(addCommand(setup, false));\r\n        mergedCmd.append(\";\");\r\n    }\r\n    if (tailLength > 0) {\r\n        mergedCmd.append(\"(\");\r\n    } else if (ProcessTree.isSetsidAvailable && useSetsid && !Shell.WINDOWS) {\r\n        mergedCmd.append(\"exec setsid \");\r\n    } else {\r\n        mergedCmd.append(\"exec \");\r\n    }\r\n    mergedCmd.append(addCommand(cmd, true));\r\n    mergedCmd.append(\" < /dev/null \");\r\n    if (tailLength > 0) {\r\n        mergedCmd.append(\" | \");\r\n        mergedCmd.append(tailCommand);\r\n        mergedCmd.append(\" -c \");\r\n        mergedCmd.append(tailLength);\r\n        mergedCmd.append(\" >> \");\r\n        mergedCmd.append(stdout);\r\n        mergedCmd.append(\" ; exit $PIPESTATUS ) 2>&1 | \");\r\n        mergedCmd.append(tailCommand);\r\n        mergedCmd.append(\" -c \");\r\n        mergedCmd.append(tailLength);\r\n        mergedCmd.append(\" >> \");\r\n        mergedCmd.append(stderr);\r\n        mergedCmd.append(\" ; exit $PIPESTATUS\");\r\n    } else {\r\n        mergedCmd.append(\" 1>> \");\r\n        mergedCmd.append(stdout);\r\n        mergedCmd.append(\" 2>> \");\r\n        mergedCmd.append(stderr);\r\n    }\r\n    return mergedCmd.toString();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "buildDebugScriptCommandLine",
  "errType" : null,
  "containingMethodsNum" : 9,
  "sourceCodeText" : "String buildDebugScriptCommandLine(List<String> cmd, String debugout) throws IOException\n{\r\n    StringBuilder mergedCmd = new StringBuilder();\r\n    mergedCmd.append(\"exec \");\r\n    boolean isExecutable = true;\r\n    for (String s : cmd) {\r\n        if (isExecutable) {\r\n            mergedCmd.append(FileUtil.makeShellPath(new File(s)));\r\n            isExecutable = false;\r\n        } else {\r\n            mergedCmd.append(s);\r\n        }\r\n        mergedCmd.append(\" \");\r\n    }\r\n    mergedCmd.append(\" < /dev/null \");\r\n    mergedCmd.append(\" >\");\r\n    mergedCmd.append(debugout);\r\n    mergedCmd.append(\" 2>&1 \");\r\n    return mergedCmd.toString();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "addCommand",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "String addCommand(List<String> cmd, boolean isExecutable) throws IOException\n{\r\n    StringBuffer command = new StringBuffer();\r\n    for (String s : cmd) {\r\n        command.append('\\'');\r\n        if (isExecutable) {\r\n            command.append(FileUtil.makeShellPath(new File(s)));\r\n            isExecutable = false;\r\n        } else {\r\n            command.append(s);\r\n        }\r\n        command.append('\\'');\r\n        command.append(\" \");\r\n    }\r\n    return command.toString();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getUserLogDir",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "File getUserLogDir()\n{\r\n    if (!LOG_DIR.exists()) {\r\n        boolean b = LOG_DIR.mkdirs();\r\n        if (!b) {\r\n            LOG.debug(\"mkdirs failed. Ignoring.\");\r\n        }\r\n    }\r\n    return LOG_DIR;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getJobDir",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "File getJobDir(JobID jobid)\n{\r\n    return new File(getUserLogDir(), jobid.toString());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getIndexInformation",
  "errType" : [ "InterruptedException" ],
  "containingMethodsNum" : 9,
  "sourceCodeText" : "IndexRecord getIndexInformation(String mapId, int reduce, Path fileName, String expectedIndexOwner) throws IOException\n{\r\n    IndexInformation info = cache.get(mapId);\r\n    if (info == null) {\r\n        info = readIndexFileToCache(fileName, mapId, expectedIndexOwner);\r\n    } else {\r\n        synchronized (info) {\r\n            while (isUnderConstruction(info)) {\r\n                try {\r\n                    info.wait();\r\n                } catch (InterruptedException e) {\r\n                    throw new IOException(\"Interrupted waiting for construction\", e);\r\n                }\r\n            }\r\n        }\r\n        LOG.debug(\"IndexCache HIT: MapId \" + mapId + \" found\");\r\n    }\r\n    if (info.mapSpillRecord.size() == 0 || info.mapSpillRecord.size() <= reduce) {\r\n        throw new IOException(\"Invalid request \" + \" Map Id = \" + mapId + \" Reducer = \" + reduce + \" Index Info Length = \" + info.mapSpillRecord.size());\r\n    }\r\n    return info.mapSpillRecord.getIndex(reduce);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "isUnderConstruction",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean isUnderConstruction(IndexInformation info)\n{\r\n    synchronized (info) {\r\n        return (null == info.mapSpillRecord);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "readIndexFileToCache",
  "errType" : [ "InterruptedException", "Throwable" ],
  "containingMethodsNum" : 10,
  "sourceCodeText" : "IndexInformation readIndexFileToCache(Path indexFileName, String mapId, String expectedIndexOwner) throws IOException\n{\r\n    IndexInformation info;\r\n    IndexInformation newInd = new IndexInformation();\r\n    if ((info = cache.putIfAbsent(mapId, newInd)) != null) {\r\n        synchronized (info) {\r\n            while (isUnderConstruction(info)) {\r\n                try {\r\n                    info.wait();\r\n                } catch (InterruptedException e) {\r\n                    throw new IOException(\"Interrupted waiting for construction\", e);\r\n                }\r\n            }\r\n        }\r\n        LOG.debug(\"IndexCache HIT: MapId \" + mapId + \" found\");\r\n        return info;\r\n    }\r\n    LOG.debug(\"IndexCache MISS: MapId \" + mapId + \" not found\");\r\n    SpillRecord tmp = null;\r\n    try {\r\n        tmp = new SpillRecord(indexFileName, conf, expectedIndexOwner);\r\n    } catch (Throwable e) {\r\n        tmp = new SpillRecord(0);\r\n        cache.remove(mapId);\r\n        throw new IOException(\"Error Reading IndexFile\", e);\r\n    } finally {\r\n        synchronized (newInd) {\r\n            newInd.mapSpillRecord = tmp;\r\n            newInd.notifyAll();\r\n        }\r\n    }\r\n    queue.add(mapId);\r\n    if (totalMemoryUsed.addAndGet(newInd.getSize()) > totalMemoryAllowed) {\r\n        freeIndexInformation();\r\n    }\r\n    return newInd;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 2,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "removeMap",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void removeMap(String mapId)\n{\r\n    IndexInformation info = cache.get(mapId);\r\n    if (info == null || isUnderConstruction(info)) {\r\n        return;\r\n    }\r\n    info = cache.remove(mapId);\r\n    if (info != null) {\r\n        totalMemoryUsed.addAndGet(-info.getSize());\r\n        if (!queue.remove(mapId)) {\r\n            LOG.warn(\"Map ID\" + mapId + \" not found in queue!!\");\r\n        }\r\n    } else {\r\n        LOG.info(\"Map ID \" + mapId + \" not found in cache\");\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "checkTotalMemoryUsed",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "boolean checkTotalMemoryUsed()\n{\r\n    int totalSize = 0;\r\n    for (IndexInformation info : cache.values()) {\r\n        totalSize += info.getSize();\r\n    }\r\n    return totalSize == totalMemoryUsed.get();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "freeIndexInformation",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void freeIndexInformation()\n{\r\n    while (totalMemoryUsed.get() > totalMemoryAllowed) {\r\n        String s = queue.remove();\r\n        IndexInformation info = cache.remove(s);\r\n        if (info != null) {\r\n            totalMemoryUsed.addAndGet(-info.getSize());\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "getDatum",
  "errType" : null,
  "containingMethodsNum" : 11,
  "sourceCodeText" : "Object getDatum()\n{\r\n    if (datum == null) {\r\n        datum = new JobFinished();\r\n        datum.setJobid(new Utf8(jobId.toString()));\r\n        datum.setFinishTime(finishTime);\r\n        datum.setFinishedMaps(succeededMaps);\r\n        datum.setFinishedReduces(succeededReduces);\r\n        datum.setFailedMaps(failedMaps);\r\n        datum.setFailedReduces(failedReduces);\r\n        datum.setKilledMaps(killedMaps);\r\n        datum.setKilledReduces(killedReduces);\r\n        datum.setMapCounters(EventWriter.toAvro(mapCounters, \"MAP_COUNTERS\"));\r\n        datum.setReduceCounters(EventWriter.toAvro(reduceCounters, \"REDUCE_COUNTERS\"));\r\n        datum.setTotalCounters(EventWriter.toAvro(totalCounters, \"TOTAL_COUNTERS\"));\r\n    }\r\n    return datum;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "setDatum",
  "errType" : null,
  "containingMethodsNum" : 11,
  "sourceCodeText" : "void setDatum(Object oDatum)\n{\r\n    this.datum = (JobFinished) oDatum;\r\n    this.jobId = JobID.forName(datum.getJobid().toString());\r\n    this.finishTime = datum.getFinishTime();\r\n    this.succeededMaps = datum.getFinishedMaps();\r\n    this.succeededReduces = datum.getFinishedReduces();\r\n    this.failedMaps = datum.getFailedMaps();\r\n    this.failedReduces = datum.getFailedReduces();\r\n    this.killedMaps = datum.getKilledMaps();\r\n    this.killedReduces = datum.getKilledReduces();\r\n    this.mapCounters = EventReader.fromAvro(datum.getMapCounters());\r\n    this.reduceCounters = EventReader.fromAvro(datum.getReduceCounters());\r\n    this.totalCounters = EventReader.fromAvro(datum.getTotalCounters());\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "getEventType",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "EventType getEventType()\n{\r\n    return EventType.JOB_FINISHED;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "getJobid",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "JobID getJobid()\n{\r\n    return jobId;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "getFinishTime",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getFinishTime()\n{\r\n    return finishTime;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "getSucceededMaps",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int getSucceededMaps()\n{\r\n    return succeededMaps;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "getSucceededReduces",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int getSucceededReduces()\n{\r\n    return succeededReduces;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "getFailedMaps",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int getFailedMaps()\n{\r\n    return failedMaps;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "getFailedReduces",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int getFailedReduces()\n{\r\n    return failedReduces;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "getKilledMaps",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int getKilledMaps()\n{\r\n    return killedMaps;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "getKilledReduces",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int getKilledReduces()\n{\r\n    return killedReduces;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "getTotalCounters",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Counters getTotalCounters()\n{\r\n    return totalCounters;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "getMapCounters",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Counters getMapCounters()\n{\r\n    return mapCounters;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "getReduceCounters",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Counters getReduceCounters()\n{\r\n    return reduceCounters;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "toTimelineEvent",
  "errType" : null,
  "containingMethodsNum" : 11,
  "sourceCodeText" : "TimelineEvent toTimelineEvent()\n{\r\n    TimelineEvent tEvent = new TimelineEvent();\r\n    tEvent.setId(StringUtils.toUpperCase(getEventType().name()));\r\n    tEvent.addInfo(\"FINISH_TIME\", getFinishTime());\r\n    tEvent.addInfo(\"NUM_MAPS\", getSucceededMaps() + getFailedMaps() + getKilledMaps());\r\n    tEvent.addInfo(\"NUM_REDUCES\", getSucceededReduces() + getFailedReduces() + getKilledReduces());\r\n    tEvent.addInfo(\"FAILED_MAPS\", getFailedMaps());\r\n    tEvent.addInfo(\"FAILED_REDUCES\", getFailedReduces());\r\n    tEvent.addInfo(\"SUCCESSFUL_MAPS\", getSucceededMaps());\r\n    tEvent.addInfo(\"SUCCESSFUL_REDUCES\", getSucceededReduces());\r\n    tEvent.addInfo(\"KILLED_MAPS\", getKilledMaps());\r\n    tEvent.addInfo(\"KILLED_REDUCES\", getKilledReduces());\r\n    tEvent.addInfo(\"JOB_STATUS\", \"SUCCEEDED\");\r\n    return tEvent;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "getTimelineMetrics",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "Set<TimelineMetric> getTimelineMetrics()\n{\r\n    Set<TimelineMetric> jobMetrics = JobHistoryEventUtils.countersToTimelineMetric(getTotalCounters(), finishTime);\r\n    jobMetrics.addAll(JobHistoryEventUtils.countersToTimelineMetric(getMapCounters(), finishTime, \"MAP:\"));\r\n    jobMetrics.addAll(JobHistoryEventUtils.countersToTimelineMetric(getReduceCounters(), finishTime, \"REDUCE:\"));\r\n    return jobMetrics;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "getDatum",
  "errType" : null,
  "containingMethodsNum" : 15,
  "sourceCodeText" : "Object getDatum()\n{\r\n    if (datum == null) {\r\n        datum = new MapAttemptFinished();\r\n        datum.setTaskid(new Utf8(attemptId.getTaskID().toString()));\r\n        datum.setAttemptId(new Utf8(attemptId.toString()));\r\n        datum.setTaskType(new Utf8(taskType.name()));\r\n        datum.setTaskStatus(new Utf8(taskStatus));\r\n        datum.setMapFinishTime(mapFinishTime);\r\n        datum.setFinishTime(finishTime);\r\n        datum.setHostname(new Utf8(hostname));\r\n        datum.setPort(port);\r\n        if (rackName != null) {\r\n            datum.setRackname(new Utf8(rackName));\r\n        }\r\n        datum.setState(new Utf8(state));\r\n        datum.setCounters(EventWriter.toAvro(counters));\r\n        datum.setClockSplits(AvroArrayUtils.toAvro(ProgressSplitsBlock.arrayGetWallclockTime(allSplits)));\r\n        datum.setCpuUsages(AvroArrayUtils.toAvro(ProgressSplitsBlock.arrayGetCPUTime(allSplits)));\r\n        datum.setVMemKbytes(AvroArrayUtils.toAvro(ProgressSplitsBlock.arrayGetVMemKbytes(allSplits)));\r\n        datum.setPhysMemKbytes(AvroArrayUtils.toAvro(ProgressSplitsBlock.arrayGetPhysMemKbytes(allSplits)));\r\n    }\r\n    return datum;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "setDatum",
  "errType" : null,
  "containingMethodsNum" : 14,
  "sourceCodeText" : "void setDatum(Object oDatum)\n{\r\n    this.datum = (MapAttemptFinished) oDatum;\r\n    this.attemptId = TaskAttemptID.forName(datum.getAttemptId().toString());\r\n    this.taskType = TaskType.valueOf(datum.getTaskType().toString());\r\n    this.taskStatus = datum.getTaskStatus().toString();\r\n    this.mapFinishTime = datum.getMapFinishTime();\r\n    this.finishTime = datum.getFinishTime();\r\n    this.hostname = datum.getHostname().toString();\r\n    this.rackName = datum.getRackname().toString();\r\n    this.port = datum.getPort();\r\n    this.state = datum.getState().toString();\r\n    this.counters = EventReader.fromAvro(datum.getCounters());\r\n    this.clockSplits = AvroArrayUtils.fromAvro(datum.getClockSplits());\r\n    this.cpuUsages = AvroArrayUtils.fromAvro(datum.getCpuUsages());\r\n    this.vMemKbytes = AvroArrayUtils.fromAvro(datum.getVMemKbytes());\r\n    this.physMemKbytes = AvroArrayUtils.fromAvro(datum.getPhysMemKbytes());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "getTaskId",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "TaskID getTaskId()\n{\r\n    return attemptId.getTaskID();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "getAttemptId",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "TaskAttemptID getAttemptId()\n{\r\n    return attemptId;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "getTaskType",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "TaskType getTaskType()\n{\r\n    return taskType;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "getTaskStatus",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String getTaskStatus()\n{\r\n    return taskStatus.toString();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "getMapFinishTime",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getMapFinishTime()\n{\r\n    return mapFinishTime;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "getFinishTime",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getFinishTime()\n{\r\n    return finishTime;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "getStartTime",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getStartTime()\n{\r\n    return startTime;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "getHostname",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String getHostname()\n{\r\n    return hostname.toString();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "getPort",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int getPort()\n{\r\n    return port;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "getRackName",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String getRackName()\n{\r\n    return rackName == null ? null : rackName.toString();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "getState",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String getState()\n{\r\n    return state.toString();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "getCounters",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Counters getCounters()\n{\r\n    return counters;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "getEventType",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "EventType getEventType()\n{\r\n    return EventType.MAP_ATTEMPT_FINISHED;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "getClockSplits",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int[] getClockSplits()\n{\r\n    return clockSplits;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "getCpuUsages",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int[] getCpuUsages()\n{\r\n    return cpuUsages;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "getVMemKbytes",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int[] getVMemKbytes()\n{\r\n    return vMemKbytes;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "getPhysMemKbytes",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int[] getPhysMemKbytes()\n{\r\n    return physMemKbytes;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "toTimelineEvent",
  "errType" : null,
  "containingMethodsNum" : 10,
  "sourceCodeText" : "TimelineEvent toTimelineEvent()\n{\r\n    TimelineEvent tEvent = new TimelineEvent();\r\n    tEvent.setId(StringUtils.toUpperCase(getEventType().name()));\r\n    tEvent.addInfo(\"TASK_TYPE\", getTaskType().toString());\r\n    tEvent.addInfo(\"FINISH_TIME\", getFinishTime());\r\n    tEvent.addInfo(\"STATUS\", getTaskStatus());\r\n    tEvent.addInfo(\"STATE\", getState());\r\n    tEvent.addInfo(\"MAP_FINISH_TIME\", getMapFinishTime());\r\n    tEvent.addInfo(\"HOSTNAME\", getHostname());\r\n    tEvent.addInfo(\"PORT\", getPort());\r\n    tEvent.addInfo(\"RACK_NAME\", getRackName());\r\n    tEvent.addInfo(\"ATTEMPT_ID\", getAttemptId() == null ? \"\" : getAttemptId().toString());\r\n    return tEvent;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "getTimelineMetrics",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Set<TimelineMetric> getTimelineMetrics()\n{\r\n    Set<TimelineMetric> metrics = JobHistoryEventUtils.countersToTimelineMetric(getCounters(), finishTime);\r\n    return metrics;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output",
  "methodName" : "getSequenceWriter",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "SequenceFile.Writer getSequenceWriter(TaskAttemptContext context, Class<?> keyClass, Class<?> valueClass) throws IOException\n{\r\n    Configuration conf = context.getConfiguration();\r\n    CompressionCodec codec = null;\r\n    CompressionType compressionType = CompressionType.NONE;\r\n    if (getCompressOutput(context)) {\r\n        compressionType = getOutputCompressionType(context);\r\n        Class<?> codecClass = getOutputCompressorClass(context, DefaultCodec.class);\r\n        codec = (CompressionCodec) ReflectionUtils.newInstance(codecClass, conf);\r\n    }\r\n    Path file = getDefaultWorkFile(context, \"\");\r\n    FileSystem fs = file.getFileSystem(conf);\r\n    return SequenceFile.createWriter(fs, conf, file, keyClass, valueClass, compressionType, codec, context);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output",
  "methodName" : "getRecordWriter",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "RecordWriter<K, V> getRecordWriter(TaskAttemptContext context) throws IOException, InterruptedException\n{\r\n    final SequenceFile.Writer out = getSequenceWriter(context, context.getOutputKeyClass(), context.getOutputValueClass());\r\n    return new RecordWriter<K, V>() {\r\n\r\n        public void write(K key, V value) throws IOException {\r\n            out.append(key, value);\r\n        }\r\n\r\n        public void close(TaskAttemptContext context) throws IOException {\r\n            out.close();\r\n        }\r\n    };\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output",
  "methodName" : "getOutputCompressionType",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "CompressionType getOutputCompressionType(JobContext job)\n{\r\n    String val = job.getConfiguration().get(FileOutputFormat.COMPRESS_TYPE, CompressionType.RECORD.toString());\r\n    return CompressionType.valueOf(val);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output",
  "methodName" : "setOutputCompressionType",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void setOutputCompressionType(Job job, CompressionType style)\n{\r\n    setCompressOutput(job, true);\r\n    job.getConfiguration().set(FileOutputFormat.COMPRESS_TYPE, style.toString());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\join",
  "methodName" : "combine",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "boolean combine(Object[] srcs, TupleWritable dst)\n{\r\n    assert srcs.length == dst.size();\r\n    return true;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\security",
  "methodName" : "get",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "SpillCallBackInjector get()\n{\r\n    return instance;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\security",
  "methodName" : "getAndSet",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "SpillCallBackInjector getAndSet(SpillCallBackInjector spillInjector)\n{\r\n    SpillCallBackInjector prev = instance;\r\n    instance = spillInjector;\r\n    return prev;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\security",
  "methodName" : "writeSpillIndexFileCB",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void writeSpillIndexFileCB(Path path)\n{\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\security",
  "methodName" : "writeSpillFileCB",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void writeSpillFileCB(Path path, FSDataOutputStream out, Configuration conf)\n{\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\security",
  "methodName" : "getSpillFileCB",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void getSpillFileCB(Path path, InputStream is, Configuration conf)\n{\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\security",
  "methodName" : "getSpilledFileReport",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getSpilledFileReport()\n{\r\n    return null;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\security",
  "methodName" : "handleErrorInSpillFill",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void handleErrorInSpillFill(Path path, Exception e)\n{\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\security",
  "methodName" : "corruptSpilledFile",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void corruptSpilledFile(Path fileName) throws IOException\n{\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\security",
  "methodName" : "addSpillIndexFileCB",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void addSpillIndexFileCB(Path path, Configuration conf)\n{\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\security",
  "methodName" : "validateSpillIndexFileCB",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void validateSpillIndexFileCB(Path path, Configuration conf)\n{\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\tools",
  "methodName" : "run",
  "errType" : [ "IllegalArgumentException", "NumberFormatException", "RemoteException", "IOException" ],
  "containingMethodsNum" : 131,
  "sourceCodeText" : "int run(String[] argv) throws Exception\n{\r\n    int exitCode = -1;\r\n    if (argv.length < 1) {\r\n        displayUsage(\"\");\r\n        return exitCode;\r\n    }\r\n    String cmd = argv[0];\r\n    String submitJobFile = null;\r\n    String jobid = null;\r\n    String taskid = null;\r\n    String historyFileOrJobId = null;\r\n    String historyOutFile = null;\r\n    String historyOutFormat = HistoryViewer.HUMAN_FORMAT;\r\n    String counterGroupName = null;\r\n    String counterName = null;\r\n    JobPriority jp = null;\r\n    String taskType = null;\r\n    String taskState = null;\r\n    int fromEvent = 0;\r\n    int nEvents = 0;\r\n    int jpvalue = 0;\r\n    String configOutFile = null;\r\n    boolean getStatus = false;\r\n    boolean getCounter = false;\r\n    boolean killJob = false;\r\n    boolean listEvents = false;\r\n    boolean viewHistory = false;\r\n    boolean viewAllHistory = false;\r\n    boolean listJobs = false;\r\n    boolean listAllJobs = false;\r\n    boolean listActiveTrackers = false;\r\n    boolean listBlacklistedTrackers = false;\r\n    boolean displayTasks = false;\r\n    boolean killTask = false;\r\n    boolean failTask = false;\r\n    boolean setJobPriority = false;\r\n    boolean logs = false;\r\n    boolean downloadConfig = false;\r\n    if (\"-submit\".equals(cmd)) {\r\n        if (argv.length != 2) {\r\n            displayUsage(cmd);\r\n            return exitCode;\r\n        }\r\n        submitJobFile = argv[1];\r\n    } else if (\"-status\".equals(cmd)) {\r\n        if (argv.length != 2) {\r\n            displayUsage(cmd);\r\n            return exitCode;\r\n        }\r\n        jobid = argv[1];\r\n        getStatus = true;\r\n    } else if (\"-counter\".equals(cmd)) {\r\n        if (argv.length != 4) {\r\n            displayUsage(cmd);\r\n            return exitCode;\r\n        }\r\n        getCounter = true;\r\n        jobid = argv[1];\r\n        counterGroupName = argv[2];\r\n        counterName = argv[3];\r\n    } else if (\"-kill\".equals(cmd)) {\r\n        if (argv.length != 2) {\r\n            displayUsage(cmd);\r\n            return exitCode;\r\n        }\r\n        jobid = argv[1];\r\n        killJob = true;\r\n    } else if (\"-set-priority\".equals(cmd)) {\r\n        if (argv.length != 3) {\r\n            displayUsage(cmd);\r\n            return exitCode;\r\n        }\r\n        jobid = argv[1];\r\n        try {\r\n            jp = JobPriority.valueOf(argv[2]);\r\n        } catch (IllegalArgumentException iae) {\r\n            try {\r\n                jpvalue = Integer.parseInt(argv[2]);\r\n            } catch (NumberFormatException ne) {\r\n                LOG.info(\"Error number format: \", ne);\r\n                displayUsage(cmd);\r\n                return exitCode;\r\n            }\r\n        }\r\n        setJobPriority = true;\r\n    } else if (\"-events\".equals(cmd)) {\r\n        if (argv.length != 4) {\r\n            displayUsage(cmd);\r\n            return exitCode;\r\n        }\r\n        jobid = argv[1];\r\n        fromEvent = Integer.parseInt(argv[2]);\r\n        nEvents = Integer.parseInt(argv[3]);\r\n        listEvents = true;\r\n    } else if (\"-history\".equals(cmd)) {\r\n        viewHistory = true;\r\n        if (argv.length < 2 || argv.length > 7) {\r\n            displayUsage(cmd);\r\n            return exitCode;\r\n        }\r\n        int index = 1;\r\n        if (\"all\".equals(argv[index])) {\r\n            index++;\r\n            viewAllHistory = true;\r\n            if (argv.length == 2) {\r\n                displayUsage(cmd);\r\n                return exitCode;\r\n            }\r\n        }\r\n        historyFileOrJobId = argv[index++];\r\n        if (argv.length > index + 1 && \"-outfile\".equals(argv[index])) {\r\n            index++;\r\n            historyOutFile = argv[index++];\r\n        }\r\n        if (argv.length > index + 1 && \"-format\".equals(argv[index])) {\r\n            index++;\r\n            historyOutFormat = argv[index++];\r\n        }\r\n        if (argv.length > index) {\r\n            displayUsage(cmd);\r\n            return exitCode;\r\n        }\r\n    } else if (\"-list\".equals(cmd)) {\r\n        if (argv.length != 1 && !(argv.length == 2 && \"all\".equals(argv[1]))) {\r\n            displayUsage(cmd);\r\n            return exitCode;\r\n        }\r\n        if (argv.length == 2 && \"all\".equals(argv[1])) {\r\n            listAllJobs = true;\r\n        } else {\r\n            listJobs = true;\r\n        }\r\n    } else if (\"-kill-task\".equals(cmd)) {\r\n        if (argv.length != 2) {\r\n            displayUsage(cmd);\r\n            return exitCode;\r\n        }\r\n        killTask = true;\r\n        taskid = argv[1];\r\n    } else if (\"-fail-task\".equals(cmd)) {\r\n        if (argv.length != 2) {\r\n            displayUsage(cmd);\r\n            return exitCode;\r\n        }\r\n        failTask = true;\r\n        taskid = argv[1];\r\n    } else if (\"-list-active-trackers\".equals(cmd)) {\r\n        if (argv.length != 1) {\r\n            displayUsage(cmd);\r\n            return exitCode;\r\n        }\r\n        listActiveTrackers = true;\r\n    } else if (\"-list-blacklisted-trackers\".equals(cmd)) {\r\n        if (argv.length != 1) {\r\n            displayUsage(cmd);\r\n            return exitCode;\r\n        }\r\n        listBlacklistedTrackers = true;\r\n    } else if (\"-list-attempt-ids\".equals(cmd)) {\r\n        if (argv.length != 4) {\r\n            displayUsage(cmd);\r\n            return exitCode;\r\n        }\r\n        jobid = argv[1];\r\n        taskType = argv[2];\r\n        taskState = argv[3];\r\n        displayTasks = true;\r\n        if (!taskTypes.contains(org.apache.hadoop.util.StringUtils.toUpperCase(taskType))) {\r\n            System.out.println(\"Error: Invalid task-type: \" + taskType);\r\n            displayUsage(cmd);\r\n            return exitCode;\r\n        }\r\n        if (!taskStates.contains(org.apache.hadoop.util.StringUtils.toLowerCase(taskState))) {\r\n            System.out.println(\"Error: Invalid task-state: \" + taskState);\r\n            displayUsage(cmd);\r\n            return exitCode;\r\n        }\r\n    } else if (\"-logs\".equals(cmd)) {\r\n        if (argv.length == 2 || argv.length == 3) {\r\n            logs = true;\r\n            jobid = argv[1];\r\n            if (argv.length == 3) {\r\n                taskid = argv[2];\r\n            } else {\r\n                taskid = null;\r\n            }\r\n        } else {\r\n            displayUsage(cmd);\r\n            return exitCode;\r\n        }\r\n    } else if (\"-config\".equals(cmd)) {\r\n        downloadConfig = true;\r\n        if (argv.length != 3) {\r\n            displayUsage(cmd);\r\n            return exitCode;\r\n        }\r\n        jobid = argv[1];\r\n        configOutFile = argv[2];\r\n    } else {\r\n        displayUsage(cmd);\r\n        return exitCode;\r\n    }\r\n    cluster = createCluster();\r\n    try {\r\n        if (submitJobFile != null) {\r\n            Job job = Job.getInstance(new JobConf(submitJobFile));\r\n            job.submit();\r\n            System.out.println(\"Created job \" + job.getJobID());\r\n            exitCode = 0;\r\n        } else if (getStatus) {\r\n            Job job = getJob(JobID.forName(jobid));\r\n            if (job == null) {\r\n                System.out.println(\"Could not find job \" + jobid);\r\n            } else {\r\n                Counters counters = job.getCounters();\r\n                System.out.println();\r\n                System.out.println(job);\r\n                if (counters != null) {\r\n                    System.out.println(counters);\r\n                } else {\r\n                    System.out.println(\"Counters not available. Job is retired.\");\r\n                }\r\n                exitCode = 0;\r\n            }\r\n        } else if (getCounter) {\r\n            Job job = getJob(JobID.forName(jobid));\r\n            if (job == null) {\r\n                System.out.println(\"Could not find job \" + jobid);\r\n            } else {\r\n                Counters counters = job.getCounters();\r\n                if (counters == null) {\r\n                    System.out.println(\"Counters not available for retired job \" + jobid);\r\n                    exitCode = -1;\r\n                } else {\r\n                    System.out.println(getCounter(counters, counterGroupName, counterName));\r\n                    exitCode = 0;\r\n                }\r\n            }\r\n        } else if (killJob) {\r\n            Job job = getJob(JobID.forName(jobid));\r\n            if (job == null) {\r\n                System.out.println(\"Could not find job \" + jobid);\r\n            } else {\r\n                JobStatus jobStatus = job.getStatus();\r\n                if (jobStatus.getState() == JobStatus.State.FAILED) {\r\n                    System.out.println(\"Could not mark the job \" + jobid + \" as killed, as it has already failed.\");\r\n                    exitCode = -1;\r\n                } else if (jobStatus.getState() == JobStatus.State.KILLED) {\r\n                    System.out.println(\"The job \" + jobid + \" has already been killed.\");\r\n                    exitCode = -1;\r\n                } else if (jobStatus.getState() == JobStatus.State.SUCCEEDED) {\r\n                    System.out.println(\"Could not kill the job \" + jobid + \", as it has already succeeded.\");\r\n                    exitCode = -1;\r\n                } else {\r\n                    job.killJob();\r\n                    System.out.println(\"Killed job \" + jobid);\r\n                    exitCode = 0;\r\n                }\r\n            }\r\n        } else if (setJobPriority) {\r\n            Job job = getJob(JobID.forName(jobid));\r\n            if (job == null) {\r\n                System.out.println(\"Could not find job \" + jobid);\r\n            } else {\r\n                if (jp != null) {\r\n                    job.setPriority(jp);\r\n                } else {\r\n                    job.setPriorityAsInteger(jpvalue);\r\n                }\r\n                System.out.println(\"Changed job priority.\");\r\n                exitCode = 0;\r\n            }\r\n        } else if (viewHistory) {\r\n            if (historyFileOrJobId.endsWith(\".jhist\")) {\r\n                viewHistory(historyFileOrJobId, viewAllHistory, historyOutFile, historyOutFormat);\r\n                exitCode = 0;\r\n            } else {\r\n                Job job = getJob(JobID.forName(historyFileOrJobId));\r\n                if (job == null) {\r\n                    System.out.println(\"Could not find job \" + jobid);\r\n                } else {\r\n                    String historyUrl = job.getHistoryUrl();\r\n                    if (historyUrl == null || historyUrl.isEmpty()) {\r\n                        System.out.println(\"History file for job \" + historyFileOrJobId + \" is currently unavailable.\");\r\n                    } else {\r\n                        viewHistory(historyUrl, viewAllHistory, historyOutFile, historyOutFormat);\r\n                        exitCode = 0;\r\n                    }\r\n                }\r\n            }\r\n        } else if (listEvents) {\r\n            Job job = getJob(JobID.forName(jobid));\r\n            if (job == null) {\r\n                System.out.println(\"Could not find job \" + jobid);\r\n            } else {\r\n                listEvents(job, fromEvent, nEvents);\r\n                exitCode = 0;\r\n            }\r\n        } else if (listJobs) {\r\n            listJobs(cluster);\r\n            exitCode = 0;\r\n        } else if (listAllJobs) {\r\n            listAllJobs(cluster);\r\n            exitCode = 0;\r\n        } else if (listActiveTrackers) {\r\n            listActiveTrackers(cluster);\r\n            exitCode = 0;\r\n        } else if (listBlacklistedTrackers) {\r\n            listBlacklistedTrackers(cluster);\r\n            exitCode = 0;\r\n        } else if (displayTasks) {\r\n            Job job = getJob(JobID.forName(jobid));\r\n            if (job == null) {\r\n                System.out.println(\"Could not find job \" + jobid);\r\n            } else {\r\n                displayTasks(getJob(JobID.forName(jobid)), taskType, taskState);\r\n                exitCode = 0;\r\n            }\r\n        } else if (killTask) {\r\n            TaskAttemptID taskID = TaskAttemptID.forName(taskid);\r\n            Job job = getJob(taskID.getJobID());\r\n            if (job == null) {\r\n                System.out.println(\"Could not find job \" + jobid);\r\n            } else if (job.killTask(taskID, false)) {\r\n                System.out.println(\"Killed task \" + taskid);\r\n                exitCode = 0;\r\n            } else {\r\n                System.out.println(\"Could not kill task \" + taskid);\r\n                exitCode = -1;\r\n            }\r\n        } else if (failTask) {\r\n            TaskAttemptID taskID = TaskAttemptID.forName(taskid);\r\n            Job job = getJob(taskID.getJobID());\r\n            if (job == null) {\r\n                System.out.println(\"Could not find job \" + jobid);\r\n            } else if (job.killTask(taskID, true)) {\r\n                System.out.println(\"Killed task \" + taskID + \" by failing it\");\r\n                exitCode = 0;\r\n            } else {\r\n                System.out.println(\"Could not fail task \" + taskid);\r\n                exitCode = -1;\r\n            }\r\n        } else if (logs) {\r\n            JobID jobID = JobID.forName(jobid);\r\n            if (getJob(jobID) == null) {\r\n                System.out.println(\"Could not find job \" + jobid);\r\n            } else {\r\n                try {\r\n                    TaskAttemptID taskAttemptID = TaskAttemptID.forName(taskid);\r\n                    LogParams logParams = cluster.getLogParams(jobID, taskAttemptID);\r\n                    LogCLIHelpers logDumper = new LogCLIHelpers();\r\n                    logDumper.setConf(getConf());\r\n                    exitCode = logDumper.dumpAContainersLogs(logParams.getApplicationId(), logParams.getContainerId(), logParams.getNodeId(), logParams.getOwner());\r\n                } catch (IOException e) {\r\n                    if (e instanceof RemoteException) {\r\n                        throw e;\r\n                    }\r\n                    System.out.println(e.getMessage());\r\n                }\r\n            }\r\n        } else if (downloadConfig) {\r\n            Job job = getJob(JobID.forName(jobid));\r\n            if (job == null) {\r\n                System.out.println(\"Could not find job \" + jobid);\r\n            } else {\r\n                String jobFile = job.getJobFile();\r\n                if (jobFile == null || jobFile.isEmpty()) {\r\n                    System.out.println(\"Config file for job \" + jobFile + \" could not be found.\");\r\n                } else {\r\n                    Path configPath = new Path(jobFile);\r\n                    FileSystem fs = FileSystem.get(getConf());\r\n                    fs.copyToLocalFile(configPath, new Path(configOutFile));\r\n                    exitCode = 0;\r\n                }\r\n            }\r\n        }\r\n    } catch (RemoteException re) {\r\n        IOException unwrappedException = re.unwrapRemoteException();\r\n        if (unwrappedException instanceof AccessControlException) {\r\n            System.out.println(unwrappedException.getMessage());\r\n        } else {\r\n            throw re;\r\n        }\r\n    } finally {\r\n        cluster.close();\r\n    }\r\n    return exitCode;\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 4,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\tools",
  "methodName" : "createCluster",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Cluster createCluster() throws IOException\n{\r\n    return new Cluster(getConf());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\tools",
  "methodName" : "getJobPriorityNames",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "String getJobPriorityNames()\n{\r\n    StringBuffer sb = new StringBuffer();\r\n    for (JobPriority p : JobPriority.values()) {\r\n        if (JobPriority.UNDEFINED_PRIORITY == p) {\r\n            continue;\r\n        }\r\n        sb.append(p.name()).append(\" \");\r\n    }\r\n    return sb.substring(0, sb.length() - 1);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\tools",
  "methodName" : "getTaskTypes",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String getTaskTypes()\n{\r\n    return StringUtils.join(taskTypes, \" \");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\tools",
  "methodName" : "displayUsage",
  "errType" : null,
  "containingMethodsNum" : 46,
  "sourceCodeText" : "void displayUsage(String cmd)\n{\r\n    String prefix = \"Usage: job \";\r\n    String jobPriorityValues = getJobPriorityNames();\r\n    String taskStates = \"pending, running, completed, failed, killed\";\r\n    if (\"-submit\".equals(cmd)) {\r\n        System.err.println(prefix + \"[\" + cmd + \" <job-file>]\");\r\n    } else if (\"-status\".equals(cmd) || \"-kill\".equals(cmd)) {\r\n        System.err.println(prefix + \"[\" + cmd + \" <job-id>]\");\r\n    } else if (\"-counter\".equals(cmd)) {\r\n        System.err.println(prefix + \"[\" + cmd + \" <job-id> <group-name> <counter-name>]\");\r\n    } else if (\"-events\".equals(cmd)) {\r\n        System.err.println(prefix + \"[\" + cmd + \" <job-id> <from-event-#> <#-of-events>]. Event #s start from 1.\");\r\n    } else if (\"-history\".equals(cmd)) {\r\n        System.err.println(prefix + \"[\" + cmd + \" [all] <jobHistoryFile|jobId> \" + \"[-outfile <file>] [-format <human|json>]]\");\r\n    } else if (\"-list\".equals(cmd)) {\r\n        System.err.println(prefix + \"[\" + cmd + \" [all]]\");\r\n    } else if (\"-kill-task\".equals(cmd) || \"-fail-task\".equals(cmd)) {\r\n        System.err.println(prefix + \"[\" + cmd + \" <task-attempt-id>]\");\r\n    } else if (\"-set-priority\".equals(cmd)) {\r\n        System.err.println(prefix + \"[\" + cmd + \" <job-id> <priority>]. \" + \"Valid values for priorities are: \" + jobPriorityValues + \". In addition to this, integers also can be used.\");\r\n    } else if (\"-list-active-trackers\".equals(cmd)) {\r\n        System.err.println(prefix + \"[\" + cmd + \"]\");\r\n    } else if (\"-list-blacklisted-trackers\".equals(cmd)) {\r\n        System.err.println(prefix + \"[\" + cmd + \"]\");\r\n    } else if (\"-list-attempt-ids\".equals(cmd)) {\r\n        System.err.println(prefix + \"[\" + cmd + \" <job-id> <task-type> <task-state>]. \" + \"Valid values for <task-type> are \" + getTaskTypes() + \". \" + \"Valid values for <task-state> are \" + taskStates);\r\n    } else if (\"-logs\".equals(cmd)) {\r\n        System.err.println(prefix + \"[\" + cmd + \" <job-id> <task-attempt-id>]. \" + \" <task-attempt-id> is optional to get task attempt logs.\");\r\n    } else if (\"-config\".equals(cmd)) {\r\n        System.err.println(prefix + \"[\" + cmd + \" <job-id> <file>]\");\r\n    } else {\r\n        System.err.printf(prefix + \"<command> <args>%n\");\r\n        System.err.printf(\"\\t[-submit <job-file>]%n\");\r\n        System.err.printf(\"\\t[-status <job-id>]%n\");\r\n        System.err.printf(\"\\t[-counter <job-id> <group-name> <counter-name>]%n\");\r\n        System.err.printf(\"\\t[-kill <job-id>]%n\");\r\n        System.err.printf(\"\\t[-set-priority <job-id> <priority>]. \" + \"Valid values for priorities are: \" + jobPriorityValues + \". In addition to this, integers also can be used.\" + \"%n\");\r\n        System.err.printf(\"\\t[-events <job-id> <from-event-#> <#-of-events>]%n\");\r\n        System.err.printf(\"\\t[-history [all] <jobHistoryFile|jobId> \" + \"[-outfile <file>] [-format <human|json>]]%n\");\r\n        System.err.printf(\"\\t[-list [all]]%n\");\r\n        System.err.printf(\"\\t[-list-active-trackers]%n\");\r\n        System.err.printf(\"\\t[-list-blacklisted-trackers]%n\");\r\n        System.err.println(\"\\t[-list-attempt-ids <job-id> <task-type> \" + \"<task-state>]. \" + \"Valid values for <task-type> are \" + getTaskTypes() + \". \" + \"Valid values for <task-state> are \" + taskStates);\r\n        System.err.printf(\"\\t[-kill-task <task-attempt-id>]%n\");\r\n        System.err.printf(\"\\t[-fail-task <task-attempt-id>]%n\");\r\n        System.err.printf(\"\\t[-logs <job-id> <task-attempt-id>]%n\");\r\n        System.err.printf(\"\\t[-config <job-id> <file>%n%n\");\r\n        ToolRunner.printGenericCommandUsage(System.out);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\tools",
  "methodName" : "viewHistory",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void viewHistory(String historyFile, boolean all, String historyOutFile, String format) throws IOException\n{\r\n    HistoryViewer historyViewer = new HistoryViewer(historyFile, getConf(), all, format);\r\n    PrintStream ps = System.out;\r\n    if (historyOutFile != null) {\r\n        ps = new PrintStream(new BufferedOutputStream(new FileOutputStream(new File(historyOutFile))), true, \"UTF-8\");\r\n    }\r\n    historyViewer.print(ps);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\tools",
  "methodName" : "getCounter",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "long getCounter(Counters counters, String counterGroupName, String counterName) throws IOException\n{\r\n    return counters.findCounter(counterGroupName, counterName).getValue();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\tools",
  "methodName" : "listEvents",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void listEvents(Job job, int fromEventId, int numEvents) throws IOException, InterruptedException\n{\r\n    TaskCompletionEvent[] events = job.getTaskCompletionEvents(fromEventId, numEvents);\r\n    System.out.println(\"Task completion events for \" + job.getJobID());\r\n    System.out.println(\"Number of events (from \" + fromEventId + \") are: \" + events.length);\r\n    for (TaskCompletionEvent event : events) {\r\n        System.out.println(event.getStatus() + \" \" + event.getTaskAttemptId() + \" \" + getTaskLogURL(event.getTaskAttemptId(), event.getTaskTrackerHttp()));\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\tools",
  "methodName" : "getTaskLogURL",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getTaskLogURL(TaskAttemptID taskId, String baseUrl)\n{\r\n    return (baseUrl + \"/tasklog?plaintext=true&attemptid=\" + taskId);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\tools",
  "methodName" : "getJob",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "Job getJob(JobID jobid) throws IOException, InterruptedException\n{\r\n    int maxRetry = getConf().getInt(MRJobConfig.MR_CLIENT_JOB_MAX_RETRIES, MRJobConfig.DEFAULT_MR_CLIENT_JOB_MAX_RETRIES);\r\n    long retryInterval = getConf().getLong(MRJobConfig.MR_CLIENT_JOB_RETRY_INTERVAL, MRJobConfig.DEFAULT_MR_CLIENT_JOB_RETRY_INTERVAL);\r\n    Job job = cluster.getJob(jobid);\r\n    for (int i = 0; i < maxRetry; ++i) {\r\n        if (job != null) {\r\n            return job;\r\n        }\r\n        LOG.info(\"Could not obtain job info after \" + String.valueOf(i + 1) + \" attempt(s). Sleeping for \" + String.valueOf(retryInterval / 1000) + \" seconds and retrying.\");\r\n        Thread.sleep(retryInterval);\r\n        job = cluster.getJob(jobid);\r\n    }\r\n    return job;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\tools",
  "methodName" : "listJobs",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void listJobs(Cluster cluster) throws IOException, InterruptedException\n{\r\n    List<JobStatus> runningJobs = new ArrayList<JobStatus>();\r\n    for (JobStatus job : cluster.getAllJobStatuses()) {\r\n        if (!job.isJobComplete()) {\r\n            runningJobs.add(job);\r\n        }\r\n    }\r\n    displayJobList(runningJobs.toArray(new JobStatus[0]));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\tools",
  "methodName" : "listAllJobs",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void listAllJobs(Cluster cluster) throws IOException, InterruptedException\n{\r\n    displayJobList(cluster.getAllJobStatuses());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\tools",
  "methodName" : "listActiveTrackers",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void listActiveTrackers(Cluster cluster) throws IOException, InterruptedException\n{\r\n    TaskTrackerInfo[] trackers = cluster.getActiveTaskTrackers();\r\n    for (TaskTrackerInfo tracker : trackers) {\r\n        System.out.println(tracker.getTaskTrackerName());\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\tools",
  "methodName" : "listBlacklistedTrackers",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void listBlacklistedTrackers(Cluster cluster) throws IOException, InterruptedException\n{\r\n    TaskTrackerInfo[] trackers = cluster.getBlackListedTaskTrackers();\r\n    if (trackers.length > 0) {\r\n        System.out.println(\"BlackListedNode \\t Reason\");\r\n    }\r\n    for (TaskTrackerInfo tracker : trackers) {\r\n        System.out.println(tracker.getTaskTrackerName() + \"\\t\" + tracker.getReasonForBlacklist());\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\tools",
  "methodName" : "printTaskAttempts",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void printTaskAttempts(TaskReport report)\n{\r\n    if (report.getCurrentStatus() == TIPStatus.COMPLETE) {\r\n        System.out.println(report.getSuccessfulTaskAttemptId());\r\n    } else if (report.getCurrentStatus() == TIPStatus.RUNNING) {\r\n        for (TaskAttemptID t : report.getRunningTaskAttemptIds()) {\r\n            System.out.println(t);\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\tools",
  "methodName" : "displayTasks",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void displayTasks(Job job, String type, String state) throws IOException, InterruptedException\n{\r\n    TaskReport[] reports = null;\r\n    reports = job.getTaskReports(TaskType.valueOf(org.apache.hadoop.util.StringUtils.toUpperCase(type)));\r\n    for (TaskReport report : reports) {\r\n        TIPStatus status = report.getCurrentStatus();\r\n        if ((state.equalsIgnoreCase(\"pending\") && status == TIPStatus.PENDING) || (state.equalsIgnoreCase(\"running\") && status == TIPStatus.RUNNING) || (state.equalsIgnoreCase(\"completed\") && status == TIPStatus.COMPLETE) || (state.equalsIgnoreCase(\"failed\") && status == TIPStatus.FAILED) || (state.equalsIgnoreCase(\"killed\") && status == TIPStatus.KILLED)) {\r\n            printTaskAttempts(report);\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\tools",
  "methodName" : "displayJobList",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void displayJobList(JobStatus[] jobs) throws IOException, InterruptedException\n{\r\n    displayJobList(jobs, new PrintWriter(new OutputStreamWriter(System.out, Charsets.UTF_8)));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\tools",
  "methodName" : "displayJobList",
  "errType" : null,
  "containingMethodsNum" : 10,
  "sourceCodeText" : "void displayJobList(JobStatus[] jobs, PrintWriter writer)\n{\r\n    writer.println(\"Total jobs:\" + jobs.length);\r\n    writer.printf(headerPattern, \"JobId\", \"JobName\", \"State\", \"StartTime\", \"UserName\", \"Queue\", \"Priority\", \"UsedContainers\", \"RsvdContainers\", \"UsedMem\", \"RsvdMem\", \"NeededMem\", \"AM info\");\r\n    for (JobStatus job : jobs) {\r\n        int numUsedSlots = job.getNumUsedSlots();\r\n        int numReservedSlots = job.getNumReservedSlots();\r\n        long usedMem = job.getUsedMem();\r\n        long rsvdMem = job.getReservedMem();\r\n        long neededMem = job.getNeededMem();\r\n        int jobNameLength = job.getJobName().length();\r\n        writer.printf(dataPattern, job.getJobID().toString(), job.getJobName().substring(0, jobNameLength > 40 ? 40 : jobNameLength), job.getState(), job.getStartTime(), job.getUsername(), job.getQueue(), job.getPriority().name(), numUsedSlots < 0 ? UNAVAILABLE : numUsedSlots, numReservedSlots < 0 ? UNAVAILABLE : numReservedSlots, usedMem < 0 ? UNAVAILABLE : String.format(memPattern, usedMem), rsvdMem < 0 ? UNAVAILABLE : String.format(memPattern, rsvdMem), neededMem < 0 ? UNAVAILABLE : String.format(memPattern, neededMem), job.getSchedulingInfo());\r\n    }\r\n    writer.flush();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\tools",
  "methodName" : "main",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void main(String[] argv) throws Exception\n{\r\n    int res = ToolRunner.run(new CLI(), argv);\r\n    ExitUtil.terminate(res);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output",
  "methodName" : "checkTokenName",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void checkTokenName(String namedOutput)\n{\r\n    if (namedOutput == null || namedOutput.length() == 0) {\r\n        throw new IllegalArgumentException(\"Name cannot be NULL or emtpy\");\r\n    }\r\n    for (char ch : namedOutput.toCharArray()) {\r\n        if ((ch >= 'A') && (ch <= 'Z')) {\r\n            continue;\r\n        }\r\n        if ((ch >= 'a') && (ch <= 'z')) {\r\n            continue;\r\n        }\r\n        if ((ch >= '0') && (ch <= '9')) {\r\n            continue;\r\n        }\r\n        throw new IllegalArgumentException(\"Name cannot be have a '\" + ch + \"' char\");\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output",
  "methodName" : "checkBaseOutputPath",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void checkBaseOutputPath(String outputPath)\n{\r\n    if (outputPath.equals(FileOutputFormat.PART)) {\r\n        throw new IllegalArgumentException(\"output name cannot be 'part'\");\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output",
  "methodName" : "checkNamedOutputName",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void checkNamedOutputName(JobContext job, String namedOutput, boolean alreadyDefined)\n{\r\n    checkTokenName(namedOutput);\r\n    checkBaseOutputPath(namedOutput);\r\n    List<String> definedChannels = getNamedOutputsList(job);\r\n    if (alreadyDefined && definedChannels.contains(namedOutput)) {\r\n        throw new IllegalArgumentException(\"Named output '\" + namedOutput + \"' already alreadyDefined\");\r\n    } else if (!alreadyDefined && !definedChannels.contains(namedOutput)) {\r\n        throw new IllegalArgumentException(\"Named output '\" + namedOutput + \"' not defined\");\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output",
  "methodName" : "getNamedOutputsList",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "List<String> getNamedOutputsList(JobContext job)\n{\r\n    List<String> names = new ArrayList<String>();\r\n    StringTokenizer st = new StringTokenizer(job.getConfiguration().get(MULTIPLE_OUTPUTS, \"\"), \" \");\r\n    while (st.hasMoreTokens()) {\r\n        names.add(st.nextToken());\r\n    }\r\n    return names;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output",
  "methodName" : "getNamedOutputFormatClass",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Class<? extends OutputFormat<?, ?>> getNamedOutputFormatClass(JobContext job, String namedOutput)\n{\r\n    return (Class<? extends OutputFormat<?, ?>>) job.getConfiguration().getClass(MO_PREFIX + namedOutput + FORMAT, null, OutputFormat.class);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output",
  "methodName" : "getNamedOutputKeyClass",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Class<?> getNamedOutputKeyClass(JobContext job, String namedOutput)\n{\r\n    return job.getConfiguration().getClass(MO_PREFIX + namedOutput + KEY, null, Object.class);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output",
  "methodName" : "getNamedOutputValueClass",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Class<?> getNamedOutputValueClass(JobContext job, String namedOutput)\n{\r\n    return job.getConfiguration().getClass(MO_PREFIX + namedOutput + VALUE, null, Object.class);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output",
  "methodName" : "addNamedOutput",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void addNamedOutput(Job job, String namedOutput, Class<? extends OutputFormat> outputFormatClass, Class<?> keyClass, Class<?> valueClass)\n{\r\n    checkNamedOutputName(job, namedOutput, true);\r\n    Configuration conf = job.getConfiguration();\r\n    conf.set(MULTIPLE_OUTPUTS, conf.get(MULTIPLE_OUTPUTS, \"\") + \" \" + namedOutput);\r\n    conf.setClass(MO_PREFIX + namedOutput + FORMAT, outputFormatClass, OutputFormat.class);\r\n    conf.setClass(MO_PREFIX + namedOutput + KEY, keyClass, Object.class);\r\n    conf.setClass(MO_PREFIX + namedOutput + VALUE, valueClass, Object.class);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output",
  "methodName" : "setCountersEnabled",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setCountersEnabled(Job job, boolean enabled)\n{\r\n    job.getConfiguration().setBoolean(COUNTERS_ENABLED, enabled);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output",
  "methodName" : "getCountersEnabled",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "boolean getCountersEnabled(JobContext job)\n{\r\n    return job.getConfiguration().getBoolean(COUNTERS_ENABLED, false);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output",
  "methodName" : "write",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void write(String namedOutput, K key, V value) throws IOException, InterruptedException\n{\r\n    write(namedOutput, key, value, namedOutput);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output",
  "methodName" : "write",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void write(String namedOutput, K key, V value, String baseOutputPath) throws IOException, InterruptedException\n{\r\n    checkNamedOutputName(context, namedOutput, false);\r\n    checkBaseOutputPath(baseOutputPath);\r\n    if (!namedOutputs.contains(namedOutput)) {\r\n        throw new IllegalArgumentException(\"Undefined named output '\" + namedOutput + \"'\");\r\n    }\r\n    TaskAttemptContext taskContext = getContext(namedOutput);\r\n    getRecordWriter(taskContext, baseOutputPath).write(key, value);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output",
  "methodName" : "write",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void write(KEYOUT key, VALUEOUT value, String baseOutputPath) throws IOException, InterruptedException\n{\r\n    checkBaseOutputPath(baseOutputPath);\r\n    if (jobOutputFormatContext == null) {\r\n        jobOutputFormatContext = new TaskAttemptContextImpl(context.getConfiguration(), context.getTaskAttemptID(), new WrappedStatusReporter(context));\r\n    }\r\n    getRecordWriter(jobOutputFormatContext, baseOutputPath).write(key, value);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output",
  "methodName" : "getRecordWriter",
  "errType" : [ "ClassNotFoundException" ],
  "containingMethodsNum" : 4,
  "sourceCodeText" : "RecordWriter getRecordWriter(TaskAttemptContext taskContext, String baseFileName) throws IOException, InterruptedException\n{\r\n    RecordWriter writer = recordWriters.get(baseFileName);\r\n    if (writer == null) {\r\n        FileOutputFormat.setOutputName(taskContext, baseFileName);\r\n        try {\r\n            writer = ((OutputFormat) ReflectionUtils.newInstance(taskContext.getOutputFormatClass(), taskContext.getConfiguration())).getRecordWriter(taskContext);\r\n        } catch (ClassNotFoundException e) {\r\n            throw new IOException(e);\r\n        }\r\n        if (countersEnabled) {\r\n            writer = new RecordWriterWithCounter(writer, baseFileName, context);\r\n        }\r\n        recordWriters.put(baseFileName, writer);\r\n    }\r\n    return writer;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output",
  "methodName" : "getContext",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "TaskAttemptContext getContext(String nameOutput) throws IOException\n{\r\n    TaskAttemptContext taskContext = taskContexts.get(nameOutput);\r\n    if (taskContext != null) {\r\n        return taskContext;\r\n    }\r\n    Job job = Job.getInstance(context.getConfiguration());\r\n    job.setOutputFormatClass(getNamedOutputFormatClass(context, nameOutput));\r\n    job.setOutputKeyClass(getNamedOutputKeyClass(context, nameOutput));\r\n    job.setOutputValueClass(getNamedOutputValueClass(context, nameOutput));\r\n    taskContext = new TaskAttemptContextImpl(job.getConfiguration(), context.getTaskAttemptID(), new WrappedStatusReporter(context));\r\n    taskContexts.put(nameOutput, taskContext);\r\n    return taskContext;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output",
  "methodName" : "close",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void close() throws IOException, InterruptedException\n{\r\n    for (RecordWriter writer : recordWriters.values()) {\r\n        writer.close(context);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getPath",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Path getPath()\n{\r\n    return fs.getPath();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getStart",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "long getStart()\n{\r\n    return fs.getStart();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getLength",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "long getLength()\n{\r\n    return fs.getLength();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "toString",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String toString()\n{\r\n    return fs.toString();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "write",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void write(DataOutput out) throws IOException\n{\r\n    fs.write(out);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "readFields",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void readFields(DataInput in) throws IOException\n{\r\n    fs.readFields(in);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getLocations",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String[] getLocations() throws IOException\n{\r\n    return fs.getLocations();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getLocationInfo",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "SplitLocationInfo[] getLocationInfo() throws IOException\n{\r\n    return fs.getLocationInfo();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\task\\reduce",
  "methodName" : "reset",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void reset(int offset)\n{\r\n    memDataIn.reset(buffer, start + offset, length - start - offset);\r\n    bytesRead = offset;\r\n    eof = false;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\task\\reduce",
  "methodName" : "getPosition",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getPosition() throws IOException\n{\r\n    return bytesRead;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\task\\reduce",
  "methodName" : "getLength",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getLength()\n{\r\n    return fileLength;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\task\\reduce",
  "methodName" : "dumpOnError",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void dumpOnError()\n{\r\n    File dumpFile = new File(\"../output/\" + taskAttemptId + \".dump\");\r\n    System.err.println(\"Dumping corrupt map-output of \" + taskAttemptId + \" to \" + dumpFile.getAbsolutePath());\r\n    try (FileOutputStream fos = new FileOutputStream(dumpFile)) {\r\n        fos.write(buffer, 0, bufferSize);\r\n    } catch (IOException ioe) {\r\n        System.err.println(\"Failed to dump map-output of \" + taskAttemptId);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\task\\reduce",
  "methodName" : "nextRawKey",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 6,
  "sourceCodeText" : "boolean nextRawKey(DataInputBuffer key) throws IOException\n{\r\n    try {\r\n        if (!positionToNextRecord(memDataIn)) {\r\n            return false;\r\n        }\r\n        int pos = memDataIn.getPosition();\r\n        byte[] data = memDataIn.getData();\r\n        key.reset(data, pos, currentKeyLength);\r\n        long skipped = memDataIn.skip(currentKeyLength);\r\n        if (skipped != currentKeyLength) {\r\n            throw new IOException(\"Rec# \" + recNo + \": Failed to skip past key of length: \" + currentKeyLength);\r\n        }\r\n        bytesRead += currentKeyLength;\r\n        return true;\r\n    } catch (IOException ioe) {\r\n        dumpOnError();\r\n        throw ioe;\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\task\\reduce",
  "methodName" : "nextRawValue",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void nextRawValue(DataInputBuffer value) throws IOException\n{\r\n    try {\r\n        int pos = memDataIn.getPosition();\r\n        byte[] data = memDataIn.getData();\r\n        value.reset(data, pos, currentValueLength);\r\n        long skipped = memDataIn.skip(currentValueLength);\r\n        if (skipped != currentValueLength) {\r\n            throw new IOException(\"Rec# \" + recNo + \": Failed to skip past value of length: \" + currentValueLength);\r\n        }\r\n        bytesRead += currentValueLength;\r\n        ++recNo;\r\n    } catch (IOException ioe) {\r\n        dumpOnError();\r\n        throw ioe;\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\task\\reduce",
  "methodName" : "close",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void close()\n{\r\n    dataIn = null;\r\n    buffer = null;\r\n    if (merger != null) {\r\n        merger.unreserve(bufferSize);\r\n    }\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "setRecordLength",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setRecordLength(Configuration conf, int recordLength)\n{\r\n    conf.setInt(FIXED_RECORD_LENGTH, recordLength);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getRecordLength",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "int getRecordLength(Configuration conf)\n{\r\n    return conf.getInt(FIXED_RECORD_LENGTH, 0);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "configure",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void configure(JobConf conf)\n{\r\n    compressionCodecs = new CompressionCodecFactory(conf);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getRecordReader",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "RecordReader<LongWritable, BytesWritable> getRecordReader(InputSplit genericSplit, JobConf job, Reporter reporter) throws IOException\n{\r\n    reporter.setStatus(genericSplit.toString());\r\n    int recordLength = getRecordLength(job);\r\n    if (recordLength <= 0) {\r\n        throw new IOException(\"Fixed record length \" + recordLength + \" is invalid.  It should be set to a value greater than zero\");\r\n    }\r\n    return new FixedLengthRecordReader(job, (FileSplit) genericSplit, recordLength);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "isSplitable",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "boolean isSplitable(FileSystem fs, Path file)\n{\r\n    final CompressionCodec codec = compressionCodecs.getCodec(file);\r\n    return (null == codec);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\split",
  "methodName" : "readSplitMetaInfo",
  "errType" : null,
  "containingMethodsNum" : 16,
  "sourceCodeText" : "JobSplit.TaskSplitMetaInfo[] readSplitMetaInfo(JobID jobId, FileSystem fs, Configuration conf, Path jobSubmitDir) throws IOException\n{\r\n    long maxMetaInfoSize = conf.getLong(MRJobConfig.SPLIT_METAINFO_MAXSIZE, MRJobConfig.DEFAULT_SPLIT_METAINFO_MAXSIZE);\r\n    Path metaSplitFile = JobSubmissionFiles.getJobSplitMetaFile(jobSubmitDir);\r\n    String jobSplitFile = JobSubmissionFiles.getJobSplitFile(jobSubmitDir).toString();\r\n    FileStatus fStatus = fs.getFileStatus(metaSplitFile);\r\n    if (maxMetaInfoSize > 0 && fStatus.getLen() > maxMetaInfoSize) {\r\n        throw new IOException(\"Split metadata size exceeded \" + maxMetaInfoSize + \". Aborting job \" + jobId);\r\n    }\r\n    FSDataInputStream in = fs.open(metaSplitFile);\r\n    byte[] header = new byte[JobSplit.META_SPLIT_FILE_HEADER.length];\r\n    in.readFully(header);\r\n    if (!Arrays.equals(JobSplit.META_SPLIT_FILE_HEADER, header)) {\r\n        throw new IOException(\"Invalid header on split file\");\r\n    }\r\n    int vers = WritableUtils.readVInt(in);\r\n    if (vers != JobSplit.META_SPLIT_VERSION) {\r\n        in.close();\r\n        throw new IOException(\"Unsupported split version \" + vers);\r\n    }\r\n    int numSplits = WritableUtils.readVInt(in);\r\n    JobSplit.TaskSplitMetaInfo[] allSplitMetaInfo = new JobSplit.TaskSplitMetaInfo[numSplits];\r\n    for (int i = 0; i < numSplits; i++) {\r\n        JobSplit.SplitMetaInfo splitMetaInfo = new JobSplit.SplitMetaInfo();\r\n        splitMetaInfo.readFields(in);\r\n        JobSplit.TaskSplitIndex splitIndex = new JobSplit.TaskSplitIndex(jobSplitFile, splitMetaInfo.getStartOffset());\r\n        allSplitMetaInfo[i] = new JobSplit.TaskSplitMetaInfo(splitIndex, splitMetaInfo.getLocations(), splitMetaInfo.getInputDataLength());\r\n    }\r\n    in.close();\r\n    return allSplitMetaInfo;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred\\lib",
  "methodName" : "map",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void map(K key, V val, OutputCollector<K, V> output, Reporter reporter) throws IOException\n{\r\n    output.collect(key, val);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "print",
  "errType" : null,
  "containingMethodsNum" : 23,
  "sourceCodeText" : "void print(PrintStream ps) throws IOException\n{\r\n    printJobDetails(ps);\r\n    printTaskSummary(ps);\r\n    printJobAnalysis(ps);\r\n    printTasks(ps, TaskType.JOB_SETUP, TaskStatus.State.FAILED.toString());\r\n    printTasks(ps, TaskType.JOB_SETUP, TaskStatus.State.KILLED.toString());\r\n    printTasks(ps, TaskType.MAP, TaskStatus.State.FAILED.toString());\r\n    printTasks(ps, TaskType.MAP, TaskStatus.State.KILLED.toString());\r\n    printTasks(ps, TaskType.REDUCE, TaskStatus.State.FAILED.toString());\r\n    printTasks(ps, TaskType.REDUCE, TaskStatus.State.KILLED.toString());\r\n    printTasks(ps, TaskType.JOB_CLEANUP, TaskStatus.State.FAILED.toString());\r\n    printTasks(ps, TaskType.JOB_CLEANUP, JobStatus.getJobRunState(JobStatus.KILLED));\r\n    if (printAll) {\r\n        printTasks(ps, TaskType.JOB_SETUP, TaskStatus.State.SUCCEEDED.toString());\r\n        printTasks(ps, TaskType.MAP, TaskStatus.State.SUCCEEDED.toString());\r\n        printTasks(ps, TaskType.REDUCE, TaskStatus.State.SUCCEEDED.toString());\r\n        printTasks(ps, TaskType.JOB_CLEANUP, TaskStatus.State.SUCCEEDED.toString());\r\n        printAllTaskAttempts(ps, TaskType.JOB_SETUP);\r\n        printAllTaskAttempts(ps, TaskType.MAP);\r\n        printAllTaskAttempts(ps, TaskType.REDUCE);\r\n        printAllTaskAttempts(ps, TaskType.JOB_CLEANUP);\r\n    }\r\n    HistoryViewer.FilteredJob filter = new HistoryViewer.FilteredJob(job, TaskStatus.State.FAILED.toString());\r\n    printFailedAttempts(ps, filter);\r\n    filter = new HistoryViewer.FilteredJob(job, TaskStatus.State.KILLED.toString());\r\n    printFailedAttempts(ps, filter);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "printJobDetails",
  "errType" : null,
  "containingMethodsNum" : 13,
  "sourceCodeText" : "void printJobDetails(PrintStream ps)\n{\r\n    StringBuilder jobDetails = new StringBuilder();\r\n    jobDetails.append(\"\\nHadoop job: \").append(job.getJobId());\r\n    jobDetails.append(\"\\n=====================================\");\r\n    jobDetails.append(\"\\nUser: \").append(job.getUsername());\r\n    jobDetails.append(\"\\nJobName: \").append(job.getJobname());\r\n    jobDetails.append(\"\\nJobConf: \").append(job.getJobConfPath());\r\n    jobDetails.append(\"\\nSubmitted At: \").append(StringUtils.getFormattedTimeWithDiff(dateFormat, job.getSubmitTime(), 0));\r\n    jobDetails.append(\"\\nLaunched At: \").append(StringUtils.getFormattedTimeWithDiff(dateFormat, job.getLaunchTime(), job.getSubmitTime()));\r\n    jobDetails.append(\"\\nFinished At: \").append(StringUtils.getFormattedTimeWithDiff(dateFormat, job.getFinishTime(), job.getLaunchTime()));\r\n    jobDetails.append(\"\\nStatus: \").append(((job.getJobStatus() == null) ? \"Incomplete\" : job.getJobStatus()));\r\n    printJobCounters(jobDetails, job.getTotalCounters(), job.getMapCounters(), job.getReduceCounters());\r\n    jobDetails.append(\"\\n\");\r\n    jobDetails.append(\"\\n=====================================\");\r\n    ps.println(jobDetails);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "printJobCounters",
  "errType" : null,
  "containingMethodsNum" : 15,
  "sourceCodeText" : "void printJobCounters(StringBuilder buff, Counters totalCounters, Counters mapCounters, Counters reduceCounters)\n{\r\n    if (totalCounters != null) {\r\n        buff.append(\"\\nCounters: \\n\\n\");\r\n        buff.append(String.format(\"|%1$-30s|%2$-30s|%3$-10s|%4$-10s|%5$-10s|\", \"Group Name\", \"Counter name\", \"Map Value\", \"Reduce Value\", \"Total Value\"));\r\n        buff.append(\"\\n------------------------------------------\" + \"---------------------------------------------\");\r\n        for (CounterGroup counterGroup : totalCounters) {\r\n            String groupName = counterGroup.getName();\r\n            CounterGroup totalGroup = totalCounters.getGroup(groupName);\r\n            CounterGroup mapGroup = mapCounters.getGroup(groupName);\r\n            CounterGroup reduceGroup = reduceCounters.getGroup(groupName);\r\n            Format decimal = new DecimalFormat();\r\n            Iterator<Counter> ctrItr = totalGroup.iterator();\r\n            while (ctrItr.hasNext()) {\r\n                org.apache.hadoop.mapreduce.Counter counter = ctrItr.next();\r\n                String name = counter.getName();\r\n                String mapValue = decimal.format(mapGroup.findCounter(name).getValue());\r\n                String reduceValue = decimal.format(reduceGroup.findCounter(name).getValue());\r\n                String totalValue = decimal.format(counter.getValue());\r\n                buff.append(String.format(\"%n|%1$-30s|%2$-30s|%3$-10s|%4$-10s|%5$-10s\", totalGroup.getDisplayName(), counter.getDisplayName(), mapValue, reduceValue, totalValue));\r\n            }\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "printAllTaskAttempts",
  "errType" : null,
  "containingMethodsNum" : 26,
  "sourceCodeText" : "void printAllTaskAttempts(PrintStream ps, TaskType taskType)\n{\r\n    Map<TaskID, JobHistoryParser.TaskInfo> tasks = job.getAllTasks();\r\n    StringBuilder taskList = new StringBuilder();\r\n    taskList.append(\"\\n\").append(taskType);\r\n    taskList.append(\" task list for \").append(job.getJobId());\r\n    taskList.append(\"\\nTaskId\\t\\tStartTime\");\r\n    if (TaskType.REDUCE.equals(taskType)) {\r\n        taskList.append(\"\\tShuffleFinished\\tSortFinished\");\r\n    }\r\n    taskList.append(\"\\tFinishTime\\tHostName\\tError\\tTaskLogs\");\r\n    taskList.append(\"\\n====================================================\");\r\n    ps.println(taskList.toString());\r\n    for (JobHistoryParser.TaskInfo task : tasks.values()) {\r\n        for (JobHistoryParser.TaskAttemptInfo attempt : task.getAllTaskAttempts().values()) {\r\n            if (taskType.equals(task.getTaskType())) {\r\n                taskList.setLength(0);\r\n                taskList.append(attempt.getAttemptId()).append(\"\\t\");\r\n                taskList.append(StringUtils.getFormattedTimeWithDiff(dateFormat, attempt.getStartTime(), 0)).append(\"\\t\");\r\n                if (TaskType.REDUCE.equals(taskType)) {\r\n                    taskList.append(StringUtils.getFormattedTimeWithDiff(dateFormat, attempt.getShuffleFinishTime(), attempt.getStartTime()));\r\n                    taskList.append(\"\\t\");\r\n                    taskList.append(StringUtils.getFormattedTimeWithDiff(dateFormat, attempt.getSortFinishTime(), attempt.getShuffleFinishTime()));\r\n                }\r\n                taskList.append(StringUtils.getFormattedTimeWithDiff(dateFormat, attempt.getFinishTime(), attempt.getStartTime()));\r\n                taskList.append(\"\\t\");\r\n                taskList.append(attempt.getHostname()).append(\"\\t\");\r\n                taskList.append(attempt.getError());\r\n                String taskLogsUrl = HistoryViewer.getTaskLogsUrl(scheme, attempt);\r\n                taskList.append(taskLogsUrl != null ? taskLogsUrl : \"n/a\");\r\n                ps.println(taskList);\r\n            }\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "printTaskSummary",
  "errType" : null,
  "containingMethodsNum" : 31,
  "sourceCodeText" : "void printTaskSummary(PrintStream ps)\n{\r\n    HistoryViewer.SummarizedJob ts = new HistoryViewer.SummarizedJob(job);\r\n    StringBuilder taskSummary = new StringBuilder();\r\n    taskSummary.append(\"\\nTask Summary\");\r\n    taskSummary.append(\"\\n============================\");\r\n    taskSummary.append(\"\\nKind\\tTotal\\t\");\r\n    taskSummary.append(\"Successful\\tFailed\\tKilled\\tStartTime\\tFinishTime\");\r\n    taskSummary.append(\"\\n\");\r\n    taskSummary.append(\"\\nSetup\\t\").append(ts.totalSetups);\r\n    taskSummary.append(\"\\t\").append(ts.numFinishedSetups);\r\n    taskSummary.append(\"\\t\\t\").append(ts.numFailedSetups);\r\n    taskSummary.append(\"\\t\").append(ts.numKilledSetups);\r\n    taskSummary.append(\"\\t\").append(StringUtils.getFormattedTimeWithDiff(dateFormat, ts.setupStarted, 0));\r\n    taskSummary.append(\"\\t\").append(StringUtils.getFormattedTimeWithDiff(dateFormat, ts.setupFinished, ts.setupStarted));\r\n    taskSummary.append(\"\\nMap\\t\").append(ts.totalMaps);\r\n    taskSummary.append(\"\\t\").append(job.getSucceededMaps());\r\n    taskSummary.append(\"\\t\\t\").append(ts.numFailedMaps);\r\n    taskSummary.append(\"\\t\").append(ts.numKilledMaps);\r\n    taskSummary.append(\"\\t\").append(StringUtils.getFormattedTimeWithDiff(dateFormat, ts.mapStarted, 0));\r\n    taskSummary.append(\"\\t\").append(StringUtils.getFormattedTimeWithDiff(dateFormat, ts.mapFinished, ts.mapStarted));\r\n    taskSummary.append(\"\\nReduce\\t\").append(ts.totalReduces);\r\n    taskSummary.append(\"\\t\").append(job.getSucceededReduces());\r\n    taskSummary.append(\"\\t\\t\").append(ts.numFailedReduces);\r\n    taskSummary.append(\"\\t\").append(ts.numKilledReduces);\r\n    taskSummary.append(\"\\t\").append(StringUtils.getFormattedTimeWithDiff(dateFormat, ts.reduceStarted, 0));\r\n    taskSummary.append(\"\\t\").append(StringUtils.getFormattedTimeWithDiff(dateFormat, ts.reduceFinished, ts.reduceStarted));\r\n    taskSummary.append(\"\\nCleanup\\t\").append(ts.totalCleanups);\r\n    taskSummary.append(\"\\t\").append(ts.numFinishedCleanups);\r\n    taskSummary.append(\"\\t\\t\").append(ts.numFailedCleanups);\r\n    taskSummary.append(\"\\t\").append(ts.numKilledCleanups);\r\n    taskSummary.append(\"\\t\").append(StringUtils.getFormattedTimeWithDiff(dateFormat, ts.cleanupStarted, 0));\r\n    taskSummary.append(\"\\t\").append(StringUtils.getFormattedTimeWithDiff(dateFormat, ts.cleanupFinished, ts.cleanupStarted));\r\n    taskSummary.append(\"\\n============================\\n\");\r\n    ps.println(taskSummary);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "printJobAnalysis",
  "errType" : null,
  "containingMethodsNum" : 12,
  "sourceCodeText" : "void printJobAnalysis(PrintStream ps)\n{\r\n    if (job.getJobStatus().equals(JobStatus.getJobRunState(JobStatus.SUCCEEDED))) {\r\n        HistoryViewer.AnalyzedJob avg = new HistoryViewer.AnalyzedJob(job);\r\n        ps.println(\"\\nAnalysis\");\r\n        ps.println(\"=========\");\r\n        printAnalysis(ps, avg.getMapTasks(), cMap, \"map\", avg.getAvgMapTime(), 10);\r\n        printLast(ps, avg.getMapTasks(), \"map\", cFinishMapRed);\r\n        if (avg.getReduceTasks().length > 0) {\r\n            printAnalysis(ps, avg.getReduceTasks(), cShuffle, \"shuffle\", avg.getAvgShuffleTime(), 10);\r\n            printLast(ps, avg.getReduceTasks(), \"shuffle\", cFinishShuffle);\r\n            printAnalysis(ps, avg.getReduceTasks(), cReduce, \"reduce\", avg.getAvgReduceTime(), 10);\r\n            printLast(ps, avg.getReduceTasks(), \"reduce\", cFinishMapRed);\r\n        }\r\n        ps.println(\"=========\");\r\n    } else {\r\n        ps.println(\"No Analysis available as job did not finish\");\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "printAnalysis",
  "errType" : null,
  "containingMethodsNum" : 24,
  "sourceCodeText" : "void printAnalysis(PrintStream ps, JobHistoryParser.TaskAttemptInfo[] tasks, Comparator<JobHistoryParser.TaskAttemptInfo> cmp, String taskType, long avg, int showTasks)\n{\r\n    Arrays.sort(tasks, cmp);\r\n    JobHistoryParser.TaskAttemptInfo min = tasks[tasks.length - 1];\r\n    StringBuilder details = new StringBuilder();\r\n    details.append(\"\\nTime taken by best performing \");\r\n    details.append(taskType).append(\" task \");\r\n    details.append(min.getAttemptId().getTaskID().toString()).append(\": \");\r\n    if (\"map\".equals(taskType)) {\r\n        details.append(StringUtils.formatTimeDiff(min.getFinishTime(), min.getStartTime()));\r\n    } else if (\"shuffle\".equals(taskType)) {\r\n        details.append(StringUtils.formatTimeDiff(min.getShuffleFinishTime(), min.getStartTime()));\r\n    } else {\r\n        details.append(StringUtils.formatTimeDiff(min.getFinishTime(), min.getShuffleFinishTime()));\r\n    }\r\n    details.append(\"\\nAverage time taken by \");\r\n    details.append(taskType).append(\" tasks: \");\r\n    details.append(StringUtils.formatTimeDiff(avg, 0));\r\n    details.append(\"\\nWorse performing \");\r\n    details.append(taskType).append(\" tasks: \");\r\n    details.append(\"\\nTaskId\\t\\tTimetaken\");\r\n    ps.println(details);\r\n    for (int i = 0; i < showTasks && i < tasks.length; i++) {\r\n        details.setLength(0);\r\n        details.append(tasks[i].getAttemptId().getTaskID()).append(\" \");\r\n        if (\"map\".equals(taskType)) {\r\n            details.append(StringUtils.formatTimeDiff(tasks[i].getFinishTime(), tasks[i].getStartTime()));\r\n        } else if (\"shuffle\".equals(taskType)) {\r\n            details.append(StringUtils.formatTimeDiff(tasks[i].getShuffleFinishTime(), tasks[i].getStartTime()));\r\n        } else {\r\n            details.append(StringUtils.formatTimeDiff(tasks[i].getFinishTime(), tasks[i].getShuffleFinishTime()));\r\n        }\r\n        ps.println(details);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "printLast",
  "errType" : null,
  "containingMethodsNum" : 9,
  "sourceCodeText" : "void printLast(PrintStream ps, JobHistoryParser.TaskAttemptInfo[] tasks, String taskType, Comparator<JobHistoryParser.TaskAttemptInfo> cmp)\n{\r\n    Arrays.sort(tasks, cFinishMapRed);\r\n    JobHistoryParser.TaskAttemptInfo last = tasks[0];\r\n    StringBuilder lastBuf = new StringBuilder();\r\n    lastBuf.append(\"The last \").append(taskType);\r\n    lastBuf.append(\" task \").append(last.getAttemptId().getTaskID());\r\n    Long finishTime;\r\n    if (\"shuffle\".equals(taskType)) {\r\n        finishTime = last.getShuffleFinishTime();\r\n    } else {\r\n        finishTime = last.getFinishTime();\r\n    }\r\n    lastBuf.append(\" finished at (relative to the Job launch time): \");\r\n    lastBuf.append(StringUtils.getFormattedTimeWithDiff(dateFormat, finishTime, job.getLaunchTime()));\r\n    ps.println(lastBuf);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "printTasks",
  "errType" : null,
  "containingMethodsNum" : 20,
  "sourceCodeText" : "void printTasks(PrintStream ps, TaskType taskType, String status)\n{\r\n    Map<TaskID, JobHistoryParser.TaskInfo> tasks = job.getAllTasks();\r\n    StringBuilder header = new StringBuilder();\r\n    header.append(\"\\n\").append(status).append(\" \");\r\n    header.append(taskType).append(\" task list for \").append(job.getJobId().toString());\r\n    header.append(\"\\nTaskId\\t\\tStartTime\\tFinishTime\\tError\");\r\n    if (TaskType.MAP.equals(taskType)) {\r\n        header.append(\"\\tInputSplits\");\r\n    }\r\n    header.append(\"\\n====================================================\");\r\n    StringBuilder taskList = new StringBuilder();\r\n    for (JobHistoryParser.TaskInfo task : tasks.values()) {\r\n        if (taskType.equals(task.getTaskType()) && (status.equals(task.getTaskStatus()) || status.equalsIgnoreCase(\"ALL\"))) {\r\n            taskList.setLength(0);\r\n            taskList.append(task.getTaskId());\r\n            taskList.append(\"\\t\").append(StringUtils.getFormattedTimeWithDiff(dateFormat, task.getStartTime(), 0));\r\n            taskList.append(\"\\t\").append(StringUtils.getFormattedTimeWithDiff(dateFormat, task.getFinishTime(), task.getStartTime()));\r\n            taskList.append(\"\\t\").append(task.getError());\r\n            if (TaskType.MAP.equals(taskType)) {\r\n                taskList.append(\"\\t\").append(task.getSplitLocations());\r\n            }\r\n            if (taskList != null) {\r\n                ps.println(header);\r\n                ps.println(taskList);\r\n            }\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "printFailedAttempts",
  "errType" : null,
  "containingMethodsNum" : 14,
  "sourceCodeText" : "void printFailedAttempts(PrintStream ps, HistoryViewer.FilteredJob filteredJob)\n{\r\n    Map<String, Set<TaskID>> badNodes = filteredJob.getFilteredMap();\r\n    StringBuilder attempts = new StringBuilder();\r\n    if (badNodes.size() > 0) {\r\n        attempts.append(\"\\n\").append(filteredJob.getFilter());\r\n        attempts.append(\" task attempts by nodes\");\r\n        attempts.append(\"\\nHostname\\tFailedTasks\");\r\n        attempts.append(\"\\n===============================\");\r\n        ps.println(attempts);\r\n        for (Map.Entry<String, Set<TaskID>> entry : badNodes.entrySet()) {\r\n            String node = entry.getKey();\r\n            Set<TaskID> failedTasks = entry.getValue();\r\n            attempts.setLength(0);\r\n            attempts.append(node).append(\"\\t\");\r\n            for (TaskID t : failedTasks) {\r\n                attempts.append(t).append(\", \");\r\n            }\r\n            ps.println(attempts);\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest\\impl",
  "methodName" : "createIOStatisticsStore",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "IOStatisticsStoreBuilder createIOStatisticsStore()\n{\r\n    final IOStatisticsStoreBuilder store = iostatisticsStore();\r\n    store.withCounters(COUNTER_STATISTICS);\r\n    store.withMaximums(COUNTER_STATISTICS);\r\n    store.withMinimums(COUNTER_STATISTICS);\r\n    store.withMeanStatistics(COUNTER_STATISTICS);\r\n    store.withDurationTracking(DURATION_STATISTICS);\r\n    return store;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest\\impl",
  "methodName" : "maybeAddIOStatistics",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void maybeAddIOStatistics(IOStatisticsAggregator ios, Object o)\n{\r\n    if (o instanceof IOStatisticsSource) {\r\n        ios.aggregate(((IOStatisticsSource) o).getIOStatistics());\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest\\impl",
  "methodName" : "buildJobUUID",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "Pair<String, String> buildJobUUID(Configuration conf, JobID jobId)\n{\r\n    String jobUUID = conf.getTrimmed(SPARK_WRITE_UUID, \"\");\r\n    if (jobUUID.isEmpty()) {\r\n        jobUUID = jobId.toString();\r\n        return Pair.of(jobUUID, JOB_ID_SOURCE_MAPREDUCE);\r\n    } else {\r\n        return Pair.of(jobUUID, SPARK_WRITE_UUID);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest\\impl",
  "methodName" : "getPendingJobAttemptsPath",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Path getPendingJobAttemptsPath(Path out)\n{\r\n    return new Path(out, PENDING_DIR_NAME);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest\\impl",
  "methodName" : "getAppAttemptId",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "int getAppAttemptId(JobContext context)\n{\r\n    return getAppAttemptId(context.getConfiguration());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest\\impl",
  "methodName" : "getAppAttemptId",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "int getAppAttemptId(Configuration conf)\n{\r\n    return conf.getInt(MRJobConfig.APPLICATION_ATTEMPT_ID, INITIAL_APP_ATTEMPT_ID);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest\\impl",
  "methodName" : "manifestPathForTask",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Path manifestPathForTask(Path manifestDir, String taskId)\n{\r\n    return new Path(manifestDir, taskId + MANIFEST_SUFFIX);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest\\impl",
  "methodName" : "manifestTempPathForTaskAttempt",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Path manifestTempPathForTaskAttempt(Path manifestDir, String taskAttemptId)\n{\r\n    return new Path(manifestDir, taskAttemptId + MANIFEST_SUFFIX + TMP_SUFFIX);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest\\impl",
  "methodName" : "createTaskManifest",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "TaskManifest createTaskManifest(StageConfig stageConfig)\n{\r\n    final TaskManifest manifest = new TaskManifest();\r\n    manifest.setTaskAttemptID(stageConfig.getTaskAttemptId());\r\n    manifest.setTaskID(stageConfig.getTaskId());\r\n    manifest.setJobId(stageConfig.getJobId());\r\n    manifest.setJobAttemptNumber(stageConfig.getJobAttemptNumber());\r\n    manifest.setTaskAttemptDir(stageConfig.getTaskAttemptDir().toUri().toString());\r\n    return manifest;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest\\impl",
  "methodName" : "createManifestOutcome",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 9,
  "sourceCodeText" : "ManifestSuccessData createManifestOutcome(StageConfig stageConfig, String stage)\n{\r\n    final ManifestSuccessData outcome = new ManifestSuccessData();\r\n    outcome.setJobId(stageConfig.getJobId());\r\n    outcome.setJobIdSource(stageConfig.getJobIdSource());\r\n    outcome.setCommitter(MANIFEST_COMMITTER_CLASSNAME);\r\n    outcome.setTimestamp(System.currentTimeMillis());\r\n    final ZonedDateTime now = ZonedDateTime.now();\r\n    outcome.setDate(now.toString());\r\n    outcome.setHostname(NetUtils.getLocalHostname());\r\n    try {\r\n        outcome.putDiagnostic(PRINCIPAL, UserGroupInformation.getCurrentUser().getShortUserName());\r\n    } catch (IOException ignored) {\r\n    }\r\n    outcome.putDiagnostic(STAGE, stage);\r\n    return outcome;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest\\impl",
  "methodName" : "createJobSummaryFilename",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String createJobSummaryFilename(String jobId)\n{\r\n    return String.format(SUMMARY_FILENAME_FORMAT, jobId);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest\\impl",
  "methodName" : "getEtag",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String getEtag(FileStatus status)\n{\r\n    if (status instanceof EtagSource) {\r\n        return ((EtagSource) status).getEtag();\r\n    } else {\r\n        return null;\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest\\impl",
  "methodName" : "createManifestStoreOperations",
  "errType" : [ "Exception" ],
  "containingMethodsNum" : 4,
  "sourceCodeText" : "ManifestStoreOperations createManifestStoreOperations(final Configuration conf, final FileSystem filesystem, final Path path) throws IOException\n{\r\n    try {\r\n        final Class<? extends ManifestStoreOperations> storeClass = conf.getClass(OPT_STORE_OPERATIONS_CLASS, ManifestStoreOperationsThroughFileSystem.class, ManifestStoreOperations.class);\r\n        final ManifestStoreOperations operations = storeClass.getDeclaredConstructor().newInstance();\r\n        operations.bindToFileSystem(filesystem, path);\r\n        return operations;\r\n    } catch (Exception e) {\r\n        throw new PathIOException(path.toString(), \"Failed to create Store Operations from configuration option \" + OPT_STORE_OPERATIONS_CLASS + \":\" + e, e);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\partition",
  "methodName" : "setKeyFieldSeparator",
  "errType" : [ "UnsupportedEncodingException" ],
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setKeyFieldSeparator(String keyFieldSeparator)\n{\r\n    try {\r\n        this.keyFieldSeparator = keyFieldSeparator.getBytes(\"UTF-8\");\r\n    } catch (UnsupportedEncodingException e) {\r\n        throw new RuntimeException(\"The current system does not \" + \"support UTF-8 encoding!\", e);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\partition",
  "methodName" : "setKeyFieldSpec",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setKeyFieldSpec(int start, int end)\n{\r\n    if (end >= start) {\r\n        KeyDescription k = new KeyDescription();\r\n        k.beginFieldIdx = start;\r\n        k.endFieldIdx = end;\r\n        keySpecSeen = true;\r\n        allKeySpecs.add(k);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\partition",
  "methodName" : "keySpecs",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "List<KeyDescription> keySpecs()\n{\r\n    return allKeySpecs;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\partition",
  "methodName" : "getWordLengths",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "int[] getWordLengths(byte[] b, int start, int end)\n{\r\n    if (!keySpecSeen) {\r\n        return new int[] { 1 };\r\n    }\r\n    int[] lengths = new int[10];\r\n    int currLenLengths = lengths.length;\r\n    int idx = 1;\r\n    int pos;\r\n    while ((pos = UTF8ByteArrayUtils.findBytes(b, start, end, keyFieldSeparator)) != -1) {\r\n        if (++idx == currLenLengths) {\r\n            int[] temp = lengths;\r\n            lengths = new int[(currLenLengths = currLenLengths * 2)];\r\n            System.arraycopy(temp, 0, lengths, 0, temp.length);\r\n        }\r\n        lengths[idx - 1] = pos - start;\r\n        start = pos + 1;\r\n    }\r\n    if (start != end) {\r\n        lengths[idx] = end - start;\r\n    }\r\n    lengths[0] = idx;\r\n    return lengths;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\partition",
  "methodName" : "getStartOffset",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int getStartOffset(byte[] b, int start, int end, int[] lengthIndices, KeyDescription k)\n{\r\n    if (lengthIndices[0] >= k.beginFieldIdx) {\r\n        int position = 0;\r\n        for (int i = 1; i < k.beginFieldIdx; i++) {\r\n            position += lengthIndices[i] + keyFieldSeparator.length;\r\n        }\r\n        if (position + k.beginChar <= (end - start)) {\r\n            return start + position + k.beginChar - 1;\r\n        }\r\n    }\r\n    return -1;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\partition",
  "methodName" : "getEndOffset",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int getEndOffset(byte[] b, int start, int end, int[] lengthIndices, KeyDescription k)\n{\r\n    if (k.endFieldIdx == 0) {\r\n        return end - 1;\r\n    }\r\n    if (lengthIndices[0] >= k.endFieldIdx) {\r\n        int position = 0;\r\n        int i;\r\n        for (i = 1; i < k.endFieldIdx; i++) {\r\n            position += lengthIndices[i] + keyFieldSeparator.length;\r\n        }\r\n        if (k.endChar == 0) {\r\n            position += lengthIndices[i];\r\n        }\r\n        if (position + k.endChar <= (end - start)) {\r\n            return start + position + k.endChar - 1;\r\n        }\r\n        return end - 1;\r\n    }\r\n    return end - 1;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\partition",
  "methodName" : "parseOption",
  "errType" : null,
  "containingMethodsNum" : 11,
  "sourceCodeText" : "void parseOption(String option)\n{\r\n    if (option == null || option.equals(\"\")) {\r\n        return;\r\n    }\r\n    StringTokenizer args = new StringTokenizer(option);\r\n    KeyDescription global = new KeyDescription();\r\n    while (args.hasMoreTokens()) {\r\n        String arg = args.nextToken();\r\n        if (arg.equals(\"-n\")) {\r\n            global.numeric = true;\r\n        }\r\n        if (arg.equals(\"-r\")) {\r\n            global.reverse = true;\r\n        }\r\n        if (arg.equals(\"-nr\")) {\r\n            global.numeric = true;\r\n            global.reverse = true;\r\n        }\r\n        if (arg.startsWith(\"-k\")) {\r\n            KeyDescription k = parseKey(arg, args);\r\n            if (k != null) {\r\n                allKeySpecs.add(k);\r\n                keySpecSeen = true;\r\n            }\r\n        }\r\n    }\r\n    for (KeyDescription key : allKeySpecs) {\r\n        if (!(key.reverse | key.numeric)) {\r\n            key.reverse = global.reverse;\r\n            key.numeric = global.numeric;\r\n        }\r\n    }\r\n    if (allKeySpecs.size() == 0) {\r\n        allKeySpecs.add(global);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\partition",
  "methodName" : "parseKey",
  "errType" : null,
  "containingMethodsNum" : 33,
  "sourceCodeText" : "KeyDescription parseKey(String arg, StringTokenizer args)\n{\r\n    String keyArgs = null;\r\n    if (arg.length() == 2) {\r\n        if (args.hasMoreTokens()) {\r\n            keyArgs = args.nextToken();\r\n        }\r\n    } else {\r\n        keyArgs = arg.substring(2);\r\n    }\r\n    if (keyArgs == null || keyArgs.length() == 0) {\r\n        return null;\r\n    }\r\n    StringTokenizer st = new StringTokenizer(keyArgs, \"nr.,\", true);\r\n    KeyDescription key = new KeyDescription();\r\n    String token;\r\n    if (st.hasMoreTokens()) {\r\n        token = st.nextToken();\r\n        key.beginFieldIdx = Integer.parseInt(token);\r\n    }\r\n    if (st.hasMoreTokens()) {\r\n        token = st.nextToken();\r\n        if (token.equals(\".\")) {\r\n            token = st.nextToken();\r\n            key.beginChar = Integer.parseInt(token);\r\n            if (st.hasMoreTokens()) {\r\n                token = st.nextToken();\r\n            } else {\r\n                return key;\r\n            }\r\n        }\r\n        do {\r\n            if (token.equals(\"n\")) {\r\n                key.numeric = true;\r\n            } else if (token.equals(\"r\")) {\r\n                key.reverse = true;\r\n            } else\r\n                break;\r\n            if (st.hasMoreTokens()) {\r\n                token = st.nextToken();\r\n            } else {\r\n                return key;\r\n            }\r\n        } while (true);\r\n        if (token.equals(\",\")) {\r\n            token = st.nextToken();\r\n            key.endFieldIdx = Integer.parseInt(token);\r\n            if (st.hasMoreTokens()) {\r\n                token = st.nextToken();\r\n                if (token.equals(\".\")) {\r\n                    token = st.nextToken();\r\n                    key.endChar = Integer.parseInt(token);\r\n                    if (st.hasMoreTokens()) {\r\n                        token = st.nextToken();\r\n                    } else {\r\n                        return key;\r\n                    }\r\n                }\r\n                do {\r\n                    if (token.equals(\"n\")) {\r\n                        key.numeric = true;\r\n                    } else if (token.equals(\"r\")) {\r\n                        key.reverse = true;\r\n                    } else {\r\n                        throw new IllegalArgumentException(\"Invalid -k argument. \" + \"Must be of the form -k pos1,[pos2], where pos is of the form \" + \"f[.c]nr\");\r\n                    }\r\n                    if (st.hasMoreTokens()) {\r\n                        token = st.nextToken();\r\n                    } else {\r\n                        break;\r\n                    }\r\n                } while (true);\r\n            }\r\n            return key;\r\n        }\r\n        throw new IllegalArgumentException(\"Invalid -k argument. \" + \"Must be of the form -k pos1,[pos2], where pos is of the form \" + \"f[.c]nr\");\r\n    }\r\n    return key;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\partition",
  "methodName" : "printKey",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void printKey(KeyDescription key)\n{\r\n    System.out.println(\"key.beginFieldIdx: \" + key.beginFieldIdx);\r\n    System.out.println(\"key.beginChar: \" + key.beginChar);\r\n    System.out.println(\"key.endFieldIdx: \" + key.endFieldIdx);\r\n    System.out.println(\"key.endChar: \" + key.endChar);\r\n    System.out.println(\"key.numeric: \" + key.numeric);\r\n    System.out.println(\"key.reverse: \" + key.reverse);\r\n    System.out.println(\"parseKey over\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\util",
  "methodName" : "toEscapedCompactString",
  "errType" : null,
  "containingMethodsNum" : 17,
  "sourceCodeText" : "String toEscapedCompactString(Counter counter)\n{\r\n    String escapedName, escapedDispName;\r\n    long currentValue;\r\n    synchronized (counter) {\r\n        escapedName = escape(counter.getName());\r\n        escapedDispName = escape(counter.getDisplayName());\r\n        currentValue = counter.getValue();\r\n    }\r\n    int length = escapedName.length() + escapedDispName.length() + 4;\r\n    length += 8;\r\n    StringBuilder builder = new StringBuilder(length);\r\n    builder.append(COUNTER_OPEN);\r\n    builder.append(UNIT_OPEN);\r\n    builder.append(escapedName);\r\n    builder.append(UNIT_CLOSE);\r\n    builder.append(UNIT_OPEN);\r\n    builder.append(escapedDispName);\r\n    builder.append(UNIT_CLOSE);\r\n    builder.append(UNIT_OPEN);\r\n    builder.append(currentValue);\r\n    builder.append(UNIT_CLOSE);\r\n    builder.append(COUNTER_CLOSE);\r\n    return builder.toString();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\util",
  "methodName" : "toEscapedCompactString",
  "errType" : null,
  "containingMethodsNum" : 18,
  "sourceCodeText" : "String toEscapedCompactString(G group)\n{\r\n    List<String> escapedStrs = Lists.newArrayList();\r\n    int length;\r\n    String escapedName, escapedDispName;\r\n    synchronized (group) {\r\n        escapedName = escape(group.getName());\r\n        escapedDispName = escape(group.getDisplayName());\r\n        int i = 0;\r\n        length = escapedName.length() + escapedDispName.length();\r\n        for (Counter counter : group) {\r\n            String escapedStr = toEscapedCompactString(counter);\r\n            escapedStrs.add(escapedStr);\r\n            length += escapedStr.length();\r\n        }\r\n    }\r\n    length += 6;\r\n    StringBuilder builder = new StringBuilder(length);\r\n    builder.append(GROUP_OPEN);\r\n    builder.append(UNIT_OPEN);\r\n    builder.append(escapedName);\r\n    builder.append(UNIT_CLOSE);\r\n    builder.append(UNIT_OPEN);\r\n    builder.append(escapedDispName);\r\n    builder.append(UNIT_CLOSE);\r\n    for (String escaped : escapedStrs) {\r\n        builder.append(escaped);\r\n    }\r\n    builder.append(GROUP_CLOSE);\r\n    return builder.toString();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\util",
  "methodName" : "toEscapedCompactString",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "String toEscapedCompactString(T counters)\n{\r\n    StringBuilder builder = new StringBuilder();\r\n    synchronized (counters) {\r\n        for (G group : counters) {\r\n            builder.append(toEscapedCompactString(group));\r\n        }\r\n    }\r\n    return builder.toString();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\util",
  "methodName" : "escape",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String escape(String string)\n{\r\n    return StringUtils.escapeString(string, StringUtils.ESCAPE_CHAR, charsToEscape);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\util",
  "methodName" : "unescape",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String unescape(String string)\n{\r\n    return StringUtils.unEscapeString(string, StringUtils.ESCAPE_CHAR, charsToEscape);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\util",
  "methodName" : "getBlock",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "String getBlock(String str, char open, char close, IntWritable index) throws ParseException\n{\r\n    StringBuilder split = new StringBuilder();\r\n    int next = StringUtils.findNext(str, open, StringUtils.ESCAPE_CHAR, index.get(), split);\r\n    split.setLength(0);\r\n    if (next >= 0) {\r\n        ++next;\r\n        next = StringUtils.findNext(str, close, StringUtils.ESCAPE_CHAR, next, split);\r\n        if (next >= 0) {\r\n            ++next;\r\n            index.set(next);\r\n            return split.toString();\r\n        } else {\r\n            throw new ParseException(\"Unexpected end of block\", next);\r\n        }\r\n    }\r\n    return null;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\util",
  "methodName" : "parseEscapedCompactString",
  "errType" : null,
  "containingMethodsNum" : 18,
  "sourceCodeText" : "T parseEscapedCompactString(String compactString, T counters) throws ParseException\n{\r\n    IntWritable index = new IntWritable(0);\r\n    String groupString = getBlock(compactString, GROUP_OPEN, GROUP_CLOSE, index);\r\n    while (groupString != null) {\r\n        IntWritable groupIndex = new IntWritable(0);\r\n        String groupName = StringInterner.weakIntern(getBlock(groupString, UNIT_OPEN, UNIT_CLOSE, groupIndex));\r\n        groupName = StringInterner.weakIntern(unescape(groupName));\r\n        String groupDisplayName = StringInterner.weakIntern(getBlock(groupString, UNIT_OPEN, UNIT_CLOSE, groupIndex));\r\n        groupDisplayName = StringInterner.weakIntern(unescape(groupDisplayName));\r\n        G group = counters.getGroup(groupName);\r\n        group.setDisplayName(groupDisplayName);\r\n        String counterString = getBlock(groupString, COUNTER_OPEN, COUNTER_CLOSE, groupIndex);\r\n        while (counterString != null) {\r\n            IntWritable counterIndex = new IntWritable(0);\r\n            String counterName = StringInterner.weakIntern(getBlock(counterString, UNIT_OPEN, UNIT_CLOSE, counterIndex));\r\n            counterName = StringInterner.weakIntern(unescape(counterName));\r\n            String counterDisplayName = StringInterner.weakIntern(getBlock(counterString, UNIT_OPEN, UNIT_CLOSE, counterIndex));\r\n            counterDisplayName = StringInterner.weakIntern(unescape(counterDisplayName));\r\n            long value = Long.parseLong(getBlock(counterString, UNIT_OPEN, UNIT_CLOSE, counterIndex));\r\n            Counter counter = group.findCounter(counterName);\r\n            counter.setDisplayName(counterDisplayName);\r\n            counter.increment(value);\r\n            counterString = getBlock(groupString, COUNTER_OPEN, COUNTER_CLOSE, groupIndex);\r\n        }\r\n        groupString = getBlock(compactString, GROUP_OPEN, GROUP_CLOSE, index);\r\n    }\r\n    return counters;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\counters",
  "methodName" : "setDisplayName",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setDisplayName(String name)\n{\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\counters",
  "methodName" : "equals",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "boolean equals(Object genericRight)\n{\r\n    if (genericRight instanceof Counter) {\r\n        synchronized (genericRight) {\r\n            Counter right = (Counter) genericRight;\r\n            return getName().equals(right.getName()) && getDisplayName().equals(right.getDisplayName()) && getValue() == right.getValue();\r\n        }\r\n    }\r\n    return false;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\counters",
  "methodName" : "hashCode",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "int hashCode()\n{\r\n    return Objects.hashCode(getName(), getDisplayName(), getValue());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\input",
  "methodName" : "getInputSplit",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "InputSplit getInputSplit()\n{\r\n    return inputSplit;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\input",
  "methodName" : "getInputFormatClass",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Class<? extends InputFormat> getInputFormatClass()\n{\r\n    return inputFormatClass;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\input",
  "methodName" : "getMapperClass",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Class<? extends Mapper> getMapperClass()\n{\r\n    return mapperClass;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\input",
  "methodName" : "getLength",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "long getLength() throws IOException, InterruptedException\n{\r\n    return inputSplit.getLength();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\input",
  "methodName" : "getLocations",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String[] getLocations() throws IOException, InterruptedException\n{\r\n    return inputSplit.getLocations();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\input",
  "methodName" : "readFields",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void readFields(DataInput in) throws IOException\n{\r\n    inputSplitClass = (Class<? extends InputSplit>) readClass(in);\r\n    inputFormatClass = (Class<? extends InputFormat<?, ?>>) readClass(in);\r\n    mapperClass = (Class<? extends Mapper<?, ?, ?, ?>>) readClass(in);\r\n    inputSplit = (InputSplit) ReflectionUtils.newInstance(inputSplitClass, conf);\r\n    SerializationFactory factory = new SerializationFactory(conf);\r\n    Deserializer deserializer = factory.getDeserializer(inputSplitClass);\r\n    deserializer.open((DataInputStream) in);\r\n    inputSplit = (InputSplit) deserializer.deserialize(inputSplit);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\input",
  "methodName" : "readClass",
  "errType" : [ "ClassNotFoundException" ],
  "containingMethodsNum" : 2,
  "sourceCodeText" : "Class<?> readClass(DataInput in) throws IOException\n{\r\n    String className = StringInterner.weakIntern(Text.readString(in));\r\n    try {\r\n        return conf.getClassByName(className);\r\n    } catch (ClassNotFoundException e) {\r\n        throw new RuntimeException(\"readObject can't find class\", e);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\input",
  "methodName" : "write",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void write(DataOutput out) throws IOException\n{\r\n    Text.writeString(out, inputSplitClass.getName());\r\n    Text.writeString(out, inputFormatClass.getName());\r\n    Text.writeString(out, mapperClass.getName());\r\n    SerializationFactory factory = new SerializationFactory(conf);\r\n    Serializer serializer = factory.getSerializer(inputSplitClass);\r\n    serializer.open((DataOutputStream) out);\r\n    serializer.serialize(inputSplit);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\input",
  "methodName" : "getConf",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Configuration getConf()\n{\r\n    return conf;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\input",
  "methodName" : "setConf",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setConf(Configuration conf)\n{\r\n    this.conf = conf;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\input",
  "methodName" : "toString",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String toString()\n{\r\n    return inputSplit.toString();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output",
  "methodName" : "getOutputPath",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Path getOutputPath()\n{\r\n    return this.outputPath;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output",
  "methodName" : "getPendingJobAttemptsPath",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Path getPendingJobAttemptsPath()\n{\r\n    return getPendingJobAttemptsPath(getOutputPath());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output",
  "methodName" : "getPendingJobAttemptsPath",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Path getPendingJobAttemptsPath(Path out)\n{\r\n    return new Path(out, PENDING_DIR_NAME);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output",
  "methodName" : "getAppAttemptId",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "int getAppAttemptId(JobContext context)\n{\r\n    return context.getConfiguration().getInt(MRJobConfig.APPLICATION_ATTEMPT_ID, 0);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output",
  "methodName" : "getJobAttemptPath",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Path getJobAttemptPath(JobContext context)\n{\r\n    return getJobAttemptPath(context, getOutputPath());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output",
  "methodName" : "getJobAttemptPath",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Path getJobAttemptPath(JobContext context, Path out)\n{\r\n    return getJobAttemptPath(getAppAttemptId(context), out);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output",
  "methodName" : "getJobAttemptPath",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Path getJobAttemptPath(int appAttemptId)\n{\r\n    return getJobAttemptPath(appAttemptId, getOutputPath());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output",
  "methodName" : "getJobAttemptPath",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "Path getJobAttemptPath(int appAttemptId, Path out)\n{\r\n    return new Path(getPendingJobAttemptsPath(out), String.valueOf(appAttemptId));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output",
  "methodName" : "getPendingTaskAttemptsPath",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Path getPendingTaskAttemptsPath(JobContext context)\n{\r\n    return getPendingTaskAttemptsPath(context, getOutputPath());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output",
  "methodName" : "getPendingTaskAttemptsPath",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Path getPendingTaskAttemptsPath(JobContext context, Path out)\n{\r\n    return new Path(getJobAttemptPath(context, out), PENDING_DIR_NAME);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output",
  "methodName" : "getTaskAttemptPath",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "Path getTaskAttemptPath(TaskAttemptContext context)\n{\r\n    return new Path(getPendingTaskAttemptsPath(context), String.valueOf(context.getTaskAttemptID()));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output",
  "methodName" : "getTaskAttemptPath",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "Path getTaskAttemptPath(TaskAttemptContext context, Path out)\n{\r\n    return new Path(getPendingTaskAttemptsPath(context, out), String.valueOf(context.getTaskAttemptID()));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output",
  "methodName" : "getCommittedTaskPath",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Path getCommittedTaskPath(TaskAttemptContext context)\n{\r\n    return getCommittedTaskPath(getAppAttemptId(context), context);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output",
  "methodName" : "getCommittedTaskPath",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Path getCommittedTaskPath(TaskAttemptContext context, Path out)\n{\r\n    return getCommittedTaskPath(getAppAttemptId(context), context, out);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output",
  "methodName" : "getCommittedTaskPath",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "Path getCommittedTaskPath(int appAttemptId, TaskAttemptContext context)\n{\r\n    return new Path(getJobAttemptPath(appAttemptId), String.valueOf(context.getTaskAttemptID().getTaskID()));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output",
  "methodName" : "getCommittedTaskPath",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "Path getCommittedTaskPath(int appAttemptId, TaskAttemptContext context, Path out)\n{\r\n    return new Path(getJobAttemptPath(appAttemptId, out), String.valueOf(context.getTaskAttemptID().getTaskID()));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output",
  "methodName" : "getAllCommittedTaskPaths",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "FileStatus[] getAllCommittedTaskPaths(JobContext context) throws IOException\n{\r\n    Path jobAttemptPath = getJobAttemptPath(context);\r\n    FileSystem fs = jobAttemptPath.getFileSystem(context.getConfiguration());\r\n    return fs.listStatus(jobAttemptPath, new CommittedTaskFilter());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output",
  "methodName" : "getWorkPath",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Path getWorkPath() throws IOException\n{\r\n    return workPath;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output",
  "methodName" : "setupJob",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void setupJob(JobContext context) throws IOException\n{\r\n    if (hasOutputPath()) {\r\n        Path jobAttemptPath = getJobAttemptPath(context);\r\n        FileSystem fs = jobAttemptPath.getFileSystem(context.getConfiguration());\r\n        if (!fs.mkdirs(jobAttemptPath)) {\r\n            LOG.error(\"Mkdirs failed to create \" + jobAttemptPath);\r\n        }\r\n    } else {\r\n        LOG.warn(\"Output Path is null in setupJob()\");\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output",
  "methodName" : "commitJob",
  "errType" : [ "Exception" ],
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void commitJob(JobContext context) throws IOException\n{\r\n    int maxAttemptsOnFailure = isCommitJobRepeatable(context) ? context.getConfiguration().getInt(FILEOUTPUTCOMMITTER_FAILURE_ATTEMPTS, FILEOUTPUTCOMMITTER_FAILURE_ATTEMPTS_DEFAULT) : 1;\r\n    int attempt = 0;\r\n    boolean jobCommitNotFinished = true;\r\n    while (jobCommitNotFinished) {\r\n        try {\r\n            commitJobInternal(context);\r\n            jobCommitNotFinished = false;\r\n        } catch (Exception e) {\r\n            if (++attempt >= maxAttemptsOnFailure) {\r\n                throw e;\r\n            } else {\r\n                LOG.warn(\"Exception get thrown in job commit, retry (\" + attempt + \") time.\", e);\r\n            }\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output",
  "methodName" : "commitJobInternal",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 13,
  "sourceCodeText" : "void commitJobInternal(JobContext context) throws IOException\n{\r\n    if (hasOutputPath()) {\r\n        Path finalOutput = getOutputPath();\r\n        FileSystem fs = finalOutput.getFileSystem(context.getConfiguration());\r\n        if (algorithmVersion == 1) {\r\n            for (FileStatus stat : getAllCommittedTaskPaths(context)) {\r\n                mergePaths(fs, stat, finalOutput, context);\r\n            }\r\n        }\r\n        if (skipCleanup) {\r\n            LOG.info(\"Skip cleanup the _temporary folders under job's output \" + \"directory in commitJob.\");\r\n        } else {\r\n            try {\r\n                cleanupJob(context);\r\n            } catch (IOException e) {\r\n                if (ignoreCleanupFailures) {\r\n                    LOG.error(\"Error in cleanup job, manually cleanup is needed.\", e);\r\n                } else {\r\n                    throw e;\r\n                }\r\n            }\r\n        }\r\n        if (context.getConfiguration().getBoolean(SUCCESSFUL_JOB_OUTPUT_DIR_MARKER, true)) {\r\n            Path markerPath = new Path(outputPath, SUCCEEDED_FILE_NAME);\r\n            if (isCommitJobRepeatable(context)) {\r\n                fs.create(markerPath, true).close();\r\n            } else {\r\n                fs.create(markerPath).close();\r\n            }\r\n        }\r\n    } else {\r\n        LOG.warn(\"Output Path is null in commitJob()\");\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output",
  "methodName" : "mergePaths",
  "errType" : [ "FileNotFoundException" ],
  "containingMethodsNum" : 13,
  "sourceCodeText" : "void mergePaths(FileSystem fs, final FileStatus from, final Path to, JobContext context) throws IOException\n{\r\n    try (DurationInfo d = new DurationInfo(LOG, false, \"Merging data from %s to %s\", from, to)) {\r\n        reportProgress(context);\r\n        FileStatus toStat;\r\n        try {\r\n            toStat = fs.getFileStatus(to);\r\n        } catch (FileNotFoundException fnfe) {\r\n            toStat = null;\r\n        }\r\n        if (from.isFile()) {\r\n            if (toStat != null) {\r\n                if (!fs.delete(to, true)) {\r\n                    throw new IOException(\"Failed to delete \" + to);\r\n                }\r\n            }\r\n            if (!fs.rename(from.getPath(), to)) {\r\n                throw new IOException(\"Failed to rename \" + from + \" to \" + to);\r\n            }\r\n        } else if (from.isDirectory()) {\r\n            if (toStat != null) {\r\n                if (!toStat.isDirectory()) {\r\n                    if (!fs.delete(to, true)) {\r\n                        throw new IOException(\"Failed to delete \" + to);\r\n                    }\r\n                    renameOrMerge(fs, from, to, context);\r\n                } else {\r\n                    for (FileStatus subFrom : fs.listStatus(from.getPath())) {\r\n                        Path subTo = new Path(to, subFrom.getPath().getName());\r\n                        mergePaths(fs, subFrom, subTo, context);\r\n                    }\r\n                }\r\n            } else {\r\n                renameOrMerge(fs, from, to, context);\r\n            }\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 2,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output",
  "methodName" : "reportProgress",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void reportProgress(JobContext context)\n{\r\n    if (context instanceof Progressable) {\r\n        ((Progressable) context).progress();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output",
  "methodName" : "renameOrMerge",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void renameOrMerge(FileSystem fs, FileStatus from, Path to, JobContext context) throws IOException\n{\r\n    if (algorithmVersion == 1) {\r\n        if (!fs.rename(from.getPath(), to)) {\r\n            throw new IOException(\"Failed to rename \" + from + \" to \" + to);\r\n        }\r\n    } else {\r\n        fs.mkdirs(to);\r\n        for (FileStatus subFrom : fs.listStatus(from.getPath())) {\r\n            Path subTo = new Path(to, subFrom.getPath().getName());\r\n            mergePaths(fs, subFrom, subTo, context);\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output",
  "methodName" : "cleanupJob",
  "errType" : [ "FileNotFoundException" ],
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void cleanupJob(JobContext context) throws IOException\n{\r\n    if (hasOutputPath()) {\r\n        Path pendingJobAttemptsPath = getPendingJobAttemptsPath();\r\n        FileSystem fs = pendingJobAttemptsPath.getFileSystem(context.getConfiguration());\r\n        try {\r\n            fs.delete(pendingJobAttemptsPath, true);\r\n        } catch (FileNotFoundException e) {\r\n            if (!isCommitJobRepeatable(context)) {\r\n                throw e;\r\n            }\r\n        }\r\n    } else {\r\n        LOG.warn(\"Output Path is null in cleanupJob()\");\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output",
  "methodName" : "abortJob",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void abortJob(JobContext context, JobStatus.State state) throws IOException\n{\r\n    cleanupJob(context);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output",
  "methodName" : "setupTask",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setupTask(TaskAttemptContext context) throws IOException\n{\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output",
  "methodName" : "commitTask",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void commitTask(TaskAttemptContext context) throws IOException\n{\r\n    commitTask(context, null);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output",
  "methodName" : "commitTask",
  "errType" : [ "FileNotFoundException" ],
  "containingMethodsNum" : 19,
  "sourceCodeText" : "void commitTask(TaskAttemptContext context, Path taskAttemptPath) throws IOException\n{\r\n    TaskAttemptID attemptId = context.getTaskAttemptID();\r\n    if (hasOutputPath()) {\r\n        context.progress();\r\n        if (taskAttemptPath == null) {\r\n            taskAttemptPath = getTaskAttemptPath(context);\r\n        }\r\n        FileSystem fs = taskAttemptPath.getFileSystem(context.getConfiguration());\r\n        FileStatus taskAttemptDirStatus;\r\n        try {\r\n            taskAttemptDirStatus = fs.getFileStatus(taskAttemptPath);\r\n        } catch (FileNotFoundException e) {\r\n            taskAttemptDirStatus = null;\r\n        }\r\n        if (taskAttemptDirStatus != null) {\r\n            if (algorithmVersion == 1) {\r\n                Path committedTaskPath = getCommittedTaskPath(context);\r\n                if (fs.exists(committedTaskPath)) {\r\n                    if (!fs.delete(committedTaskPath, true)) {\r\n                        throw new IOException(\"Could not delete \" + committedTaskPath);\r\n                    }\r\n                }\r\n                if (!fs.rename(taskAttemptPath, committedTaskPath)) {\r\n                    throw new IOException(\"Could not rename \" + taskAttemptPath + \" to \" + committedTaskPath);\r\n                }\r\n                LOG.info(\"Saved output of task '\" + attemptId + \"' to \" + committedTaskPath);\r\n            } else {\r\n                mergePaths(fs, taskAttemptDirStatus, outputPath, context);\r\n                LOG.info(\"Saved output of task '\" + attemptId + \"' to \" + outputPath);\r\n                if (context.getConfiguration().getBoolean(FILEOUTPUTCOMMITTER_TASK_CLEANUP_ENABLED, FILEOUTPUTCOMMITTER_TASK_CLEANUP_ENABLED_DEFAULT)) {\r\n                    LOG.debug(String.format(\"Deleting the temporary directory of '%s': '%s'\", attemptId, taskAttemptPath));\r\n                    if (!fs.delete(taskAttemptPath, true)) {\r\n                        LOG.warn(\"Could not delete \" + taskAttemptPath);\r\n                    }\r\n                }\r\n            }\r\n        } else {\r\n            LOG.warn(\"No Output found for \" + attemptId);\r\n        }\r\n    } else {\r\n        LOG.warn(\"Output Path is null in commitTask()\");\r\n    }\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output",
  "methodName" : "abortTask",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void abortTask(TaskAttemptContext context) throws IOException\n{\r\n    abortTask(context, null);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output",
  "methodName" : "abortTask",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void abortTask(TaskAttemptContext context, Path taskAttemptPath) throws IOException\n{\r\n    if (hasOutputPath()) {\r\n        context.progress();\r\n        if (taskAttemptPath == null) {\r\n            taskAttemptPath = getTaskAttemptPath(context);\r\n        }\r\n        FileSystem fs = taskAttemptPath.getFileSystem(context.getConfiguration());\r\n        if (!fs.delete(taskAttemptPath, true)) {\r\n            LOG.warn(\"Could not delete \" + taskAttemptPath);\r\n        }\r\n    } else {\r\n        LOG.warn(\"Output Path is null in abortTask()\");\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output",
  "methodName" : "needsTaskCommit",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "boolean needsTaskCommit(TaskAttemptContext context) throws IOException\n{\r\n    return needsTaskCommit(context, null);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output",
  "methodName" : "needsTaskCommit",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "boolean needsTaskCommit(TaskAttemptContext context, Path taskAttemptPath) throws IOException\n{\r\n    if (hasOutputPath()) {\r\n        if (taskAttemptPath == null) {\r\n            taskAttemptPath = getTaskAttemptPath(context);\r\n        }\r\n        FileSystem fs = taskAttemptPath.getFileSystem(context.getConfiguration());\r\n        return fs.exists(taskAttemptPath);\r\n    }\r\n    return false;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output",
  "methodName" : "isRecoverySupported",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean isRecoverySupported()\n{\r\n    return true;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output",
  "methodName" : "isCommitJobRepeatable",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean isCommitJobRepeatable(JobContext context) throws IOException\n{\r\n    return algorithmVersion == 2;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output",
  "methodName" : "recoverTask",
  "errType" : [ "FileNotFoundException" ],
  "containingMethodsNum" : 21,
  "sourceCodeText" : "void recoverTask(TaskAttemptContext context) throws IOException\n{\r\n    if (hasOutputPath()) {\r\n        context.progress();\r\n        TaskAttemptID attemptId = context.getTaskAttemptID();\r\n        int previousAttempt = getAppAttemptId(context) - 1;\r\n        if (previousAttempt < 0) {\r\n            throw new IOException(\"Cannot recover task output for first attempt...\");\r\n        }\r\n        Path previousCommittedTaskPath = getCommittedTaskPath(previousAttempt, context);\r\n        FileSystem fs = previousCommittedTaskPath.getFileSystem(context.getConfiguration());\r\n        if (LOG.isDebugEnabled()) {\r\n            LOG.debug(\"Trying to recover task from \" + previousCommittedTaskPath);\r\n        }\r\n        if (algorithmVersion == 1) {\r\n            if (fs.exists(previousCommittedTaskPath)) {\r\n                Path committedTaskPath = getCommittedTaskPath(context);\r\n                if (!fs.delete(committedTaskPath, true) && fs.exists(committedTaskPath)) {\r\n                    throw new IOException(\"Could not delete \" + committedTaskPath);\r\n                }\r\n                Path committedParent = committedTaskPath.getParent();\r\n                fs.mkdirs(committedParent);\r\n                if (!fs.rename(previousCommittedTaskPath, committedTaskPath)) {\r\n                    throw new IOException(\"Could not rename \" + previousCommittedTaskPath + \" to \" + committedTaskPath);\r\n                }\r\n            } else {\r\n                LOG.warn(attemptId + \" had no output to recover.\");\r\n            }\r\n        } else {\r\n            try {\r\n                FileStatus from = fs.getFileStatus(previousCommittedTaskPath);\r\n                LOG.info(\"Recovering task for upgrading scenario, moving files from \" + previousCommittedTaskPath + \" to \" + outputPath);\r\n                mergePaths(fs, from, outputPath, context);\r\n            } catch (FileNotFoundException ignored) {\r\n            }\r\n            LOG.info(\"Done recovering task \" + attemptId);\r\n        }\r\n    } else {\r\n        LOG.warn(\"Output Path is null in recoverTask()\");\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output",
  "methodName" : "toString",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "String toString()\n{\r\n    final StringBuilder sb = new StringBuilder(\"FileOutputCommitter{\");\r\n    sb.append(super.toString()).append(\"; \");\r\n    sb.append(\"outputPath=\").append(outputPath);\r\n    sb.append(\", workPath=\").append(workPath);\r\n    sb.append(\", algorithmVersion=\").append(algorithmVersion);\r\n    sb.append(\", skipCleanup=\").append(skipCleanup);\r\n    sb.append(\", ignoreCleanupFailures=\").append(ignoreCleanupFailures);\r\n    sb.append('}');\r\n    return sb.toString();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest\\files",
  "methodName" : "marshallPath",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String marshallPath(@Nullable Path path)\n{\r\n    return path != null ? path.toUri().toString() : null;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest\\files",
  "methodName" : "unmarshallPath",
  "errType" : [ "URISyntaxException" ],
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Path unmarshallPath(String path)\n{\r\n    try {\r\n        return new Path(new URI(requireNonNull(path, \"No path\")));\r\n    } catch (URISyntaxException e) {\r\n        throw new RuntimeException(\"Failed to parse \\\"\" + path + \"\\\" : \" + e, e);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest\\files",
  "methodName" : "validate",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "T validate() throws IOException",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest\\files",
  "methodName" : "toBytes",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "byte[] toBytes() throws IOException",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest\\files",
  "methodName" : "save",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void save(FileSystem fs, Path path, boolean overwrite) throws IOException",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest\\files",
  "methodName" : "createSerializer",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "JsonSerialization<T> createSerializer()",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest\\files",
  "methodName" : "validateCollectionClass",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void validateCollectionClass(Iterable it, Class classname) throws IOException\n{\r\n    for (Object o : it) {\r\n        verify(o.getClass().equals(classname), \"Collection element is not a %s: %s\", classname, o.getClass());\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest\\files",
  "methodName" : "verify",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void verify(boolean expression, String message, Object... args) throws IOException\n{\r\n    if (!expression) {\r\n        throw new IOException(String.format(message, args));\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred\\lib",
  "methodName" : "configure",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void configure(JobConf job)\n{\r\n    super.setConf(job);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\join",
  "methodName" : "emit",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "V emit(TupleWritable dst) throws IOException",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\join",
  "methodName" : "combine",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean combine(Object[] srcs, TupleWritable dst)\n{\r\n    return true;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\join",
  "methodName" : "nextKeyValue",
  "errType" : null,
  "containingMethodsNum" : 16,
  "sourceCodeText" : "boolean nextKeyValue() throws IOException, InterruptedException\n{\r\n    if (key == null) {\r\n        key = createKey();\r\n    }\r\n    if (value == null) {\r\n        value = createValue();\r\n    }\r\n    if (jc.flush(ivalue)) {\r\n        ReflectionUtils.copy(conf, jc.key(), key);\r\n        ReflectionUtils.copy(conf, emit(ivalue), value);\r\n        return true;\r\n    }\r\n    if (ivalue == null) {\r\n        ivalue = createTupleWritable();\r\n    }\r\n    jc.clear();\r\n    final PriorityQueue<ComposableRecordReader<K, ?>> q = getRecordReaderQueue();\r\n    K iterkey = createKey();\r\n    while (q != null && !q.isEmpty()) {\r\n        fillJoinCollector(iterkey);\r\n        jc.reset(iterkey);\r\n        if (jc.flush(ivalue)) {\r\n            ReflectionUtils.copy(conf, jc.key(), key);\r\n            ReflectionUtils.copy(conf, emit(ivalue), value);\r\n            return true;\r\n        }\r\n        jc.clear();\r\n    }\r\n    return false;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\join",
  "methodName" : "initialize",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void initialize(InputSplit split, TaskAttemptContext context) throws IOException, InterruptedException\n{\r\n    super.initialize(split, context);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\join",
  "methodName" : "getDelegate",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "ResetableIterator<V> getDelegate()\n{\r\n    return new MultiFilterDelegationIterator();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "initialize",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void initialize(InputSplit split, TaskAttemptContext context) throws IOException, InterruptedException",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "nextKeyValue",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean nextKeyValue() throws IOException, InterruptedException",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "getCurrentKey",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "KEYIN getCurrentKey() throws IOException, InterruptedException",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "getCurrentValue",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "VALUEIN getCurrentValue() throws IOException, InterruptedException",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "getProgress",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "float getProgress() throws IOException, InterruptedException",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "close",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void close() throws IOException",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred\\lib\\aggregate",
  "methodName" : "configure",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void configure(JobConf job)\n{\r\n    this.initializeMySpec(job);\r\n    this.logSpec();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred\\lib\\aggregate",
  "methodName" : "getValueAggregatorDescriptor",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "ValueAggregatorDescriptor getValueAggregatorDescriptor(String spec, JobConf job)\n{\r\n    if (spec == null)\r\n        return null;\r\n    String[] segments = spec.split(\",\", -1);\r\n    String type = segments[0];\r\n    if (type.compareToIgnoreCase(\"UserDefined\") == 0) {\r\n        String className = segments[1];\r\n        return new UserDefinedValueAggregatorDescriptor(className, job);\r\n    }\r\n    return null;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred\\lib\\aggregate",
  "methodName" : "getAggregatorDescriptors",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "ArrayList<ValueAggregatorDescriptor> getAggregatorDescriptors(JobConf job)\n{\r\n    String advn = \"aggregator.descriptor\";\r\n    int num = job.getInt(advn + \".num\", 0);\r\n    ArrayList<ValueAggregatorDescriptor> retv = new ArrayList<ValueAggregatorDescriptor>(num);\r\n    for (int i = 0; i < num; i++) {\r\n        String spec = job.get(advn + \".\" + i);\r\n        ValueAggregatorDescriptor ad = getValueAggregatorDescriptor(spec, job);\r\n        if (ad != null) {\r\n            retv.add(ad);\r\n        }\r\n    }\r\n    return retv;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred\\lib\\aggregate",
  "methodName" : "initializeMySpec",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void initializeMySpec(JobConf job)\n{\r\n    this.aggregatorDescriptorList = getAggregatorDescriptors(job);\r\n    if (this.aggregatorDescriptorList.size() == 0) {\r\n        this.aggregatorDescriptorList.add(new UserDefinedValueAggregatorDescriptor(ValueAggregatorBaseDescriptor.class.getCanonicalName(), job));\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred\\lib\\aggregate",
  "methodName" : "logSpec",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void logSpec()\n{\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred\\lib\\aggregate",
  "methodName" : "close",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void close() throws IOException\n{\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\input",
  "methodName" : "fillBuffer",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "int fillBuffer(InputStream in, byte[] buffer, boolean inDelimiter) throws IOException\n{\r\n    int bytesRead = in.read(buffer);\r\n    if (inDelimiter && bytesRead > 0) {\r\n        if (usingCRLF) {\r\n            needAdditionalRecord = (buffer[0] != '\\n');\r\n        } else {\r\n            needAdditionalRecord = true;\r\n        }\r\n    }\r\n    return bytesRead;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\input",
  "methodName" : "readLine",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "int readLine(Text str, int maxLineLength, int maxBytesToConsume) throws IOException\n{\r\n    int bytesRead = 0;\r\n    if (!finished) {\r\n        if (scin.getPos() > scin.getAdjustedEnd()) {\r\n            finished = true;\r\n        }\r\n        bytesRead = super.readLine(str, maxLineLength, maxBytesToConsume);\r\n    }\r\n    return bytesRead;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\input",
  "methodName" : "needAdditionalRecordAfterSplit",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean needAdditionalRecordAfterSplit()\n{\r\n    return !finished && needAdditionalRecord;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\input",
  "methodName" : "unsetNeedAdditionalRecordAfterSplit",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void unsetNeedAdditionalRecordAfterSplit()\n{\r\n    needAdditionalRecord = false;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\input",
  "methodName" : "initialize",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void initialize(InputSplit split, TaskAttemptContext context) throws IOException, InterruptedException\n{\r\n    sequenceFileRecordReader.initialize(split, context);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\input",
  "methodName" : "getCurrentKey",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Text getCurrentKey() throws IOException, InterruptedException\n{\r\n    return key;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\input",
  "methodName" : "getCurrentValue",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Text getCurrentValue() throws IOException, InterruptedException\n{\r\n    return value;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\input",
  "methodName" : "nextKeyValue",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "boolean nextKeyValue() throws IOException, InterruptedException\n{\r\n    if (!sequenceFileRecordReader.nextKeyValue()) {\r\n        return false;\r\n    }\r\n    if (key == null) {\r\n        key = new Text();\r\n    }\r\n    if (value == null) {\r\n        value = new Text();\r\n    }\r\n    key.set(sequenceFileRecordReader.getCurrentKey().toString());\r\n    value.set(sequenceFileRecordReader.getCurrentValue().toString());\r\n    return true;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\input",
  "methodName" : "getProgress",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "float getProgress() throws IOException, InterruptedException\n{\r\n    return sequenceFileRecordReader.getProgress();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\input",
  "methodName" : "close",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void close() throws IOException\n{\r\n    sequenceFileRecordReader.close();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "getJobSplitFile",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Path getJobSplitFile(Path jobSubmissionDir)\n{\r\n    return new Path(jobSubmissionDir, \"job.split\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "getJobSplitMetaFile",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Path getJobSplitMetaFile(Path jobSubmissionDir)\n{\r\n    return new Path(jobSubmissionDir, \"job.splitmetainfo\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "getJobConfPath",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Path getJobConfPath(Path jobSubmitDir)\n{\r\n    return new Path(jobSubmitDir, \"job.xml\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "getJobJar",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Path getJobJar(Path jobSubmitDir)\n{\r\n    return new Path(jobSubmitDir, \"job.jar\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "getJobDistCacheFiles",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Path getJobDistCacheFiles(Path jobSubmitDir)\n{\r\n    return new Path(jobSubmitDir, \"files\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "getJobLog4jFile",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Path getJobLog4jFile(Path jobSubmitDir)\n{\r\n    return new Path(jobSubmitDir, \"log4j\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "getJobDistCacheArchives",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Path getJobDistCacheArchives(Path jobSubmitDir)\n{\r\n    return new Path(jobSubmitDir, \"archives\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "getJobDistCacheLibjars",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Path getJobDistCacheLibjars(Path jobSubmitDir)\n{\r\n    return new Path(jobSubmitDir, \"libjars\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "getStagingDir",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "Path getStagingDir(Cluster cluster, Configuration conf) throws IOException, InterruptedException\n{\r\n    UserGroupInformation user = UserGroupInformation.getLoginUser();\r\n    return getStagingDir(cluster, conf, user);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "getStagingDir",
  "errType" : [ "FileNotFoundException" ],
  "containingMethodsNum" : 18,
  "sourceCodeText" : "Path getStagingDir(Cluster cluster, Configuration conf, UserGroupInformation realUser) throws IOException, InterruptedException\n{\r\n    Path stagingArea = cluster.getStagingAreaDir();\r\n    FileSystem fs = stagingArea.getFileSystem(conf);\r\n    UserGroupInformation currentUser = realUser.getCurrentUser();\r\n    try {\r\n        FileStatus fsStatus = fs.getFileStatus(stagingArea);\r\n        String fileOwner = fsStatus.getOwner();\r\n        if (!(fileOwner.equals(currentUser.getShortUserName()) || fileOwner.equalsIgnoreCase(currentUser.getUserName()) || fileOwner.equals(realUser.getShortUserName()) || fileOwner.equalsIgnoreCase(realUser.getUserName()))) {\r\n            String errorMessage = \"The ownership on the staging directory \" + stagingArea + \" is not as expected. \" + \"It is owned by \" + fileOwner + \". The directory must \" + \"be owned by the submitter \" + currentUser.getShortUserName() + \" or \" + currentUser.getUserName();\r\n            if (!realUser.getUserName().equals(currentUser.getUserName())) {\r\n                throw new IOException(errorMessage + \" or \" + realUser.getShortUserName() + \" or \" + realUser.getUserName());\r\n            } else {\r\n                throw new IOException(errorMessage);\r\n            }\r\n        }\r\n        if (!fsStatus.getPermission().equals(JOB_DIR_PERMISSION)) {\r\n            LOG.info(\"Permissions on staging directory \" + stagingArea + \" are \" + \"incorrect: \" + fsStatus.getPermission() + \". Fixing permissions \" + \"to correct value \" + JOB_DIR_PERMISSION);\r\n            fs.setPermission(stagingArea, JOB_DIR_PERMISSION);\r\n        }\r\n    } catch (FileNotFoundException e) {\r\n        fs.mkdirs(stagingArea, new FsPermission(JOB_DIR_PERMISSION));\r\n    }\r\n    return stagingArea;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getName",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getName()\n{\r\n    return name;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "setName",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setName(String name)\n{\r\n    this.name = name;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getAcls",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Map<String, AccessControlList> getAcls()\n{\r\n    return acls;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "setAcls",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setAcls(Map<String, AccessControlList> acls)\n{\r\n    this.acls = acls;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getState",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "QueueState getState()\n{\r\n    return state;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "setState",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setState(QueueState state)\n{\r\n    this.state = state;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getSchedulingInfo",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Object getSchedulingInfo()\n{\r\n    return schedulingInfo;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "setSchedulingInfo",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setSchedulingInfo(Object schedulingInfo)\n{\r\n    this.schedulingInfo = schedulingInfo;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "copySchedulingInfo",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void copySchedulingInfo(Queue sourceQueue)\n{\r\n    Set<Queue> destChildren = getChildren();\r\n    if (destChildren != null) {\r\n        Iterator<Queue> itr1 = destChildren.iterator();\r\n        Iterator<Queue> itr2 = sourceQueue.getChildren().iterator();\r\n        while (itr1.hasNext()) {\r\n            itr1.next().copySchedulingInfo(itr2.next());\r\n        }\r\n    }\r\n    setSchedulingInfo(sourceQueue.getSchedulingInfo());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "addChild",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void addChild(Queue child)\n{\r\n    if (children == null) {\r\n        children = new TreeSet<Queue>();\r\n    }\r\n    children.add(child);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getChildren",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Set<Queue> getChildren()\n{\r\n    return children;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "setProperties",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setProperties(Properties props)\n{\r\n    this.props = props;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getProperties",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Properties getProperties()\n{\r\n    return this.props;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getInnerQueues",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "Map<String, Queue> getInnerQueues()\n{\r\n    Map<String, Queue> l = new HashMap<String, Queue>();\r\n    if (children == null) {\r\n        return l;\r\n    }\r\n    for (Queue child : children) {\r\n        if (child.getChildren() != null && child.getChildren().size() > 0) {\r\n            l.put(child.getName(), child);\r\n            l.putAll(child.getInnerQueues());\r\n        }\r\n    }\r\n    return l;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getLeafQueues",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "Map<String, Queue> getLeafQueues()\n{\r\n    Map<String, Queue> l = new HashMap<String, Queue>();\r\n    if (children == null) {\r\n        l.put(name, this);\r\n        return l;\r\n    }\r\n    for (Queue child : children) {\r\n        l.putAll(child.getLeafQueues());\r\n    }\r\n    return l;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "compareTo",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "int compareTo(Queue queue)\n{\r\n    return name.compareTo(queue.getName());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "equals",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "boolean equals(Object o)\n{\r\n    if (o == this) {\r\n        return true;\r\n    }\r\n    if (!(o instanceof Queue)) {\r\n        return false;\r\n    }\r\n    return ((Queue) o).getName().equals(name);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "toString",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String toString()\n{\r\n    return this.getName();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "hashCode",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "int hashCode()\n{\r\n    return this.getName().hashCode();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getJobQueueInfo",
  "errType" : null,
  "containingMethodsNum" : 10,
  "sourceCodeText" : "JobQueueInfo getJobQueueInfo()\n{\r\n    JobQueueInfo queueInfo = new JobQueueInfo();\r\n    queueInfo.setQueueName(name);\r\n    LOG.debug(\"created jobQInfo \" + queueInfo.getQueueName());\r\n    queueInfo.setQueueState(state.getStateName());\r\n    if (schedulingInfo != null) {\r\n        queueInfo.setSchedulingInfo(schedulingInfo.toString());\r\n    }\r\n    if (props != null) {\r\n        Properties newProps = new Properties();\r\n        for (Object key : props.keySet()) {\r\n            newProps.setProperty(key.toString(), props.getProperty(key.toString()));\r\n        }\r\n        queueInfo.setProperties(newProps);\r\n    }\r\n    if (children != null && children.size() > 0) {\r\n        List<JobQueueInfo> list = new ArrayList<JobQueueInfo>();\r\n        for (Queue child : children) {\r\n            list.add(child.getJobQueueInfo());\r\n        }\r\n        queueInfo.setChildren(list);\r\n    }\r\n    return queueInfo;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "isHierarchySameAs",
  "errType" : null,
  "containingMethodsNum" : 19,
  "sourceCodeText" : "boolean isHierarchySameAs(Queue newState)\n{\r\n    if (newState == null) {\r\n        return false;\r\n    }\r\n    if (!(name.equals(newState.getName()))) {\r\n        LOG.info(\" current name \" + name + \" not equal to \" + newState.getName());\r\n        return false;\r\n    }\r\n    if (children == null || children.size() == 0) {\r\n        if (newState.getChildren() != null && newState.getChildren().size() > 0) {\r\n            LOG.info(newState + \" has added children in refresh \");\r\n            return false;\r\n        }\r\n    } else if (children.size() > 0) {\r\n        if (newState.getChildren() == null) {\r\n            LOG.error(\"In the current state, queue \" + getName() + \" has \" + children.size() + \" but the new state has none!\");\r\n            return false;\r\n        }\r\n        int childrenSize = children.size();\r\n        int newChildrenSize = newState.getChildren().size();\r\n        if (childrenSize != newChildrenSize) {\r\n            LOG.error(\"Number of children for queue \" + newState.getName() + \" in newState is \" + newChildrenSize + \" which is not equal to \" + childrenSize + \" in the current state.\");\r\n            return false;\r\n        }\r\n        Iterator<Queue> itr1 = children.iterator();\r\n        Iterator<Queue> itr2 = newState.getChildren().iterator();\r\n        while (itr1.hasNext()) {\r\n            Queue q = itr1.next();\r\n            Queue newq = itr2.next();\r\n            if (!(q.isHierarchySameAs(newq))) {\r\n                LOG.info(\" Queue \" + q.getName() + \" not equal to \" + newq.getName());\r\n                return false;\r\n            }\r\n        }\r\n    }\r\n    return true;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "downgrade",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "TaskReport downgrade(org.apache.hadoop.mapreduce.TaskReport report)\n{\r\n    return new TaskReport(TaskID.downgrade(report.getTaskID()), report.getProgress(), report.getState(), report.getDiagnostics(), report.getCurrentStatus(), report.getStartTime(), report.getFinishTime(), Counters.downgrade(report.getTaskCounters()));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "downgradeArray",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "TaskReport[] downgradeArray(org.apache.hadoop.mapreduce.TaskReport[] reports)\n{\r\n    List<TaskReport> ret = new ArrayList<TaskReport>();\r\n    for (org.apache.hadoop.mapreduce.TaskReport report : reports) {\r\n        ret.add(downgrade(report));\r\n    }\r\n    return ret.toArray(new TaskReport[0]);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getTaskId",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String getTaskId()\n{\r\n    return TaskID.downgrade(super.getTaskID()).toString();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getTaskID",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "TaskID getTaskID()\n{\r\n    return TaskID.downgrade(super.getTaskID());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getCounters",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Counters getCounters()\n{\r\n    return Counters.downgrade(super.getTaskCounters());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "setSuccessfulAttempt",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setSuccessfulAttempt(TaskAttemptID t)\n{\r\n    super.setSuccessfulAttemptId(t);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getSuccessfulTaskAttempt",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "TaskAttemptID getSuccessfulTaskAttempt()\n{\r\n    return TaskAttemptID.downgrade(super.getSuccessfulTaskAttemptId());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "setRunningTaskAttempts",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void setRunningTaskAttempts(Collection<TaskAttemptID> runningAttempts)\n{\r\n    Collection<org.apache.hadoop.mapreduce.TaskAttemptID> attempts = new ArrayList<org.apache.hadoop.mapreduce.TaskAttemptID>();\r\n    for (TaskAttemptID id : runningAttempts) {\r\n        attempts.add(id);\r\n    }\r\n    super.setRunningTaskAttemptIds(attempts);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getRunningTaskAttempts",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "Collection<TaskAttemptID> getRunningTaskAttempts()\n{\r\n    Collection<TaskAttemptID> attempts = new ArrayList<TaskAttemptID>();\r\n    for (org.apache.hadoop.mapreduce.TaskAttemptID id : super.getRunningTaskAttemptIds()) {\r\n        attempts.add(TaskAttemptID.downgrade(id));\r\n    }\r\n    return attempts;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "setFinishTime",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setFinishTime(long finishTime)\n{\r\n    super.setFinishTime(finishTime);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "setStartTime",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setStartTime(long startTime)\n{\r\n    super.setStartTime(startTime);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getIsMap",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean getIsMap()\n{\r\n    return true;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "setFinishTime",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void setFinishTime(long finishTime)\n{\r\n    super.setFinishTime(finishTime);\r\n    if (getMapFinishTime() == 0) {\r\n        setMapFinishTime(finishTime);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getShuffleFinishTime",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getShuffleFinishTime()\n{\r\n    throw new UnsupportedOperationException(\"getShuffleFinishTime() not supported for MapTask\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "setShuffleFinishTime",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setShuffleFinishTime(long shuffleFinishTime)\n{\r\n    throw new UnsupportedOperationException(\"setShuffleFinishTime() not supported for MapTask\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getMapFinishTime",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getMapFinishTime()\n{\r\n    return mapFinishTime;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "setMapFinishTime",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setMapFinishTime(long mapFinishTime)\n{\r\n    this.mapFinishTime = mapFinishTime;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "statusUpdate",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void statusUpdate(TaskStatus status)\n{\r\n    super.statusUpdate(status);\r\n    if (status.getMapFinishTime() != 0) {\r\n        this.mapFinishTime = status.getMapFinishTime();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "readFields",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void readFields(DataInput in) throws IOException\n{\r\n    super.readFields(in);\r\n    mapFinishTime = in.readLong();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "write",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void write(DataOutput out) throws IOException\n{\r\n    super.write(out);\r\n    out.writeLong(mapFinishTime);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "addFetchFailedMap",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void addFetchFailedMap(TaskAttemptID mapTaskId)\n{\r\n    throw new UnsupportedOperationException(\"addFetchFailedMap() not supported for MapTask\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred\\pipes",
  "methodName" : "getExecutable",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String getExecutable(JobConf conf)\n{\r\n    return conf.get(Submitter.EXECUTABLE);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred\\pipes",
  "methodName" : "setExecutable",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setExecutable(JobConf conf, String executable)\n{\r\n    conf.set(Submitter.EXECUTABLE, executable);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred\\pipes",
  "methodName" : "setIsJavaRecordReader",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setIsJavaRecordReader(JobConf conf, boolean value)\n{\r\n    conf.setBoolean(Submitter.IS_JAVA_RR, value);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred\\pipes",
  "methodName" : "getIsJavaRecordReader",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "boolean getIsJavaRecordReader(JobConf conf)\n{\r\n    return conf.getBoolean(Submitter.IS_JAVA_RR, false);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred\\pipes",
  "methodName" : "setIsJavaMapper",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setIsJavaMapper(JobConf conf, boolean value)\n{\r\n    conf.setBoolean(Submitter.IS_JAVA_MAP, value);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred\\pipes",
  "methodName" : "getIsJavaMapper",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "boolean getIsJavaMapper(JobConf conf)\n{\r\n    return conf.getBoolean(Submitter.IS_JAVA_MAP, false);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred\\pipes",
  "methodName" : "setIsJavaReducer",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setIsJavaReducer(JobConf conf, boolean value)\n{\r\n    conf.setBoolean(Submitter.IS_JAVA_REDUCE, value);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred\\pipes",
  "methodName" : "getIsJavaReducer",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "boolean getIsJavaReducer(JobConf conf)\n{\r\n    return conf.getBoolean(Submitter.IS_JAVA_REDUCE, false);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred\\pipes",
  "methodName" : "setIsJavaRecordWriter",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setIsJavaRecordWriter(JobConf conf, boolean value)\n{\r\n    conf.setBoolean(Submitter.IS_JAVA_RW, value);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred\\pipes",
  "methodName" : "getIsJavaRecordWriter",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "boolean getIsJavaRecordWriter(JobConf conf)\n{\r\n    return conf.getBoolean(Submitter.IS_JAVA_RW, false);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred\\pipes",
  "methodName" : "setIfUnset",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void setIfUnset(JobConf conf, String key, String value)\n{\r\n    if (conf.get(key) == null) {\r\n        conf.set(key, value);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred\\pipes",
  "methodName" : "setJavaPartitioner",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setJavaPartitioner(JobConf conf, Class cls)\n{\r\n    conf.set(Submitter.PARTITIONER, cls.getName());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred\\pipes",
  "methodName" : "getJavaPartitioner",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Class<? extends Partitioner> getJavaPartitioner(JobConf conf)\n{\r\n    return conf.getClass(Submitter.PARTITIONER, HashPartitioner.class, Partitioner.class);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred\\pipes",
  "methodName" : "getKeepCommandFile",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "boolean getKeepCommandFile(JobConf conf)\n{\r\n    return conf.getBoolean(Submitter.PRESERVE_COMMANDFILE, false);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred\\pipes",
  "methodName" : "setKeepCommandFile",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setKeepCommandFile(JobConf conf, boolean keep)\n{\r\n    conf.setBoolean(Submitter.PRESERVE_COMMANDFILE, keep);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred\\pipes",
  "methodName" : "submitJob",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "RunningJob submitJob(JobConf conf) throws IOException\n{\r\n    return runJob(conf);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred\\pipes",
  "methodName" : "runJob",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "RunningJob runJob(JobConf conf) throws IOException\n{\r\n    setupPipesJob(conf);\r\n    return JobClient.runJob(conf);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred\\pipes",
  "methodName" : "jobSubmit",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "RunningJob jobSubmit(JobConf conf) throws IOException\n{\r\n    setupPipesJob(conf);\r\n    return new JobClient(conf).submitJob(conf);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred\\pipes",
  "methodName" : "setupPipesJob",
  "errType" : [ "URISyntaxException" ],
  "containingMethodsNum" : 25,
  "sourceCodeText" : "void setupPipesJob(JobConf conf) throws IOException\n{\r\n    if (!getIsJavaMapper(conf)) {\r\n        conf.setMapRunnerClass(PipesMapRunner.class);\r\n        setJavaPartitioner(conf, conf.getPartitionerClass());\r\n        conf.setPartitionerClass(PipesPartitioner.class);\r\n    }\r\n    if (!getIsJavaReducer(conf)) {\r\n        conf.setReducerClass(PipesReducer.class);\r\n        if (!getIsJavaRecordWriter(conf)) {\r\n            conf.setOutputFormat(NullOutputFormat.class);\r\n        }\r\n    }\r\n    String textClassname = Text.class.getName();\r\n    setIfUnset(conf, MRJobConfig.MAP_OUTPUT_KEY_CLASS, textClassname);\r\n    setIfUnset(conf, MRJobConfig.MAP_OUTPUT_VALUE_CLASS, textClassname);\r\n    setIfUnset(conf, MRJobConfig.OUTPUT_KEY_CLASS, textClassname);\r\n    setIfUnset(conf, MRJobConfig.OUTPUT_VALUE_CLASS, textClassname);\r\n    if (!getIsJavaRecordReader(conf) && !getIsJavaMapper(conf)) {\r\n        conf.setClass(Submitter.INPUT_FORMAT, conf.getInputFormat().getClass(), InputFormat.class);\r\n        conf.setInputFormat(PipesNonJavaInputFormat.class);\r\n    }\r\n    String exec = getExecutable(conf);\r\n    if (exec == null) {\r\n        throw new IllegalArgumentException(\"No application program defined.\");\r\n    }\r\n    if (exec.contains(\"#\")) {\r\n        String defScript = \"$HADOOP_HOME/src/c++/pipes/debug/pipes-default-script\";\r\n        setIfUnset(conf, MRJobConfig.MAP_DEBUG_SCRIPT, defScript);\r\n        setIfUnset(conf, MRJobConfig.REDUCE_DEBUG_SCRIPT, defScript);\r\n    }\r\n    URI[] fileCache = JobContextImpl.getCacheFiles(conf);\r\n    if (fileCache == null) {\r\n        fileCache = new URI[1];\r\n    } else {\r\n        URI[] tmp = new URI[fileCache.length + 1];\r\n        System.arraycopy(fileCache, 0, tmp, 1, fileCache.length);\r\n        fileCache = tmp;\r\n    }\r\n    try {\r\n        fileCache[0] = new URI(exec);\r\n    } catch (URISyntaxException e) {\r\n        IOException ie = new IOException(\"Problem parsing execable URI \" + exec);\r\n        ie.initCause(e);\r\n        throw ie;\r\n    }\r\n    Job.setCacheFiles(fileCache, conf);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred\\pipes",
  "methodName" : "getClass",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Class<? extends InterfaceType> getClass(CommandLine cl, String key, JobConf conf, Class<InterfaceType> cls) throws ClassNotFoundException\n{\r\n    return conf.getClassByName(cl.getOptionValue(key)).asSubclass(cls);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred\\pipes",
  "methodName" : "run",
  "errType" : [ "ParseException" ],
  "containingMethodsNum" : 60,
  "sourceCodeText" : "int run(String[] args) throws Exception\n{\r\n    CommandLineParser cli = new CommandLineParser();\r\n    if (args.length == 0) {\r\n        cli.printUsage();\r\n        return 1;\r\n    }\r\n    cli.addOption(\"input\", false, \"input path to the maps\", \"path\");\r\n    cli.addOption(\"output\", false, \"output path from the reduces\", \"path\");\r\n    cli.addOption(\"jar\", false, \"job jar file\", \"path\");\r\n    cli.addOption(\"inputformat\", false, \"java classname of InputFormat\", \"class\");\r\n    cli.addOption(\"map\", false, \"java classname of Mapper\", \"class\");\r\n    cli.addOption(\"partitioner\", false, \"java classname of Partitioner\", \"class\");\r\n    cli.addOption(\"reduce\", false, \"java classname of Reducer\", \"class\");\r\n    cli.addOption(\"writer\", false, \"java classname of OutputFormat\", \"class\");\r\n    cli.addOption(\"program\", false, \"URI to application executable\", \"class\");\r\n    cli.addOption(\"reduces\", false, \"number of reduces\", \"num\");\r\n    cli.addOption(\"jobconf\", false, \"\\\"n1=v1,n2=v2,..\\\" (Deprecated) Optional. Add or override a JobConf property.\", \"key=val\");\r\n    cli.addOption(\"lazyOutput\", false, \"Optional. Create output lazily\", \"boolean\");\r\n    Parser parser = cli.createParser();\r\n    try {\r\n        GenericOptionsParser genericParser = new GenericOptionsParser(getConf(), args);\r\n        CommandLine results = parser.parse(cli.options, genericParser.getRemainingArgs());\r\n        JobConf job = new JobConf(getConf());\r\n        if (results.hasOption(\"input\")) {\r\n            FileInputFormat.setInputPaths(job, results.getOptionValue(\"input\"));\r\n        }\r\n        if (results.hasOption(\"output\")) {\r\n            FileOutputFormat.setOutputPath(job, new Path(results.getOptionValue(\"output\")));\r\n        }\r\n        if (results.hasOption(\"jar\")) {\r\n            job.setJar(results.getOptionValue(\"jar\"));\r\n        }\r\n        if (results.hasOption(\"inputformat\")) {\r\n            setIsJavaRecordReader(job, true);\r\n            job.setInputFormat(getClass(results, \"inputformat\", job, InputFormat.class));\r\n        }\r\n        if (results.hasOption(\"javareader\")) {\r\n            setIsJavaRecordReader(job, true);\r\n        }\r\n        if (results.hasOption(\"map\")) {\r\n            setIsJavaMapper(job, true);\r\n            job.setMapperClass(getClass(results, \"map\", job, Mapper.class));\r\n        }\r\n        if (results.hasOption(\"partitioner\")) {\r\n            job.setPartitionerClass(getClass(results, \"partitioner\", job, Partitioner.class));\r\n        }\r\n        if (results.hasOption(\"reduce\")) {\r\n            setIsJavaReducer(job, true);\r\n            job.setReducerClass(getClass(results, \"reduce\", job, Reducer.class));\r\n        }\r\n        if (results.hasOption(\"reduces\")) {\r\n            job.setNumReduceTasks(Integer.parseInt(results.getOptionValue(\"reduces\")));\r\n        }\r\n        if (results.hasOption(\"writer\")) {\r\n            setIsJavaRecordWriter(job, true);\r\n            job.setOutputFormat(getClass(results, \"writer\", job, OutputFormat.class));\r\n        }\r\n        if (results.hasOption(\"lazyOutput\")) {\r\n            if (Boolean.parseBoolean(results.getOptionValue(\"lazyOutput\"))) {\r\n                LazyOutputFormat.setOutputFormatClass(job, job.getOutputFormat().getClass());\r\n            }\r\n        }\r\n        if (results.hasOption(\"program\")) {\r\n            setExecutable(job, results.getOptionValue(\"program\"));\r\n        }\r\n        if (results.hasOption(\"jobconf\")) {\r\n            LOG.warn(\"-jobconf option is deprecated, please use -D instead.\");\r\n            String options = results.getOptionValue(\"jobconf\");\r\n            StringTokenizer tokenizer = new StringTokenizer(options, \",\");\r\n            while (tokenizer.hasMoreTokens()) {\r\n                String keyVal = tokenizer.nextToken().trim();\r\n                String[] keyValSplit = keyVal.split(\"=\");\r\n                job.set(keyValSplit[0], keyValSplit[1]);\r\n            }\r\n        }\r\n        String jarFile = job.getJar();\r\n        if (jarFile != null) {\r\n            final URL[] urls = new URL[] { FileSystem.getLocal(job).pathToFile(new Path(jarFile)).toURL() };\r\n            ClassLoader loader = AccessController.doPrivileged(new PrivilegedAction<ClassLoader>() {\r\n\r\n                public ClassLoader run() {\r\n                    return new URLClassLoader(urls);\r\n                }\r\n            });\r\n            job.setClassLoader(loader);\r\n        }\r\n        runJob(job);\r\n        return 0;\r\n    } catch (ParseException pe) {\r\n        LOG.info(\"Error : \" + pe);\r\n        cli.printUsage();\r\n        return 1;\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred\\pipes",
  "methodName" : "main",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void main(String[] args) throws Exception\n{\r\n    int exitCode = new Submitter().run(args);\r\n    ExitUtil.terminate(exitCode);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\aggregate",
  "methodName" : "addNextValue",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void addNextValue(Object val)\n{\r\n    long newVal = Long.parseLong(val.toString());\r\n    if (this.minVal > newVal) {\r\n        this.minVal = newVal;\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\aggregate",
  "methodName" : "addNextValue",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void addNextValue(long newVal)\n{\r\n    if (this.minVal > newVal) {\r\n        this.minVal = newVal;\r\n    }\r\n    ;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\aggregate",
  "methodName" : "getVal",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getVal()\n{\r\n    return this.minVal;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\aggregate",
  "methodName" : "getReport",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getReport()\n{\r\n    return \"\" + minVal;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\aggregate",
  "methodName" : "reset",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void reset()\n{\r\n    minVal = Long.MAX_VALUE;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\aggregate",
  "methodName" : "getCombinerOutput",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "ArrayList<String> getCombinerOutput()\n{\r\n    ArrayList<String> retv = new ArrayList<String>(1);\r\n    retv.add(\"\" + minVal);\r\n    return retv;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output",
  "methodName" : "setSequenceFileOutputKeyClass",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setSequenceFileOutputKeyClass(Job job, Class<?> theClass)\n{\r\n    job.getConfiguration().setClass(KEY_CLASS, theClass, Object.class);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output",
  "methodName" : "setSequenceFileOutputValueClass",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setSequenceFileOutputValueClass(Job job, Class<?> theClass)\n{\r\n    job.getConfiguration().setClass(VALUE_CLASS, theClass, Object.class);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output",
  "methodName" : "getSequenceFileOutputKeyClass",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Class<? extends WritableComparable> getSequenceFileOutputKeyClass(JobContext job)\n{\r\n    return job.getConfiguration().getClass(KEY_CLASS, job.getOutputKeyClass().asSubclass(WritableComparable.class), WritableComparable.class);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output",
  "methodName" : "getSequenceFileOutputValueClass",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Class<? extends Writable> getSequenceFileOutputValueClass(JobContext job)\n{\r\n    return job.getConfiguration().getClass(VALUE_CLASS, job.getOutputValueClass().asSubclass(Writable.class), Writable.class);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output",
  "methodName" : "getRecordWriter",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "RecordWriter<BytesWritable, BytesWritable> getRecordWriter(TaskAttemptContext context) throws IOException\n{\r\n    final SequenceFile.Writer out = getSequenceWriter(context, getSequenceFileOutputKeyClass(context), getSequenceFileOutputValueClass(context));\r\n    return new RecordWriter<BytesWritable, BytesWritable>() {\r\n\r\n        private WritableValueBytes wvaluebytes = new WritableValueBytes();\r\n\r\n        public void write(BytesWritable bkey, BytesWritable bvalue) throws IOException {\r\n            wvaluebytes.reset(bvalue);\r\n            out.appendRaw(bkey.getBytes(), 0, bkey.getLength(), wvaluebytes);\r\n            wvaluebytes.reset(null);\r\n        }\r\n\r\n        public void close(TaskAttemptContext context) throws IOException {\r\n            out.close();\r\n        }\r\n    };\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output",
  "methodName" : "checkOutputSpecs",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void checkOutputSpecs(JobContext job) throws IOException\n{\r\n    super.checkOutputSpecs(job);\r\n    if (getCompressOutput(job) && getOutputCompressionType(job) == CompressionType.RECORD) {\r\n        throw new InvalidJobConfException(\"SequenceFileAsBinaryOutputFormat \" + \"doesn't support Record Compression\");\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "write",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void write(K key, V value) throws IOException, InterruptedException",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "close",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void close(TaskAttemptContext context) throws IOException, InterruptedException",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "skipRangeIterator",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "SkipRangeIterator skipRangeIterator()\n{\r\n    return new SkipRangeIterator(ranges.iterator());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getIndicesCount",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getIndicesCount()\n{\r\n    return indicesCount;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getRanges",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "SortedSet<Range> getRanges()\n{\r\n    return ranges;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "add",
  "errType" : null,
  "containingMethodsNum" : 23,
  "sourceCodeText" : "void add(Range range)\n{\r\n    if (range.isEmpty()) {\r\n        return;\r\n    }\r\n    long startIndex = range.getStartIndex();\r\n    long endIndex = range.getEndIndex();\r\n    SortedSet<Range> headSet = ranges.headSet(range);\r\n    if (headSet.size() > 0) {\r\n        Range previousRange = headSet.last();\r\n        LOG.debug(\"previousRange \" + previousRange);\r\n        if (startIndex < previousRange.getEndIndex()) {\r\n            if (ranges.remove(previousRange)) {\r\n                indicesCount -= previousRange.getLength();\r\n            }\r\n            startIndex = previousRange.getStartIndex();\r\n            endIndex = endIndex >= previousRange.getEndIndex() ? endIndex : previousRange.getEndIndex();\r\n        }\r\n    }\r\n    Iterator<Range> tailSetIt = ranges.tailSet(range).iterator();\r\n    while (tailSetIt.hasNext()) {\r\n        Range nextRange = tailSetIt.next();\r\n        LOG.debug(\"nextRange \" + nextRange + \"   startIndex:\" + startIndex + \"  endIndex:\" + endIndex);\r\n        if (endIndex >= nextRange.getStartIndex()) {\r\n            tailSetIt.remove();\r\n            indicesCount -= nextRange.getLength();\r\n            if (endIndex < nextRange.getEndIndex()) {\r\n                endIndex = nextRange.getEndIndex();\r\n                break;\r\n            }\r\n        } else {\r\n            break;\r\n        }\r\n    }\r\n    add(startIndex, endIndex);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "remove",
  "errType" : null,
  "containingMethodsNum" : 23,
  "sourceCodeText" : "void remove(Range range)\n{\r\n    if (range.isEmpty()) {\r\n        return;\r\n    }\r\n    long startIndex = range.getStartIndex();\r\n    long endIndex = range.getEndIndex();\r\n    SortedSet<Range> headSet = ranges.headSet(range);\r\n    if (headSet.size() > 0) {\r\n        Range previousRange = headSet.last();\r\n        LOG.debug(\"previousRange \" + previousRange);\r\n        if (startIndex < previousRange.getEndIndex()) {\r\n            if (ranges.remove(previousRange)) {\r\n                indicesCount -= previousRange.getLength();\r\n                LOG.debug(\"removed previousRange \" + previousRange);\r\n            }\r\n            add(previousRange.getStartIndex(), startIndex);\r\n            if (endIndex <= previousRange.getEndIndex()) {\r\n                add(endIndex, previousRange.getEndIndex());\r\n            }\r\n        }\r\n    }\r\n    Iterator<Range> tailSetIt = ranges.tailSet(range).iterator();\r\n    while (tailSetIt.hasNext()) {\r\n        Range nextRange = tailSetIt.next();\r\n        LOG.debug(\"nextRange \" + nextRange + \"   startIndex:\" + startIndex + \"  endIndex:\" + endIndex);\r\n        if (endIndex > nextRange.getStartIndex()) {\r\n            tailSetIt.remove();\r\n            indicesCount -= nextRange.getLength();\r\n            if (endIndex < nextRange.getEndIndex()) {\r\n                add(endIndex, nextRange.getEndIndex());\r\n                break;\r\n            }\r\n        } else {\r\n            break;\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "add",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void add(long start, long end)\n{\r\n    if (end > start) {\r\n        Range recRange = new Range(start, end - start);\r\n        ranges.add(recRange);\r\n        indicesCount += recRange.getLength();\r\n        LOG.debug(\"added \" + recRange);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "readFields",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void readFields(DataInput in) throws IOException\n{\r\n    indicesCount = in.readLong();\r\n    ranges = new TreeSet<Range>();\r\n    int size = in.readInt();\r\n    for (int i = 0; i < size; i++) {\r\n        Range range = new Range();\r\n        range.readFields(in);\r\n        ranges.add(range);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "write",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void write(DataOutput out) throws IOException\n{\r\n    out.writeLong(indicesCount);\r\n    out.writeInt(ranges.size());\r\n    Iterator<Range> it = ranges.iterator();\r\n    while (it.hasNext()) {\r\n        Range range = it.next();\r\n        range.write(out);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "toString",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "String toString()\n{\r\n    StringBuffer sb = new StringBuffer();\r\n    Iterator<Range> it = ranges.iterator();\r\n    while (it.hasNext()) {\r\n        Range range = it.next();\r\n        sb.append(range.toString() + \"\\n\");\r\n    }\r\n    return sb.toString();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\input",
  "methodName" : "setMaxSplitSize",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setMaxSplitSize(long maxSplitSize)\n{\r\n    this.maxSplitSize = maxSplitSize;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\input",
  "methodName" : "setMinSplitSizeNode",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setMinSplitSizeNode(long minSplitSizeNode)\n{\r\n    this.minSplitSizeNode = minSplitSizeNode;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\input",
  "methodName" : "setMinSplitSizeRack",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setMinSplitSizeRack(long minSplitSizeRack)\n{\r\n    this.minSplitSizeRack = minSplitSizeRack;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\input",
  "methodName" : "createPool",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void createPool(List<PathFilter> filters)\n{\r\n    pools.add(new MultiPathFilter(filters));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\input",
  "methodName" : "createPool",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void createPool(PathFilter... filters)\n{\r\n    MultiPathFilter multi = new MultiPathFilter();\r\n    for (PathFilter f : filters) {\r\n        multi.add(f);\r\n    }\r\n    pools.add(multi);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\input",
  "methodName" : "isSplitable",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "boolean isSplitable(JobContext context, Path file)\n{\r\n    final CompressionCodec codec = new CompressionCodecFactory(context.getConfiguration()).getCodec(file);\r\n    if (null == codec) {\r\n        return true;\r\n    }\r\n    return codec instanceof SplittableCompressionCodec;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\input",
  "methodName" : "getSplits",
  "errType" : null,
  "containingMethodsNum" : 15,
  "sourceCodeText" : "List<InputSplit> getSplits(JobContext job) throws IOException\n{\r\n    long minSizeNode = 0;\r\n    long minSizeRack = 0;\r\n    long maxSize = 0;\r\n    Configuration conf = job.getConfiguration();\r\n    if (minSplitSizeNode != 0) {\r\n        minSizeNode = minSplitSizeNode;\r\n    } else {\r\n        minSizeNode = conf.getLong(SPLIT_MINSIZE_PERNODE, 0);\r\n    }\r\n    if (minSplitSizeRack != 0) {\r\n        minSizeRack = minSplitSizeRack;\r\n    } else {\r\n        minSizeRack = conf.getLong(SPLIT_MINSIZE_PERRACK, 0);\r\n    }\r\n    if (maxSplitSize != 0) {\r\n        maxSize = maxSplitSize;\r\n    } else {\r\n        maxSize = conf.getLong(\"mapreduce.input.fileinputformat.split.maxsize\", 0);\r\n    }\r\n    if (minSizeNode != 0 && maxSize != 0 && minSizeNode > maxSize) {\r\n        throw new IOException(\"Minimum split size pernode \" + minSizeNode + \" cannot be larger than maximum split size \" + maxSize);\r\n    }\r\n    if (minSizeRack != 0 && maxSize != 0 && minSizeRack > maxSize) {\r\n        throw new IOException(\"Minimum split size per rack \" + minSizeRack + \" cannot be larger than maximum split size \" + maxSize);\r\n    }\r\n    if (minSizeRack != 0 && minSizeNode > minSizeRack) {\r\n        throw new IOException(\"Minimum split size per node \" + minSizeNode + \" cannot be larger than minimum split \" + \"size per rack \" + minSizeRack);\r\n    }\r\n    List<FileStatus> stats = listStatus(job);\r\n    List<InputSplit> splits = new ArrayList<InputSplit>();\r\n    if (stats.size() == 0) {\r\n        return splits;\r\n    }\r\n    for (MultiPathFilter onepool : pools) {\r\n        ArrayList<FileStatus> myPaths = new ArrayList<FileStatus>();\r\n        for (Iterator<FileStatus> iter = stats.iterator(); iter.hasNext(); ) {\r\n            FileStatus p = iter.next();\r\n            if (onepool.accept(p.getPath())) {\r\n                myPaths.add(p);\r\n                iter.remove();\r\n            }\r\n        }\r\n        getMoreSplits(job, myPaths, maxSize, minSizeNode, minSizeRack, splits);\r\n    }\r\n    getMoreSplits(job, stats, maxSize, minSizeNode, minSizeRack, splits);\r\n    rackToNodes.clear();\r\n    return splits;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\input",
  "methodName" : "getMoreSplits",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void getMoreSplits(JobContext job, List<FileStatus> stats, long maxSize, long minSizeNode, long minSizeRack, List<InputSplit> splits) throws IOException\n{\r\n    Configuration conf = job.getConfiguration();\r\n    OneFileInfo[] files;\r\n    HashMap<String, List<OneBlockInfo>> rackToBlocks = new HashMap<String, List<OneBlockInfo>>();\r\n    HashMap<OneBlockInfo, String[]> blockToNodes = new HashMap<OneBlockInfo, String[]>();\r\n    HashMap<String, Set<OneBlockInfo>> nodeToBlocks = new HashMap<String, Set<OneBlockInfo>>();\r\n    files = new OneFileInfo[stats.size()];\r\n    if (stats.size() == 0) {\r\n        return;\r\n    }\r\n    long totLength = 0;\r\n    int i = 0;\r\n    for (FileStatus stat : stats) {\r\n        files[i] = new OneFileInfo(stat, conf, isSplitable(job, stat.getPath()), rackToBlocks, blockToNodes, nodeToBlocks, rackToNodes, maxSize);\r\n        totLength += files[i].getLength();\r\n    }\r\n    createSplits(nodeToBlocks, blockToNodes, rackToBlocks, totLength, maxSize, minSizeNode, minSizeRack, splits);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\input",
  "methodName" : "createSplits",
  "errType" : null,
  "containingMethodsNum" : 58,
  "sourceCodeText" : "void createSplits(Map<String, Set<OneBlockInfo>> nodeToBlocks, Map<OneBlockInfo, String[]> blockToNodes, Map<String, List<OneBlockInfo>> rackToBlocks, long totLength, long maxSize, long minSizeNode, long minSizeRack, List<InputSplit> splits)\n{\r\n    ArrayList<OneBlockInfo> validBlocks = new ArrayList<OneBlockInfo>();\r\n    long curSplitSize = 0;\r\n    int totalNodes = nodeToBlocks.size();\r\n    long totalLength = totLength;\r\n    Multiset<String> splitsPerNode = HashMultiset.create();\r\n    Set<String> completedNodes = new HashSet<String>();\r\n    while (true) {\r\n        for (Iterator<Map.Entry<String, Set<OneBlockInfo>>> iter = nodeToBlocks.entrySet().iterator(); iter.hasNext(); ) {\r\n            Map.Entry<String, Set<OneBlockInfo>> one = iter.next();\r\n            String node = one.getKey();\r\n            if (completedNodes.contains(node)) {\r\n                continue;\r\n            }\r\n            Set<OneBlockInfo> blocksInCurrentNode = one.getValue();\r\n            Iterator<OneBlockInfo> oneBlockIter = blocksInCurrentNode.iterator();\r\n            while (oneBlockIter.hasNext()) {\r\n                OneBlockInfo oneblock = oneBlockIter.next();\r\n                if (!blockToNodes.containsKey(oneblock)) {\r\n                    oneBlockIter.remove();\r\n                    continue;\r\n                }\r\n                validBlocks.add(oneblock);\r\n                blockToNodes.remove(oneblock);\r\n                curSplitSize += oneblock.length;\r\n                if (maxSize != 0 && curSplitSize >= maxSize) {\r\n                    addCreatedSplit(splits, Collections.singleton(node), validBlocks);\r\n                    totalLength -= curSplitSize;\r\n                    curSplitSize = 0;\r\n                    splitsPerNode.add(node);\r\n                    blocksInCurrentNode.removeAll(validBlocks);\r\n                    validBlocks.clear();\r\n                    break;\r\n                }\r\n            }\r\n            if (validBlocks.size() != 0) {\r\n                if (minSizeNode != 0 && curSplitSize >= minSizeNode && splitsPerNode.count(node) == 0) {\r\n                    addCreatedSplit(splits, Collections.singleton(node), validBlocks);\r\n                    totalLength -= curSplitSize;\r\n                    splitsPerNode.add(node);\r\n                    blocksInCurrentNode.removeAll(validBlocks);\r\n                } else {\r\n                    for (OneBlockInfo oneblock : validBlocks) {\r\n                        blockToNodes.put(oneblock, oneblock.hosts);\r\n                    }\r\n                }\r\n                validBlocks.clear();\r\n                curSplitSize = 0;\r\n                completedNodes.add(node);\r\n            } else {\r\n                if (blocksInCurrentNode.size() == 0) {\r\n                    completedNodes.add(node);\r\n                }\r\n            }\r\n        }\r\n        if (completedNodes.size() == totalNodes || totalLength == 0) {\r\n            LOG.debug(\"Terminated node allocation with : CompletedNodes: {}, size left: {}\", completedNodes.size(), totalLength);\r\n            break;\r\n        }\r\n    }\r\n    ArrayList<OneBlockInfo> overflowBlocks = new ArrayList<OneBlockInfo>();\r\n    Set<String> racks = new HashSet<String>();\r\n    while (blockToNodes.size() > 0) {\r\n        for (Iterator<Map.Entry<String, List<OneBlockInfo>>> iter = rackToBlocks.entrySet().iterator(); iter.hasNext(); ) {\r\n            Map.Entry<String, List<OneBlockInfo>> one = iter.next();\r\n            racks.add(one.getKey());\r\n            List<OneBlockInfo> blocks = one.getValue();\r\n            boolean createdSplit = false;\r\n            for (OneBlockInfo oneblock : blocks) {\r\n                if (blockToNodes.containsKey(oneblock)) {\r\n                    validBlocks.add(oneblock);\r\n                    blockToNodes.remove(oneblock);\r\n                    curSplitSize += oneblock.length;\r\n                    if (maxSize != 0 && curSplitSize >= maxSize) {\r\n                        addCreatedSplit(splits, getHosts(racks), validBlocks);\r\n                        createdSplit = true;\r\n                        break;\r\n                    }\r\n                }\r\n            }\r\n            if (createdSplit) {\r\n                curSplitSize = 0;\r\n                validBlocks.clear();\r\n                racks.clear();\r\n                continue;\r\n            }\r\n            if (!validBlocks.isEmpty()) {\r\n                if (minSizeRack != 0 && curSplitSize >= minSizeRack) {\r\n                    addCreatedSplit(splits, getHosts(racks), validBlocks);\r\n                } else {\r\n                    overflowBlocks.addAll(validBlocks);\r\n                }\r\n            }\r\n            curSplitSize = 0;\r\n            validBlocks.clear();\r\n            racks.clear();\r\n        }\r\n    }\r\n    assert blockToNodes.isEmpty();\r\n    assert curSplitSize == 0;\r\n    assert validBlocks.isEmpty();\r\n    assert racks.isEmpty();\r\n    for (OneBlockInfo oneblock : overflowBlocks) {\r\n        validBlocks.add(oneblock);\r\n        curSplitSize += oneblock.length;\r\n        for (int i = 0; i < oneblock.racks.length; i++) {\r\n            racks.add(oneblock.racks[i]);\r\n        }\r\n        if (maxSize != 0 && curSplitSize >= maxSize) {\r\n            addCreatedSplit(splits, getHosts(racks), validBlocks);\r\n            curSplitSize = 0;\r\n            validBlocks.clear();\r\n            racks.clear();\r\n        }\r\n    }\r\n    if (!validBlocks.isEmpty()) {\r\n        addCreatedSplit(splits, getHosts(racks), validBlocks);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\input",
  "methodName" : "addCreatedSplit",
  "errType" : null,
  "containingMethodsNum" : 9,
  "sourceCodeText" : "void addCreatedSplit(List<InputSplit> splitList, Collection<String> locations, ArrayList<OneBlockInfo> validBlocks)\n{\r\n    Path[] fl = new Path[validBlocks.size()];\r\n    long[] offset = new long[validBlocks.size()];\r\n    long[] length = new long[validBlocks.size()];\r\n    for (int i = 0; i < validBlocks.size(); i++) {\r\n        fl[i] = validBlocks.get(i).onepath;\r\n        offset[i] = validBlocks.get(i).offset;\r\n        length[i] = validBlocks.get(i).length;\r\n    }\r\n    CombineFileSplit thissplit = new CombineFileSplit(fl, offset, length, locations.toArray(new String[0]));\r\n    splitList.add(thissplit);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\input",
  "methodName" : "createRecordReader",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "RecordReader<K, V> createRecordReader(InputSplit split, TaskAttemptContext context) throws IOException",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\input",
  "methodName" : "getFileBlockLocations",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "BlockLocation[] getFileBlockLocations(FileSystem fs, FileStatus stat) throws IOException\n{\r\n    if (stat instanceof LocatedFileStatus) {\r\n        return ((LocatedFileStatus) stat).getBlockLocations();\r\n    }\r\n    return fs.getFileBlockLocations(stat, 0, stat.getLen());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\input",
  "methodName" : "addHostToRack",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void addHostToRack(Map<String, Set<String>> rackToNodes, String rack, String host)\n{\r\n    Set<String> hosts = rackToNodes.get(rack);\r\n    if (hosts == null) {\r\n        hosts = new HashSet<String>();\r\n        rackToNodes.put(rack, hosts);\r\n    }\r\n    hosts.add(host);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\input",
  "methodName" : "getHosts",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "Set<String> getHosts(Set<String> racks)\n{\r\n    Set<String> hosts = new HashSet<String>();\r\n    for (String rack : racks) {\r\n        if (rackToNodes.containsKey(rack)) {\r\n            hosts.addAll(rackToNodes.get(rack));\r\n        }\r\n    }\r\n    return hosts;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "setup",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setup(Context context) throws IOException, InterruptedException\n{\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "reduce",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void reduce(KEYIN key, Iterable<VALUEIN> values, Context context) throws IOException, InterruptedException\n{\r\n    for (VALUEIN value : values) {\r\n        context.write((KEYOUT) key, (VALUEOUT) value);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "cleanup",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void cleanup(Context context) throws IOException, InterruptedException\n{\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "run",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void run(Context context) throws IOException, InterruptedException\n{\r\n    setup(context);\r\n    try {\r\n        while (context.nextKey()) {\r\n            reduce(context.getCurrentKey(), context.getValues(), context);\r\n            Iterator<VALUEIN> iter = context.getValues().iterator();\r\n            if (iter instanceof ReduceContext.ValueIterator) {\r\n                ((ReduceContext.ValueIterator<VALUEIN>) iter).resetBackupStore();\r\n            }\r\n        }\r\n    } finally {\r\n        cleanup(context);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\map",
  "methodName" : "setup",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void setup(Context context)\n{\r\n    Configuration conf = context.getConfiguration();\r\n    pattern = Pattern.compile(conf.get(PATTERN));\r\n    group = conf.getInt(GROUP, 0);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\map",
  "methodName" : "map",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void map(K key, Text value, Context context) throws IOException, InterruptedException\n{\r\n    String text = value.toString();\r\n    Matcher matcher = pattern.matcher(text);\r\n    while (matcher.find()) {\r\n        context.write(new Text(matcher.group(group)), new LongWritable(1));\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\partition",
  "methodName" : "setConf",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 13,
  "sourceCodeText" : "void setConf(Configuration conf)\n{\r\n    try {\r\n        this.conf = conf;\r\n        String parts = getPartitionFile(conf);\r\n        final Path partFile = new Path(parts);\r\n        final FileSystem fs = (DEFAULT_PATH.equals(parts)) ? FileSystem.getLocal(conf) : partFile.getFileSystem(conf);\r\n        Job job = Job.getInstance(conf);\r\n        Class<K> keyClass = (Class<K>) job.getMapOutputKeyClass();\r\n        K[] splitPoints = readPartitions(fs, partFile, keyClass, conf);\r\n        if (splitPoints.length != job.getNumReduceTasks() - 1) {\r\n            throw new IOException(\"Wrong number of partitions in keyset\");\r\n        }\r\n        RawComparator<K> comparator = (RawComparator<K>) job.getSortComparator();\r\n        for (int i = 0; i < splitPoints.length - 1; ++i) {\r\n            if (comparator.compare(splitPoints[i], splitPoints[i + 1]) >= 0) {\r\n                throw new IOException(\"Split points are out of order\");\r\n            }\r\n        }\r\n        boolean natOrder = conf.getBoolean(NATURAL_ORDER, true);\r\n        if (natOrder && BinaryComparable.class.isAssignableFrom(keyClass)) {\r\n            partitions = buildTrie((BinaryComparable[]) splitPoints, 0, splitPoints.length, new byte[0], conf.getInt(MAX_TRIE_DEPTH, 200));\r\n        } else {\r\n            partitions = new BinarySearchNode(splitPoints, comparator);\r\n        }\r\n    } catch (IOException e) {\r\n        throw new IllegalArgumentException(\"Can't read partitions file\", e);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\partition",
  "methodName" : "getConf",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Configuration getConf()\n{\r\n    return conf;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\partition",
  "methodName" : "getPartition",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "int getPartition(K key, V value, int numPartitions)\n{\r\n    return partitions.findPartition(key);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\partition",
  "methodName" : "setPartitionFile",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setPartitionFile(Configuration conf, Path p)\n{\r\n    conf.set(PARTITIONER_PATH, p.toString());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\partition",
  "methodName" : "getPartitionFile",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String getPartitionFile(Configuration conf)\n{\r\n    return conf.get(PARTITIONER_PATH, DEFAULT_PATH);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\partition",
  "methodName" : "LeafTrieNodeFactory",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "TrieNode LeafTrieNodeFactory(int level, BinaryComparable[] splitPoints, int lower, int upper)\n{\r\n    switch(upper - lower) {\r\n        case 0:\r\n            return new UnsplitTrieNode(level, lower);\r\n        case 1:\r\n            return new SinglySplitTrieNode(level, splitPoints, lower);\r\n        default:\r\n            return new LeafTrieNode(level, splitPoints, lower, upper);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\partition",
  "methodName" : "readPartitions",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "K[] readPartitions(FileSystem fs, Path p, Class<K> keyClass, Configuration conf) throws IOException\n{\r\n    SequenceFile.Reader reader = new SequenceFile.Reader(conf, SequenceFile.Reader.file(p));\r\n    ArrayList<K> parts = new ArrayList<K>();\r\n    K key = ReflectionUtils.newInstance(keyClass, conf);\r\n    try {\r\n        while ((key = (K) reader.next(key)) != null) {\r\n            parts.add(key);\r\n            key = ReflectionUtils.newInstance(keyClass, conf);\r\n        }\r\n        reader.close();\r\n        reader = null;\r\n    } finally {\r\n        IOUtils.cleanupWithLogger(LOG, reader);\r\n    }\r\n    return parts.toArray((K[]) Array.newInstance(keyClass, parts.size()));\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\partition",
  "methodName" : "buildTrie",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "TrieNode buildTrie(BinaryComparable[] splits, int lower, int upper, byte[] prefix, int maxDepth)\n{\r\n    return buildTrieRec(splits, lower, upper, prefix, maxDepth, new CarriedTrieNodeRef());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\partition",
  "methodName" : "buildTrieRec",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "TrieNode buildTrieRec(BinaryComparable[] splits, int lower, int upper, byte[] prefix, int maxDepth, CarriedTrieNodeRef ref)\n{\r\n    final int depth = prefix.length;\r\n    if (depth >= maxDepth || lower >= upper - 1) {\r\n        if (lower == upper && ref.content != null) {\r\n            return ref.content;\r\n        }\r\n        TrieNode result = LeafTrieNodeFactory(depth, splits, lower, upper);\r\n        ref.content = lower == upper ? result : null;\r\n        return result;\r\n    }\r\n    InnerTrieNode result = new InnerTrieNode(depth);\r\n    byte[] trial = Arrays.copyOf(prefix, prefix.length + 1);\r\n    int currentBound = lower;\r\n    for (int ch = 0; ch < 0xFF; ++ch) {\r\n        trial[depth] = (byte) (ch + 1);\r\n        lower = currentBound;\r\n        while (currentBound < upper) {\r\n            if (splits[currentBound].compareTo(trial, 0, trial.length) >= 0) {\r\n                break;\r\n            }\r\n            currentBound += 1;\r\n        }\r\n        trial[depth] = (byte) ch;\r\n        result.child[0xFF & ch] = buildTrieRec(splits, lower, currentBound, trial, maxDepth, ref);\r\n    }\r\n    trial[depth] = (byte) 0xFF;\r\n    result.child[0xFF] = buildTrieRec(splits, lower, currentBound, trial, maxDepth, ref);\r\n    return result;\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\aggregate",
  "methodName" : "setMaxItems",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long setMaxItems(long n)\n{\r\n    if (n >= numItems) {\r\n        this.maxNumItems = n;\r\n    } else if (this.maxNumItems >= this.numItems) {\r\n        this.maxNumItems = this.numItems;\r\n    }\r\n    return this.maxNumItems;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\aggregate",
  "methodName" : "addNextValue",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void addNextValue(Object val)\n{\r\n    if (this.numItems <= this.maxNumItems) {\r\n        uniqItems.put(val.toString(), \"1\");\r\n        this.numItems = this.uniqItems.size();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\aggregate",
  "methodName" : "getReport",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String getReport()\n{\r\n    return \"\" + uniqItems.size();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\aggregate",
  "methodName" : "getUniqueItems",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Set<Object> getUniqueItems()\n{\r\n    return uniqItems.keySet();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\aggregate",
  "methodName" : "reset",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void reset()\n{\r\n    uniqItems = new TreeMap<Object, Object>();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\aggregate",
  "methodName" : "getCombinerOutput",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "ArrayList<Object> getCombinerOutput()\n{\r\n    Object key = null;\r\n    Iterator<Object> iter = uniqItems.keySet().iterator();\r\n    ArrayList<Object> retv = new ArrayList<Object>();\r\n    while (iter.hasNext()) {\r\n        key = iter.next();\r\n        retv.add(key);\r\n    }\r\n    return retv;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\aggregate",
  "methodName" : "addNextValue",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void addNextValue(Object val)\n{\r\n    String newVal = val.toString();\r\n    if (this.minVal == null || this.minVal.compareTo(newVal) > 0) {\r\n        this.minVal = newVal;\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\aggregate",
  "methodName" : "getVal",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getVal()\n{\r\n    return this.minVal;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\aggregate",
  "methodName" : "getReport",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getReport()\n{\r\n    return minVal;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\aggregate",
  "methodName" : "reset",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void reset()\n{\r\n    minVal = null;\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\aggregate",
  "methodName" : "getCombinerOutput",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "ArrayList<String> getCombinerOutput()\n{\r\n    ArrayList<String> retv = new ArrayList<String>(1);\r\n    retv.add(minVal);\r\n    return retv;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\jobcontrol",
  "methodName" : "toString",
  "errType" : null,
  "containingMethodsNum" : 12,
  "sourceCodeText" : "String toString()\n{\r\n    StringBuffer sb = new StringBuffer();\r\n    sb.append(\"job name:\\t\").append(this.job.getJobName()).append(\"\\n\");\r\n    sb.append(\"job id:\\t\").append(this.controlID).append(\"\\n\");\r\n    sb.append(\"job state:\\t\").append(this.state).append(\"\\n\");\r\n    sb.append(\"job mapred id:\\t\").append(this.job.getJobID()).append(\"\\n\");\r\n    sb.append(\"job message:\\t\").append(this.message).append(\"\\n\");\r\n    if (this.dependingJobs == null || this.dependingJobs.size() == 0) {\r\n        sb.append(\"job has no depending job:\\t\").append(\"\\n\");\r\n    } else {\r\n        sb.append(\"job has \").append(this.dependingJobs.size()).append(\" dependeng jobs:\\n\");\r\n        for (int i = 0; i < this.dependingJobs.size(); i++) {\r\n            sb.append(\"\\t depending job \").append(i).append(\":\\t\");\r\n            sb.append((this.dependingJobs.get(i)).getJobName()).append(\"\\n\");\r\n        }\r\n    }\r\n    return sb.toString();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\jobcontrol",
  "methodName" : "getJobName",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String getJobName()\n{\r\n    return job.getJobName();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\jobcontrol",
  "methodName" : "setJobName",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setJobName(String jobName)\n{\r\n    job.setJobName(jobName);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\jobcontrol",
  "methodName" : "getJobID",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getJobID()\n{\r\n    return this.controlID;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\jobcontrol",
  "methodName" : "setJobID",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setJobID(String id)\n{\r\n    this.controlID = id;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\jobcontrol",
  "methodName" : "getMapredJobId",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "JobID getMapredJobId()\n{\r\n    return this.job.getJobID();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\jobcontrol",
  "methodName" : "getJob",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Job getJob()\n{\r\n    return this.job;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\jobcontrol",
  "methodName" : "setJob",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setJob(Job job)\n{\r\n    this.job = job;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\jobcontrol",
  "methodName" : "getJobState",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "State getJobState()\n{\r\n    return this.state;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\jobcontrol",
  "methodName" : "setJobState",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setJobState(State state)\n{\r\n    this.state = state;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\jobcontrol",
  "methodName" : "getMessage",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getMessage()\n{\r\n    return this.message;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\jobcontrol",
  "methodName" : "setMessage",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setMessage(String message)\n{\r\n    this.message = message;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\jobcontrol",
  "methodName" : "getDependentJobs",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "List<ControlledJob> getDependentJobs()\n{\r\n    return this.dependingJobs;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\jobcontrol",
  "methodName" : "addDependingJob",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "boolean addDependingJob(ControlledJob dependingJob)\n{\r\n    if (this.state == State.WAITING) {\r\n        if (this.dependingJobs == null) {\r\n            this.dependingJobs = new ArrayList<ControlledJob>();\r\n        }\r\n        return this.dependingJobs.add(dependingJob);\r\n    } else {\r\n        return false;\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\jobcontrol",
  "methodName" : "isCompleted",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean isCompleted()\n{\r\n    return this.state == State.FAILED || this.state == State.DEPENDENT_FAILED || this.state == State.SUCCESS;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\jobcontrol",
  "methodName" : "isReady",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean isReady()\n{\r\n    return this.state == State.READY;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\jobcontrol",
  "methodName" : "killJob",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void killJob() throws IOException, InterruptedException\n{\r\n    job.killJob();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\jobcontrol",
  "methodName" : "failJob",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void failJob(String message) throws IOException, InterruptedException\n{\r\n    try {\r\n        if (job != null && this.state == State.RUNNING) {\r\n            job.killJob();\r\n        }\r\n    } finally {\r\n        this.state = State.FAILED;\r\n        this.message = message;\r\n    }\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\jobcontrol",
  "methodName" : "checkRunningState",
  "errType" : [ "IOException", "IOException" ],
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void checkRunningState() throws IOException, InterruptedException\n{\r\n    try {\r\n        if (job.isComplete()) {\r\n            if (job.isSuccessful()) {\r\n                this.state = State.SUCCESS;\r\n            } else {\r\n                this.state = State.FAILED;\r\n                this.message = \"Job failed!\";\r\n            }\r\n        }\r\n    } catch (IOException ioe) {\r\n        this.state = State.FAILED;\r\n        this.message = StringUtils.stringifyException(ioe);\r\n        try {\r\n            if (job != null) {\r\n                job.killJob();\r\n            }\r\n        } catch (IOException e) {\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 2,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\jobcontrol",
  "methodName" : "checkState",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "State checkState() throws IOException, InterruptedException\n{\r\n    if (this.state == State.RUNNING) {\r\n        checkRunningState();\r\n    }\r\n    if (this.state != State.WAITING) {\r\n        return this.state;\r\n    }\r\n    if (this.dependingJobs == null || this.dependingJobs.size() == 0) {\r\n        this.state = State.READY;\r\n        return this.state;\r\n    }\r\n    ControlledJob pred = null;\r\n    int n = this.dependingJobs.size();\r\n    for (int i = 0; i < n; i++) {\r\n        pred = this.dependingJobs.get(i);\r\n        State s = pred.checkState();\r\n        if (s == State.WAITING || s == State.READY || s == State.RUNNING) {\r\n            break;\r\n        }\r\n        if (s == State.FAILED || s == State.DEPENDENT_FAILED) {\r\n            this.state = State.DEPENDENT_FAILED;\r\n            this.message = \"depending job \" + i + \" with jobID \" + pred.getJobID() + \" failed. \" + pred.getMessage();\r\n            break;\r\n        }\r\n        if (i == n - 1) {\r\n            this.state = State.READY;\r\n        }\r\n    }\r\n    return this.state;\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\jobcontrol",
  "methodName" : "submit",
  "errType" : [ "Exception", "IOException" ],
  "containingMethodsNum" : 9,
  "sourceCodeText" : "void submit()\n{\r\n    try {\r\n        Configuration conf = job.getConfiguration();\r\n        if (conf.getBoolean(CREATE_DIR, false)) {\r\n            FileSystem fs = FileSystem.get(conf);\r\n            Path[] inputPaths = FileInputFormat.getInputPaths(job);\r\n            for (int i = 0; i < inputPaths.length; i++) {\r\n                if (!fs.exists(inputPaths[i])) {\r\n                    try {\r\n                        fs.mkdirs(inputPaths[i]);\r\n                    } catch (IOException e) {\r\n                    }\r\n                }\r\n            }\r\n        }\r\n        job.submit();\r\n        this.state = State.RUNNING;\r\n    } catch (Exception ioe) {\r\n        LOG.info(getJobName() + \" got an error while submitting \", ioe);\r\n        this.state = State.FAILED;\r\n        this.message = StringUtils.stringifyException(ioe);\r\n    }\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 2,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output",
  "methodName" : "getRecordWriter",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "RecordWriter<K, V> getRecordWriter(TaskAttemptContext context)\n{\r\n    return new RecordWriter<K, V>() {\r\n\r\n        public void write(K key, V value) {\r\n        }\r\n\r\n        public void close(TaskAttemptContext context) {\r\n        }\r\n    };\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output",
  "methodName" : "checkOutputSpecs",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void checkOutputSpecs(JobContext context)\n{\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output",
  "methodName" : "getOutputCommitter",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "OutputCommitter getOutputCommitter(TaskAttemptContext context)\n{\r\n    return new OutputCommitter() {\r\n\r\n        public void abortTask(TaskAttemptContext taskContext) {\r\n        }\r\n\r\n        public void cleanupJob(JobContext jobContext) {\r\n        }\r\n\r\n        public void commitTask(TaskAttemptContext taskContext) {\r\n        }\r\n\r\n        public boolean needsTaskCommit(TaskAttemptContext taskContext) {\r\n            return false;\r\n        }\r\n\r\n        public void setupJob(JobContext jobContext) {\r\n        }\r\n\r\n        public void setupTask(TaskAttemptContext taskContext) {\r\n        }\r\n\r\n        @Override\r\n        @Deprecated\r\n        public boolean isRecoverySupported() {\r\n            return true;\r\n        }\r\n\r\n        @Override\r\n        public void recoverTask(TaskAttemptContext taskContext) throws IOException {\r\n        }\r\n    };\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "write",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void write(DataInputBuffer key, DataInputBuffer value) throws IOException\n{\r\n    assert (key != null && value != null);\r\n    if (fileCache.isActive()) {\r\n        fileCache.write(key, value);\r\n        return;\r\n    }\r\n    if (memCache.reserveSpace(key, value)) {\r\n        memCache.write(key, value);\r\n    } else {\r\n        fileCache.activate();\r\n        fileCache.write(key, value);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "mark",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void mark() throws IOException\n{\r\n    if (nextKVOffset == 0) {\r\n        assert (readSegmentIndex != 0);\r\n        assert (currentKVOffset != 0);\r\n        readSegmentIndex--;\r\n    }\r\n    int i = 0;\r\n    Iterator<Segment<K, V>> itr = segmentList.iterator();\r\n    while (itr.hasNext()) {\r\n        Segment<K, V> s = itr.next();\r\n        if (i == readSegmentIndex) {\r\n            break;\r\n        }\r\n        s.close();\r\n        itr.remove();\r\n        i++;\r\n        LOG.debug(\"Dropping a segment\");\r\n    }\r\n    firstSegmentOffset = currentKVOffset;\r\n    readSegmentIndex = 0;\r\n    LOG.debug(\"Setting the FirsSegmentOffset to \" + currentKVOffset);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "reset",
  "errType" : null,
  "containingMethodsNum" : 10,
  "sourceCodeText" : "void reset() throws IOException\n{\r\n    if (!inReset) {\r\n        if (fileCache.isActive) {\r\n            fileCache.createInDiskSegment();\r\n        } else {\r\n            memCache.createInMemorySegment();\r\n        }\r\n    }\r\n    inReset = true;\r\n    for (int i = 0; i < segmentList.size(); i++) {\r\n        Segment<K, V> s = segmentList.get(i);\r\n        if (s.inMemory()) {\r\n            int offset = (i == 0) ? firstSegmentOffset : 0;\r\n            s.getReader().reset(offset);\r\n        } else {\r\n            s.closeReader();\r\n            if (i == 0) {\r\n                s.reinitReader(firstSegmentOffset);\r\n                s.getReader().disableChecksumValidation();\r\n            }\r\n        }\r\n    }\r\n    currentKVOffset = firstSegmentOffset;\r\n    nextKVOffset = -1;\r\n    readSegmentIndex = 0;\r\n    hasMore = false;\r\n    lastSegmentEOF = false;\r\n    LOG.debug(\"Reset - First segment offset is \" + firstSegmentOffset + \" Segment List Size is \" + segmentList.size());\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "hasNext",
  "errType" : null,
  "containingMethodsNum" : 15,
  "sourceCodeText" : "boolean hasNext() throws IOException\n{\r\n    if (lastSegmentEOF) {\r\n        return false;\r\n    }\r\n    if (hasMore) {\r\n        return true;\r\n    }\r\n    Segment<K, V> seg = segmentList.get(readSegmentIndex);\r\n    nextKVOffset = (int) seg.getActualPosition();\r\n    if (seg.nextRawKey()) {\r\n        currentKey = seg.getKey();\r\n        seg.getValue(currentValue);\r\n        hasMore = true;\r\n        return true;\r\n    } else {\r\n        if (!seg.inMemory()) {\r\n            seg.closeReader();\r\n        }\r\n    }\r\n    if (readSegmentIndex == segmentList.size() - 1) {\r\n        nextKVOffset = -1;\r\n        lastSegmentEOF = true;\r\n        return false;\r\n    }\r\n    nextKVOffset = 0;\r\n    readSegmentIndex++;\r\n    Segment<K, V> nextSegment = segmentList.get(readSegmentIndex);\r\n    if (!nextSegment.inMemory()) {\r\n        currentValue.reset(currentDiskValue.getData(), currentDiskValue.getLength());\r\n        nextSegment.init(null);\r\n    }\r\n    if (nextSegment.nextRawKey()) {\r\n        currentKey = nextSegment.getKey();\r\n        nextSegment.getValue(currentValue);\r\n        hasMore = true;\r\n        return true;\r\n    } else {\r\n        throw new IOException(\"New segment did not have even one K/V\");\r\n    }\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "next",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void next() throws IOException\n{\r\n    if (!hasNext()) {\r\n        throw new NoSuchElementException(\"iterate past last value\");\r\n    }\r\n    hasMore = false;\r\n    currentKVOffset = nextKVOffset;\r\n    nextKVOffset = -1;\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "nextValue",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "DataInputBuffer nextValue()\n{\r\n    return currentValue;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "nextKey",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "DataInputBuffer nextKey()\n{\r\n    return currentKey;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "reinitialize",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void reinitialize() throws IOException\n{\r\n    if (segmentList.size() != 0) {\r\n        clearSegmentList();\r\n    }\r\n    memCache.reinitialize(true);\r\n    fileCache.reinitialize();\r\n    readSegmentIndex = firstSegmentOffset = 0;\r\n    currentKVOffset = 0;\r\n    nextKVOffset = -1;\r\n    hasMore = inReset = clearMarkFlag = false;\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "exitResetMode",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void exitResetMode() throws IOException\n{\r\n    inReset = false;\r\n    if (clearMarkFlag) {\r\n        reinitialize();\r\n        return;\r\n    }\r\n    if (!fileCache.isActive) {\r\n        memCache.reinitialize(false);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getOutputStream",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "DataOutputStream getOutputStream(int length) throws IOException\n{\r\n    if (memCache.reserveSpace(length)) {\r\n        return memCache.dataOut;\r\n    } else {\r\n        fileCache.activate();\r\n        return fileCache.writer.getOutputStream();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "updateCounters",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void updateCounters(int length)\n{\r\n    if (fileCache.isActive) {\r\n        fileCache.writer.updateCountersForExternalAppend(length);\r\n    } else {\r\n        memCache.usedSize += length;\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "clearMark",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void clearMark() throws IOException\n{\r\n    if (inReset) {\r\n        clearMarkFlag = true;\r\n    } else {\r\n        reinitialize();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "clearSegmentList",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void clearSegmentList() throws IOException\n{\r\n    for (Segment<K, V> segment : segmentList) {\r\n        long len = segment.getLength();\r\n        segment.close();\r\n        if (segment.inMemory()) {\r\n            memCache.unreserve(len);\r\n        }\r\n    }\r\n    segmentList.clear();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred\\lib",
  "methodName" : "getRecordReader",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "RecordReader<K, V> getRecordReader(InputSplit split, JobConf conf, Reporter reporter) throws IOException\n{\r\n    return new CombineFileRecordReader(conf, (CombineFileSplit) split, reporter, SequenceFileRecordReaderWrapper.class);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "toAvro",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "List<Integer> toAvro(int[] values)\n{\r\n    List<Integer> result = new ArrayList<Integer>(values.length);\r\n    for (int i = 0; i < values.length; ++i) {\r\n        result.add(values[i]);\r\n    }\r\n    return result;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "fromAvro",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "int[] fromAvro(List<Integer> avro)\n{\r\n    int[] result = new int[avro.size()];\r\n    int i = 0;\r\n    for (Iterator<Integer> iter = avro.iterator(); iter.hasNext(); ++i) {\r\n        result[i] = iter.next();\r\n    }\r\n    return result;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getUser",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getUser()\n{\r\n    return user;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getJobID",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "JobID getJobID()\n{\r\n    return jobid;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getJobId",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String getJobId()\n{\r\n    return jobid.toString();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getJobFile",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getJobFile()\n{\r\n    return jobFile;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getURL",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 0,
  "sourceCodeText" : "URL getURL()\n{\r\n    try {\r\n        return new URL(url);\r\n    } catch (IOException ie) {\r\n        return null;\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getJobName",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getJobName()\n{\r\n    return name;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getQueueName",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getQueueName()\n{\r\n    return queueName;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "write",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void write(DataOutput out) throws IOException\n{\r\n    jobid.write(out);\r\n    Text.writeString(out, jobFile);\r\n    Text.writeString(out, url);\r\n    Text.writeString(out, user);\r\n    Text.writeString(out, name);\r\n    Text.writeString(out, queueName);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "readFields",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void readFields(DataInput in) throws IOException\n{\r\n    jobid.readFields(in);\r\n    this.jobFile = StringInterner.weakIntern(Text.readString(in));\r\n    this.url = StringInterner.weakIntern(Text.readString(in));\r\n    this.user = StringInterner.weakIntern(Text.readString(in));\r\n    this.name = StringInterner.weakIntern(Text.readString(in));\r\n    this.queueName = StringInterner.weakIntern(Text.readString(in));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\input",
  "methodName" : "initialize",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void initialize(InputSplit genericSplit, TaskAttemptContext context) throws IOException\n{\r\n    FileSplit split = (FileSplit) genericSplit;\r\n    Configuration job = context.getConfiguration();\r\n    final Path file = split.getPath();\r\n    initialize(job, split.getStart(), split.getLength(), file);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\input",
  "methodName" : "initialize",
  "errType" : null,
  "containingMethodsNum" : 10,
  "sourceCodeText" : "void initialize(Configuration job, long splitStart, long splitLength, Path file) throws IOException\n{\r\n    start = splitStart;\r\n    end = start + splitLength;\r\n    long partialRecordLength = start % recordLength;\r\n    long numBytesToSkip = 0;\r\n    if (partialRecordLength != 0) {\r\n        numBytesToSkip = recordLength - partialRecordLength;\r\n    }\r\n    final FutureDataInputStreamBuilder builder = file.getFileSystem(job).openFile(file);\r\n    FutureIOSupport.propagateOptions(builder, job, MRJobConfig.INPUT_FILE_OPTION_PREFIX, MRJobConfig.INPUT_FILE_MANDATORY_PREFIX);\r\n    fileIn = FutureIOSupport.awaitFuture(builder.build());\r\n    CompressionCodec codec = new CompressionCodecFactory(job).getCodec(file);\r\n    if (null != codec) {\r\n        isCompressedInput = true;\r\n        decompressor = CodecPool.getDecompressor(codec);\r\n        CompressionInputStream cIn = codec.createInputStream(fileIn, decompressor);\r\n        filePosition = cIn;\r\n        inputStream = cIn;\r\n        numRecordsRemainingInSplit = Long.MAX_VALUE;\r\n        LOG.info(\"Compressed input; cannot compute number of records in the split\");\r\n    } else {\r\n        fileIn.seek(start);\r\n        filePosition = fileIn;\r\n        inputStream = fileIn;\r\n        long splitSize = end - start - numBytesToSkip;\r\n        numRecordsRemainingInSplit = (splitSize + recordLength - 1) / recordLength;\r\n        if (numRecordsRemainingInSplit < 0) {\r\n            numRecordsRemainingInSplit = 0;\r\n        }\r\n        LOG.info(\"Expecting \" + numRecordsRemainingInSplit + \" records each with a length of \" + recordLength + \" bytes in the split with an effective size of \" + splitSize + \" bytes\");\r\n    }\r\n    if (numBytesToSkip != 0) {\r\n        start += inputStream.skip(numBytesToSkip);\r\n    }\r\n    this.pos = start;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\input",
  "methodName" : "nextKeyValue",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "boolean nextKeyValue() throws IOException\n{\r\n    if (key == null) {\r\n        key = new LongWritable();\r\n    }\r\n    if (value == null) {\r\n        value = new BytesWritable(new byte[recordLength]);\r\n    }\r\n    boolean dataRead = false;\r\n    value.setSize(recordLength);\r\n    byte[] record = value.getBytes();\r\n    if (numRecordsRemainingInSplit > 0) {\r\n        key.set(pos);\r\n        int offset = 0;\r\n        int numBytesToRead = recordLength;\r\n        int numBytesRead = 0;\r\n        while (numBytesToRead > 0) {\r\n            numBytesRead = inputStream.read(record, offset, numBytesToRead);\r\n            if (numBytesRead == -1) {\r\n                break;\r\n            }\r\n            offset += numBytesRead;\r\n            numBytesToRead -= numBytesRead;\r\n        }\r\n        numBytesRead = recordLength - numBytesToRead;\r\n        pos += numBytesRead;\r\n        if (numBytesRead > 0) {\r\n            dataRead = true;\r\n            if (numBytesRead >= recordLength) {\r\n                if (!isCompressedInput) {\r\n                    numRecordsRemainingInSplit--;\r\n                }\r\n            } else {\r\n                throw new IOException(\"Partial record(length = \" + numBytesRead + \") found at the end of split.\");\r\n            }\r\n        } else {\r\n            numRecordsRemainingInSplit = 0L;\r\n        }\r\n    }\r\n    return dataRead;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\input",
  "methodName" : "getCurrentKey",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "LongWritable getCurrentKey()\n{\r\n    return key;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\input",
  "methodName" : "getCurrentValue",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "BytesWritable getCurrentValue()\n{\r\n    return value;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\input",
  "methodName" : "getProgress",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "float getProgress() throws IOException\n{\r\n    if (start == end) {\r\n        return 0.0f;\r\n    } else {\r\n        return Math.min(1.0f, (getFilePosition() - start) / (float) (end - start));\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\input",
  "methodName" : "close",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void close() throws IOException\n{\r\n    try {\r\n        if (inputStream != null) {\r\n            inputStream.close();\r\n            inputStream = null;\r\n        }\r\n    } finally {\r\n        if (decompressor != null) {\r\n            CodecPool.returnDecompressor(decompressor);\r\n            decompressor = null;\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\input",
  "methodName" : "getPos",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getPos()\n{\r\n    return pos;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\input",
  "methodName" : "getFilePosition",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "long getFilePosition() throws IOException\n{\r\n    long retVal;\r\n    if (isCompressedInput && null != filePosition) {\r\n        retVal = filePosition.getPos();\r\n    } else {\r\n        retVal = pos;\r\n    }\r\n    return retVal;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "accept",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "boolean accept(Path path)\n{\r\n    return LOG_FILTER.accept(path);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "getTaskId",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String getTaskId()\n{\r\n    return taskid.toString();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "getTaskID",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "TaskID getTaskID()\n{\r\n    return taskid;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "getProgress",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "float getProgress()\n{\r\n    return progress;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "getState",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getState()\n{\r\n    return state;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "getDiagnostics",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String[] getDiagnostics()\n{\r\n    return diagnostics;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "getTaskCounters",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Counters getTaskCounters()\n{\r\n    return counters;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "getCurrentStatus",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "TIPStatus getCurrentStatus()\n{\r\n    return currentStatus;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "getFinishTime",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getFinishTime()\n{\r\n    return finishTime;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "setFinishTime",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setFinishTime(long finishTime)\n{\r\n    this.finishTime = finishTime;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "getStartTime",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getStartTime()\n{\r\n    return startTime;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "setStartTime",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setStartTime(long startTime)\n{\r\n    this.startTime = startTime;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "setSuccessfulAttemptId",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setSuccessfulAttemptId(TaskAttemptID t)\n{\r\n    successfulAttempt = t;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "getSuccessfulTaskAttemptId",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "TaskAttemptID getSuccessfulTaskAttemptId()\n{\r\n    return successfulAttempt;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "setRunningTaskAttemptIds",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setRunningTaskAttemptIds(Collection<TaskAttemptID> runningAttempts)\n{\r\n    this.runningAttempts = runningAttempts;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "getRunningTaskAttemptIds",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Collection<TaskAttemptID> getRunningTaskAttemptIds()\n{\r\n    return runningAttempts;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "equals",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "boolean equals(Object o)\n{\r\n    if (o == null)\r\n        return false;\r\n    if (o.getClass().equals(this.getClass())) {\r\n        TaskReport report = (TaskReport) o;\r\n        return counters.equals(report.getTaskCounters()) && Arrays.toString(this.diagnostics).equals(Arrays.toString(report.getDiagnostics())) && this.finishTime == report.getFinishTime() && this.progress == report.getProgress() && this.startTime == report.getStartTime() && this.state.equals(report.getState()) && this.taskid.equals(report.getTaskID());\r\n    }\r\n    return false;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "hashCode",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "int hashCode()\n{\r\n    return (counters.toString() + Arrays.toString(this.diagnostics) + this.finishTime + this.progress + this.startTime + this.state + this.taskid.toString()).hashCode();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "write",
  "errType" : null,
  "containingMethodsNum" : 12,
  "sourceCodeText" : "void write(DataOutput out) throws IOException\n{\r\n    taskid.write(out);\r\n    out.writeFloat(progress);\r\n    Text.writeString(out, state);\r\n    out.writeLong(startTime);\r\n    out.writeLong(finishTime);\r\n    WritableUtils.writeStringArray(out, diagnostics);\r\n    counters.write(out);\r\n    WritableUtils.writeEnum(out, currentStatus);\r\n    if (currentStatus == TIPStatus.RUNNING) {\r\n        WritableUtils.writeVInt(out, runningAttempts.size());\r\n        TaskAttemptID[] t = new TaskAttemptID[0];\r\n        t = runningAttempts.toArray(t);\r\n        for (int i = 0; i < t.length; i++) {\r\n            t[i].write(out);\r\n        }\r\n    } else if (currentStatus == TIPStatus.COMPLETE) {\r\n        successfulAttempt.write(out);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "readFields",
  "errType" : null,
  "containingMethodsNum" : 12,
  "sourceCodeText" : "void readFields(DataInput in) throws IOException\n{\r\n    this.taskid.readFields(in);\r\n    this.progress = in.readFloat();\r\n    this.state = StringInterner.weakIntern(Text.readString(in));\r\n    this.startTime = in.readLong();\r\n    this.finishTime = in.readLong();\r\n    diagnostics = WritableUtils.readStringArray(in);\r\n    counters = new Counters();\r\n    counters.readFields(in);\r\n    currentStatus = WritableUtils.readEnum(in, TIPStatus.class);\r\n    if (currentStatus == TIPStatus.RUNNING) {\r\n        int num = WritableUtils.readVInt(in);\r\n        for (int i = 0; i < num; i++) {\r\n            TaskAttemptID t = new TaskAttemptID();\r\n            t.readFields(in);\r\n            runningAttempts.add(t);\r\n        }\r\n    } else if (currentStatus == TIPStatus.COMPLETE) {\r\n        successfulAttempt.readFields(in);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred\\lib\\db",
  "methodName" : "configureDB",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void configureDB(JobConf job, String driverClass, String dbUrl, String userName, String passwd)\n{\r\n    job.set(DRIVER_CLASS_PROPERTY, driverClass);\r\n    job.set(URL_PROPERTY, dbUrl);\r\n    if (userName != null)\r\n        job.set(USERNAME_PROPERTY, userName);\r\n    if (passwd != null)\r\n        job.set(PASSWORD_PROPERTY, passwd);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred\\lib\\db",
  "methodName" : "configureDB",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void configureDB(JobConf job, String driverClass, String dbUrl)\n{\r\n    configureDB(job, driverClass, dbUrl, null, null);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\join",
  "methodName" : "hasNext",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "boolean hasNext()\n{\r\n    return infbuf != null && inbuf.available() > 0;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\join",
  "methodName" : "next",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "boolean next(X val) throws IOException\n{\r\n    if (hasNext()) {\r\n        inbuf.mark(0);\r\n        val.readFields(infbuf);\r\n        return true;\r\n    }\r\n    return false;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\join",
  "methodName" : "replay",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "boolean replay(X val) throws IOException\n{\r\n    inbuf.reset();\r\n    if (0 == inbuf.available())\r\n        return false;\r\n    val.readFields(infbuf);\r\n    return true;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\join",
  "methodName" : "reset",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void reset()\n{\r\n    if (null != outfbuf) {\r\n        inbuf = new ReplayableByteInputStream(outbuf.toByteArray());\r\n        infbuf = new DataInputStream(inbuf);\r\n        outfbuf = null;\r\n    }\r\n    inbuf.resetStream();\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\join",
  "methodName" : "add",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void add(X item) throws IOException\n{\r\n    item.write(outfbuf);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\join",
  "methodName" : "close",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void close() throws IOException\n{\r\n    if (null != infbuf)\r\n        infbuf.close();\r\n    if (null != outfbuf)\r\n        outfbuf.close();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\join",
  "methodName" : "clear",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void clear()\n{\r\n    if (null != inbuf)\r\n        inbuf.resetStream();\r\n    outbuf.reset();\r\n    outfbuf = new DataOutputStream(outbuf);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getRecordWriter",
  "errType" : null,
  "containingMethodsNum" : 11,
  "sourceCodeText" : "RecordWriter<WritableComparable, Writable> getRecordWriter(FileSystem ignored, JobConf job, String name, Progressable progress) throws IOException\n{\r\n    Path file = FileOutputFormat.getTaskOutputPath(job, name);\r\n    FileSystem fs = file.getFileSystem(job);\r\n    CompressionCodec codec = null;\r\n    CompressionType compressionType = CompressionType.NONE;\r\n    if (getCompressOutput(job)) {\r\n        compressionType = SequenceFileOutputFormat.getOutputCompressionType(job);\r\n        Class<? extends CompressionCodec> codecClass = getOutputCompressorClass(job, DefaultCodec.class);\r\n        codec = ReflectionUtils.newInstance(codecClass, job);\r\n    }\r\n    final MapFile.Writer out = new MapFile.Writer(job, fs, file.toString(), job.getOutputKeyClass().asSubclass(WritableComparable.class), job.getOutputValueClass().asSubclass(Writable.class), compressionType, codec, progress);\r\n    return new RecordWriter<WritableComparable, Writable>() {\r\n\r\n        public void write(WritableComparable key, Writable value) throws IOException {\r\n            out.append(key, value);\r\n        }\r\n\r\n        public void close(Reporter reporter) throws IOException {\r\n            out.close();\r\n        }\r\n    };\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getReaders",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "MapFile.Reader[] getReaders(FileSystem ignored, Path dir, Configuration conf) throws IOException\n{\r\n    return org.apache.hadoop.mapreduce.lib.output.MapFileOutputFormat.getReaders(dir, conf);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getEntry",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "Writable getEntry(MapFile.Reader[] readers, Partitioner<K, V> partitioner, K key, V value) throws IOException\n{\r\n    int readerLength = readers.length;\r\n    int part;\r\n    if (readerLength <= 1) {\r\n        part = 0;\r\n    } else {\r\n        part = partitioner.getPartition(key, value, readers.length);\r\n    }\r\n    return readers[part].get(key, value);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred\\lib",
  "methodName" : "getBaseRecordWriter",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "RecordWriter<K, V> getBaseRecordWriter(FileSystem fs, JobConf job, String name, Progressable arg3) throws IOException\n{\r\n    if (theSequenceFileOutputFormat == null) {\r\n        theSequenceFileOutputFormat = new SequenceFileOutputFormat<K, V>();\r\n    }\r\n    return theSequenceFileOutputFormat.getRecordWriter(fs, job, name, arg3);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\fieldsel",
  "methodName" : "setup",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void setup(Context context) throws IOException, InterruptedException\n{\r\n    Configuration conf = context.getConfiguration();\r\n    this.fieldSeparator = conf.get(FieldSelectionHelper.DATA_FIELD_SEPARATOR, \"\\t\");\r\n    this.reduceOutputKeyValueSpec = conf.get(FieldSelectionHelper.REDUCE_OUTPUT_KEY_VALUE_SPEC, \"0-:\");\r\n    allReduceValueFieldsFrom = FieldSelectionHelper.parseOutputKeyValueSpec(reduceOutputKeyValueSpec, reduceOutputKeyFieldList, reduceOutputValueFieldList);\r\n    LOG.info(FieldSelectionHelper.specToString(fieldSeparator, reduceOutputKeyValueSpec, allReduceValueFieldsFrom, reduceOutputKeyFieldList, reduceOutputValueFieldList));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\fieldsel",
  "methodName" : "reduce",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void reduce(Text key, Iterable<Text> values, Context context) throws IOException, InterruptedException\n{\r\n    String keyStr = key.toString() + this.fieldSeparator;\r\n    for (Text val : values) {\r\n        FieldSelectionHelper helper = new FieldSelectionHelper();\r\n        helper.extractOutputKeyValue(keyStr, val.toString(), fieldSeparator, reduceOutputKeyFieldList, reduceOutputValueFieldList, allReduceValueFieldsFrom, false, false);\r\n        context.write(helper.getKey(), helper.getValue());\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest",
  "methodName" : "toString",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String toString()\n{\r\n    return \"ManifestCommitterConfig{\" + \"name=\" + name + \", destinationDir=\" + destinationDir + \", role='\" + role + '\\'' + \", taskAttemptDir=\" + taskAttemptDir + \", createJobMarker=\" + createJobMarker + \", jobUniqueId='\" + jobUniqueId + '\\'' + \", jobUniqueIdSource='\" + jobUniqueIdSource + '\\'' + \", jobAttemptNumber=\" + jobAttemptNumber + \", jobAttemptId='\" + jobAttemptId + '\\'' + \", taskId='\" + taskId + '\\'' + \", taskAttemptId='\" + taskAttemptId + '\\'' + '}';\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest",
  "methodName" : "getDestinationFileSystem",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "FileSystem getDestinationFileSystem() throws IOException\n{\r\n    return FileSystem.get(destinationDir.toUri(), conf);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest",
  "methodName" : "createStageConfig",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "StageConfig createStageConfig()\n{\r\n    StageConfig stageConfig = new StageConfig();\r\n    stageConfig.withIOStatistics(iostatistics).withJobAttemptNumber(jobAttemptNumber).withJobDirectories(dirs).withJobId(jobUniqueId).withJobIdSource(jobUniqueIdSource).withName(name).withProgressable(progressable).withStageEventCallbacks(stageEventCallbacks).withTaskAttemptDir(taskAttemptDir).withTaskAttemptId(taskAttemptId).withTaskId(taskId).withDeleteTargetPaths(deleteTargetPaths);\r\n    return stageConfig;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest",
  "methodName" : "getDestinationDir",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Path getDestinationDir()\n{\r\n    return destinationDir;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest",
  "methodName" : "getRole",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getRole()\n{\r\n    return role;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest",
  "methodName" : "getTaskAttemptDir",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Path getTaskAttemptDir()\n{\r\n    return taskAttemptDir;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest",
  "methodName" : "getJobAttemptDir",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Path getJobAttemptDir()\n{\r\n    return dirs.getJobAttemptDir();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest",
  "methodName" : "getTaskManifestDir",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Path getTaskManifestDir()\n{\r\n    return dirs.getTaskManifestDir();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest",
  "methodName" : "getConf",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Configuration getConf()\n{\r\n    return conf;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest",
  "methodName" : "getJobContext",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "JobContext getJobContext()\n{\r\n    return jobContext;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest",
  "methodName" : "getCreateJobMarker",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean getCreateJobMarker()\n{\r\n    return createJobMarker;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest",
  "methodName" : "getJobAttemptId",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getJobAttemptId()\n{\r\n    return jobAttemptId;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest",
  "methodName" : "getTaskAttemptId",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getTaskAttemptId()\n{\r\n    return taskAttemptId;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest",
  "methodName" : "getTaskId",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getTaskId()\n{\r\n    return taskId;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest",
  "methodName" : "getJobUniqueId",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getJobUniqueId()\n{\r\n    return jobUniqueId;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest",
  "methodName" : "getValidateOutput",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean getValidateOutput()\n{\r\n    return validateOutput;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest",
  "methodName" : "getName",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getName()\n{\r\n    return name;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest",
  "methodName" : "getIOStatistics",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "IOStatisticsStore getIOStatistics()\n{\r\n    return iostatistics;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest",
  "methodName" : "createSubmitter",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "CloseableTaskPoolSubmitter createSubmitter()\n{\r\n    return createSubmitter(OPT_IO_PROCESSORS, OPT_IO_PROCESSORS_DEFAULT);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest",
  "methodName" : "createSubmitter",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "CloseableTaskPoolSubmitter createSubmitter(String key, int defVal)\n{\r\n    int numThreads = conf.getInt(key, defVal);\r\n    if (numThreads <= 0) {\r\n        numThreads = defVal;\r\n    }\r\n    return createCloseableTaskSubmitter(numThreads, getJobAttemptId());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest",
  "methodName" : "createCloseableTaskSubmitter",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "CloseableTaskPoolSubmitter createCloseableTaskSubmitter(final int numThreads, final String jobAttemptId)\n{\r\n    return new CloseableTaskPoolSubmitter(HadoopExecutors.newFixedThreadPool(numThreads, new ThreadFactoryBuilder().setDaemon(true).setNameFormat(\"manifest-committer-\" + jobAttemptId + \"-%d\").build()));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\aggregate",
  "methodName" : "createValueAggregatorJobs",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "JobControl createValueAggregatorJobs(String[] args, Class<? extends ValueAggregatorDescriptor>[] descriptors) throws IOException\n{\r\n    JobControl theControl = new JobControl(\"ValueAggregatorJobs\");\r\n    ArrayList<ControlledJob> dependingJobs = new ArrayList<ControlledJob>();\r\n    Configuration conf = new Configuration();\r\n    if (descriptors != null) {\r\n        conf = setAggregatorDescriptors(descriptors);\r\n    }\r\n    Job job = createValueAggregatorJob(conf, args);\r\n    ControlledJob cjob = new ControlledJob(job, dependingJobs);\r\n    theControl.addJob(cjob);\r\n    return theControl;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\aggregate",
  "methodName" : "createValueAggregatorJobs",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "JobControl createValueAggregatorJobs(String[] args) throws IOException\n{\r\n    return createValueAggregatorJobs(args, null);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\aggregate",
  "methodName" : "createValueAggregatorJob",
  "errType" : null,
  "containingMethodsNum" : 24,
  "sourceCodeText" : "Job createValueAggregatorJob(Configuration conf, String[] args) throws IOException\n{\r\n    GenericOptionsParser genericParser = new GenericOptionsParser(conf, args);\r\n    args = genericParser.getRemainingArgs();\r\n    if (args.length < 2) {\r\n        System.out.println(\"usage: inputDirs outDir \" + \"[numOfReducer [textinputformat|seq [specfile [jobName]]]]\");\r\n        GenericOptionsParser.printGenericCommandUsage(System.out);\r\n        System.exit(2);\r\n    }\r\n    String inputDir = args[0];\r\n    String outputDir = args[1];\r\n    int numOfReducers = 1;\r\n    if (args.length > 2) {\r\n        numOfReducers = Integer.parseInt(args[2]);\r\n    }\r\n    Class<? extends InputFormat> theInputFormat = null;\r\n    if (args.length > 3 && args[3].compareToIgnoreCase(\"textinputformat\") == 0) {\r\n        theInputFormat = TextInputFormat.class;\r\n    } else {\r\n        theInputFormat = SequenceFileInputFormat.class;\r\n    }\r\n    Path specFile = null;\r\n    if (args.length > 4) {\r\n        specFile = new Path(args[4]);\r\n    }\r\n    String jobName = \"\";\r\n    if (args.length > 5) {\r\n        jobName = args[5];\r\n    }\r\n    if (specFile != null) {\r\n        conf.addResource(specFile);\r\n    }\r\n    String userJarFile = conf.get(ValueAggregatorJobBase.USER_JAR);\r\n    if (userJarFile != null) {\r\n        conf.set(MRJobConfig.JAR, userJarFile);\r\n    }\r\n    Job theJob = Job.getInstance(conf);\r\n    if (userJarFile == null) {\r\n        theJob.setJarByClass(ValueAggregator.class);\r\n    }\r\n    theJob.setJobName(\"ValueAggregatorJob: \" + jobName);\r\n    FileInputFormat.addInputPaths(theJob, inputDir);\r\n    theJob.setInputFormatClass(theInputFormat);\r\n    theJob.setMapperClass(ValueAggregatorMapper.class);\r\n    FileOutputFormat.setOutputPath(theJob, new Path(outputDir));\r\n    theJob.setOutputFormatClass(TextOutputFormat.class);\r\n    theJob.setMapOutputKeyClass(Text.class);\r\n    theJob.setMapOutputValueClass(Text.class);\r\n    theJob.setOutputKeyClass(Text.class);\r\n    theJob.setOutputValueClass(Text.class);\r\n    theJob.setReducerClass(ValueAggregatorReducer.class);\r\n    theJob.setCombinerClass(ValueAggregatorCombiner.class);\r\n    theJob.setNumReduceTasks(numOfReducers);\r\n    return theJob;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\aggregate",
  "methodName" : "createValueAggregatorJob",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Job createValueAggregatorJob(String[] args, Class<? extends ValueAggregatorDescriptor>[] descriptors) throws IOException\n{\r\n    return createValueAggregatorJob(setAggregatorDescriptors(descriptors), args);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\aggregate",
  "methodName" : "setAggregatorDescriptors",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "Configuration setAggregatorDescriptors(Class<? extends ValueAggregatorDescriptor>[] descriptors)\n{\r\n    Configuration conf = new Configuration();\r\n    conf.setInt(ValueAggregatorJobBase.DESCRIPTOR_NUM, descriptors.length);\r\n    for (int i = 0; i < descriptors.length; i++) {\r\n        conf.set(ValueAggregatorJobBase.DESCRIPTOR + i, \"UserDefined,\" + descriptors[i].getName());\r\n    }\r\n    return conf;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\aggregate",
  "methodName" : "main",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void main(String[] args) throws IOException, InterruptedException, ClassNotFoundException\n{\r\n    Job job = ValueAggregatorJob.createValueAggregatorJob(new Configuration(), args);\r\n    int ret = job.waitForCompletion(true) ? 0 : 1;\r\n    System.exit(ret);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\counters",
  "methodName" : "getName",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String getName()\n{\r\n    return enumClass.getName();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\counters",
  "methodName" : "getDisplayName",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String getDisplayName()\n{\r\n    if (displayName == null) {\r\n        displayName = ResourceBundles.getCounterGroupName(getName(), getName());\r\n    }\r\n    return displayName;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\counters",
  "methodName" : "setDisplayName",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setDisplayName(String displayName)\n{\r\n    this.displayName = displayName;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\counters",
  "methodName" : "valueOf",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "T valueOf(String name)\n{\r\n    return Enum.valueOf(enumClass, name);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\counters",
  "methodName" : "addCounter",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void addCounter(C counter)\n{\r\n    C ours = findCounter(counter.getName());\r\n    if (ours != null) {\r\n        ours.setValue(counter.getValue());\r\n    } else {\r\n        LOG.warn(counter.getName() + \"is not a known counter.\");\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\counters",
  "methodName" : "addCounter",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "C addCounter(String name, String displayName, long value)\n{\r\n    C counter = findCounter(name);\r\n    if (counter != null) {\r\n        counter.setValue(value);\r\n    } else {\r\n        LOG.warn(name + \"is not a known counter.\");\r\n    }\r\n    return counter;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\counters",
  "methodName" : "findCounter",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "C findCounter(String counterName, String displayName)\n{\r\n    return findCounter(counterName);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\counters",
  "methodName" : "findCounter",
  "errType" : [ "Exception" ],
  "containingMethodsNum" : 1,
  "sourceCodeText" : "C findCounter(String counterName, boolean create)\n{\r\n    try {\r\n        return findCounter(valueOf(counterName));\r\n    } catch (Exception e) {\r\n        if (create)\r\n            throw new IllegalArgumentException(e);\r\n        return null;\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\counters",
  "methodName" : "findCounter",
  "errType" : [ "IllegalArgumentException" ],
  "containingMethodsNum" : 3,
  "sourceCodeText" : "C findCounter(String counterName)\n{\r\n    try {\r\n        T enumValue = valueOf(counterName);\r\n        return findCounter(enumValue);\r\n    } catch (IllegalArgumentException e) {\r\n        LOG.warn(counterName + \" is not a recognized counter.\");\r\n        return null;\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\counters",
  "methodName" : "findCounter",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "C findCounter(T key)\n{\r\n    int i = key.ordinal();\r\n    if (counters[i] == null) {\r\n        counters[i] = newCounter(key);\r\n    }\r\n    return (C) counters[i];\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\counters",
  "methodName" : "newCounter",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "C newCounter(T key)",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\counters",
  "methodName" : "size",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int size()\n{\r\n    int n = 0;\r\n    for (int i = 0; i < counters.length; ++i) {\r\n        if (counters[i] != null)\r\n            ++n;\r\n    }\r\n    return n;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\counters",
  "methodName" : "incrAllCounters",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void incrAllCounters(CounterGroupBase<C> other)\n{\r\n    if (checkNotNull(other, \"other counter group\") instanceof FrameworkCounterGroup<?, ?>) {\r\n        for (Counter counter : other) {\r\n            C c = findCounter(((FrameworkCounter) counter).key.name());\r\n            if (c != null) {\r\n                c.increment(counter.getValue());\r\n            }\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\counters",
  "methodName" : "write",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void write(DataOutput out) throws IOException\n{\r\n    WritableUtils.writeVInt(out, size());\r\n    for (int i = 0; i < counters.length; ++i) {\r\n        Counter counter = (C) counters[i];\r\n        if (counter != null) {\r\n            WritableUtils.writeVInt(out, i);\r\n            WritableUtils.writeVLong(out, counter.getValue());\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\counters",
  "methodName" : "readFields",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void readFields(DataInput in) throws IOException\n{\r\n    clear();\r\n    int len = WritableUtils.readVInt(in);\r\n    T[] enums = enumClass.getEnumConstants();\r\n    for (int i = 0; i < len; ++i) {\r\n        int ord = WritableUtils.readVInt(in);\r\n        Counter counter = newCounter(enums[ord]);\r\n        counter.setValue(WritableUtils.readVLong(in));\r\n        counters[ord] = counter;\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\counters",
  "methodName" : "clear",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void clear()\n{\r\n    for (int i = 0; i < counters.length; ++i) {\r\n        counters[i] = null;\r\n    }\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\counters",
  "methodName" : "iterator",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Iterator<C> iterator()\n{\r\n    return new AbstractIterator<C>() {\r\n\r\n        int i = 0;\r\n\r\n        @Override\r\n        protected C computeNext() {\r\n            while (i < counters.length) {\r\n                @SuppressWarnings(\"unchecked\")\r\n                C counter = (C) counters[i++];\r\n                if (counter != null)\r\n                    return counter;\r\n            }\r\n            return endOfData();\r\n        }\r\n    };\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\counters",
  "methodName" : "equals",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "boolean equals(Object genericRight)\n{\r\n    if (genericRight instanceof CounterGroupBase<?>) {\r\n        @SuppressWarnings(\"unchecked\")\r\n        CounterGroupBase<C> right = (CounterGroupBase<C>) genericRight;\r\n        return Iterators.elementsEqual(iterator(), right.iterator());\r\n    }\r\n    return false;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\counters",
  "methodName" : "hashCode",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "int hashCode()\n{\r\n    return Arrays.deepHashCode(new Object[] { enumClass, counters, displayName });\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\input",
  "methodName" : "fillBuffer",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "int fillBuffer(InputStream in, byte[] buffer, boolean inDelimiter) throws IOException\n{\r\n    int maxBytesToRead = buffer.length;\r\n    if (totalBytesRead < splitLength) {\r\n        long bytesLeftInSplit = splitLength - totalBytesRead;\r\n        if (bytesLeftInSplit < maxBytesToRead) {\r\n            maxBytesToRead = (int) bytesLeftInSplit;\r\n        }\r\n    }\r\n    int bytesRead = in.read(buffer, 0, maxBytesToRead);\r\n    if (totalBytesRead == splitLength && inDelimiter && bytesRead > 0) {\r\n        if (usingCRLF) {\r\n            needAdditionalRecord = (buffer[0] != '\\n');\r\n        } else {\r\n            needAdditionalRecord = true;\r\n        }\r\n    }\r\n    if (bytesRead > 0) {\r\n        totalBytesRead += bytesRead;\r\n    }\r\n    return bytesRead;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\input",
  "methodName" : "readLine",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "int readLine(Text str, int maxLineLength, int maxBytesToConsume) throws IOException\n{\r\n    int bytesRead = 0;\r\n    if (!finished) {\r\n        if (totalBytesRead > splitLength) {\r\n            finished = true;\r\n        }\r\n        bytesRead = super.readLine(str, maxLineLength, maxBytesToConsume);\r\n    }\r\n    return bytesRead;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\input",
  "methodName" : "needAdditionalRecordAfterSplit",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean needAdditionalRecordAfterSplit()\n{\r\n    return !finished && needAdditionalRecord;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\input",
  "methodName" : "unsetNeedAdditionalRecordAfterSplit",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void unsetNeedAdditionalRecordAfterSplit()\n{\r\n    needAdditionalRecord = false;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "isMapTask",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean isMapTask()\n{\r\n    return true;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "localizeConfiguration",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void localizeConfiguration(JobConf conf) throws IOException\n{\r\n    super.localizeConfiguration(conf);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "write",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void write(DataOutput out) throws IOException\n{\r\n    super.write(out);\r\n    if (isMapOrReduce()) {\r\n        splitMetaInfo.write(out);\r\n        splitMetaInfo = null;\r\n    }\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "readFields",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void readFields(DataInput in) throws IOException\n{\r\n    super.readFields(in);\r\n    if (isMapOrReduce()) {\r\n        splitMetaInfo.readFields(in);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "run",
  "errType" : null,
  "containingMethodsNum" : 14,
  "sourceCodeText" : "void run(final JobConf job, final TaskUmbilicalProtocol umbilical) throws IOException, ClassNotFoundException, InterruptedException\n{\r\n    this.umbilical = umbilical;\r\n    if (isMapTask()) {\r\n        if (conf.getNumReduceTasks() == 0) {\r\n            mapPhase = getProgress().addPhase(\"map\", 1.0f);\r\n        } else {\r\n            mapPhase = getProgress().addPhase(\"map\", 0.667f);\r\n            sortPhase = getProgress().addPhase(\"sort\", 0.333f);\r\n        }\r\n    }\r\n    TaskReporter reporter = startReporter(umbilical);\r\n    boolean useNewApi = job.getUseNewMapper();\r\n    initialize(job, getJobID(), reporter, useNewApi);\r\n    if (jobCleanup) {\r\n        runJobCleanupTask(umbilical, reporter);\r\n        return;\r\n    }\r\n    if (jobSetup) {\r\n        runJobSetupTask(umbilical, reporter);\r\n        return;\r\n    }\r\n    if (taskCleanup) {\r\n        runTaskCleanupTask(umbilical, reporter);\r\n        return;\r\n    }\r\n    if (useNewApi) {\r\n        runNewMapper(job, splitMetaInfo, umbilical, reporter);\r\n    } else {\r\n        runOldMapper(job, splitMetaInfo, umbilical, reporter);\r\n    }\r\n    done(umbilical, reporter);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getSortPhase",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Progress getSortPhase()\n{\r\n    return sortPhase;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getSplitDetails",
  "errType" : [ "ClassNotFoundException" ],
  "containingMethodsNum" : 12,
  "sourceCodeText" : "T getSplitDetails(Path file, long offset) throws IOException\n{\r\n    FileSystem fs = file.getFileSystem(conf);\r\n    FSDataInputStream inFile = fs.open(file);\r\n    inFile.seek(offset);\r\n    String className = StringInterner.weakIntern(Text.readString(inFile));\r\n    Class<T> cls;\r\n    try {\r\n        cls = (Class<T>) conf.getClassByName(className);\r\n    } catch (ClassNotFoundException ce) {\r\n        IOException wrap = new IOException(\"Split class \" + className + \" not found\");\r\n        wrap.initCause(ce);\r\n        throw wrap;\r\n    }\r\n    SerializationFactory factory = new SerializationFactory(conf);\r\n    Deserializer<T> deserializer = (Deserializer<T>) factory.getDeserializer(cls);\r\n    deserializer.open(inFile);\r\n    T split = deserializer.deserialize(null);\r\n    long pos = inFile.getPos();\r\n    getCounters().findCounter(TaskCounter.SPLIT_RAW_BYTES).increment(pos - offset);\r\n    inFile.close();\r\n    return split;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "createSortingCollector",
  "errType" : [ "Exception" ],
  "containingMethodsNum" : 11,
  "sourceCodeText" : "MapOutputCollector<KEY, VALUE> createSortingCollector(JobConf job, TaskReporter reporter) throws IOException, ClassNotFoundException\n{\r\n    MapOutputCollector.Context context = new MapOutputCollector.Context(this, job, reporter);\r\n    Class<?>[] collectorClasses = job.getClasses(JobContext.MAP_OUTPUT_COLLECTOR_CLASS_ATTR, MapOutputBuffer.class);\r\n    int remainingCollectors = collectorClasses.length;\r\n    Exception lastException = null;\r\n    for (Class clazz : collectorClasses) {\r\n        try {\r\n            if (!MapOutputCollector.class.isAssignableFrom(clazz)) {\r\n                throw new IOException(\"Invalid output collector class: \" + clazz.getName() + \" (does not implement MapOutputCollector)\");\r\n            }\r\n            Class<? extends MapOutputCollector> subclazz = clazz.asSubclass(MapOutputCollector.class);\r\n            LOG.debug(\"Trying map output collector class: \" + subclazz.getName());\r\n            MapOutputCollector<KEY, VALUE> collector = ReflectionUtils.newInstance(subclazz, job);\r\n            collector.init(context);\r\n            LOG.info(\"Map output collector class = \" + collector.getClass().getName());\r\n            return collector;\r\n        } catch (Exception e) {\r\n            String msg = \"Unable to initialize MapOutputCollector \" + clazz.getName();\r\n            if (--remainingCollectors > 0) {\r\n                msg += \" (\" + remainingCollectors + \" more collector(s) to try)\";\r\n            }\r\n            lastException = e;\r\n            LOG.warn(msg, e);\r\n        }\r\n    }\r\n    if (lastException != null) {\r\n        throw new IOException(\"Initialization of all the collectors failed. \" + \"Error in last collector was:\" + lastException.toString(), lastException);\r\n    } else {\r\n        throw new IOException(\"Initialization of all the collectors failed.\");\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "runOldMapper",
  "errType" : null,
  "containingMethodsNum" : 19,
  "sourceCodeText" : "void runOldMapper(final JobConf job, final TaskSplitIndex splitIndex, final TaskUmbilicalProtocol umbilical, TaskReporter reporter) throws IOException, InterruptedException, ClassNotFoundException\n{\r\n    InputSplit inputSplit = getSplitDetails(new Path(splitIndex.getSplitLocation()), splitIndex.getStartOffset());\r\n    updateJobWithSplit(job, inputSplit);\r\n    reporter.setInputSplit(inputSplit);\r\n    RecordReader<INKEY, INVALUE> in = isSkipping() ? new SkippingRecordReader<INKEY, INVALUE>(umbilical, reporter, job) : new TrackedRecordReader<INKEY, INVALUE>(reporter, job);\r\n    job.setBoolean(JobContext.SKIP_RECORDS, isSkipping());\r\n    int numReduceTasks = conf.getNumReduceTasks();\r\n    LOG.info(\"numReduceTasks: \" + numReduceTasks);\r\n    MapOutputCollector<OUTKEY, OUTVALUE> collector = null;\r\n    if (numReduceTasks > 0) {\r\n        collector = createSortingCollector(job, reporter);\r\n    } else {\r\n        collector = new DirectMapOutputCollector<OUTKEY, OUTVALUE>();\r\n        MapOutputCollector.Context context = new MapOutputCollector.Context(this, job, reporter);\r\n        collector.init(context);\r\n    }\r\n    MapRunnable<INKEY, INVALUE, OUTKEY, OUTVALUE> runner = ReflectionUtils.newInstance(job.getMapRunnerClass(), job);\r\n    try {\r\n        runner.run(in, new OldOutputCollector(collector, conf), reporter);\r\n        mapPhase.complete();\r\n        if (numReduceTasks > 0) {\r\n            setPhase(TaskStatus.Phase.SORT);\r\n        }\r\n        statusUpdate(umbilical);\r\n        collector.flush();\r\n        in.close();\r\n        in = null;\r\n        collector.close();\r\n        collector = null;\r\n    } finally {\r\n        closeQuietly(in);\r\n        closeQuietly(collector);\r\n    }\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "updateJobWithSplit",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void updateJobWithSplit(final JobConf job, InputSplit inputSplit)\n{\r\n    if (inputSplit instanceof FileSplit) {\r\n        FileSplit fileSplit = (FileSplit) inputSplit;\r\n        job.set(JobContext.MAP_INPUT_FILE, fileSplit.getPath().toString());\r\n        job.setLong(JobContext.MAP_INPUT_START, fileSplit.getStart());\r\n        job.setLong(JobContext.MAP_INPUT_PATH, fileSplit.getLength());\r\n    }\r\n    LOG.info(\"Processing split: \" + inputSplit);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "runNewMapper",
  "errType" : null,
  "containingMethodsNum" : 18,
  "sourceCodeText" : "void runNewMapper(final JobConf job, final TaskSplitIndex splitIndex, final TaskUmbilicalProtocol umbilical, TaskReporter reporter) throws IOException, ClassNotFoundException, InterruptedException\n{\r\n    org.apache.hadoop.mapreduce.TaskAttemptContext taskContext = new org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl(job, getTaskID(), reporter);\r\n    org.apache.hadoop.mapreduce.Mapper<INKEY, INVALUE, OUTKEY, OUTVALUE> mapper = (org.apache.hadoop.mapreduce.Mapper<INKEY, INVALUE, OUTKEY, OUTVALUE>) ReflectionUtils.newInstance(taskContext.getMapperClass(), job);\r\n    org.apache.hadoop.mapreduce.InputFormat<INKEY, INVALUE> inputFormat = (org.apache.hadoop.mapreduce.InputFormat<INKEY, INVALUE>) ReflectionUtils.newInstance(taskContext.getInputFormatClass(), job);\r\n    org.apache.hadoop.mapreduce.InputSplit split = null;\r\n    split = getSplitDetails(new Path(splitIndex.getSplitLocation()), splitIndex.getStartOffset());\r\n    LOG.info(\"Processing split: \" + split);\r\n    org.apache.hadoop.mapreduce.RecordReader<INKEY, INVALUE> input = new NewTrackingRecordReader<INKEY, INVALUE>(split, inputFormat, reporter, taskContext);\r\n    job.setBoolean(JobContext.SKIP_RECORDS, isSkipping());\r\n    org.apache.hadoop.mapreduce.RecordWriter output = null;\r\n    if (job.getNumReduceTasks() == 0) {\r\n        output = new NewDirectOutputCollector(taskContext, job, umbilical, reporter);\r\n    } else {\r\n        output = new NewOutputCollector(taskContext, job, umbilical, reporter);\r\n    }\r\n    org.apache.hadoop.mapreduce.MapContext<INKEY, INVALUE, OUTKEY, OUTVALUE> mapContext = new MapContextImpl<INKEY, INVALUE, OUTKEY, OUTVALUE>(job, getTaskID(), input, output, committer, reporter, split);\r\n    org.apache.hadoop.mapreduce.Mapper<INKEY, INVALUE, OUTKEY, OUTVALUE>.Context mapperContext = new WrappedMapper<INKEY, INVALUE, OUTKEY, OUTVALUE>().getMapContext(mapContext);\r\n    try {\r\n        input.initialize(split, mapperContext);\r\n        mapper.run(mapperContext);\r\n        mapPhase.complete();\r\n        setPhase(TaskStatus.Phase.SORT);\r\n        statusUpdate(umbilical);\r\n        input.close();\r\n        input = null;\r\n        output.close(mapperContext);\r\n        output = null;\r\n    } finally {\r\n        closeQuietly(input);\r\n        closeQuietly(output, mapperContext);\r\n    }\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "closeQuietly",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void closeQuietly(RecordReader<INKEY, INVALUE> c)\n{\r\n    if (c != null) {\r\n        try {\r\n            c.close();\r\n        } catch (IOException ie) {\r\n            LOG.info(\"Ignoring exception during close for \" + c, ie);\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "closeQuietly",
  "errType" : [ "Exception" ],
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void closeQuietly(MapOutputCollector<OUTKEY, OUTVALUE> c)\n{\r\n    if (c != null) {\r\n        try {\r\n            c.close();\r\n        } catch (Exception ie) {\r\n            LOG.info(\"Ignoring exception during close for \" + c, ie);\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "closeQuietly",
  "errType" : [ "Exception" ],
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void closeQuietly(org.apache.hadoop.mapreduce.RecordReader<INKEY, INVALUE> c)\n{\r\n    if (c != null) {\r\n        try {\r\n            c.close();\r\n        } catch (Exception ie) {\r\n            LOG.info(\"Ignoring exception during close for \" + c, ie);\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "closeQuietly",
  "errType" : [ "Exception" ],
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void closeQuietly(org.apache.hadoop.mapreduce.RecordWriter<OUTKEY, OUTVALUE> c, org.apache.hadoop.mapreduce.Mapper<INKEY, INVALUE, OUTKEY, OUTVALUE>.Context mapperContext)\n{\r\n    if (c != null) {\r\n        try {\r\n            c.close(mapperContext);\r\n        } catch (Exception ie) {\r\n            LOG.info(\"Ignoring exception during close for \" + c, ie);\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\reduce",
  "methodName" : "reduce",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void reduce(KEY key, Iterable<LongWritable> values, Context context) throws IOException, InterruptedException\n{\r\n    long sum = 0;\r\n    for (LongWritable val : values) {\r\n        sum += val.get();\r\n    }\r\n    result.set(sum);\r\n    context.write(key, result);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\input",
  "methodName" : "createRecordReader",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "RecordReader<K, V> createRecordReader(InputSplit split, TaskAttemptContext context) throws IOException\n{\r\n    context.setStatus(split.toString());\r\n    return new FilterRecordReader<K, V>(context.getConfiguration());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\input",
  "methodName" : "setFilterClass",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setFilterClass(Job job, Class<?> filterClass)\n{\r\n    job.getConfiguration().set(FILTER_CLASS, filterClass.getName());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\task\\reduce",
  "methodName" : "createInMemoryMerger",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "MergeThread<InMemoryMapOutput<K, V>, K, V> createInMemoryMerger()\n{\r\n    return new InMemoryMerger(this);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\task\\reduce",
  "methodName" : "createOnDiskMerger",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "MergeThread<CompressAwarePath, K, V> createOnDiskMerger()\n{\r\n    return new OnDiskMerger(this);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\task\\reduce",
  "methodName" : "getReduceId",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "TaskAttemptID getReduceId()\n{\r\n    return reduceId;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\task\\reduce",
  "methodName" : "getExceptionReporter",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "ExceptionReporter getExceptionReporter()\n{\r\n    return exceptionReporter;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\task\\reduce",
  "methodName" : "waitForResource",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void waitForResource() throws InterruptedException\n{\r\n    inMemoryMerger.waitForMerge();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\task\\reduce",
  "methodName" : "reserve",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "MapOutput<K, V> reserve(TaskAttemptID mapId, long requestedSize, int fetcher) throws IOException\n{\r\n    if (requestedSize > maxSingleShuffleLimit) {\r\n        LOG.info(mapId + \": Shuffling to disk since \" + requestedSize + \" is greater than maxSingleShuffleLimit (\" + maxSingleShuffleLimit + \")\");\r\n        return new OnDiskMapOutput<K, V>(mapId, this, requestedSize, jobConf, fetcher, true, FileSystem.getLocal(jobConf).getRaw(), mapOutputFile.getInputFileForWrite(mapId.getTaskID(), requestedSize));\r\n    }\r\n    if (usedMemory > memoryLimit) {\r\n        LOG.debug(mapId + \": Stalling shuffle since usedMemory (\" + usedMemory + \") is greater than memoryLimit (\" + memoryLimit + \").\" + \" CommitMemory is (\" + commitMemory + \")\");\r\n        return null;\r\n    }\r\n    LOG.debug(mapId + \": Proceeding with shuffle since usedMemory (\" + usedMemory + \") is lesser than memoryLimit (\" + memoryLimit + \").\" + \"CommitMemory is (\" + commitMemory + \")\");\r\n    return unconditionalReserve(mapId, requestedSize, true);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\task\\reduce",
  "methodName" : "unconditionalReserve",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "InMemoryMapOutput<K, V> unconditionalReserve(TaskAttemptID mapId, long requestedSize, boolean primaryMapOutput)\n{\r\n    usedMemory += requestedSize;\r\n    return new InMemoryMapOutput<K, V>(jobConf, mapId, this, (int) requestedSize, codec, primaryMapOutput);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\task\\reduce",
  "methodName" : "unreserve",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void unreserve(long size)\n{\r\n    usedMemory -= size;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\task\\reduce",
  "methodName" : "closeInMemoryFile",
  "errType" : null,
  "containingMethodsNum" : 9,
  "sourceCodeText" : "void closeInMemoryFile(InMemoryMapOutput<K, V> mapOutput)\n{\r\n    inMemoryMapOutputs.add(mapOutput);\r\n    LOG.info(\"closeInMemoryFile -> map-output of size: \" + mapOutput.getSize() + \", inMemoryMapOutputs.size() -> \" + inMemoryMapOutputs.size() + \", commitMemory -> \" + commitMemory + \", usedMemory ->\" + usedMemory);\r\n    commitMemory += mapOutput.getSize();\r\n    if (commitMemory >= mergeThreshold) {\r\n        LOG.info(\"Starting inMemoryMerger's merge since commitMemory=\" + commitMemory + \" > mergeThreshold=\" + mergeThreshold + \". Current usedMemory=\" + usedMemory);\r\n        inMemoryMapOutputs.addAll(inMemoryMergedMapOutputs);\r\n        inMemoryMergedMapOutputs.clear();\r\n        inMemoryMerger.startMerge(inMemoryMapOutputs);\r\n        commitMemory = 0L;\r\n    }\r\n    if (memToMemMerger != null) {\r\n        if (inMemoryMapOutputs.size() >= memToMemMergeOutputsThreshold) {\r\n            memToMemMerger.startMerge(inMemoryMapOutputs);\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\task\\reduce",
  "methodName" : "closeInMemoryMergedFile",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void closeInMemoryMergedFile(InMemoryMapOutput<K, V> mapOutput)\n{\r\n    inMemoryMergedMapOutputs.add(mapOutput);\r\n    LOG.info(\"closeInMemoryMergedFile -> size: \" + mapOutput.getSize() + \", inMemoryMergedMapOutputs.size() -> \" + inMemoryMergedMapOutputs.size());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\task\\reduce",
  "methodName" : "closeOnDiskFile",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void closeOnDiskFile(CompressAwarePath file)\n{\r\n    onDiskMapOutputs.add(file);\r\n    if (onDiskMapOutputs.size() >= (2 * ioSortFactor - 1)) {\r\n        onDiskMerger.startMerge(onDiskMapOutputs);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\task\\reduce",
  "methodName" : "close",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "RawKeyValueIterator close() throws Throwable\n{\r\n    if (memToMemMerger != null) {\r\n        memToMemMerger.close();\r\n    }\r\n    inMemoryMerger.close();\r\n    onDiskMerger.close();\r\n    List<InMemoryMapOutput<K, V>> memory = new ArrayList<InMemoryMapOutput<K, V>>(inMemoryMergedMapOutputs);\r\n    inMemoryMergedMapOutputs.clear();\r\n    memory.addAll(inMemoryMapOutputs);\r\n    inMemoryMapOutputs.clear();\r\n    List<CompressAwarePath> disk = new ArrayList<CompressAwarePath>(onDiskMapOutputs);\r\n    onDiskMapOutputs.clear();\r\n    return finalMerge(jobConf, rfs, memory, disk);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\task\\reduce",
  "methodName" : "combineAndSpill",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void combineAndSpill(RawKeyValueIterator kvIter, Counters.Counter inCounter) throws IOException\n{\r\n    JobConf job = jobConf;\r\n    Reducer combiner = ReflectionUtils.newInstance(combinerClass, job);\r\n    Class<K> keyClass = (Class<K>) job.getMapOutputKeyClass();\r\n    Class<V> valClass = (Class<V>) job.getMapOutputValueClass();\r\n    RawComparator<K> comparator = (RawComparator<K>) job.getCombinerKeyGroupingComparator();\r\n    try {\r\n        CombineValuesIterator values = new CombineValuesIterator(kvIter, comparator, keyClass, valClass, job, Reporter.NULL, inCounter);\r\n        while (values.more()) {\r\n            combiner.reduce(values.getKey(), values, combineCollector, Reporter.NULL);\r\n            values.nextKey();\r\n        }\r\n    } finally {\r\n        combiner.close();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\task\\reduce",
  "methodName" : "createInMemorySegments",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "long createInMemorySegments(List<InMemoryMapOutput<K, V>> inMemoryMapOutputs, List<Segment<K, V>> inMemorySegments, long leaveBytes) throws IOException\n{\r\n    long totalSize = 0L;\r\n    long fullSize = 0L;\r\n    for (InMemoryMapOutput<K, V> mo : inMemoryMapOutputs) {\r\n        fullSize += mo.getMemory().length;\r\n    }\r\n    while (fullSize > leaveBytes) {\r\n        InMemoryMapOutput<K, V> mo = inMemoryMapOutputs.remove(0);\r\n        byte[] data = mo.getMemory();\r\n        long size = data.length;\r\n        totalSize += size;\r\n        fullSize -= size;\r\n        Reader<K, V> reader = new InMemoryReader<K, V>(MergeManagerImpl.this, mo.getMapId(), data, 0, (int) size, jobConf);\r\n        inMemorySegments.add(new Segment<K, V>(reader, true, (mo.isPrimaryMapOutput() ? mergedMapOutputsCounter : null)));\r\n    }\r\n    return totalSize;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\task\\reduce",
  "methodName" : "getMaxInMemReduceLimit",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "long getMaxInMemReduceLimit()\n{\r\n    final float maxRedPer = jobConf.getFloat(MRJobConfig.REDUCE_INPUT_BUFFER_PERCENT, 0f);\r\n    if (maxRedPer > 1.0 || maxRedPer < 0.0) {\r\n        throw new RuntimeException(maxRedPer + \": \" + MRJobConfig.REDUCE_INPUT_BUFFER_PERCENT + \" must be a float between 0 and 1.0\");\r\n    }\r\n    return (long) (memoryLimit * maxRedPer);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\task\\reduce",
  "methodName" : "finalMerge",
  "errType" : [ "IOException", "IOException" ],
  "containingMethodsNum" : 41,
  "sourceCodeText" : "RawKeyValueIterator finalMerge(JobConf job, FileSystem fs, List<InMemoryMapOutput<K, V>> inMemoryMapOutputs, List<CompressAwarePath> onDiskMapOutputs) throws IOException\n{\r\n    LOG.info(\"finalMerge called with \" + inMemoryMapOutputs.size() + \" in-memory map-outputs and \" + onDiskMapOutputs.size() + \" on-disk map-outputs\");\r\n    final long maxInMemReduce = getMaxInMemReduceLimit();\r\n    Class<K> keyClass = (Class<K>) job.getMapOutputKeyClass();\r\n    Class<V> valueClass = (Class<V>) job.getMapOutputValueClass();\r\n    boolean keepInputs = job.getKeepFailedTaskFiles();\r\n    final Path tmpDir = new Path(reduceId.toString());\r\n    final RawComparator<K> comparator = (RawComparator<K>) job.getOutputKeyComparator();\r\n    List<Segment<K, V>> memDiskSegments = new ArrayList<Segment<K, V>>();\r\n    long inMemToDiskBytes = 0;\r\n    boolean mergePhaseFinished = false;\r\n    if (inMemoryMapOutputs.size() > 0) {\r\n        TaskID mapId = inMemoryMapOutputs.get(0).getMapId().getTaskID();\r\n        inMemToDiskBytes = createInMemorySegments(inMemoryMapOutputs, memDiskSegments, maxInMemReduce);\r\n        final int numMemDiskSegments = memDiskSegments.size();\r\n        if (numMemDiskSegments > 0 && ioSortFactor > onDiskMapOutputs.size()) {\r\n            mergePhaseFinished = true;\r\n            final Path outputPath = mapOutputFile.getInputFileForWrite(mapId, inMemToDiskBytes).suffix(Task.MERGED_OUTPUT_PREFIX);\r\n            final RawKeyValueIterator rIter = Merger.merge(job, fs, keyClass, valueClass, memDiskSegments, numMemDiskSegments, tmpDir, comparator, reporter, spilledRecordsCounter, null, mergePhase);\r\n            FSDataOutputStream out = IntermediateEncryptedStream.wrapIfNecessary(job, fs.create(outputPath), outputPath);\r\n            Writer<K, V> writer = new Writer<K, V>(job, out, keyClass, valueClass, codec, null, true);\r\n            try {\r\n                Merger.writeFile(rIter, writer, reporter, job);\r\n                writer.close();\r\n                onDiskMapOutputs.add(new CompressAwarePath(outputPath, writer.getRawLength(), writer.getCompressedLength()));\r\n                writer = null;\r\n            } catch (IOException e) {\r\n                if (null != outputPath) {\r\n                    try {\r\n                        fs.delete(outputPath, true);\r\n                    } catch (IOException ie) {\r\n                    }\r\n                }\r\n                throw e;\r\n            } finally {\r\n                if (null != writer) {\r\n                    writer.close();\r\n                }\r\n            }\r\n            LOG.info(\"Merged \" + numMemDiskSegments + \" segments, \" + inMemToDiskBytes + \" bytes to disk to satisfy \" + \"reduce memory limit\");\r\n            inMemToDiskBytes = 0;\r\n            memDiskSegments.clear();\r\n        } else if (inMemToDiskBytes != 0) {\r\n            LOG.info(\"Keeping \" + numMemDiskSegments + \" segments, \" + inMemToDiskBytes + \" bytes in memory for \" + \"intermediate, on-disk merge\");\r\n        }\r\n    }\r\n    List<Segment<K, V>> diskSegments = new ArrayList<Segment<K, V>>();\r\n    long onDiskBytes = inMemToDiskBytes;\r\n    long rawBytes = inMemToDiskBytes;\r\n    CompressAwarePath[] onDisk = onDiskMapOutputs.toArray(new CompressAwarePath[onDiskMapOutputs.size()]);\r\n    for (CompressAwarePath file : onDisk) {\r\n        long fileLength = fs.getFileStatus(file).getLen();\r\n        onDiskBytes += fileLength;\r\n        rawBytes += (file.getRawDataLength() > 0) ? file.getRawDataLength() : fileLength;\r\n        LOG.debug(\"Disk file: \" + file + \" Length is \" + fileLength);\r\n        diskSegments.add(new Segment<K, V>(job, fs, file, codec, keepInputs, (file.toString().endsWith(Task.MERGED_OUTPUT_PREFIX) ? null : mergedMapOutputsCounter), file.getRawDataLength()));\r\n    }\r\n    LOG.info(\"Merging \" + onDisk.length + \" files, \" + onDiskBytes + \" bytes from disk\");\r\n    Collections.sort(diskSegments, new Comparator<Segment<K, V>>() {\r\n\r\n        public int compare(Segment<K, V> o1, Segment<K, V> o2) {\r\n            if (o1.getLength() == o2.getLength()) {\r\n                return 0;\r\n            }\r\n            return o1.getLength() < o2.getLength() ? -1 : 1;\r\n        }\r\n    });\r\n    List<Segment<K, V>> finalSegments = new ArrayList<Segment<K, V>>();\r\n    long inMemBytes = createInMemorySegments(inMemoryMapOutputs, finalSegments, 0);\r\n    LOG.info(\"Merging \" + finalSegments.size() + \" segments, \" + inMemBytes + \" bytes from memory into reduce\");\r\n    if (0 != onDiskBytes) {\r\n        final int numInMemSegments = memDiskSegments.size();\r\n        diskSegments.addAll(0, memDiskSegments);\r\n        memDiskSegments.clear();\r\n        Progress thisPhase = (mergePhaseFinished) ? null : mergePhase;\r\n        RawKeyValueIterator diskMerge = Merger.merge(job, fs, keyClass, valueClass, codec, diskSegments, ioSortFactor, numInMemSegments, tmpDir, comparator, reporter, false, spilledRecordsCounter, null, thisPhase);\r\n        diskSegments.clear();\r\n        if (0 == finalSegments.size()) {\r\n            return diskMerge;\r\n        }\r\n        finalSegments.add(new Segment<K, V>(new RawKVIteratorReader(diskMerge, onDiskBytes), true, rawBytes));\r\n    }\r\n    return Merger.merge(job, fs, keyClass, valueClass, finalSegments, finalSegments.size(), tmpDir, comparator, reporter, spilledRecordsCounter, null, null);\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 2,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\input",
  "methodName" : "needAdditionalRecordAfterSplit",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean needAdditionalRecordAfterSplit()\n{\r\n    return false;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "setCompressOutput",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setCompressOutput(JobConf conf, boolean compress)\n{\r\n    conf.setBoolean(org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.COMPRESS, compress);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getCompressOutput",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "boolean getCompressOutput(JobConf conf)\n{\r\n    return conf.getBoolean(org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.COMPRESS, false);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "setOutputCompressorClass",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void setOutputCompressorClass(JobConf conf, Class<? extends CompressionCodec> codecClass)\n{\r\n    setCompressOutput(conf, true);\r\n    conf.setClass(org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.COMPRESS_CODEC, codecClass, CompressionCodec.class);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getOutputCompressorClass",
  "errType" : [ "ClassNotFoundException" ],
  "containingMethodsNum" : 2,
  "sourceCodeText" : "Class<? extends CompressionCodec> getOutputCompressorClass(JobConf conf, Class<? extends CompressionCodec> defaultValue)\n{\r\n    Class<? extends CompressionCodec> codecClass = defaultValue;\r\n    String name = conf.get(org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.COMPRESS_CODEC);\r\n    if (name != null) {\r\n        try {\r\n            codecClass = conf.getClassByName(name).asSubclass(CompressionCodec.class);\r\n        } catch (ClassNotFoundException e) {\r\n            throw new IllegalArgumentException(\"Compression codec \" + name + \" was not found.\", e);\r\n        }\r\n    }\r\n    return codecClass;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getRecordWriter",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "RecordWriter<K, V> getRecordWriter(FileSystem ignored, JobConf job, String name, Progressable progress) throws IOException",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "checkOutputSpecs",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void checkOutputSpecs(FileSystem ignored, JobConf job) throws FileAlreadyExistsException, InvalidJobConfException, IOException\n{\r\n    Path outDir = getOutputPath(job);\r\n    if (outDir == null && job.getNumReduceTasks() != 0) {\r\n        throw new InvalidJobConfException(\"Output directory not set in JobConf.\");\r\n    }\r\n    if (outDir != null) {\r\n        FileSystem fs = outDir.getFileSystem(job);\r\n        outDir = fs.makeQualified(outDir);\r\n        setOutputPath(job, outDir);\r\n        TokenCache.obtainTokensForNamenodes(job.getCredentials(), new Path[] { outDir }, job);\r\n        if (fs.exists(outDir)) {\r\n            throw new FileAlreadyExistsException(\"Output directory \" + outDir + \" already exists\");\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "setOutputPath",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void setOutputPath(JobConf conf, Path outputDir)\n{\r\n    outputDir = new Path(conf.getWorkingDirectory(), outputDir);\r\n    conf.set(org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.OUTDIR, outputDir.toString());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "setWorkOutputPath",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void setWorkOutputPath(JobConf conf, Path outputDir)\n{\r\n    outputDir = new Path(conf.getWorkingDirectory(), outputDir);\r\n    conf.set(JobContext.TASK_OUTPUT_DIR, outputDir.toString());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getOutputPath",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Path getOutputPath(JobConf conf)\n{\r\n    String name = conf.get(org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.OUTDIR);\r\n    return name == null ? null : new Path(name);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getWorkOutputPath",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Path getWorkOutputPath(JobConf conf)\n{\r\n    String name = conf.get(JobContext.TASK_OUTPUT_DIR);\r\n    return name == null ? null : new Path(name);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getTaskOutputPath",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "Path getTaskOutputPath(JobConf conf, String name) throws IOException\n{\r\n    Path outputPath = getOutputPath(conf);\r\n    if (outputPath == null) {\r\n        throw new IOException(\"Undefined job output-path\");\r\n    }\r\n    OutputCommitter committer = conf.getOutputCommitter();\r\n    Path workPath = outputPath;\r\n    TaskAttemptContext context = new TaskAttemptContextImpl(conf, TaskAttemptID.forName(conf.get(JobContext.TASK_ATTEMPT_ID)));\r\n    if (committer instanceof FileOutputCommitter) {\r\n        workPath = ((FileOutputCommitter) committer).getWorkPath(context, outputPath);\r\n    }\r\n    return new Path(workPath, name);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getUniqueName",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "String getUniqueName(JobConf conf, String name)\n{\r\n    int partition = conf.getInt(JobContext.TASK_PARTITION, -1);\r\n    if (partition == -1) {\r\n        throw new IllegalArgumentException(\"This method can only be called from within a Job\");\r\n    }\r\n    String taskType = conf.getBoolean(JobContext.TASK_ISMAP, JobContext.DEFAULT_TASK_ISMAP) ? \"m\" : \"r\";\r\n    NumberFormat numberFormat = NumberFormat.getInstance();\r\n    numberFormat.setMinimumIntegerDigits(5);\r\n    numberFormat.setGroupingUsed(false);\r\n    return name + \"-\" + taskType + \"-\" + numberFormat.format(partition);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getPathForCustomFile",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "Path getPathForCustomFile(JobConf conf, String name)\n{\r\n    return new Path(getWorkOutputPath(conf), getUniqueName(conf, name));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "downgrade",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "JobID downgrade(org.apache.hadoop.mapreduce.JobID old)\n{\r\n    if (old instanceof JobID) {\r\n        return (JobID) old;\r\n    } else {\r\n        return new JobID(old.getJtIdentifier(), old.getId());\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "read",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "JobID read(DataInput in) throws IOException\n{\r\n    JobID jobId = new JobID();\r\n    jobId.readFields(in);\r\n    return jobId;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "forName",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "JobID forName(String str) throws IllegalArgumentException\n{\r\n    return (JobID) org.apache.hadoop.mapreduce.JobID.forName(str);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getJobIDsPattern",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "String getJobIDsPattern(String jtIdentifier, Integer jobId)\n{\r\n    StringBuilder builder = new StringBuilder(JOB).append(SEPARATOR);\r\n    builder.append(getJobIDsPatternWOPrefix(jtIdentifier, jobId));\r\n    return builder.toString();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getJobIDsPatternWOPrefix",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "StringBuilder getJobIDsPatternWOPrefix(String jtIdentifier, Integer jobId)\n{\r\n    StringBuilder builder = new StringBuilder();\r\n    if (jtIdentifier != null) {\r\n        builder.append(jtIdentifier);\r\n    } else {\r\n        builder.append(\"[^\").append(SEPARATOR).append(\"]*\");\r\n    }\r\n    builder.append(SEPARATOR).append(jobId != null ? idFormat.format(jobId) : \"[0-9]*\");\r\n    return builder;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output",
  "methodName" : "getRecordWriter",
  "errType" : null,
  "containingMethodsNum" : 12,
  "sourceCodeText" : "RecordWriter<WritableComparable<?>, Writable> getRecordWriter(TaskAttemptContext context) throws IOException\n{\r\n    Configuration conf = context.getConfiguration();\r\n    CompressionCodec codec = null;\r\n    CompressionType compressionType = CompressionType.NONE;\r\n    if (getCompressOutput(context)) {\r\n        compressionType = SequenceFileOutputFormat.getOutputCompressionType(context);\r\n        Class<?> codecClass = getOutputCompressorClass(context, DefaultCodec.class);\r\n        codec = (CompressionCodec) ReflectionUtils.newInstance(codecClass, conf);\r\n    }\r\n    Path file = getDefaultWorkFile(context, \"\");\r\n    FileSystem fs = file.getFileSystem(conf);\r\n    final MapFile.Writer out = new MapFile.Writer(conf, fs, file.toString(), context.getOutputKeyClass().asSubclass(WritableComparable.class), context.getOutputValueClass().asSubclass(Writable.class), compressionType, codec, context);\r\n    return new RecordWriter<WritableComparable<?>, Writable>() {\r\n\r\n        public void write(WritableComparable<?> key, Writable value) throws IOException {\r\n            out.append(key, value);\r\n        }\r\n\r\n        public void close(TaskAttemptContext context) throws IOException {\r\n            out.close();\r\n        }\r\n    };\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output",
  "methodName" : "getReaders",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "MapFile.Reader[] getReaders(Path dir, Configuration conf) throws IOException\n{\r\n    FileSystem fs = dir.getFileSystem(conf);\r\n    PathFilter filter = new PathFilter() {\r\n\r\n        @Override\r\n        public boolean accept(Path path) {\r\n            String name = path.getName();\r\n            if (name.startsWith(\"_\") || name.startsWith(\".\"))\r\n                return false;\r\n            return true;\r\n        }\r\n    };\r\n    Path[] names = FileUtil.stat2Paths(fs.listStatus(dir, filter));\r\n    Arrays.sort(names);\r\n    MapFile.Reader[] parts = new MapFile.Reader[names.length];\r\n    for (int i = 0; i < names.length; i++) {\r\n        parts[i] = new MapFile.Reader(fs, names[i].toString(), conf);\r\n    }\r\n    return parts;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output",
  "methodName" : "getEntry",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "Writable getEntry(MapFile.Reader[] readers, Partitioner<K, V> partitioner, K key, V value) throws IOException\n{\r\n    int readerLength = readers.length;\r\n    int part;\r\n    if (readerLength <= 1) {\r\n        part = 0;\r\n    } else {\r\n        part = partitioner.getPartition(key, value, readers.length);\r\n    }\r\n    return readers[part].get(key, value);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest\\stages",
  "methodName" : "getFilesCommitted",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "List<FileEntry> getFilesCommitted()\n{\r\n    return filesCommitted;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest\\stages",
  "methodName" : "getTotalFileSize",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getTotalFileSize()\n{\r\n    return totalFileSize;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest\\stages",
  "methodName" : "executeStage",
  "errType" : null,
  "containingMethodsNum" : 11,
  "sourceCodeText" : "ManifestSuccessData executeStage(Pair<List<TaskManifest>, Set<Path>> args) throws IOException\n{\r\n    final List<TaskManifest> taskManifests = args.getLeft();\r\n    createdDirectories = args.getRight();\r\n    final ManifestSuccessData success = createManifestOutcome(getStageConfig(), OP_STAGE_JOB_COMMIT);\r\n    final int manifestCount = taskManifests.size();\r\n    LOG.info(\"{}: Executing Manifest Job Commit with {} manifests in {}\", getName(), manifestCount, getTaskManifestDir());\r\n    final Iterable<FileEntry> filesToCommit = concat(taskManifests.stream().map(TaskManifest::getFilesToCommit).collect(Collectors.toList()));\r\n    TaskPool.foreach(filesToCommit).executeWith(getIOProcessors()).stopOnFailure().run(this::commitOneFile);\r\n    List<FileEntry> committed = getFilesCommitted();\r\n    LOG.info(\"{}: Files committed: {}. Total size {}\", getName(), committed.size(), getTotalFileSize());\r\n    success.setFilenamePaths(committed.subList(0, Math.min(committed.size(), SUCCESS_MARKER_FILE_LIMIT)).stream().map(FileEntry::getDestPath).collect(Collectors.toList()));\r\n    success.setSuccess(true);\r\n    return success;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest\\stages",
  "methodName" : "commitOneFile",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void commitOneFile(FileEntry entry) throws IOException\n{\r\n    updateAuditContext(OP_STAGE_JOB_RENAME_FILES);\r\n    progress();\r\n    final boolean deleteDest = getStageConfig().getDeleteTargetPaths() && !createdDirectories.contains(entry.getDestPath().getParent());\r\n    commitFile(entry, deleteDest);\r\n    synchronized (this) {\r\n        filesCommitted.add(entry);\r\n        totalFileSize += entry.getSize();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\input",
  "methodName" : "initialize",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void initialize(InputSplit split, TaskAttemptContext context) throws IOException, InterruptedException\n{\r\n    assert fileSplitIsValid(context);\r\n    delegate.initialize(fileSplit, context);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\input",
  "methodName" : "fileSplitIsValid",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "boolean fileSplitIsValid(TaskAttemptContext context)\n{\r\n    Configuration conf = context.getConfiguration();\r\n    long offset = conf.getLong(MRJobConfig.MAP_INPUT_START, 0L);\r\n    if (fileSplit.getStart() != offset) {\r\n        return false;\r\n    }\r\n    long length = conf.getLong(MRJobConfig.MAP_INPUT_PATH, 0L);\r\n    if (fileSplit.getLength() != length) {\r\n        return false;\r\n    }\r\n    String path = conf.get(MRJobConfig.MAP_INPUT_FILE);\r\n    if (!fileSplit.getPath().toString().equals(path)) {\r\n        return false;\r\n    }\r\n    return true;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\input",
  "methodName" : "nextKeyValue",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "boolean nextKeyValue() throws IOException, InterruptedException\n{\r\n    return delegate.nextKeyValue();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\input",
  "methodName" : "getCurrentKey",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "K getCurrentKey() throws IOException, InterruptedException\n{\r\n    return delegate.getCurrentKey();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\input",
  "methodName" : "getCurrentValue",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "V getCurrentValue() throws IOException, InterruptedException\n{\r\n    return delegate.getCurrentValue();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\input",
  "methodName" : "getProgress",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "float getProgress() throws IOException, InterruptedException\n{\r\n    return delegate.getProgress();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\input",
  "methodName" : "close",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void close() throws IOException\n{\r\n    delegate.close();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\aggregate",
  "methodName" : "addNextValue",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void addNextValue(Object val)\n{\r\n    this.sum += Double.parseDouble(val.toString());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\aggregate",
  "methodName" : "addNextValue",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void addNextValue(double val)\n{\r\n    this.sum += val;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\aggregate",
  "methodName" : "getReport",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getReport()\n{\r\n    return \"\" + sum;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\aggregate",
  "methodName" : "getSum",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "double getSum()\n{\r\n    return this.sum;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\aggregate",
  "methodName" : "reset",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void reset()\n{\r\n    sum = 0;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\aggregate",
  "methodName" : "getCombinerOutput",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "ArrayList<String> getCombinerOutput()\n{\r\n    ArrayList<String> retv = new ArrayList<String>(1);\r\n    retv.add(\"\" + sum);\r\n    return retv;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\map",
  "methodName" : "getNumberOfThreads",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "int getNumberOfThreads(JobContext job)\n{\r\n    return job.getConfiguration().getInt(NUM_THREADS, 10);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\map",
  "methodName" : "setNumberOfThreads",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setNumberOfThreads(Job job, int threads)\n{\r\n    job.getConfiguration().setInt(NUM_THREADS, threads);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\map",
  "methodName" : "getMapperClass",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Class<Mapper<K1, V1, K2, V2>> getMapperClass(JobContext job)\n{\r\n    return (Class<Mapper<K1, V1, K2, V2>>) job.getConfiguration().getClass(MAP_CLASS, Mapper.class);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\map",
  "methodName" : "setMapperClass",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void setMapperClass(Job job, Class<? extends Mapper<K1, V1, K2, V2>> cls)\n{\r\n    if (MultithreadedMapper.class.isAssignableFrom(cls)) {\r\n        throw new IllegalArgumentException(\"Can't have recursive \" + \"MultithreadedMapper instances.\");\r\n    }\r\n    job.getConfiguration().setClass(MAP_CLASS, cls, Mapper.class);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\map",
  "methodName" : "run",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void run(Context context) throws IOException, InterruptedException\n{\r\n    outer = context;\r\n    int numberOfThreads = getNumberOfThreads(context);\r\n    mapClass = getMapperClass(context);\r\n    if (LOG.isDebugEnabled()) {\r\n        LOG.debug(\"Configuring multithread runner to use \" + numberOfThreads + \" threads\");\r\n    }\r\n    runners = new ArrayList<MapRunner>(numberOfThreads);\r\n    for (int i = 0; i < numberOfThreads; ++i) {\r\n        MapRunner thread = new MapRunner(context);\r\n        thread.start();\r\n        runners.add(i, thread);\r\n    }\r\n    for (int i = 0; i < numberOfThreads; ++i) {\r\n        MapRunner thread = runners.get(i);\r\n        thread.join();\r\n        Throwable th = thread.throwable;\r\n        if (th != null) {\r\n            if (th instanceof IOException) {\r\n                throw (IOException) th;\r\n            } else if (th instanceof InterruptedException) {\r\n                throw (InterruptedException) th;\r\n            } else {\r\n                throw new RuntimeException(th);\r\n            }\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\input",
  "methodName" : "addInputPath",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void addInputPath(Job job, Path path, Class<? extends InputFormat> inputFormatClass)\n{\r\n    String inputFormatMapping = path.toString() + \";\" + inputFormatClass.getName();\r\n    Configuration conf = job.getConfiguration();\r\n    String inputFormats = conf.get(DIR_FORMATS);\r\n    conf.set(DIR_FORMATS, inputFormats == null ? inputFormatMapping : inputFormats + \",\" + inputFormatMapping);\r\n    job.setInputFormatClass(DelegatingInputFormat.class);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\input",
  "methodName" : "addInputPath",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void addInputPath(Job job, Path path, Class<? extends InputFormat> inputFormatClass, Class<? extends Mapper> mapperClass)\n{\r\n    addInputPath(job, path, inputFormatClass);\r\n    Configuration conf = job.getConfiguration();\r\n    String mapperMapping = path.toString() + \";\" + mapperClass.getName();\r\n    String mappers = conf.get(DIR_MAPPERS);\r\n    conf.set(DIR_MAPPERS, mappers == null ? mapperMapping : mappers + \",\" + mapperMapping);\r\n    job.setMapperClass(DelegatingMapper.class);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\input",
  "methodName" : "getInputFormatMap",
  "errType" : [ "ClassNotFoundException" ],
  "containingMethodsNum" : 5,
  "sourceCodeText" : "Map<Path, InputFormat> getInputFormatMap(JobContext job)\n{\r\n    Map<Path, InputFormat> m = new HashMap<Path, InputFormat>();\r\n    Configuration conf = job.getConfiguration();\r\n    String[] pathMappings = conf.get(DIR_FORMATS).split(\",\");\r\n    for (String pathMapping : pathMappings) {\r\n        String[] split = pathMapping.split(\";\");\r\n        InputFormat inputFormat;\r\n        try {\r\n            inputFormat = (InputFormat) ReflectionUtils.newInstance(conf.getClassByName(split[1]), conf);\r\n        } catch (ClassNotFoundException e) {\r\n            throw new RuntimeException(e);\r\n        }\r\n        m.put(new Path(split[0]), inputFormat);\r\n    }\r\n    return m;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\input",
  "methodName" : "getMapperTypeMap",
  "errType" : [ "ClassNotFoundException" ],
  "containingMethodsNum" : 7,
  "sourceCodeText" : "Map<Path, Class<? extends Mapper>> getMapperTypeMap(JobContext job)\n{\r\n    Configuration conf = job.getConfiguration();\r\n    if (conf.get(DIR_MAPPERS) == null) {\r\n        return Collections.emptyMap();\r\n    }\r\n    Map<Path, Class<? extends Mapper>> m = new HashMap<Path, Class<? extends Mapper>>();\r\n    String[] pathMappings = conf.get(DIR_MAPPERS).split(\",\");\r\n    for (String pathMapping : pathMappings) {\r\n        String[] split = pathMapping.split(\";\");\r\n        Class<? extends Mapper> mapClass;\r\n        try {\r\n            mapClass = (Class<? extends Mapper>) conf.getClassByName(split[1]);\r\n        } catch (ClassNotFoundException e) {\r\n            throw new RuntimeException(e);\r\n        }\r\n        m.put(new Path(split[0]), mapClass);\r\n    }\r\n    return m;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "downgrade",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "TaskCompletionEvent downgrade(org.apache.hadoop.mapreduce.TaskCompletionEvent event)\n{\r\n    return new TaskCompletionEvent(event.getEventId(), TaskAttemptID.downgrade(event.getTaskAttemptId()), event.idWithinJob(), event.isMapTask(), Status.valueOf(event.getStatus().name()), event.getTaskTrackerHttp());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getTaskId",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String getTaskId()\n{\r\n    return getTaskAttemptId().toString();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getTaskAttemptId",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "TaskAttemptID getTaskAttemptId()\n{\r\n    return TaskAttemptID.downgrade(super.getTaskAttemptId());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getTaskStatus",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Status getTaskStatus()\n{\r\n    return Status.valueOf(super.getStatus().name());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "setTaskId",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setTaskId(String taskId)\n{\r\n    this.setTaskAttemptId(TaskAttemptID.forName(taskId));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "setTaskID",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setTaskID(TaskAttemptID taskId)\n{\r\n    this.setTaskAttemptId(taskId);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "setTaskAttemptId",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setTaskAttemptId(TaskAttemptID taskId)\n{\r\n    super.setTaskAttemptId(taskId);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "setTaskStatus",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setTaskStatus(Status status)\n{\r\n    super.setTaskStatus(org.apache.hadoop.mapreduce.TaskCompletionEvent.Status.valueOf(status.name()));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "setTaskRunTime",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setTaskRunTime(int taskCompletionTime)\n{\r\n    super.setTaskRunTime(taskCompletionTime);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "setEventId",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setEventId(int eventId)\n{\r\n    super.setEventId(eventId);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "setTaskTrackerHttp",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setTaskTrackerHttp(String taskTrackerHttp)\n{\r\n    super.setTaskTrackerHttp(taskTrackerHttp);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "refreshQueues",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void refreshQueues(List<JobQueueInfo> newRootQueues) throws Throwable",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getRecordReader",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "RecordReader<Text, Text> getRecordReader(InputSplit split, JobConf job, Reporter reporter) throws IOException\n{\r\n    reporter.setStatus(split.toString());\r\n    return new SequenceFileAsTextRecordReader(job, (FileSplit) split);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output",
  "methodName" : "createOutputCommitter",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "PathOutputCommitter createOutputCommitter(Path outputPath, TaskAttemptContext context) throws IOException\n{\r\n    return createFileOutputCommitter(outputPath, context);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "getDatum",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Object getDatum()\n{\r\n    return datum;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "setDatum",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setDatum(Object datum)\n{\r\n    this.datum = (TaskAttemptStarted) datum;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "getTaskId",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "TaskID getTaskId()\n{\r\n    return TaskID.forName(datum.getTaskid().toString());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "getTrackerName",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String getTrackerName()\n{\r\n    return datum.getTrackerName().toString();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "getStartTime",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "long getStartTime()\n{\r\n    return datum.getStartTime();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "getTaskType",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "TaskType getTaskType()\n{\r\n    return TaskType.valueOf(datum.getTaskType().toString());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "getHttpPort",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "int getHttpPort()\n{\r\n    return datum.getHttpPort();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "getShufflePort",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "int getShufflePort()\n{\r\n    return datum.getShufflePort();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "getTaskAttemptId",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "TaskAttemptID getTaskAttemptId()\n{\r\n    return TaskAttemptID.forName(datum.getAttemptId().toString());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "getEventType",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "EventType getEventType()\n{\r\n    return getTaskId().getTaskType() == TaskType.MAP ? EventType.MAP_ATTEMPT_STARTED : EventType.REDUCE_ATTEMPT_STARTED;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "getContainerId",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "ContainerId getContainerId()\n{\r\n    return ContainerId.fromString(datum.getContainerId().toString());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "getLocality",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "String getLocality()\n{\r\n    if (datum.getLocality() != null) {\r\n        return datum.getLocality().toString();\r\n    }\r\n    return null;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "getAvataar",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "String getAvataar()\n{\r\n    if (datum.getAvataar() != null) {\r\n        return datum.getAvataar().toString();\r\n    }\r\n    return null;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "toTimelineEvent",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "TimelineEvent toTimelineEvent()\n{\r\n    TimelineEvent tEvent = new TimelineEvent();\r\n    tEvent.setId(StringUtils.toUpperCase(getEventType().name()));\r\n    tEvent.addInfo(\"TASK_TYPE\", getTaskType().toString());\r\n    tEvent.addInfo(\"TASK_ATTEMPT_ID\", getTaskAttemptId().toString());\r\n    tEvent.addInfo(\"START_TIME\", getStartTime());\r\n    tEvent.addInfo(\"HTTP_PORT\", getHttpPort());\r\n    tEvent.addInfo(\"TRACKER_NAME\", getTrackerName());\r\n    tEvent.addInfo(\"SHUFFLE_PORT\", getShufflePort());\r\n    tEvent.addInfo(\"CONTAINER_ID\", getContainerId() == null ? \"\" : getContainerId().toString());\r\n    return tEvent;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "getTimelineMetrics",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Set<TimelineMetric> getTimelineMetrics()\n{\r\n    return null;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "setQueueName",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setQueueName(String queueName)\n{\r\n    this.queueName = queueName;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "getQueueName",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getQueueName()\n{\r\n    return queueName;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "setSchedulingInfo",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setSchedulingInfo(String schedulingInfo)\n{\r\n    this.schedulingInfo = schedulingInfo;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "getSchedulingInfo",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getSchedulingInfo()\n{\r\n    if (schedulingInfo != null) {\r\n        return schedulingInfo;\r\n    } else {\r\n        return \"N/A\";\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "setState",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setState(QueueState state)\n{\r\n    queueState = state;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "getState",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "QueueState getState()\n{\r\n    return queueState;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "setJobStatuses",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setJobStatuses(JobStatus[] stats)\n{\r\n    this.stats = stats;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "getQueueChildren",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "List<QueueInfo> getQueueChildren()\n{\r\n    return children;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "setQueueChildren",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setQueueChildren(List<QueueInfo> children)\n{\r\n    this.children = children;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "getProperties",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Properties getProperties()\n{\r\n    return props;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "setProperties",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setProperties(Properties props)\n{\r\n    this.props = props;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "getJobStatuses",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "JobStatus[] getJobStatuses()\n{\r\n    return stats;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "readFields",
  "errType" : null,
  "containingMethodsNum" : 9,
  "sourceCodeText" : "void readFields(DataInput in) throws IOException\n{\r\n    queueName = StringInterner.weakIntern(Text.readString(in));\r\n    queueState = WritableUtils.readEnum(in, QueueState.class);\r\n    schedulingInfo = StringInterner.weakIntern(Text.readString(in));\r\n    int length = in.readInt();\r\n    stats = new JobStatus[length];\r\n    for (int i = 0; i < length; i++) {\r\n        stats[i] = new JobStatus();\r\n        stats[i].readFields(in);\r\n    }\r\n    int count = in.readInt();\r\n    children.clear();\r\n    for (int i = 0; i < count; i++) {\r\n        QueueInfo childQueueInfo = new QueueInfo();\r\n        childQueueInfo.readFields(in);\r\n        children.add(childQueueInfo);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "write",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void write(DataOutput out) throws IOException\n{\r\n    Text.writeString(out, queueName);\r\n    WritableUtils.writeEnum(out, queueState);\r\n    if (schedulingInfo != null) {\r\n        Text.writeString(out, schedulingInfo);\r\n    } else {\r\n        Text.writeString(out, \"N/A\");\r\n    }\r\n    out.writeInt(stats.length);\r\n    for (JobStatus stat : stats) {\r\n        stat.write(out);\r\n    }\r\n    out.writeInt(children.size());\r\n    for (QueueInfo childQueueInfo : children) {\r\n        childQueueInfo.write(out);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\filecache",
  "methodName" : "addLocalArchives",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void addLocalArchives(Configuration conf, String str)\n{\r\n    String archives = conf.get(CACHE_LOCALARCHIVES);\r\n    conf.set(CACHE_LOCALARCHIVES, archives == null ? str : archives + \",\" + str);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\filecache",
  "methodName" : "addLocalFiles",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void addLocalFiles(Configuration conf, String str)\n{\r\n    String files = conf.get(CACHE_LOCALFILES);\r\n    conf.set(CACHE_LOCALFILES, files == null ? str : files + \",\" + str);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\filecache",
  "methodName" : "createAllSymlink",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void createAllSymlink(Configuration conf, File jobCacheDir, File workDir) throws IOException\n{\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\filecache",
  "methodName" : "getFileStatus",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "FileStatus getFileStatus(Configuration conf, URI cache) throws IOException\n{\r\n    FileSystem fileSystem = FileSystem.get(cache, conf);\r\n    return fileSystem.getFileStatus(new Path(cache.getPath()));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\filecache",
  "methodName" : "getTimestamp",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "long getTimestamp(Configuration conf, URI cache) throws IOException\n{\r\n    return getFileStatus(conf, cache).getModificationTime();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\filecache",
  "methodName" : "setArchiveTimestamps",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setArchiveTimestamps(Configuration conf, String timestamps)\n{\r\n    conf.set(CACHE_ARCHIVES_TIMESTAMPS, timestamps);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\filecache",
  "methodName" : "setFileTimestamps",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setFileTimestamps(Configuration conf, String timestamps)\n{\r\n    conf.set(CACHE_FILES_TIMESTAMPS, timestamps);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\filecache",
  "methodName" : "setLocalArchives",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setLocalArchives(Configuration conf, String str)\n{\r\n    conf.set(CACHE_LOCALARCHIVES, str);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\filecache",
  "methodName" : "setLocalFiles",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setLocalFiles(Configuration conf, String str)\n{\r\n    conf.set(CACHE_LOCALFILES, str);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\join",
  "methodName" : "add",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void add(InputSplit s) throws IOException, InterruptedException\n{\r\n    if (null == splits) {\r\n        throw new IOException(\"Uninitialized InputSplit\");\r\n    }\r\n    if (fill == splits.length) {\r\n        throw new IOException(\"Too many splits\");\r\n    }\r\n    splits[fill++] = s;\r\n    totsize += s.getLength();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\join",
  "methodName" : "get",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "InputSplit get(int i)\n{\r\n    return splits[i];\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\join",
  "methodName" : "getLength",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getLength() throws IOException\n{\r\n    return totsize;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\join",
  "methodName" : "getLength",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "long getLength(int i) throws IOException, InterruptedException\n{\r\n    return splits[i].getLength();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\join",
  "methodName" : "getLocations",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "String[] getLocations() throws IOException, InterruptedException\n{\r\n    HashSet<String> hosts = new HashSet<String>();\r\n    for (InputSplit s : splits) {\r\n        String[] hints = s.getLocations();\r\n        if (hints != null && hints.length > 0) {\r\n            for (String host : hints) {\r\n                hosts.add(host);\r\n            }\r\n        }\r\n    }\r\n    return hosts.toArray(new String[hosts.size()]);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\join",
  "methodName" : "getLocation",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String[] getLocation(int i) throws IOException, InterruptedException\n{\r\n    return splits[i].getLocations();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\join",
  "methodName" : "write",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void write(DataOutput out) throws IOException\n{\r\n    WritableUtils.writeVInt(out, splits.length);\r\n    for (InputSplit s : splits) {\r\n        Text.writeString(out, s.getClass().getName());\r\n    }\r\n    for (InputSplit s : splits) {\r\n        SerializationFactory factory = new SerializationFactory(conf);\r\n        Serializer serializer = factory.getSerializer(s.getClass());\r\n        serializer.open((DataOutputStream) out);\r\n        serializer.serialize(s);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\join",
  "methodName" : "readFields",
  "errType" : [ "ClassNotFoundException" ],
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void readFields(DataInput in) throws IOException\n{\r\n    int card = WritableUtils.readVInt(in);\r\n    if (splits == null || splits.length != card) {\r\n        splits = new InputSplit[card];\r\n    }\r\n    Class<? extends InputSplit>[] cls = new Class[card];\r\n    try {\r\n        for (int i = 0; i < card; ++i) {\r\n            cls[i] = Class.forName(Text.readString(in)).asSubclass(InputSplit.class);\r\n        }\r\n        for (int i = 0; i < card; ++i) {\r\n            splits[i] = ReflectionUtils.newInstance(cls[i], null);\r\n            SerializationFactory factory = new SerializationFactory(conf);\r\n            Deserializer deserializer = factory.getDeserializer(cls[i]);\r\n            deserializer.open((DataInputStream) in);\r\n            splits[i] = (InputSplit) deserializer.deserialize(splits[i]);\r\n        }\r\n    } catch (ClassNotFoundException e) {\r\n        throw new IOException(\"Failed split init\", e);\r\n    }\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "parse",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void parse(HistoryEventHandler handler) throws IOException\n{\r\n    parse(new EventReader(in), handler);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "parse",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void parse(EventReader reader, HistoryEventHandler handler) throws IOException\n{\r\n    int eventCtr = 0;\r\n    HistoryEvent event;\r\n    try {\r\n        while ((event = reader.getNextEvent()) != null) {\r\n            handler.handleEvent(event);\r\n            ++eventCtr;\r\n        }\r\n    } catch (IOException ioe) {\r\n        LOG.info(\"Caught exception parsing history file after \" + eventCtr + \" events\", ioe);\r\n        parseException = ioe;\r\n    } finally {\r\n        in.close();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "parse",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "JobInfo parse() throws IOException\n{\r\n    return parse(new EventReader(in));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "parse",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "JobInfo parse(EventReader reader) throws IOException\n{\r\n    if (info != null) {\r\n        return info;\r\n    }\r\n    info = new JobInfo();\r\n    parse(reader, this);\r\n    return info;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "getParseException",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "IOException getParseException()\n{\r\n    return parseException;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "handleEvent",
  "errType" : null,
  "containingMethodsNum" : 18,
  "sourceCodeText" : "void handleEvent(HistoryEvent event)\n{\r\n    EventType type = event.getEventType();\r\n    switch(type) {\r\n        case JOB_SUBMITTED:\r\n            handleJobSubmittedEvent((JobSubmittedEvent) event);\r\n            break;\r\n        case JOB_STATUS_CHANGED:\r\n            break;\r\n        case JOB_INFO_CHANGED:\r\n            handleJobInfoChangeEvent((JobInfoChangeEvent) event);\r\n            break;\r\n        case JOB_INITED:\r\n            handleJobInitedEvent((JobInitedEvent) event);\r\n            break;\r\n        case JOB_PRIORITY_CHANGED:\r\n            handleJobPriorityChangeEvent((JobPriorityChangeEvent) event);\r\n            break;\r\n        case JOB_QUEUE_CHANGED:\r\n            handleJobQueueChangeEvent((JobQueueChangeEvent) event);\r\n            break;\r\n        case JOB_FAILED:\r\n        case JOB_KILLED:\r\n        case JOB_ERROR:\r\n            handleJobFailedEvent((JobUnsuccessfulCompletionEvent) event);\r\n            break;\r\n        case JOB_FINISHED:\r\n            handleJobFinishedEvent((JobFinishedEvent) event);\r\n            break;\r\n        case TASK_STARTED:\r\n            handleTaskStartedEvent((TaskStartedEvent) event);\r\n            break;\r\n        case TASK_FAILED:\r\n            handleTaskFailedEvent((TaskFailedEvent) event);\r\n            break;\r\n        case TASK_UPDATED:\r\n            handleTaskUpdatedEvent((TaskUpdatedEvent) event);\r\n            break;\r\n        case TASK_FINISHED:\r\n            handleTaskFinishedEvent((TaskFinishedEvent) event);\r\n            break;\r\n        case MAP_ATTEMPT_STARTED:\r\n        case CLEANUP_ATTEMPT_STARTED:\r\n        case REDUCE_ATTEMPT_STARTED:\r\n        case SETUP_ATTEMPT_STARTED:\r\n            handleTaskAttemptStartedEvent((TaskAttemptStartedEvent) event);\r\n            break;\r\n        case MAP_ATTEMPT_FAILED:\r\n        case CLEANUP_ATTEMPT_FAILED:\r\n        case REDUCE_ATTEMPT_FAILED:\r\n        case SETUP_ATTEMPT_FAILED:\r\n        case MAP_ATTEMPT_KILLED:\r\n        case CLEANUP_ATTEMPT_KILLED:\r\n        case REDUCE_ATTEMPT_KILLED:\r\n        case SETUP_ATTEMPT_KILLED:\r\n            handleTaskAttemptFailedEvent((TaskAttemptUnsuccessfulCompletionEvent) event);\r\n            break;\r\n        case MAP_ATTEMPT_FINISHED:\r\n            handleMapAttemptFinishedEvent((MapAttemptFinishedEvent) event);\r\n            break;\r\n        case REDUCE_ATTEMPT_FINISHED:\r\n            handleReduceAttemptFinishedEvent((ReduceAttemptFinishedEvent) event);\r\n            break;\r\n        case SETUP_ATTEMPT_FINISHED:\r\n        case CLEANUP_ATTEMPT_FINISHED:\r\n            handleTaskAttemptFinishedEvent((TaskAttemptFinishedEvent) event);\r\n            break;\r\n        case AM_STARTED:\r\n            handleAMStartedEvent((AMStartedEvent) event);\r\n            break;\r\n        default:\r\n            break;\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "handleTaskAttemptFinishedEvent",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void handleTaskAttemptFinishedEvent(TaskAttemptFinishedEvent event)\n{\r\n    TaskInfo taskInfo = info.tasksMap.get(event.getTaskId());\r\n    TaskAttemptInfo attemptInfo = taskInfo.attemptsMap.get(event.getAttemptId());\r\n    attemptInfo.finishTime = event.getFinishTime();\r\n    attemptInfo.status = StringInterner.weakIntern(event.getTaskStatus());\r\n    attemptInfo.state = StringInterner.weakIntern(event.getState());\r\n    attemptInfo.counters = event.getCounters();\r\n    attemptInfo.hostname = StringInterner.weakIntern(event.getHostname());\r\n    info.completedTaskAttemptsMap.put(event.getAttemptId(), attemptInfo);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "handleReduceAttemptFinishedEvent",
  "errType" : null,
  "containingMethodsNum" : 12,
  "sourceCodeText" : "void handleReduceAttemptFinishedEvent(ReduceAttemptFinishedEvent event)\n{\r\n    TaskInfo taskInfo = info.tasksMap.get(event.getTaskId());\r\n    TaskAttemptInfo attemptInfo = taskInfo.attemptsMap.get(event.getAttemptId());\r\n    attemptInfo.finishTime = event.getFinishTime();\r\n    attemptInfo.status = StringInterner.weakIntern(event.getTaskStatus());\r\n    attemptInfo.state = StringInterner.weakIntern(event.getState());\r\n    attemptInfo.shuffleFinishTime = event.getShuffleFinishTime();\r\n    attemptInfo.sortFinishTime = event.getSortFinishTime();\r\n    attemptInfo.counters = event.getCounters();\r\n    attemptInfo.hostname = StringInterner.weakIntern(event.getHostname());\r\n    attemptInfo.port = event.getPort();\r\n    attemptInfo.rackname = StringInterner.weakIntern(event.getRackName());\r\n    info.completedTaskAttemptsMap.put(event.getAttemptId(), attemptInfo);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "handleMapAttemptFinishedEvent",
  "errType" : null,
  "containingMethodsNum" : 11,
  "sourceCodeText" : "void handleMapAttemptFinishedEvent(MapAttemptFinishedEvent event)\n{\r\n    TaskInfo taskInfo = info.tasksMap.get(event.getTaskId());\r\n    TaskAttemptInfo attemptInfo = taskInfo.attemptsMap.get(event.getAttemptId());\r\n    attemptInfo.finishTime = event.getFinishTime();\r\n    attemptInfo.status = StringInterner.weakIntern(event.getTaskStatus());\r\n    attemptInfo.state = StringInterner.weakIntern(event.getState());\r\n    attemptInfo.mapFinishTime = event.getMapFinishTime();\r\n    attemptInfo.counters = event.getCounters();\r\n    attemptInfo.hostname = StringInterner.weakIntern(event.getHostname());\r\n    attemptInfo.port = event.getPort();\r\n    attemptInfo.rackname = StringInterner.weakIntern(event.getRackName());\r\n    info.completedTaskAttemptsMap.put(event.getAttemptId(), attemptInfo);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "handleTaskAttemptFailedEvent",
  "errType" : null,
  "containingMethodsNum" : 17,
  "sourceCodeText" : "void handleTaskAttemptFailedEvent(TaskAttemptUnsuccessfulCompletionEvent event)\n{\r\n    TaskInfo taskInfo = info.tasksMap.get(event.getTaskId());\r\n    if (taskInfo == null) {\r\n        LOG.warn(\"TaskInfo is null for TaskAttemptUnsuccessfulCompletionEvent\" + \" taskId:  \" + event.getTaskId().toString());\r\n        return;\r\n    }\r\n    TaskAttemptInfo attemptInfo = taskInfo.attemptsMap.get(event.getTaskAttemptId());\r\n    if (attemptInfo == null) {\r\n        LOG.warn(\"AttemptInfo is null for TaskAttemptUnsuccessfulCompletionEvent\" + \" taskAttemptId:  \" + event.getTaskAttemptId().toString());\r\n        return;\r\n    }\r\n    attemptInfo.finishTime = event.getFinishTime();\r\n    attemptInfo.error = StringInterner.weakIntern(event.getError());\r\n    attemptInfo.status = StringInterner.weakIntern(event.getTaskStatus());\r\n    attemptInfo.hostname = StringInterner.weakIntern(event.getHostname());\r\n    attemptInfo.port = event.getPort();\r\n    attemptInfo.rackname = StringInterner.weakIntern(event.getRackName());\r\n    attemptInfo.shuffleFinishTime = event.getFinishTime();\r\n    attemptInfo.sortFinishTime = event.getFinishTime();\r\n    attemptInfo.mapFinishTime = event.getFinishTime();\r\n    attemptInfo.counters = event.getCounters();\r\n    if (TaskStatus.State.SUCCEEDED.toString().equals(taskInfo.status)) {\r\n        if (attemptInfo.getAttemptId().equals(taskInfo.getSuccessfulAttemptId())) {\r\n            taskInfo.counters = null;\r\n            taskInfo.finishTime = -1;\r\n            taskInfo.status = null;\r\n            taskInfo.successfulAttemptId = null;\r\n        }\r\n    }\r\n    info.completedTaskAttemptsMap.put(event.getTaskAttemptId(), attemptInfo);\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "handleTaskAttemptStartedEvent",
  "errType" : null,
  "containingMethodsNum" : 10,
  "sourceCodeText" : "void handleTaskAttemptStartedEvent(TaskAttemptStartedEvent event)\n{\r\n    TaskAttemptID attemptId = event.getTaskAttemptId();\r\n    TaskInfo taskInfo = info.tasksMap.get(event.getTaskId());\r\n    TaskAttemptInfo attemptInfo = new TaskAttemptInfo();\r\n    attemptInfo.startTime = event.getStartTime();\r\n    attemptInfo.attemptId = event.getTaskAttemptId();\r\n    attemptInfo.httpPort = event.getHttpPort();\r\n    attemptInfo.trackerName = StringInterner.weakIntern(event.getTrackerName());\r\n    attemptInfo.taskType = event.getTaskType();\r\n    attemptInfo.shufflePort = event.getShufflePort();\r\n    attemptInfo.containerId = event.getContainerId();\r\n    taskInfo.attemptsMap.put(attemptId, attemptInfo);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "handleTaskFinishedEvent",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void handleTaskFinishedEvent(TaskFinishedEvent event)\n{\r\n    TaskInfo taskInfo = info.tasksMap.get(event.getTaskId());\r\n    taskInfo.counters = event.getCounters();\r\n    taskInfo.finishTime = event.getFinishTime();\r\n    taskInfo.status = TaskStatus.State.SUCCEEDED.toString();\r\n    taskInfo.successfulAttemptId = event.getSuccessfulTaskAttemptId();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "handleTaskUpdatedEvent",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void handleTaskUpdatedEvent(TaskUpdatedEvent event)\n{\r\n    TaskInfo taskInfo = info.tasksMap.get(event.getTaskId());\r\n    taskInfo.finishTime = event.getFinishTime();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "handleTaskFailedEvent",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void handleTaskFailedEvent(TaskFailedEvent event)\n{\r\n    TaskInfo taskInfo = info.tasksMap.get(event.getTaskId());\r\n    taskInfo.status = TaskStatus.State.FAILED.toString();\r\n    taskInfo.finishTime = event.getFinishTime();\r\n    taskInfo.error = StringInterner.weakIntern(event.getError());\r\n    taskInfo.failedDueToAttemptId = event.getFailedAttemptID();\r\n    taskInfo.counters = event.getCounters();\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "handleTaskStartedEvent",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void handleTaskStartedEvent(TaskStartedEvent event)\n{\r\n    TaskInfo taskInfo = new TaskInfo();\r\n    taskInfo.taskId = event.getTaskId();\r\n    taskInfo.startTime = event.getStartTime();\r\n    taskInfo.taskType = event.getTaskType();\r\n    taskInfo.splitLocations = event.getSplitLocations();\r\n    info.tasksMap.put(event.getTaskId(), taskInfo);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "handleJobFailedEvent",
  "errType" : null,
  "containingMethodsNum" : 9,
  "sourceCodeText" : "void handleJobFailedEvent(JobUnsuccessfulCompletionEvent event)\n{\r\n    info.finishTime = event.getFinishTime();\r\n    info.succeededMaps = event.getSucceededMaps();\r\n    info.succeededReduces = event.getSucceededReduces();\r\n    info.failedMaps = event.getFailedMaps();\r\n    info.failedReduces = event.getFailedReduces();\r\n    info.killedMaps = event.getKilledMaps();\r\n    info.killedReduces = event.getKilledReduces();\r\n    info.jobStatus = StringInterner.weakIntern(event.getStatus());\r\n    info.errorInfo = StringInterner.weakIntern(event.getDiagnostics());\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "handleJobFinishedEvent",
  "errType" : null,
  "containingMethodsNum" : 11,
  "sourceCodeText" : "void handleJobFinishedEvent(JobFinishedEvent event)\n{\r\n    info.finishTime = event.getFinishTime();\r\n    info.succeededMaps = event.getSucceededMaps();\r\n    info.succeededReduces = event.getSucceededReduces();\r\n    info.failedMaps = event.getFailedMaps();\r\n    info.failedReduces = event.getFailedReduces();\r\n    info.killedMaps = event.getKilledMaps();\r\n    info.killedReduces = event.getKilledReduces();\r\n    info.totalCounters = event.getTotalCounters();\r\n    info.mapCounters = event.getMapCounters();\r\n    info.reduceCounters = event.getReduceCounters();\r\n    info.jobStatus = JobStatus.getJobRunState(JobStatus.SUCCEEDED);\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "handleJobPriorityChangeEvent",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void handleJobPriorityChangeEvent(JobPriorityChangeEvent event)\n{\r\n    info.priority = event.getPriority();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "handleJobQueueChangeEvent",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void handleJobQueueChangeEvent(JobQueueChangeEvent event)\n{\r\n    info.jobQueueName = event.getJobQueueName();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "handleJobInitedEvent",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void handleJobInitedEvent(JobInitedEvent event)\n{\r\n    info.launchTime = event.getLaunchTime();\r\n    info.totalMaps = event.getTotalMaps();\r\n    info.totalReduces = event.getTotalReduces();\r\n    info.uberized = event.getUberized();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "handleAMStartedEvent",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void handleAMStartedEvent(AMStartedEvent event)\n{\r\n    AMInfo amInfo = new AMInfo();\r\n    amInfo.appAttemptId = event.getAppAttemptId();\r\n    amInfo.startTime = event.getStartTime();\r\n    amInfo.containerId = event.getContainerId();\r\n    amInfo.nodeManagerHost = StringInterner.weakIntern(event.getNodeManagerHost());\r\n    amInfo.nodeManagerPort = event.getNodeManagerPort();\r\n    amInfo.nodeManagerHttpPort = event.getNodeManagerHttpPort();\r\n    if (info.amInfos == null) {\r\n        info.amInfos = new LinkedList<AMInfo>();\r\n    }\r\n    info.amInfos.add(amInfo);\r\n    info.latestAmInfo = amInfo;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "handleJobInfoChangeEvent",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void handleJobInfoChangeEvent(JobInfoChangeEvent event)\n{\r\n    info.submitTime = event.getSubmitTime();\r\n    info.launchTime = event.getLaunchTime();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "handleJobSubmittedEvent",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void handleJobSubmittedEvent(JobSubmittedEvent event)\n{\r\n    info.jobid = event.getJobId();\r\n    info.jobname = event.getJobName();\r\n    info.username = StringInterner.weakIntern(event.getUserName());\r\n    info.submitTime = event.getSubmitTime();\r\n    info.jobConfPath = event.getJobConfPath();\r\n    info.jobACLs = event.getJobAcls();\r\n    info.jobQueueName = StringInterner.weakIntern(event.getJobQueueName());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred\\lib",
  "methodName" : "addMapper",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void addMapper(JobConf job, Class<? extends Mapper<K1, V1, K2, V2>> klass, Class<? extends K1> inputKeyClass, Class<? extends V1> inputValueClass, Class<? extends K2> outputKeyClass, Class<? extends V2> outputValueClass, boolean byValue, JobConf mapperConf)\n{\r\n    job.setMapperClass(ChainMapper.class);\r\n    job.setMapOutputKeyClass(outputKeyClass);\r\n    job.setMapOutputValueClass(outputValueClass);\r\n    Chain.addMapper(true, job, klass, inputKeyClass, inputValueClass, outputKeyClass, outputValueClass, byValue, mapperConf);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred\\lib",
  "methodName" : "configure",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void configure(JobConf job)\n{\r\n    chain.configure(job);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred\\lib",
  "methodName" : "map",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void map(Object key, Object value, OutputCollector output, Reporter reporter) throws IOException\n{\r\n    Mapper mapper = chain.getFirstMap();\r\n    if (mapper != null) {\r\n        mapper.map(key, value, chain.getMapperCollector(0, output, reporter), reporter);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred\\lib",
  "methodName" : "close",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void close() throws IOException\n{\r\n    chain.close();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\reduce",
  "methodName" : "getReducerContext",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Reducer<KEYIN, VALUEIN, KEYOUT, VALUEOUT>.Context getReducerContext(ReduceContext<KEYIN, VALUEIN, KEYOUT, VALUEOUT> reduceContext)\n{\r\n    return new Context(reduceContext);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred\\pipes",
  "methodName" : "configure",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void configure(JobConf job)\n{\r\n    this.job = job;\r\n    SkipBadRecords.setAutoIncrReducerProcCount(job, false);\r\n    skipping = job.getBoolean(MRJobConfig.SKIP_RECORDS, false);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred\\pipes",
  "methodName" : "reduce",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void reduce(K2 key, Iterator<V2> values, OutputCollector<K3, V3> output, Reporter reporter) throws IOException\n{\r\n    isOk = false;\r\n    startApplication(output, reporter);\r\n    downlink.reduceKey(key);\r\n    while (values.hasNext()) {\r\n        downlink.reduceValue(values.next());\r\n    }\r\n    if (skipping) {\r\n        downlink.flush();\r\n    }\r\n    isOk = true;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred\\pipes",
  "methodName" : "startApplication",
  "errType" : [ "InterruptedException" ],
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void startApplication(OutputCollector<K3, V3> output, Reporter reporter) throws IOException\n{\r\n    if (application == null) {\r\n        try {\r\n            LOG.info(\"starting application\");\r\n            application = new Application<K2, V2, K3, V3>(job, null, output, reporter, (Class<? extends K3>) job.getOutputKeyClass(), (Class<? extends V3>) job.getOutputValueClass());\r\n            downlink = application.getDownlink();\r\n        } catch (InterruptedException ie) {\r\n            throw new RuntimeException(\"interrupted\", ie);\r\n        }\r\n        int reduce = 0;\r\n        downlink.runReduce(reduce, Submitter.getIsJavaRecordWriter(job));\r\n    }\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred\\pipes",
  "methodName" : "close",
  "errType" : [ "Throwable" ],
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void close() throws IOException\n{\r\n    if (isOk) {\r\n        OutputCollector<K3, V3> nullCollector = new OutputCollector<K3, V3>() {\r\n\r\n            public void collect(K3 key, V3 value) throws IOException {\r\n            }\r\n        };\r\n        startApplication(nullCollector, Reporter.NULL);\r\n    }\r\n    try {\r\n        if (isOk) {\r\n            application.getDownlink().endOfInput();\r\n        } else {\r\n            application.getDownlink().abort();\r\n        }\r\n        LOG.info(\"waiting for finish\");\r\n        application.waitForFinish();\r\n        LOG.info(\"got done\");\r\n    } catch (Throwable t) {\r\n        application.abort(t);\r\n    } finally {\r\n        application.cleanup();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\join",
  "methodName" : "has",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "boolean has(int i)\n{\r\n    return written.get(i);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\join",
  "methodName" : "get",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Writable get(int i)\n{\r\n    return values[i];\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\join",
  "methodName" : "size",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int size()\n{\r\n    return values.length;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\join",
  "methodName" : "equals",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "boolean equals(Object other)\n{\r\n    if (other instanceof TupleWritable) {\r\n        TupleWritable that = (TupleWritable) other;\r\n        if (!this.written.equals(that.written)) {\r\n            return false;\r\n        }\r\n        for (int i = 0; i < values.length; ++i) {\r\n            if (!has(i))\r\n                continue;\r\n            if (!values[i].equals(that.get(i))) {\r\n                return false;\r\n            }\r\n        }\r\n        return true;\r\n    }\r\n    return false;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\join",
  "methodName" : "hashCode",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "int hashCode()\n{\r\n    assert false : \"hashCode not designed\";\r\n    return written.hashCode();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\join",
  "methodName" : "iterator",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "Iterator<Writable> iterator()\n{\r\n    final TupleWritable t = this;\r\n    return new Iterator<Writable>() {\r\n\r\n        int bitIndex = written.nextSetBit(0);\r\n\r\n        public boolean hasNext() {\r\n            return bitIndex >= 0;\r\n        }\r\n\r\n        public Writable next() {\r\n            int returnIndex = bitIndex;\r\n            if (returnIndex < 0)\r\n                throw new NoSuchElementException();\r\n            bitIndex = written.nextSetBit(bitIndex + 1);\r\n            return t.get(returnIndex);\r\n        }\r\n\r\n        public void remove() {\r\n            if (!written.get(bitIndex)) {\r\n                throw new IllegalStateException(\"Attempt to remove non-existent val\");\r\n            }\r\n            written.clear(bitIndex);\r\n        }\r\n    };\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\join",
  "methodName" : "toString",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "String toString()\n{\r\n    StringBuffer buf = new StringBuffer(\"[\");\r\n    for (int i = 0; i < values.length; ++i) {\r\n        buf.append(has(i) ? values[i].toString() : \"\");\r\n        buf.append(\",\");\r\n    }\r\n    if (values.length != 0)\r\n        buf.setCharAt(buf.length() - 1, ']');\r\n    else\r\n        buf.append(']');\r\n    return buf.toString();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\join",
  "methodName" : "write",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void write(DataOutput out) throws IOException\n{\r\n    WritableUtils.writeVInt(out, values.length);\r\n    writeBitSet(out, values.length, written);\r\n    for (int i = 0; i < values.length; ++i) {\r\n        Text.writeString(out, values[i].getClass().getName());\r\n    }\r\n    for (int i = 0; i < values.length; ++i) {\r\n        if (has(i)) {\r\n            values[i].write(out);\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\join",
  "methodName" : "readFields",
  "errType" : [ "ClassNotFoundException", "IllegalAccessException", "InstantiationException" ],
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void readFields(DataInput in) throws IOException\n{\r\n    int card = WritableUtils.readVInt(in);\r\n    values = new Writable[card];\r\n    readBitSet(in, card, written);\r\n    Class<? extends Writable>[] cls = new Class[card];\r\n    try {\r\n        for (int i = 0; i < card; ++i) {\r\n            cls[i] = Class.forName(Text.readString(in)).asSubclass(Writable.class);\r\n        }\r\n        for (int i = 0; i < card; ++i) {\r\n            if (cls[i].equals(NullWritable.class)) {\r\n                values[i] = NullWritable.get();\r\n            } else {\r\n                values[i] = cls[i].newInstance();\r\n            }\r\n            if (has(i)) {\r\n                values[i].readFields(in);\r\n            }\r\n        }\r\n    } catch (ClassNotFoundException e) {\r\n        throw new IOException(\"Failed tuple init\", e);\r\n    } catch (IllegalAccessException e) {\r\n        throw new IOException(\"Failed tuple init\", e);\r\n    } catch (InstantiationException e) {\r\n        throw new IOException(\"Failed tuple init\", e);\r\n    }\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\join",
  "methodName" : "setWritten",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setWritten(int i)\n{\r\n    written.set(i);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\join",
  "methodName" : "clearWritten",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void clearWritten(int i)\n{\r\n    written.clear(i);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\join",
  "methodName" : "clearWritten",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void clearWritten()\n{\r\n    written.clear();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\join",
  "methodName" : "writeBitSet",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void writeBitSet(DataOutput stream, int nbits, BitSet bitSet) throws IOException\n{\r\n    long bits = 0L;\r\n    int bitSetIndex = bitSet.nextSetBit(0);\r\n    for (; bitSetIndex >= 0 && bitSetIndex < Long.SIZE; bitSetIndex = bitSet.nextSetBit(bitSetIndex + 1)) {\r\n        bits |= 1L << bitSetIndex;\r\n    }\r\n    WritableUtils.writeVLong(stream, bits);\r\n    if (nbits > Long.SIZE) {\r\n        bits = 0L;\r\n        for (int lastWordWritten = 0; bitSetIndex >= 0 && bitSetIndex < nbits; bitSetIndex = bitSet.nextSetBit(bitSetIndex + 1)) {\r\n            int bitsIndex = bitSetIndex % Byte.SIZE;\r\n            int word = (bitSetIndex - Long.SIZE) / Byte.SIZE;\r\n            if (word > lastWordWritten) {\r\n                stream.writeByte((byte) bits);\r\n                bits = 0L;\r\n                for (lastWordWritten++; lastWordWritten < word; lastWordWritten++) {\r\n                    stream.writeByte((byte) bits);\r\n                }\r\n            }\r\n            bits |= 1L << bitsIndex;\r\n        }\r\n        stream.writeByte((byte) bits);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\join",
  "methodName" : "readBitSet",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void readBitSet(DataInput stream, int nbits, BitSet bitSet) throws IOException\n{\r\n    bitSet.clear();\r\n    long initialBits = WritableUtils.readVLong(stream);\r\n    long last = 0L;\r\n    while (0L != initialBits) {\r\n        last = Long.lowestOneBit(initialBits);\r\n        initialBits ^= last;\r\n        bitSet.set(Long.numberOfTrailingZeros(last));\r\n    }\r\n    for (int offset = Long.SIZE; offset < nbits; offset += Byte.SIZE) {\r\n        byte bits = stream.readByte();\r\n        while (0 != bits) {\r\n            last = Long.lowestOneBit(bits);\r\n            bits ^= last;\r\n            bitSet.set(Long.numberOfTrailingZeros(last) + offset);\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getFileDescriptorIfAvail",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 3,
  "sourceCodeText" : "FileDescriptor getFileDescriptorIfAvail(InputStream in)\n{\r\n    FileDescriptor fd = null;\r\n    try {\r\n        if (in instanceof HasFileDescriptor) {\r\n            fd = ((HasFileDescriptor) in).getFileDescriptor();\r\n        } else if (in instanceof FileInputStream) {\r\n            fd = ((FileInputStream) in).getFD();\r\n        }\r\n    } catch (IOException e) {\r\n        LOG.info(\"Unable to determine FileDescriptor\", e);\r\n    }\r\n    return fd;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "close",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void close() throws IOException\n{\r\n    if (curReadahead != null) {\r\n        curReadahead.cancel();\r\n    }\r\n    if (currentOffset < dataLength) {\r\n        byte[] t = new byte[Math.min((int) (Integer.MAX_VALUE & (dataLength - currentOffset)), 32 * 1024)];\r\n        while (currentOffset < dataLength) {\r\n            int n = read(t, 0, t.length);\r\n            if (0 == n) {\r\n                throw new EOFException(\"Could not validate checksum\");\r\n            }\r\n        }\r\n    }\r\n    in.close();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "skip",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long skip(long n) throws IOException\n{\r\n    throw new IOException(\"Skip not supported for IFileInputStream\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getPosition",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getPosition()\n{\r\n    return (currentOffset >= dataLength) ? dataLength : currentOffset;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getSize",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getSize()\n{\r\n    return checksumSize;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "read",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "int read(byte[] b, int off, int len) throws IOException\n{\r\n    if (currentOffset >= dataLength) {\r\n        return -1;\r\n    }\r\n    doReadahead();\r\n    return doRead(b, off, len);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "doReadahead",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void doReadahead()\n{\r\n    if (raPool != null && inFd != null && readahead) {\r\n        curReadahead = raPool.readaheadStream(\"ifile\", inFd, currentOffset, readaheadLength, dataLength, curReadahead);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "readWithChecksum",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "int readWithChecksum(byte[] b, int off, int len) throws IOException\n{\r\n    if (currentOffset == length) {\r\n        return -1;\r\n    } else if (currentOffset >= dataLength) {\r\n        int lenToCopy = (int) (checksumSize - (currentOffset - dataLength));\r\n        if (len < lenToCopy) {\r\n            lenToCopy = len;\r\n        }\r\n        System.arraycopy(csum, (int) (currentOffset - dataLength), b, off, lenToCopy);\r\n        currentOffset += lenToCopy;\r\n        return lenToCopy;\r\n    }\r\n    int bytesRead = doRead(b, off, len);\r\n    if (currentOffset == dataLength) {\r\n        if (len >= bytesRead + checksumSize) {\r\n            System.arraycopy(csum, 0, b, off + bytesRead, checksumSize);\r\n            bytesRead += checksumSize;\r\n            currentOffset += checksumSize;\r\n        }\r\n    }\r\n    return bytesRead;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "doRead",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "int doRead(byte[] b, int off, int len) throws IOException\n{\r\n    if (currentOffset + len > dataLength) {\r\n        len = (int) dataLength - (int) currentOffset;\r\n    }\r\n    int bytesRead = in.read(b, off, len);\r\n    if (bytesRead < 0) {\r\n        throw new ChecksumException(\"Checksum Error\", 0);\r\n    }\r\n    sum.update(b, off, bytesRead);\r\n    currentOffset += bytesRead;\r\n    if (disableChecksumValidation) {\r\n        return bytesRead;\r\n    }\r\n    if (currentOffset == dataLength) {\r\n        csum = new byte[checksumSize];\r\n        IOUtils.readFully(in, csum, 0, checksumSize);\r\n        if (!sum.compare(csum, 0)) {\r\n            throw new ChecksumException(\"Checksum Error\", 0);\r\n        }\r\n    }\r\n    return bytesRead;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "read",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "int read() throws IOException\n{\r\n    b[0] = 0;\r\n    int l = read(b, 0, 1);\r\n    if (l < 0)\r\n        return l;\r\n    int result = 0xFF & b[0];\r\n    return result;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getChecksum",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "byte[] getChecksum()\n{\r\n    return csum;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "disableChecksumValidation",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void disableChecksumValidation()\n{\r\n    disableChecksumValidation = true;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest\\files",
  "methodName" : "setSource",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setSource(final String source)\n{\r\n    this.source = source;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest\\files",
  "methodName" : "getSource",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getSource()\n{\r\n    return source;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest\\files",
  "methodName" : "getSourcePath",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Path getSourcePath()\n{\r\n    return unmarshallPath(source);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest\\files",
  "methodName" : "setDest",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setDest(final String dest)\n{\r\n    this.dest = dest;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest\\files",
  "methodName" : "getDest",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getDest()\n{\r\n    return dest;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest\\files",
  "methodName" : "getDestPath",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Path getDestPath()\n{\r\n    return unmarshallPath(dest);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest\\files",
  "methodName" : "getSize",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getSize()\n{\r\n    return size;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest\\files",
  "methodName" : "setSize",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setSize(final long size)\n{\r\n    this.size = size;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest\\files",
  "methodName" : "getEtag",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getEtag()\n{\r\n    return etag;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest\\files",
  "methodName" : "setEtag",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setEtag(final String etag)\n{\r\n    this.etag = etag;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest\\files",
  "methodName" : "validate",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void validate() throws IOException\n{\r\n    final String s = toString();\r\n    verify(source != null && source.length() > 0, \"Source is missing from \" + s);\r\n    verify(dest != null && dest.length() > 0, \"Source is missing from \" + s);\r\n    verify(size >= 0, \"Invalid size in \" + s);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest\\files",
  "methodName" : "toString",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "String toString()\n{\r\n    final StringBuilder sb = new StringBuilder(\"FileOrDirEntry{\");\r\n    sb.append(\"source='\").append(source).append('\\'');\r\n    sb.append(\", dest='\").append(dest).append('\\'');\r\n    sb.append(\", size=\").append(size);\r\n    sb.append(\", etag='\").append(etag).append('\\'');\r\n    sb.append('}');\r\n    return sb.toString();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest\\files",
  "methodName" : "equals",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "boolean equals(Object o)\n{\r\n    if (this == o) {\r\n        return true;\r\n    }\r\n    if (o == null || getClass() != o.getClass()) {\r\n        return false;\r\n    }\r\n    FileEntry that = (FileEntry) o;\r\n    return size == that.size && source.equals(that.source) && dest.equals(that.dest) && Objects.equals(etag, that.etag);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest\\files",
  "methodName" : "hashCode",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "int hashCode()\n{\r\n    return Objects.hash(source, dest);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred\\lib",
  "methodName" : "configure",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void configure(JobConf job)\n{\r\n    super.setConf(job);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred\\lib",
  "methodName" : "next",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "boolean next(K key, V value) throws IOException\n{\r\n    while ((curReader == null) || !curReader.next(key, value)) {\r\n        if (!initNextRecordReader()) {\r\n            return false;\r\n        }\r\n    }\r\n    return true;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred\\lib",
  "methodName" : "createKey",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "K createKey()\n{\r\n    return curReader.createKey();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred\\lib",
  "methodName" : "createValue",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "V createValue()\n{\r\n    return curReader.createValue();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred\\lib",
  "methodName" : "getPos",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getPos() throws IOException\n{\r\n    return progress;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred\\lib",
  "methodName" : "close",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void close() throws IOException\n{\r\n    if (curReader != null) {\r\n        curReader.close();\r\n        curReader = null;\r\n    }\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred\\lib",
  "methodName" : "getProgress",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "float getProgress() throws IOException\n{\r\n    return Math.min(1.0f, progress / (float) (split.getLength()));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred\\lib",
  "methodName" : "initNextRecordReader",
  "errType" : [ "Exception" ],
  "containingMethodsNum" : 8,
  "sourceCodeText" : "boolean initNextRecordReader() throws IOException\n{\r\n    if (curReader != null) {\r\n        curReader.close();\r\n        curReader = null;\r\n        if (idx > 0) {\r\n            progress += split.getLength(idx - 1);\r\n        }\r\n    }\r\n    if (idx == split.getNumPaths()) {\r\n        return false;\r\n    }\r\n    reporter.progress();\r\n    try {\r\n        curReader = rrConstructor.newInstance(new Object[] { split, jc, reporter, Integer.valueOf(idx) });\r\n        jc.set(JobContext.MAP_INPUT_FILE, split.getPath(idx).toString());\r\n        jc.setLong(JobContext.MAP_INPUT_START, split.getOffset(idx));\r\n        jc.setLong(JobContext.MAP_INPUT_PATH, split.getLength(idx));\r\n    } catch (Exception e) {\r\n        throw new RuntimeException(e);\r\n    }\r\n    idx++;\r\n    return true;\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\security\\token",
  "methodName" : "createSecretKey",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "SecretKey createSecretKey(byte[] key)\n{\r\n    return SecretManager.createSecretKey(key);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\security\\token",
  "methodName" : "computeHash",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "byte[] computeHash(byte[] msg, SecretKey key)\n{\r\n    return createPassword(msg, key);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\security\\token",
  "methodName" : "createPassword",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "byte[] createPassword(JobTokenIdentifier identifier)\n{\r\n    byte[] result = createPassword(identifier.getBytes(), masterKey);\r\n    return result;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\security\\token",
  "methodName" : "addTokenForJob",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void addTokenForJob(String jobId, Token<JobTokenIdentifier> token)\n{\r\n    SecretKey tokenSecret = createSecretKey(token.getPassword());\r\n    synchronized (currentJobTokens) {\r\n        currentJobTokens.put(jobId, tokenSecret);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\security\\token",
  "methodName" : "removeTokenForJob",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void removeTokenForJob(String jobId)\n{\r\n    synchronized (currentJobTokens) {\r\n        currentJobTokens.remove(jobId);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\security\\token",
  "methodName" : "retrieveTokenSecret",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "SecretKey retrieveTokenSecret(String jobId) throws InvalidToken\n{\r\n    SecretKey tokenSecret = null;\r\n    synchronized (currentJobTokens) {\r\n        tokenSecret = currentJobTokens.get(jobId);\r\n    }\r\n    if (tokenSecret == null) {\r\n        throw new InvalidToken(\"Can't find job token for job \" + jobId + \" !!\");\r\n    }\r\n    return tokenSecret;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\security\\token",
  "methodName" : "retrievePassword",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "byte[] retrievePassword(JobTokenIdentifier identifier) throws InvalidToken\n{\r\n    return retrieveTokenSecret(identifier.getJobId().toString()).getEncoded();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\security\\token",
  "methodName" : "createIdentifier",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "JobTokenIdentifier createIdentifier()\n{\r\n    return new JobTokenIdentifier();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "downgrade",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "TaskID downgrade(org.apache.hadoop.mapreduce.TaskID old)\n{\r\n    if (old instanceof TaskID) {\r\n        return (TaskID) old;\r\n    } else {\r\n        return new TaskID(JobID.downgrade(old.getJobID()), old.getTaskType(), old.getId());\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "read",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "TaskID read(DataInput in) throws IOException\n{\r\n    TaskID tipId = new TaskID();\r\n    tipId.readFields(in);\r\n    return tipId;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getJobID",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "JobID getJobID()\n{\r\n    return (JobID) super.getJobID();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getTaskIDsPattern",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String getTaskIDsPattern(String jtIdentifier, Integer jobId, Boolean isMap, Integer taskId)\n{\r\n    return getTaskIDsPattern(jtIdentifier, jobId, isMap ? TaskType.MAP : TaskType.REDUCE, taskId);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getTaskIDsPattern",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "String getTaskIDsPattern(String jtIdentifier, Integer jobId, TaskType type, Integer taskId)\n{\r\n    StringBuilder builder = new StringBuilder(TASK).append(SEPARATOR).append(getTaskIDsPatternWOPrefix(jtIdentifier, jobId, type, taskId));\r\n    return builder.toString();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getTaskIDsPatternWOPrefix",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "StringBuilder getTaskIDsPatternWOPrefix(String jtIdentifier, Integer jobId, TaskType type, Integer taskId)\n{\r\n    StringBuilder builder = new StringBuilder();\r\n    builder.append(JobID.getJobIDsPatternWOPrefix(jtIdentifier, jobId)).append(SEPARATOR).append(type != null ? (org.apache.hadoop.mapreduce.TaskID.getRepresentingCharacter(type)) : org.apache.hadoop.mapreduce.TaskID.getAllTaskTypes()).append(SEPARATOR).append(taskId != null ? idFormat.format(taskId) : \"[0-9]*\");\r\n    return builder;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "forName",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "TaskID forName(String str) throws IllegalArgumentException\n{\r\n    return (TaskID) org.apache.hadoop.mapreduce.TaskID.forName(str);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "init",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void init(JobConf conf) throws IOException\n{\r\n    setConf(conf);\r\n    jc = new JobClient(conf);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "run",
  "errType" : null,
  "containingMethodsNum" : 15,
  "sourceCodeText" : "int run(String[] argv) throws Exception\n{\r\n    int exitcode = -1;\r\n    if (argv.length < 1) {\r\n        displayUsage(\"\");\r\n        return exitcode;\r\n    }\r\n    String cmd = argv[0];\r\n    boolean displayQueueList = false;\r\n    boolean displayQueueInfoWithJobs = false;\r\n    boolean displayQueueInfoWithoutJobs = false;\r\n    boolean displayQueueAclsInfoForCurrentUser = false;\r\n    if (\"-list\".equals(cmd)) {\r\n        displayQueueList = true;\r\n    } else if (\"-showacls\".equals(cmd)) {\r\n        displayQueueAclsInfoForCurrentUser = true;\r\n    } else if (\"-info\".equals(cmd)) {\r\n        if (argv.length == 2 && !(argv[1].equals(\"-showJobs\"))) {\r\n            displayQueueInfoWithoutJobs = true;\r\n        } else if (argv.length == 3) {\r\n            if (argv[2].equals(\"-showJobs\")) {\r\n                displayQueueInfoWithJobs = true;\r\n            } else {\r\n                displayUsage(cmd);\r\n                return exitcode;\r\n            }\r\n        } else {\r\n            displayUsage(cmd);\r\n            return exitcode;\r\n        }\r\n    } else {\r\n        displayUsage(cmd);\r\n        return exitcode;\r\n    }\r\n    JobConf conf = new JobConf(getConf());\r\n    init(conf);\r\n    if (displayQueueList) {\r\n        displayQueueList();\r\n        exitcode = 0;\r\n    } else if (displayQueueInfoWithoutJobs) {\r\n        displayQueueInfo(argv[1], false);\r\n        exitcode = 0;\r\n    } else if (displayQueueInfoWithJobs) {\r\n        displayQueueInfo(argv[1], true);\r\n        exitcode = 0;\r\n    } else if (displayQueueAclsInfoForCurrentUser) {\r\n        this.displayQueueAclsInfoForCurrentUser();\r\n        exitcode = 0;\r\n    }\r\n    return exitcode;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "printJobQueueInfo",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void printJobQueueInfo(JobQueueInfo jobQueueInfo, Writer writer) throws IOException\n{\r\n    printJobQueueInfo(jobQueueInfo, writer, \"\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "printJobQueueInfo",
  "errType" : null,
  "containingMethodsNum" : 11,
  "sourceCodeText" : "void printJobQueueInfo(JobQueueInfo jobQueueInfo, Writer writer, String prefix) throws IOException\n{\r\n    if (jobQueueInfo == null) {\r\n        writer.write(\"No queue found.\\n\");\r\n        writer.flush();\r\n        return;\r\n    }\r\n    writer.write(String.format(prefix + \"======================\\n\"));\r\n    writer.write(String.format(prefix + \"Queue Name : %s \\n\", jobQueueInfo.getQueueName()));\r\n    writer.write(String.format(prefix + \"Queue State : %s \\n\", jobQueueInfo.getQueueState()));\r\n    writer.write(String.format(prefix + \"Scheduling Info : %s \\n\", jobQueueInfo.getSchedulingInfo()));\r\n    List<JobQueueInfo> childQueues = jobQueueInfo.getChildren();\r\n    if (childQueues != null && childQueues.size() > 0) {\r\n        for (int i = 0; i < childQueues.size(); i++) {\r\n            printJobQueueInfo(childQueues.get(i), writer, \"    \" + prefix);\r\n        }\r\n    }\r\n    writer.flush();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "displayQueueList",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void displayQueueList() throws IOException\n{\r\n    JobQueueInfo[] rootQueues = jc.getRootQueues();\r\n    for (JobQueueInfo queue : rootQueues) {\r\n        printJobQueueInfo(queue, new PrintWriter(new OutputStreamWriter(System.out, Charsets.UTF_8)));\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "expandQueueList",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "List<JobQueueInfo> expandQueueList(JobQueueInfo[] rootQueues)\n{\r\n    List<JobQueueInfo> allQueues = new ArrayList<JobQueueInfo>();\r\n    for (JobQueueInfo queue : rootQueues) {\r\n        allQueues.add(queue);\r\n        if (queue.getChildren() != null) {\r\n            JobQueueInfo[] childQueues = queue.getChildren().toArray(new JobQueueInfo[0]);\r\n            allQueues.addAll(expandQueueList(childQueues));\r\n        }\r\n    }\r\n    return allQueues;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "displayQueueInfo",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void displayQueueInfo(String queue, boolean showJobs) throws IOException, InterruptedException\n{\r\n    JobQueueInfo jobQueueInfo = jc.getQueueInfo(queue);\r\n    if (jobQueueInfo == null) {\r\n        System.out.println(\"Queue \\\"\" + queue + \"\\\" does not exist.\");\r\n        return;\r\n    }\r\n    printJobQueueInfo(jobQueueInfo, new PrintWriter(new OutputStreamWriter(System.out, Charsets.UTF_8)));\r\n    if (showJobs && (jobQueueInfo.getChildren() == null || jobQueueInfo.getChildren().size() == 0)) {\r\n        JobStatus[] jobs = jobQueueInfo.getJobStatuses();\r\n        if (jobs == null)\r\n            jobs = new JobStatus[0];\r\n        jc.displayJobList(jobs);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "displayQueueAclsInfoForCurrentUser",
  "errType" : null,
  "containingMethodsNum" : 12,
  "sourceCodeText" : "void displayQueueAclsInfoForCurrentUser() throws IOException\n{\r\n    QueueAclsInfo[] queueAclsInfoList = jc.getQueueAclsForCurrentUser();\r\n    UserGroupInformation ugi = UserGroupInformation.getCurrentUser();\r\n    if (queueAclsInfoList.length > 0) {\r\n        System.out.println(\"Queue acls for user :  \" + ugi.getShortUserName());\r\n        System.out.println(\"\\nQueue  Operations\");\r\n        System.out.println(\"=====================\");\r\n        for (QueueAclsInfo queueInfo : queueAclsInfoList) {\r\n            System.out.print(queueInfo.getQueueName() + \"  \");\r\n            String[] ops = queueInfo.getOperations();\r\n            Arrays.sort(ops);\r\n            int max = ops.length - 1;\r\n            for (int j = 0; j < ops.length; j++) {\r\n                System.out.print(ops[j].replaceFirst(\"acl-\", \"\"));\r\n                if (j < max) {\r\n                    System.out.print(\",\");\r\n                }\r\n            }\r\n            System.out.println();\r\n        }\r\n    } else {\r\n        System.out.println(\"User \" + ugi.getShortUserName() + \" does not have access to any queue. \\n\");\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "displayUsage",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void displayUsage(String cmd)\n{\r\n    String prefix = \"Usage: queue \";\r\n    if (\"-queueinfo\".equals(cmd)) {\r\n        System.err.println(prefix + \"[\" + cmd + \"<job-queue-name> [-showJobs]]\");\r\n    } else {\r\n        System.err.printf(prefix + \"<command> <args>%n\");\r\n        System.err.printf(\"\\t[-list]%n\");\r\n        System.err.printf(\"\\t[-info <job-queue-name> [-showJobs]]%n\");\r\n        System.err.printf(\"\\t[-showacls] %n%n\");\r\n        ToolRunner.printGenericCommandUsage(System.out);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "main",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void main(String[] argv) throws Exception\n{\r\n    int res = ToolRunner.run(new JobQueueClient(), argv);\r\n    System.exit(res);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\input",
  "methodName" : "createRecordReader",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "RecordReader<LongWritable, Text> createRecordReader(InputSplit split, TaskAttemptContext context)\n{\r\n    String delimiter = context.getConfiguration().get(\"textinputformat.record.delimiter\");\r\n    byte[] recordDelimiterBytes = null;\r\n    if (null != delimiter)\r\n        recordDelimiterBytes = delimiter.getBytes(Charsets.UTF_8);\r\n    return new LineRecordReader(recordDelimiterBytes);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\input",
  "methodName" : "isSplitable",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "boolean isSplitable(JobContext context, Path file)\n{\r\n    final CompressionCodec codec = new CompressionCodecFactory(context.getConfiguration()).getCodec(file);\r\n    if (null == codec) {\r\n        return true;\r\n    }\r\n    return codec instanceof SplittableCompressionCodec;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred\\lib",
  "methodName" : "getInputSplit",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "InputSplit getInputSplit()\n{\r\n    return inputSplit;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred\\lib",
  "methodName" : "getInputFormatClass",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Class<? extends InputFormat> getInputFormatClass()\n{\r\n    return inputFormatClass;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred\\lib",
  "methodName" : "getMapperClass",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Class<? extends Mapper> getMapperClass()\n{\r\n    return mapperClass;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred\\lib",
  "methodName" : "getLength",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "long getLength() throws IOException\n{\r\n    return inputSplit.getLength();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred\\lib",
  "methodName" : "getLocations",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String[] getLocations() throws IOException\n{\r\n    return inputSplit.getLocations();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred\\lib",
  "methodName" : "readFields",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void readFields(DataInput in) throws IOException\n{\r\n    inputSplitClass = (Class<? extends InputSplit>) readClass(in);\r\n    inputSplit = (InputSplit) ReflectionUtils.newInstance(inputSplitClass, conf);\r\n    inputSplit.readFields(in);\r\n    inputFormatClass = (Class<? extends InputFormat>) readClass(in);\r\n    mapperClass = (Class<? extends Mapper>) readClass(in);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred\\lib",
  "methodName" : "readClass",
  "errType" : [ "ClassNotFoundException" ],
  "containingMethodsNum" : 2,
  "sourceCodeText" : "Class<?> readClass(DataInput in) throws IOException\n{\r\n    String className = StringInterner.weakIntern(Text.readString(in));\r\n    try {\r\n        return conf.getClassByName(className);\r\n    } catch (ClassNotFoundException e) {\r\n        throw new RuntimeException(\"readObject can't find class\", e);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred\\lib",
  "methodName" : "write",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void write(DataOutput out) throws IOException\n{\r\n    Text.writeString(out, inputSplitClass.getName());\r\n    inputSplit.write(out);\r\n    Text.writeString(out, inputFormatClass.getName());\r\n    Text.writeString(out, mapperClass.getName());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred\\lib",
  "methodName" : "getConf",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Configuration getConf()\n{\r\n    return conf;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred\\lib",
  "methodName" : "setConf",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setConf(Configuration conf)\n{\r\n    this.conf = conf;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred\\lib",
  "methodName" : "toString",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String toString()\n{\r\n    return inputSplit.toString();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getOutputPath",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "Path getOutputPath(JobContext context)\n{\r\n    JobConf conf = context.getJobConf();\r\n    return FileOutputFormat.getOutputPath(conf);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getOutputPath",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "Path getOutputPath(TaskAttemptContext context)\n{\r\n    JobConf conf = context.getJobConf();\r\n    return FileOutputFormat.getOutputPath(conf);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getWrapped",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter getWrapped(JobContext context) throws IOException\n{\r\n    if (wrapped == null) {\r\n        wrapped = new org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter(getOutputPath(context), context);\r\n    }\r\n    return wrapped;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getWrapped",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter getWrapped(TaskAttemptContext context) throws IOException\n{\r\n    if (wrapped == null) {\r\n        wrapped = new org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter(getOutputPath(context), context);\r\n    }\r\n    return wrapped;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getJobAttemptPath",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "Path getJobAttemptPath(JobContext context)\n{\r\n    Path out = getOutputPath(context);\r\n    return out == null ? null : org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.getJobAttemptPath(context, out);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getTaskAttemptPath",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "Path getTaskAttemptPath(TaskAttemptContext context) throws IOException\n{\r\n    Path out = getOutputPath(context);\r\n    return out == null ? null : getTaskAttemptPath(context, out);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getTaskAttemptPath",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "Path getTaskAttemptPath(TaskAttemptContext context, Path out) throws IOException\n{\r\n    Path workPath = FileOutputFormat.getWorkOutputPath(context.getJobConf());\r\n    if (workPath == null && out != null) {\r\n        return org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.getTaskAttemptPath(context, out);\r\n    }\r\n    return workPath;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getCommittedTaskPath",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "Path getCommittedTaskPath(TaskAttemptContext context)\n{\r\n    Path out = getOutputPath(context);\r\n    return out == null ? null : org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.getCommittedTaskPath(context, out);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getWorkPath",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Path getWorkPath(TaskAttemptContext context, Path outputPath) throws IOException\n{\r\n    return outputPath == null ? null : getTaskAttemptPath(context, outputPath);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "setupJob",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setupJob(JobContext context) throws IOException\n{\r\n    getWrapped(context).setupJob(context);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "commitJob",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void commitJob(JobContext context) throws IOException\n{\r\n    getWrapped(context).commitJob(context);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "cleanupJob",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void cleanupJob(JobContext context) throws IOException\n{\r\n    getWrapped(context).cleanupJob(context);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "abortJob",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void abortJob(JobContext context, int runState) throws IOException\n{\r\n    JobStatus.State state;\r\n    if (runState == JobStatus.State.RUNNING.getValue()) {\r\n        state = JobStatus.State.RUNNING;\r\n    } else if (runState == JobStatus.State.SUCCEEDED.getValue()) {\r\n        state = JobStatus.State.SUCCEEDED;\r\n    } else if (runState == JobStatus.State.FAILED.getValue()) {\r\n        state = JobStatus.State.FAILED;\r\n    } else if (runState == JobStatus.State.PREP.getValue()) {\r\n        state = JobStatus.State.PREP;\r\n    } else if (runState == JobStatus.State.KILLED.getValue()) {\r\n        state = JobStatus.State.KILLED;\r\n    } else {\r\n        throw new IllegalArgumentException(runState + \" is not a valid runState.\");\r\n    }\r\n    getWrapped(context).abortJob(context, state);\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "setupTask",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setupTask(TaskAttemptContext context) throws IOException\n{\r\n    getWrapped(context).setupTask(context);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "commitTask",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void commitTask(TaskAttemptContext context) throws IOException\n{\r\n    getWrapped(context).commitTask(context, getTaskAttemptPath(context));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "abortTask",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void abortTask(TaskAttemptContext context) throws IOException\n{\r\n    getWrapped(context).abortTask(context, getTaskAttemptPath(context));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "needsTaskCommit",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "boolean needsTaskCommit(TaskAttemptContext context) throws IOException\n{\r\n    return getWrapped(context).needsTaskCommit(context, getTaskAttemptPath(context));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "isRecoverySupported",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean isRecoverySupported()\n{\r\n    return true;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "isCommitJobRepeatable",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "boolean isCommitJobRepeatable(JobContext context) throws IOException\n{\r\n    return getWrapped(context).isCommitJobRepeatable(context);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "isRecoverySupported",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "boolean isRecoverySupported(JobContext context) throws IOException\n{\r\n    return getWrapped(context).isRecoverySupported(context);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "recoverTask",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void recoverTask(TaskAttemptContext context) throws IOException\n{\r\n    getWrapped(context).recoverTask(context);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest\\stages",
  "methodName" : "getFilesCommitted",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "List<FileEntry> getFilesCommitted()\n{\r\n    return filesCommitted;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest\\stages",
  "methodName" : "addFileCommitted",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void addFileCommitted(FileEntry entry)\n{\r\n    filesCommitted.add(entry);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest\\stages",
  "methodName" : "executeStage",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "List<FileEntry> executeStage(final List<TaskManifest> taskManifests) throws IOException\n{\r\n    synchronized (this) {\r\n        filesCommitted = new ArrayList<>(taskManifests.size());\r\n    }\r\n    final Iterable<FileEntry> filesToCommit = concat(taskManifests.stream().map(TaskManifest::getFilesToCommit).collect(Collectors.toList()));\r\n    TaskPool.foreach(filesToCommit).executeWith(getIOProcessors()).stopOnFailure().run(this::validateOneFile);\r\n    return getFilesCommitted();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output\\committer\\manifest\\stages",
  "methodName" : "validateOneFile",
  "errType" : [ "FileNotFoundException" ],
  "containingMethodsNum" : 18,
  "sourceCodeText" : "void validateOneFile(FileEntry entry) throws IOException\n{\r\n    updateAuditContext(OP_STAGE_JOB_VALIDATE_OUTPUT);\r\n    if (halt.get()) {\r\n        return;\r\n    }\r\n    progress();\r\n    FileStatus destStatus;\r\n    final Path sourcePath = entry.getSourcePath();\r\n    Path destPath = entry.getDestPath();\r\n    try {\r\n        destStatus = getFileStatus(destPath);\r\n        if (!destStatus.isFile()) {\r\n            throw new OutputValidationException(destPath, \"Expected a file renamed from \" + sourcePath + \"; found \" + destStatus);\r\n        }\r\n        final long sourceSize = entry.getSize();\r\n        final long destSize = destStatus.getLen();\r\n        final String sourceEtag = entry.getEtag();\r\n        if (isNotBlank(sourceEtag)) {\r\n            final String destEtag = ManifestCommitterSupport.getEtag(destStatus);\r\n            if (!sourceEtag.equals(destEtag)) {\r\n                LOG.warn(\"Etag of dest file {}: {} does not match that of manifest entry {}\", destPath, destStatus, entry);\r\n                throw new OutputValidationException(destPath, String.format(\"Expected the file\" + \" renamed from %s\" + \" with etag %s and length %s\" + \" but found a file with etag %s and length %d\", sourcePath, sourceEtag, sourceSize, destEtag, destSize));\r\n            }\r\n        }\r\n        if (destSize != sourceSize) {\r\n            LOG.warn(\"Length of dest file {}: {} does not match that of manifest entry {}\", destPath, destStatus, entry);\r\n            throw new OutputValidationException(destPath, String.format(\"Expected the file\" + \" renamed from %s\" + \" with length %d\" + \" but found a file of length %d\", sourcePath, sourceSize, destSize));\r\n        }\r\n    } catch (FileNotFoundException e) {\r\n        throw new OutputValidationException(destPath, \"Expected a file, but it was not found\", e);\r\n    }\r\n    addFileCommitted(entry);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "setupJob",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setupJob(JobContext jobContext) throws IOException",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "cleanupJob",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void cleanupJob(JobContext jobContext) throws IOException\n{\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "commitJob",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void commitJob(JobContext jobContext) throws IOException\n{\r\n    cleanupJob(jobContext);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "abortJob",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void abortJob(JobContext jobContext, JobStatus.State state) throws IOException\n{\r\n    cleanupJob(jobContext);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "setupTask",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setupTask(TaskAttemptContext taskContext) throws IOException",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "needsTaskCommit",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean needsTaskCommit(TaskAttemptContext taskContext) throws IOException",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "commitTask",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void commitTask(TaskAttemptContext taskContext) throws IOException",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "abortTask",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void abortTask(TaskAttemptContext taskContext) throws IOException",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "isRecoverySupported",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean isRecoverySupported()\n{\r\n    return false;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "isCommitJobRepeatable",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean isCommitJobRepeatable(JobContext jobContext) throws IOException\n{\r\n    return false;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "isRecoverySupported",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "boolean isRecoverySupported(JobContext jobContext) throws IOException\n{\r\n    return isRecoverySupported();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "recoverTask",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void recoverTask(TaskAttemptContext taskContext) throws IOException\n{\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred\\lib",
  "methodName" : "getRecordWriter",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "RecordWriter<K, V> getRecordWriter(FileSystem ignored, JobConf job, String name, Progressable progress) throws IOException\n{\r\n    return getBaseOut().getRecordWriter(ignored, job, name, progress);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred\\lib",
  "methodName" : "checkOutputSpecs",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void checkOutputSpecs(FileSystem ignored, JobConf job) throws IOException\n{\r\n    getBaseOut().checkOutputSpecs(ignored, job);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred\\lib",
  "methodName" : "getBaseOut",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "OutputFormat<K, V> getBaseOut() throws IOException\n{\r\n    if (baseOut == null) {\r\n        throw new IOException(\"Outputformat not set for FilterOutputFormat\");\r\n    }\r\n    return baseOut;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "setLocalMapFiles",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setLocalMapFiles(Map<TaskAttemptID, MapOutputFile> mapFiles)\n{\r\n    this.localMapFiles = mapFiles;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "initCodec",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "CompressionCodec initCodec()\n{\r\n    if (conf.getCompressMapOutput()) {\r\n        Class<? extends CompressionCodec> codecClass = conf.getMapOutputCompressorClass(DefaultCodec.class);\r\n        return ReflectionUtils.newInstance(codecClass, conf);\r\n    }\r\n    return null;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "isMapTask",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean isMapTask()\n{\r\n    return false;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getNumMaps",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int getNumMaps()\n{\r\n    return numMaps;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "localizeConfiguration",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void localizeConfiguration(JobConf conf) throws IOException\n{\r\n    super.localizeConfiguration(conf);\r\n    conf.setNumMapTasks(numMaps);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "write",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void write(DataOutput out) throws IOException\n{\r\n    super.write(out);\r\n    out.writeInt(numMaps);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "readFields",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void readFields(DataInput in) throws IOException\n{\r\n    super.readFields(in);\r\n    numMaps = in.readInt();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getMapFiles",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "Path[] getMapFiles(FileSystem fs) throws IOException\n{\r\n    List<Path> fileList = new ArrayList<Path>();\r\n    for (int i = 0; i < numMaps; ++i) {\r\n        fileList.add(mapOutputFile.getInputFile(i));\r\n    }\r\n    return fileList.toArray(new Path[0]);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "run",
  "errType" : null,
  "containingMethodsNum" : 31,
  "sourceCodeText" : "void run(JobConf job, final TaskUmbilicalProtocol umbilical) throws IOException, InterruptedException, ClassNotFoundException\n{\r\n    job.setBoolean(JobContext.SKIP_RECORDS, isSkipping());\r\n    if (isMapOrReduce()) {\r\n        copyPhase = getProgress().addPhase(\"copy\");\r\n        sortPhase = getProgress().addPhase(\"sort\");\r\n        reducePhase = getProgress().addPhase(\"reduce\");\r\n    }\r\n    TaskReporter reporter = startReporter(umbilical);\r\n    boolean useNewApi = job.getUseNewReducer();\r\n    initialize(job, getJobID(), reporter, useNewApi);\r\n    if (jobCleanup) {\r\n        runJobCleanupTask(umbilical, reporter);\r\n        return;\r\n    }\r\n    if (jobSetup) {\r\n        runJobSetupTask(umbilical, reporter);\r\n        return;\r\n    }\r\n    if (taskCleanup) {\r\n        runTaskCleanupTask(umbilical, reporter);\r\n        return;\r\n    }\r\n    codec = initCodec();\r\n    RawKeyValueIterator rIter = null;\r\n    ShuffleConsumerPlugin shuffleConsumerPlugin = null;\r\n    Class combinerClass = conf.getCombinerClass();\r\n    CombineOutputCollector combineCollector = (null != combinerClass) ? new CombineOutputCollector(reduceCombineOutputCounter, reporter, conf) : null;\r\n    Class<? extends ShuffleConsumerPlugin> clazz = job.getClass(MRConfig.SHUFFLE_CONSUMER_PLUGIN, Shuffle.class, ShuffleConsumerPlugin.class);\r\n    shuffleConsumerPlugin = ReflectionUtils.newInstance(clazz, job);\r\n    LOG.info(\"Using ShuffleConsumerPlugin: \" + shuffleConsumerPlugin);\r\n    ShuffleConsumerPlugin.Context shuffleContext = new ShuffleConsumerPlugin.Context(getTaskID(), job, FileSystem.getLocal(job), umbilical, super.lDirAlloc, reporter, codec, combinerClass, combineCollector, spilledRecordsCounter, reduceCombineInputCounter, shuffledMapsCounter, reduceShuffleBytes, failedShuffleCounter, mergedMapOutputsCounter, taskStatus, copyPhase, sortPhase, this, mapOutputFile, localMapFiles);\r\n    shuffleConsumerPlugin.init(shuffleContext);\r\n    rIter = shuffleConsumerPlugin.run();\r\n    mapOutputFilesOnDisk.clear();\r\n    sortPhase.complete();\r\n    setPhase(TaskStatus.Phase.REDUCE);\r\n    statusUpdate(umbilical);\r\n    Class keyClass = job.getMapOutputKeyClass();\r\n    Class valueClass = job.getMapOutputValueClass();\r\n    RawComparator comparator = job.getOutputValueGroupingComparator();\r\n    if (useNewApi) {\r\n        runNewReducer(job, umbilical, reporter, rIter, comparator, keyClass, valueClass);\r\n    } else {\r\n        runOldReducer(job, umbilical, reporter, rIter, comparator, keyClass, valueClass);\r\n    }\r\n    shuffleConsumerPlugin.close();\r\n    done(umbilical, reporter);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "runOldReducer",
  "errType" : null,
  "containingMethodsNum" : 18,
  "sourceCodeText" : "void runOldReducer(JobConf job, TaskUmbilicalProtocol umbilical, final TaskReporter reporter, RawKeyValueIterator rIter, RawComparator<INKEY> comparator, Class<INKEY> keyClass, Class<INVALUE> valueClass) throws IOException\n{\r\n    Reducer<INKEY, INVALUE, OUTKEY, OUTVALUE> reducer = ReflectionUtils.newInstance(job.getReducerClass(), job);\r\n    String finalName = getOutputName(getPartition());\r\n    RecordWriter<OUTKEY, OUTVALUE> out = new OldTrackingRecordWriter<OUTKEY, OUTVALUE>(this, job, reporter, finalName);\r\n    final RecordWriter<OUTKEY, OUTVALUE> finalOut = out;\r\n    OutputCollector<OUTKEY, OUTVALUE> collector = new OutputCollector<OUTKEY, OUTVALUE>() {\r\n\r\n        public void collect(OUTKEY key, OUTVALUE value) throws IOException {\r\n            finalOut.write(key, value);\r\n            reporter.progress();\r\n        }\r\n    };\r\n    try {\r\n        boolean incrProcCount = SkipBadRecords.getReducerMaxSkipGroups(job) > 0 && SkipBadRecords.getAutoIncrReducerProcCount(job);\r\n        ReduceValuesIterator<INKEY, INVALUE> values = isSkipping() ? new SkippingReduceValuesIterator<INKEY, INVALUE>(rIter, comparator, keyClass, valueClass, job, reporter, umbilical) : new ReduceValuesIterator<INKEY, INVALUE>(rIter, comparator, keyClass, valueClass, job, reporter);\r\n        values.informReduceProgress();\r\n        while (values.more()) {\r\n            reduceInputKeyCounter.increment(1);\r\n            reducer.reduce(values.getKey(), values, collector, reporter);\r\n            if (incrProcCount) {\r\n                reporter.incrCounter(SkipBadRecords.COUNTER_GROUP, SkipBadRecords.COUNTER_REDUCE_PROCESSED_GROUPS, 1);\r\n            }\r\n            values.nextKey();\r\n            values.informReduceProgress();\r\n        }\r\n        reducer.close();\r\n        reducer = null;\r\n        out.close(reporter);\r\n        out = null;\r\n    } finally {\r\n        IOUtils.cleanupWithLogger(LOG, reducer);\r\n        closeQuietly(out, reporter);\r\n    }\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "runNewReducer",
  "errType" : null,
  "containingMethodsNum" : 13,
  "sourceCodeText" : "void runNewReducer(JobConf job, final TaskUmbilicalProtocol umbilical, final TaskReporter reporter, RawKeyValueIterator rIter, RawComparator<INKEY> comparator, Class<INKEY> keyClass, Class<INVALUE> valueClass) throws IOException, InterruptedException, ClassNotFoundException\n{\r\n    final RawKeyValueIterator rawIter = rIter;\r\n    rIter = new RawKeyValueIterator() {\r\n\r\n        public void close() throws IOException {\r\n            rawIter.close();\r\n        }\r\n\r\n        public DataInputBuffer getKey() throws IOException {\r\n            return rawIter.getKey();\r\n        }\r\n\r\n        public Progress getProgress() {\r\n            return rawIter.getProgress();\r\n        }\r\n\r\n        public DataInputBuffer getValue() throws IOException {\r\n            return rawIter.getValue();\r\n        }\r\n\r\n        public boolean next() throws IOException {\r\n            boolean ret = rawIter.next();\r\n            reporter.setProgress(rawIter.getProgress().getProgress());\r\n            return ret;\r\n        }\r\n    };\r\n    org.apache.hadoop.mapreduce.TaskAttemptContext taskContext = new org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl(job, getTaskID(), reporter);\r\n    org.apache.hadoop.mapreduce.Reducer<INKEY, INVALUE, OUTKEY, OUTVALUE> reducer = (org.apache.hadoop.mapreduce.Reducer<INKEY, INVALUE, OUTKEY, OUTVALUE>) ReflectionUtils.newInstance(taskContext.getReducerClass(), job);\r\n    org.apache.hadoop.mapreduce.RecordWriter<OUTKEY, OUTVALUE> trackedRW = new NewTrackingRecordWriter<OUTKEY, OUTVALUE>(this, taskContext);\r\n    job.setBoolean(\"mapred.skip.on\", isSkipping());\r\n    job.setBoolean(JobContext.SKIP_RECORDS, isSkipping());\r\n    org.apache.hadoop.mapreduce.Reducer.Context reducerContext = createReduceContext(reducer, job, getTaskID(), rIter, reduceInputKeyCounter, reduceInputValueCounter, trackedRW, committer, reporter, comparator, keyClass, valueClass);\r\n    try {\r\n        reducer.run(reducerContext);\r\n    } finally {\r\n        trackedRW.close(reducerContext);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "closeQuietly",
  "errType" : [ "Exception" ],
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void closeQuietly(RecordWriter<OUTKEY, OUTVALUE> c, Reporter r)\n{\r\n    if (c != null) {\r\n        try {\r\n            c.close(r);\r\n        } catch (Exception e) {\r\n            LOG.info(\"Exception in closing \" + c, e);\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "sort",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "RawKeyValueIterator sort()\n{\r\n    MergeSort m = new MergeSort(this);\r\n    int count = super.count;\r\n    if (count == 0)\r\n        return null;\r\n    int[] pointers = super.pointers;\r\n    int[] pointersCopy = new int[count];\r\n    System.arraycopy(pointers, 0, pointersCopy, 0, count);\r\n    m.mergeSort(pointers, pointersCopy, 0, count);\r\n    return new MRSortResultIterator(super.keyValBuffer, pointersCopy, super.startOffsets, super.keyLengths, super.valueLengths);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "compare",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "int compare(IntWritable i, IntWritable j)\n{\r\n    if (progressCalls < progressUpdateFrequency) {\r\n        progressCalls++;\r\n    } else {\r\n        progressCalls = 0;\r\n        reporter.progress();\r\n    }\r\n    return comparator.compare(keyValBuffer.getData(), startOffsets[i.get()], keyLengths[i.get()], keyValBuffer.getData(), startOffsets[j.get()], keyLengths[j.get()]);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getMemoryUtilized",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "long getMemoryUtilized()\n{\r\n    return super.getMemoryUtilized() + super.count * 4;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\db",
  "methodName" : "split",
  "errType" : [ "NullPointerException", "NullPointerException" ],
  "containingMethodsNum" : 17,
  "sourceCodeText" : "List<InputSplit> split(Configuration conf, ResultSet results, String colName) throws SQLException\n{\r\n    long minVal;\r\n    long maxVal;\r\n    int sqlDataType = results.getMetaData().getColumnType(1);\r\n    minVal = resultSetColToLong(results, 1, sqlDataType);\r\n    maxVal = resultSetColToLong(results, 2, sqlDataType);\r\n    String lowClausePrefix = colName + \" >= \";\r\n    String highClausePrefix = colName + \" < \";\r\n    int numSplits = conf.getInt(MRJobConfig.NUM_MAPS, 1);\r\n    if (numSplits < 1) {\r\n        numSplits = 1;\r\n    }\r\n    if (minVal == Long.MIN_VALUE && maxVal == Long.MIN_VALUE) {\r\n        List<InputSplit> splits = new ArrayList<InputSplit>();\r\n        splits.add(new DataDrivenDBInputFormat.DataDrivenDBInputSplit(colName + \" IS NULL\", colName + \" IS NULL\"));\r\n        return splits;\r\n    }\r\n    List<Long> splitPoints = split(numSplits, minVal, maxVal);\r\n    List<InputSplit> splits = new ArrayList<InputSplit>();\r\n    long start = splitPoints.get(0);\r\n    Date startDate = longToDate(start, sqlDataType);\r\n    if (sqlDataType == Types.TIMESTAMP) {\r\n        try {\r\n            ((java.sql.Timestamp) startDate).setNanos(results.getTimestamp(1).getNanos());\r\n        } catch (NullPointerException npe) {\r\n        }\r\n    }\r\n    for (int i = 1; i < splitPoints.size(); i++) {\r\n        long end = splitPoints.get(i);\r\n        Date endDate = longToDate(end, sqlDataType);\r\n        if (i == splitPoints.size() - 1) {\r\n            if (sqlDataType == Types.TIMESTAMP) {\r\n                try {\r\n                    ((java.sql.Timestamp) endDate).setNanos(results.getTimestamp(2).getNanos());\r\n                } catch (NullPointerException npe) {\r\n                }\r\n            }\r\n            splits.add(new DataDrivenDBInputFormat.DataDrivenDBInputSplit(lowClausePrefix + dateToString(startDate), colName + \" <= \" + dateToString(endDate)));\r\n        } else {\r\n            splits.add(new DataDrivenDBInputFormat.DataDrivenDBInputSplit(lowClausePrefix + dateToString(startDate), highClausePrefix + dateToString(endDate)));\r\n        }\r\n        start = end;\r\n        startDate = endDate;\r\n    }\r\n    if (minVal == Long.MIN_VALUE || maxVal == Long.MIN_VALUE) {\r\n        splits.add(new DataDrivenDBInputFormat.DataDrivenDBInputSplit(colName + \" IS NULL\", colName + \" IS NULL\"));\r\n    }\r\n    return splits;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 2,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\db",
  "methodName" : "resultSetColToLong",
  "errType" : [ "NullPointerException" ],
  "containingMethodsNum" : 4,
  "sourceCodeText" : "long resultSetColToLong(ResultSet rs, int colNum, int sqlDataType) throws SQLException\n{\r\n    try {\r\n        switch(sqlDataType) {\r\n            case Types.DATE:\r\n                return rs.getDate(colNum).getTime();\r\n            case Types.TIME:\r\n                return rs.getTime(colNum).getTime();\r\n            case Types.TIMESTAMP:\r\n                return rs.getTimestamp(colNum).getTime();\r\n            default:\r\n                throw new SQLException(\"Not a date-type field\");\r\n        }\r\n    } catch (NullPointerException npe) {\r\n        LOG.warn(\"Encountered a NULL date in the split column. Splits may be poorly balanced.\");\r\n        return Long.MIN_VALUE;\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\db",
  "methodName" : "longToDate",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Date longToDate(long val, int sqlDataType)\n{\r\n    switch(sqlDataType) {\r\n        case Types.DATE:\r\n            return new java.sql.Date(val);\r\n        case Types.TIME:\r\n            return new java.sql.Time(val);\r\n        case Types.TIMESTAMP:\r\n            return new java.sql.Timestamp(val);\r\n        default:\r\n            return null;\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\db",
  "methodName" : "dateToString",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String dateToString(Date d)\n{\r\n    return \"'\" + d.toString() + \"'\";\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getAttemptsToStartSkipping",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "int getAttemptsToStartSkipping(Configuration conf)\n{\r\n    return conf.getInt(ATTEMPTS_TO_START_SKIPPING, 2);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "setAttemptsToStartSkipping",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setAttemptsToStartSkipping(Configuration conf, int attemptsToStartSkipping)\n{\r\n    conf.setInt(ATTEMPTS_TO_START_SKIPPING, attemptsToStartSkipping);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getAutoIncrMapperProcCount",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "boolean getAutoIncrMapperProcCount(Configuration conf)\n{\r\n    return conf.getBoolean(AUTO_INCR_MAP_PROC_COUNT, true);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "setAutoIncrMapperProcCount",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setAutoIncrMapperProcCount(Configuration conf, boolean autoIncr)\n{\r\n    conf.setBoolean(AUTO_INCR_MAP_PROC_COUNT, autoIncr);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getAutoIncrReducerProcCount",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "boolean getAutoIncrReducerProcCount(Configuration conf)\n{\r\n    return conf.getBoolean(AUTO_INCR_REDUCE_PROC_COUNT, true);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "setAutoIncrReducerProcCount",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setAutoIncrReducerProcCount(Configuration conf, boolean autoIncr)\n{\r\n    conf.setBoolean(AUTO_INCR_REDUCE_PROC_COUNT, autoIncr);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getSkipOutputPath",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "Path getSkipOutputPath(Configuration conf)\n{\r\n    String name = conf.get(OUT_PATH);\r\n    if (name != null) {\r\n        if (\"none\".equals(name)) {\r\n            return null;\r\n        }\r\n        return new Path(name);\r\n    }\r\n    Path outPath = FileOutputFormat.getOutputPath(new JobConf(conf));\r\n    return outPath == null ? null : new Path(outPath, \"_logs\" + Path.SEPARATOR + \"skip\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "setSkipOutputPath",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void setSkipOutputPath(JobConf conf, Path path)\n{\r\n    String pathStr = null;\r\n    if (path == null) {\r\n        pathStr = \"none\";\r\n    } else {\r\n        pathStr = path.toString();\r\n    }\r\n    conf.set(OUT_PATH, pathStr);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getMapperMaxSkipRecords",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "long getMapperMaxSkipRecords(Configuration conf)\n{\r\n    return conf.getLong(MAPPER_MAX_SKIP_RECORDS, 0);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "setMapperMaxSkipRecords",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setMapperMaxSkipRecords(Configuration conf, long maxSkipRecs)\n{\r\n    conf.setLong(MAPPER_MAX_SKIP_RECORDS, maxSkipRecs);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getReducerMaxSkipGroups",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "long getReducerMaxSkipGroups(Configuration conf)\n{\r\n    return conf.getLong(REDUCER_MAX_SKIP_GROUPS, 0);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "setReducerMaxSkipGroups",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setReducerMaxSkipGroups(Configuration conf, long maxSkipGrps)\n{\r\n    conf.setLong(REDUCER_MAX_SKIP_GROUPS, maxSkipGrps);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output",
  "methodName" : "createOutputCommitter",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "PathOutputCommitter createOutputCommitter(Path outputPath, TaskAttemptContext context) throws IOException\n{\r\n    return createFileOutputCommitter(outputPath, context);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output",
  "methodName" : "createFileOutputCommitter",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "PathOutputCommitter createFileOutputCommitter(Path outputPath, TaskAttemptContext context) throws IOException\n{\r\n    LOG.debug(\"Creating FileOutputCommitter for path {} and context {}\", outputPath, context);\r\n    return new FileOutputCommitter(outputPath, context);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output",
  "methodName" : "getCommitterFactory",
  "errType" : null,
  "containingMethodsNum" : 13,
  "sourceCodeText" : "PathOutputCommitterFactory getCommitterFactory(Path outputPath, Configuration conf)\n{\r\n    LOG.debug(\"Looking for committer factory for path {}\", outputPath);\r\n    String key = COMMITTER_FACTORY_CLASS;\r\n    if (StringUtils.isEmpty(conf.getTrimmed(key)) && outputPath != null) {\r\n        String scheme = outputPath.toUri().getScheme();\r\n        String schemeKey = String.format(COMMITTER_FACTORY_SCHEME_PATTERN, scheme);\r\n        if (StringUtils.isNotEmpty(conf.getTrimmed(schemeKey))) {\r\n            LOG.info(\"Using schema-specific factory for {}\", outputPath);\r\n            key = schemeKey;\r\n        } else {\r\n            LOG.debug(\"No scheme-specific factory defined in {}\", schemeKey);\r\n        }\r\n    }\r\n    Class<? extends PathOutputCommitterFactory> factory;\r\n    String trimmedValue = conf.getTrimmed(key, \"\");\r\n    if (StringUtils.isEmpty(trimmedValue)) {\r\n        LOG.info(\"No output committer factory defined,\" + \" defaulting to FileOutputCommitterFactory\");\r\n        factory = FileOutputCommitterFactory.class;\r\n    } else {\r\n        factory = conf.getClass(key, FileOutputCommitterFactory.class, PathOutputCommitterFactory.class);\r\n        LOG.info(\"Using OutputCommitter factory class {} from key {}\", factory, key);\r\n    }\r\n    return ReflectionUtils.newInstance(factory, conf);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output",
  "methodName" : "createCommitter",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "PathOutputCommitter createCommitter(Path outputPath, TaskAttemptContext context) throws IOException\n{\r\n    return getCommitterFactory(outputPath, context.getConfiguration()).createOutputCommitter(outputPath, context);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\task\\reduce",
  "methodName" : "getTempPath",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Path getTempPath(Path outPath, int fetcher)\n{\r\n    return outPath.suffix(String.valueOf(fetcher));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\task\\reduce",
  "methodName" : "doShuffle",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 10,
  "sourceCodeText" : "void doShuffle(MapHost host, IFileInputStream input, long compressedLength, long decompressedLength, ShuffleClientMetrics metrics, Reporter reporter) throws IOException\n{\r\n    long bytesLeft = compressedLength;\r\n    try {\r\n        final int BYTES_TO_READ = 64 * 1024;\r\n        byte[] buf = new byte[BYTES_TO_READ];\r\n        while (bytesLeft > 0) {\r\n            int n = input.readWithChecksum(buf, 0, (int) Math.min(bytesLeft, BYTES_TO_READ));\r\n            if (n < 0) {\r\n                throw new IOException(\"read past end of stream reading \" + getMapId());\r\n            }\r\n            disk.write(buf, 0, n);\r\n            bytesLeft -= n;\r\n            metrics.inputBytes(n);\r\n            reporter.progress();\r\n        }\r\n        LOG.info(\"Read \" + (compressedLength - bytesLeft) + \" bytes from map-output for \" + getMapId());\r\n        disk.close();\r\n    } catch (IOException ioe) {\r\n        IOUtils.cleanupWithLogger(LOG, disk);\r\n        throw ioe;\r\n    }\r\n    if (bytesLeft != 0) {\r\n        throw new IOException(\"Incomplete map output received for \" + getMapId() + \" from \" + host.getHostName() + \" (\" + bytesLeft + \" bytes missing of \" + compressedLength + \")\");\r\n    }\r\n    this.compressedSize = compressedLength;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\task\\reduce",
  "methodName" : "commit",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void commit() throws IOException\n{\r\n    fs.rename(tmpOutputPath, outputPath);\r\n    CompressAwarePath compressAwarePath = new CompressAwarePath(outputPath, getSize(), this.compressedSize);\r\n    getMerger().closeOnDiskFile(compressAwarePath);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\task\\reduce",
  "methodName" : "abort",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void abort()\n{\r\n    try {\r\n        fs.delete(tmpOutputPath, false);\r\n    } catch (IOException ie) {\r\n        LOG.info(\"failure to clean up \" + tmpOutputPath, ie);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce\\task\\reduce",
  "methodName" : "getDescription",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getDescription()\n{\r\n    return \"DISK\";\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "initProviderList",
  "errType" : [ "ServiceConfigurationError" ],
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void initProviderList()\n{\r\n    if (providerList == null) {\r\n        synchronized (frameworkLoader) {\r\n            if (providerList == null) {\r\n                List<ClientProtocolProvider> localProviderList = new ArrayList<ClientProtocolProvider>();\r\n                try {\r\n                    for (ClientProtocolProvider provider : frameworkLoader) {\r\n                        localProviderList.add(provider);\r\n                    }\r\n                } catch (ServiceConfigurationError e) {\r\n                    LOG.info(\"Failed to instantiate ClientProtocolProvider, please \" + \"check the /META-INF/services/org.apache.\" + \"hadoop.mapreduce.protocol.ClientProtocolProvider \" + \"files on the classpath\", e);\r\n                }\r\n                providerList = localProviderList;\r\n            }\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "initialize",
  "errType" : [ "Exception" ],
  "containingMethodsNum" : 10,
  "sourceCodeText" : "void initialize(InetSocketAddress jobTrackAddr, Configuration conf) throws IOException\n{\r\n    initProviderList();\r\n    final IOException initEx = new IOException(\"Cannot initialize Cluster. Please check your configuration for \" + MRConfig.FRAMEWORK_NAME + \" and the correspond server addresses.\");\r\n    if (jobTrackAddr != null) {\r\n        LOG.info(\"Initializing cluster for Job Tracker=\" + jobTrackAddr.toString());\r\n    }\r\n    for (ClientProtocolProvider provider : providerList) {\r\n        LOG.debug(\"Trying ClientProtocolProvider : \" + provider.getClass().getName());\r\n        ClientProtocol clientProtocol = null;\r\n        try {\r\n            if (jobTrackAddr == null) {\r\n                clientProtocol = provider.create(conf);\r\n            } else {\r\n                clientProtocol = provider.create(jobTrackAddr, conf);\r\n            }\r\n            if (clientProtocol != null) {\r\n                clientProtocolProvider = provider;\r\n                client = clientProtocol;\r\n                LOG.debug(\"Picked \" + provider.getClass().getName() + \" as the ClientProtocolProvider\");\r\n                break;\r\n            } else {\r\n                LOG.debug(\"Cannot pick \" + provider.getClass().getName() + \" as the ClientProtocolProvider - returned null protocol\");\r\n            }\r\n        } catch (Exception e) {\r\n            final String errMsg = \"Failed to use \" + provider.getClass().getName() + \" due to error: \";\r\n            initEx.addSuppressed(new IOException(errMsg, e));\r\n            LOG.info(errMsg, e);\r\n        }\r\n    }\r\n    if (null == clientProtocolProvider || null == client) {\r\n        throw initEx;\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "getClient",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "ClientProtocol getClient()\n{\r\n    return client;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "getConf",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Configuration getConf()\n{\r\n    return conf;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "close",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void close() throws IOException\n{\r\n    clientProtocolProvider.close(client);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "getJobs",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "Job[] getJobs(JobStatus[] stats) throws IOException\n{\r\n    List<Job> jobs = new ArrayList<Job>();\r\n    for (JobStatus stat : stats) {\r\n        jobs.add(Job.getInstance(this, stat, new JobConf(stat.getJobFile())));\r\n    }\r\n    return jobs.toArray(new Job[0]);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "getFileSystem",
  "errType" : [ "InterruptedException" ],
  "containingMethodsNum" : 1,
  "sourceCodeText" : "FileSystem getFileSystem() throws IOException, InterruptedException\n{\r\n    if (this.fs == null) {\r\n        try {\r\n            this.fs = ugi.doAs(new PrivilegedExceptionAction<FileSystem>() {\r\n\r\n                public FileSystem run() throws IOException, InterruptedException {\r\n                    final Path sysDir = new Path(client.getSystemDir());\r\n                    return sysDir.getFileSystem(getConf());\r\n                }\r\n            });\r\n        } catch (InterruptedException e) {\r\n            throw new RuntimeException(e);\r\n        }\r\n    }\r\n    return fs;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "getJob",
  "errType" : [ "RuntimeException" ],
  "containingMethodsNum" : 4,
  "sourceCodeText" : "Job getJob(JobID jobId) throws IOException, InterruptedException\n{\r\n    JobStatus status = client.getJobStatus(jobId);\r\n    if (status != null) {\r\n        JobConf conf;\r\n        try {\r\n            conf = new JobConf(status.getJobFile());\r\n        } catch (RuntimeException ex) {\r\n            if (ex.getCause() instanceof FileNotFoundException) {\r\n                return null;\r\n            } else {\r\n                throw ex;\r\n            }\r\n        }\r\n        return Job.getInstance(this, status, conf);\r\n    }\r\n    return null;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "getQueues",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "QueueInfo[] getQueues() throws IOException, InterruptedException\n{\r\n    return client.getQueues();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "getQueue",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "QueueInfo getQueue(String name) throws IOException, InterruptedException\n{\r\n    return client.getQueue(name);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "getLogParams",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "LogParams getLogParams(JobID jobID, TaskAttemptID taskAttemptID) throws IOException, InterruptedException\n{\r\n    return client.getLogFileParams(jobID, taskAttemptID);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "getClusterStatus",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "ClusterMetrics getClusterStatus() throws IOException, InterruptedException\n{\r\n    return client.getClusterMetrics();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "getActiveTaskTrackers",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "TaskTrackerInfo[] getActiveTaskTrackers() throws IOException, InterruptedException\n{\r\n    return client.getActiveTrackers();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "getBlackListedTaskTrackers",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "TaskTrackerInfo[] getBlackListedTaskTrackers() throws IOException, InterruptedException\n{\r\n    return client.getBlacklistedTrackers();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "getAllJobs",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Job[] getAllJobs() throws IOException, InterruptedException\n{\r\n    return getJobs(client.getAllJobs());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "getAllJobStatuses",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "JobStatus[] getAllJobStatuses() throws IOException, InterruptedException\n{\r\n    return client.getAllJobs();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "getSystemDir",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Path getSystemDir() throws IOException, InterruptedException\n{\r\n    if (sysDir == null) {\r\n        sysDir = new Path(client.getSystemDir());\r\n    }\r\n    return sysDir;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "getStagingAreaDir",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Path getStagingAreaDir() throws IOException, InterruptedException\n{\r\n    if (stagingAreaDir == null) {\r\n        stagingAreaDir = new Path(client.getStagingAreaDir());\r\n    }\r\n    return stagingAreaDir;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "getJobHistoryUrl",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "String getJobHistoryUrl(JobID jobId) throws IOException, InterruptedException\n{\r\n    if (jobHistoryDir == null) {\r\n        jobHistoryDir = new Path(client.getJobHistoryDir());\r\n    }\r\n    return new Path(jobHistoryDir, jobId.toString() + \"_\" + ugi.getShortUserName()).toString();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "getQueueAclsForCurrentUser",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "QueueAclsInfo[] getQueueAclsForCurrentUser() throws IOException, InterruptedException\n{\r\n    return client.getQueueAclsForCurrentUser();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "getRootQueues",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "QueueInfo[] getRootQueues() throws IOException, InterruptedException\n{\r\n    return client.getRootQueues();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "getChildQueues",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "QueueInfo[] getChildQueues(String queueName) throws IOException, InterruptedException\n{\r\n    return client.getChildQueues(queueName);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "getJobTrackerStatus",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "JobTrackerStatus getJobTrackerStatus() throws IOException, InterruptedException\n{\r\n    return client.getJobTrackerStatus();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "getTaskTrackerExpiryInterval",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "long getTaskTrackerExpiryInterval() throws IOException, InterruptedException\n{\r\n    return client.getTaskTrackerExpiryInterval();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "getDelegationToken",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Token<DelegationTokenIdentifier> getDelegationToken(Text renewer) throws IOException, InterruptedException\n{\r\n    return client.getDelegationToken(renewer);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "renewDelegationToken",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "long renewDelegationToken(Token<DelegationTokenIdentifier> token) throws InvalidToken, IOException, InterruptedException\n{\r\n    return token.renew(getConf());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-core\\src\\main\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "cancelDelegationToken",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void cancelDelegationToken(Token<DelegationTokenIdentifier> token) throws IOException, InterruptedException\n{\r\n    token.cancel(getConf());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
} ]
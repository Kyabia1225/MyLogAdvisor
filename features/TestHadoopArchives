[ {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-archives\\src\\test\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "createFile",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String createFile(Path root, FileSystem fs, String... dirsAndFile) throws IOException\n{\r\n    String fileBaseName = dirsAndFile[dirsAndFile.length - 1];\r\n    return createFile(root, fs, fileBaseName.getBytes(\"UTF-8\"), dirsAndFile);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-archives\\src\\test\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "createFile",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "String createFile(Path root, FileSystem fs, byte[] fileContent, String... dirsAndFile) throws IOException\n{\r\n    StringBuilder sb = new StringBuilder();\r\n    for (String segment : dirsAndFile) {\r\n        if (sb.length() > 0) {\r\n            sb.append(Path.SEPARATOR);\r\n        }\r\n        sb.append(segment);\r\n    }\r\n    final Path f = new Path(root, sb.toString());\r\n    final FSDataOutputStream out = fs.create(f);\r\n    try {\r\n        out.write(fileContent);\r\n    } finally {\r\n        out.close();\r\n    }\r\n    return sb.toString();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-archives\\src\\test\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "setUp",
  "errType" : null,
  "containingMethodsNum" : 12,
  "sourceCodeText" : "void setUp() throws Exception\n{\r\n    conf = new Configuration();\r\n    conf.set(CapacitySchedulerConfiguration.PREFIX + CapacitySchedulerConfiguration.ROOT + \".\" + CapacitySchedulerConfiguration.QUEUES, \"default\");\r\n    conf.set(CapacitySchedulerConfiguration.PREFIX + CapacitySchedulerConfiguration.ROOT + \".default.\" + CapacitySchedulerConfiguration.CAPACITY, \"100\");\r\n    dfscluster = new MiniDFSCluster.Builder(conf).checkExitOnShutdown(true).numDataNodes(3).format(true).racks(null).build();\r\n    fs = dfscluster.getFileSystem();\r\n    archivePath = new Path(fs.getHomeDirectory(), \"archive\");\r\n    fs.delete(archivePath, true);\r\n    inputPath = new Path(fs.getHomeDirectory(), inputDir);\r\n    fs.delete(inputPath, true);\r\n    fs.mkdirs(inputPath);\r\n    fileList.add(createFile(inputPath, fs, \"a\"));\r\n    fileList.add(createFile(inputPath, fs, \"b\"));\r\n    fileList.add(createFile(inputPath, fs, \"c\"));\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-archives\\src\\test\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "tearDown",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void tearDown() throws Exception\n{\r\n    if (dfscluster != null) {\r\n        dfscluster.shutdown();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-archives\\src\\test\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "testRelativePath",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void testRelativePath() throws Exception\n{\r\n    final Path sub1 = new Path(inputPath, \"dir1\");\r\n    fs.mkdirs(sub1);\r\n    createFile(inputPath, fs, sub1.getName(), \"a\");\r\n    final FsShell shell = new FsShell(conf);\r\n    final List<String> originalPaths = lsr(shell, \"input\");\r\n    System.out.println(\"originalPaths: \" + originalPaths);\r\n    final String fullHarPathStr = makeArchive();\r\n    final List<String> harPaths = lsr(shell, fullHarPathStr);\r\n    Assert.assertEquals(originalPaths, harPaths);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-archives\\src\\test\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "testRelativePathWitRepl",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void testRelativePathWitRepl() throws Exception\n{\r\n    final Path sub1 = new Path(inputPath, \"dir1\");\r\n    fs.mkdirs(sub1);\r\n    createFile(inputPath, fs, sub1.getName(), \"a\");\r\n    final FsShell shell = new FsShell(conf);\r\n    final List<String> originalPaths = lsr(shell, \"input\");\r\n    System.out.println(\"originalPaths: \" + originalPaths);\r\n    final String fullHarPathStr = makeArchiveWithRepl();\r\n    final List<String> harPaths = lsr(shell, fullHarPathStr);\r\n    Assert.assertEquals(originalPaths, harPaths);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-archives\\src\\test\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "testOutputPathValidity",
  "errType" : null,
  "containingMethodsNum" : 16,
  "sourceCodeText" : "void testOutputPathValidity() throws Exception\n{\r\n    final String inputPathStr = inputPath.toUri().getPath();\r\n    final URI uri = fs.getUri();\r\n    final String harName = \"foo.har\";\r\n    System.setProperty(HadoopArchives.TEST_HADOOP_ARCHIVES_JAR_PATH, HADOOP_ARCHIVES_JAR);\r\n    final HadoopArchives har = new HadoopArchives(conf);\r\n    PrintStream stderr = System.err;\r\n    ByteArrayOutputStream byteStream = new ByteArrayOutputStream();\r\n    PrintStream newErr = new PrintStream(byteStream);\r\n    System.setErr(newErr);\r\n    createFile(archivePath, fs, harName);\r\n    final String[] args = { \"-archiveName\", harName, \"-p\", inputPathStr, \"*\", archivePath.toString() };\r\n    Assert.assertEquals(-1, ToolRunner.run(har, args));\r\n    String output = byteStream.toString();\r\n    final Path outputPath = new Path(archivePath, harName);\r\n    Assert.assertTrue(output.indexOf(\"Archive path: \" + outputPath.toString() + \" already exists\") != -1);\r\n    byteStream.reset();\r\n    createFile(archivePath, fs, \"sub1\");\r\n    final Path archivePath2 = new Path(archivePath, \"sub1\");\r\n    final String[] args2 = { \"-archiveName\", harName, \"-p\", inputPathStr, \"*\", archivePath2.toString() };\r\n    Assert.assertEquals(-1, ToolRunner.run(har, args2));\r\n    output = byteStream.toString();\r\n    Assert.assertTrue(output.indexOf(\"Destination \" + archivePath2.toString() + \" should be a directory but is a file\") != -1);\r\n    System.setErr(stderr);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-archives\\src\\test\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "testPathWithSpaces",
  "errType" : null,
  "containingMethodsNum" : 14,
  "sourceCodeText" : "void testPathWithSpaces() throws Exception\n{\r\n    createFile(inputPath, fs, \"c c\");\r\n    final Path sub1 = new Path(inputPath, \"sub 1\");\r\n    fs.mkdirs(sub1);\r\n    createFile(sub1, fs, \"file x y z\");\r\n    createFile(sub1, fs, \"file\");\r\n    createFile(sub1, fs, \"x\");\r\n    createFile(sub1, fs, \"y\");\r\n    createFile(sub1, fs, \"z\");\r\n    final Path sub2 = new Path(inputPath, \"sub 1 with suffix\");\r\n    fs.mkdirs(sub2);\r\n    createFile(sub2, fs, \"z\");\r\n    final FsShell shell = new FsShell(conf);\r\n    final String inputPathStr = inputPath.toUri().getPath();\r\n    final List<String> originalPaths = lsr(shell, inputPathStr);\r\n    final String fullHarPathStr = makeArchive();\r\n    final List<String> harPaths = lsr(shell, fullHarPathStr);\r\n    Assert.assertEquals(originalPaths, harPaths);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-archives\\src\\test\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "testSingleFile",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void testSingleFile() throws Exception\n{\r\n    final Path sub1 = new Path(inputPath, \"dir1\");\r\n    fs.mkdirs(sub1);\r\n    String singleFileName = \"a\";\r\n    createFile(inputPath, fs, sub1.getName(), singleFileName);\r\n    final FsShell shell = new FsShell(conf);\r\n    final List<String> originalPaths = lsr(shell, sub1.toString());\r\n    System.out.println(\"originalPaths: \" + originalPaths);\r\n    final String fullHarPathStr = makeArchive(sub1, singleFileName);\r\n    final List<String> harPaths = lsr(shell, fullHarPathStr);\r\n    Assert.assertEquals(originalPaths, harPaths);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-archives\\src\\test\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "testGlobFiles",
  "errType" : null,
  "containingMethodsNum" : 9,
  "sourceCodeText" : "void testGlobFiles() throws Exception\n{\r\n    final Path sub1 = new Path(inputPath, \"dir1\");\r\n    final Path sub2 = new Path(inputPath, \"dir2\");\r\n    fs.mkdirs(sub1);\r\n    String fileName = \"a\";\r\n    createFile(inputPath, fs, sub1.getName(), fileName);\r\n    createFile(inputPath, fs, sub2.getName(), fileName);\r\n    createFile(inputPath, fs, sub1.getName(), \"b\");\r\n    final String glob = \"dir{1,2}/a\";\r\n    final FsShell shell = new FsShell(conf);\r\n    final List<String> originalPaths = lsr(shell, inputPath.toString(), inputPath + \"/\" + glob);\r\n    System.out.println(\"originalPaths: \" + originalPaths);\r\n    final String fullHarPathStr = makeArchive(inputPath, glob);\r\n    final List<String> harPaths = lsr(shell, fullHarPathStr, fullHarPathStr + \"/\" + glob);\r\n    Assert.assertEquals(originalPaths, harPaths);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-archives\\src\\test\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "lsr",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "List<String> lsr(final FsShell shell, String rootDir) throws Exception\n{\r\n    return lsr(shell, rootDir, null);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-archives\\src\\test\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "lsr",
  "errType" : null,
  "containingMethodsNum" : 17,
  "sourceCodeText" : "List<String> lsr(final FsShell shell, String rootDir, String glob) throws Exception\n{\r\n    final String dir = glob == null ? rootDir : glob;\r\n    System.out.println(\"lsr root=\" + rootDir);\r\n    final ByteArrayOutputStream bytes = new ByteArrayOutputStream();\r\n    final PrintStream out = new PrintStream(bytes);\r\n    final PrintStream oldOut = System.out;\r\n    final PrintStream oldErr = System.err;\r\n    System.setOut(out);\r\n    System.setErr(out);\r\n    final String results;\r\n    try {\r\n        Assert.assertEquals(0, shell.run(new String[] { \"-lsr\", dir }));\r\n        results = bytes.toString();\r\n    } finally {\r\n        IOUtils.closeStream(out);\r\n        System.setOut(oldOut);\r\n        System.setErr(oldErr);\r\n    }\r\n    System.out.println(\"lsr results:\\n\" + results);\r\n    String dirname = rootDir;\r\n    if (rootDir.lastIndexOf(Path.SEPARATOR) != -1) {\r\n        dirname = rootDir.substring(rootDir.lastIndexOf(Path.SEPARATOR));\r\n    }\r\n    final List<String> paths = new ArrayList<String>();\r\n    for (StringTokenizer t = new StringTokenizer(results, \"\\n\"); t.hasMoreTokens(); ) {\r\n        final String s = t.nextToken();\r\n        final int i = s.indexOf(dirname);\r\n        if (i >= 0) {\r\n            paths.add(s.substring(i + dirname.length()));\r\n        }\r\n    }\r\n    Collections.sort(paths);\r\n    System.out.println(\"lsr paths = \" + paths.toString().replace(\", \", \",\\n  \"));\r\n    return paths;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-archives\\src\\test\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "testReadFileContent",
  "errType" : null,
  "containingMethodsNum" : 35,
  "sourceCodeText" : "void testReadFileContent() throws Exception\n{\r\n    fileList.add(createFile(inputPath, fs, \"c c\"));\r\n    final Path sub1 = new Path(inputPath, \"sub 1\");\r\n    fs.mkdirs(sub1);\r\n    fileList.add(createFile(inputPath, fs, sub1.getName(), \"file x y z\"));\r\n    fileList.add(createFile(inputPath, fs, sub1.getName(), \"file\"));\r\n    fileList.add(createFile(inputPath, fs, sub1.getName(), \"x\"));\r\n    fileList.add(createFile(inputPath, fs, sub1.getName(), \"y\"));\r\n    fileList.add(createFile(inputPath, fs, sub1.getName(), \"z\"));\r\n    final Path sub2 = new Path(inputPath, \"sub 1 with suffix\");\r\n    fs.mkdirs(sub2);\r\n    fileList.add(createFile(inputPath, fs, sub2.getName(), \"z\"));\r\n    final byte[] binContent = prepareBin();\r\n    fileList.add(createFile(inputPath, fs, binContent, sub2.getName(), \"bin\"));\r\n    fileList.add(createFile(inputPath, fs, new byte[0], sub2.getName(), \"zero-length\"));\r\n    final String fullHarPathStr = makeArchive();\r\n    final HarFileSystem harFileSystem = new HarFileSystem(fs);\r\n    try {\r\n        final URI harUri = new URI(fullHarPathStr);\r\n        harFileSystem.initialize(harUri, fs.getConf());\r\n        int readFileCount = 0;\r\n        for (final String pathStr0 : fileList) {\r\n            final Path path = new Path(fullHarPathStr + Path.SEPARATOR + pathStr0);\r\n            final String baseName = path.getName();\r\n            final FileStatus status = harFileSystem.getFileStatus(path);\r\n            if (status.isFile()) {\r\n                final byte[] actualContentSimple = readAllSimple(harFileSystem.open(path), true);\r\n                final byte[] actualContentBuffer = readAllWithBuffer(harFileSystem.open(path), true);\r\n                assertArrayEquals(actualContentSimple, actualContentBuffer);\r\n                final byte[] actualContentFully = readAllWithReadFully(actualContentSimple.length, harFileSystem.open(path), true);\r\n                assertArrayEquals(actualContentSimple, actualContentFully);\r\n                final byte[] actualContentSeek = readAllWithSeek(actualContentSimple.length, harFileSystem.open(path), true);\r\n                assertArrayEquals(actualContentSimple, actualContentSeek);\r\n                final byte[] actualContentRead4 = readAllWithRead4(harFileSystem.open(path), true);\r\n                assertArrayEquals(actualContentSimple, actualContentRead4);\r\n                final byte[] actualContentSkip = readAllWithSkip(actualContentSimple.length, harFileSystem.open(path), harFileSystem.open(path), true);\r\n                assertArrayEquals(actualContentSimple, actualContentSkip);\r\n                if (\"bin\".equals(baseName)) {\r\n                    assertArrayEquals(binContent, actualContentSimple);\r\n                } else if (\"zero-length\".equals(baseName)) {\r\n                    assertEquals(0, actualContentSimple.length);\r\n                } else {\r\n                    String actual = new String(actualContentSimple, \"UTF-8\");\r\n                    assertEquals(baseName, actual);\r\n                }\r\n                readFileCount++;\r\n            }\r\n        }\r\n        assertThat(fileList.size()).isEqualTo(readFileCount);\r\n    } finally {\r\n        harFileSystem.close();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-archives\\src\\test\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "readAllSimple",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "byte[] readAllSimple(FSDataInputStream fsdis, boolean close) throws IOException\n{\r\n    final ByteArrayOutputStream baos = new ByteArrayOutputStream();\r\n    try {\r\n        int b;\r\n        while (true) {\r\n            b = fsdis.read();\r\n            if (b < 0) {\r\n                break;\r\n            } else {\r\n                baos.write(b);\r\n            }\r\n        }\r\n        baos.close();\r\n        return baos.toByteArray();\r\n    } finally {\r\n        if (close) {\r\n            fsdis.close();\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-archives\\src\\test\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "readAllWithBuffer",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "byte[] readAllWithBuffer(FSDataInputStream fsdis, boolean close) throws IOException\n{\r\n    try {\r\n        final int available = fsdis.available();\r\n        final byte[] buffer;\r\n        final ByteArrayOutputStream baos;\r\n        if (available < 0) {\r\n            buffer = new byte[1024];\r\n            baos = new ByteArrayOutputStream(buffer.length * 2);\r\n        } else {\r\n            buffer = new byte[available];\r\n            baos = new ByteArrayOutputStream(available);\r\n        }\r\n        int readIntoBuffer = 0;\r\n        int read;\r\n        while (true) {\r\n            read = fsdis.read(buffer, readIntoBuffer, buffer.length - readIntoBuffer);\r\n            if (read <= 0) {\r\n                if (readIntoBuffer > 0) {\r\n                    baos.write(buffer, 0, readIntoBuffer);\r\n                }\r\n                return baos.toByteArray();\r\n            } else {\r\n                readIntoBuffer += read;\r\n                if (readIntoBuffer == buffer.length) {\r\n                    baos.write(buffer);\r\n                    readIntoBuffer = 0;\r\n                } else if (readIntoBuffer > buffer.length) {\r\n                    throw new IOException(\"Read more than the buffer length: \" + readIntoBuffer + \", buffer length = \" + buffer.length);\r\n                }\r\n            }\r\n        }\r\n    } finally {\r\n        if (close) {\r\n            fsdis.close();\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-archives\\src\\test\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "readAllWithReadFully",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 10,
  "sourceCodeText" : "byte[] readAllWithReadFully(int totalLength, FSDataInputStream fsdis, boolean close) throws IOException\n{\r\n    final ByteArrayOutputStream baos = new ByteArrayOutputStream();\r\n    final byte[] buffer = new byte[17];\r\n    final int times = totalLength / buffer.length;\r\n    final int remainder = totalLength % buffer.length;\r\n    int position = 0;\r\n    try {\r\n        for (int i = 0; i < times; i++) {\r\n            fsdis.readFully(position, buffer);\r\n            position += buffer.length;\r\n            baos.write(buffer);\r\n        }\r\n        if (remainder > 0) {\r\n            fsdis.readFully(position, buffer, 0, remainder);\r\n            position += remainder;\r\n            baos.write(buffer, 0, remainder);\r\n        }\r\n        try {\r\n            fsdis.readFully(position, buffer, 0, 1);\r\n            assertTrue(false);\r\n        } catch (IOException ioe) {\r\n        }\r\n        assertEquals(totalLength, position);\r\n        final byte[] result = baos.toByteArray();\r\n        assertEquals(totalLength, result.length);\r\n        return result;\r\n    } finally {\r\n        if (close) {\r\n            fsdis.close();\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 2,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-archives\\src\\test\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "readAllWithRead4",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "byte[] readAllWithRead4(FSDataInputStream fsdis, boolean close) throws IOException\n{\r\n    try {\r\n        final ByteArrayOutputStream baos = new ByteArrayOutputStream();\r\n        final byte[] buffer = new byte[17];\r\n        int totalRead = 0;\r\n        int read;\r\n        while (true) {\r\n            read = fsdis.read(totalRead, buffer, 0, buffer.length);\r\n            if (read > 0) {\r\n                totalRead += read;\r\n                baos.write(buffer, 0, read);\r\n            } else if (read < 0) {\r\n                break;\r\n            } else {\r\n                throw new AssertionError(\"FSDataInputStream#read(4) returned 0, while \" + \" the 4th method parameter is \" + buffer.length + \".\");\r\n            }\r\n        }\r\n        final byte[] result = baos.toByteArray();\r\n        return result;\r\n    } finally {\r\n        if (close) {\r\n            fsdis.close();\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-archives\\src\\test\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "readAllWithSeek",
  "errType" : null,
  "containingMethodsNum" : 12,
  "sourceCodeText" : "byte[] readAllWithSeek(final int totalLength, final FSDataInputStream fsdis, final boolean close) throws IOException\n{\r\n    final byte[] result = new byte[totalLength];\r\n    long pos;\r\n    try {\r\n        final byte[] buffer = new byte[17];\r\n        final int times = totalLength / buffer.length;\r\n        int read;\r\n        int expectedRead;\r\n        for (int i = times; i >= 0; i--) {\r\n            pos = i * buffer.length;\r\n            fsdis.seek(pos);\r\n            assertEquals(pos, fsdis.getPos());\r\n            read = fsdis.read(buffer);\r\n            if (i == times) {\r\n                expectedRead = totalLength % buffer.length;\r\n                if (expectedRead == 0) {\r\n                    expectedRead = -1;\r\n                }\r\n            } else {\r\n                expectedRead = buffer.length;\r\n            }\r\n            assertEquals(expectedRead, read);\r\n            if (read > 0) {\r\n                System.arraycopy(buffer, 0, result, (int) pos, read);\r\n            }\r\n        }\r\n        expectSeekIOE(fsdis, Long.MAX_VALUE, \"Seek to Long.MAX_VALUE should lead to IOE.\");\r\n        expectSeekIOE(fsdis, Long.MIN_VALUE, \"Seek to Long.MIN_VALUE should lead to IOE.\");\r\n        long pp = -1L;\r\n        expectSeekIOE(fsdis, pp, \"Seek to \" + pp + \" should lead to IOE.\");\r\n        fsdis.seek(totalLength);\r\n        assertEquals(totalLength, fsdis.getPos());\r\n        pp = totalLength + 1;\r\n        expectSeekIOE(fsdis, pp, \"Seek to the length position + 1 (\" + pp + \") should lead to IOE.\");\r\n        return result;\r\n    } finally {\r\n        if (close) {\r\n            fsdis.close();\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-archives\\src\\test\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "expectSeekIOE",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void expectSeekIOE(FSDataInputStream fsdis, long seekPos, String message)\n{\r\n    try {\r\n        fsdis.seek(seekPos);\r\n        assertTrue(message + \" (Position = \" + fsdis.getPos() + \")\", false);\r\n    } catch (IOException ioe) {\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-archives\\src\\test\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "readAllWithSkip",
  "errType" : null,
  "containingMethodsNum" : 19,
  "sourceCodeText" : "byte[] readAllWithSkip(final int totalLength, final FSDataInputStream fsdis1, final FSDataInputStream fsdis2, final boolean close) throws IOException\n{\r\n    assertEquals(0, fsdis1.skip(-1));\r\n    assertEquals(0, fsdis1.skip(0));\r\n    final ByteArrayOutputStream baos = new ByteArrayOutputStream(totalLength);\r\n    try {\r\n        final byte[] buffer = new byte[17];\r\n        final int times = totalLength / buffer.length;\r\n        final int remainder = totalLength % buffer.length;\r\n        long skipped;\r\n        long expectedPosition;\r\n        int toGo;\r\n        for (int i = 0; i <= times; i++) {\r\n            toGo = (i < times) ? buffer.length : remainder;\r\n            if (i % 2 == 0) {\r\n                fsdis1.readFully(buffer, 0, toGo);\r\n                skipped = skipUntilZero(fsdis2, toGo);\r\n            } else {\r\n                fsdis2.readFully(buffer, 0, toGo);\r\n                skipped = skipUntilZero(fsdis1, toGo);\r\n            }\r\n            if (i < times) {\r\n                assertEquals(buffer.length, skipped);\r\n                expectedPosition = (i + 1) * buffer.length;\r\n            } else {\r\n                if (remainder > 0) {\r\n                    assertEquals(remainder, skipped);\r\n                } else {\r\n                    assertEquals(0, skipped);\r\n                }\r\n                expectedPosition = totalLength;\r\n            }\r\n            assertEquals(expectedPosition, fsdis1.getPos());\r\n            assertEquals(expectedPosition, fsdis2.getPos());\r\n            if (toGo > 0) {\r\n                baos.write(buffer, 0, toGo);\r\n            }\r\n        }\r\n        assertEquals(0, fsdis1.skip(-1));\r\n        assertEquals(0, fsdis1.skip(0));\r\n        assertEquals(0, fsdis1.skip(1));\r\n        assertEquals(0, fsdis1.skip(Long.MAX_VALUE));\r\n        return baos.toByteArray();\r\n    } finally {\r\n        if (close) {\r\n            fsdis1.close();\r\n            fsdis2.close();\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-archives\\src\\test\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "skipUntilZero",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "long skipUntilZero(final FilterInputStream fis, final long toSkip) throws IOException\n{\r\n    long skipped = 0;\r\n    long remainsToSkip = toSkip;\r\n    long s;\r\n    while (skipped < toSkip) {\r\n        s = fis.skip(remainsToSkip);\r\n        if (s == 0) {\r\n            return skipped;\r\n        }\r\n        skipped += s;\r\n        remainsToSkip -= s;\r\n    }\r\n    return skipped;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-archives\\src\\test\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "prepareBin",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "byte[] prepareBin()\n{\r\n    byte[] bb = new byte[77777];\r\n    for (int i = 0; i < bb.length; i++) {\r\n        double d = Math.log(i + 2);\r\n        long bits = Double.doubleToLongBits(d);\r\n        bb[i] = (byte) bits;\r\n    }\r\n    return bb;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-archives\\src\\test\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "makeArchive",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String makeArchive() throws Exception\n{\r\n    return makeArchive(inputPath, null);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-archives\\src\\test\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "makeArchive",
  "errType" : null,
  "containingMethodsNum" : 9,
  "sourceCodeText" : "String makeArchive(Path parentPath, String relGlob) throws Exception\n{\r\n    final String parentPathStr = parentPath.toUri().getPath();\r\n    final String relPathGlob = relGlob == null ? \"*\" : relGlob;\r\n    System.out.println(\"parentPathStr = \" + parentPathStr);\r\n    final URI uri = fs.getUri();\r\n    final String prefix = \"har://hdfs-\" + uri.getHost() + \":\" + uri.getPort() + archivePath.toUri().getPath() + Path.SEPARATOR;\r\n    final String harName = \"foo.har\";\r\n    final String fullHarPathStr = prefix + harName;\r\n    final String[] args = { \"-archiveName\", harName, \"-p\", parentPathStr, relPathGlob, archivePath.toString() };\r\n    System.setProperty(HadoopArchives.TEST_HADOOP_ARCHIVES_JAR_PATH, HADOOP_ARCHIVES_JAR);\r\n    final HadoopArchives har = new HadoopArchives(conf);\r\n    assertEquals(0, ToolRunner.run(har, args));\r\n    return fullHarPathStr;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-archives\\src\\test\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "makeArchiveWithRepl",
  "errType" : null,
  "containingMethodsNum" : 14,
  "sourceCodeText" : "String makeArchiveWithRepl() throws Exception\n{\r\n    final String inputPathStr = inputPath.toUri().getPath();\r\n    System.out.println(\"inputPathStr = \" + inputPathStr);\r\n    final URI uri = fs.getUri();\r\n    final String prefix = \"har://hdfs-\" + uri.getHost() + \":\" + uri.getPort() + archivePath.toUri().getPath() + Path.SEPARATOR;\r\n    final String harName = \"foo.har\";\r\n    final String fullHarPathStr = prefix + harName;\r\n    final String[] args = { \"-archiveName\", harName, \"-p\", inputPathStr, \"-r\", \"2\", \"*\", archivePath.toString() };\r\n    System.setProperty(HadoopArchives.TEST_HADOOP_ARCHIVES_JAR_PATH, HADOOP_ARCHIVES_JAR);\r\n    final HadoopArchives har = new HadoopArchives(conf);\r\n    assertEquals(0, ToolRunner.run(har, args));\r\n    RemoteIterator<LocatedFileStatus> listFiles = fs.listFiles(new Path(archivePath.toString() + \"/\" + harName), false);\r\n    while (listFiles.hasNext()) {\r\n        LocatedFileStatus next = listFiles.next();\r\n        if (!next.getPath().toString().endsWith(\"_SUCCESS\")) {\r\n            assertEquals(next.getPath().toString(), 2, next.getReplication());\r\n        }\r\n    }\r\n    return fullHarPathStr;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-archives\\src\\test\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "testCopyToLocal",
  "errType" : null,
  "containingMethodsNum" : 12,
  "sourceCodeText" : "void testCopyToLocal() throws Exception\n{\r\n    final String fullHarPathStr = makeArchive();\r\n    final String tmpDir = System.getProperty(\"test.build.data\", \"build/test/data\") + \"/work-dir/har-fs-tmp\";\r\n    final Path tmpPath = new Path(tmpDir);\r\n    final LocalFileSystem localFs = FileSystem.getLocal(new Configuration());\r\n    localFs.delete(tmpPath, true);\r\n    localFs.mkdirs(tmpPath);\r\n    assertTrue(localFs.exists(tmpPath));\r\n    final HarFileSystem harFileSystem = new HarFileSystem(fs);\r\n    try {\r\n        final URI harUri = new URI(fullHarPathStr);\r\n        harFileSystem.initialize(harUri, fs.getConf());\r\n        final Path sourcePath = new Path(fullHarPathStr + Path.SEPARATOR + \"a\");\r\n        final Path targetPath = new Path(tmpPath, \"straus\");\r\n        harFileSystem.copyToLocalFile(false, sourcePath, targetPath);\r\n        FileStatus straus = localFs.getFileStatus(targetPath);\r\n        assertEquals(1, straus.getLen());\r\n    } finally {\r\n        harFileSystem.close();\r\n        localFs.delete(tmpPath, true);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
} ]
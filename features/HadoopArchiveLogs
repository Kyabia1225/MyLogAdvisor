[ {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-archive-logs\\src\\main\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "main",
  "errType" : [ "Exception" ],
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void main(String[] args)\n{\r\n    JobConf job = new JobConf(HadoopArchiveLogs.class);\r\n    HadoopArchiveLogs hal = new HadoopArchiveLogs(job);\r\n    int ret = 0;\r\n    try {\r\n        ret = ToolRunner.run(hal, args);\r\n    } catch (Exception e) {\r\n        LOG.debug(\"Exception\", e);\r\n        System.err.println(e.getClass().getSimpleName());\r\n        final String s = e.getLocalizedMessage();\r\n        if (s != null) {\r\n            System.err.println(s);\r\n        } else {\r\n            e.printStackTrace(System.err);\r\n        }\r\n        System.exit(1);\r\n    }\r\n    System.exit(ret);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-archive-logs\\src\\main\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "run",
  "errType" : null,
  "containingMethodsNum" : 30,
  "sourceCodeText" : "int run(String[] args) throws Exception\n{\r\n    int exitCode = 1;\r\n    handleOpts(args);\r\n    FileSystem fs = null;\r\n    LogAggregationFileControllerFactory factory = new LogAggregationFileControllerFactory(conf);\r\n    List<LogAggregationFileController> fileControllers = factory.getConfiguredLogAggregationFileControllerList();\r\n    if (fileControllers == null || fileControllers.isEmpty()) {\r\n        LOG.info(\"Can not find any valid fileControllers.\");\r\n        if (verbose) {\r\n            LOG.info(\"The configurated fileControllers:\" + YarnConfiguration.LOG_AGGREGATION_FILE_FORMATS);\r\n        }\r\n        return 0;\r\n    }\r\n    try {\r\n        fs = FileSystem.get(conf);\r\n        int previousTotal = 0;\r\n        for (LogAggregationFileController fileController : fileControllers) {\r\n            Path remoteRootLogDir = fileController.getRemoteRootLogDir();\r\n            String suffix = fileController.getRemoteRootLogDirSuffix();\r\n            Path workingDir = new Path(remoteRootLogDir, \"archive-logs-work\");\r\n            if (verbose) {\r\n                LOG.info(\"LogAggregationFileController:\" + fileController.getClass().getName());\r\n                LOG.info(\"Remote Log Dir Root: \" + remoteRootLogDir);\r\n                LOG.info(\"Log Suffix: \" + suffix);\r\n                LOG.info(\"Working Dir: \" + workingDir);\r\n            }\r\n            checkFilesAndSeedApps(fs, remoteRootLogDir, suffix, workingDir);\r\n            filterAppsByAggregatedStatus();\r\n            if (eligibleApplications.size() > previousTotal) {\r\n                workingDirs.add(workingDir);\r\n                previousTotal = eligibleApplications.size();\r\n            }\r\n        }\r\n        checkMaxEligible();\r\n        if (workingDirs.isEmpty() || eligibleApplications.isEmpty()) {\r\n            LOG.info(\"No eligible applications to process\");\r\n            return 0;\r\n        }\r\n        for (Path workingDir : workingDirs) {\r\n            if (!prepareWorkingDir(fs, workingDir)) {\r\n                LOG.error(\"Failed to create the workingDir:\" + workingDir.toString());\r\n                return 1;\r\n            }\r\n        }\r\n        StringBuilder sb = new StringBuilder(\"Will process the following applications:\");\r\n        for (AppInfo app : eligibleApplications) {\r\n            sb.append(\"\\n\\t\").append(app.getAppId());\r\n        }\r\n        LOG.info(sb.toString());\r\n        File localScript = File.createTempFile(\"hadoop-archive-logs-\", \".sh\");\r\n        generateScript(localScript);\r\n        exitCode = runDistributedShell(localScript) ? 0 : 1;\r\n    } finally {\r\n        if (fs != null) {\r\n            for (Path workingDir : workingDirs) {\r\n                fs.delete(workingDir, true);\r\n            }\r\n            fs.close();\r\n        }\r\n    }\r\n    return exitCode;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-archive-logs\\src\\main\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "handleOpts",
  "errType" : [ "ParseException" ],
  "containingMethodsNum" : 30,
  "sourceCodeText" : "void handleOpts(String[] args) throws ParseException\n{\r\n    Options opts = new Options();\r\n    Option helpOpt = new Option(HELP_OPTION, false, \"Prints this message\");\r\n    Option maxEligibleOpt = new Option(MAX_ELIGIBLE_APPS_OPTION, true, \"The maximum number of eligible apps to process (default: \" + DEFAULT_MAX_ELIGIBLE + \" (all))\");\r\n    maxEligibleOpt.setArgName(\"n\");\r\n    Option minNumLogFilesOpt = new Option(MIN_NUM_LOG_FILES_OPTION, true, \"The minimum number of log files required to be eligible (default: \" + DEFAULT_MIN_NUM_LOG_FILES + \")\");\r\n    minNumLogFilesOpt.setArgName(\"n\");\r\n    Option maxTotalLogsSizeOpt = new Option(MAX_TOTAL_LOGS_SIZE_OPTION, true, \"The maximum total logs size (in megabytes) required to be eligible\" + \" (default: \" + DEFAULT_MAX_TOTAL_LOGS_SIZE + \")\");\r\n    maxTotalLogsSizeOpt.setArgName(\"megabytes\");\r\n    Option memoryOpt = new Option(MEMORY_OPTION, true, \"The amount of memory (in megabytes) for each container (default: \" + DEFAULT_MEMORY + \")\");\r\n    memoryOpt.setArgName(\"megabytes\");\r\n    Option verboseOpt = new Option(VERBOSE_OPTION, false, \"Print more details.\");\r\n    Option forceOpt = new Option(FORCE_OPTION, false, \"Force recreating the working directory if an existing one is found. \" + \"This should only be used if you know that another instance is \" + \"not currently running\");\r\n    Option noProxyOpt = new Option(NO_PROXY_OPTION, false, \"When specified, all processing will be done as the user running this\" + \" command (or the Yarn user if DefaultContainerExecutor is in \" + \"use). When not specified, all processing will be done as the \" + \"user who owns that application; if the user running this command\" + \" is not allowed to impersonate that user, it will fail\");\r\n    opts.addOption(helpOpt);\r\n    opts.addOption(maxEligibleOpt);\r\n    opts.addOption(minNumLogFilesOpt);\r\n    opts.addOption(maxTotalLogsSizeOpt);\r\n    opts.addOption(memoryOpt);\r\n    opts.addOption(verboseOpt);\r\n    opts.addOption(forceOpt);\r\n    opts.addOption(noProxyOpt);\r\n    try {\r\n        CommandLineParser parser = new GnuParser();\r\n        CommandLine commandLine = parser.parse(opts, args);\r\n        if (commandLine.hasOption(HELP_OPTION)) {\r\n            HelpFormatter formatter = new HelpFormatter();\r\n            formatter.printHelp(\"mapred archive-logs\", opts);\r\n            System.exit(0);\r\n        }\r\n        if (commandLine.hasOption(MAX_ELIGIBLE_APPS_OPTION)) {\r\n            maxEligible = Integer.parseInt(commandLine.getOptionValue(MAX_ELIGIBLE_APPS_OPTION));\r\n            if (maxEligible == 0) {\r\n                LOG.info(\"Setting \" + MAX_ELIGIBLE_APPS_OPTION + \" to 0 accomplishes \" + \"nothing. Please either set it to a negative value \" + \"(default, all) or a more reasonable value.\");\r\n                System.exit(0);\r\n            }\r\n        }\r\n        if (commandLine.hasOption(MIN_NUM_LOG_FILES_OPTION)) {\r\n            minNumLogFiles = Integer.parseInt(commandLine.getOptionValue(MIN_NUM_LOG_FILES_OPTION));\r\n        }\r\n        if (commandLine.hasOption(MAX_TOTAL_LOGS_SIZE_OPTION)) {\r\n            maxTotalLogsSize = Long.parseLong(commandLine.getOptionValue(MAX_TOTAL_LOGS_SIZE_OPTION));\r\n            maxTotalLogsSize *= 1024L * 1024L;\r\n        }\r\n        if (commandLine.hasOption(MEMORY_OPTION)) {\r\n            memory = Long.parseLong(commandLine.getOptionValue(MEMORY_OPTION));\r\n        }\r\n        if (commandLine.hasOption(VERBOSE_OPTION)) {\r\n            verbose = true;\r\n        }\r\n        if (commandLine.hasOption(FORCE_OPTION)) {\r\n            force = true;\r\n        }\r\n        if (commandLine.hasOption(NO_PROXY_OPTION)) {\r\n            proxy = false;\r\n        }\r\n    } catch (ParseException pe) {\r\n        HelpFormatter formatter = new HelpFormatter();\r\n        formatter.printHelp(\"mapred archive-logs\", opts);\r\n        throw pe;\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-archive-logs\\src\\main\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "prepareWorkingDir",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "boolean prepareWorkingDir(FileSystem fs, Path workingDir) throws IOException\n{\r\n    if (fs.exists(workingDir)) {\r\n        if (force) {\r\n            LOG.info(\"Existing Working Dir detected: -\" + FORCE_OPTION + \" specified -> recreating Working Dir\");\r\n            fs.delete(workingDir, true);\r\n        } else {\r\n            LOG.info(\"Existing Working Dir detected: -\" + FORCE_OPTION + \" not specified -> exiting\");\r\n            return false;\r\n        }\r\n    }\r\n    fs.mkdirs(workingDir);\r\n    fs.setPermission(workingDir, new FsPermission(FsAction.ALL, FsAction.ALL, FsAction.ALL, true));\r\n    return true;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-archive-logs\\src\\main\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "filterAppsByAggregatedStatus",
  "errType" : [ "ApplicationNotFoundException" ],
  "containingMethodsNum" : 19,
  "sourceCodeText" : "void filterAppsByAggregatedStatus() throws IOException, YarnException\n{\r\n    YarnClient client = YarnClient.createYarnClient();\r\n    try {\r\n        client.init(getConf());\r\n        client.start();\r\n        for (Iterator<AppInfo> it = eligibleApplications.iterator(); it.hasNext(); ) {\r\n            AppInfo app = it.next();\r\n            try {\r\n                ApplicationReport report = client.getApplicationReport(ApplicationId.fromString(app.getAppId()));\r\n                LogAggregationStatus aggStatus = report.getLogAggregationStatus();\r\n                if (aggStatus.equals(LogAggregationStatus.RUNNING) || aggStatus.equals(LogAggregationStatus.RUNNING_WITH_FAILURE) || aggStatus.equals(LogAggregationStatus.NOT_START) || aggStatus.equals(LogAggregationStatus.DISABLED) || aggStatus.equals(LogAggregationStatus.FAILED)) {\r\n                    if (verbose) {\r\n                        LOG.info(\"Skipping \" + app.getAppId() + \" due to aggregation status being \" + aggStatus);\r\n                    }\r\n                    it.remove();\r\n                } else {\r\n                    if (verbose) {\r\n                        LOG.info(app.getAppId() + \" has aggregation status \" + aggStatus);\r\n                    }\r\n                    app.setFinishTime(report.getFinishTime());\r\n                }\r\n            } catch (ApplicationNotFoundException e) {\r\n                if (verbose) {\r\n                    LOG.info(app.getAppId() + \" not in the ResourceManager\");\r\n                }\r\n            }\r\n        }\r\n    } finally {\r\n        if (client != null) {\r\n            client.stop();\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 2,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-archive-logs\\src\\main\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "checkFilesAndSeedApps",
  "errType" : [ "IOException", "IOException" ],
  "containingMethodsNum" : 21,
  "sourceCodeText" : "void checkFilesAndSeedApps(FileSystem fs, Path remoteRootLogDir, String suffix, Path workingDir) throws IOException\n{\r\n    for (RemoteIterator<FileStatus> userIt = fs.listStatusIterator(remoteRootLogDir); userIt.hasNext(); ) {\r\n        Path userLogPath = userIt.next().getPath();\r\n        try {\r\n            for (RemoteIterator<FileStatus> appIt = fs.listStatusIterator(new Path(userLogPath, suffix)); appIt.hasNext(); ) {\r\n                Path appLogPath = appIt.next().getPath();\r\n                try {\r\n                    FileStatus[] files = fs.listStatus(appLogPath);\r\n                    if (files.length >= minNumLogFiles) {\r\n                        boolean eligible = true;\r\n                        long totalFileSize = 0L;\r\n                        for (FileStatus file : files) {\r\n                            if (file.getPath().getName().equals(appLogPath.getName() + \".har\")) {\r\n                                eligible = false;\r\n                                if (verbose) {\r\n                                    LOG.info(\"Skipping \" + appLogPath.getName() + \" due to existing .har file\");\r\n                                }\r\n                                break;\r\n                            }\r\n                            totalFileSize += file.getLen();\r\n                            if (totalFileSize > maxTotalLogsSize) {\r\n                                eligible = false;\r\n                                if (verbose) {\r\n                                    LOG.info(\"Skipping \" + appLogPath.getName() + \" due to \" + \"total file size being too large (\" + totalFileSize + \" > \" + maxTotalLogsSize + \")\");\r\n                                }\r\n                                break;\r\n                            }\r\n                        }\r\n                        if (eligible) {\r\n                            if (verbose) {\r\n                                LOG.info(\"Adding \" + appLogPath.getName() + \" for user \" + userLogPath.getName());\r\n                            }\r\n                            AppInfo context = new AppInfo();\r\n                            context.setAppId(appLogPath.getName());\r\n                            context.setUser(userLogPath.getName());\r\n                            context.setSuffix(suffix);\r\n                            context.setRemoteRootLogDir(remoteRootLogDir);\r\n                            context.setWorkingDir(workingDir);\r\n                            eligibleApplications.add(context);\r\n                        }\r\n                    } else {\r\n                        if (verbose) {\r\n                            LOG.info(\"Skipping \" + appLogPath.getName() + \" due to not \" + \"having enough log files (\" + files.length + \" < \" + minNumLogFiles + \")\");\r\n                        }\r\n                    }\r\n                } catch (IOException ioe) {\r\n                    if (verbose) {\r\n                        LOG.info(\"Skipping logs under \" + appLogPath + \" due to \" + ioe.getMessage());\r\n                    }\r\n                }\r\n            }\r\n        } catch (IOException ioe) {\r\n            if (verbose) {\r\n                LOG.info(\"Skipping all logs under \" + userLogPath + \" due to \" + ioe.getMessage());\r\n            }\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 2,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-archive-logs\\src\\main\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "checkMaxEligible",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void checkMaxEligible()\n{\r\n    if (maxEligible > 0 && eligibleApplications.size() > maxEligible) {\r\n        if (verbose) {\r\n            LOG.info(\"Too many applications (\" + eligibleApplications.size() + \" > \" + maxEligible + \")\");\r\n        }\r\n        List<AppInfo> sortedApplications = new ArrayList<AppInfo>(eligibleApplications);\r\n        Collections.sort(sortedApplications, new Comparator<AppInfo>() {\r\n\r\n            @Override\r\n            public int compare(AppInfo o1, AppInfo o2) {\r\n                int lCompare = Long.compare(o1.getFinishTime(), o2.getFinishTime());\r\n                if (lCompare == 0) {\r\n                    return o1.getAppId().compareTo(o2.getAppId());\r\n                }\r\n                return lCompare;\r\n            }\r\n        });\r\n        for (int i = maxEligible; i < sortedApplications.size(); i++) {\r\n            if (verbose) {\r\n                LOG.info(\"Removing \" + sortedApplications.get(i));\r\n            }\r\n            eligibleApplications.remove(sortedApplications.get(i));\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-archive-logs\\src\\main\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "generateScript",
  "errType" : null,
  "containingMethodsNum" : 34,
  "sourceCodeText" : "void generateScript(File localScript) throws IOException\n{\r\n    if (verbose) {\r\n        LOG.info(\"Generating script at: \" + localScript.getAbsolutePath());\r\n    }\r\n    String halrJarPath = HadoopArchiveLogsRunner.class.getProtectionDomain().getCodeSource().getLocation().getPath();\r\n    String harJarPath = HadoopArchives.class.getProtectionDomain().getCodeSource().getLocation().getPath();\r\n    String classpath = halrJarPath + File.pathSeparator + harJarPath;\r\n    FileWriterWithEncoding fw = null;\r\n    try {\r\n        fw = new FileWriterWithEncoding(localScript, \"UTF-8\");\r\n        fw.write(\"#!/bin/bash\\nset -e\\nset -x\\n\");\r\n        int containerCount = 1;\r\n        for (AppInfo context : eligibleApplications) {\r\n            fw.write(\"if [ \\\"$YARN_SHELL_ID\\\" == \\\"\");\r\n            fw.write(Integer.toString(containerCount));\r\n            fw.write(\"\\\" ]; then\\n\\tappId=\\\"\");\r\n            fw.write(context.getAppId());\r\n            fw.write(\"\\\"\\n\\tuser=\\\"\");\r\n            fw.write(context.getUser());\r\n            fw.write(\"\\\"\\n\\tworkingDir=\\\"\");\r\n            fw.write(context.getWorkingDir().toString());\r\n            fw.write(\"\\\"\\n\\tremoteRootLogDir=\\\"\");\r\n            fw.write(context.getRemoteRootLogDir().toString());\r\n            fw.write(\"\\\"\\n\\tsuffix=\\\"\");\r\n            fw.write(context.getSuffix());\r\n            fw.write(\"\\\"\\nel\");\r\n            containerCount++;\r\n        }\r\n        fw.write(\"se\\n\\techo \\\"Unknown Mapping!\\\"\\n\\texit 1\\nfi\\n\");\r\n        fw.write(\"export HADOOP_CLIENT_OPTS=\\\"-Xmx\");\r\n        fw.write(Long.toString(memory));\r\n        fw.write(\"m\\\"\\n\");\r\n        fw.write(\"export HADOOP_CLASSPATH=\");\r\n        fw.write(classpath);\r\n        fw.write(\"\\n\\\"$HADOOP_HOME\\\"/bin/hadoop \");\r\n        fw.write(HadoopArchiveLogsRunner.class.getName());\r\n        fw.write(\" -appId \\\"$appId\\\" -user \\\"$user\\\" -workingDir \");\r\n        fw.write(\"\\\"$workingDir\\\"\");\r\n        fw.write(\" -remoteRootLogDir \");\r\n        fw.write(\"\\\"$remoteRootLogDir\\\"\");\r\n        fw.write(\" -suffix \");\r\n        fw.write(\"\\\"$suffix\\\"\");\r\n        if (!proxy) {\r\n            fw.write(\" -noProxy\\n\");\r\n        }\r\n        fw.write(\"\\n\");\r\n    } finally {\r\n        if (fw != null) {\r\n            fw.close();\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-archive-logs\\src\\main\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "runDistributedShell",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "boolean runDistributedShell(File localScript) throws Exception\n{\r\n    String[] dsArgs = { \"--appname\", \"ArchiveLogs\", \"--jar\", ApplicationMaster.class.getProtectionDomain().getCodeSource().getLocation().getPath(), \"--num_containers\", Integer.toString(eligibleApplications.size()), \"--container_memory\", Long.toString(memory), \"--shell_script\", localScript.getAbsolutePath() };\r\n    if (verbose) {\r\n        LOG.info(\"Running Distributed Shell with arguments: \" + Arrays.toString(dsArgs));\r\n    }\r\n    final Client dsClient = new Client(new Configuration(conf));\r\n    dsClient.init(dsArgs);\r\n    return dsClient.run();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-archive-logs\\src\\main\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "setConf",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setConf(Configuration conf)\n{\r\n    if (conf instanceof JobConf) {\r\n        this.conf = (JobConf) conf;\r\n    } else {\r\n        this.conf = new JobConf(conf, HadoopArchiveLogs.class);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-archive-logs\\src\\main\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "getConf",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Configuration getConf()\n{\r\n    return this.conf;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-archive-logs\\src\\main\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "main",
  "errType" : [ "Exception" ],
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void main(String[] args)\n{\r\n    JobConf job = new JobConf(HadoopArchiveLogsRunner.class);\r\n    HadoopArchiveLogsRunner halr = new HadoopArchiveLogsRunner(job);\r\n    int ret = 0;\r\n    try {\r\n        ret = ToolRunner.run(halr, args);\r\n    } catch (Exception e) {\r\n        LOG.debug(\"Exception\", e);\r\n        System.err.println(e.getClass().getSimpleName());\r\n        final String s = e.getLocalizedMessage();\r\n        if (s != null) {\r\n            System.err.println(s);\r\n        } else {\r\n            e.printStackTrace(System.err);\r\n        }\r\n        System.exit(1);\r\n    }\r\n    System.exit(ret);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-archive-logs\\src\\main\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "run",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "int run(String[] args) throws Exception\n{\r\n    handleOpts(args);\r\n    Integer exitCode = 1;\r\n    UserGroupInformation loginUser = UserGroupInformation.getLoginUser();\r\n    if (!proxy || loginUser.getShortUserName().equals(user)) {\r\n        LOG.info(\"Running as \" + user);\r\n        exitCode = runInternal();\r\n    } else {\r\n        LOG.info(\"Running as \" + loginUser.getShortUserName() + \" but will \" + \"impersonate \" + user);\r\n        UserGroupInformation proxyUser = UserGroupInformation.createProxyUser(user, loginUser);\r\n        exitCode = proxyUser.doAs(new PrivilegedExceptionAction<Integer>() {\r\n\r\n            @Override\r\n            public Integer run() throws Exception {\r\n                return runInternal();\r\n            }\r\n        });\r\n    }\r\n    return exitCode;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-archive-logs\\src\\main\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "runInternal",
  "errType" : null,
  "containingMethodsNum" : 16,
  "sourceCodeText" : "int runInternal() throws Exception\n{\r\n    String remoteAppLogDir = remoteLogDir + File.separator + user + File.separator + suffix + File.separator + appId;\r\n    conf.set(\"mapreduce.framework.name\", \"local\");\r\n    conf.set(\"fs.permissions.umask-mode\", \"027\");\r\n    String harName = appId + \".har\";\r\n    String[] haArgs = { \"-archiveName\", harName, \"-p\", remoteAppLogDir, \"*\", workingDir };\r\n    StringBuilder sb = new StringBuilder(\"Executing 'hadoop archives'\");\r\n    for (String haArg : haArgs) {\r\n        sb.append(\"\\n\\t\").append(haArg);\r\n    }\r\n    LOG.info(sb.toString());\r\n    int exitCode = hadoopArchives.run(haArgs);\r\n    if (exitCode != 0) {\r\n        LOG.warn(\"Failed to create archives for \" + appId);\r\n        return -1;\r\n    }\r\n    FileSystem fs = null;\r\n    try {\r\n        fs = FileSystem.get(conf);\r\n        Path harPath = new Path(workingDir, harName);\r\n        if (!fs.exists(harPath) || fs.listStatus(harPath).length == 0) {\r\n            LOG.warn(\"The created archive \\\"\" + harName + \"\\\" is missing or empty.\");\r\n            return -1;\r\n        }\r\n        Path harDest = new Path(remoteAppLogDir, harName);\r\n        LOG.info(\"Moving har to original location\");\r\n        fs.rename(harPath, harDest);\r\n        LOG.info(\"Deleting original logs\");\r\n        for (FileStatus original : fs.listStatus(new Path(remoteAppLogDir), new PathFilter() {\r\n\r\n            @Override\r\n            public boolean accept(Path path) {\r\n                return !path.getName().endsWith(\".har\");\r\n            }\r\n        })) {\r\n            fs.delete(original.getPath(), false);\r\n        }\r\n    } finally {\r\n        if (fs != null) {\r\n            fs.close();\r\n        }\r\n    }\r\n    return 0;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-archive-logs\\src\\main\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "handleOpts",
  "errType" : null,
  "containingMethodsNum" : 18,
  "sourceCodeText" : "void handleOpts(String[] args) throws ParseException\n{\r\n    Options opts = new Options();\r\n    Option appIdOpt = new Option(APP_ID_OPTION, true, \"Application ID\");\r\n    appIdOpt.setRequired(true);\r\n    Option userOpt = new Option(USER_OPTION, true, \"User\");\r\n    userOpt.setRequired(true);\r\n    Option workingDirOpt = new Option(WORKING_DIR_OPTION, true, \"Working Directory\");\r\n    workingDirOpt.setRequired(true);\r\n    Option remoteLogDirOpt = new Option(REMOTE_ROOT_LOG_DIR_OPTION, true, \"Remote Root Log Directory\");\r\n    remoteLogDirOpt.setRequired(true);\r\n    Option suffixOpt = new Option(SUFFIX_OPTION, true, \"Suffix\");\r\n    suffixOpt.setRequired(true);\r\n    Option useProxyOpt = new Option(NO_PROXY_OPTION, false, \"Use Proxy\");\r\n    opts.addOption(appIdOpt);\r\n    opts.addOption(userOpt);\r\n    opts.addOption(workingDirOpt);\r\n    opts.addOption(remoteLogDirOpt);\r\n    opts.addOption(suffixOpt);\r\n    opts.addOption(useProxyOpt);\r\n    CommandLineParser parser = new GnuParser();\r\n    CommandLine commandLine = parser.parse(opts, args);\r\n    appId = commandLine.getOptionValue(APP_ID_OPTION);\r\n    user = commandLine.getOptionValue(USER_OPTION);\r\n    workingDir = commandLine.getOptionValue(WORKING_DIR_OPTION);\r\n    remoteLogDir = commandLine.getOptionValue(REMOTE_ROOT_LOG_DIR_OPTION);\r\n    suffix = commandLine.getOptionValue(SUFFIX_OPTION);\r\n    proxy = true;\r\n    if (commandLine.hasOption(NO_PROXY_OPTION)) {\r\n        proxy = false;\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-archive-logs\\src\\main\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "setConf",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setConf(Configuration conf)\n{\r\n    if (conf instanceof JobConf) {\r\n        this.conf = (JobConf) conf;\r\n    } else {\r\n        this.conf = new JobConf(conf, HadoopArchiveLogsRunner.class);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-archive-logs\\src\\main\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "getConf",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Configuration getConf()\n{\r\n    return this.conf;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
} ]
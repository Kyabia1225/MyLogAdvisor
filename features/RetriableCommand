[ {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\main\\java\\org\\apache\\hadoop\\tools\\util",
  "methodName" : "doExecute",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Object doExecute(Object... arguments) throws Exception",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\main\\java\\org\\apache\\hadoop\\tools\\util",
  "methodName" : "execute",
  "errType" : [ "Exception" ],
  "containingMethodsNum" : 4,
  "sourceCodeText" : "Object execute(Object... arguments) throws Exception\n{\r\n    Exception latestException;\r\n    int counter = 0;\r\n    while (true) {\r\n        try {\r\n            return doExecute(arguments);\r\n        } catch (Exception exception) {\r\n            LOG.error(\"Failure in Retriable command: \" + description, exception);\r\n            latestException = exception;\r\n        }\r\n        counter++;\r\n        RetryAction action = retryPolicy.shouldRetry(latestException, counter, 0, true);\r\n        if (action.action == RetryPolicy.RetryAction.RetryDecision.RETRY) {\r\n            ThreadUtil.sleepAtLeastIgnoreInterrupts(action.delayMillis);\r\n        } else {\r\n            break;\r\n        }\r\n    }\r\n    throw new IOException(\"Couldn't run retriable-command: \" + description, latestException);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\main\\java\\org\\apache\\hadoop\\tools\\util",
  "methodName" : "setRetryPolicy",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "RetriableCommand setRetryPolicy(RetryPolicy retryHandler)\n{\r\n    this.retryPolicy = retryHandler;\r\n    return this;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\main\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "validatePaths",
  "errType" : [ "FileNotFoundException" ],
  "containingMethodsNum" : 19,
  "sourceCodeText" : "void validatePaths(DistCpContext context) throws IOException, InvalidInputException\n{\r\n    Path targetPath = context.getTargetPath();\r\n    FileSystem targetFS = targetPath.getFileSystem(getConf());\r\n    boolean targetExists = false;\r\n    boolean targetIsFile = false;\r\n    try {\r\n        targetIsFile = targetFS.getFileStatus(targetPath).isFile();\r\n        targetExists = true;\r\n    } catch (FileNotFoundException ignored) {\r\n    }\r\n    targetPath = targetFS.makeQualified(targetPath);\r\n    final boolean targetIsReservedRaw = Path.getPathWithoutSchemeAndAuthority(targetPath).toString().startsWith(HDFS_RESERVED_RAW_DIRECTORY_NAME);\r\n    if (targetIsFile) {\r\n        if (context.getSourcePaths().size() > 1) {\r\n            throw new InvalidInputException(\"Multiple source being copied to a file: \" + targetPath);\r\n        }\r\n        Path srcPath = context.getSourcePaths().get(0);\r\n        FileSystem sourceFS = srcPath.getFileSystem(getConf());\r\n        if (!sourceFS.isFile(srcPath)) {\r\n            throw new InvalidInputException(\"Cannot copy \" + srcPath + \", which is not a file to \" + targetPath);\r\n        }\r\n    }\r\n    if (context.shouldAtomicCommit() && targetExists) {\r\n        throw new InvalidInputException(\"Target path for atomic-commit already exists: \" + targetPath + \". Cannot atomic-commit to pre-existing target-path.\");\r\n    }\r\n    for (Path path : context.getSourcePaths()) {\r\n        FileSystem fs = path.getFileSystem(getConf());\r\n        if (!fs.exists(path)) {\r\n            throw new InvalidInputException(path + \" doesn't exist\");\r\n        }\r\n        if (Path.getPathWithoutSchemeAndAuthority(path).toString().startsWith(HDFS_RESERVED_RAW_DIRECTORY_NAME)) {\r\n            if (!targetIsReservedRaw) {\r\n                final String msg = \"The source path '\" + path + \"' starts with \" + HDFS_RESERVED_RAW_DIRECTORY_NAME + \" but the target path '\" + targetPath + \"' does not. Either all or none of the paths must \" + \"have this prefix.\";\r\n                throw new InvalidInputException(msg);\r\n            }\r\n        } else if (targetIsReservedRaw) {\r\n            final String msg = \"The target path '\" + targetPath + \"' starts with \" + HDFS_RESERVED_RAW_DIRECTORY_NAME + \" but the source path '\" + path + \"' does not. Either all or none of the paths must \" + \"have this prefix.\";\r\n            throw new InvalidInputException(msg);\r\n        }\r\n    }\r\n    if (targetIsReservedRaw) {\r\n        context.setPreserveRawXattrs(true);\r\n        getConf().setBoolean(DistCpConstants.CONF_LABEL_PRESERVE_RAWXATTRS, true);\r\n    }\r\n    Credentials credentials = getCredentials();\r\n    if (credentials != null) {\r\n        Path[] inputPaths = context.getSourcePaths().toArray(new Path[1]);\r\n        TokenCache.obtainTokensForNamenodes(credentials, inputPaths, getConf());\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\main\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "doBuildListing",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void doBuildListing(Path pathToListingFile, DistCpContext context) throws IOException\n{\r\n    if (context.shouldUseSnapshotDiff()) {\r\n        doBuildListingWithSnapshotDiff(getWriter(pathToListingFile), context);\r\n    } else {\r\n        doBuildListing(getWriter(pathToListingFile), context);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\main\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "getPathWithSchemeAndAuthority",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "Path getPathWithSchemeAndAuthority(Path path) throws IOException\n{\r\n    FileSystem fs = path.getFileSystem(getConf());\r\n    String scheme = path.toUri().getScheme();\r\n    if (scheme == null) {\r\n        scheme = fs.getUri().getScheme();\r\n    }\r\n    String authority = path.toUri().getAuthority();\r\n    if (authority == null) {\r\n        authority = fs.getUri().getAuthority();\r\n    }\r\n    return new Path(scheme, authority, makeQualified(path).toUri().getPath());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\main\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "addToFileListing",
  "errType" : null,
  "containingMethodsNum" : 10,
  "sourceCodeText" : "void addToFileListing(SequenceFile.Writer fileListWriter, Path sourceRoot, Path path, DistCpContext context) throws IOException\n{\r\n    sourceRoot = getPathWithSchemeAndAuthority(sourceRoot);\r\n    path = getPathWithSchemeAndAuthority(path);\r\n    path = makeQualified(path);\r\n    FileSystem sourceFS = sourceRoot.getFileSystem(getConf());\r\n    FileStatus fileStatus = sourceFS.getFileStatus(path);\r\n    final boolean preserveAcls = context.shouldPreserve(FileAttribute.ACL);\r\n    final boolean preserveXAttrs = context.shouldPreserve(FileAttribute.XATTR);\r\n    final boolean preserveRawXAttrs = context.shouldPreserveRawXattrs();\r\n    LinkedList<CopyListingFileStatus> fileCopyListingStatus = DistCpUtils.toCopyListingFileStatus(sourceFS, fileStatus, preserveAcls, preserveXAttrs, preserveRawXAttrs, context.getBlocksPerChunk());\r\n    writeToFileListingRoot(fileListWriter, fileCopyListingStatus, sourceRoot, context);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\main\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "doBuildListingWithSnapshotDiff",
  "errType" : null,
  "containingMethodsNum" : 18,
  "sourceCodeText" : "void doBuildListingWithSnapshotDiff(SequenceFile.Writer fileListWriter, DistCpContext context) throws IOException\n{\r\n    ArrayList<DiffInfo> diffList = distCpSync.prepareDiffListForCopyListing();\r\n    Path sourceRoot = context.getSourcePaths().get(0);\r\n    FileSystem sourceFS = sourceRoot.getFileSystem(getConf());\r\n    try {\r\n        List<FileStatusInfo> fileStatuses = Lists.newArrayList();\r\n        for (DiffInfo diff : diffList) {\r\n            diff.setTarget(new Path(context.getSourcePaths().get(0), diff.getTarget()));\r\n            if (diff.getType() == SnapshotDiffReport.DiffType.MODIFY) {\r\n                addToFileListing(fileListWriter, sourceRoot, diff.getTarget(), context);\r\n            } else if (diff.getType() == SnapshotDiffReport.DiffType.CREATE) {\r\n                addToFileListing(fileListWriter, sourceRoot, diff.getTarget(), context);\r\n                FileStatus sourceStatus = sourceFS.getFileStatus(diff.getTarget());\r\n                if (sourceStatus.isDirectory()) {\r\n                    LOG.debug(\"Adding source dir for traverse: {}\", sourceStatus.getPath());\r\n                    HashSet<String> excludeList = distCpSync.getTraverseExcludeList(diff.getSource(), context.getSourcePaths().get(0));\r\n                    ArrayList<FileStatus> sourceDirs = new ArrayList<>();\r\n                    sourceDirs.add(sourceStatus);\r\n                    new TraverseDirectory(fileListWriter, sourceFS, sourceDirs, sourceRoot, context, excludeList, fileStatuses).traverseDirectory();\r\n                }\r\n            }\r\n        }\r\n        if (randomizeFileListing) {\r\n            writeToFileListing(fileStatuses, fileListWriter);\r\n        }\r\n        fileListWriter.close();\r\n        fileListWriter = null;\r\n    } finally {\r\n        IOUtils.cleanupWithLogger(LOG, fileListWriter);\r\n    }\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\main\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "doBuildListing",
  "errType" : null,
  "containingMethodsNum" : 28,
  "sourceCodeText" : "void doBuildListing(SequenceFile.Writer fileListWriter, DistCpContext context) throws IOException\n{\r\n    if (context.getNumListstatusThreads() > 0) {\r\n        numListstatusThreads = context.getNumListstatusThreads();\r\n    }\r\n    try {\r\n        List<FileStatusInfo> statusList = Lists.newArrayList();\r\n        for (Path path : context.getSourcePaths()) {\r\n            FileSystem sourceFS = path.getFileSystem(getConf());\r\n            final boolean preserveAcls = context.shouldPreserve(FileAttribute.ACL);\r\n            final boolean preserveXAttrs = context.shouldPreserve(FileAttribute.XATTR);\r\n            final boolean preserveRawXAttrs = context.shouldPreserveRawXattrs();\r\n            path = makeQualified(path);\r\n            FileStatus rootStatus = sourceFS.getFileStatus(path);\r\n            Path sourcePathRoot = computeSourceRootPath(rootStatus, context);\r\n            FileStatus[] sourceFiles = sourceFS.listStatus(path);\r\n            boolean explore = (sourceFiles != null && sourceFiles.length > 0);\r\n            if (!explore || rootStatus.isDirectory()) {\r\n                LinkedList<CopyListingFileStatus> rootCopyListingStatus = DistCpUtils.toCopyListingFileStatus(sourceFS, rootStatus, preserveAcls, preserveXAttrs, preserveRawXAttrs, context.getBlocksPerChunk());\r\n                writeToFileListingRoot(fileListWriter, rootCopyListingStatus, sourcePathRoot, context);\r\n            }\r\n            if (explore) {\r\n                ArrayList<FileStatus> sourceDirs = new ArrayList<FileStatus>();\r\n                for (FileStatus sourceStatus : sourceFiles) {\r\n                    LOG.debug(\"Recording source-path: {} for copy.\", sourceStatus.getPath());\r\n                    LinkedList<CopyListingFileStatus> sourceCopyListingStatus = DistCpUtils.toCopyListingFileStatus(sourceFS, sourceStatus, preserveAcls && sourceStatus.isDirectory(), preserveXAttrs && sourceStatus.isDirectory(), preserveRawXAttrs && sourceStatus.isDirectory(), context.getBlocksPerChunk());\r\n                    for (CopyListingFileStatus fs : sourceCopyListingStatus) {\r\n                        if (randomizeFileListing) {\r\n                            addToFileListing(statusList, new FileStatusInfo(fs, sourcePathRoot), fileListWriter);\r\n                        } else {\r\n                            writeToFileListing(fileListWriter, fs, sourcePathRoot);\r\n                        }\r\n                    }\r\n                    if (sourceStatus.isDirectory()) {\r\n                        LOG.debug(\"Adding source dir for traverse: {}\", sourceStatus.getPath());\r\n                        sourceDirs.add(sourceStatus);\r\n                    }\r\n                }\r\n                new TraverseDirectory(fileListWriter, sourceFS, sourceDirs, sourcePathRoot, context, null, statusList).traverseDirectory();\r\n            }\r\n        }\r\n        if (randomizeFileListing) {\r\n            writeToFileListing(statusList, fileListWriter);\r\n        }\r\n        fileListWriter.close();\r\n        printStats();\r\n        LOG.info(\"Build file listing completed.\");\r\n        fileListWriter = null;\r\n    } finally {\r\n        IOUtils.cleanupWithLogger(LOG, fileListWriter);\r\n    }\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\main\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "addToFileListing",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void addToFileListing(List<FileStatusInfo> fileStatusInfoList, FileStatusInfo statusInfo, SequenceFile.Writer fileListWriter) throws IOException\n{\r\n    fileStatusInfoList.add(statusInfo);\r\n    if (fileStatusInfoList.size() > fileStatusLimit) {\r\n        writeToFileListing(fileStatusInfoList, fileListWriter);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\main\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "setSeedForRandomListing",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setSeedForRandomListing(long seed)\n{\r\n    this.rnd.setSeed(seed);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\main\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "writeToFileListing",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void writeToFileListing(List<FileStatusInfo> fileStatusInfoList, SequenceFile.Writer fileListWriter) throws IOException\n{\r\n    Collections.shuffle(fileStatusInfoList, rnd);\r\n    for (FileStatusInfo fileStatusInfo : fileStatusInfoList) {\r\n        LOG.debug(\"Adding {}\", fileStatusInfo.fileStatus.getPath());\r\n        writeToFileListing(fileListWriter, fileStatusInfo.fileStatus, fileStatusInfo.sourceRootPath);\r\n    }\r\n    LOG.debug(\"Number of paths written to fileListing={}\", fileStatusInfoList.size());\r\n    fileStatusInfoList.clear();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\main\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "computeSourceRootPath",
  "errType" : null,
  "containingMethodsNum" : 15,
  "sourceCodeText" : "Path computeSourceRootPath(FileStatus sourceStatus, DistCpContext context) throws IOException\n{\r\n    Path target = context.getTargetPath();\r\n    FileSystem targetFS = target.getFileSystem(getConf());\r\n    final boolean targetPathExists = context.isTargetPathExists();\r\n    boolean solitaryFile = context.getSourcePaths().size() == 1 && !sourceStatus.isDirectory();\r\n    if (solitaryFile) {\r\n        if (!targetPathExists || targetFS.isFile(target)) {\r\n            return sourceStatus.getPath();\r\n        } else {\r\n            return sourceStatus.getPath().getParent();\r\n        }\r\n    } else {\r\n        boolean specialHandling = (context.getSourcePaths().size() == 1 && !targetPathExists) || context.shouldSyncFolder() || context.shouldOverwrite();\r\n        if ((specialHandling && sourceStatus.isDirectory()) || sourceStatus.getPath().isRoot()) {\r\n            return sourceStatus.getPath();\r\n        } else {\r\n            return sourceStatus.getPath().getParent();\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\main\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "shouldCopy",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "boolean shouldCopy(Path path)\n{\r\n    return copyFilter.shouldCopy(path);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\main\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "getBytesToCopy",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getBytesToCopy()\n{\r\n    return totalBytesToCopy;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\main\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "getNumberOfPaths",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getNumberOfPaths()\n{\r\n    return totalPaths;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\main\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "makeQualified",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "Path makeQualified(Path path) throws IOException\n{\r\n    final FileSystem fs = path.getFileSystem(getConf());\r\n    return path.makeQualified(fs.getUri(), fs.getWorkingDirectory());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\main\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "getWriter",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "SequenceFile.Writer getWriter(Path pathToListFile) throws IOException\n{\r\n    FileSystem fs = pathToListFile.getFileSystem(getConf());\r\n    fs.delete(pathToListFile, false);\r\n    return SequenceFile.createWriter(getConf(), SequenceFile.Writer.file(pathToListFile), SequenceFile.Writer.keyClass(Text.class), SequenceFile.Writer.valueClass(CopyListingFileStatus.class), SequenceFile.Writer.compression(SequenceFile.CompressionType.NONE));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\main\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "printStats",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void printStats()\n{\r\n    LOG.info(\"Paths (files+dirs) cnt = {}; dirCnt = {}\", totalPaths, totalDirs);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\main\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "maybePrintStats",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void maybePrintStats()\n{\r\n    if (totalPaths % 100000 == 0) {\r\n        printStats();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\main\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "writeToFileListingRoot",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void writeToFileListingRoot(SequenceFile.Writer fileListWriter, LinkedList<CopyListingFileStatus> fileStatus, Path sourcePathRoot, DistCpContext context) throws IOException\n{\r\n    boolean syncOrOverwrite = context.shouldSyncFolder() || context.shouldOverwrite();\r\n    boolean skipRootPath = syncOrOverwrite && !context.shouldUpdateRoot();\r\n    for (CopyListingFileStatus fs : fileStatus) {\r\n        if (fs.getPath().equals(sourcePathRoot) && fs.isDirectory() && skipRootPath) {\r\n            LOG.debug(\"Skip {}\", fs.getPath());\r\n            return;\r\n        }\r\n        writeToFileListing(fileListWriter, fs, sourcePathRoot);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\main\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "writeToFileListing",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void writeToFileListing(SequenceFile.Writer fileListWriter, CopyListingFileStatus fileStatus, Path sourcePathRoot) throws IOException\n{\r\n    LOG.debug(\"REL PATH: {}, FULL PATH: {}\", DistCpUtils.getRelativePath(sourcePathRoot, fileStatus.getPath()), fileStatus.getPath());\r\n    if (!shouldCopy(fileStatus.getPath())) {\r\n        return;\r\n    }\r\n    fileListWriter.append(getFileListingKey(sourcePathRoot, fileStatus), getFileListingValue(fileStatus));\r\n    fileListWriter.sync();\r\n    if (!fileStatus.isDirectory()) {\r\n        totalBytesToCopy += fileStatus.getSizeToCopy();\r\n    } else {\r\n        totalDirs++;\r\n    }\r\n    totalPaths++;\r\n    maybePrintStats();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\main\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "prepareFileListing",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void prepareFileListing(Job job) throws Exception\n{\r\n    if (context.shouldUseSnapshotDiff()) {\r\n        DistCpSync distCpSync = new DistCpSync(context, job.getConfiguration());\r\n        if (distCpSync.sync()) {\r\n            createInputFileListingWithDiff(job, distCpSync);\r\n        } else {\r\n            throw new Exception(\"DistCp sync failed, input options: \" + context);\r\n        }\r\n    } else {\r\n        createInputFileListing(job);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\main\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "run",
  "errType" : [ "Throwable", "InvalidInputException", "DuplicateFileException", "AclsNotSupportedException", "XAttrsNotSupportedException", "Exception", "IOException" ],
  "containingMethodsNum" : 17,
  "sourceCodeText" : "int run(String[] argv)\n{\r\n    if (argv.length < 1) {\r\n        OptionsParser.usage();\r\n        return DistCpConstants.INVALID_ARGUMENT;\r\n    }\r\n    try {\r\n        context = new DistCpContext(OptionsParser.parse(argv));\r\n        checkSplitLargeFile();\r\n        setTargetPathExists();\r\n        LOG.info(\"Input Options: \" + context);\r\n    } catch (Throwable e) {\r\n        LOG.error(\"Invalid arguments: \", e);\r\n        System.err.println(\"Invalid arguments: \" + e.getMessage());\r\n        OptionsParser.usage();\r\n        return DistCpConstants.INVALID_ARGUMENT;\r\n    }\r\n    Job job = null;\r\n    try {\r\n        job = execute();\r\n    } catch (InvalidInputException e) {\r\n        LOG.error(\"Invalid input: \", e);\r\n        return DistCpConstants.INVALID_ARGUMENT;\r\n    } catch (DuplicateFileException e) {\r\n        LOG.error(\"Duplicate files in input path: \", e);\r\n        return DistCpConstants.DUPLICATE_INPUT;\r\n    } catch (AclsNotSupportedException e) {\r\n        LOG.error(\"ACLs not supported on at least one file system: \", e);\r\n        return DistCpConstants.ACLS_NOT_SUPPORTED;\r\n    } catch (XAttrsNotSupportedException e) {\r\n        LOG.error(\"XAttrs not supported on at least one file system: \", e);\r\n        return DistCpConstants.XATTRS_NOT_SUPPORTED;\r\n    } catch (Exception e) {\r\n        LOG.error(\"Exception encountered \", e);\r\n        return DistCpConstants.UNKNOWN_ERROR;\r\n    } finally {\r\n        if (job != null && context.shouldBlock()) {\r\n            try {\r\n                job.close();\r\n            } catch (IOException e) {\r\n                LOG.error(\"Exception encountered while closing distcp job\", e);\r\n            }\r\n        }\r\n    }\r\n    return DistCpConstants.SUCCESS;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 3,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\main\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "execute",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "Job execute() throws Exception\n{\r\n    Preconditions.checkState(context != null, \"The DistCpContext should have been created before running DistCp!\");\r\n    Job job = createAndSubmitJob();\r\n    if (context.shouldBlock()) {\r\n        waitForJobCompletion(job);\r\n    }\r\n    return job;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\main\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "createAndSubmitJob",
  "errType" : null,
  "containingMethodsNum" : 11,
  "sourceCodeText" : "Job createAndSubmitJob() throws Exception\n{\r\n    assert context != null;\r\n    assert getConf() != null;\r\n    Job job = null;\r\n    try {\r\n        synchronized (this) {\r\n            metaFolder = createMetaFolderPath();\r\n            jobFS = metaFolder.getFileSystem(getConf());\r\n            job = createJob();\r\n        }\r\n        prepareFileListing(job);\r\n        job.submit();\r\n        submitted = true;\r\n    } finally {\r\n        if (!submitted) {\r\n            cleanup();\r\n        }\r\n    }\r\n    String jobID = job.getJobID().toString();\r\n    job.getConfiguration().set(DistCpConstants.CONF_LABEL_DISTCP_JOB_ID, jobID);\r\n    getConf().set(DistCpConstants.CONF_LABEL_DISTCP_JOB_ID, jobID);\r\n    LOG.info(\"DistCp job-id: \" + jobID);\r\n    return job;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\main\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "waitForJobCompletion",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void waitForJobCompletion(Job job) throws Exception\n{\r\n    assert job != null;\r\n    if (!job.waitForCompletion(true)) {\r\n        throw new IOException(\"DistCp failure: Job \" + job.getJobID() + \" has failed: \" + job.getStatus().getFailureInfo());\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\main\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "setTargetPathExists",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void setTargetPathExists() throws IOException\n{\r\n    Path target = context.getTargetPath();\r\n    FileSystem targetFS = target.getFileSystem(getConf());\r\n    boolean targetExists = targetFS.exists(target);\r\n    context.setTargetPathExists(targetExists);\r\n    getConf().setBoolean(DistCpConstants.CONF_LABEL_TARGET_PATH_EXISTS, targetExists);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\main\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "checkSplitLargeFile",
  "errType" : [ "UnsupportedOperationException", "Exception" ],
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void checkSplitLargeFile() throws IOException\n{\r\n    if (!context.splitLargeFile()) {\r\n        return;\r\n    }\r\n    final Path target = context.getTargetPath();\r\n    final FileSystem targetFS = target.getFileSystem(getConf());\r\n    try {\r\n        Path[] src = null;\r\n        Path tgt = null;\r\n        targetFS.concat(tgt, src);\r\n    } catch (UnsupportedOperationException use) {\r\n        throw new UnsupportedOperationException(DistCpOptionSwitch.BLOCKS_PER_CHUNK.getSwitch() + \" is not supported since the target file system doesn't\" + \" support concat.\", use);\r\n    } catch (Exception e) {\r\n    }\r\n    LOG.info(\"Set \" + DistCpConstants.CONF_LABEL_SIMPLE_LISTING_RANDOMIZE_FILES + \" to false since \" + DistCpOptionSwitch.BLOCKS_PER_CHUNK.getSwitch() + \" is passed.\");\r\n    getConf().setBoolean(DistCpConstants.CONF_LABEL_SIMPLE_LISTING_RANDOMIZE_FILES, false);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\main\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "createJob",
  "errType" : null,
  "containingMethodsNum" : 14,
  "sourceCodeText" : "Job createJob() throws IOException\n{\r\n    String jobName = \"distcp\";\r\n    String userChosenName = getConf().get(JobContext.JOB_NAME);\r\n    if (userChosenName != null)\r\n        jobName += \": \" + userChosenName;\r\n    Job job = Job.getInstance(getConf());\r\n    job.setJobName(jobName);\r\n    job.setInputFormatClass(DistCpUtils.getStrategy(getConf(), context));\r\n    job.setJarByClass(CopyMapper.class);\r\n    configureOutputFormat(job);\r\n    job.setMapperClass(CopyMapper.class);\r\n    job.setNumReduceTasks(0);\r\n    job.setMapOutputKeyClass(Text.class);\r\n    job.setMapOutputValueClass(Text.class);\r\n    job.setOutputFormatClass(CopyOutputFormat.class);\r\n    job.getConfiguration().set(JobContext.MAP_SPECULATIVE, \"false\");\r\n    job.getConfiguration().set(JobContext.NUM_MAPS, String.valueOf(context.getMaxMaps()));\r\n    context.appendToConf(job.getConfiguration());\r\n    return job;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\main\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "configureOutputFormat",
  "errType" : null,
  "containingMethodsNum" : 21,
  "sourceCodeText" : "void configureOutputFormat(Job job) throws IOException\n{\r\n    final Configuration configuration = job.getConfiguration();\r\n    Path targetPath = context.getTargetPath();\r\n    FileSystem targetFS = targetPath.getFileSystem(configuration);\r\n    targetPath = targetPath.makeQualified(targetFS.getUri(), targetFS.getWorkingDirectory());\r\n    if (context.shouldPreserve(DistCpOptions.FileAttribute.ACL)) {\r\n        DistCpUtils.checkFileSystemAclSupport(targetFS);\r\n    }\r\n    if (context.shouldPreserve(DistCpOptions.FileAttribute.XATTR)) {\r\n        DistCpUtils.checkFileSystemXAttrSupport(targetFS);\r\n    }\r\n    if (context.shouldAtomicCommit()) {\r\n        Path workDir = context.getAtomicWorkPath();\r\n        if (workDir == null) {\r\n            workDir = targetPath.getParent();\r\n        }\r\n        workDir = new Path(workDir, WIP_PREFIX + targetPath.getName() + rand.nextInt());\r\n        FileSystem workFS = workDir.getFileSystem(configuration);\r\n        if (!FileUtil.compareFs(targetFS, workFS)) {\r\n            throw new IllegalArgumentException(\"Work path \" + workDir + \" and target path \" + targetPath + \" are in different file system\");\r\n        }\r\n        CopyOutputFormat.setWorkingDirectory(job, workDir);\r\n    } else {\r\n        CopyOutputFormat.setWorkingDirectory(job, targetPath);\r\n    }\r\n    CopyOutputFormat.setCommitDirectory(job, targetPath);\r\n    Path logPath = context.getLogPath();\r\n    if (logPath == null) {\r\n        logPath = new Path(metaFolder, \"_logs\");\r\n    } else {\r\n        LOG.info(\"DistCp job log path: \" + logPath);\r\n    }\r\n    CopyOutputFormat.setOutputPath(job, logPath);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\main\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "createInputFileListing",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "Path createInputFileListing(Job job) throws IOException\n{\r\n    Path fileListingPath = getFileListingPath();\r\n    CopyListing copyListing = CopyListing.getCopyListing(job.getConfiguration(), job.getCredentials(), context);\r\n    copyListing.buildListing(fileListingPath, context);\r\n    return fileListingPath;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\main\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "createInputFileListingWithDiff",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "Path createInputFileListingWithDiff(Job job, DistCpSync distCpSync) throws IOException\n{\r\n    Path fileListingPath = getFileListingPath();\r\n    CopyListing copyListing = new SimpleCopyListing(job.getConfiguration(), job.getCredentials(), distCpSync);\r\n    copyListing.buildListing(fileListingPath, context);\r\n    return fileListingPath;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\main\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "getFileListingPath",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Path getFileListingPath() throws IOException\n{\r\n    String fileListPathStr = metaFolder + \"/fileList.seq\";\r\n    Path path = new Path(fileListPathStr);\r\n    return new Path(path.toUri().normalize().toString());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\main\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "createMetaFolderPath",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "Path createMetaFolderPath() throws Exception\n{\r\n    Configuration configuration = getConf();\r\n    Path stagingDir = JobSubmissionFiles.getStagingDir(new Cluster(configuration), configuration);\r\n    Path metaFolderPath = new Path(stagingDir, PREFIX + String.valueOf(rand.nextInt()));\r\n    if (LOG.isDebugEnabled())\r\n        LOG.debug(\"Meta folder location: \" + metaFolderPath);\r\n    configuration.set(DistCpConstants.CONF_LABEL_META_FOLDER, metaFolderPath.toString());\r\n    return metaFolderPath;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\main\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "getContext",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "DistCpContext getContext()\n{\r\n    return context;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\main\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "main",
  "errType" : [ "Exception" ],
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void main(String[] argv)\n{\r\n    int exitCode;\r\n    try {\r\n        DistCp distCp = new DistCp();\r\n        Cleanup cleanup = new Cleanup(distCp);\r\n        ShutdownHookManager.get().addShutdownHook(cleanup, SHUTDOWN_HOOK_PRIORITY);\r\n        exitCode = ToolRunner.run(getDefaultConf(), distCp, argv);\r\n    } catch (Exception e) {\r\n        LOG.error(\"Couldn't complete DistCp operation: \", e);\r\n        exitCode = DistCpConstants.UNKNOWN_ERROR;\r\n    }\r\n    System.exit(exitCode);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\main\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "getDefaultConf",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "Configuration getDefaultConf()\n{\r\n    Configuration config = new Configuration();\r\n    config.addResource(DISTCP_DEFAULT_XML);\r\n    config.addResource(DISTCP_SITE_XML);\r\n    return config;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\main\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "cleanup",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void cleanup()\n{\r\n    try {\r\n        if (metaFolder != null) {\r\n            if (jobFS != null) {\r\n                jobFS.delete(metaFolder, true);\r\n            }\r\n            metaFolder = null;\r\n        }\r\n    } catch (IOException e) {\r\n        LOG.error(\"Unable to cleanup meta folder: \" + metaFolder, e);\r\n    }\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\main\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "isSubmitted",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean isSubmitted()\n{\r\n    return submitted;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\main\\java\\org\\apache\\hadoop\\tools\\util",
  "methodName" : "addWorker",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void addWorker(WorkRequestProcessor<T, R> processor)\n{\r\n    executor.execute(new Worker(processor));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\main\\java\\org\\apache\\hadoop\\tools\\util",
  "methodName" : "shutdown",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void shutdown()\n{\r\n    if (hasWork()) {\r\n        LOG.warn(\"Shutdown() is called but there are still unprocessed work!\");\r\n    }\r\n    executor.shutdownNow();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\main\\java\\org\\apache\\hadoop\\tools\\util",
  "methodName" : "getWorkCnt",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "int getWorkCnt()\n{\r\n    return workCnt.get();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\main\\java\\org\\apache\\hadoop\\tools\\util",
  "methodName" : "hasWork",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "boolean hasWork()\n{\r\n    return workCnt.get() > 0;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\main\\java\\org\\apache\\hadoop\\tools\\util",
  "methodName" : "put",
  "errType" : [ "InterruptedException" ],
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void put(WorkRequest<T> workRequest)\n{\r\n    boolean isDone = false;\r\n    while (!isDone) {\r\n        try {\r\n            inputQueue.put(workRequest);\r\n            workCnt.incrementAndGet();\r\n            isDone = true;\r\n        } catch (InterruptedException ie) {\r\n            LOG.error(\"Could not put workRequest into inputQueue. Retrying...\");\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\main\\java\\org\\apache\\hadoop\\tools\\util",
  "methodName" : "take",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "WorkReport<R> take() throws InterruptedException\n{\r\n    WorkReport<R> report = outputQueue.take();\r\n    workCnt.decrementAndGet();\r\n    return report;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\main\\java\\org\\apache\\hadoop\\tools\\util",
  "methodName" : "blockingTake",
  "errType" : [ "InterruptedException" ],
  "containingMethodsNum" : 3,
  "sourceCodeText" : "WorkReport<R> blockingTake()\n{\r\n    while (true) {\r\n        try {\r\n            WorkReport<R> report = outputQueue.take();\r\n            workCnt.decrementAndGet();\r\n            return report;\r\n        } catch (InterruptedException ie) {\r\n            LOG.debug(\"Retrying in blockingTake...\");\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\main\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "setSource",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setSource(final Path source)\n{\r\n    this.source = source;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\main\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "getSource",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Path getSource()\n{\r\n    return source;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\main\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "setTarget",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setTarget(final Path target)\n{\r\n    this.target = target;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\main\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "getTarget",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Path getTarget()\n{\r\n    return target;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\main\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "setType",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setType(final SnapshotDiffReport.DiffType type)\n{\r\n    this.type = type;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\main\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "getType",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "SnapshotDiffReport.DiffType getType()\n{\r\n    return type;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\main\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "setTmp",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setTmp(Path tmp)\n{\r\n    this.tmp = tmp;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\main\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "getTmp",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Path getTmp()\n{\r\n    return tmp;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\main\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "toString",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "String toString()\n{\r\n    return type + \": src=\" + String.valueOf(source) + \" tgt=\" + String.valueOf(target) + \" tmp=\" + String.valueOf(tmp);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\main\\java\\org\\apache\\hadoop\\tools\\mapred\\lib",
  "methodName" : "initialize",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void initialize(InputSplit inputSplit, TaskAttemptContext taskAttemptContext) throws IOException, InterruptedException\n{\r\n    numRecordsPerChunk = DynamicInputFormat.getNumEntriesPerChunk(taskAttemptContext.getConfiguration());\r\n    this.taskAttemptContext = taskAttemptContext;\r\n    configuration = taskAttemptContext.getConfiguration();\r\n    taskId = taskAttemptContext.getTaskAttemptID().getTaskID();\r\n    chunk = chunkContext.acquire(this.taskAttemptContext);\r\n    timeOfLastChunkDirScan = System.currentTimeMillis();\r\n    isChunkDirAlreadyScanned = false;\r\n    totalNumRecords = getTotalNumRecords();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\main\\java\\org\\apache\\hadoop\\tools\\mapred\\lib",
  "methodName" : "getTotalNumRecords",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "int getTotalNumRecords()\n{\r\n    return DistCpUtils.getInt(configuration, DistCpConstants.CONF_LABEL_TOTAL_NUMBER_OF_RECORDS);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\main\\java\\org\\apache\\hadoop\\tools\\mapred\\lib",
  "methodName" : "nextKeyValue",
  "errType" : null,
  "containingMethodsNum" : 9,
  "sourceCodeText" : "boolean nextKeyValue() throws IOException, InterruptedException\n{\r\n    if (chunk == null) {\r\n        if (LOG.isDebugEnabled())\r\n            LOG.debug(taskId + \": RecordReader is null. No records to be read.\");\r\n        return false;\r\n    }\r\n    if (chunk.getReader().nextKeyValue()) {\r\n        ++numRecordsProcessedByThisMap;\r\n        return true;\r\n    }\r\n    if (LOG.isDebugEnabled())\r\n        LOG.debug(taskId + \": Current chunk exhausted. \" + \" Attempting to pick up new one.\");\r\n    chunk.release();\r\n    timeOfLastChunkDirScan = System.currentTimeMillis();\r\n    isChunkDirAlreadyScanned = false;\r\n    chunk = chunkContext.acquire(taskAttemptContext);\r\n    if (chunk == null)\r\n        return false;\r\n    if (chunk.getReader().nextKeyValue()) {\r\n        ++numRecordsProcessedByThisMap;\r\n        return true;\r\n    } else {\r\n        return false;\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\main\\java\\org\\apache\\hadoop\\tools\\mapred\\lib",
  "methodName" : "getCurrentKey",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "K getCurrentKey() throws IOException, InterruptedException\n{\r\n    return chunk.getReader().getCurrentKey();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\main\\java\\org\\apache\\hadoop\\tools\\mapred\\lib",
  "methodName" : "getCurrentValue",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "V getCurrentValue() throws IOException, InterruptedException\n{\r\n    return chunk.getReader().getCurrentValue();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\main\\java\\org\\apache\\hadoop\\tools\\mapred\\lib",
  "methodName" : "getProgress",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "float getProgress() throws IOException, InterruptedException\n{\r\n    final int numChunksLeft = getNumChunksLeft();\r\n    if (numChunksLeft < 0) {\r\n        assert numRecordsProcessedByThisMap <= numRecordsPerChunk : \"numRecordsProcessedByThisMap:\" + numRecordsProcessedByThisMap + \" exceeds numRecordsPerChunk:\" + numRecordsPerChunk;\r\n        return ((float) numRecordsProcessedByThisMap) / totalNumRecords;\r\n    }\r\n    return ((float) numRecordsProcessedByThisMap) / (numRecordsProcessedByThisMap + numRecordsPerChunk * numChunksLeft);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\main\\java\\org\\apache\\hadoop\\tools\\mapred\\lib",
  "methodName" : "getNumChunksLeft",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "int getNumChunksLeft() throws IOException\n{\r\n    long now = System.currentTimeMillis();\r\n    boolean tooLongSinceLastDirScan = now - timeOfLastChunkDirScan > TIME_THRESHOLD_FOR_DIR_SCANS;\r\n    if (tooLongSinceLastDirScan || (!isChunkDirAlreadyScanned && numRecordsProcessedByThisMap % numRecordsPerChunk > numRecordsPerChunk / 2)) {\r\n        chunkContext.getListOfChunkFiles();\r\n        isChunkDirAlreadyScanned = true;\r\n        timeOfLastChunkDirScan = now;\r\n    }\r\n    return chunkContext.getNumChunksLeft();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\main\\java\\org\\apache\\hadoop\\tools\\mapred\\lib",
  "methodName" : "close",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void close() throws IOException\n{\r\n    if (chunk != null)\r\n        chunk.close();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\main\\java\\org\\apache\\hadoop\\tools\\mapred\\lib",
  "methodName" : "getConfiguration",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Configuration getConfiguration()\n{\r\n    return configuration;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\main\\java\\org\\apache\\hadoop\\tools\\mapred\\lib",
  "methodName" : "getChunkRootPath",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Path getChunkRootPath()\n{\r\n    return chunkRootPath;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\main\\java\\org\\apache\\hadoop\\tools\\mapred\\lib",
  "methodName" : "getChunkFilePrefix",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getChunkFilePrefix()\n{\r\n    return chunkFilePrefix;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\main\\java\\org\\apache\\hadoop\\tools\\mapred\\lib",
  "methodName" : "getFs",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "FileSystem getFs()\n{\r\n    return fs;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\main\\java\\org\\apache\\hadoop\\tools\\mapred\\lib",
  "methodName" : "getListingFilePath",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "String getListingFilePath(Configuration configuration)\n{\r\n    final String listingFileString = configuration.get(DistCpConstants.CONF_LABEL_LISTING_FILE_PATH, \"\");\r\n    assert !listingFileString.equals(\"\") : \"Listing file not found.\";\r\n    return listingFileString;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\main\\java\\org\\apache\\hadoop\\tools\\mapred\\lib",
  "methodName" : "getNumChunksLeft",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int getNumChunksLeft()\n{\r\n    return numChunksLeft;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\main\\java\\org\\apache\\hadoop\\tools\\mapred\\lib",
  "methodName" : "acquire",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "DynamicInputChunk acquire(TaskAttemptContext taskAttemptContext) throws IOException, InterruptedException\n{\r\n    String taskId = taskAttemptContext.getTaskAttemptID().getTaskID().toString();\r\n    Path acquiredFilePath = new Path(getChunkRootPath(), taskId);\r\n    if (fs.exists(acquiredFilePath)) {\r\n        LOG.info(\"Acquiring pre-assigned chunk: \" + acquiredFilePath);\r\n        return new DynamicInputChunk(acquiredFilePath, taskAttemptContext, this);\r\n    }\r\n    for (FileStatus chunkFile : getListOfChunkFiles()) {\r\n        if (fs.rename(chunkFile.getPath(), acquiredFilePath)) {\r\n            LOG.info(taskId + \" acquired \" + chunkFile.getPath());\r\n            return new DynamicInputChunk(acquiredFilePath, taskAttemptContext, this);\r\n        }\r\n    }\r\n    return null;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\main\\java\\org\\apache\\hadoop\\tools\\mapred\\lib",
  "methodName" : "createChunkForWrite",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "DynamicInputChunk createChunkForWrite(String chunkId) throws IOException\n{\r\n    return new DynamicInputChunk(chunkId, this);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\main\\java\\org\\apache\\hadoop\\tools\\mapred\\lib",
  "methodName" : "getListOfChunkFiles",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "FileStatus[] getListOfChunkFiles() throws IOException\n{\r\n    Path chunkFilePattern = new Path(chunkRootPath, chunkFilePrefix + \"*\");\r\n    FileStatus[] chunkFiles = fs.globStatus(chunkFilePattern);\r\n    numChunksLeft = chunkFiles.length;\r\n    return chunkFiles;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\main\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "validatePaths",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void validatePaths(DistCpContext context) throws IOException, InvalidInputException\n{\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\main\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "doBuildListing",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void doBuildListing(Path pathToListFile, DistCpContext context) throws IOException\n{\r\n    context.setSourcePaths(fetchFileList(context.getSourceFileListing()));\r\n    globbedListing.buildListing(pathToListFile, context);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\main\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "fetchFileList",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "List<Path> fetchFileList(Path sourceListing) throws IOException\n{\r\n    List<Path> result = new ArrayList<Path>();\r\n    FileSystem fs = sourceListing.getFileSystem(getConf());\r\n    BufferedReader input = null;\r\n    try {\r\n        input = new BufferedReader(new InputStreamReader(fs.open(sourceListing), Charset.forName(\"UTF-8\")));\r\n        String line = input.readLine();\r\n        while (line != null) {\r\n            result.add(new Path(line));\r\n            line = input.readLine();\r\n        }\r\n    } finally {\r\n        IOUtils.closeStream(input);\r\n    }\r\n    return result;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\main\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "getBytesToCopy",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "long getBytesToCopy()\n{\r\n    return globbedListing.getBytesToCopy();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\main\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "getNumberOfPaths",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "long getNumberOfPaths()\n{\r\n    return globbedListing.getNumberOfPaths();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\main\\java\\org\\apache\\hadoop\\tools\\mapred",
  "methodName" : "setWorkingDirectory",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setWorkingDirectory(Job job, Path workingDirectory)\n{\r\n    job.getConfiguration().set(DistCpConstants.CONF_LABEL_TARGET_WORK_PATH, workingDirectory.toString());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\main\\java\\org\\apache\\hadoop\\tools\\mapred",
  "methodName" : "setCommitDirectory",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setCommitDirectory(Job job, Path commitDirectory)\n{\r\n    job.getConfiguration().set(DistCpConstants.CONF_LABEL_TARGET_FINAL_PATH, commitDirectory.toString());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\main\\java\\org\\apache\\hadoop\\tools\\mapred",
  "methodName" : "getWorkingDirectory",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Path getWorkingDirectory(Job job)\n{\r\n    return getWorkingDirectory(job.getConfiguration());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\main\\java\\org\\apache\\hadoop\\tools\\mapred",
  "methodName" : "getWorkingDirectory",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "Path getWorkingDirectory(Configuration conf)\n{\r\n    String workingDirectory = conf.get(DistCpConstants.CONF_LABEL_TARGET_WORK_PATH);\r\n    if (workingDirectory == null || workingDirectory.isEmpty()) {\r\n        return null;\r\n    } else {\r\n        return new Path(workingDirectory);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\main\\java\\org\\apache\\hadoop\\tools\\mapred",
  "methodName" : "getCommitDirectory",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Path getCommitDirectory(Job job)\n{\r\n    return getCommitDirectory(job.getConfiguration());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\main\\java\\org\\apache\\hadoop\\tools\\mapred",
  "methodName" : "getCommitDirectory",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "Path getCommitDirectory(Configuration conf)\n{\r\n    String commitDirectory = conf.get(DistCpConstants.CONF_LABEL_TARGET_FINAL_PATH);\r\n    if (commitDirectory == null || commitDirectory.isEmpty()) {\r\n        return null;\r\n    } else {\r\n        return new Path(commitDirectory);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\main\\java\\org\\apache\\hadoop\\tools\\mapred",
  "methodName" : "getOutputCommitter",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "OutputCommitter getOutputCommitter(TaskAttemptContext context) throws IOException\n{\r\n    return new CopyCommitter(getOutputPath(context), context);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\main\\java\\org\\apache\\hadoop\\tools\\mapred",
  "methodName" : "checkOutputSpecs",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void checkOutputSpecs(JobContext context) throws IOException\n{\r\n    Configuration conf = context.getConfiguration();\r\n    if (getCommitDirectory(conf) == null) {\r\n        throw new IllegalStateException(\"Commit directory not configured\");\r\n    }\r\n    Path workingPath = getWorkingDirectory(conf);\r\n    if (workingPath == null) {\r\n        throw new IllegalStateException(\"Working directory not configured\");\r\n    }\r\n    TokenCache.obtainTokensForNamenodes(context.getCredentials(), new Path[] { workingPath }, conf);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\main\\java\\org\\apache\\hadoop\\tools\\util",
  "methodName" : "close",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void close() throws IOException\n{\r\n    rawStream.close();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\main\\java\\org\\apache\\hadoop\\tools\\util",
  "methodName" : "read",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "int read() throws IOException\n{\r\n    throttle();\r\n    int data = rawStream.read();\r\n    if (data != -1) {\r\n        bytesRead++;\r\n    }\r\n    return data;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\main\\java\\org\\apache\\hadoop\\tools\\util",
  "methodName" : "read",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "int read(byte[] b) throws IOException\n{\r\n    throttle();\r\n    int readLen = rawStream.read(b);\r\n    if (readLen != -1) {\r\n        bytesRead += readLen;\r\n    }\r\n    return readLen;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\main\\java\\org\\apache\\hadoop\\tools\\util",
  "methodName" : "read",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "int read(byte[] b, int off, int len) throws IOException\n{\r\n    if (len == 0) {\r\n        return 0;\r\n    }\r\n    throttle();\r\n    int readLen = rawStream.read(b, off, len);\r\n    if (readLen != -1) {\r\n        bytesRead += readLen;\r\n    }\r\n    return readLen;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\main\\java\\org\\apache\\hadoop\\tools\\util",
  "methodName" : "throttle",
  "errType" : [ "InterruptedException" ],
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void throttle() throws IOException\n{\r\n    while (getBytesPerSec() > maxBytesPerSec) {\r\n        try {\r\n            Thread.sleep(SLEEP_DURATION_MS);\r\n            totalSleepTime += SLEEP_DURATION_MS;\r\n        } catch (InterruptedException e) {\r\n            throw new IOException(\"Thread aborted\", e);\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\main\\java\\org\\apache\\hadoop\\tools\\util",
  "methodName" : "getTotalBytesRead",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getTotalBytesRead()\n{\r\n    return bytesRead;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\main\\java\\org\\apache\\hadoop\\tools\\util",
  "methodName" : "getBytesPerSec",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "long getBytesPerSec()\n{\r\n    long elapsed = (System.currentTimeMillis() - startTime) / 1000;\r\n    if (elapsed == 0) {\r\n        return bytesRead;\r\n    } else {\r\n        return bytesRead / elapsed;\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\main\\java\\org\\apache\\hadoop\\tools\\util",
  "methodName" : "getTotalSleepTime",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getTotalSleepTime()\n{\r\n    return totalSleepTime;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\main\\java\\org\\apache\\hadoop\\tools\\util",
  "methodName" : "toString",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String toString()\n{\r\n    return \"ThrottledInputStream{\" + \"bytesRead=\" + bytesRead + \", maxBytesPerSec=\" + maxBytesPerSec + \", bytesPerSec=\" + getBytesPerSec() + \", totalSleepTime=\" + totalSleepTime + '}';\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\main\\java\\org\\apache\\hadoop\\tools\\util",
  "methodName" : "checkSeekable",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void checkSeekable() throws IOException\n{\r\n    if (!(rawStream instanceof Seekable)) {\r\n        throw new UnsupportedOperationException(\"seek operations are unsupported by the internal stream\");\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\main\\java\\org\\apache\\hadoop\\tools\\util",
  "methodName" : "seek",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void seek(long pos) throws IOException\n{\r\n    checkSeekable();\r\n    ((Seekable) rawStream).seek(pos);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\main\\java\\org\\apache\\hadoop\\tools\\util",
  "methodName" : "getPos",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "long getPos() throws IOException\n{\r\n    checkSeekable();\r\n    return ((Seekable) rawStream).getPos();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\main\\java\\org\\apache\\hadoop\\tools\\util",
  "methodName" : "seekToNewSource",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "boolean seekToNewSource(long targetPos) throws IOException\n{\r\n    checkSeekable();\r\n    return ((Seekable) rawStream).seekToNewSource(targetPos);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\main\\java\\org\\apache\\hadoop\\tools\\util",
  "methodName" : "getItem",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "T getItem()\n{\r\n    return item;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\main\\java\\org\\apache\\hadoop\\tools\\util",
  "methodName" : "getSuccess",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean getSuccess()\n{\r\n    return success;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\main\\java\\org\\apache\\hadoop\\tools\\util",
  "methodName" : "getRetry",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int getRetry()\n{\r\n    return retry;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\main\\java\\org\\apache\\hadoop\\tools\\util",
  "methodName" : "getException",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Exception getException()\n{\r\n    return exception;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\main\\java\\org\\apache\\hadoop\\tools\\mapred",
  "methodName" : "doExecute",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "Object doExecute(Object... arguments) throws Exception\n{\r\n    assert arguments.length == 3 : \"Unexpected argument list.\";\r\n    Path target = (Path) arguments[0];\r\n    Mapper.Context context = (Mapper.Context) arguments[1];\r\n    FileStatus sourceStatus = (FileStatus) arguments[2];\r\n    FileSystem targetFS = target.getFileSystem(context.getConfiguration());\r\n    if (!targetFS.mkdirs(target)) {\r\n        return false;\r\n    }\r\n    boolean preserveEC = getFileAttributeSettings(context).contains(DistCpOptions.FileAttribute.ERASURECODINGPOLICY);\r\n    if (preserveEC && sourceStatus.isErasureCoded() && targetFS instanceof DistributedFileSystem) {\r\n        ErasureCodingPolicy ecPolicy = ((HdfsFileStatus) sourceStatus).getErasureCodingPolicy();\r\n        DistributedFileSystem dfs = (DistributedFileSystem) targetFS;\r\n        dfs.setErasureCodingPolicy(target, ecPolicy.getName());\r\n    }\r\n    return true;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\main\\java\\org\\apache\\hadoop\\tools\\util",
  "methodName" : "getFileSize",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "long getFileSize(Path path, Configuration configuration) throws IOException\n{\r\n    if (LOG.isDebugEnabled())\r\n        LOG.debug(\"Retrieving file size for: \" + path);\r\n    return path.getFileSystem(configuration).getFileStatus(path).getLen();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\main\\java\\org\\apache\\hadoop\\tools\\util",
  "methodName" : "publish",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void publish(Configuration configuration, String label, T value)\n{\r\n    configuration.set(label, String.valueOf(value));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\main\\java\\org\\apache\\hadoop\\tools\\util",
  "methodName" : "getInt",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "int getInt(Configuration configuration, String label)\n{\r\n    int value = configuration.getInt(label, -1);\r\n    assert value >= 0 : \"Couldn't find \" + label;\r\n    return value;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\main\\java\\org\\apache\\hadoop\\tools\\util",
  "methodName" : "getLong",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "long getLong(Configuration configuration, String label)\n{\r\n    long value = configuration.getLong(label, -1);\r\n    assert value >= 0 : \"Couldn't find \" + label;\r\n    return value;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\main\\java\\org\\apache\\hadoop\\tools\\util",
  "methodName" : "getStrategy",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "Class<? extends InputFormat> getStrategy(Configuration conf, DistCpContext context)\n{\r\n    String confLabel = \"distcp.\" + StringUtils.toLowerCase(context.getCopyStrategy()) + \".strategy\" + \".impl\";\r\n    return conf.getClass(confLabel, UniformSizeInputFormat.class, InputFormat.class);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\main\\java\\org\\apache\\hadoop\\tools\\util",
  "methodName" : "getRelativePath",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "String getRelativePath(Path sourceRootPath, Path childPath)\n{\r\n    String childPathString = childPath.toUri().getPath();\r\n    String sourceRootPathString = sourceRootPath.toUri().getPath();\r\n    return sourceRootPathString.equals(\"/\") ? childPathString : childPathString.substring(sourceRootPathString.length());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\main\\java\\org\\apache\\hadoop\\tools\\util",
  "methodName" : "packAttributes",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "String packAttributes(EnumSet<FileAttribute> attributes)\n{\r\n    StringBuffer buffer = new StringBuffer(FileAttribute.values().length);\r\n    int len = 0;\r\n    for (FileAttribute attribute : attributes) {\r\n        buffer.append(attribute.name().charAt(0));\r\n        len++;\r\n    }\r\n    return buffer.substring(0, len);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\main\\java\\org\\apache\\hadoop\\tools\\util",
  "methodName" : "unpackAttributes",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "EnumSet<FileAttribute> unpackAttributes(String attributes)\n{\r\n    EnumSet<FileAttribute> retValue = EnumSet.noneOf(FileAttribute.class);\r\n    if (attributes != null) {\r\n        for (int index = 0; index < attributes.length(); index++) {\r\n            retValue.add(FileAttribute.getAttribute(attributes.charAt(index)));\r\n        }\r\n    }\r\n    return retValue;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\main\\java\\org\\apache\\hadoop\\tools\\util",
  "methodName" : "preserve",
  "errType" : null,
  "containingMethodsNum" : 43,
  "sourceCodeText" : "void preserve(FileSystem targetFS, Path path, CopyListingFileStatus srcFileStatus, EnumSet<FileAttribute> attributes, boolean preserveRawXattrs) throws IOException\n{\r\n    attributes.remove(FileAttribute.BLOCKSIZE);\r\n    attributes.remove(FileAttribute.CHECKSUMTYPE);\r\n    FileStatus targetFileStatus = attributes.isEmpty() ? null : targetFS.getFileStatus(path);\r\n    String group = targetFileStatus == null ? null : targetFileStatus.getGroup();\r\n    String user = targetFileStatus == null ? null : targetFileStatus.getOwner();\r\n    boolean chown = false;\r\n    if (attributes.contains(FileAttribute.ACL)) {\r\n        List<AclEntry> srcAcl = srcFileStatus.getAclEntries();\r\n        List<AclEntry> targetAcl = getAcl(targetFS, targetFileStatus);\r\n        if (!srcAcl.equals(targetAcl)) {\r\n            targetFS.removeAcl(path);\r\n            targetFS.setAcl(path, srcAcl);\r\n        }\r\n        if (srcFileStatus.getPermission().getStickyBit() != targetFileStatus.getPermission().getStickyBit()) {\r\n            targetFS.setPermission(path, srcFileStatus.getPermission());\r\n        }\r\n    } else if (attributes.contains(FileAttribute.PERMISSION) && !srcFileStatus.getPermission().equals(targetFileStatus.getPermission())) {\r\n        targetFS.setPermission(path, srcFileStatus.getPermission());\r\n    }\r\n    final boolean preserveXAttrs = attributes.contains(FileAttribute.XATTR);\r\n    if (preserveXAttrs || preserveRawXattrs) {\r\n        final String rawNS = StringUtils.toLowerCase(XAttr.NameSpace.RAW.name());\r\n        Map<String, byte[]> srcXAttrs = srcFileStatus.getXAttrs();\r\n        Map<String, byte[]> targetXAttrs = getXAttrs(targetFS, path);\r\n        if (srcXAttrs != null && !srcXAttrs.equals(targetXAttrs)) {\r\n            for (Entry<String, byte[]> entry : srcXAttrs.entrySet()) {\r\n                String xattrName = entry.getKey();\r\n                if (xattrName.startsWith(rawNS) || preserveXAttrs) {\r\n                    targetFS.setXAttr(path, xattrName, entry.getValue());\r\n                }\r\n            }\r\n        }\r\n    }\r\n    if (attributes.contains(FileAttribute.REPLICATION) && !targetFileStatus.isDirectory() && !targetFileStatus.isErasureCoded() && !srcFileStatus.isErasureCoded() && srcFileStatus.getReplication() != targetFileStatus.getReplication()) {\r\n        targetFS.setReplication(path, srcFileStatus.getReplication());\r\n    }\r\n    if (attributes.contains(FileAttribute.GROUP) && !group.equals(srcFileStatus.getGroup())) {\r\n        group = srcFileStatus.getGroup();\r\n        chown = true;\r\n    }\r\n    if (attributes.contains(FileAttribute.USER) && !user.equals(srcFileStatus.getOwner())) {\r\n        user = srcFileStatus.getOwner();\r\n        chown = true;\r\n    }\r\n    if (chown) {\r\n        targetFS.setOwner(path, user, group);\r\n    }\r\n    if (attributes.contains(FileAttribute.TIMES)) {\r\n        targetFS.setTimes(path, srcFileStatus.getModificationTime(), srcFileStatus.getAccessTime());\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\main\\java\\org\\apache\\hadoop\\tools\\util",
  "methodName" : "getAcl",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "List<AclEntry> getAcl(FileSystem fileSystem, FileStatus fileStatus) throws IOException\n{\r\n    List<AclEntry> entries = fileSystem.getAclStatus(fileStatus.getPath()).getEntries();\r\n    return AclUtil.getAclFromPermAndEntries(fileStatus.getPermission(), entries);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\main\\java\\org\\apache\\hadoop\\tools\\util",
  "methodName" : "getXAttrs",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Map<String, byte[]> getXAttrs(FileSystem fileSystem, Path path) throws IOException\n{\r\n    return fileSystem.getXAttrs(path);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\main\\java\\org\\apache\\hadoop\\tools\\util",
  "methodName" : "toCopyListingFileStatus",
  "errType" : null,
  "containingMethodsNum" : 19,
  "sourceCodeText" : "LinkedList<CopyListingFileStatus> toCopyListingFileStatus(FileSystem fileSystem, FileStatus fileStatus, boolean preserveAcls, boolean preserveXAttrs, boolean preserveRawXAttrs, int blocksPerChunk) throws IOException\n{\r\n    LinkedList<CopyListingFileStatus> copyListingFileStatus = new LinkedList<CopyListingFileStatus>();\r\n    final CopyListingFileStatus clfs = toCopyListingFileStatusHelper(fileSystem, fileStatus, preserveAcls, preserveXAttrs, preserveRawXAttrs, 0, fileStatus.getLen());\r\n    final long blockSize = fileStatus.getBlockSize();\r\n    if (LOG.isDebugEnabled()) {\r\n        LOG.debug(\"toCopyListing: \" + fileStatus + \" chunkSize: \" + blocksPerChunk + \" isDFS: \" + (fileSystem instanceof DistributedFileSystem));\r\n    }\r\n    if ((blocksPerChunk > 0) && !fileStatus.isDirectory() && (fileStatus.getLen() > blockSize * blocksPerChunk)) {\r\n        final BlockLocation[] blockLocations;\r\n        blockLocations = fileSystem.getFileBlockLocations(fileStatus, 0, fileStatus.getLen());\r\n        int numBlocks = blockLocations.length;\r\n        long curPos = 0;\r\n        if (numBlocks <= blocksPerChunk) {\r\n            if (LOG.isDebugEnabled()) {\r\n                LOG.debug(\"  add file \" + clfs);\r\n            }\r\n            copyListingFileStatus.add(clfs);\r\n        } else {\r\n            int i = 0;\r\n            while (i < numBlocks) {\r\n                long curLength = 0;\r\n                for (int j = 0; j < blocksPerChunk && i < numBlocks; ++j, ++i) {\r\n                    curLength += blockLocations[i].getLength();\r\n                }\r\n                if (curLength > 0) {\r\n                    CopyListingFileStatus clfs1 = new CopyListingFileStatus(clfs);\r\n                    clfs1.setChunkOffset(curPos);\r\n                    clfs1.setChunkLength(curLength);\r\n                    if (LOG.isDebugEnabled()) {\r\n                        LOG.debug(\"  add file chunk \" + clfs1);\r\n                    }\r\n                    copyListingFileStatus.add(clfs1);\r\n                    curPos += curLength;\r\n                }\r\n            }\r\n        }\r\n    } else {\r\n        if (LOG.isDebugEnabled()) {\r\n            LOG.debug(\"  add file/dir \" + clfs);\r\n        }\r\n        copyListingFileStatus.add(clfs);\r\n    }\r\n    return copyListingFileStatus;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\main\\java\\org\\apache\\hadoop\\tools\\util",
  "methodName" : "toCopyListingFileStatusHelper",
  "errType" : null,
  "containingMethodsNum" : 13,
  "sourceCodeText" : "CopyListingFileStatus toCopyListingFileStatusHelper(FileSystem fileSystem, FileStatus fileStatus, boolean preserveAcls, boolean preserveXAttrs, boolean preserveRawXAttrs, long chunkOffset, long chunkLength) throws IOException\n{\r\n    CopyListingFileStatus copyListingFileStatus = new CopyListingFileStatus(fileStatus, chunkOffset, chunkLength);\r\n    if (preserveAcls) {\r\n        if (fileStatus.hasAcl()) {\r\n            List<AclEntry> aclEntries = fileSystem.getAclStatus(fileStatus.getPath()).getEntries();\r\n            copyListingFileStatus.setAclEntries(aclEntries);\r\n        }\r\n    }\r\n    if (preserveXAttrs || preserveRawXAttrs) {\r\n        Map<String, byte[]> srcXAttrs = fileSystem.getXAttrs(fileStatus.getPath());\r\n        if (preserveXAttrs && preserveRawXAttrs) {\r\n            copyListingFileStatus.setXAttrs(srcXAttrs);\r\n        } else {\r\n            Map<String, byte[]> trgXAttrs = Maps.newHashMap();\r\n            final String rawNS = StringUtils.toLowerCase(XAttr.NameSpace.RAW.name());\r\n            for (Map.Entry<String, byte[]> ent : srcXAttrs.entrySet()) {\r\n                final String xattrName = ent.getKey();\r\n                if (xattrName.startsWith(rawNS)) {\r\n                    if (preserveRawXAttrs) {\r\n                        trgXAttrs.put(xattrName, ent.getValue());\r\n                    }\r\n                } else if (preserveXAttrs) {\r\n                    trgXAttrs.put(xattrName, ent.getValue());\r\n                }\r\n            }\r\n            copyListingFileStatus.setXAttrs(trgXAttrs);\r\n        }\r\n    }\r\n    return copyListingFileStatus;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\main\\java\\org\\apache\\hadoop\\tools\\util",
  "methodName" : "sortListing",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "Path sortListing(Configuration conf, Path sourceListing) throws IOException\n{\r\n    Path output = new Path(sourceListing.toString() + \"_sorted\");\r\n    sortListing(conf, sourceListing, output);\r\n    return output;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\main\\java\\org\\apache\\hadoop\\tools\\util",
  "methodName" : "sortListing",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void sortListing(final Configuration conf, final Path sourceListing, final Path output) throws IOException\n{\r\n    FileSystem fs = sourceListing.getFileSystem(conf);\r\n    fs.makeQualified(output);\r\n    SequenceFile.Sorter sorter = new SequenceFile.Sorter(fs, Text.class, CopyListingFileStatus.class, conf);\r\n    fs.delete(output, false);\r\n    sorter.sort(sourceListing, output);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\main\\java\\org\\apache\\hadoop\\tools\\util",
  "methodName" : "checkFileSystemAclSupport",
  "errType" : [ "Exception" ],
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void checkFileSystemAclSupport(FileSystem fs) throws AclsNotSupportedException\n{\r\n    try {\r\n        fs.getAclStatus(new Path(Path.SEPARATOR));\r\n    } catch (Exception e) {\r\n        throw new AclsNotSupportedException(\"ACLs not supported for file system: \" + fs.getUri());\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\main\\java\\org\\apache\\hadoop\\tools\\util",
  "methodName" : "checkFileSystemXAttrSupport",
  "errType" : [ "Exception" ],
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void checkFileSystemXAttrSupport(FileSystem fs) throws XAttrsNotSupportedException\n{\r\n    try {\r\n        fs.getXAttrs(new Path(Path.SEPARATOR));\r\n    } catch (Exception e) {\r\n        throw new XAttrsNotSupportedException(\"XAttrs not supported for file system: \" + fs.getUri());\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\main\\java\\org\\apache\\hadoop\\tools\\util",
  "methodName" : "getFormatter",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "DecimalFormat getFormatter()\n{\r\n    return FORMATTER.get();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\main\\java\\org\\apache\\hadoop\\tools\\util",
  "methodName" : "getStringDescriptionFor",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String getStringDescriptionFor(long nBytes)\n{\r\n    char[] units = { 'B', 'K', 'M', 'G', 'T', 'P' };\r\n    double current = nBytes;\r\n    double prev = current;\r\n    int index = 0;\r\n    while ((current = current / 1024) >= 1) {\r\n        prev = current;\r\n        ++index;\r\n    }\r\n    assert index < units.length : \"Too large a number.\";\r\n    return getFormatter().format(prev) + units[index];\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\main\\java\\org\\apache\\hadoop\\tools\\util",
  "methodName" : "checksumsAreEqual",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 4,
  "sourceCodeText" : "boolean checksumsAreEqual(FileSystem sourceFS, Path source, FileChecksum sourceChecksum, FileSystem targetFS, Path target, long sourceLen) throws IOException\n{\r\n    FileChecksum targetChecksum = null;\r\n    try {\r\n        sourceChecksum = sourceChecksum != null ? sourceChecksum : sourceFS.getFileChecksum(source, sourceLen);\r\n        if (sourceChecksum != null) {\r\n            targetChecksum = targetFS.getFileChecksum(target);\r\n        }\r\n    } catch (IOException e) {\r\n        LOG.error(\"Unable to retrieve checksum for \" + source + \" or \" + target, e);\r\n    }\r\n    return (sourceChecksum == null || targetChecksum == null || sourceChecksum.equals(targetChecksum));\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\main\\java\\org\\apache\\hadoop\\tools\\util",
  "methodName" : "compareFileLengthsAndChecksums",
  "errType" : null,
  "containingMethodsNum" : 11,
  "sourceCodeText" : "void compareFileLengthsAndChecksums(long srcLen, FileSystem sourceFS, Path source, FileChecksum sourceChecksum, FileSystem targetFS, Path target, boolean skipCrc, long targetLen) throws IOException\n{\r\n    if (srcLen != targetLen) {\r\n        throw new IOException(DistCpConstants.LENGTH_MISMATCH_ERROR_MSG + source + \" (\" + srcLen + \") and target:\" + target + \" (\" + targetLen + \")\");\r\n    }\r\n    if ((srcLen != 0) && (!skipCrc)) {\r\n        if (!checksumsAreEqual(sourceFS, source, sourceChecksum, targetFS, target, srcLen)) {\r\n            StringBuilder errorMessage = new StringBuilder(DistCpConstants.CHECKSUM_MISMATCH_ERROR_MSG).append(source).append(\" and \").append(target).append(\".\");\r\n            boolean addSkipHint = false;\r\n            String srcScheme = sourceFS.getScheme();\r\n            String targetScheme = targetFS.getScheme();\r\n            if (!srcScheme.equals(targetScheme)) {\r\n                errorMessage.append(\"Source and destination filesystems are of\" + \" different types\\n\").append(\"Their checksum algorithms may be incompatible\");\r\n                addSkipHint = true;\r\n            } else if (sourceFS.getFileStatus(source).getBlockSize() != targetFS.getFileStatus(target).getBlockSize()) {\r\n                errorMessage.append(\" Source and target differ in block-size.\\n\").append(\" Use -pb to preserve block-sizes during copy.\");\r\n                addSkipHint = true;\r\n            }\r\n            if (addSkipHint) {\r\n                errorMessage.append(\" You can choose file-level checksum validation via \" + \"-Ddfs.checksum.combine.mode=COMPOSITE_CRC when block-sizes\" + \" or filesystems are different.\").append(\" Or you can skip checksum-checks altogether \" + \" with -skipcrccheck.\\n\").append(\" (NOTE: By skipping checksums, one runs the risk of \" + \"masking data-corruption during file-transfer.)\\n\");\r\n            }\r\n            throw new IOException(errorMessage.toString());\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\main\\java\\org\\apache\\hadoop\\tools\\util",
  "methodName" : "getSplitChunkPath",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "Path getSplitChunkPath(Path targetFile, CopyListingFileStatus srcFileStatus)\n{\r\n    return new Path(targetFile.toString() + \".____distcpSplit____\" + srcFileStatus.getChunkOffset() + \".\" + srcFileStatus.getChunkLength());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\main\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "shouldCopy",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean shouldCopy(Path path)\n{\r\n    return true;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\main\\java\\org\\apache\\hadoop\\tools\\mapred",
  "methodName" : "setup",
  "errType" : [ "FileNotFoundException" ],
  "containingMethodsNum" : 14,
  "sourceCodeText" : "void setup(Context context) throws IOException, InterruptedException\n{\r\n    conf = context.getConfiguration();\r\n    syncFolders = conf.getBoolean(DistCpOptionSwitch.SYNC_FOLDERS.getConfigLabel(), false);\r\n    ignoreFailures = conf.getBoolean(DistCpOptionSwitch.IGNORE_FAILURES.getConfigLabel(), false);\r\n    skipCrc = conf.getBoolean(DistCpOptionSwitch.SKIP_CRC.getConfigLabel(), false);\r\n    overWrite = conf.getBoolean(DistCpOptionSwitch.OVERWRITE.getConfigLabel(), false);\r\n    append = conf.getBoolean(DistCpOptionSwitch.APPEND.getConfigLabel(), false);\r\n    verboseLog = conf.getBoolean(DistCpOptionSwitch.VERBOSE_LOG.getConfigLabel(), false);\r\n    preserve = DistCpUtils.unpackAttributes(conf.get(DistCpOptionSwitch.PRESERVE_STATUS.getConfigLabel()));\r\n    directWrite = conf.getBoolean(DistCpOptionSwitch.DIRECT_WRITE.getConfigLabel(), false);\r\n    targetWorkPath = new Path(conf.get(DistCpConstants.CONF_LABEL_TARGET_WORK_PATH));\r\n    Path targetFinalPath = new Path(conf.get(DistCpConstants.CONF_LABEL_TARGET_FINAL_PATH));\r\n    targetFS = targetFinalPath.getFileSystem(conf);\r\n    try {\r\n        overWrite = overWrite || targetFS.getFileStatus(targetFinalPath).isFile();\r\n    } catch (FileNotFoundException ignored) {\r\n    }\r\n    startEpoch = System.currentTimeMillis();\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\main\\java\\org\\apache\\hadoop\\tools\\mapred",
  "methodName" : "map",
  "errType" : [ "IOException", "FileNotFoundException", "FileNotFoundException" ],
  "containingMethodsNum" : 34,
  "sourceCodeText" : "void map(Text relPath, CopyListingFileStatus sourceFileStatus, Context context) throws IOException, InterruptedException\n{\r\n    Path sourcePath = sourceFileStatus.getPath();\r\n    if (LOG.isDebugEnabled())\r\n        LOG.debug(\"DistCpMapper::map(): Received \" + sourcePath + \", \" + relPath);\r\n    Path target = new Path(targetWorkPath.makeQualified(targetFS.getUri(), targetFS.getWorkingDirectory()) + relPath.toString());\r\n    EnumSet<DistCpOptions.FileAttribute> fileAttributes = getFileAttributeSettings(context);\r\n    final boolean preserveRawXattrs = context.getConfiguration().getBoolean(DistCpConstants.CONF_LABEL_PRESERVE_RAWXATTRS, false);\r\n    final String description = \"Copying \" + sourcePath + \" to \" + target;\r\n    context.setStatus(description);\r\n    LOG.info(description);\r\n    try {\r\n        CopyListingFileStatus sourceCurrStatus;\r\n        FileSystem sourceFS;\r\n        FileStatus sourceStatus;\r\n        try {\r\n            sourceFS = sourcePath.getFileSystem(conf);\r\n            sourceStatus = sourceFS.getFileStatus(sourcePath);\r\n            final boolean preserveXAttrs = fileAttributes.contains(FileAttribute.XATTR);\r\n            sourceCurrStatus = DistCpUtils.toCopyListingFileStatusHelper(sourceFS, sourceStatus, fileAttributes.contains(FileAttribute.ACL), preserveXAttrs, preserveRawXattrs, sourceFileStatus.getChunkOffset(), sourceFileStatus.getChunkLength());\r\n        } catch (FileNotFoundException e) {\r\n            throw new IOException(new RetriableFileCopyCommand.CopyReadException(e));\r\n        }\r\n        FileStatus targetStatus = null;\r\n        try {\r\n            targetStatus = targetFS.getFileStatus(target);\r\n        } catch (FileNotFoundException ignore) {\r\n            if (LOG.isDebugEnabled())\r\n                LOG.debug(\"Path could not be found: \" + target, ignore);\r\n        }\r\n        if (targetStatus != null && (targetStatus.isDirectory() != sourceCurrStatus.isDirectory())) {\r\n            throw new IOException(\"Can't replace \" + target + \". Target is \" + getFileType(targetStatus) + \", Source is \" + getFileType(sourceCurrStatus));\r\n        }\r\n        if (sourceCurrStatus.isDirectory()) {\r\n            createTargetDirsWithRetry(description, target, context, sourceStatus);\r\n            return;\r\n        }\r\n        FileAction action = checkUpdate(sourceFS, sourceCurrStatus, target, targetStatus);\r\n        Path tmpTarget = target;\r\n        if (action == FileAction.SKIP) {\r\n            LOG.info(\"Skipping copy of \" + sourceCurrStatus.getPath() + \" to \" + target);\r\n            updateSkipCounters(context, sourceCurrStatus);\r\n            context.write(null, new Text(\"SKIP: \" + sourceCurrStatus.getPath()));\r\n            if (verboseLog) {\r\n                context.write(null, new Text(\"FILE_SKIPPED: source=\" + sourceFileStatus.getPath() + \", size=\" + sourceFileStatus.getLen() + \" --> \" + \"target=\" + target + \", size=\" + (targetStatus == null ? 0 : targetStatus.getLen())));\r\n            }\r\n        } else {\r\n            if (sourceCurrStatus.isSplit()) {\r\n                tmpTarget = DistCpUtils.getSplitChunkPath(target, sourceCurrStatus);\r\n            }\r\n            if (LOG.isDebugEnabled()) {\r\n                LOG.debug(\"copying \" + sourceCurrStatus + \" \" + tmpTarget);\r\n            }\r\n            copyFileWithRetry(description, sourceCurrStatus, tmpTarget, targetStatus, context, action, fileAttributes, sourceStatus);\r\n        }\r\n        DistCpUtils.preserve(target.getFileSystem(conf), tmpTarget, sourceCurrStatus, fileAttributes, preserveRawXattrs);\r\n    } catch (IOException exception) {\r\n        handleFailures(exception, sourceFileStatus, target, context);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 3,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\main\\java\\org\\apache\\hadoop\\tools\\mapred",
  "methodName" : "getFileType",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String getFileType(CopyListingFileStatus fileStatus)\n{\r\n    if (null == fileStatus) {\r\n        return \"N/A\";\r\n    }\r\n    return fileStatus.isDirectory() ? \"dir\" : \"file\";\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\main\\java\\org\\apache\\hadoop\\tools\\mapred",
  "methodName" : "getFileType",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String getFileType(FileStatus fileStatus)\n{\r\n    if (null == fileStatus) {\r\n        return \"N/A\";\r\n    }\r\n    return fileStatus.isDirectory() ? \"dir\" : \"file\";\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\main\\java\\org\\apache\\hadoop\\tools\\mapred",
  "methodName" : "getFileAttributeSettings",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "EnumSet<DistCpOptions.FileAttribute> getFileAttributeSettings(Mapper.Context context)\n{\r\n    String attributeString = context.getConfiguration().get(DistCpOptionSwitch.PRESERVE_STATUS.getConfigLabel());\r\n    return DistCpUtils.unpackAttributes(attributeString);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\main\\java\\org\\apache\\hadoop\\tools\\mapred",
  "methodName" : "copyFileWithRetry",
  "errType" : [ "Exception" ],
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void copyFileWithRetry(String description, CopyListingFileStatus sourceFileStatus, Path target, FileStatus targrtFileStatus, Context context, FileAction action, EnumSet<FileAttribute> fileAttributes, FileStatus sourceStatus) throws IOException, InterruptedException\n{\r\n    long bytesCopied;\r\n    try {\r\n        bytesCopied = (Long) new RetriableFileCopyCommand(skipCrc, description, action, directWrite).execute(sourceFileStatus, target, context, fileAttributes, sourceStatus);\r\n    } catch (Exception e) {\r\n        context.setStatus(\"Copy Failure: \" + sourceFileStatus.getPath());\r\n        throw new IOException(\"File copy failed: \" + sourceFileStatus.getPath() + \" --> \" + target, e);\r\n    }\r\n    incrementCounter(context, Counter.BYTESEXPECTED, sourceFileStatus.getLen());\r\n    incrementCounter(context, Counter.BYTESCOPIED, bytesCopied);\r\n    incrementCounter(context, Counter.COPY, 1);\r\n    totalBytesCopied += bytesCopied;\r\n    if (verboseLog) {\r\n        context.write(null, new Text(\"FILE_COPIED: source=\" + sourceFileStatus.getPath() + \",\" + \" size=\" + sourceFileStatus.getLen() + \" --> \" + \"target=\" + target + \", size=\" + (targrtFileStatus == null ? 0 : targrtFileStatus.getLen())));\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\main\\java\\org\\apache\\hadoop\\tools\\mapred",
  "methodName" : "createTargetDirsWithRetry",
  "errType" : [ "Exception" ],
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void createTargetDirsWithRetry(String description, Path target, Context context, FileStatus sourceStatus) throws IOException\n{\r\n    try {\r\n        new RetriableDirectoryCreateCommand(description).execute(target, context, sourceStatus);\r\n    } catch (Exception e) {\r\n        throw new IOException(\"mkdir failed for \" + target, e);\r\n    }\r\n    incrementCounter(context, Counter.DIR_COPY, 1);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\main\\java\\org\\apache\\hadoop\\tools\\mapred",
  "methodName" : "updateSkipCounters",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void updateSkipCounters(Context context, CopyListingFileStatus sourceFile)\n{\r\n    incrementCounter(context, Counter.SKIP, 1);\r\n    incrementCounter(context, Counter.BYTESSKIPPED, sourceFile.getLen());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\main\\java\\org\\apache\\hadoop\\tools\\mapred",
  "methodName" : "handleFailures",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void handleFailures(IOException exception, CopyListingFileStatus sourceFileStatus, Path target, Context context) throws IOException, InterruptedException\n{\r\n    LOG.error(\"Failure in copying \" + sourceFileStatus.getPath() + (sourceFileStatus.isSplit() ? \",\" + \" offset=\" + sourceFileStatus.getChunkOffset() + \" chunkLength=\" + sourceFileStatus.getChunkLength() : \"\") + \" to \" + target, exception);\r\n    if (ignoreFailures && ExceptionUtils.indexOfType(exception, CopyReadException.class) != -1) {\r\n        incrementCounter(context, Counter.FAIL, 1);\r\n        incrementCounter(context, Counter.BYTESFAILED, sourceFileStatus.getLen());\r\n        context.write(null, new Text(\"FAIL: \" + sourceFileStatus.getPath() + \" - \" + StringUtils.stringifyException(exception)));\r\n    } else\r\n        throw exception;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\main\\java\\org\\apache\\hadoop\\tools\\mapred",
  "methodName" : "incrementCounter",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void incrementCounter(Context context, Counter counter, long value)\n{\r\n    context.getCounter(counter).increment(value);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\main\\java\\org\\apache\\hadoop\\tools\\mapred",
  "methodName" : "checkUpdate",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "FileAction checkUpdate(FileSystem sourceFS, CopyListingFileStatus source, Path target, FileStatus targetFileStatus) throws IOException\n{\r\n    if (targetFileStatus != null && !overWrite) {\r\n        if (canSkip(sourceFS, source, targetFileStatus)) {\r\n            return FileAction.SKIP;\r\n        } else if (append) {\r\n            long targetLen = targetFileStatus.getLen();\r\n            if (targetLen < source.getLen()) {\r\n                FileChecksum sourceChecksum = sourceFS.getFileChecksum(source.getPath(), targetLen);\r\n                if (sourceChecksum != null && sourceChecksum.equals(targetFS.getFileChecksum(target))) {\r\n                    return FileAction.APPEND;\r\n                }\r\n            }\r\n        }\r\n    }\r\n    return FileAction.OVERWRITE;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\main\\java\\org\\apache\\hadoop\\tools\\mapred",
  "methodName" : "canSkip",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "boolean canSkip(FileSystem sourceFS, CopyListingFileStatus source, FileStatus target) throws IOException\n{\r\n    if (!syncFolders) {\r\n        return true;\r\n    }\r\n    boolean sameLength = target.getLen() == source.getLen();\r\n    boolean sameBlockSize = source.getBlockSize() == target.getBlockSize() || !preserve.contains(FileAttribute.BLOCKSIZE);\r\n    if (sameLength && sameBlockSize) {\r\n        return skipCrc || DistCpUtils.checksumsAreEqual(sourceFS, source.getPath(), null, targetFS, target.getPath(), source.getLen());\r\n    } else {\r\n        return false;\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\main\\java\\org\\apache\\hadoop\\tools\\mapred",
  "methodName" : "cleanup",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void cleanup(Context context) throws IOException, InterruptedException\n{\r\n    super.cleanup(context);\r\n    long secs = (System.currentTimeMillis() - startEpoch) / 1000;\r\n    incrementCounter(context, Counter.BANDWIDTH_IN_BYTES, totalBytesCopied / ((secs == 0 ? 1 : secs)));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\main\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "setSourcePaths",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setSourcePaths(List<Path> sourcePaths)\n{\r\n    this.sourcePaths = sourcePaths;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\main\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "getSourcePaths",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "List<Path> getSourcePaths()\n{\r\n    return sourcePaths;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\main\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "getSourceFileListing",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Path getSourceFileListing()\n{\r\n    return options.getSourceFileListing();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\main\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "getTargetPath",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Path getTargetPath()\n{\r\n    return options.getTargetPath();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\main\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "shouldAtomicCommit",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "boolean shouldAtomicCommit()\n{\r\n    return options.shouldAtomicCommit();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\main\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "shouldSyncFolder",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "boolean shouldSyncFolder()\n{\r\n    return options.shouldSyncFolder();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\main\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "shouldDeleteMissing",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "boolean shouldDeleteMissing()\n{\r\n    return options.shouldDeleteMissing();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\main\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "shouldIgnoreFailures",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "boolean shouldIgnoreFailures()\n{\r\n    return options.shouldIgnoreFailures();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\main\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "shouldOverwrite",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "boolean shouldOverwrite()\n{\r\n    return options.shouldOverwrite();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\main\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "shouldAppend",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "boolean shouldAppend()\n{\r\n    return options.shouldAppend();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\main\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "shouldSkipCRC",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "boolean shouldSkipCRC()\n{\r\n    return options.shouldSkipCRC();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\main\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "shouldBlock",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "boolean shouldBlock()\n{\r\n    return options.shouldBlock();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\main\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "shouldUseDiff",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "boolean shouldUseDiff()\n{\r\n    return options.shouldUseDiff();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\main\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "shouldUseRdiff",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "boolean shouldUseRdiff()\n{\r\n    return options.shouldUseRdiff();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\main\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "shouldUseSnapshotDiff",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "boolean shouldUseSnapshotDiff()\n{\r\n    return options.shouldUseSnapshotDiff();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\main\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "getFromSnapshot",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String getFromSnapshot()\n{\r\n    return options.getFromSnapshot();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\main\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "getToSnapshot",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String getToSnapshot()\n{\r\n    return options.getToSnapshot();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\main\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "getFiltersFile",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String getFiltersFile()\n{\r\n    return options.getFiltersFile();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\main\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "getNumListstatusThreads",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "int getNumListstatusThreads()\n{\r\n    return options.getNumListstatusThreads();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\main\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "getMaxMaps",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "int getMaxMaps()\n{\r\n    return options.getMaxMaps();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\main\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "getMapBandwidth",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "float getMapBandwidth()\n{\r\n    return options.getMapBandwidth();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\main\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "getPreserveAttributes",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Set<FileAttribute> getPreserveAttributes()\n{\r\n    return options.getPreserveAttributes();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\main\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "shouldPreserve",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "boolean shouldPreserve(FileAttribute attribute)\n{\r\n    return options.shouldPreserve(attribute);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\main\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "shouldPreserveRawXattrs",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean shouldPreserveRawXattrs()\n{\r\n    return preserveRawXattrs;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\main\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "setPreserveRawXattrs",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setPreserveRawXattrs(boolean preserveRawXattrs)\n{\r\n    this.preserveRawXattrs = preserveRawXattrs;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\main\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "getAtomicWorkPath",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Path getAtomicWorkPath()\n{\r\n    return options.getAtomicWorkPath();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\main\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "getLogPath",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Path getLogPath()\n{\r\n    return options.getLogPath();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\main\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "getCopyStrategy",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String getCopyStrategy()\n{\r\n    return options.getCopyStrategy();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\main\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "getBlocksPerChunk",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "int getBlocksPerChunk()\n{\r\n    return options.getBlocksPerChunk();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\main\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "shouldUseIterator",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "boolean shouldUseIterator()\n{\r\n    return options.shouldUseIterator();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\main\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "shouldUpdateRoot",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "boolean shouldUpdateRoot()\n{\r\n    return options.shouldUpdateRoot();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\main\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "splitLargeFile",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "boolean splitLargeFile()\n{\r\n    return options.getBlocksPerChunk() > 0;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\main\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "getCopyBufferSize",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "int getCopyBufferSize()\n{\r\n    return options.getCopyBufferSize();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\main\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "shouldDirectWrite",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "boolean shouldDirectWrite()\n{\r\n    return options.shouldDirectWrite();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\main\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "setTargetPathExists",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setTargetPathExists(boolean targetPathExists)\n{\r\n    this.targetPathExists = targetPathExists;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\main\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "isTargetPathExists",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean isTargetPathExists()\n{\r\n    return targetPathExists;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\main\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "appendToConf",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void appendToConf(Configuration conf)\n{\r\n    options.appendToConf(conf);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\main\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "toString",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String toString()\n{\r\n    return options.toString() + \", sourcePaths=\" + sourcePaths + \", targetPathExists=\" + targetPathExists + \", preserveRawXattrs=\" + preserveRawXattrs;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\main\\java\\org\\apache\\hadoop\\tools\\mapred",
  "methodName" : "commitJob",
  "errType" : null,
  "containingMethodsNum" : 21,
  "sourceCodeText" : "void commitJob(JobContext jobContext) throws IOException\n{\r\n    Configuration conf = jobContext.getConfiguration();\r\n    syncFolder = conf.getBoolean(DistCpConstants.CONF_LABEL_SYNC_FOLDERS, false);\r\n    overwrite = conf.getBoolean(DistCpConstants.CONF_LABEL_OVERWRITE, false);\r\n    updateRoot = conf.getBoolean(CONF_LABEL_UPDATE_ROOT, false);\r\n    targetPathExists = conf.getBoolean(DistCpConstants.CONF_LABEL_TARGET_PATH_EXISTS, true);\r\n    ignoreFailures = conf.getBoolean(DistCpOptionSwitch.IGNORE_FAILURES.getConfigLabel(), false);\r\n    if (blocksPerChunk > 0) {\r\n        concatFileChunks(conf);\r\n    }\r\n    super.commitJob(jobContext);\r\n    cleanupTempFiles(jobContext);\r\n    try {\r\n        if (conf.getBoolean(DistCpConstants.CONF_LABEL_DELETE_MISSING, false)) {\r\n            deleteMissing(conf);\r\n        } else if (conf.getBoolean(DistCpConstants.CONF_LABEL_ATOMIC_COPY, false)) {\r\n            commitData(conf);\r\n        } else if (conf.get(CONF_LABEL_TRACK_MISSING) != null) {\r\n            trackMissing(conf);\r\n        }\r\n        String attributes = conf.get(DistCpConstants.CONF_LABEL_PRESERVE_STATUS);\r\n        final boolean preserveRawXattrs = conf.getBoolean(DistCpConstants.CONF_LABEL_PRESERVE_RAWXATTRS, false);\r\n        if ((attributes != null && !attributes.isEmpty()) || preserveRawXattrs) {\r\n            preserveFileAttributesForDirectories(conf);\r\n        }\r\n        taskAttemptContext.setStatus(\"Commit Successful\");\r\n    } finally {\r\n        cleanup(conf);\r\n    }\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\main\\java\\org\\apache\\hadoop\\tools\\mapred",
  "methodName" : "abortJob",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void abortJob(JobContext jobContext, JobStatus.State state) throws IOException\n{\r\n    try {\r\n        super.abortJob(jobContext, state);\r\n    } finally {\r\n        cleanupTempFiles(jobContext);\r\n        cleanup(jobContext.getConfiguration());\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\main\\java\\org\\apache\\hadoop\\tools\\mapred",
  "methodName" : "cleanupTempFiles",
  "errType" : [ "Throwable" ],
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void cleanupTempFiles(JobContext context)\n{\r\n    try {\r\n        Configuration conf = context.getConfiguration();\r\n        Path targetWorkPath = new Path(conf.get(DistCpConstants.CONF_LABEL_TARGET_WORK_PATH));\r\n        FileSystem targetFS = targetWorkPath.getFileSystem(conf);\r\n        String jobId = context.getJobID().toString();\r\n        deleteAttemptTempFiles(targetWorkPath, targetFS, jobId);\r\n        deleteAttemptTempFiles(targetWorkPath.getParent(), targetFS, jobId);\r\n    } catch (Throwable t) {\r\n        LOG.warn(\"Unable to cleanup temp files\", t);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\main\\java\\org\\apache\\hadoop\\tools\\mapred",
  "methodName" : "deleteAttemptTempFiles",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void deleteAttemptTempFiles(Path targetWorkPath, FileSystem targetFS, String jobId) throws IOException\n{\r\n    if (targetWorkPath == null) {\r\n        return;\r\n    }\r\n    FileStatus[] tempFiles = targetFS.globStatus(new Path(targetWorkPath, \".distcp.tmp.\" + jobId.replaceAll(\"job\", \"attempt\") + \"*\"));\r\n    if (tempFiles != null && tempFiles.length > 0) {\r\n        for (FileStatus file : tempFiles) {\r\n            LOG.info(\"Cleaning up \" + file.getPath());\r\n            targetFS.delete(file.getPath(), false);\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\main\\java\\org\\apache\\hadoop\\tools\\mapred",
  "methodName" : "cleanup",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void cleanup(Configuration conf)\n{\r\n    Path metaFolder = new Path(conf.get(DistCpConstants.CONF_LABEL_META_FOLDER));\r\n    try {\r\n        FileSystem fs = metaFolder.getFileSystem(conf);\r\n        LOG.info(\"Cleaning up temporary work folder: \" + metaFolder);\r\n        fs.delete(metaFolder, true);\r\n    } catch (IOException ignore) {\r\n        LOG.error(\"Exception encountered \", ignore);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\main\\java\\org\\apache\\hadoop\\tools\\mapred",
  "methodName" : "isFileNotFoundException",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "boolean isFileNotFoundException(IOException e)\n{\r\n    if (e instanceof FileNotFoundException) {\r\n        return true;\r\n    }\r\n    if (e instanceof RemoteException) {\r\n        return ((RemoteException) e).unwrapRemoteException() instanceof FileNotFoundException;\r\n    }\r\n    return false;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\main\\java\\org\\apache\\hadoop\\tools\\mapred",
  "methodName" : "concatFileChunks",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 27,
  "sourceCodeText" : "void concatFileChunks(Configuration conf) throws IOException\n{\r\n    LOG.info(\"concat file chunks ...\");\r\n    String spath = conf.get(DistCpConstants.CONF_LABEL_LISTING_FILE_PATH);\r\n    if (spath == null || spath.isEmpty()) {\r\n        return;\r\n    }\r\n    Path sourceListing = new Path(spath);\r\n    SequenceFile.Reader sourceReader = new SequenceFile.Reader(conf, SequenceFile.Reader.file(sourceListing));\r\n    Path targetRoot = new Path(conf.get(DistCpConstants.CONF_LABEL_TARGET_WORK_PATH));\r\n    try {\r\n        CopyListingFileStatus srcFileStatus = new CopyListingFileStatus();\r\n        Text srcRelPath = new Text();\r\n        CopyListingFileStatus lastFileStatus = null;\r\n        LinkedList<Path> allChunkPaths = new LinkedList<Path>();\r\n        while (sourceReader.next(srcRelPath, srcFileStatus)) {\r\n            if (srcFileStatus.isDirectory()) {\r\n                continue;\r\n            }\r\n            Path targetFile = new Path(targetRoot.toString() + \"/\" + srcRelPath);\r\n            Path targetFileChunkPath = DistCpUtils.getSplitChunkPath(targetFile, srcFileStatus);\r\n            if (LOG.isDebugEnabled()) {\r\n                LOG.debug(\"  add \" + targetFileChunkPath + \" to concat.\");\r\n            }\r\n            allChunkPaths.add(targetFileChunkPath);\r\n            if (srcFileStatus.getChunkOffset() + srcFileStatus.getChunkLength() == srcFileStatus.getLen()) {\r\n                try {\r\n                    concatFileChunks(conf, srcFileStatus.getPath(), targetFile, allChunkPaths, srcFileStatus);\r\n                } catch (IOException e) {\r\n                    if (!isFileNotFoundException(e)) {\r\n                        String emsg = \"Failed to concat chunk files for \" + targetFile;\r\n                        if (!ignoreFailures) {\r\n                            throw new IOException(emsg, e);\r\n                        } else {\r\n                            LOG.warn(emsg, e);\r\n                        }\r\n                    }\r\n                }\r\n                allChunkPaths.clear();\r\n                lastFileStatus = null;\r\n            } else {\r\n                if (lastFileStatus == null) {\r\n                    lastFileStatus = new CopyListingFileStatus(srcFileStatus);\r\n                } else {\r\n                    if (!srcFileStatus.getPath().equals(lastFileStatus.getPath()) || srcFileStatus.getChunkOffset() != (lastFileStatus.getChunkOffset() + lastFileStatus.getChunkLength())) {\r\n                        String emsg = \"Inconsistent sequence file: current \" + \"chunk file \" + srcFileStatus + \" doesnt match prior \" + \"entry \" + lastFileStatus;\r\n                        if (!ignoreFailures) {\r\n                            throw new IOException(emsg);\r\n                        } else {\r\n                            LOG.warn(emsg + \", skipping concat this set.\");\r\n                        }\r\n                    } else {\r\n                        lastFileStatus.setChunkOffset(srcFileStatus.getChunkOffset());\r\n                        lastFileStatus.setChunkLength(srcFileStatus.getChunkLength());\r\n                    }\r\n                }\r\n            }\r\n        }\r\n    } finally {\r\n        IOUtils.closeStream(sourceReader);\r\n    }\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 2,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\main\\java\\org\\apache\\hadoop\\tools\\mapred",
  "methodName" : "preserveFileAttributesForDirectories",
  "errType" : null,
  "containingMethodsNum" : 19,
  "sourceCodeText" : "void preserveFileAttributesForDirectories(Configuration conf) throws IOException\n{\r\n    String attrSymbols = conf.get(DistCpConstants.CONF_LABEL_PRESERVE_STATUS);\r\n    final boolean syncOrOverwrite = syncFolder || overwrite;\r\n    LOG.info(\"About to preserve attributes: \" + attrSymbols);\r\n    EnumSet<FileAttribute> attributes = DistCpUtils.unpackAttributes(attrSymbols);\r\n    final boolean preserveRawXattrs = conf.getBoolean(DistCpConstants.CONF_LABEL_PRESERVE_RAWXATTRS, false);\r\n    Path sourceListing = new Path(conf.get(DistCpConstants.CONF_LABEL_LISTING_FILE_PATH));\r\n    FileSystem clusterFS = sourceListing.getFileSystem(conf);\r\n    SequenceFile.Reader sourceReader = new SequenceFile.Reader(conf, SequenceFile.Reader.file(sourceListing));\r\n    long totalLen = clusterFS.getFileStatus(sourceListing).getLen();\r\n    Path targetRoot = new Path(conf.get(DistCpConstants.CONF_LABEL_TARGET_FINAL_PATH));\r\n    long preservedEntries = 0;\r\n    try {\r\n        CopyListingFileStatus srcFileStatus = new CopyListingFileStatus();\r\n        Text srcRelPath = new Text();\r\n        while (sourceReader.next(srcRelPath, srcFileStatus)) {\r\n            if (!srcFileStatus.isDirectory())\r\n                continue;\r\n            Path targetFile = new Path(targetRoot.toString() + \"/\" + srcRelPath);\r\n            boolean skipRoot = syncOrOverwrite && !updateRoot;\r\n            if (targetRoot.equals(targetFile) && skipRoot) {\r\n                continue;\r\n            }\r\n            FileSystem targetFS = targetFile.getFileSystem(conf);\r\n            DistCpUtils.preserve(targetFS, targetFile, srcFileStatus, attributes, preserveRawXattrs);\r\n            taskAttemptContext.progress();\r\n            taskAttemptContext.setStatus(\"Preserving status on directory entries. [\" + sourceReader.getPosition() * 100 / totalLen + \"%]\");\r\n        }\r\n    } finally {\r\n        IOUtils.closeStream(sourceReader);\r\n    }\r\n    LOG.info(\"Preserved status on \" + preservedEntries + \" dir entries on target\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\main\\java\\org\\apache\\hadoop\\tools\\mapred",
  "methodName" : "trackMissing",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void trackMissing(Configuration conf) throws IOException\n{\r\n    Path trackDir = new Path(conf.get(DistCpConstants.CONF_LABEL_TRACK_MISSING));\r\n    Path sourceListing = new Path(conf.get(DistCpConstants.CONF_LABEL_LISTING_FILE_PATH));\r\n    LOG.info(\"Tracking file changes to directory {}\", trackDir);\r\n    Path sourceSortedListing = new Path(trackDir, DistCpConstants.SOURCE_SORTED_FILE);\r\n    LOG.info(\"Source listing {}\", sourceSortedListing);\r\n    DistCpUtils.sortListing(conf, sourceListing, sourceSortedListing);\r\n    Path targetListing = new Path(trackDir, TARGET_LISTING_FILE);\r\n    Path sortedTargetListing = new Path(trackDir, TARGET_SORTED_FILE);\r\n    listTargetFiles(conf, targetListing, sortedTargetListing);\r\n    LOG.info(\"Target listing {}\", sortedTargetListing);\r\n    targetListing.getFileSystem(conf).delete(targetListing, false);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\main\\java\\org\\apache\\hadoop\\tools\\mapred",
  "methodName" : "deleteMissing",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 41,
  "sourceCodeText" : "void deleteMissing(Configuration conf) throws IOException\n{\r\n    LOG.info(\"-delete option is enabled. About to remove entries from \" + \"target that are missing in source\");\r\n    long listingStart = System.currentTimeMillis();\r\n    Path sourceListing = new Path(conf.get(DistCpConstants.CONF_LABEL_LISTING_FILE_PATH));\r\n    FileSystem clusterFS = sourceListing.getFileSystem(conf);\r\n    Path sortedSourceListing = DistCpUtils.sortListing(conf, sourceListing);\r\n    long sourceListingCompleted = System.currentTimeMillis();\r\n    LOG.info(\"Source listing completed in {}\", formatDuration(sourceListingCompleted - listingStart));\r\n    Path targetListing = new Path(sourceListing.getParent(), \"targetListing.seq\");\r\n    Path sortedTargetListing = new Path(targetListing.toString() + \"_sorted\");\r\n    Path targetFinalPath = listTargetFiles(conf, targetListing, sortedTargetListing);\r\n    long totalLen = clusterFS.getFileStatus(sortedTargetListing).getLen();\r\n    SequenceFile.Reader sourceReader = new SequenceFile.Reader(conf, SequenceFile.Reader.file(sortedSourceListing));\r\n    SequenceFile.Reader targetReader = new SequenceFile.Reader(conf, SequenceFile.Reader.file(sortedTargetListing));\r\n    long deletionStart = System.currentTimeMillis();\r\n    LOG.info(\"Destination listing completed in {}\", formatDuration(deletionStart - sourceListingCompleted));\r\n    long deletedEntries = 0;\r\n    long filesDeleted = 0;\r\n    long missingDeletes = 0;\r\n    long failedDeletes = 0;\r\n    long skippedDeletes = 0;\r\n    long deletedDirectories = 0;\r\n    final DeletedDirTracker tracker = new DeletedDirTracker(1000);\r\n    try {\r\n        CopyListingFileStatus srcFileStatus = new CopyListingFileStatus();\r\n        Text srcRelPath = new Text();\r\n        CopyListingFileStatus trgtFileStatus = new CopyListingFileStatus();\r\n        Text trgtRelPath = new Text();\r\n        final FileSystem targetFS = targetFinalPath.getFileSystem(conf);\r\n        boolean showProgress;\r\n        boolean srcAvailable = sourceReader.next(srcRelPath, srcFileStatus);\r\n        while (targetReader.next(trgtRelPath, trgtFileStatus)) {\r\n            while (srcAvailable && trgtRelPath.compareTo(srcRelPath) > 0) {\r\n                srcAvailable = sourceReader.next(srcRelPath, srcFileStatus);\r\n            }\r\n            Path targetEntry = trgtFileStatus.getPath();\r\n            LOG.debug(\"Comparing {} and {}\", srcFileStatus.getPath(), targetEntry);\r\n            if (srcAvailable && trgtRelPath.equals(srcRelPath))\r\n                continue;\r\n            if (tracker.shouldDelete(trgtFileStatus)) {\r\n                showProgress = true;\r\n                try {\r\n                    if (targetFS.delete(targetEntry, true)) {\r\n                        LOG.info(\"Deleted \" + targetEntry + \" - missing at source\");\r\n                        deletedEntries++;\r\n                        if (trgtFileStatus.isDirectory()) {\r\n                            deletedDirectories++;\r\n                        } else {\r\n                            filesDeleted++;\r\n                        }\r\n                    } else {\r\n                        LOG.info(\"delete({}) returned false ({})\", targetEntry, trgtFileStatus);\r\n                        missingDeletes++;\r\n                    }\r\n                } catch (IOException e) {\r\n                    if (!ignoreFailures) {\r\n                        throw e;\r\n                    } else {\r\n                        LOG.info(\"Failed to delete {}, ignoring exception {}\", targetEntry, e.toString());\r\n                        LOG.debug(\"Failed to delete {}\", targetEntry, e);\r\n                        failedDeletes++;\r\n                    }\r\n                }\r\n            } else {\r\n                LOG.debug(\"Skipping deletion of {}\", targetEntry);\r\n                skippedDeletes++;\r\n                showProgress = false;\r\n            }\r\n            if (showProgress) {\r\n                taskAttemptContext.progress();\r\n                taskAttemptContext.setStatus(\"Deleting removed files from target. [\" + targetReader.getPosition() * 100 / totalLen + \"%]\");\r\n            }\r\n        }\r\n        LOG.info(\"Completed deletion of files from {}\", targetFS);\r\n    } finally {\r\n        IOUtils.closeStream(sourceReader);\r\n        IOUtils.closeStream(targetReader);\r\n    }\r\n    long deletionEnd = System.currentTimeMillis();\r\n    long deletedFileCount = deletedEntries - deletedDirectories;\r\n    LOG.info(\"Deleted from target: files: {} directories: {};\" + \" skipped deletions {}; deletions already missing {};\" + \" failed deletes {}\", deletedFileCount, deletedDirectories, skippedDeletes, missingDeletes, failedDeletes);\r\n    LOG.info(\"Number of tracked deleted directories {}\", tracker.size());\r\n    LOG.info(\"Duration of deletions: {}\", formatDuration(deletionEnd - deletionStart));\r\n    LOG.info(\"Total duration of deletion operation: {}\", formatDuration(deletionEnd - listingStart));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 2,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\main\\java\\org\\apache\\hadoop\\tools\\mapred",
  "methodName" : "formatDuration",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String formatDuration(long duration)\n{\r\n    long seconds = duration > 0 ? (duration / 1000) : 0;\r\n    long minutes = (seconds / 60);\r\n    long hours = (minutes / 60);\r\n    return String.format(\"%d:%02d:%02d.%03d\", hours, minutes % 60, seconds % 60, duration % 1000);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\main\\java\\org\\apache\\hadoop\\tools\\mapred",
  "methodName" : "listTargetFiles",
  "errType" : null,
  "containingMethodsNum" : 9,
  "sourceCodeText" : "Path listTargetFiles(final Configuration conf, final Path targetListing, final Path sortedTargetListing) throws IOException\n{\r\n    CopyListing target = new GlobbedCopyListing(new Configuration(conf), null);\r\n    Path targetFinalPath = new Path(conf.get(DistCpConstants.CONF_LABEL_TARGET_FINAL_PATH));\r\n    List<Path> targets = new ArrayList<>(1);\r\n    targets.add(targetFinalPath);\r\n    int threads = conf.getInt(DistCpConstants.CONF_LABEL_LISTSTATUS_THREADS, DistCpConstants.DEFAULT_LISTSTATUS_THREADS);\r\n    boolean useIterator = conf.getBoolean(DistCpConstants.CONF_LABEL_USE_ITERATOR, false);\r\n    LOG.info(\"Scanning destination directory {} with thread count: {}\", targetFinalPath, threads);\r\n    DistCpOptions options = new DistCpOptions.Builder(targets, targetFinalPath).withOverwrite(overwrite).withSyncFolder(syncFolder).withNumListstatusThreads(threads).withUseIterator(useIterator).build();\r\n    DistCpContext distCpContext = new DistCpContext(options);\r\n    distCpContext.setTargetPathExists(targetPathExists);\r\n    target.buildListing(targetListing, distCpContext);\r\n    DistCpUtils.sortListing(conf, targetListing, sortedTargetListing);\r\n    return targetFinalPath;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\main\\java\\org\\apache\\hadoop\\tools\\mapred",
  "methodName" : "commitData",
  "errType" : null,
  "containingMethodsNum" : 14,
  "sourceCodeText" : "void commitData(Configuration conf) throws IOException\n{\r\n    Path workDir = new Path(conf.get(DistCpConstants.CONF_LABEL_TARGET_WORK_PATH));\r\n    Path finalDir = new Path(conf.get(DistCpConstants.CONF_LABEL_TARGET_FINAL_PATH));\r\n    FileSystem targetFS = workDir.getFileSystem(conf);\r\n    LOG.info(\"Atomic commit enabled. Moving \" + workDir + \" to \" + finalDir);\r\n    if (targetFS.exists(finalDir) && targetFS.exists(workDir)) {\r\n        LOG.error(\"Pre-existing final-path found at: \" + finalDir);\r\n        throw new IOException(\"Target-path can't be committed to because it \" + \"exists at \" + finalDir + \". Copied data is in temp-dir: \" + workDir + \". \");\r\n    }\r\n    boolean result = targetFS.rename(workDir, finalDir);\r\n    if (!result) {\r\n        LOG.warn(\"Rename failed. Perhaps data already moved. Verifying...\");\r\n        result = targetFS.exists(finalDir) && !targetFS.exists(workDir);\r\n    }\r\n    if (result) {\r\n        LOG.info(\"Data committed successfully to \" + finalDir);\r\n        taskAttemptContext.setStatus(\"Data committed successfully to \" + finalDir);\r\n    } else {\r\n        LOG.error(\"Unable to commit data to \" + finalDir);\r\n        throw new IOException(\"Atomic commit failed. Temporary data in \" + workDir + \", Unable to move to \" + finalDir);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\main\\java\\org\\apache\\hadoop\\tools\\mapred",
  "methodName" : "concatFileChunks",
  "errType" : null,
  "containingMethodsNum" : 16,
  "sourceCodeText" : "void concatFileChunks(Configuration conf, Path sourceFile, Path targetFile, LinkedList<Path> allChunkPaths, CopyListingFileStatus srcFileStatus) throws IOException\n{\r\n    if (allChunkPaths.size() == 1) {\r\n        return;\r\n    }\r\n    if (LOG.isDebugEnabled()) {\r\n        LOG.debug(\"concat \" + targetFile + \" allChunkSize+ \" + allChunkPaths.size());\r\n    }\r\n    FileSystem dstfs = targetFile.getFileSystem(conf);\r\n    FileSystem srcfs = sourceFile.getFileSystem(conf);\r\n    Path firstChunkFile = allChunkPaths.removeFirst();\r\n    Path[] restChunkFiles = new Path[allChunkPaths.size()];\r\n    allChunkPaths.toArray(restChunkFiles);\r\n    if (LOG.isDebugEnabled()) {\r\n        LOG.debug(\"concat: firstchunk: \" + dstfs.getFileStatus(firstChunkFile));\r\n        int i = 0;\r\n        for (Path f : restChunkFiles) {\r\n            LOG.debug(\"concat: other chunk: \" + i + \": \" + dstfs.getFileStatus(f));\r\n            ++i;\r\n        }\r\n    }\r\n    dstfs.concat(firstChunkFile, restChunkFiles);\r\n    if (LOG.isDebugEnabled()) {\r\n        LOG.debug(\"concat: result: \" + dstfs.getFileStatus(firstChunkFile));\r\n    }\r\n    rename(dstfs, firstChunkFile, targetFile);\r\n    DistCpUtils.compareFileLengthsAndChecksums(srcFileStatus.getLen(), srcfs, sourceFile, null, dstfs, targetFile, skipCrc, srcFileStatus.getLen());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\main\\java\\org\\apache\\hadoop\\tools\\mapred",
  "methodName" : "rename",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void rename(FileSystem destFileSys, Path tmp, Path dst) throws IOException\n{\r\n    try {\r\n        if (destFileSys.exists(dst)) {\r\n            destFileSys.delete(dst, true);\r\n        }\r\n        destFileSys.rename(tmp, dst);\r\n    } catch (IOException ioe) {\r\n        throw new IOException(\"Fail to rename tmp file (=\" + tmp + \") to destination file (=\" + dst + \")\", ioe);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\main\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "getPath",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Path getPath()\n{\r\n    return path;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\main\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "getLen",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getLen()\n{\r\n    return length;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\main\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "getBlockSize",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getBlockSize()\n{\r\n    return blocksize;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\main\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "isDirectory",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean isDirectory()\n{\r\n    return isdir;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\main\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "getReplication",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "short getReplication()\n{\r\n    return blockReplication;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\main\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "getModificationTime",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getModificationTime()\n{\r\n    return modificationTime;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\main\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "getOwner",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getOwner()\n{\r\n    return owner;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\main\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "getGroup",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getGroup()\n{\r\n    return group;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\main\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "getAccessTime",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getAccessTime()\n{\r\n    return accessTime;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\main\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "getPermission",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "FsPermission getPermission()\n{\r\n    return permission;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\main\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "isErasureCoded",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "boolean isErasureCoded()\n{\r\n    return getPermission().getErasureCodedBit();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\main\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "getAclEntries",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "List<AclEntry> getAclEntries()\n{\r\n    return AclUtil.getAclFromPermAndEntries(getPermission(), aclEntries != null ? aclEntries : Collections.<AclEntry>emptyList());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\main\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "setAclEntries",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setAclEntries(List<AclEntry> aclEntries)\n{\r\n    this.aclEntries = aclEntries;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\main\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "getXAttrs",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Map<String, byte[]> getXAttrs()\n{\r\n    return xAttrs != null ? xAttrs : Collections.<String, byte[]>emptyMap();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\main\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "setXAttrs",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setXAttrs(Map<String, byte[]> xAttrs)\n{\r\n    this.xAttrs = xAttrs;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\main\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "getChunkOffset",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getChunkOffset()\n{\r\n    return chunkOffset;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\main\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "setChunkOffset",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setChunkOffset(long offset)\n{\r\n    this.chunkOffset = offset;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\main\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "getChunkLength",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getChunkLength()\n{\r\n    return chunkLength;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\main\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "setChunkLength",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setChunkLength(long chunkLength)\n{\r\n    this.chunkLength = chunkLength;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\main\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "isSplit",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "boolean isSplit()\n{\r\n    return getChunkLength() != Long.MAX_VALUE && getChunkLength() != getLen();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\main\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "getSizeToCopy",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "long getSizeToCopy()\n{\r\n    return isSplit() ? getChunkLength() : getLen();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\main\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "write",
  "errType" : null,
  "containingMethodsNum" : 28,
  "sourceCodeText" : "void write(DataOutput out) throws IOException\n{\r\n    Text.writeString(out, getPath().toString(), Text.DEFAULT_MAX_LEN);\r\n    out.writeLong(getLen());\r\n    out.writeBoolean(isDirectory());\r\n    out.writeShort(getReplication());\r\n    out.writeLong(getBlockSize());\r\n    out.writeLong(getModificationTime());\r\n    out.writeLong(getAccessTime());\r\n    out.writeShort(getPermission().toShort());\r\n    Text.writeString(out, getOwner(), Text.DEFAULT_MAX_LEN);\r\n    Text.writeString(out, getGroup(), Text.DEFAULT_MAX_LEN);\r\n    if (aclEntries != null) {\r\n        out.writeByte(aclEntries.size());\r\n        for (AclEntry entry : aclEntries) {\r\n            out.writeByte(entry.getScope().ordinal());\r\n            out.writeByte(entry.getType().ordinal());\r\n            WritableUtils.writeString(out, entry.getName());\r\n            out.writeByte(entry.getPermission().ordinal());\r\n        }\r\n    } else {\r\n        out.writeByte(NO_ACL_ENTRIES);\r\n    }\r\n    if (xAttrs != null) {\r\n        out.writeInt(xAttrs.size());\r\n        Iterator<Entry<String, byte[]>> iter = xAttrs.entrySet().iterator();\r\n        while (iter.hasNext()) {\r\n            Entry<String, byte[]> entry = iter.next();\r\n            WritableUtils.writeString(out, entry.getKey());\r\n            final byte[] value = entry.getValue();\r\n            if (value != null) {\r\n                out.writeInt(value.length);\r\n                if (value.length > 0) {\r\n                    out.write(value);\r\n                }\r\n            } else {\r\n                out.writeInt(-1);\r\n            }\r\n        }\r\n    } else {\r\n        out.writeInt(NO_XATTRS);\r\n    }\r\n    out.writeLong(chunkOffset);\r\n    out.writeLong(chunkLength);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\main\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "readFields",
  "errType" : null,
  "containingMethodsNum" : 21,
  "sourceCodeText" : "void readFields(DataInput in) throws IOException\n{\r\n    String strPath = Text.readString(in, Text.DEFAULT_MAX_LEN);\r\n    this.path = new Path(strPath);\r\n    this.length = in.readLong();\r\n    this.isdir = in.readBoolean();\r\n    this.blockReplication = in.readShort();\r\n    blocksize = in.readLong();\r\n    modificationTime = in.readLong();\r\n    accessTime = in.readLong();\r\n    permission.fromShort(in.readShort());\r\n    owner = Text.readString(in, Text.DEFAULT_MAX_LEN);\r\n    group = Text.readString(in, Text.DEFAULT_MAX_LEN);\r\n    byte aclEntriesSize = in.readByte();\r\n    if (aclEntriesSize != NO_ACL_ENTRIES) {\r\n        aclEntries = Lists.newArrayListWithCapacity(aclEntriesSize);\r\n        for (int i = 0; i < aclEntriesSize; ++i) {\r\n            aclEntries.add(new AclEntry.Builder().setScope(ACL_ENTRY_SCOPES[in.readByte()]).setType(ACL_ENTRY_TYPES[in.readByte()]).setName(WritableUtils.readString(in)).setPermission(FS_ACTIONS[in.readByte()]).build());\r\n        }\r\n    } else {\r\n        aclEntries = null;\r\n    }\r\n    int xAttrsSize = in.readInt();\r\n    if (xAttrsSize != NO_XATTRS) {\r\n        xAttrs = Maps.newHashMap();\r\n        for (int i = 0; i < xAttrsSize; ++i) {\r\n            final String name = WritableUtils.readString(in);\r\n            final int valueLen = in.readInt();\r\n            byte[] value = null;\r\n            if (valueLen > -1) {\r\n                value = new byte[valueLen];\r\n                if (valueLen > 0) {\r\n                    in.readFully(value);\r\n                }\r\n            }\r\n            xAttrs.put(name, value);\r\n        }\r\n    } else {\r\n        xAttrs = null;\r\n    }\r\n    chunkOffset = in.readLong();\r\n    chunkLength = in.readLong();\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\main\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "equals",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "boolean equals(Object o)\n{\r\n    if (null == o) {\r\n        return false;\r\n    }\r\n    if (getClass() != o.getClass()) {\r\n        return false;\r\n    }\r\n    CopyListingFileStatus other = (CopyListingFileStatus) o;\r\n    return getPath().equals(other.getPath()) && Objects.equal(aclEntries, other.aclEntries) && Objects.equal(xAttrs, other.xAttrs);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\main\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "hashCode",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "int hashCode()\n{\r\n    return Objects.hashCode(super.hashCode(), aclEntries, xAttrs);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\main\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "toString",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "String toString()\n{\r\n    StringBuilder sb = new StringBuilder(super.toString());\r\n    sb.append('{');\r\n    sb.append(this.getPath() == null ? \"\" : this.getPath().toString()).append(\" length = \").append(this.getLen()).append(\" aclEntries = \").append(aclEntries).append(\", xAttrs = \").append(xAttrs).append(\", modTime = \").append(modificationTime);\r\n    if (isSplit()) {\r\n        sb.append(\", chunkOffset = \").append(this.getChunkOffset()).append(\", chunkLength = \").append(this.getChunkLength());\r\n    }\r\n    sb.append('}');\r\n    return sb.toString();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\main\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "setCopyFilter",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setCopyFilter(CopyFilter copyFilter)\n{\r\n    this.copyFilter = copyFilter;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\main\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "isRdiff",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "boolean isRdiff()\n{\r\n    return context.shouldUseRdiff();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\main\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "preSyncCheck",
  "errType" : [ "FileNotFoundException" ],
  "containingMethodsNum" : 24,
  "sourceCodeText" : "boolean preSyncCheck() throws IOException\n{\r\n    List<Path> sourcePaths = context.getSourcePaths();\r\n    if (sourcePaths.size() != 1) {\r\n        throw new IllegalArgumentException(sourcePaths.size() + \" source paths are provided\");\r\n    }\r\n    final Path sourceDir = sourcePaths.get(0);\r\n    final Path targetDir = context.getTargetPath();\r\n    final FileSystem srcFs = sourceDir.getFileSystem(conf);\r\n    final FileSystem tgtFs = targetDir.getFileSystem(conf);\r\n    final FileSystem snapshotDiffFs = isRdiff() ? tgtFs : srcFs;\r\n    final Path snapshotDiffDir = isRdiff() ? targetDir : sourceDir;\r\n    if (!(srcFs instanceof DistributedFileSystem || srcFs instanceof WebHdfsFileSystem)) {\r\n        throw new IllegalArgumentException(\"Unsupported source file system: \" + srcFs.getScheme() + \"://. \" + \"Supported file systems: hdfs://, webhdfs:// and swebhdfs://.\");\r\n    }\r\n    if (!(tgtFs instanceof DistributedFileSystem || tgtFs instanceof WebHdfsFileSystem)) {\r\n        throw new IllegalArgumentException(\"Unsupported target file system: \" + tgtFs.getScheme() + \"://. \" + \"Supported file systems: hdfs://, webhdfs:// and swebhdfs://.\");\r\n    }\r\n    if (!checkNoChange(tgtFs, targetDir)) {\r\n        context.setSourcePaths(Arrays.asList(getSnapshotPath(sourceDir, context.getToSnapshot())));\r\n        return false;\r\n    }\r\n    final String from = getSnapshotName(context.getFromSnapshot());\r\n    final String to = getSnapshotName(context.getToSnapshot());\r\n    try {\r\n        final FileStatus fromSnapshotStat = snapshotDiffFs.getFileStatus(getSnapshotPath(snapshotDiffDir, from));\r\n        final FileStatus toSnapshotStat = snapshotDiffFs.getFileStatus(getSnapshotPath(snapshotDiffDir, to));\r\n        if (isRdiff()) {\r\n            if (!from.equals(\"\") && fromSnapshotStat.getModificationTime() < toSnapshotStat.getModificationTime()) {\r\n                throw new HadoopIllegalArgumentException(\"Snapshot \" + from + \" should be newer than \" + to);\r\n            }\r\n        } else {\r\n            if (!to.equals(\"\") && fromSnapshotStat.getModificationTime() > toSnapshotStat.getModificationTime()) {\r\n                throw new HadoopIllegalArgumentException(\"Snapshot \" + to + \" should be newer than \" + from);\r\n            }\r\n        }\r\n    } catch (FileNotFoundException nfe) {\r\n        throw new InvalidInputException(\"Input snapshot is not found\", nfe);\r\n    }\r\n    return true;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\main\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "sync",
  "errType" : [ "Exception" ],
  "containingMethodsNum" : 12,
  "sourceCodeText" : "boolean sync() throws IOException\n{\r\n    if (!preSyncCheck()) {\r\n        return false;\r\n    }\r\n    if (!getAllDiffs()) {\r\n        return false;\r\n    }\r\n    List<Path> sourcePaths = context.getSourcePaths();\r\n    final Path sourceDir = sourcePaths.get(0);\r\n    final Path targetDir = context.getTargetPath();\r\n    final FileSystem tfs = targetDir.getFileSystem(conf);\r\n    Path tmpDir = null;\r\n    try {\r\n        tmpDir = createTargetTmpDir(tfs, targetDir);\r\n        DiffInfo[] renameAndDeleteDiffs = getRenameAndDeleteDiffsForSync(targetDir);\r\n        if (renameAndDeleteDiffs.length > 0) {\r\n            syncDiff(renameAndDeleteDiffs, tfs, tmpDir);\r\n        }\r\n        return true;\r\n    } catch (Exception e) {\r\n        DistCp.LOG.warn(\"Failed to use snapshot diff for distcp\", e);\r\n        return false;\r\n    } finally {\r\n        deleteTargetTmpDir(tfs, tmpDir);\r\n        context.setSourcePaths(Arrays.asList(getSnapshotPath(sourceDir, context.getToSnapshot())));\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\main\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "getAllDiffs",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 30,
  "sourceCodeText" : "boolean getAllDiffs() throws IOException\n{\r\n    Path ssDir = isRdiff() ? context.getTargetPath() : context.getSourcePaths().get(0);\r\n    try {\r\n        SnapshotDiffReport report = null;\r\n        FileSystem fs = ssDir.getFileSystem(conf);\r\n        final String from = getSnapshotName(context.getFromSnapshot());\r\n        final String to = getSnapshotName(context.getToSnapshot());\r\n        if (fs instanceof DistributedFileSystem) {\r\n            DistributedFileSystem dfs = (DistributedFileSystem) fs;\r\n            report = dfs.getSnapshotDiffReport(ssDir, from, to);\r\n        } else if (fs instanceof WebHdfsFileSystem) {\r\n            WebHdfsFileSystem webHdfs = (WebHdfsFileSystem) fs;\r\n            report = webHdfs.getSnapshotDiffReport(ssDir, from, to);\r\n        } else {\r\n            throw new IllegalArgumentException(\"Unsupported file system: \" + fs.getScheme() + \"://. \" + \"Supported file systems: hdfs://, webhdfs:// and swebhdfs://.\");\r\n        }\r\n        this.diffMap = new EnumMap<>(SnapshotDiffReport.DiffType.class);\r\n        for (SnapshotDiffReport.DiffType type : SnapshotDiffReport.DiffType.values()) {\r\n            diffMap.put(type, new ArrayList<DiffInfo>());\r\n        }\r\n        deletedByExclusionDiffs = null;\r\n        for (SnapshotDiffReport.DiffReportEntry entry : report.getDiffList()) {\r\n            if (entry.getSourcePath().length <= 0) {\r\n                continue;\r\n            }\r\n            SnapshotDiffReport.DiffType dt = entry.getType();\r\n            List<DiffInfo> list = diffMap.get(dt);\r\n            final Path source = new Path(DFSUtilClient.bytes2String(entry.getSourcePath()));\r\n            final Path relativeSource = new Path(Path.SEPARATOR + source);\r\n            if (dt == SnapshotDiffReport.DiffType.MODIFY || dt == SnapshotDiffReport.DiffType.CREATE || dt == SnapshotDiffReport.DiffType.DELETE) {\r\n                if (copyFilter.shouldCopy(relativeSource)) {\r\n                    list.add(new DiffInfo(source, null, dt));\r\n                }\r\n            } else if (dt == SnapshotDiffReport.DiffType.RENAME) {\r\n                final Path target = new Path(DFSUtilClient.bytes2String(entry.getTargetPath()));\r\n                final Path relativeTarget = new Path(Path.SEPARATOR + target);\r\n                if (copyFilter.shouldCopy(relativeSource)) {\r\n                    if (copyFilter.shouldCopy(relativeTarget)) {\r\n                        list.add(new DiffInfo(source, target, dt));\r\n                    } else {\r\n                        list = diffMap.get(SnapshotDiffReport.DiffType.DELETE);\r\n                        DiffInfo info = new DiffInfo(source, null, SnapshotDiffReport.DiffType.DELETE);\r\n                        list.add(info);\r\n                        if (deletedByExclusionDiffs == null) {\r\n                            deletedByExclusionDiffs = new ArrayList<>();\r\n                        }\r\n                        deletedByExclusionDiffs.add(info);\r\n                    }\r\n                } else if (copyFilter.shouldCopy(relativeTarget)) {\r\n                    list = diffMap.get(SnapshotDiffReport.DiffType.CREATE);\r\n                    list.add(new DiffInfo(target, null, SnapshotDiffReport.DiffType.CREATE));\r\n                }\r\n            }\r\n        }\r\n        if (deletedByExclusionDiffs != null) {\r\n            Collections.sort(deletedByExclusionDiffs, DiffInfo.sourceComparator);\r\n        }\r\n        return true;\r\n    } catch (IOException e) {\r\n        DistCp.LOG.warn(\"Failed to compute snapshot diff on \" + ssDir, e);\r\n    }\r\n    this.diffMap = null;\r\n    return false;\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\main\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "getSnapshotName",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String getSnapshotName(String name)\n{\r\n    return Path.CUR_DIR.equals(name) ? \"\" : name;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\main\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "getSnapshotPath",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Path getSnapshotPath(Path inputDir, String snapshotName)\n{\r\n    if (Path.CUR_DIR.equals(snapshotName)) {\r\n        return inputDir;\r\n    } else {\r\n        return new Path(inputDir, HdfsConstants.DOT_SNAPSHOT_DIR + Path.SEPARATOR + snapshotName);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\main\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "createTargetTmpDir",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "Path createTargetTmpDir(FileSystem targetFs, Path targetDir) throws IOException\n{\r\n    final Path tmp = new Path(targetDir, DistCpConstants.HDFS_DISTCP_DIFF_DIRECTORY_NAME + DistCp.rand.nextInt());\r\n    if (!targetFs.mkdirs(tmp)) {\r\n        throw new IOException(\"The tmp directory \" + tmp + \" already exists\");\r\n    }\r\n    return tmp;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\main\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "deleteTargetTmpDir",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void deleteTargetTmpDir(FileSystem targetFs, Path tmpDir)\n{\r\n    try {\r\n        if (tmpDir != null) {\r\n            targetFs.delete(tmpDir, true);\r\n        }\r\n    } catch (IOException e) {\r\n        DistCp.LOG.error(\"Unable to cleanup tmp dir: \" + tmpDir, e);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\main\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "checkNoChange",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 6,
  "sourceCodeText" : "boolean checkNoChange(FileSystem fs, Path path)\n{\r\n    try {\r\n        final String from = getSnapshotName(context.getFromSnapshot());\r\n        SnapshotDiffReport targetDiff = null;\r\n        if (fs instanceof DistributedFileSystem) {\r\n            DistributedFileSystem dfs = (DistributedFileSystem) fs;\r\n            targetDiff = dfs.getSnapshotDiffReport(path, from, \"\");\r\n        } else {\r\n            WebHdfsFileSystem webHdfs = (WebHdfsFileSystem) fs;\r\n            targetDiff = webHdfs.getSnapshotDiffReport(path, from, \"\");\r\n        }\r\n        if (!targetDiff.getDiffList().isEmpty()) {\r\n            DistCp.LOG.warn(\"The target has been modified since snapshot \" + context.getFromSnapshot());\r\n            return false;\r\n        } else {\r\n            return true;\r\n        }\r\n    } catch (IOException e) {\r\n        DistCp.LOG.warn(\"Failed to compute snapshot diff on \" + path + \" at snapshot \" + context.getFromSnapshot(), e);\r\n    }\r\n    return false;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\main\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "syncDiff",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void syncDiff(DiffInfo[] diffs, FileSystem targetFs, Path tmpDir) throws IOException\n{\r\n    moveToTmpDir(diffs, targetFs, tmpDir);\r\n    moveToTarget(diffs, targetFs);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\main\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "moveToTmpDir",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void moveToTmpDir(DiffInfo[] diffs, FileSystem targetFs, Path tmpDir) throws IOException\n{\r\n    Arrays.sort(diffs, DiffInfo.sourceComparator);\r\n    Random random = new Random();\r\n    for (DiffInfo diff : diffs) {\r\n        Path tmpTarget = new Path(tmpDir, diff.getSource().getName());\r\n        while (targetFs.exists(tmpTarget)) {\r\n            tmpTarget = new Path(tmpDir, diff.getSource().getName() + random.nextInt());\r\n        }\r\n        diff.setTmp(tmpTarget);\r\n        targetFs.rename(diff.getSource(), tmpTarget);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\main\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "moveToTarget",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void moveToTarget(DiffInfo[] diffs, FileSystem targetFs) throws IOException\n{\r\n    Arrays.sort(diffs, DiffInfo.targetComparator);\r\n    for (DiffInfo diff : diffs) {\r\n        if (diff.getTarget() != null) {\r\n            targetFs.mkdirs(diff.getTarget().getParent());\r\n            targetFs.rename(diff.getTmp(), diff.getTarget());\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\main\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "getRenameAndDeleteDiffsForSync",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "DiffInfo[] getRenameAndDeleteDiffsForSync(Path targetDir)\n{\r\n    if (isRdiff()) {\r\n        return getRenameAndDeleteDiffsRdiff(targetDir);\r\n    } else {\r\n        return getRenameAndDeleteDiffsFdiff(targetDir);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\main\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "getRenameAndDeleteDiffsRdiff",
  "errType" : null,
  "containingMethodsNum" : 15,
  "sourceCodeText" : "DiffInfo[] getRenameAndDeleteDiffsRdiff(Path targetDir)\n{\r\n    List<DiffInfo> renameDiffsList = diffMap.get(SnapshotDiffReport.DiffType.RENAME);\r\n    List<DiffInfo> renameDiffsListReversed = new ArrayList<DiffInfo>(renameDiffsList.size());\r\n    for (DiffInfo diff : renameDiffsList) {\r\n        renameDiffsListReversed.add(new DiffInfo(diff.getTarget(), diff.getSource(), diff.getType()));\r\n    }\r\n    DiffInfo[] renameDiffArray = renameDiffsListReversed.toArray(new DiffInfo[renameDiffsList.size()]);\r\n    Arrays.sort(renameDiffArray, DiffInfo.sourceComparator);\r\n    List<DiffInfo> renameAndDeleteDiff = new ArrayList<>();\r\n    for (DiffInfo diff : diffMap.get(SnapshotDiffReport.DiffType.DELETE)) {\r\n        DiffInfo renameItem = getRenameItem(diff, renameDiffArray);\r\n        Path source;\r\n        if (renameItem != null) {\r\n            source = new Path(targetDir, translateRenamedPath(diff.getSource(), renameItem));\r\n        } else {\r\n            source = new Path(targetDir, diff.getSource());\r\n        }\r\n        renameAndDeleteDiff.add(new DiffInfo(source, null, SnapshotDiffReport.DiffType.DELETE));\r\n    }\r\n    for (DiffInfo diff : diffMap.get(SnapshotDiffReport.DiffType.RENAME)) {\r\n        Path source = new Path(targetDir, diff.getSource());\r\n        Path target = new Path(targetDir, diff.getTarget());\r\n        renameAndDeleteDiff.add(new DiffInfo(source, target, diff.getType()));\r\n    }\r\n    return renameAndDeleteDiff.toArray(new DiffInfo[renameAndDeleteDiff.size()]);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\main\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "getRenameAndDeleteDiffsFdiff",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "DiffInfo[] getRenameAndDeleteDiffsFdiff(Path targetDir)\n{\r\n    List<DiffInfo> renameAndDeleteDiff = new ArrayList<>();\r\n    for (DiffInfo diff : diffMap.get(SnapshotDiffReport.DiffType.DELETE)) {\r\n        Path source = new Path(targetDir, diff.getSource());\r\n        renameAndDeleteDiff.add(new DiffInfo(source, diff.getTarget(), diff.getType()));\r\n    }\r\n    for (DiffInfo diff : diffMap.get(SnapshotDiffReport.DiffType.RENAME)) {\r\n        Path source = new Path(targetDir, diff.getSource());\r\n        Path target = new Path(targetDir, diff.getTarget());\r\n        renameAndDeleteDiff.add(new DiffInfo(source, target, diff.getType()));\r\n    }\r\n    return renameAndDeleteDiff.toArray(new DiffInfo[renameAndDeleteDiff.size()]);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\main\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "getCreateAndModifyDiffs",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "DiffInfo[] getCreateAndModifyDiffs()\n{\r\n    List<DiffInfo> createDiff = diffMap.get(SnapshotDiffReport.DiffType.CREATE);\r\n    List<DiffInfo> modifyDiff = diffMap.get(SnapshotDiffReport.DiffType.MODIFY);\r\n    List<DiffInfo> diffs = new ArrayList<>(createDiff.size() + modifyDiff.size());\r\n    diffs.addAll(createDiff);\r\n    diffs.addAll(modifyDiff);\r\n    return diffs.toArray(new DiffInfo[diffs.size()]);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\main\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "isParentOf",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "boolean isParentOf(Path parent, Path child)\n{\r\n    String parentPath = parent.toString();\r\n    String childPath = child.toString();\r\n    if (!parentPath.endsWith(Path.SEPARATOR)) {\r\n        parentPath += Path.SEPARATOR;\r\n    }\r\n    return childPath.length() > parentPath.length() && childPath.startsWith(parentPath);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\main\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "getRenameItem",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "DiffInfo getRenameItem(DiffInfo diff, DiffInfo[] renameDiffArray)\n{\r\n    for (DiffInfo renameItem : renameDiffArray) {\r\n        if (diff.getSource().equals(renameItem.getSource())) {\r\n            if (diff.getType() == SnapshotDiffReport.DiffType.MODIFY) {\r\n                return renameItem;\r\n            }\r\n        } else if (isParentOf(renameItem.getSource(), diff.getSource())) {\r\n            return renameItem;\r\n        }\r\n    }\r\n    return null;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\main\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "isParentOrSelfMarkedDeleted",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "boolean isParentOrSelfMarkedDeleted(DiffInfo diff, List<DiffInfo> deletedDirDiffArray)\n{\r\n    for (DiffInfo item : deletedDirDiffArray) {\r\n        if (item.getSource().equals(diff.getSource())) {\r\n            if (diff.getType() == SnapshotDiffReport.DiffType.MODIFY) {\r\n                return true;\r\n            }\r\n        } else if (isParentOf(item.getSource(), diff.getSource())) {\r\n            return true;\r\n        }\r\n    }\r\n    return false;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\main\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "translateRenamedPath",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "Path translateRenamedPath(Path sourcePath, DiffInfo renameItem)\n{\r\n    if (sourcePath.equals(renameItem.getSource())) {\r\n        return renameItem.getTarget();\r\n    }\r\n    StringBuffer sb = new StringBuffer(sourcePath.toString());\r\n    String remain = sb.substring(renameItem.getSource().toString().length() + 1);\r\n    return new Path(renameItem.getTarget(), remain);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\main\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "prepareDiffListForCopyListing",
  "errType" : null,
  "containingMethodsNum" : 12,
  "sourceCodeText" : "ArrayList<DiffInfo> prepareDiffListForCopyListing()\n{\r\n    DiffInfo[] modifyAndCreateDiffs = getCreateAndModifyDiffs();\r\n    ArrayList<DiffInfo> finalListWithTarget = new ArrayList<>();\r\n    if (isRdiff()) {\r\n        for (DiffInfo diff : modifyAndCreateDiffs) {\r\n            diff.setTarget(diff.getSource());\r\n            finalListWithTarget.add(diff);\r\n        }\r\n    } else {\r\n        List<DiffInfo> renameDiffsList = diffMap.get(SnapshotDiffReport.DiffType.RENAME);\r\n        DiffInfo[] renameDiffArray = renameDiffsList.toArray(new DiffInfo[renameDiffsList.size()]);\r\n        Arrays.sort(renameDiffArray, DiffInfo.sourceComparator);\r\n        for (DiffInfo diff : modifyAndCreateDiffs) {\r\n            if (deletedByExclusionDiffs != null && isParentOrSelfMarkedDeleted(diff, deletedByExclusionDiffs)) {\r\n                continue;\r\n            }\r\n            DiffInfo renameItem = getRenameItem(diff, renameDiffArray);\r\n            if (renameItem == null) {\r\n                diff.setTarget(diff.getSource());\r\n            } else {\r\n                diff.setTarget(translateRenamedPath(diff.getSource(), renameItem));\r\n            }\r\n            finalListWithTarget.add(diff);\r\n        }\r\n    }\r\n    return finalListWithTarget;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\main\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "getTraverseExcludeList",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "HashSet<String> getTraverseExcludeList(Path newDir, Path prefix)\n{\r\n    if (renameDiffs == null) {\r\n        List<DiffInfo> renameList = diffMap.get(SnapshotDiffReport.DiffType.RENAME);\r\n        renameDiffs = renameList.toArray(new DiffInfo[renameList.size()]);\r\n        Arrays.sort(renameDiffs, DiffInfo.targetComparator);\r\n    }\r\n    if (renameDiffs.length <= 0) {\r\n        return null;\r\n    }\r\n    boolean foundChild = false;\r\n    HashSet<String> excludeList = new HashSet<>();\r\n    for (DiffInfo diff : renameDiffs) {\r\n        if (isParentOf(newDir, diff.getTarget())) {\r\n            foundChild = true;\r\n            excludeList.add(new Path(prefix, diff.getTarget()).toUri().getPath());\r\n        } else if (foundChild) {\r\n            break;\r\n        }\r\n    }\r\n    return excludeList;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\main\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "shouldCopy",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "boolean shouldCopy(Path path)\n{\r\n    for (Pattern filter : filters) {\r\n        if (filter.matcher(path.toString()).matches()) {\r\n            LOG.debug(\"Skipping {} as it matches the filter regex\", path.toString());\r\n            return false;\r\n        }\r\n    }\r\n    return true;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\main\\java\\org\\apache\\hadoop\\tools\\mapred\\lib",
  "methodName" : "getSplits",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "List<InputSplit> getSplits(JobContext jobContext) throws IOException, InterruptedException\n{\r\n    LOG.info(\"DynamicInputFormat: Getting splits for job:\" + jobContext.getJobID());\r\n    chunkContext = getChunkContext(jobContext.getConfiguration());\r\n    return createSplits(jobContext, splitCopyListingIntoChunksWithShuffle(jobContext));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\main\\java\\org\\apache\\hadoop\\tools\\mapred\\lib",
  "methodName" : "createSplits",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "List<InputSplit> createSplits(JobContext jobContext, List<DynamicInputChunk> chunks) throws IOException\n{\r\n    int numMaps = getNumMapTasks(jobContext.getConfiguration());\r\n    final int nSplits = Math.min(numMaps, chunks.size());\r\n    List<InputSplit> splits = new ArrayList<InputSplit>(nSplits);\r\n    for (int i = 0; i < nSplits; ++i) {\r\n        TaskID taskId = new TaskID(jobContext.getJobID(), TaskType.MAP, i);\r\n        chunks.get(i).assignTo(taskId);\r\n        splits.add(new FileSplit(chunks.get(i).getPath(), 0, getMinRecordsPerChunk(jobContext.getConfiguration()), null));\r\n    }\r\n    DistCpUtils.publish(jobContext.getConfiguration(), CONF_LABEL_NUM_SPLITS, splits.size());\r\n    return splits;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\main\\java\\org\\apache\\hadoop\\tools\\mapred\\lib",
  "methodName" : "getChunkContext",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "DynamicInputChunkContext<K, V> getChunkContext(Configuration configuration) throws IOException\n{\r\n    if (chunkContext == null) {\r\n        chunkContext = new DynamicInputChunkContext<K, V>(configuration);\r\n    }\r\n    return chunkContext;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\main\\java\\org\\apache\\hadoop\\tools\\mapred\\lib",
  "methodName" : "splitCopyListingIntoChunksWithShuffle",
  "errType" : null,
  "containingMethodsNum" : 23,
  "sourceCodeText" : "List<DynamicInputChunk> splitCopyListingIntoChunksWithShuffle(JobContext context) throws IOException\n{\r\n    final Configuration configuration = context.getConfiguration();\r\n    int numRecords = getNumberOfRecords(configuration);\r\n    int numMaps = getNumMapTasks(configuration);\r\n    int maxChunksTolerable = getMaxChunksTolerable(configuration);\r\n    int splitRatio = getListingSplitRatio(configuration, numMaps, numRecords);\r\n    validateNumChunksUsing(splitRatio, numMaps, maxChunksTolerable);\r\n    int numEntriesPerChunk = (int) Math.ceil((float) numRecords / (splitRatio * numMaps));\r\n    DistCpUtils.publish(context.getConfiguration(), CONF_LABEL_NUM_ENTRIES_PER_CHUNK, numEntriesPerChunk);\r\n    final int nChunksTotal = (int) Math.ceil((float) numRecords / numEntriesPerChunk);\r\n    int nChunksOpenAtOnce = Math.min(N_CHUNKS_OPEN_AT_ONCE_DEFAULT, nChunksTotal);\r\n    Path listingPath = getListingFilePath(configuration);\r\n    SequenceFile.Reader reader = new SequenceFile.Reader(configuration, SequenceFile.Reader.file(listingPath));\r\n    List<DynamicInputChunk> openChunks = new ArrayList<DynamicInputChunk>();\r\n    List<DynamicInputChunk> chunksFinal = new ArrayList<DynamicInputChunk>();\r\n    CopyListingFileStatus fileStatus = new CopyListingFileStatus();\r\n    Text relPath = new Text();\r\n    int recordCounter = 0;\r\n    int chunkCount = 0;\r\n    try {\r\n        while (reader.next(relPath, fileStatus)) {\r\n            if (recordCounter % (nChunksOpenAtOnce * numEntriesPerChunk) == 0) {\r\n                closeAll(openChunks);\r\n                chunksFinal.addAll(openChunks);\r\n                openChunks = createChunks(chunkCount, nChunksTotal, nChunksOpenAtOnce);\r\n                chunkCount += openChunks.size();\r\n                nChunksOpenAtOnce = openChunks.size();\r\n                recordCounter = 0;\r\n            }\r\n            openChunks.get(recordCounter % nChunksOpenAtOnce).write(relPath, fileStatus);\r\n            ++recordCounter;\r\n        }\r\n    } finally {\r\n        closeAll(openChunks);\r\n        chunksFinal.addAll(openChunks);\r\n        IOUtils.closeStream(reader);\r\n    }\r\n    LOG.info(\"Number of dynamic-chunk-files created: \" + chunksFinal.size());\r\n    return chunksFinal;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\main\\java\\org\\apache\\hadoop\\tools\\mapred\\lib",
  "methodName" : "validateNumChunksUsing",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void validateNumChunksUsing(int splitRatio, int numMaps, int maxChunksTolerable) throws IOException\n{\r\n    if (splitRatio * numMaps > maxChunksTolerable)\r\n        throw new IOException(\"Too many chunks created with splitRatio:\" + splitRatio + \", numMaps:\" + numMaps + \". Reduce numMaps or decrease split-ratio to proceed.\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\main\\java\\org\\apache\\hadoop\\tools\\mapred\\lib",
  "methodName" : "closeAll",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void closeAll(List<DynamicInputChunk> chunks)\n{\r\n    for (DynamicInputChunk chunk : chunks) chunk.close();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\main\\java\\org\\apache\\hadoop\\tools\\mapred\\lib",
  "methodName" : "createChunks",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "List<DynamicInputChunk> createChunks(int chunkCount, int nChunksTotal, int nChunksOpenAtOnce) throws IOException\n{\r\n    List<DynamicInputChunk> chunks = new ArrayList<DynamicInputChunk>();\r\n    int chunkIdUpperBound = Math.min(nChunksTotal, chunkCount + nChunksOpenAtOnce);\r\n    if (nChunksTotal - chunkIdUpperBound < nChunksOpenAtOnce)\r\n        chunkIdUpperBound = nChunksTotal;\r\n    for (int i = chunkCount; i < chunkIdUpperBound; ++i) chunks.add(createChunk(i));\r\n    return chunks;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\main\\java\\org\\apache\\hadoop\\tools\\mapred\\lib",
  "methodName" : "createChunk",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "DynamicInputChunk createChunk(int chunkId) throws IOException\n{\r\n    return chunkContext.createChunkForWrite(String.format(\"%05d\", chunkId));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\main\\java\\org\\apache\\hadoop\\tools\\mapred\\lib",
  "methodName" : "getListingFilePath",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 4,
  "sourceCodeText" : "Path getListingFilePath(Configuration configuration)\n{\r\n    String listingFilePathString = configuration.get(DistCpConstants.CONF_LABEL_LISTING_FILE_PATH, \"\");\r\n    assert !listingFilePathString.equals(\"\") : \"Listing file not found.\";\r\n    Path listingFilePath = new Path(listingFilePathString);\r\n    try {\r\n        assert listingFilePath.getFileSystem(configuration).exists(listingFilePath) : \"Listing file: \" + listingFilePath + \" not found.\";\r\n    } catch (IOException e) {\r\n        assert false : \"Listing file: \" + listingFilePath + \" couldn't be accessed. \" + e.getMessage();\r\n    }\r\n    return listingFilePath;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\main\\java\\org\\apache\\hadoop\\tools\\mapred\\lib",
  "methodName" : "getNumberOfRecords",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "int getNumberOfRecords(Configuration configuration)\n{\r\n    return DistCpUtils.getInt(configuration, DistCpConstants.CONF_LABEL_TOTAL_NUMBER_OF_RECORDS);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\main\\java\\org\\apache\\hadoop\\tools\\mapred\\lib",
  "methodName" : "getNumMapTasks",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "int getNumMapTasks(Configuration configuration)\n{\r\n    return DistCpUtils.getInt(configuration, JobContext.NUM_MAPS);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\main\\java\\org\\apache\\hadoop\\tools\\mapred\\lib",
  "methodName" : "getListingSplitRatio",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "int getListingSplitRatio(Configuration configuration, int numMaps, int numPaths)\n{\r\n    return configuration.getInt(CONF_LABEL_LISTING_SPLIT_RATIO, getSplitRatio(numMaps, numPaths, configuration));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\main\\java\\org\\apache\\hadoop\\tools\\mapred\\lib",
  "methodName" : "getMaxChunksTolerable",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "int getMaxChunksTolerable(Configuration conf)\n{\r\n    int maxChunksTolerable = conf.getInt(DistCpConstants.CONF_LABEL_MAX_CHUNKS_TOLERABLE, DistCpConstants.MAX_CHUNKS_TOLERABLE_DEFAULT);\r\n    if (maxChunksTolerable <= 0) {\r\n        LOG.warn(DistCpConstants.CONF_LABEL_MAX_CHUNKS_TOLERABLE + \" should be positive. Fall back to default value: \" + DistCpConstants.MAX_CHUNKS_TOLERABLE_DEFAULT);\r\n        maxChunksTolerable = DistCpConstants.MAX_CHUNKS_TOLERABLE_DEFAULT;\r\n    }\r\n    return maxChunksTolerable;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\main\\java\\org\\apache\\hadoop\\tools\\mapred\\lib",
  "methodName" : "getMaxChunksIdeal",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "int getMaxChunksIdeal(Configuration conf)\n{\r\n    int maxChunksIdeal = conf.getInt(DistCpConstants.CONF_LABEL_MAX_CHUNKS_IDEAL, DistCpConstants.MAX_CHUNKS_IDEAL_DEFAULT);\r\n    if (maxChunksIdeal <= 0) {\r\n        LOG.warn(DistCpConstants.CONF_LABEL_MAX_CHUNKS_IDEAL + \" should be positive. Fall back to default value: \" + DistCpConstants.MAX_CHUNKS_IDEAL_DEFAULT);\r\n        maxChunksIdeal = DistCpConstants.MAX_CHUNKS_IDEAL_DEFAULT;\r\n    }\r\n    return maxChunksIdeal;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\main\\java\\org\\apache\\hadoop\\tools\\mapred\\lib",
  "methodName" : "getMinRecordsPerChunk",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "int getMinRecordsPerChunk(Configuration conf)\n{\r\n    int minRecordsPerChunk = conf.getInt(DistCpConstants.CONF_LABEL_MIN_RECORDS_PER_CHUNK, DistCpConstants.MIN_RECORDS_PER_CHUNK_DEFAULT);\r\n    if (minRecordsPerChunk <= 0) {\r\n        LOG.warn(DistCpConstants.CONF_LABEL_MIN_RECORDS_PER_CHUNK + \" should be positive. Fall back to default value: \" + DistCpConstants.MIN_RECORDS_PER_CHUNK_DEFAULT);\r\n        minRecordsPerChunk = DistCpConstants.MIN_RECORDS_PER_CHUNK_DEFAULT;\r\n    }\r\n    return minRecordsPerChunk;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\main\\java\\org\\apache\\hadoop\\tools\\mapred\\lib",
  "methodName" : "getSplitRatio",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "int getSplitRatio(Configuration conf)\n{\r\n    int splitRatio = conf.getInt(DistCpConstants.CONF_LABEL_SPLIT_RATIO, DistCpConstants.SPLIT_RATIO_DEFAULT);\r\n    if (splitRatio <= 0) {\r\n        LOG.warn(DistCpConstants.CONF_LABEL_SPLIT_RATIO + \" should be positive. Fall back to default value: \" + DistCpConstants.SPLIT_RATIO_DEFAULT);\r\n        splitRatio = DistCpConstants.SPLIT_RATIO_DEFAULT;\r\n    }\r\n    return splitRatio;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\main\\java\\org\\apache\\hadoop\\tools\\mapred\\lib",
  "methodName" : "getSplitRatio",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "int getSplitRatio(int nMaps, int nRecords)\n{\r\n    return getSplitRatio(nMaps, nRecords, new Configuration());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\main\\java\\org\\apache\\hadoop\\tools\\mapred\\lib",
  "methodName" : "getSplitRatio",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "int getSplitRatio(int nMaps, int nRecords, Configuration conf)\n{\r\n    int maxChunksIdeal = getMaxChunksIdeal(conf);\r\n    int minRecordsPerChunk = getMinRecordsPerChunk(conf);\r\n    int splitRatio = getSplitRatio(conf);\r\n    if (nMaps == 1) {\r\n        LOG.warn(\"nMaps == 1. Why use DynamicInputFormat?\");\r\n        return 1;\r\n    }\r\n    if (nMaps > maxChunksIdeal)\r\n        return splitRatio;\r\n    int nPickups = (int) Math.ceil((float) maxChunksIdeal / nMaps);\r\n    int nRecordsPerChunk = (int) Math.ceil((float) nRecords / (nMaps * nPickups));\r\n    return nRecordsPerChunk < minRecordsPerChunk ? splitRatio : nPickups;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\main\\java\\org\\apache\\hadoop\\tools\\mapred\\lib",
  "methodName" : "getNumEntriesPerChunk",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "int getNumEntriesPerChunk(Configuration configuration)\n{\r\n    return DistCpUtils.getInt(configuration, CONF_LABEL_NUM_ENTRIES_PER_CHUNK);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\main\\java\\org\\apache\\hadoop\\tools\\mapred\\lib",
  "methodName" : "createRecordReader",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "RecordReader<K, V> createRecordReader(InputSplit inputSplit, TaskAttemptContext taskAttemptContext) throws IOException, InterruptedException\n{\r\n    chunkContext = getChunkContext(taskAttemptContext.getConfiguration());\r\n    return new DynamicRecordReader<K, V>(chunkContext);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\main\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "getSourceFileListing",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Path getSourceFileListing()\n{\r\n    return sourceFileListing;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\main\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "getSourcePaths",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "List<Path> getSourcePaths()\n{\r\n    return sourcePaths == null ? null : Collections.unmodifiableList(getUniquePaths(sourcePaths));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\main\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "getUniquePaths",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "List<Path> getUniquePaths(List<Path> srcPaths)\n{\r\n    Set<Path> uniquePaths = new LinkedHashSet<>();\r\n    for (Path path : srcPaths) {\r\n        if (!uniquePaths.add(path)) {\r\n            LOG.info(\"Path: {} added multiple times, ignoring the redundant entry.\", path);\r\n        }\r\n    }\r\n    return new ArrayList<>(uniquePaths);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\main\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "getTargetPath",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Path getTargetPath()\n{\r\n    return targetPath;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\main\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "shouldAtomicCommit",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean shouldAtomicCommit()\n{\r\n    return atomicCommit;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\main\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "getAtomicWorkPath",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Path getAtomicWorkPath()\n{\r\n    return atomicWorkPath;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\main\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "shouldSyncFolder",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean shouldSyncFolder()\n{\r\n    return syncFolder;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\main\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "shouldDeleteMissing",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean shouldDeleteMissing()\n{\r\n    return deleteMissing;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\main\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "shouldIgnoreFailures",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean shouldIgnoreFailures()\n{\r\n    return ignoreFailures;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\main\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "shouldOverwrite",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean shouldOverwrite()\n{\r\n    return overwrite;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\main\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "shouldAppend",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean shouldAppend()\n{\r\n    return append;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\main\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "shouldSkipCRC",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean shouldSkipCRC()\n{\r\n    return skipCRC;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\main\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "shouldBlock",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean shouldBlock()\n{\r\n    return blocking;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\main\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "shouldUseDiff",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean shouldUseDiff()\n{\r\n    return this.useDiff;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\main\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "shouldUseRdiff",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean shouldUseRdiff()\n{\r\n    return this.useRdiff;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\main\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "shouldUseSnapshotDiff",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "boolean shouldUseSnapshotDiff()\n{\r\n    return shouldUseDiff() || shouldUseRdiff();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\main\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "getFromSnapshot",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getFromSnapshot()\n{\r\n    return this.fromSnapshot;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\main\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "getToSnapshot",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getToSnapshot()\n{\r\n    return this.toSnapshot;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\main\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "getFiltersFile",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getFiltersFile()\n{\r\n    return filtersFile;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\main\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "getLogPath",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Path getLogPath()\n{\r\n    return logPath;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\main\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "getCopyStrategy",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getCopyStrategy()\n{\r\n    return copyStrategy;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\main\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "getNumListstatusThreads",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int getNumListstatusThreads()\n{\r\n    return numListstatusThreads;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\main\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "getMaxMaps",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int getMaxMaps()\n{\r\n    return maxMaps;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\main\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "getMapBandwidth",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "float getMapBandwidth()\n{\r\n    return mapBandwidth;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\main\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "getPreserveAttributes",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Set<FileAttribute> getPreserveAttributes()\n{\r\n    return (preserveStatus == null) ? null : Collections.unmodifiableSet(preserveStatus);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\main\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "shouldPreserve",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "boolean shouldPreserve(FileAttribute attribute)\n{\r\n    return preserveStatus.contains(attribute);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\main\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "getBlocksPerChunk",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int getBlocksPerChunk()\n{\r\n    return blocksPerChunk;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\main\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "getCopyBufferSize",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int getCopyBufferSize()\n{\r\n    return copyBufferSize;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\main\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "shouldVerboseLog",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean shouldVerboseLog()\n{\r\n    return verboseLog;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\main\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "getTrackPath",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Path getTrackPath()\n{\r\n    return trackPath;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\main\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "shouldDirectWrite",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean shouldDirectWrite()\n{\r\n    return directWrite;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\main\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "shouldUseIterator",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean shouldUseIterator()\n{\r\n    return useIterator;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\main\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "shouldUpdateRoot",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean shouldUpdateRoot()\n{\r\n    return updateRoot;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\main\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "appendToConf",
  "errType" : null,
  "containingMethodsNum" : 20,
  "sourceCodeText" : "void appendToConf(Configuration conf)\n{\r\n    DistCpOptionSwitch.addToConf(conf, DistCpOptionSwitch.ATOMIC_COMMIT, String.valueOf(atomicCommit));\r\n    DistCpOptionSwitch.addToConf(conf, DistCpOptionSwitch.IGNORE_FAILURES, String.valueOf(ignoreFailures));\r\n    DistCpOptionSwitch.addToConf(conf, DistCpOptionSwitch.SYNC_FOLDERS, String.valueOf(syncFolder));\r\n    DistCpOptionSwitch.addToConf(conf, DistCpOptionSwitch.DELETE_MISSING, String.valueOf(deleteMissing));\r\n    DistCpOptionSwitch.addToConf(conf, DistCpOptionSwitch.OVERWRITE, String.valueOf(overwrite));\r\n    DistCpOptionSwitch.addToConf(conf, DistCpOptionSwitch.APPEND, String.valueOf(append));\r\n    DistCpOptionSwitch.addToConf(conf, DistCpOptionSwitch.DIFF, String.valueOf(useDiff));\r\n    DistCpOptionSwitch.addToConf(conf, DistCpOptionSwitch.RDIFF, String.valueOf(useRdiff));\r\n    DistCpOptionSwitch.addToConf(conf, DistCpOptionSwitch.SKIP_CRC, String.valueOf(skipCRC));\r\n    if (mapBandwidth > 0) {\r\n        DistCpOptionSwitch.addToConf(conf, DistCpOptionSwitch.BANDWIDTH, String.valueOf(mapBandwidth));\r\n    }\r\n    DistCpOptionSwitch.addToConf(conf, DistCpOptionSwitch.PRESERVE_STATUS, DistCpUtils.packAttributes(preserveStatus));\r\n    if (filtersFile != null) {\r\n        DistCpOptionSwitch.addToConf(conf, DistCpOptionSwitch.FILTERS, filtersFile);\r\n    }\r\n    DistCpOptionSwitch.addToConf(conf, DistCpOptionSwitch.BLOCKS_PER_CHUNK, String.valueOf(blocksPerChunk));\r\n    DistCpOptionSwitch.addToConf(conf, DistCpOptionSwitch.COPY_BUFFER_SIZE, String.valueOf(copyBufferSize));\r\n    DistCpOptionSwitch.addToConf(conf, DistCpOptionSwitch.VERBOSE_LOG, String.valueOf(verboseLog));\r\n    if (trackPath != null) {\r\n        DistCpOptionSwitch.addToConf(conf, DistCpOptionSwitch.TRACK_MISSING, String.valueOf(trackPath));\r\n    }\r\n    if (numListstatusThreads > 0) {\r\n        DistCpOptionSwitch.addToConf(conf, DistCpOptionSwitch.NUM_LISTSTATUS_THREADS, Integer.toString(numListstatusThreads));\r\n    }\r\n    DistCpOptionSwitch.addToConf(conf, DistCpOptionSwitch.DIRECT_WRITE, String.valueOf(directWrite));\r\n    DistCpOptionSwitch.addToConf(conf, DistCpOptionSwitch.USE_ITERATOR, String.valueOf(useIterator));\r\n    DistCpOptionSwitch.addToConf(conf, DistCpOptionSwitch.UPDATE_ROOT, String.valueOf(updateRoot));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\main\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "toString",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String toString()\n{\r\n    return \"DistCpOptions{\" + \"atomicCommit=\" + atomicCommit + \", syncFolder=\" + syncFolder + \", deleteMissing=\" + deleteMissing + \", ignoreFailures=\" + ignoreFailures + \", overwrite=\" + overwrite + \", append=\" + append + \", useDiff=\" + useDiff + \", useRdiff=\" + useRdiff + \", fromSnapshot=\" + fromSnapshot + \", toSnapshot=\" + toSnapshot + \", skipCRC=\" + skipCRC + \", blocking=\" + blocking + \", numListstatusThreads=\" + numListstatusThreads + \", maxMaps=\" + maxMaps + \", mapBandwidth=\" + mapBandwidth + \", copyStrategy='\" + copyStrategy + '\\'' + \", preserveStatus=\" + preserveStatus + \", atomicWorkPath=\" + atomicWorkPath + \", logPath=\" + logPath + \", sourceFileListing=\" + sourceFileListing + \", sourcePaths=\" + sourcePaths + \", targetPath=\" + targetPath + \", filtersFile='\" + filtersFile + '\\'' + \", blocksPerChunk=\" + blocksPerChunk + \", copyBufferSize=\" + copyBufferSize + \", verboseLog=\" + verboseLog + \", directWrite=\" + directWrite + \", useiterator=\" + useIterator + \", updateRoot=\" + updateRoot + '}';\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\main\\java\\org\\apache\\hadoop\\tools\\mapred",
  "methodName" : "isDirectoryOrAncestorDeleted",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "boolean isDirectoryOrAncestorDeleted(Path dir)\n{\r\n    if (dir == null) {\r\n        return false;\r\n    } else if (isContained(dir)) {\r\n        return true;\r\n    } else {\r\n        return isDirectoryOrAncestorDeleted(dir.getParent());\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\main\\java\\org\\apache\\hadoop\\tools\\mapred",
  "methodName" : "isInDeletedDirectory",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "boolean isInDeletedDirectory(Path path)\n{\r\n    Preconditions.checkArgument(!path.isRoot(), \"Root Dir\");\r\n    return isDirectoryOrAncestorDeleted(path.getParent());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\main\\java\\org\\apache\\hadoop\\tools\\mapred",
  "methodName" : "shouldDelete",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "boolean shouldDelete(CopyListingFileStatus status)\n{\r\n    Path path = status.getPath();\r\n    Preconditions.checkArgument(!path.isRoot(), \"Root Dir\");\r\n    if (status.isDirectory()) {\r\n        boolean deleted = isDirectoryOrAncestorDeleted(path);\r\n        directories.put(path, path);\r\n        return !deleted;\r\n    } else {\r\n        return !isInDeletedDirectory(path);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\main\\java\\org\\apache\\hadoop\\tools\\mapred",
  "methodName" : "isContained",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "boolean isContained(Path dir)\n{\r\n    return directories.getIfPresent(dir) != null;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\main\\java\\org\\apache\\hadoop\\tools\\mapred",
  "methodName" : "toString",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "String toString()\n{\r\n    final StringBuilder sb = new StringBuilder(\"DeletedDirTracker{\");\r\n    sb.append(\"maximum size=\").append(cacheSize);\r\n    sb.append(\"; current size=\").append(directories.size());\r\n    sb.append('}');\r\n    return sb.toString();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\main\\java\\org\\apache\\hadoop\\tools\\mapred",
  "methodName" : "size",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "long size()\n{\r\n    return directories.size();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\main\\java\\org\\apache\\hadoop\\tools\\util",
  "methodName" : "getItem",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "T getItem()\n{\r\n    return item;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\main\\java\\org\\apache\\hadoop\\tools\\util",
  "methodName" : "getRetry",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int getRetry()\n{\r\n    return retry;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\main\\java\\org\\apache\\hadoop\\tools\\mapred",
  "methodName" : "getSplits",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "List<InputSplit> getSplits(JobContext context) throws IOException, InterruptedException\n{\r\n    Configuration configuration = context.getConfiguration();\r\n    int numSplits = DistCpUtils.getInt(configuration, JobContext.NUM_MAPS);\r\n    if (numSplits == 0)\r\n        return new ArrayList<InputSplit>();\r\n    return getSplits(configuration, numSplits, DistCpUtils.getLong(configuration, DistCpConstants.CONF_LABEL_TOTAL_BYTES_TO_BE_COPIED));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\main\\java\\org\\apache\\hadoop\\tools\\mapred",
  "methodName" : "getSplits",
  "errType" : null,
  "containingMethodsNum" : 16,
  "sourceCodeText" : "List<InputSplit> getSplits(Configuration configuration, int numSplits, long totalSizeBytes) throws IOException\n{\r\n    List<InputSplit> splits = new ArrayList<InputSplit>(numSplits);\r\n    long nBytesPerSplit = (long) Math.ceil(totalSizeBytes * 1.0 / numSplits);\r\n    CopyListingFileStatus srcFileStatus = new CopyListingFileStatus();\r\n    Text srcRelPath = new Text();\r\n    long currentSplitSize = 0;\r\n    long lastSplitStart = 0;\r\n    long lastPosition = 0;\r\n    final Path listingFilePath = getListingFilePath(configuration);\r\n    if (LOG.isDebugEnabled()) {\r\n        LOG.debug(\"Average bytes per map: \" + nBytesPerSplit + \", Number of maps: \" + numSplits + \", total size: \" + totalSizeBytes);\r\n    }\r\n    SequenceFile.Reader reader = null;\r\n    try {\r\n        reader = getListingFileReader(configuration);\r\n        while (reader.next(srcRelPath, srcFileStatus)) {\r\n            if (currentSplitSize + srcFileStatus.getChunkLength() > nBytesPerSplit && lastPosition != 0) {\r\n                FileSplit split = new FileSplit(listingFilePath, lastSplitStart, lastPosition - lastSplitStart, null);\r\n                if (LOG.isDebugEnabled()) {\r\n                    LOG.debug(\"Creating split : \" + split + \", bytes in split: \" + currentSplitSize);\r\n                }\r\n                splits.add(split);\r\n                lastSplitStart = lastPosition;\r\n                currentSplitSize = 0;\r\n            }\r\n            currentSplitSize += srcFileStatus.getChunkLength();\r\n            lastPosition = reader.getPosition();\r\n        }\r\n        if (lastPosition > lastSplitStart) {\r\n            FileSplit split = new FileSplit(listingFilePath, lastSplitStart, lastPosition - lastSplitStart, null);\r\n            if (LOG.isDebugEnabled()) {\r\n                LOG.debug(\"Creating split : \" + split + \", bytes in split: \" + currentSplitSize);\r\n            }\r\n            splits.add(split);\r\n        }\r\n    } finally {\r\n        IOUtils.closeStream(reader);\r\n    }\r\n    return splits;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\main\\java\\org\\apache\\hadoop\\tools\\mapred",
  "methodName" : "getListingFilePath",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "Path getListingFilePath(Configuration configuration)\n{\r\n    final String listingFilePathString = configuration.get(DistCpConstants.CONF_LABEL_LISTING_FILE_PATH, \"\");\r\n    assert !listingFilePathString.equals(\"\") : \"Couldn't find listing file. Invalid input.\";\r\n    return new Path(listingFilePathString);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\main\\java\\org\\apache\\hadoop\\tools\\mapred",
  "methodName" : "getListingFileReader",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 5,
  "sourceCodeText" : "SequenceFile.Reader getListingFileReader(Configuration configuration)\n{\r\n    final Path listingFilePath = getListingFilePath(configuration);\r\n    try {\r\n        final FileSystem fileSystem = listingFilePath.getFileSystem(configuration);\r\n        if (!fileSystem.exists(listingFilePath))\r\n            throw new IllegalArgumentException(\"Listing file doesn't exist at: \" + listingFilePath);\r\n        return new SequenceFile.Reader(configuration, SequenceFile.Reader.file(listingFilePath));\r\n    } catch (IOException exception) {\r\n        LOG.error(\"Couldn't find listing file at: \" + listingFilePath, exception);\r\n        throw new IllegalArgumentException(\"Couldn't find listing-file at: \" + listingFilePath, exception);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\main\\java\\org\\apache\\hadoop\\tools\\mapred",
  "methodName" : "createRecordReader",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "RecordReader<Text, CopyListingFileStatus> createRecordReader(InputSplit split, TaskAttemptContext context) throws IOException, InterruptedException\n{\r\n    return new SequenceFileRecordReader<Text, CopyListingFileStatus>();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\main\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "buildListing",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void buildListing(Path pathToListFile, DistCpContext distCpContext) throws IOException\n{\r\n    validatePaths(distCpContext);\r\n    doBuildListing(pathToListFile, distCpContext);\r\n    Configuration config = getConf();\r\n    config.set(DistCpConstants.CONF_LABEL_LISTING_FILE_PATH, pathToListFile.toString());\r\n    config.setLong(DistCpConstants.CONF_LABEL_TOTAL_BYTES_TO_BE_COPIED, getBytesToCopy());\r\n    config.setLong(DistCpConstants.CONF_LABEL_TOTAL_NUMBER_OF_RECORDS, getNumberOfPaths());\r\n    validateFinalListing(pathToListFile, distCpContext);\r\n    LOG.info(\"Number of paths in the copy list: \" + this.getNumberOfPaths());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\main\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "validatePaths",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void validatePaths(DistCpContext distCpContext) throws IOException, InvalidInputException",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\main\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "doBuildListing",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void doBuildListing(Path pathToListFile, DistCpContext distCpContext) throws IOException",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\main\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "getBytesToCopy",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getBytesToCopy()",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\main\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "getNumberOfPaths",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getNumberOfPaths()",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\main\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "validateFinalListing",
  "errType" : null,
  "containingMethodsNum" : 36,
  "sourceCodeText" : "void validateFinalListing(Path pathToListFile, DistCpContext context) throws DuplicateFileException, IOException\n{\r\n    Configuration config = getConf();\r\n    final boolean splitLargeFile = context.splitLargeFile();\r\n    Path checkPath = splitLargeFile ? pathToListFile : DistCpUtils.sortListing(config, pathToListFile);\r\n    SequenceFile.Reader reader = new SequenceFile.Reader(config, SequenceFile.Reader.file(checkPath));\r\n    try {\r\n        Text lastKey = new Text(\"*\");\r\n        long lastChunkOffset = -1;\r\n        long lastChunkLength = -1;\r\n        CopyListingFileStatus lastFileStatus = new CopyListingFileStatus();\r\n        Text currentKey = new Text();\r\n        Set<URI> aclSupportCheckFsSet = Sets.newHashSet();\r\n        Set<URI> xAttrSupportCheckFsSet = Sets.newHashSet();\r\n        long idx = 0;\r\n        while (reader.next(currentKey)) {\r\n            if (currentKey.equals(lastKey)) {\r\n                CopyListingFileStatus currentFileStatus = new CopyListingFileStatus();\r\n                reader.getCurrentValue(currentFileStatus);\r\n                if (!splitLargeFile) {\r\n                    throw new DuplicateFileException(\"File \" + lastFileStatus.getPath() + \" and \" + currentFileStatus.getPath() + \" would cause duplicates. Aborting\");\r\n                } else {\r\n                    if (lastChunkOffset + lastChunkLength != currentFileStatus.getChunkOffset()) {\r\n                        throw new InvalidInputException(\"File \" + lastFileStatus.getPath() + \" \" + lastChunkOffset + \",\" + lastChunkLength + \" and \" + currentFileStatus.getPath() + \" \" + currentFileStatus.getChunkOffset() + \",\" + currentFileStatus.getChunkLength() + \" are not continuous. Aborting\");\r\n                    }\r\n                }\r\n            }\r\n            reader.getCurrentValue(lastFileStatus);\r\n            if (context.shouldPreserve(DistCpOptions.FileAttribute.ACL)) {\r\n                FileSystem lastFs = lastFileStatus.getPath().getFileSystem(config);\r\n                URI lastFsUri = lastFs.getUri();\r\n                if (!aclSupportCheckFsSet.contains(lastFsUri)) {\r\n                    DistCpUtils.checkFileSystemAclSupport(lastFs);\r\n                    aclSupportCheckFsSet.add(lastFsUri);\r\n                }\r\n            }\r\n            if (context.shouldPreserve(DistCpOptions.FileAttribute.XATTR)) {\r\n                FileSystem lastFs = lastFileStatus.getPath().getFileSystem(config);\r\n                URI lastFsUri = lastFs.getUri();\r\n                if (!xAttrSupportCheckFsSet.contains(lastFsUri)) {\r\n                    DistCpUtils.checkFileSystemXAttrSupport(lastFs);\r\n                    xAttrSupportCheckFsSet.add(lastFsUri);\r\n                }\r\n            }\r\n            lastKey.set(currentKey);\r\n            if (splitLargeFile) {\r\n                lastChunkOffset = lastFileStatus.getChunkOffset();\r\n                lastChunkLength = lastFileStatus.getChunkLength();\r\n            }\r\n            if (context.shouldUseDiff() && LOG.isDebugEnabled()) {\r\n                LOG.debug(\"Copy list entry \" + idx + \": \" + lastFileStatus.getPath().toUri().getPath());\r\n                idx++;\r\n            }\r\n        }\r\n    } finally {\r\n        IOUtils.closeStream(reader);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\main\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "setCredentials",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setCredentials(Credentials credentials)\n{\r\n    this.credentials = credentials;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\main\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "getCredentials",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Credentials getCredentials()\n{\r\n    return credentials;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\main\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "getFileListingKey",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Text getFileListingKey(Path sourcePathRoot, CopyListingFileStatus fileStatus)\n{\r\n    return new Text(DistCpUtils.getRelativePath(sourcePathRoot, fileStatus.getPath()));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\main\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "getFileListingValue",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "CopyListingFileStatus getFileListingValue(CopyListingFileStatus fileStatus)\n{\r\n    return fileStatus;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\main\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "getCopyListing",
  "errType" : [ "Exception" ],
  "containingMethodsNum" : 7,
  "sourceCodeText" : "CopyListing getCopyListing(Configuration configuration, Credentials credentials, DistCpContext context) throws IOException\n{\r\n    String copyListingClassName = configuration.get(DistCpConstants.CONF_LABEL_COPY_LISTING_CLASS, \"\");\r\n    Class<? extends CopyListing> copyListingClass;\r\n    try {\r\n        if (!copyListingClassName.isEmpty()) {\r\n            copyListingClass = configuration.getClass(DistCpConstants.CONF_LABEL_COPY_LISTING_CLASS, GlobbedCopyListing.class, CopyListing.class);\r\n        } else {\r\n            if (context.getSourceFileListing() == null) {\r\n                copyListingClass = GlobbedCopyListing.class;\r\n            } else {\r\n                copyListingClass = FileBasedCopyListing.class;\r\n            }\r\n        }\r\n        copyListingClassName = copyListingClass.getName();\r\n        Constructor<? extends CopyListing> constructor = copyListingClass.getDeclaredConstructor(Configuration.class, Credentials.class);\r\n        return constructor.newInstance(configuration, credentials);\r\n    } catch (Exception e) {\r\n        throw new IOException(\"Unable to instantiate \" + copyListingClassName, e);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\main\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "validatePaths",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void validatePaths(DistCpContext context) throws IOException, InvalidInputException\n{\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\main\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "doBuildListing",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void doBuildListing(Path pathToListingFile, DistCpContext context) throws IOException\n{\r\n    List<Path> globbedPaths = new ArrayList<Path>();\r\n    if (context.getSourcePaths().isEmpty()) {\r\n        throw new InvalidInputException(\"Nothing to process. Source paths::EMPTY\");\r\n    }\r\n    for (Path p : context.getSourcePaths()) {\r\n        FileSystem fs = p.getFileSystem(getConf());\r\n        FileStatus[] inputs = fs.globStatus(p);\r\n        if (inputs != null && inputs.length > 0) {\r\n            for (FileStatus onePath : inputs) {\r\n                globbedPaths.add(onePath.getPath());\r\n            }\r\n        } else {\r\n            throw new InvalidInputException(p + \" doesn't exist\");\r\n        }\r\n    }\r\n    context.setSourcePaths(globbedPaths);\r\n    simpleListing.buildListing(pathToListingFile, context);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\main\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "getBytesToCopy",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "long getBytesToCopy()\n{\r\n    return simpleListing.getBytesToCopy();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\main\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "getNumberOfPaths",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "long getNumberOfPaths()\n{\r\n    return simpleListing.getNumberOfPaths();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\main\\java\\org\\apache\\hadoop\\tools\\mapred",
  "methodName" : "doExecute",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "Object doExecute(Object... arguments) throws Exception\n{\r\n    assert arguments.length == 5 : \"Unexpected argument list.\";\r\n    CopyListingFileStatus source = (CopyListingFileStatus) arguments[0];\r\n    assert !source.isDirectory() : \"Unexpected file-status. Expected file.\";\r\n    Path target = (Path) arguments[1];\r\n    Mapper.Context context = (Mapper.Context) arguments[2];\r\n    EnumSet<FileAttribute> fileAttributes = (EnumSet<FileAttribute>) arguments[3];\r\n    FileStatus sourceStatus = (FileStatus) arguments[4];\r\n    return doCopy(source, target, context, fileAttributes, sourceStatus);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\main\\java\\org\\apache\\hadoop\\tools\\mapred",
  "methodName" : "doCopy",
  "errType" : null,
  "containingMethodsNum" : 18,
  "sourceCodeText" : "long doCopy(CopyListingFileStatus source, Path target, Mapper.Context context, EnumSet<FileAttribute> fileAttributes, FileStatus sourceStatus) throws IOException\n{\r\n    LOG.info(\"Copying {} to {}\", source.getPath(), target);\r\n    final boolean toAppend = action == FileAction.APPEND;\r\n    final boolean useTempTarget = !toAppend && !directWrite;\r\n    Path targetPath = useTempTarget ? getTempFile(target, context) : target;\r\n    LOG.info(\"Writing to {} target file path {}\", useTempTarget ? \"temporary\" : \"direct\", targetPath);\r\n    final Configuration configuration = context.getConfiguration();\r\n    FileSystem targetFS = target.getFileSystem(configuration);\r\n    try {\r\n        final Path sourcePath = source.getPath();\r\n        final FileSystem sourceFS = sourcePath.getFileSystem(configuration);\r\n        final FileChecksum sourceChecksum = fileAttributes.contains(FileAttribute.CHECKSUMTYPE) ? sourceFS.getFileChecksum(sourcePath) : null;\r\n        long offset = (action == FileAction.APPEND) ? targetFS.getFileStatus(target).getLen() : source.getChunkOffset();\r\n        long bytesRead = copyToFile(targetPath, targetFS, source, offset, context, fileAttributes, sourceChecksum, sourceStatus);\r\n        if (!source.isSplit()) {\r\n            DistCpUtils.compareFileLengthsAndChecksums(source.getLen(), sourceFS, sourcePath, sourceChecksum, targetFS, targetPath, skipCrc, offset + bytesRead);\r\n        }\r\n        if (useTempTarget) {\r\n            LOG.info(\"Renaming temporary target file path {} to {}\", targetPath, target);\r\n            promoteTmpToTarget(targetPath, target, targetFS);\r\n        }\r\n        LOG.info(\"Completed writing {} ({} bytes)\", target, bytesRead);\r\n        return bytesRead;\r\n    } finally {\r\n        if (useTempTarget) {\r\n            targetFS.delete(targetPath, false);\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\main\\java\\org\\apache\\hadoop\\tools\\mapred",
  "methodName" : "getChecksumOpt",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "ChecksumOpt getChecksumOpt(EnumSet<FileAttribute> fileAttributes, FileChecksum sourceChecksum)\n{\r\n    if (fileAttributes.contains(FileAttribute.CHECKSUMTYPE) && sourceChecksum != null) {\r\n        return sourceChecksum.getChecksumOpt();\r\n    }\r\n    return null;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\main\\java\\org\\apache\\hadoop\\tools\\mapred",
  "methodName" : "copyToFile",
  "errType" : null,
  "containingMethodsNum" : 14,
  "sourceCodeText" : "long copyToFile(Path targetPath, FileSystem targetFS, CopyListingFileStatus source, long sourceOffset, Mapper.Context context, EnumSet<FileAttribute> fileAttributes, final FileChecksum sourceChecksum, FileStatus sourceStatus) throws IOException\n{\r\n    FsPermission permission = FsPermission.getFileDefault().applyUMask(FsPermission.getUMask(targetFS.getConf()));\r\n    int copyBufferSize = context.getConfiguration().getInt(DistCpOptionSwitch.COPY_BUFFER_SIZE.getConfigLabel(), DistCpConstants.COPY_BUFFER_SIZE_DEFAULT);\r\n    boolean preserveEC = getFileAttributeSettings(context).contains(DistCpOptions.FileAttribute.ERASURECODINGPOLICY);\r\n    ErasureCodingPolicy ecPolicy = null;\r\n    if (preserveEC && sourceStatus.isErasureCoded() && sourceStatus instanceof HdfsFileStatus && targetFS instanceof DistributedFileSystem) {\r\n        ecPolicy = ((HdfsFileStatus) sourceStatus).getErasureCodingPolicy();\r\n    }\r\n    final OutputStream outStream;\r\n    if (action == FileAction.OVERWRITE) {\r\n        final short repl = getReplicationFactor(fileAttributes, source, targetFS, targetPath);\r\n        final long blockSize = getBlockSize(fileAttributes, source, targetFS, targetPath);\r\n        FSDataOutputStream out;\r\n        ChecksumOpt checksumOpt = getChecksumOpt(fileAttributes, sourceChecksum);\r\n        if (!preserveEC || ecPolicy == null) {\r\n            out = targetFS.create(targetPath, permission, EnumSet.of(CreateFlag.CREATE, CreateFlag.OVERWRITE), copyBufferSize, repl, blockSize, context, checksumOpt);\r\n        } else {\r\n            DistributedFileSystem dfs = (DistributedFileSystem) targetFS;\r\n            DistributedFileSystem.HdfsDataOutputStreamBuilder builder = dfs.createFile(targetPath).permission(permission).create().overwrite(true).bufferSize(copyBufferSize).replication(repl).blockSize(blockSize).progress(context).recursive().ecPolicyName(ecPolicy.getName());\r\n            if (checksumOpt != null) {\r\n                builder.checksumOpt(checksumOpt);\r\n            }\r\n            out = builder.build();\r\n        }\r\n        outStream = new BufferedOutputStream(out);\r\n    } else {\r\n        outStream = new BufferedOutputStream(targetFS.append(targetPath, copyBufferSize));\r\n    }\r\n    return copyBytes(source, sourceOffset, outStream, copyBufferSize, context);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\main\\java\\org\\apache\\hadoop\\tools\\mapred",
  "methodName" : "promoteTmpToTarget",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void promoteTmpToTarget(Path tmpTarget, Path target, FileSystem fs) throws IOException\n{\r\n    if ((fs.exists(target) && !fs.delete(target, false)) || (!fs.exists(target.getParent()) && !fs.mkdirs(target.getParent())) || !fs.rename(tmpTarget, target)) {\r\n        throw new IOException(\"Failed to promote tmp-file:\" + tmpTarget + \" to: \" + target);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\main\\java\\org\\apache\\hadoop\\tools\\mapred",
  "methodName" : "getTempFile",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "Path getTempFile(Path target, Mapper.Context context)\n{\r\n    Path targetWorkPath = new Path(context.getConfiguration().get(DistCpConstants.CONF_LABEL_TARGET_WORK_PATH));\r\n    Path root = target.equals(targetWorkPath) ? targetWorkPath.getParent() : targetWorkPath;\r\n    Path tempFile = new Path(root, \".distcp.tmp.\" + context.getTaskAttemptID().toString() + \".\" + String.valueOf(System.currentTimeMillis()));\r\n    LOG.info(\"Creating temp file: {}\", tempFile);\r\n    return tempFile;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\main\\java\\org\\apache\\hadoop\\tools\\mapred",
  "methodName" : "copyBytes",
  "errType" : null,
  "containingMethodsNum" : 13,
  "sourceCodeText" : "long copyBytes(CopyListingFileStatus source2, long sourceOffset, OutputStream outStream, int bufferSize, Mapper.Context context) throws IOException\n{\r\n    Path source = source2.getPath();\r\n    byte[] buf = new byte[bufferSize];\r\n    ThrottledInputStream inStream = null;\r\n    long totalBytesRead = 0;\r\n    long chunkLength = source2.getChunkLength();\r\n    boolean finished = false;\r\n    try {\r\n        inStream = getInputStream(source, context.getConfiguration());\r\n        long fileLength = source2.getLen();\r\n        int numBytesToRead = (int) getNumBytesToRead(fileLength, sourceOffset, bufferSize);\r\n        seekIfRequired(inStream, sourceOffset);\r\n        int bytesRead = readBytes(inStream, buf, numBytesToRead);\r\n        while (bytesRead > 0) {\r\n            if (chunkLength > 0 && (totalBytesRead + bytesRead) >= chunkLength) {\r\n                bytesRead = (int) (chunkLength - totalBytesRead);\r\n                finished = true;\r\n            }\r\n            totalBytesRead += bytesRead;\r\n            sourceOffset += bytesRead;\r\n            outStream.write(buf, 0, bytesRead);\r\n            updateContextStatus(totalBytesRead, context, source2);\r\n            if (finished) {\r\n                break;\r\n            }\r\n            numBytesToRead = (int) getNumBytesToRead(fileLength, sourceOffset, bufferSize);\r\n            bytesRead = readBytes(inStream, buf, numBytesToRead);\r\n        }\r\n        outStream.close();\r\n        outStream = null;\r\n    } finally {\r\n        IOUtils.cleanupWithLogger(LOG, outStream, inStream);\r\n    }\r\n    return totalBytesRead;\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\main\\java\\org\\apache\\hadoop\\tools\\mapred",
  "methodName" : "getNumBytesToRead",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getNumBytesToRead(long fileLength, long position, long bufLength)\n{\r\n    if (position + bufLength < fileLength) {\r\n        return bufLength;\r\n    } else {\r\n        return fileLength - position;\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\main\\java\\org\\apache\\hadoop\\tools\\mapred",
  "methodName" : "updateContextStatus",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void updateContextStatus(long totalBytesRead, Mapper.Context context, CopyListingFileStatus source2)\n{\r\n    StringBuilder message = new StringBuilder(DistCpUtils.getFormatter().format(totalBytesRead * 100.0f / source2.getLen()));\r\n    message.append(\"% \").append(description).append(\" [\").append(DistCpUtils.getStringDescriptionFor(totalBytesRead)).append('/').append(DistCpUtils.getStringDescriptionFor(source2.getLen())).append(']');\r\n    context.setStatus(message.toString());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\main\\java\\org\\apache\\hadoop\\tools\\mapred",
  "methodName" : "readBytes",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 1,
  "sourceCodeText" : "int readBytes(ThrottledInputStream inStream, byte[] buf, int numBytes) throws IOException\n{\r\n    try {\r\n        return inStream.read(buf, 0, numBytes);\r\n    } catch (IOException e) {\r\n        throw new CopyReadException(e);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\main\\java\\org\\apache\\hadoop\\tools\\mapred",
  "methodName" : "seekIfRequired",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void seekIfRequired(ThrottledInputStream inStream, long sourceOffset) throws IOException\n{\r\n    try {\r\n        if (sourceOffset != inStream.getPos()) {\r\n            inStream.seek(sourceOffset);\r\n        }\r\n    } catch (IOException e) {\r\n        throw new CopyReadException(e);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\main\\java\\org\\apache\\hadoop\\tools\\mapred",
  "methodName" : "getInputStream",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 3,
  "sourceCodeText" : "ThrottledInputStream getInputStream(Path path, Configuration conf) throws IOException\n{\r\n    try {\r\n        FileSystem fs = path.getFileSystem(conf);\r\n        float bandwidthMB = conf.getFloat(DistCpConstants.CONF_LABEL_BANDWIDTH_MB, DistCpConstants.DEFAULT_BANDWIDTH_MB);\r\n        FSDataInputStream in = fs.open(path);\r\n        return new ThrottledInputStream(in, bandwidthMB * 1024 * 1024);\r\n    } catch (IOException e) {\r\n        throw new CopyReadException(e);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\main\\java\\org\\apache\\hadoop\\tools\\mapred",
  "methodName" : "getReplicationFactor",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "short getReplicationFactor(EnumSet<FileAttribute> fileAttributes, CopyListingFileStatus source, FileSystem targetFS, Path tmpTargetPath)\n{\r\n    return fileAttributes.contains(FileAttribute.REPLICATION) ? source.getReplication() : targetFS.getDefaultReplication(tmpTargetPath);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\main\\java\\org\\apache\\hadoop\\tools\\mapred",
  "methodName" : "getBlockSize",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "long getBlockSize(EnumSet<FileAttribute> fileAttributes, CopyListingFileStatus source, FileSystem targetFS, Path tmpTargetPath)\n{\r\n    boolean preserve = fileAttributes.contains(FileAttribute.BLOCKSIZE) || fileAttributes.contains(FileAttribute.CHECKSUMTYPE);\r\n    return preserve ? source.getBlockSize() : targetFS.getDefaultBlockSize(tmpTargetPath);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\main\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "initialize",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void initialize()\n{\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\main\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "shouldCopy",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean shouldCopy(Path path)",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\main\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "getCopyFilter",
  "errType" : [ "Exception" ],
  "containingMethodsNum" : 7,
  "sourceCodeText" : "CopyFilter getCopyFilter(Configuration conf)\n{\r\n    String filtersClassName = conf.get(DistCpConstants.CONF_LABEL_FILTERS_CLASS);\r\n    if (filtersClassName != null) {\r\n        try {\r\n            Class<? extends CopyFilter> filtersClass = conf.getClassByName(filtersClassName).asSubclass(CopyFilter.class);\r\n            filtersClassName = filtersClass.getName();\r\n            Constructor<? extends CopyFilter> constructor = filtersClass.getDeclaredConstructor(Configuration.class);\r\n            return constructor.newInstance(conf);\r\n        } catch (Exception e) {\r\n            LOG.error(DistCpConstants.CLASS_INSTANTIATION_ERROR_MSG + filtersClassName, e);\r\n            throw new RuntimeException(DistCpConstants.CLASS_INSTANTIATION_ERROR_MSG + filtersClassName, e);\r\n        }\r\n    } else {\r\n        return getDefaultCopyFilter(conf);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\main\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "getDefaultCopyFilter",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "CopyFilter getDefaultCopyFilter(Configuration conf)\n{\r\n    String filtersFilename = conf.get(DistCpConstants.CONF_LABEL_FILTERS_FILE);\r\n    if (filtersFilename == null) {\r\n        return new TrueCopyFilter();\r\n    } else {\r\n        String filterFilename = conf.get(DistCpConstants.CONF_LABEL_FILTERS_FILE);\r\n        return new RegexCopyFilter(filterFilename);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\main\\java\\org\\apache\\hadoop\\tools\\mapred\\lib",
  "methodName" : "openForWrite",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void openForWrite() throws IOException\n{\r\n    writer = SequenceFile.createWriter(chunkContext.getFs(), chunkContext.getConfiguration(), chunkFilePath, Text.class, CopyListingFileStatus.class, SequenceFile.CompressionType.NONE);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\main\\java\\org\\apache\\hadoop\\tools\\mapred\\lib",
  "methodName" : "write",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void write(Text key, CopyListingFileStatus value) throws IOException\n{\r\n    writer.append(key, value);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\main\\java\\org\\apache\\hadoop\\tools\\mapred\\lib",
  "methodName" : "close",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void close()\n{\r\n    IOUtils.cleanupWithLogger(LOG, reader, writer);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\main\\java\\org\\apache\\hadoop\\tools\\mapred\\lib",
  "methodName" : "assignTo",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void assignTo(TaskID taskId) throws IOException\n{\r\n    Path newPath = new Path(chunkContext.getChunkRootPath(), taskId.toString());\r\n    if (!chunkContext.getFs().rename(chunkFilePath, newPath)) {\r\n        LOG.warn(chunkFilePath + \" could not be assigned to \" + taskId);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\main\\java\\org\\apache\\hadoop\\tools\\mapred\\lib",
  "methodName" : "openForRead",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void openForRead(TaskAttemptContext taskAttemptContext) throws IOException, InterruptedException\n{\r\n    reader = new SequenceFileRecordReader<K, V>();\r\n    reader.initialize(new FileSplit(chunkFilePath, 0, DistCpUtils.getFileSize(chunkFilePath, chunkContext.getConfiguration()), null), taskAttemptContext);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\main\\java\\org\\apache\\hadoop\\tools\\mapred\\lib",
  "methodName" : "release",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void release() throws IOException\n{\r\n    close();\r\n    if (!chunkContext.getFs().delete(chunkFilePath, false)) {\r\n        LOG.error(\"Unable to release chunk at path: \" + chunkFilePath);\r\n        throw new IOException(\"Unable to release chunk at path: \" + chunkFilePath);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\main\\java\\org\\apache\\hadoop\\tools\\mapred\\lib",
  "methodName" : "getPath",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Path getPath()\n{\r\n    return chunkFilePath;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\main\\java\\org\\apache\\hadoop\\tools\\mapred\\lib",
  "methodName" : "getReader",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "SequenceFileRecordReader<K, V> getReader()\n{\r\n    assert reader != null : \"Reader un-initialized!\";\r\n    return reader;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\main\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "checkSnapshotsArgs",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void checkSnapshotsArgs(final String[] snapshots)\n{\r\n    Preconditions.checkArgument(snapshots != null && snapshots.length == 2 && !StringUtils.isBlank(snapshots[0]) && !StringUtils.isBlank(snapshots[1]), \"Must provide both the starting and ending snapshot names\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\main\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "parse",
  "errType" : [ "ParseException", "NumberFormatException", "NumberFormatException", "NumberFormatException", "NumberFormatException", "NumberFormatException" ],
  "containingMethodsNum" : 52,
  "sourceCodeText" : "DistCpOptions parse(String[] args) throws IllegalArgumentException\n{\r\n    CommandLineParser parser = new CustomParser();\r\n    CommandLine command;\r\n    try {\r\n        command = parser.parse(cliOptions, args, true);\r\n    } catch (ParseException e) {\r\n        throw new IllegalArgumentException(\"Unable to parse arguments. \" + Arrays.toString(args), e);\r\n    }\r\n    DistCpOptions.Builder builder = parseSourceAndTargetPaths(command);\r\n    builder.withAtomicCommit(command.hasOption(DistCpOptionSwitch.ATOMIC_COMMIT.getSwitch())).withSyncFolder(command.hasOption(DistCpOptionSwitch.SYNC_FOLDERS.getSwitch())).withDeleteMissing(command.hasOption(DistCpOptionSwitch.DELETE_MISSING.getSwitch())).withIgnoreFailures(command.hasOption(DistCpOptionSwitch.IGNORE_FAILURES.getSwitch())).withOverwrite(command.hasOption(DistCpOptionSwitch.OVERWRITE.getSwitch())).withAppend(command.hasOption(DistCpOptionSwitch.APPEND.getSwitch())).withCRC(command.hasOption(DistCpOptionSwitch.SKIP_CRC.getSwitch())).withBlocking(!command.hasOption(DistCpOptionSwitch.BLOCKING.getSwitch())).withVerboseLog(command.hasOption(DistCpOptionSwitch.VERBOSE_LOG.getSwitch())).withDirectWrite(command.hasOption(DistCpOptionSwitch.DIRECT_WRITE.getSwitch())).withUseIterator(command.hasOption(DistCpOptionSwitch.USE_ITERATOR.getSwitch())).withUpdateRoot(command.hasOption(DistCpOptionSwitch.UPDATE_ROOT.getSwitch()));\r\n    if (command.hasOption(DistCpOptionSwitch.DIFF.getSwitch())) {\r\n        String[] snapshots = getVals(command, DistCpOptionSwitch.DIFF.getSwitch());\r\n        checkSnapshotsArgs(snapshots);\r\n        builder.withUseDiff(snapshots[0], snapshots[1]);\r\n    }\r\n    if (command.hasOption(DistCpOptionSwitch.RDIFF.getSwitch())) {\r\n        String[] snapshots = getVals(command, DistCpOptionSwitch.RDIFF.getSwitch());\r\n        checkSnapshotsArgs(snapshots);\r\n        builder.withUseRdiff(snapshots[0], snapshots[1]);\r\n    }\r\n    if (command.hasOption(DistCpOptionSwitch.FILTERS.getSwitch())) {\r\n        builder.withFiltersFile(getVal(command, DistCpOptionSwitch.FILTERS.getSwitch()));\r\n    }\r\n    if (command.hasOption(DistCpOptionSwitch.LOG_PATH.getSwitch())) {\r\n        builder.withLogPath(new Path(getVal(command, DistCpOptionSwitch.LOG_PATH.getSwitch())));\r\n    }\r\n    if (command.hasOption(DistCpOptionSwitch.WORK_PATH.getSwitch())) {\r\n        final String workPath = getVal(command, DistCpOptionSwitch.WORK_PATH.getSwitch());\r\n        if (workPath != null && !workPath.isEmpty()) {\r\n            builder.withAtomicWorkPath(new Path(workPath));\r\n        }\r\n    }\r\n    if (command.hasOption(DistCpOptionSwitch.TRACK_MISSING.getSwitch())) {\r\n        builder.withTrackMissing(new Path(getVal(command, DistCpOptionSwitch.TRACK_MISSING.getSwitch())));\r\n    }\r\n    if (command.hasOption(DistCpOptionSwitch.BANDWIDTH.getSwitch())) {\r\n        try {\r\n            final Float mapBandwidth = Float.parseFloat(getVal(command, DistCpOptionSwitch.BANDWIDTH.getSwitch()));\r\n            builder.withMapBandwidth(mapBandwidth);\r\n        } catch (NumberFormatException e) {\r\n            throw new IllegalArgumentException(\"Bandwidth specified is invalid: \" + getVal(command, DistCpOptionSwitch.BANDWIDTH.getSwitch()), e);\r\n        }\r\n    }\r\n    if (command.hasOption(DistCpOptionSwitch.NUM_LISTSTATUS_THREADS.getSwitch())) {\r\n        try {\r\n            final Integer numThreads = Integer.parseInt(getVal(command, DistCpOptionSwitch.NUM_LISTSTATUS_THREADS.getSwitch()));\r\n            builder.withNumListstatusThreads(numThreads);\r\n        } catch (NumberFormatException e) {\r\n            throw new IllegalArgumentException(\"Number of liststatus threads is invalid: \" + getVal(command, DistCpOptionSwitch.NUM_LISTSTATUS_THREADS.getSwitch()), e);\r\n        }\r\n    }\r\n    if (command.hasOption(DistCpOptionSwitch.MAX_MAPS.getSwitch())) {\r\n        try {\r\n            final Integer maps = Integer.parseInt(getVal(command, DistCpOptionSwitch.MAX_MAPS.getSwitch()));\r\n            builder.maxMaps(maps);\r\n        } catch (NumberFormatException e) {\r\n            throw new IllegalArgumentException(\"Number of maps is invalid: \" + getVal(command, DistCpOptionSwitch.MAX_MAPS.getSwitch()), e);\r\n        }\r\n    }\r\n    if (command.hasOption(DistCpOptionSwitch.COPY_STRATEGY.getSwitch())) {\r\n        builder.withCopyStrategy(getVal(command, DistCpOptionSwitch.COPY_STRATEGY.getSwitch()));\r\n    }\r\n    if (command.hasOption(DistCpOptionSwitch.PRESERVE_STATUS.getSwitch())) {\r\n        builder.preserve(getVal(command, DistCpOptionSwitch.PRESERVE_STATUS.getSwitch()));\r\n    }\r\n    if (command.hasOption(DistCpOptionSwitch.FILE_LIMIT.getSwitch())) {\r\n        LOG.warn(DistCpOptionSwitch.FILE_LIMIT.getSwitch() + \" is a deprecated\" + \" option. Ignoring.\");\r\n    }\r\n    if (command.hasOption(DistCpOptionSwitch.SIZE_LIMIT.getSwitch())) {\r\n        LOG.warn(DistCpOptionSwitch.SIZE_LIMIT.getSwitch() + \" is a deprecated\" + \" option. Ignoring.\");\r\n    }\r\n    if (command.hasOption(DistCpOptionSwitch.BLOCKS_PER_CHUNK.getSwitch())) {\r\n        final String chunkSizeStr = getVal(command, DistCpOptionSwitch.BLOCKS_PER_CHUNK.getSwitch().trim());\r\n        try {\r\n            int csize = Integer.parseInt(chunkSizeStr);\r\n            csize = csize > 0 ? csize : 0;\r\n            LOG.info(\"Set distcp blocksPerChunk to \" + csize);\r\n            builder.withBlocksPerChunk(csize);\r\n        } catch (NumberFormatException e) {\r\n            throw new IllegalArgumentException(\"blocksPerChunk is invalid: \" + chunkSizeStr, e);\r\n        }\r\n    }\r\n    if (command.hasOption(DistCpOptionSwitch.COPY_BUFFER_SIZE.getSwitch())) {\r\n        final String copyBufferSizeStr = getVal(command, DistCpOptionSwitch.COPY_BUFFER_SIZE.getSwitch().trim());\r\n        try {\r\n            int copyBufferSize = Integer.parseInt(copyBufferSizeStr);\r\n            builder.withCopyBufferSize(copyBufferSize);\r\n        } catch (NumberFormatException e) {\r\n            throw new IllegalArgumentException(\"copyBufferSize is invalid: \" + copyBufferSizeStr, e);\r\n        }\r\n    }\r\n    return builder.build();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 6,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\main\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "parseSourceAndTargetPaths",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "DistCpOptions.Builder parseSourceAndTargetPaths(CommandLine command)\n{\r\n    Path targetPath;\r\n    List<Path> sourcePaths = new ArrayList<Path>();\r\n    String[] leftOverArgs = command.getArgs();\r\n    if (leftOverArgs == null || leftOverArgs.length < 1) {\r\n        throw new IllegalArgumentException(\"Target path not specified\");\r\n    }\r\n    targetPath = new Path(leftOverArgs[leftOverArgs.length - 1].trim());\r\n    for (int index = 0; index < leftOverArgs.length - 1; index++) {\r\n        sourcePaths.add(new Path(leftOverArgs[index].trim()));\r\n    }\r\n    if (command.hasOption(DistCpOptionSwitch.SOURCE_FILE_LISTING.getSwitch())) {\r\n        if (!sourcePaths.isEmpty()) {\r\n            throw new IllegalArgumentException(\"Both source file listing and \" + \"source paths present\");\r\n        }\r\n        return new DistCpOptions.Builder(new Path(getVal(command, DistCpOptionSwitch.SOURCE_FILE_LISTING.getSwitch())), targetPath);\r\n    } else {\r\n        if (sourcePaths.isEmpty()) {\r\n            throw new IllegalArgumentException(\"Neither source file listing nor \" + \"source paths present\");\r\n        }\r\n        return new DistCpOptions.Builder(sourcePaths, targetPath);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\main\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "getVal",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "String getVal(CommandLine command, String swtch)\n{\r\n    if (swtch == null) {\r\n        return null;\r\n    }\r\n    String optionValue = command.getOptionValue(swtch.trim());\r\n    if (optionValue == null) {\r\n        return null;\r\n    } else {\r\n        return optionValue.trim();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\main\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "getVals",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String[] getVals(CommandLine command, String option)\n{\r\n    return command.getOptionValues(option);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\main\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "usage",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void usage()\n{\r\n    HelpFormatter formatter = new HelpFormatter();\r\n    formatter.printHelp(\"distcp OPTIONS [source_path...] <target_path>\\n\\nOPTIONS\", cliOptions);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\main\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "initialize",
  "errType" : [ "FileNotFoundException", "IOException" ],
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void initialize()\n{\r\n    BufferedReader reader = null;\r\n    try {\r\n        InputStream is = Files.newInputStream(filtersFile.toPath());\r\n        reader = new BufferedReader(new InputStreamReader(is, Charset.forName(\"UTF-8\")));\r\n        String line;\r\n        while ((line = reader.readLine()) != null) {\r\n            Pattern pattern = Pattern.compile(line);\r\n            filters.add(pattern);\r\n        }\r\n    } catch (FileNotFoundException notFound) {\r\n        LOG.error(\"Can't find filters file \" + filtersFile);\r\n    } catch (IOException cantRead) {\r\n        LOG.error(\"An error occurred while attempting to read from \" + filtersFile);\r\n    } finally {\r\n        IOUtils.cleanupWithLogger(LOG, reader);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\main\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "setFilters",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setFilters(List<Pattern> filtersList)\n{\r\n    this.filters = filtersList;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-distcp\\src\\main\\java\\org\\apache\\hadoop\\tools",
  "methodName" : "shouldCopy",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "boolean shouldCopy(Path path)\n{\r\n    for (Pattern filter : filters) {\r\n        if (filter.matcher(path.toString()).matches()) {\r\n            return false;\r\n        }\r\n    }\r\n    return true;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
} ]
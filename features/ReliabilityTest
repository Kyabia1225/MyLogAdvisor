[ {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "displayUsage",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void displayUsage()\n{\r\n    LOG.info(\"This must be run in only the distributed mode \" + \"(LocalJobRunner not supported).\\n\\tUsage: MRReliabilityTest \" + \"-libjars <path to hadoop-examples.jar> [-scratchdir <dir>]\" + \"\\n[-scratchdir] points to a scratch space on this host where temp\" + \" files for this test will be created. Defaults to current working\" + \" dir. \\nPasswordless SSH must be set up between this host and the\" + \" nodes which the test is going to use.\\n\" + \"The test should be run on a free cluster with no parallel job submission\" + \" going on, as the test requires to restart TaskTrackers and kill tasks\" + \" any job submission while the tests are running can cause jobs/tests to fail\");\r\n    System.exit(-1);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "run",
  "errType" : null,
  "containingMethodsNum" : 12,
  "sourceCodeText" : "int run(String[] args) throws Exception\n{\r\n    Configuration conf = getConf();\r\n    if (\"local\".equals(conf.get(JTConfig.JT_IPC_ADDRESS, \"local\"))) {\r\n        displayUsage();\r\n    }\r\n    String[] otherArgs = new GenericOptionsParser(conf, args).getRemainingArgs();\r\n    if (otherArgs.length == 2) {\r\n        if (otherArgs[0].equals(\"-scratchdir\")) {\r\n            dir = otherArgs[1];\r\n        } else {\r\n            displayUsage();\r\n        }\r\n    } else if (otherArgs.length == 0) {\r\n        dir = System.getProperty(\"user.dir\");\r\n    } else {\r\n        displayUsage();\r\n    }\r\n    conf.setInt(JobContext.MAP_MAX_ATTEMPTS, 10);\r\n    conf.setInt(JobContext.REDUCE_MAX_ATTEMPTS, 10);\r\n    runSleepJobTest(new JobClient(new JobConf(conf)), conf);\r\n    runSortJobTests(new JobClient(new JobConf(conf)), conf);\r\n    return 0;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "runSleepJobTest",
  "errType" : null,
  "containingMethodsNum" : 9,
  "sourceCodeText" : "void runSleepJobTest(final JobClient jc, final Configuration conf) throws Exception\n{\r\n    ClusterStatus c = jc.getClusterStatus();\r\n    int maxMaps = c.getMaxMapTasks() * 2;\r\n    int maxReduces = maxMaps;\r\n    int mapSleepTime = (int) c.getTTExpiryInterval();\r\n    int reduceSleepTime = mapSleepTime;\r\n    String[] sleepJobArgs = new String[] { \"-m\", Integer.toString(maxMaps), \"-r\", Integer.toString(maxReduces), \"-mt\", Integer.toString(mapSleepTime), \"-rt\", Integer.toString(reduceSleepTime) };\r\n    runTest(jc, conf, \"org.apache.hadoop.mapreduce.SleepJob\", sleepJobArgs, new KillTaskThread(jc, 2, 0.2f, false, 2), new KillTrackerThread(jc, 2, 0.4f, false, 1));\r\n    LOG.info(\"SleepJob done\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "runSortJobTests",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void runSortJobTests(final JobClient jc, final Configuration conf) throws Exception\n{\r\n    String inputPath = \"my_reliability_test_input\";\r\n    String outputPath = \"my_reliability_test_output\";\r\n    FileSystem fs = jc.getFs();\r\n    fs.delete(new Path(inputPath), true);\r\n    fs.delete(new Path(outputPath), true);\r\n    runRandomWriterTest(jc, conf, inputPath);\r\n    runSortTest(jc, conf, inputPath, outputPath);\r\n    runSortValidatorTest(jc, conf, inputPath, outputPath);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "runRandomWriterTest",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void runRandomWriterTest(final JobClient jc, final Configuration conf, final String inputPath) throws Exception\n{\r\n    runTest(jc, conf, \"org.apache.hadoop.examples.RandomWriter\", new String[] { inputPath }, null, new KillTrackerThread(jc, 0, 0.4f, false, 1));\r\n    LOG.info(\"RandomWriter job done\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "runSortTest",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void runSortTest(final JobClient jc, final Configuration conf, final String inputPath, final String outputPath) throws Exception\n{\r\n    runTest(jc, conf, \"org.apache.hadoop.examples.Sort\", new String[] { inputPath, outputPath }, new KillTaskThread(jc, 2, 0.2f, false, 2), new KillTrackerThread(jc, 2, 0.8f, false, 1));\r\n    LOG.info(\"Sort job done\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "runSortValidatorTest",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void runSortValidatorTest(final JobClient jc, final Configuration conf, final String inputPath, final String outputPath) throws Exception\n{\r\n    runTest(jc, conf, \"org.apache.hadoop.mapred.SortValidator\", new String[] { \"-sortInput\", inputPath, \"-sortOutput\", outputPath }, new KillTaskThread(jc, 2, 0.2f, false, 1), new KillTrackerThread(jc, 2, 0.8f, false, 1));\r\n    LOG.info(\"SortValidator job done\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "normalizeCommandPath",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String normalizeCommandPath(String command)\n{\r\n    final String hadoopHome;\r\n    if ((hadoopHome = System.getenv(\"HADOOP_HOME\")) != null) {\r\n        command = hadoopHome + \"/\" + command;\r\n    }\r\n    return command;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "checkJobExitStatus",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void checkJobExitStatus(int status, String jobName)\n{\r\n    if (status != 0) {\r\n        LOG.info(jobName + \" job failed with status: \" + status);\r\n        System.exit(status);\r\n    } else {\r\n        LOG.info(jobName + \" done.\");\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "runTest",
  "errType" : [ "Exception" ],
  "containingMethodsNum" : 28,
  "sourceCodeText" : "void runTest(final JobClient jc, final Configuration conf, final String jobClass, final String[] args, KillTaskThread killTaskThread, KillTrackerThread killTrackerThread) throws Exception\n{\r\n    Thread t = new Thread(\"Job Test\") {\r\n\r\n        public void run() {\r\n            try {\r\n                Class<?> jobClassObj = conf.getClassByName(jobClass);\r\n                int status = ToolRunner.run(conf, (Tool) (jobClassObj.newInstance()), args);\r\n                checkJobExitStatus(status, jobClass);\r\n            } catch (Exception e) {\r\n                LOG.error(\"JOB \" + jobClass + \" failed to run\");\r\n                System.exit(-1);\r\n            }\r\n        }\r\n    };\r\n    t.setDaemon(true);\r\n    t.start();\r\n    JobStatus[] jobs;\r\n    while ((jobs = jc.jobsToComplete()).length == 0) {\r\n        LOG.info(\"Waiting for the job \" + jobClass + \" to start\");\r\n        Thread.sleep(1000);\r\n    }\r\n    JobID jobId = jobs[jobs.length - 1].getJobID();\r\n    RunningJob rJob = jc.getJob(jobId);\r\n    if (rJob.isComplete()) {\r\n        LOG.error(\"The last job returned by the querying \" + \"JobTracker is complete :\" + rJob.getJobID() + \" .Exiting the test\");\r\n        System.exit(-1);\r\n    }\r\n    while (rJob.getJobState() == JobStatus.PREP) {\r\n        LOG.info(\"JobID : \" + jobId + \" not started RUNNING yet\");\r\n        Thread.sleep(1000);\r\n        rJob = jc.getJob(jobId);\r\n    }\r\n    if (killTaskThread != null) {\r\n        killTaskThread.setRunningJob(rJob);\r\n        killTaskThread.start();\r\n        killTaskThread.join();\r\n        LOG.info(\"DONE WITH THE TASK KILL/FAIL TESTS\");\r\n    }\r\n    if (killTrackerThread != null) {\r\n        killTrackerThread.setRunningJob(rJob);\r\n        killTrackerThread.start();\r\n        killTrackerThread.join();\r\n        LOG.info(\"DONE WITH THE TESTS TO DO WITH LOST TASKTRACKERS\");\r\n    }\r\n    t.join();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "main",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void main(String[] args) throws Exception\n{\r\n    int res = ToolRunner.run(new Configuration(), new ReliabilityTest(), args);\r\n    System.exit(res);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\join",
  "methodName" : "setUp",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setUp() throws Exception\n{\r\n    Configuration conf = new Configuration();\r\n    cluster = new MiniDFSCluster.Builder(conf).numDataNodes(2).build();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\join",
  "methodName" : "tearDown",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void tearDown() throws Exception\n{\r\n    if (cluster != null) {\r\n        cluster.shutdown();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\join",
  "methodName" : "createWriters",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "SequenceFile.Writer[] createWriters(Path testdir, Configuration conf, int srcs, Path[] src) throws IOException\n{\r\n    for (int i = 0; i < srcs; ++i) {\r\n        src[i] = new Path(testdir, Integer.toString(i + 10, 36));\r\n    }\r\n    SequenceFile.Writer[] out = new SequenceFile.Writer[srcs];\r\n    for (int i = 0; i < srcs; ++i) {\r\n        out[i] = new SequenceFile.Writer(testdir.getFileSystem(conf), conf, src[i], IntWritable.class, IntWritable.class);\r\n    }\r\n    return out;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\join",
  "methodName" : "writeSimpleSrc",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "Path[] writeSimpleSrc(Path testdir, Configuration conf, int srcs) throws IOException\n{\r\n    SequenceFile.Writer[] out = null;\r\n    Path[] src = new Path[srcs];\r\n    try {\r\n        out = createWriters(testdir, conf, srcs, src);\r\n        final int capacity = srcs * 2 + 1;\r\n        IntWritable key = new IntWritable();\r\n        IntWritable val = new IntWritable();\r\n        for (int k = 0; k < capacity; ++k) {\r\n            for (int i = 0; i < srcs; ++i) {\r\n                key.set(k % srcs == 0 ? k * srcs : k * srcs + i);\r\n                val.set(10 * k + i);\r\n                out[i].append(key, val);\r\n                if (i == k) {\r\n                    out[i].append(key, val);\r\n                }\r\n            }\r\n        }\r\n    } finally {\r\n        if (out != null) {\r\n            for (int i = 0; i < srcs; ++i) {\r\n                if (out[i] != null)\r\n                    out[i].close();\r\n            }\r\n        }\r\n    }\r\n    return src;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\join",
  "methodName" : "stringify",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "String stringify(IntWritable key, Writable val)\n{\r\n    StringBuilder sb = new StringBuilder();\r\n    sb.append(\"(\" + key);\r\n    sb.append(\",\" + val + \")\");\r\n    return sb.toString();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\join",
  "methodName" : "joinAs",
  "errType" : null,
  "containingMethodsNum" : 17,
  "sourceCodeText" : "void joinAs(String jointype, Class<? extends SimpleCheckerMapBase<?>> map, Class<? extends SimpleCheckerReduceBase> reduce) throws Exception\n{\r\n    final int srcs = 4;\r\n    Configuration conf = new Configuration();\r\n    Path base = cluster.getFileSystem().makeQualified(new Path(\"/\" + jointype));\r\n    Path[] src = writeSimpleSrc(base, conf, srcs);\r\n    conf.set(CompositeInputFormat.JOIN_EXPR, CompositeInputFormat.compose(jointype, SequenceFileInputFormat.class, src));\r\n    conf.setInt(\"testdatamerge.sources\", srcs);\r\n    Job job = Job.getInstance(conf);\r\n    job.setInputFormatClass(CompositeInputFormat.class);\r\n    FileOutputFormat.setOutputPath(job, new Path(base, \"out\"));\r\n    job.setMapperClass(map);\r\n    job.setReducerClass(reduce);\r\n    job.setOutputFormatClass(SequenceFileOutputFormat.class);\r\n    job.setOutputKeyClass(IntWritable.class);\r\n    job.setOutputValueClass(IntWritable.class);\r\n    job.waitForCompletion(true);\r\n    assertTrue(\"Job failed\", job.isSuccessful());\r\n    if (\"outer\".equals(jointype)) {\r\n        checkOuterConsistency(job, src);\r\n    }\r\n    base.getFileSystem(conf).delete(base, true);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\join",
  "methodName" : "testSimpleInnerJoin",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testSimpleInnerJoin() throws Exception\n{\r\n    joinAs(\"inner\", InnerJoinMapChecker.class, InnerJoinReduceChecker.class);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\join",
  "methodName" : "testSimpleOuterJoin",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testSimpleOuterJoin() throws Exception\n{\r\n    joinAs(\"outer\", OuterJoinMapChecker.class, OuterJoinReduceChecker.class);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\join",
  "methodName" : "checkOuterConsistency",
  "errType" : null,
  "containingMethodsNum" : 10,
  "sourceCodeText" : "void checkOuterConsistency(Job job, Path[] src) throws IOException\n{\r\n    Path outf = FileOutputFormat.getOutputPath(job);\r\n    FileStatus[] outlist = cluster.getFileSystem().listStatus(outf, new Utils.OutputFileUtils.OutputFilesFilter());\r\n    assertEquals(\"number of part files is more than 1. It is\" + outlist.length, 1, outlist.length);\r\n    assertTrue(\"output file with zero length\" + outlist[0].getLen(), 0 < outlist[0].getLen());\r\n    SequenceFile.Reader r = new SequenceFile.Reader(cluster.getFileSystem(), outlist[0].getPath(), job.getConfiguration());\r\n    IntWritable k = new IntWritable();\r\n    IntWritable v = new IntWritable();\r\n    while (r.next(k, v)) {\r\n        assertEquals(\"counts does not match\", v.get(), countProduct(k, src, job.getConfiguration()));\r\n    }\r\n    r.close();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\join",
  "methodName" : "countProduct",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "int countProduct(IntWritable key, Path[] src, Configuration conf) throws IOException\n{\r\n    int product = 1;\r\n    for (Path p : src) {\r\n        int count = 0;\r\n        SequenceFile.Reader r = new SequenceFile.Reader(cluster.getFileSystem(), p, conf);\r\n        IntWritable k = new IntWritable();\r\n        IntWritable v = new IntWritable();\r\n        while (r.next(k, v)) {\r\n            if (k.equals(key)) {\r\n                count++;\r\n            }\r\n        }\r\n        r.close();\r\n        if (count != 0) {\r\n            product *= count;\r\n        }\r\n    }\r\n    return product;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\join",
  "methodName" : "testSimpleOverride",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testSimpleOverride() throws Exception\n{\r\n    joinAs(\"override\", OverrideMapChecker.class, OverrideReduceChecker.class);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\join",
  "methodName" : "testNestedJoin",
  "errType" : null,
  "containingMethodsNum" : 44,
  "sourceCodeText" : "void testNestedJoin() throws Exception\n{\r\n    final int SOURCES = 3;\r\n    final int ITEMS = (SOURCES + 1) * (SOURCES + 1);\r\n    Configuration conf = new Configuration();\r\n    Path base = cluster.getFileSystem().makeQualified(new Path(\"/nested\"));\r\n    int[][] source = new int[SOURCES][];\r\n    for (int i = 0; i < SOURCES; ++i) {\r\n        source[i] = new int[ITEMS];\r\n        for (int j = 0; j < ITEMS; ++j) {\r\n            source[i][j] = (i + 2) * (j + 1);\r\n        }\r\n    }\r\n    Path[] src = new Path[SOURCES];\r\n    SequenceFile.Writer[] out = createWriters(base, conf, SOURCES, src);\r\n    IntWritable k = new IntWritable();\r\n    for (int i = 0; i < SOURCES; ++i) {\r\n        IntWritable v = new IntWritable();\r\n        v.set(i);\r\n        for (int j = 0; j < ITEMS; ++j) {\r\n            k.set(source[i][j]);\r\n            out[i].append(k, v);\r\n        }\r\n        out[i].close();\r\n    }\r\n    out = null;\r\n    StringBuilder sb = new StringBuilder();\r\n    sb.append(\"outer(inner(\");\r\n    for (int i = 0; i < SOURCES; ++i) {\r\n        sb.append(CompositeInputFormat.compose(SequenceFileInputFormat.class, src[i].toString()));\r\n        if (i + 1 != SOURCES)\r\n            sb.append(\",\");\r\n    }\r\n    sb.append(\"),outer(\");\r\n    sb.append(CompositeInputFormat.compose(MapReduceTestUtil.Fake_IF.class, \"foobar\"));\r\n    sb.append(\",\");\r\n    for (int i = 0; i < SOURCES; ++i) {\r\n        sb.append(CompositeInputFormat.compose(SequenceFileInputFormat.class, src[i].toString()));\r\n        sb.append(\",\");\r\n    }\r\n    sb.append(CompositeInputFormat.compose(MapReduceTestUtil.Fake_IF.class, \"raboof\") + \"))\");\r\n    conf.set(CompositeInputFormat.JOIN_EXPR, sb.toString());\r\n    MapReduceTestUtil.Fake_IF.setKeyClass(conf, IntWritable.class);\r\n    MapReduceTestUtil.Fake_IF.setValClass(conf, IntWritable.class);\r\n    Job job = Job.getInstance(conf);\r\n    Path outf = new Path(base, \"out\");\r\n    FileOutputFormat.setOutputPath(job, outf);\r\n    job.setInputFormatClass(CompositeInputFormat.class);\r\n    job.setMapperClass(Mapper.class);\r\n    job.setReducerClass(Reducer.class);\r\n    job.setNumReduceTasks(0);\r\n    job.setOutputKeyClass(IntWritable.class);\r\n    job.setOutputValueClass(TupleWritable.class);\r\n    job.setOutputFormatClass(SequenceFileOutputFormat.class);\r\n    job.waitForCompletion(true);\r\n    assertTrue(\"Job failed\", job.isSuccessful());\r\n    FileStatus[] outlist = cluster.getFileSystem().listStatus(outf, new Utils.OutputFileUtils.OutputFilesFilter());\r\n    assertEquals(1, outlist.length);\r\n    assertTrue(0 < outlist[0].getLen());\r\n    SequenceFile.Reader r = new SequenceFile.Reader(cluster.getFileSystem(), outlist[0].getPath(), conf);\r\n    TupleWritable v = new TupleWritable();\r\n    while (r.next(k, v)) {\r\n        assertFalse(((TupleWritable) v.get(1)).has(0));\r\n        assertFalse(((TupleWritable) v.get(1)).has(SOURCES + 1));\r\n        boolean chk = true;\r\n        int ki = k.get();\r\n        for (int i = 2; i < SOURCES + 2; ++i) {\r\n            if ((ki % i) == 0 && ki <= i * ITEMS) {\r\n                assertEquals(i - 2, ((IntWritable) ((TupleWritable) v.get(1)).get((i - 1))).get());\r\n            } else\r\n                chk = false;\r\n        }\r\n        if (chk) {\r\n            assertTrue(v.has(0));\r\n            for (int i = 0; i < SOURCES; ++i) assertTrue(((TupleWritable) v.get(0)).has(i));\r\n        } else {\r\n            assertFalse(v.has(0));\r\n        }\r\n    }\r\n    r.close();\r\n    base.getFileSystem(conf).delete(base, true);\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\join",
  "methodName" : "testEmptyJoin",
  "errType" : null,
  "containingMethodsNum" : 13,
  "sourceCodeText" : "void testEmptyJoin() throws Exception\n{\r\n    Configuration conf = new Configuration();\r\n    Path base = cluster.getFileSystem().makeQualified(new Path(\"/empty\"));\r\n    Path[] src = { new Path(base, \"i0\"), new Path(\"i1\"), new Path(\"i2\") };\r\n    conf.set(CompositeInputFormat.JOIN_EXPR, CompositeInputFormat.compose(\"outer\", MapReduceTestUtil.Fake_IF.class, src));\r\n    MapReduceTestUtil.Fake_IF.setKeyClass(conf, MapReduceTestUtil.IncomparableKey.class);\r\n    Job job = Job.getInstance(conf);\r\n    job.setInputFormatClass(CompositeInputFormat.class);\r\n    FileOutputFormat.setOutputPath(job, new Path(base, \"out\"));\r\n    job.setMapperClass(Mapper.class);\r\n    job.setReducerClass(Reducer.class);\r\n    job.setOutputKeyClass(MapReduceTestUtil.IncomparableKey.class);\r\n    job.setOutputValueClass(NullWritable.class);\r\n    job.waitForCompletion(true);\r\n    assertTrue(job.isSuccessful());\r\n    base.getFileSystem(conf).delete(base, true);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "getTestParameters",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Collection<Object[]> getTestParameters()\n{\r\n    return Arrays.asList(new Object[][] { { \"testSingleReducer\", 3, 1, false }, { \"testUberMode\", 3, 1, true }, { \"testMultipleMapsPerNode\", 8, 1, false }, { \"testMultipleReducers\", 2, 4, false } });\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "setupClass",
  "errType" : null,
  "containingMethodsNum" : 12,
  "sourceCodeText" : "void setupClass() throws Exception\n{\r\n    testRootDir = GenericTestUtils.setupTestRootDir(TestMRIntermediateDataEncryption.class);\r\n    final File dfsFolder = new File(testRootDir, \"dfs\");\r\n    final Path jobsDirPath = new Path(JOB_DIR_PATH);\r\n    commonConfig = createBaseConfiguration();\r\n    dfsCluster = new MiniDFSCluster.Builder(commonConfig, dfsFolder).numDataNodes(2).build();\r\n    dfsCluster.waitActive();\r\n    mrCluster = MiniMRClientClusterFactory.create(TestMRIntermediateDataEncryption.class, 2, commonConfig);\r\n    mrCluster.start();\r\n    fs = dfsCluster.getFileSystem();\r\n    if (fs.exists(jobsDirPath) && !fs.delete(jobsDirPath, true)) {\r\n        throw new IOException(\"Could not delete JobsDirPath\" + jobsDirPath);\r\n    }\r\n    fs.mkdirs(jobsDirPath);\r\n    jobInputDirPath = new Path(jobsDirPath, \"in-dir\");\r\n    Assert.assertEquals(\"Generating input should succeed\", 0, generateInputTextFile());\r\n    runReferenceJob();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "tearDown",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void tearDown() throws IOException\n{\r\n    if (mrCluster != null) {\r\n        mrCluster.stop();\r\n    }\r\n    if (dfsCluster != null) {\r\n        dfsCluster.shutdown();\r\n    }\r\n    final File textInputFile = new File(testRootDir, \"input.txt\");\r\n    if (textInputFile.exists()) {\r\n        Assert.assertTrue(textInputFile.delete());\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "createBaseConfiguration",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "Configuration createBaseConfiguration()\n{\r\n    Configuration conf = MRJobConfUtil.initEncryptedIntermediateConfigsForTesting(null);\r\n    conf = MRJobConfUtil.setLocalDirectoriesConfigForTesting(conf, testRootDir);\r\n    conf.setLong(\"dfs.blocksize\", BLOCK_SIZE_DEFAULT);\r\n    return conf;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "getTextInputWriter",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "BufferedWriter getTextInputWriter() throws IOException\n{\r\n    if (inputBufferedWriter == null) {\r\n        final File textInputFile = new File(testRootDir, \"input.txt\");\r\n        inputBufferedWriter = new BufferedWriter(new FileWriter(textInputFile));\r\n    }\r\n    return inputBufferedWriter;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "generateInputTextFile",
  "errType" : null,
  "containingMethodsNum" : 16,
  "sourceCodeText" : "int generateInputTextFile() throws Exception\n{\r\n    final File textInputFile = new File(testRootDir, \"input.txt\");\r\n    final AtomicLong actualWrittenBytes = new AtomicLong(0);\r\n    final ExecutorService executor = Executors.newFixedThreadPool(INPUT_GEN_NUM_THREADS);\r\n    final List<Future<Long>> inputGenerators = new ArrayList<>();\r\n    final Callable<Long> callableGen = new InputGeneratorTask();\r\n    final long startTime = Time.monotonicNow();\r\n    for (int i = 0; i < INPUT_GEN_NUM_THREADS; i++) {\r\n        Future<Long> genFutureTask = executor.submit(callableGen);\r\n        inputGenerators.add(genFutureTask);\r\n    }\r\n    for (Future<Long> genFutureTask : inputGenerators) {\r\n        LOG.info(\"Received one task. Current total bytes: {}\", actualWrittenBytes.addAndGet(genFutureTask.get()));\r\n    }\r\n    getTextInputWriter().close();\r\n    final long endTime = Time.monotonicNow();\r\n    LOG.info(\"Finished generating input. Wrote {} bytes in {} seconds\", actualWrittenBytes.get(), ((endTime - startTime) * 1.0) / 1000);\r\n    executor.shutdown();\r\n    fs.mkdirs(jobInputDirPath);\r\n    Path textInputPath = fs.makeQualified(new Path(jobInputDirPath, \"input.txt\"));\r\n    fs.copyFromLocalFile(true, new Path(textInputFile.getAbsolutePath()), textInputPath);\r\n    if (!fs.exists(textInputPath)) {\r\n        return 1;\r\n    }\r\n    FileStatus[] fileStatus = fs.listStatus(textInputPath);\r\n    inputFileSize = fileStatus[0].getLen();\r\n    LOG.info(\"Text input file; path: {}, size: {}\", textInputPath, inputFileSize);\r\n    return 0;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "runReferenceJob",
  "errType" : null,
  "containingMethodsNum" : 10,
  "sourceCodeText" : "void runReferenceJob() throws Exception\n{\r\n    final String jobRefLabel = \"job-reference\";\r\n    final Path jobRefDirPath = new Path(JOB_DIR_PATH, jobRefLabel);\r\n    if (fs.exists(jobRefDirPath) && !fs.delete(jobRefDirPath, true)) {\r\n        throw new IOException(\"Could not delete \" + jobRefDirPath);\r\n    }\r\n    Assert.assertTrue(fs.mkdirs(jobRefDirPath));\r\n    Path jobRefOutputPath = new Path(jobRefDirPath, \"out-dir\");\r\n    Configuration referenceConf = new Configuration(commonConfig);\r\n    referenceConf.setBoolean(MRJobConfig.MR_ENCRYPTED_INTERMEDIATE_DATA, false);\r\n    Job jobReference = runWordCountJob(jobRefLabel, jobRefOutputPath, referenceConf, 4, 1);\r\n    Assert.assertTrue(jobReference.isSuccessful());\r\n    FileStatus[] fileStatusArr = fs.listStatus(jobRefOutputPath, new Utils.OutputFileUtils.OutputFilesFilter());\r\n    Assert.assertEquals(1, fileStatusArr.length);\r\n    checkSumReference = fs.getFileChecksum(fileStatusArr[0].getPath());\r\n    Assert.assertTrue(fs.delete(jobRefDirPath, true));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "runWordCountJob",
  "errType" : null,
  "containingMethodsNum" : 17,
  "sourceCodeText" : "Job runWordCountJob(String postfixName, Path jOutputPath, Configuration jConf, int mappers, int reducers) throws Exception\n{\r\n    Job job = Job.getInstance(jConf);\r\n    job.getConfiguration().setInt(MRJobConfig.NUM_MAPS, mappers);\r\n    job.setJarByClass(TestMRIntermediateDataEncryption.class);\r\n    job.setJobName(\"mr-spill-\" + postfixName);\r\n    job.setMapperClass(TokenizerMapper.class);\r\n    job.setInputFormatClass(TextInputFormat.class);\r\n    job.setCombinerClass(LongSumReducer.class);\r\n    FileInputFormat.setMinInputSplitSize(job, (inputFileSize + mappers) / mappers);\r\n    job.setReducerClass(LongSumReducer.class);\r\n    job.setNumReduceTasks(reducers);\r\n    job.setOutputKeyClass(Text.class);\r\n    job.setOutputValueClass(LongWritable.class);\r\n    FileInputFormat.addInputPath(job, jobInputDirPath);\r\n    FileOutputFormat.setOutputPath(job, jOutputPath);\r\n    if (job.waitForCompletion(true)) {\r\n        FileStatus[] fileStatusArr = fs.listStatus(jOutputPath, new Utils.OutputFileUtils.OutputFilesFilter());\r\n        for (FileStatus fStatus : fileStatusArr) {\r\n            LOG.info(\"Job: {} .. Output file {} .. Size = {}\", postfixName, fStatus.getPath(), fStatus.getLen());\r\n        }\r\n    }\r\n    return job;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "validateJobOutput",
  "errType" : null,
  "containingMethodsNum" : 21,
  "sourceCodeText" : "boolean validateJobOutput() throws Exception\n{\r\n    Assert.assertTrue(\"Job Output path [\" + jobOutputPath + \"] should exist\", fs.exists(jobOutputPath));\r\n    Path outputPath = jobOutputPath;\r\n    if (numReducers != 1) {\r\n        final String jobRefLabel = testTitleName + \"-combine\";\r\n        final Path jobRefDirPath = new Path(JOB_DIR_PATH, jobRefLabel);\r\n        if (fs.exists(jobRefDirPath) && !fs.delete(jobRefDirPath, true)) {\r\n            throw new IOException(\"Could not delete \" + jobRefDirPath);\r\n        }\r\n        fs.mkdirs(jobRefDirPath);\r\n        outputPath = new Path(jobRefDirPath, \"out-dir\");\r\n        Configuration referenceConf = new Configuration(commonConfig);\r\n        referenceConf.setBoolean(MRJobConfig.MR_ENCRYPTED_INTERMEDIATE_DATA, false);\r\n        Job combinerJob = Job.getInstance(referenceConf);\r\n        combinerJob.setJarByClass(TestMRIntermediateDataEncryption.class);\r\n        combinerJob.setJobName(\"mr-spill-\" + jobRefLabel);\r\n        combinerJob.setMapperClass(CombinerJobMapper.class);\r\n        FileInputFormat.addInputPath(combinerJob, jobOutputPath);\r\n        combinerJob.setReducerClass(LongSumReducer.class);\r\n        combinerJob.setNumReduceTasks(1);\r\n        combinerJob.setOutputKeyClass(Text.class);\r\n        combinerJob.setOutputValueClass(LongWritable.class);\r\n        FileOutputFormat.setOutputPath(combinerJob, outputPath);\r\n        if (!combinerJob.waitForCompletion(true)) {\r\n            return false;\r\n        }\r\n        FileStatus[] fileStatusArr = fs.listStatus(outputPath, new Utils.OutputFileUtils.OutputFilesFilter());\r\n        LOG.info(\"Job-Combination: {} .. Output file {} .. Size = {}\", jobRefDirPath, fileStatusArr[0].getPath(), fileStatusArr[0].getLen());\r\n    }\r\n    FileStatus[] fileStatusArr = fs.listStatus(outputPath, new Utils.OutputFileUtils.OutputFilesFilter());\r\n    FileChecksum jobFileChecksum = fs.getFileChecksum(fileStatusArr[0].getPath());\r\n    return checkSumReference.equals(jobFileChecksum);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "setup",
  "errType" : null,
  "containingMethodsNum" : 13,
  "sourceCodeText" : "void setup() throws Exception\n{\r\n    LOG.info(\"Starting TestMRIntermediateDataEncryption#{}.......\", testTitleName);\r\n    final Path jobDirPath = new Path(JOB_DIR_PATH, testTitleName);\r\n    if (fs.exists(jobDirPath) && !fs.delete(jobDirPath, true)) {\r\n        throw new IOException(\"Could not delete \" + jobDirPath);\r\n    }\r\n    fs.mkdirs(jobDirPath);\r\n    jobOutputPath = new Path(jobDirPath, \"out-dir\");\r\n    config = new Configuration(commonConfig);\r\n    config.setBoolean(MRJobConfig.JOB_UBERTASK_ENABLE, isUber);\r\n    config.setFloat(MRJobConfig.COMPLETED_MAPS_FOR_REDUCE_SLOWSTART, 1.0F);\r\n    long ioSortMb = TASK_SORT_IO_MB_DEFAULT;\r\n    config.setLong(MRJobConfig.IO_SORT_MB, ioSortMb);\r\n    long mapMb = Math.max(2 * ioSortMb, config.getInt(MRJobConfig.MAP_MEMORY_MB, MRJobConfig.DEFAULT_MAP_MEMORY_MB));\r\n    config.setLong(MRJobConfig.MAP_MEMORY_MB, mapMb);\r\n    config.set(MRJobConfig.MAP_JAVA_OPTS, \"-Xmx\" + (mapMb - 200) + \"m\");\r\n    config.setInt(MRJobConfig.NUM_MAPS, numMappers);\r\n    config.setInt(\"mapreduce.map.maxattempts\", 1);\r\n    config.setInt(\"mapreduce.reduce.maxattempts\", 1);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "testWordCount",
  "errType" : null,
  "containingMethodsNum" : 21,
  "sourceCodeText" : "void testWordCount() throws Exception\n{\r\n    LOG.info(\"........Starting main Job Driver #{} starting at {}.......\", testTitleName, Time.formatTime(System.currentTimeMillis()));\r\n    SpillCallBackPathsFinder spillInjector = (SpillCallBackPathsFinder) IntermediateEncryptedStream.setSpillCBInjector(new SpillCallBackPathsFinder());\r\n    StringBuilder testSummary = new StringBuilder(String.format(\"%n ===== test %s summary ======\", testTitleName));\r\n    try {\r\n        long startTime = Time.monotonicNow();\r\n        testSummary.append(String.format(\"%nJob %s started at %s\", testTitleName, Time.formatTime(System.currentTimeMillis())));\r\n        Job job = runWordCountJob(testTitleName, jobOutputPath, config, numMappers, numReducers);\r\n        Assert.assertTrue(job.isSuccessful());\r\n        long endTime = Time.monotonicNow();\r\n        testSummary.append(String.format(\"%nJob %s ended at %s\", job.getJobName(), Time.formatTime(System.currentTimeMillis())));\r\n        testSummary.append(String.format(\"%n\\tThe job took %.3f seconds\", (1.0 * (endTime - startTime)) / 1000));\r\n        FileStatus[] fileStatusArr = fs.listStatus(jobOutputPath, new Utils.OutputFileUtils.OutputFilesFilter());\r\n        for (FileStatus fStatus : fileStatusArr) {\r\n            long fileSize = fStatus.getLen();\r\n            testSummary.append(String.format(\"%n\\tOutput file %s: %d\", fStatus.getPath(), fileSize));\r\n        }\r\n        Assert.assertTrue(validateJobOutput());\r\n        long spilledRecords = job.getCounters().findCounter(TaskCounter.SPILLED_RECORDS).getValue();\r\n        Assert.assertTrue(\"Spill records must be greater than 0\", spilledRecords > 0);\r\n        Assert.assertFalse(\"The encrypted spilled files should not be empty.\", spillInjector.getEncryptedSpilledFiles().isEmpty());\r\n        Assert.assertTrue(\"Invalid access to spill file positions\", spillInjector.getInvalidSpillEntries().isEmpty());\r\n    } finally {\r\n        testSummary.append(spillInjector.getSpilledFileReport());\r\n        LOG.info(testSummary.toString());\r\n        IntermediateEncryptedStream.resetSpillCBInjector();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "createInputFile",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void createInputFile(Configuration conf) throws IOException\n{\r\n    FileSystem localFs = FileSystem.getLocal(conf);\r\n    Path file = new Path(inputDir, \"test.txt\");\r\n    Writer writer = new OutputStreamWriter(localFs.create(file));\r\n    writer.write(\"abc\\ndef\\t\\nghi\\njkl\");\r\n    writer.close();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "readOutputFile",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "String readOutputFile(Configuration conf) throws IOException\n{\r\n    FileSystem localFs = FileSystem.getLocal(conf);\r\n    Path file = new Path(outputDir, \"part-00000\");\r\n    return UtilsForTests.slurpHadoop(file, localFs);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "createAndRunJob",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void createAndRunJob(Configuration conf) throws IOException, InterruptedException, ClassNotFoundException\n{\r\n    JobConf job = new JobConf(conf);\r\n    job.setJarByClass(TestLineRecordReaderJobs.class);\r\n    job.setMapperClass(IdentityMapper.class);\r\n    job.setReducerClass(IdentityReducer.class);\r\n    FileInputFormat.addInputPath(job, inputDir);\r\n    FileOutputFormat.setOutputPath(job, outputDir);\r\n    JobClient.runJob(job);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "testCustomRecordDelimiters",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void testCustomRecordDelimiters() throws IOException, InterruptedException, ClassNotFoundException\n{\r\n    Configuration conf = new Configuration();\r\n    conf.set(\"textinputformat.record.delimiter\", \"\\t\\n\");\r\n    conf.setInt(\"mapreduce.job.maps\", 1);\r\n    FileSystem localFs = FileSystem.getLocal(conf);\r\n    localFs.delete(workDir, true);\r\n    createInputFile(conf);\r\n    createAndRunJob(conf);\r\n    String expected = \"0\\tabc\\ndef\\n9\\tghi\\njkl\\n\";\r\n    assertEquals(expected, readOutputFile(conf));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "testDefaultRecordDelimiters",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void testDefaultRecordDelimiters() throws IOException, InterruptedException, ClassNotFoundException\n{\r\n    Configuration conf = new Configuration();\r\n    FileSystem localFs = FileSystem.getLocal(conf);\r\n    localFs.delete(workDir, true);\r\n    createInputFile(conf);\r\n    createAndRunJob(conf);\r\n    String expected = \"0\\tabc\\n4\\tdef\\t\\n9\\tghi\\n13\\tjkl\\n\";\r\n    assertEquals(expected, readOutputFile(conf));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred\\jobcontrol",
  "methodName" : "cleanData",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void cleanData(FileSystem fs, Path dirPath) throws IOException\n{\r\n    fs.delete(dirPath, true);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred\\jobcontrol",
  "methodName" : "generateRandomWord",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String generateRandomWord()\n{\r\n    return idFormat.format(rand.nextLong());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred\\jobcontrol",
  "methodName" : "generateRandomLine",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "String generateRandomLine()\n{\r\n    long r = rand.nextLong() % 7;\r\n    long n = r + 20;\r\n    StringBuffer sb = new StringBuffer();\r\n    for (int i = 0; i < n; i++) {\r\n        sb.append(generateRandomWord()).append(\" \");\r\n    }\r\n    sb.append(\"\\n\");\r\n    return sb.toString();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred\\jobcontrol",
  "methodName" : "generateData",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void generateData(FileSystem fs, Path dirPath) throws IOException\n{\r\n    FSDataOutputStream out = fs.create(new Path(dirPath, \"data.txt\"));\r\n    for (int i = 0; i < 10000; i++) {\r\n        String line = generateRandomLine();\r\n        out.write(line.getBytes(\"UTF-8\"));\r\n    }\r\n    out.close();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred\\jobcontrol",
  "methodName" : "createCopyJob",
  "errType" : null,
  "containingMethodsNum" : 9,
  "sourceCodeText" : "JobConf createCopyJob(List<Path> indirs, Path outdir) throws Exception\n{\r\n    Configuration defaults = new Configuration();\r\n    JobConf theJob = new JobConf(defaults, TestJobControl.class);\r\n    theJob.setJobName(\"DataMoveJob\");\r\n    FileInputFormat.setInputPaths(theJob, indirs.toArray(new Path[0]));\r\n    theJob.setMapperClass(DataCopy.class);\r\n    FileOutputFormat.setOutputPath(theJob, outdir);\r\n    theJob.setOutputKeyClass(Text.class);\r\n    theJob.setOutputValueClass(Text.class);\r\n    theJob.setReducerClass(DataCopy.class);\r\n    theJob.setNumMapTasks(12);\r\n    theJob.setNumReduceTasks(4);\r\n    return theJob;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\fs\\slive",
  "methodName" : "verifyBuffer",
  "errType" : null,
  "containingMethodsNum" : 21,
  "sourceCodeText" : "VerifyInfo verifyBuffer(ByteBuffer buf, int size, long startOffset, DataHasher hasher)\n{\r\n    ByteBuffer cmpBuf = ByteBuffer.wrap(new byte[BYTES_PER_LONG]);\r\n    long hashOffset = startOffset;\r\n    long chunksSame = 0;\r\n    long chunksDifferent = 0;\r\n    for (long i = 0; i < size; ++i) {\r\n        cmpBuf.put(buf.get());\r\n        if (!cmpBuf.hasRemaining()) {\r\n            cmpBuf.rewind();\r\n            long receivedData = cmpBuf.getLong();\r\n            cmpBuf.rewind();\r\n            long expected = hasher.generate(hashOffset);\r\n            hashOffset += BYTES_PER_LONG;\r\n            if (receivedData == expected) {\r\n                ++chunksSame;\r\n            } else {\r\n                ++chunksDifferent;\r\n            }\r\n        }\r\n    }\r\n    if (cmpBuf.hasRemaining() && cmpBuf.position() != 0) {\r\n        int curSize = cmpBuf.position();\r\n        while (cmpBuf.hasRemaining()) {\r\n            cmpBuf.put((byte) 0);\r\n        }\r\n        long expected = hasher.generate(hashOffset);\r\n        ByteBuffer tempBuf = ByteBuffer.wrap(new byte[BYTES_PER_LONG]);\r\n        tempBuf.putLong(expected);\r\n        tempBuf.position(curSize);\r\n        while (tempBuf.hasRemaining()) {\r\n            tempBuf.put((byte) 0);\r\n        }\r\n        cmpBuf.rewind();\r\n        tempBuf.rewind();\r\n        if (cmpBuf.equals(tempBuf)) {\r\n            ++chunksSame;\r\n        } else {\r\n            ++chunksDifferent;\r\n        }\r\n    }\r\n    return new VerifyInfo(chunksSame, chunksDifferent);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\fs\\slive",
  "methodName" : "determineOffset",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long determineOffset(long byteRead)\n{\r\n    if (byteRead < 0) {\r\n        byteRead = 0;\r\n    }\r\n    return (byteRead / BYTES_PER_LONG) * BYTES_PER_LONG;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\fs\\slive",
  "methodName" : "verifyFile",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "VerifyOutput verifyFile(long byteAm, DataInputStream in) throws IOException, BadFileException\n{\r\n    return verifyBytes(byteAm, 0, in);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\fs\\slive",
  "methodName" : "verifyBytes",
  "errType" : [ "EOFException", "EOFException" ],
  "containingMethodsNum" : 17,
  "sourceCodeText" : "VerifyOutput verifyBytes(long byteAm, long bytesRead, DataInputStream in) throws IOException, BadFileException\n{\r\n    if (byteAm <= 0) {\r\n        return new VerifyOutput(0, 0, 0, 0);\r\n    }\r\n    long chunksSame = 0;\r\n    long chunksDifferent = 0;\r\n    long readTime = 0;\r\n    long bytesLeft = byteAm;\r\n    long bufLeft = 0;\r\n    long bufRead = 0;\r\n    long seqNum = 0;\r\n    DataHasher hasher = null;\r\n    ByteBuffer readBuf = ByteBuffer.wrap(new byte[bufferSize]);\r\n    while (bytesLeft > 0) {\r\n        if (bufLeft <= 0) {\r\n            if (bytesLeft < DataWriter.getHeaderLength()) {\r\n                break;\r\n            }\r\n            ReadInfo header = null;\r\n            try {\r\n                header = readHeader(in);\r\n            } catch (EOFException e) {\r\n                break;\r\n            }\r\n            ++seqNum;\r\n            hasher = new DataHasher(header.getHashValue());\r\n            bufLeft = header.getByteAm();\r\n            readTime += header.getTimeTaken();\r\n            bytesRead += header.getBytesRead();\r\n            bytesLeft -= header.getBytesRead();\r\n            bufRead = 0;\r\n            if (bufLeft > bytesLeft) {\r\n                bufLeft = bytesLeft;\r\n            }\r\n            if (bufLeft <= 0) {\r\n                continue;\r\n            }\r\n        }\r\n        int bufSize = bufferSize;\r\n        if (bytesLeft < bufSize) {\r\n            bufSize = (int) bytesLeft;\r\n        }\r\n        if (bufLeft < bufSize) {\r\n            bufSize = (int) bufLeft;\r\n        }\r\n        try {\r\n            readBuf.rewind();\r\n            long startTime = Timer.now();\r\n            in.readFully(readBuf.array(), 0, bufSize);\r\n            readTime += Timer.elapsed(startTime);\r\n        } catch (EOFException e) {\r\n            throw new BadFileException(\"Could not read the number of expected data bytes \" + bufSize + \" due to unexpected end of file during sequence \" + seqNum, e);\r\n        }\r\n        bytesRead += bufSize;\r\n        bytesLeft -= bufSize;\r\n        bufLeft -= bufSize;\r\n        readBuf.rewind();\r\n        long vOffset = determineOffset(bufRead);\r\n        bufRead += bufSize;\r\n        VerifyInfo verifyRes = verifyBuffer(readBuf, bufSize, vOffset, hasher);\r\n        chunksSame += verifyRes.getSame();\r\n        chunksDifferent += verifyRes.getDifferent();\r\n    }\r\n    return new VerifyOutput(chunksSame, chunksDifferent, bytesRead, readTime);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 2,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\fs\\slive",
  "methodName" : "readHeader",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "ReadInfo readHeader(DataInputStream in) throws IOException, BadFileException\n{\r\n    int headerLen = DataWriter.getHeaderLength();\r\n    ByteBuffer headerBuf = ByteBuffer.wrap(new byte[headerLen]);\r\n    long elapsed = 0;\r\n    {\r\n        long startTime = Timer.now();\r\n        in.readFully(headerBuf.array());\r\n        elapsed += Timer.elapsed(startTime);\r\n    }\r\n    headerBuf.rewind();\r\n    long hashValue = headerBuf.getLong();\r\n    long byteAvailable = headerBuf.getLong();\r\n    if (byteAvailable < 0) {\r\n        throw new BadFileException(\"Invalid negative amount \" + byteAvailable + \" determined for header data amount\");\r\n    }\r\n    return new ReadInfo(byteAvailable, hashValue, elapsed, headerLen);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "setup",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void setup() throws Exception\n{\r\n    fs = FileSystem.getLocal(new Configuration());\r\n    fs.delete(testRootTempDir, true);\r\n    fs.mkdirs(testRootTempDir);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "cleanup",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void cleanup() throws Exception\n{\r\n    fs.delete(testRootTempDir, true);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "testReporterProgressForMapOnlyJob",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void testReporterProgressForMapOnlyJob() throws IOException\n{\r\n    Path test = new Path(testRootTempDir, \"testReporterProgressForMapOnlyJob\");\r\n    JobConf conf = new JobConf();\r\n    conf.setMapperClass(ProgressTesterMapper.class);\r\n    conf.setMapOutputKeyClass(Text.class);\r\n    conf.setMaxMapAttempts(1);\r\n    conf.setMaxReduceAttempts(0);\r\n    RunningJob job = UtilsForTests.runJob(conf, new Path(test, \"in\"), new Path(test, \"out\"), 1, 0, INPUT);\r\n    job.waitForCompletion();\r\n    assertTrue(\"Job failed\", job.isSuccessful());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "testReporterProgressForMRJob",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void testReporterProgressForMRJob() throws IOException\n{\r\n    Path test = new Path(testRootTempDir, \"testReporterProgressForMRJob\");\r\n    JobConf conf = new JobConf();\r\n    conf.setMapperClass(ProgressTesterMapper.class);\r\n    conf.setReducerClass(ProgressTestingReducer.class);\r\n    conf.setMapOutputKeyClass(Text.class);\r\n    conf.setMaxMapAttempts(1);\r\n    conf.setMaxReduceAttempts(1);\r\n    RunningJob job = UtilsForTests.runJob(conf, new Path(test, \"in\"), new Path(test, \"out\"), 1, 1, INPUT);\r\n    job.waitForCompletion();\r\n    assertTrue(\"Job failed\", job.isSuccessful());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "testStatusLimit",
  "errType" : null,
  "containingMethodsNum" : 16,
  "sourceCodeText" : "void testStatusLimit() throws IOException, InterruptedException, ClassNotFoundException\n{\r\n    Path test = new Path(testRootTempDir, \"testStatusLimit\");\r\n    Configuration conf = new Configuration();\r\n    Path inDir = new Path(test, \"in\");\r\n    Path outDir = new Path(test, \"out\");\r\n    FileSystem fs = FileSystem.get(conf);\r\n    if (fs.exists(inDir)) {\r\n        fs.delete(inDir, true);\r\n    }\r\n    fs.mkdirs(inDir);\r\n    DataOutputStream file = fs.create(new Path(inDir, \"part-\" + 0));\r\n    file.writeBytes(\"testStatusLimit\");\r\n    file.close();\r\n    if (fs.exists(outDir)) {\r\n        fs.delete(outDir, true);\r\n    }\r\n    Job job = Job.getInstance(conf, \"testStatusLimit\");\r\n    job.setMapperClass(StatusLimitMapper.class);\r\n    job.setNumReduceTasks(0);\r\n    FileInputFormat.addInputPath(job, inDir);\r\n    FileOutputFormat.setOutputPath(job, outDir);\r\n    job.waitForCompletion(true);\r\n    assertTrue(\"Job failed\", job.isSuccessful());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "testInputPath",
  "errType" : null,
  "containingMethodsNum" : 60,
  "sourceCodeText" : "void testInputPath() throws Exception\n{\r\n    JobConf jobConf = new JobConf();\r\n    Path workingDir = jobConf.getWorkingDirectory();\r\n    Path path = new Path(workingDir, \"xx{y\" + StringUtils.COMMA_STR + \"z}\");\r\n    FileInputFormat.setInputPaths(jobConf, path);\r\n    Path[] paths = FileInputFormat.getInputPaths(jobConf);\r\n    assertEquals(1, paths.length);\r\n    assertEquals(path.toString(), paths[0].toString());\r\n    StringBuilder pathStr = new StringBuilder();\r\n    pathStr.append(StringUtils.ESCAPE_CHAR);\r\n    pathStr.append(StringUtils.ESCAPE_CHAR);\r\n    pathStr.append(StringUtils.COMMA);\r\n    pathStr.append(StringUtils.COMMA);\r\n    pathStr.append('a');\r\n    path = new Path(workingDir, pathStr.toString());\r\n    FileInputFormat.setInputPaths(jobConf, path);\r\n    paths = FileInputFormat.getInputPaths(jobConf);\r\n    assertEquals(1, paths.length);\r\n    assertEquals(path.toString(), paths[0].toString());\r\n    pathStr.setLength(0);\r\n    pathStr.append(StringUtils.ESCAPE_CHAR);\r\n    pathStr.append(\"xx\");\r\n    pathStr.append(StringUtils.ESCAPE_CHAR);\r\n    path = new Path(workingDir, pathStr.toString());\r\n    Path path1 = new Path(workingDir, \"yy\" + StringUtils.COMMA_STR + \"zz\");\r\n    FileInputFormat.setInputPaths(jobConf, path);\r\n    FileInputFormat.addInputPath(jobConf, path1);\r\n    paths = FileInputFormat.getInputPaths(jobConf);\r\n    assertEquals(2, paths.length);\r\n    assertEquals(path.toString(), paths[0].toString());\r\n    assertEquals(path1.toString(), paths[1].toString());\r\n    FileInputFormat.setInputPaths(jobConf, path, path1);\r\n    paths = FileInputFormat.getInputPaths(jobConf);\r\n    assertEquals(2, paths.length);\r\n    assertEquals(path.toString(), paths[0].toString());\r\n    assertEquals(path1.toString(), paths[1].toString());\r\n    Path[] input = new Path[] { path, path1 };\r\n    FileInputFormat.setInputPaths(jobConf, input);\r\n    paths = FileInputFormat.getInputPaths(jobConf);\r\n    assertEquals(2, paths.length);\r\n    assertEquals(path.toString(), paths[0].toString());\r\n    assertEquals(path1.toString(), paths[1].toString());\r\n    pathStr.setLength(0);\r\n    String str1 = \"{a{b,c},de}\";\r\n    String str2 = \"xyz\";\r\n    String str3 = \"x{y,z}\";\r\n    pathStr.append(str1);\r\n    pathStr.append(StringUtils.COMMA);\r\n    pathStr.append(str2);\r\n    pathStr.append(StringUtils.COMMA);\r\n    pathStr.append(str3);\r\n    FileInputFormat.setInputPaths(jobConf, pathStr.toString());\r\n    paths = FileInputFormat.getInputPaths(jobConf);\r\n    assertEquals(3, paths.length);\r\n    assertEquals(new Path(workingDir, str1).toString(), paths[0].toString());\r\n    assertEquals(new Path(workingDir, str2).toString(), paths[1].toString());\r\n    assertEquals(new Path(workingDir, str3).toString(), paths[2].toString());\r\n    pathStr.setLength(0);\r\n    String str4 = \"abc\";\r\n    String str5 = \"pq{r,s}\";\r\n    pathStr.append(str4);\r\n    pathStr.append(StringUtils.COMMA);\r\n    pathStr.append(str5);\r\n    FileInputFormat.addInputPaths(jobConf, pathStr.toString());\r\n    paths = FileInputFormat.getInputPaths(jobConf);\r\n    assertEquals(5, paths.length);\r\n    assertEquals(new Path(workingDir, str1).toString(), paths[0].toString());\r\n    assertEquals(new Path(workingDir, str2).toString(), paths[1].toString());\r\n    assertEquals(new Path(workingDir, str3).toString(), paths[2].toString());\r\n    assertEquals(new Path(workingDir, str4).toString(), paths[3].toString());\r\n    assertEquals(new Path(workingDir, str5).toString(), paths[4].toString());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "map",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void map(IntWritable key, IntWritable val, Context context) throws IOException\n{\r\n    Configuration tlConf = new YarnConfiguration();\r\n    TimelineCollectorManager manager = new TimelineCollectorManager(\"test\");\r\n    manager.init(tlConf);\r\n    manager.start();\r\n    try {\r\n        writeEntities(tlConf, manager, context);\r\n    } finally {\r\n        manager.close();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "writeEntities",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void writeEntities(Configuration tlConf, TimelineCollectorManager manager, Context context) throws IOException",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\fs\\slive",
  "methodName" : "getDeleteFile",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Path getDeleteFile()\n{\r\n    Path fn = getFinder().getFile();\r\n    return fn;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\fs\\slive",
  "methodName" : "run",
  "errType" : [ "FileNotFoundException", "IOException" ],
  "containingMethodsNum" : 14,
  "sourceCodeText" : "List<OperationOutput> run(FileSystem fs)\n{\r\n    List<OperationOutput> out = super.run(fs);\r\n    try {\r\n        Path fn = getDeleteFile();\r\n        long timeTaken = 0;\r\n        boolean deleteStatus = false;\r\n        {\r\n            long startTime = Timer.now();\r\n            deleteStatus = fs.delete(fn, false);\r\n            timeTaken = Timer.elapsed(startTime);\r\n        }\r\n        if (!deleteStatus) {\r\n            out.add(new OperationOutput(OutputType.LONG, getType(), ReportWriter.FAILURES, 1L));\r\n            LOG.info(\"Could not delete \" + fn);\r\n        } else {\r\n            out.add(new OperationOutput(OutputType.LONG, getType(), ReportWriter.OK_TIME_TAKEN, timeTaken));\r\n            out.add(new OperationOutput(OutputType.LONG, getType(), ReportWriter.SUCCESSES, 1L));\r\n            LOG.info(\"Could delete \" + fn);\r\n        }\r\n    } catch (FileNotFoundException e) {\r\n        out.add(new OperationOutput(OutputType.LONG, getType(), ReportWriter.NOT_FOUND, 1L));\r\n        LOG.warn(\"Error with deleting\", e);\r\n    } catch (IOException e) {\r\n        out.add(new OperationOutput(OutputType.LONG, getType(), ReportWriter.FAILURES, 1L));\r\n        LOG.warn(\"Error with deleting\", e);\r\n    }\r\n    return out;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "writeData",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void writeData(RecordWriter<Text, Text> rw) throws IOException\n{\r\n    for (int i = 10; i < 40; i++) {\r\n        String k = \"\" + i;\r\n        String v = \"\" + i;\r\n        rw.write(new Text(k), new Text(v));\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "test1",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void test1(JobConf job) throws IOException\n{\r\n    FileSystem fs = FileSystem.getLocal(job);\r\n    String name = \"part-00000\";\r\n    KeyBasedMultipleTextOutputFormat theOutputFormat = new KeyBasedMultipleTextOutputFormat();\r\n    RecordWriter<Text, Text> rw = theOutputFormat.getRecordWriter(fs, job, name, null);\r\n    writeData(rw);\r\n    rw.close(null);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "test2",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void test2(JobConf job) throws IOException\n{\r\n    FileSystem fs = FileSystem.getLocal(job);\r\n    String name = \"part-00000\";\r\n    job.set(JobContext.MAP_INPUT_FILE, \"1/2/3\");\r\n    job.set(\"mapred.outputformat.numOfTrailingLegs\", \"2\");\r\n    MultipleTextOutputFormat<Text, Text> theOutputFormat = new MultipleTextOutputFormat<Text, Text>();\r\n    RecordWriter<Text, Text> rw = theOutputFormat.getRecordWriter(fs, job, name, null);\r\n    writeData(rw);\r\n    rw.close(null);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "testFormat",
  "errType" : null,
  "containingMethodsNum" : 24,
  "sourceCodeText" : "void testFormat() throws Exception\n{\r\n    JobConf job = new JobConf();\r\n    job.set(JobContext.TASK_ATTEMPT_ID, attempt);\r\n    FileOutputFormat.setOutputPath(job, workDir.getParent().getParent());\r\n    FileOutputFormat.setWorkOutputPath(job, workDir);\r\n    FileSystem fs = workDir.getFileSystem(job);\r\n    if (!fs.mkdirs(workDir)) {\r\n        fail(\"Failed to create output directory\");\r\n    }\r\n    TestMultipleTextOutputFormat.test1(job);\r\n    TestMultipleTextOutputFormat.test2(job);\r\n    String file_11 = \"1-part-00000\";\r\n    File expectedFile_11 = new File(new Path(workDir, file_11).toString());\r\n    StringBuffer expectedOutput = new StringBuffer();\r\n    for (int i = 10; i < 20; i++) {\r\n        expectedOutput.append(\"\" + i).append('\\t').append(\"\" + i).append(\"\\n\");\r\n    }\r\n    String output = UtilsForTests.slurp(expectedFile_11);\r\n    assertThat(output).isEqualTo(expectedOutput.toString());\r\n    String file_12 = \"2-part-00000\";\r\n    File expectedFile_12 = new File(new Path(workDir, file_12).toString());\r\n    expectedOutput = new StringBuffer();\r\n    for (int i = 20; i < 30; i++) {\r\n        expectedOutput.append(\"\" + i).append('\\t').append(\"\" + i).append(\"\\n\");\r\n    }\r\n    output = UtilsForTests.slurp(expectedFile_12);\r\n    assertThat(output).isEqualTo(expectedOutput.toString());\r\n    String file_13 = \"3-part-00000\";\r\n    File expectedFile_13 = new File(new Path(workDir, file_13).toString());\r\n    expectedOutput = new StringBuffer();\r\n    for (int i = 30; i < 40; i++) {\r\n        expectedOutput.append(\"\" + i).append('\\t').append(\"\" + i).append(\"\\n\");\r\n    }\r\n    output = UtilsForTests.slurp(expectedFile_13);\r\n    assertThat(output).isEqualTo(expectedOutput.toString());\r\n    String file_2 = \"2/3\";\r\n    File expectedFile_2 = new File(new Path(workDir, file_2).toString());\r\n    expectedOutput = new StringBuffer();\r\n    for (int i = 10; i < 40; i++) {\r\n        expectedOutput.append(\"\" + i).append('\\t').append(\"\" + i).append(\"\\n\");\r\n    }\r\n    output = UtilsForTests.slurp(expectedFile_2);\r\n    assertThat(output).isEqualTo(expectedOutput.toString());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "testQueueConfigurationParser",
  "errType" : null,
  "containingMethodsNum" : 11,
  "sourceCodeText" : "void testQueueConfigurationParser() throws ParserConfigurationException, Exception\n{\r\n    JobQueueInfo info = new JobQueueInfo(\"root\", \"rootInfo\");\r\n    JobQueueInfo infoChild1 = new JobQueueInfo(\"child1\", \"child1Info\");\r\n    JobQueueInfo infoChild2 = new JobQueueInfo(\"child2\", \"child1Info\");\r\n    info.addChild(infoChild1);\r\n    info.addChild(infoChild2);\r\n    DocumentBuilderFactory docBuilderFactory = DocumentBuilderFactory.newInstance();\r\n    DocumentBuilder builder = docBuilderFactory.newDocumentBuilder();\r\n    Document document = builder.newDocument();\r\n    Element e = QueueConfigurationParser.getQueueElement(document, info);\r\n    DOMSource domSource = new DOMSource(e);\r\n    StringWriter writer = new StringWriter();\r\n    StreamResult result = new StreamResult(writer);\r\n    TransformerFactory tf = TransformerFactory.newInstance();\r\n    Transformer transformer = tf.newTransformer();\r\n    transformer.transform(domSource, result);\r\n    String str = writer.toString();\r\n    assertTrue(str.endsWith(\"<queue><name>root</name><properties/><state>running</state><queue><name>child1</name><properties/><state>running</state></queue><queue><name>child2</name><properties/><state>running</state></queue></queue>\"));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "testReadWrite",
  "errType" : null,
  "containingMethodsNum" : 10,
  "sourceCodeText" : "void testReadWrite() throws Exception\n{\r\n    MultiFileSplit split = new MultiFileSplit(new JobConf(), new Path[] { new Path(\"/test/path/1\"), new Path(\"/test/path/2\") }, new long[] { 100, 200 });\r\n    ByteArrayOutputStream bos = null;\r\n    byte[] result = null;\r\n    try {\r\n        bos = new ByteArrayOutputStream();\r\n        split.write(new DataOutputStream(bos));\r\n        result = bos.toByteArray();\r\n    } finally {\r\n        IOUtils.closeStream(bos);\r\n    }\r\n    MultiFileSplit readSplit = new MultiFileSplit();\r\n    ByteArrayInputStream bis = null;\r\n    try {\r\n        bis = new ByteArrayInputStream(result);\r\n        readSplit.readFields(new DataInputStream(bis));\r\n    } finally {\r\n        IOUtils.closeStream(bis);\r\n    }\r\n    assertTrue(split.getLength() != 0);\r\n    assertEquals(split.getLength(), readSplit.getLength());\r\n    assertThat(readSplit.getPaths()).containsExactly(split.getPaths());\r\n    assertThat(readSplit.getLengths()).containsExactly(split.getLengths());\r\n    System.out.println(split.toString());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 2,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "testgetLocations",
  "errType" : null,
  "containingMethodsNum" : 9,
  "sourceCodeText" : "void testgetLocations() throws IOException\n{\r\n    JobConf job = new JobConf();\r\n    File tmpFile = File.createTempFile(\"test\", \"txt\");\r\n    tmpFile.createNewFile();\r\n    OutputStream out = new FileOutputStream(tmpFile);\r\n    out.write(\"tempfile\".getBytes());\r\n    out.flush();\r\n    out.close();\r\n    Path[] path = { new Path(tmpFile.getAbsolutePath()) };\r\n    long[] lengths = { 100 };\r\n    MultiFileSplit split = new MultiFileSplit(job, path, lengths);\r\n    String[] locations = split.getLocations();\r\n    assertThat(locations.length).isOne();\r\n    assertThat(locations[0]).isEqualTo(\"localhost\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "tearDown",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void tearDown() throws Exception\n{\r\n    getFileSystem().delete(new Path(BASE_DIR), true);\r\n    getFileSystem().delete(new Path(NNBench.DEFAULT_RES_FILE_NAME), true);\r\n    super.tearDown();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "testNNBenchCreateReadAndDelete",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void testNNBenchCreateReadAndDelete() throws Exception\n{\r\n    runNNBench(createJobConf(), \"create_write\");\r\n    Path path = new Path(BASE_DIR + \"/data/file_0_0\");\r\n    assertTrue(\"create_write should create the file\", getFileSystem().exists(path));\r\n    runNNBench(createJobConf(), \"open_read\");\r\n    runNNBench(createJobConf(), \"delete\");\r\n    assertFalse(\"Delete operation should delete the file\", getFileSystem().exists(path));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "testNNBenchCreateAndRename",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void testNNBenchCreateAndRename() throws Exception\n{\r\n    runNNBench(createJobConf(), \"create_write\");\r\n    Path path = new Path(BASE_DIR + \"/data/file_0_0\");\r\n    assertTrue(\"create_write should create the file\", getFileSystem().exists(path));\r\n    runNNBench(createJobConf(), \"rename\");\r\n    Path renamedPath = new Path(BASE_DIR + \"/data/file_0_r_0\");\r\n    assertFalse(\"Rename should rename the file\", getFileSystem().exists(path));\r\n    assertTrue(\"Rename should rename the file\", getFileSystem().exists(renamedPath));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "testNNBenchCrossCluster",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void testNNBenchCrossCluster() throws Exception\n{\r\n    MiniDFSCluster dfsCluster = new MiniDFSCluster.Builder(new JobConf()).numDataNodes(1).build();\r\n    dfsCluster.waitClusterUp();\r\n    String nnAddress = dfsCluster.getNameNode(0).getHostAndPort();\r\n    String baseDir = \"hdfs://\" + nnAddress + BASE_DIR;\r\n    runNNBench(createJobConf(), \"create_write\", baseDir);\r\n    Path path = new Path(BASE_DIR + \"/data/file_0_0\");\r\n    assertTrue(\"create_write should create the file\", dfsCluster.getFileSystem().exists(path));\r\n    dfsCluster.shutdown();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "runNNBench",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void runNNBench(Configuration conf, String operation, String baseDir) throws Exception\n{\r\n    String[] genArgs = { \"-operation\", operation, \"-baseDir\", baseDir, \"-startTime\", \"\" + (Time.now() / 1000 + 3), \"-blockSize\", \"1024\" };\r\n    assertEquals(0, ToolRunner.run(conf, new NNBench(), genArgs));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "runNNBench",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void runNNBench(Configuration conf, String operation) throws Exception\n{\r\n    runNNBench(conf, operation, BASE_DIR);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\fs\\slive",
  "methodName" : "configure",
  "errType" : [ "Exception" ],
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void configure(JobConf conf)\n{\r\n    try {\r\n        config = new ConfigExtractor(conf);\r\n        ConfigExtractor.dumpOptions(config);\r\n        filesystem = config.getBaseDirectory().getFileSystem(conf);\r\n    } catch (Exception e) {\r\n        LOG.error(\"Unable to setup slive \" + StringUtils.stringifyException(e));\r\n        throw new RuntimeException(\"Unable to setup slive configuration\", e);\r\n    }\r\n    if (conf.get(MRJobConfig.TASK_ATTEMPT_ID) != null) {\r\n        this.taskId = TaskAttemptID.forName(conf.get(MRJobConfig.TASK_ATTEMPT_ID)).getTaskID().getId();\r\n    } else {\r\n        this.taskId = TaskAttemptID.forName(conf.get(\"mapred.task.id\")).getTaskID().getId();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\fs\\slive",
  "methodName" : "getConfig",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "ConfigExtractor getConfig()\n{\r\n    return config;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\fs\\slive",
  "methodName" : "logAndSetStatus",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void logAndSetStatus(Reporter r, String msg)\n{\r\n    r.setStatus(msg);\r\n    LOG.info(msg);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\fs\\slive",
  "methodName" : "runOperation",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void runOperation(Operation op, Reporter reporter, OutputCollector<Text, Text> output, long opNum) throws IOException\n{\r\n    if (op == null) {\r\n        return;\r\n    }\r\n    logAndSetStatus(reporter, \"Running operation #\" + opNum + \" (\" + op + \")\");\r\n    List<OperationOutput> opOut = op.run(filesystem);\r\n    logAndSetStatus(reporter, \"Finished operation #\" + opNum + \" (\" + op + \")\");\r\n    if (opOut != null && !opOut.isEmpty()) {\r\n        for (OperationOutput outData : opOut) {\r\n            output.collect(outData.getKey(), outData.getOutputValue());\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\fs\\slive",
  "methodName" : "map",
  "errType" : [ "Exception" ],
  "containingMethodsNum" : 19,
  "sourceCodeText" : "void map(Object key, Object value, OutputCollector<Text, Text> output, Reporter reporter) throws IOException\n{\r\n    logAndSetStatus(reporter, \"Running slive mapper for dummy key \" + key + \" and dummy value \" + value);\r\n    Random rnd = config.getRandomSeed() != null ? new Random(this.taskId + config.getRandomSeed()) : new Random();\r\n    WeightSelector selector = new WeightSelector(config, rnd);\r\n    long startTime = Timer.now();\r\n    long opAm = 0;\r\n    long sleepOps = 0;\r\n    int duration = getConfig().getDurationMilliseconds();\r\n    Range<Long> sleepRange = getConfig().getSleepRange();\r\n    Operation sleeper = null;\r\n    if (sleepRange != null) {\r\n        sleeper = new SleepOp(getConfig(), rnd);\r\n    }\r\n    while (Timer.elapsed(startTime) < duration) {\r\n        try {\r\n            logAndSetStatus(reporter, \"Attempting to select operation #\" + (opAm + 1));\r\n            int currElapsed = (int) (Timer.elapsed(startTime));\r\n            Operation op = selector.select(currElapsed, duration);\r\n            if (op == null) {\r\n                break;\r\n            } else {\r\n                ++opAm;\r\n                runOperation(op, reporter, output, opAm);\r\n            }\r\n            if (sleeper != null) {\r\n                ++sleepOps;\r\n                runOperation(sleeper, reporter, output, sleepOps);\r\n            }\r\n        } catch (Exception e) {\r\n            logAndSetStatus(reporter, \"Failed at running due to \" + StringUtils.stringifyException(e));\r\n            if (getConfig().shouldExitOnFirstError()) {\r\n                break;\r\n            }\r\n        }\r\n    }\r\n    {\r\n        long timeTaken = Timer.elapsed(startTime);\r\n        OperationOutput opCount = new OperationOutput(OutputType.LONG, OP_TYPE, ReportWriter.OP_COUNT, opAm);\r\n        output.collect(opCount.getKey(), opCount.getOutputValue());\r\n        OperationOutput overallTime = new OperationOutput(OutputType.LONG, OP_TYPE, ReportWriter.OK_TIME_TAKEN, timeTaken);\r\n        output.collect(overallTime.getKey(), overallTime.getOutputValue());\r\n        logAndSetStatus(reporter, \"Finished \" + opAm + \" operations in \" + timeTaken + \" milliseconds\");\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "map",
  "errType" : [ "Exception" ],
  "containingMethodsNum" : 31,
  "sourceCodeText" : "void map(IntWritable key, IntWritable val, Context context) throws IOException\n{\r\n    TimelineClient tlc = TimelineClient.createTimelineClient();\r\n    TimelineEntityConverterV1 converter = new TimelineEntityConverterV1();\r\n    JobHistoryFileReplayHelper helper = new JobHistoryFileReplayHelper(context);\r\n    int replayMode = helper.getReplayMode();\r\n    Collection<JobFiles> jobs = helper.getJobFiles();\r\n    JobHistoryFileParser parser = helper.getParser();\r\n    if (jobs.isEmpty()) {\r\n        LOG.info(context.getTaskAttemptID().getTaskID() + \" will process no jobs\");\r\n    } else {\r\n        LOG.info(context.getTaskAttemptID().getTaskID() + \" will process \" + jobs.size() + \" jobs\");\r\n    }\r\n    for (JobFiles job : jobs) {\r\n        String jobIdStr = job.getJobId();\r\n        LOG.info(\"processing \" + jobIdStr + \"...\");\r\n        JobId jobId = TypeConverter.toYarn(JobID.forName(jobIdStr));\r\n        ApplicationId appId = jobId.getAppId();\r\n        try {\r\n            Path historyFilePath = job.getJobHistoryFilePath();\r\n            Path confFilePath = job.getJobConfFilePath();\r\n            if ((historyFilePath == null) || (confFilePath == null)) {\r\n                continue;\r\n            }\r\n            JobInfo jobInfo = parser.parseHistoryFile(historyFilePath);\r\n            Configuration jobConf = parser.parseConfiguration(confFilePath);\r\n            LOG.info(\"parsed the job history file and the configuration file for job \" + jobIdStr);\r\n            long totalTime = 0;\r\n            Set<TimelineEntity> entitySet = converter.createTimelineEntities(jobInfo, jobConf);\r\n            LOG.info(\"converted them into timeline entities for job \" + jobIdStr);\r\n            UserGroupInformation ugi = UserGroupInformation.getCurrentUser();\r\n            long startWrite = System.nanoTime();\r\n            try {\r\n                switch(replayMode) {\r\n                    case JobHistoryFileReplayHelper.WRITE_ALL_AT_ONCE:\r\n                        writeAllEntities(tlc, entitySet, ugi);\r\n                        break;\r\n                    case JobHistoryFileReplayHelper.WRITE_PER_ENTITY:\r\n                        writePerEntity(tlc, entitySet, ugi);\r\n                        break;\r\n                    default:\r\n                        break;\r\n                }\r\n            } catch (Exception e) {\r\n                context.getCounter(PerfCounters.TIMELINE_SERVICE_WRITE_FAILURES).increment(1);\r\n                LOG.error(\"writing to the timeline service failed\", e);\r\n            }\r\n            long endWrite = System.nanoTime();\r\n            totalTime += TimeUnit.NANOSECONDS.toMillis(endWrite - startWrite);\r\n            int numEntities = entitySet.size();\r\n            LOG.info(\"wrote \" + numEntities + \" entities in \" + totalTime + \" ms\");\r\n            context.getCounter(PerfCounters.TIMELINE_SERVICE_WRITE_TIME).increment(totalTime);\r\n            context.getCounter(PerfCounters.TIMELINE_SERVICE_WRITE_COUNTER).increment(numEntities);\r\n        } finally {\r\n            context.progress();\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 2,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "writeAllEntities",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void writeAllEntities(TimelineClient tlc, Set<TimelineEntity> entitySet, UserGroupInformation ugi) throws IOException, YarnException\n{\r\n    tlc.putEntities((TimelineEntity[]) entitySet.toArray());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "writePerEntity",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void writePerEntity(TimelineClient tlc, Set<TimelineEntity> entitySet, UserGroupInformation ugi) throws IOException, YarnException\n{\r\n    for (TimelineEntity entity : entitySet) {\r\n        tlc.putEntities(entity);\r\n        LOG.info(\"wrote entity \" + entity.getEntityId());\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "configure",
  "errType" : null,
  "containingMethodsNum" : 20,
  "sourceCodeText" : "void configure() throws Exception\n{\r\n    Path testdir = new Path(TEST_DIR.getAbsolutePath());\r\n    Path inDir = new Path(testdir, \"in\");\r\n    Path outDir = new Path(testdir, \"out\");\r\n    FileSystem fs = FileSystem.get(conf);\r\n    fs.delete(testdir, true);\r\n    conf.setInt(JobContext.IO_SORT_MB, 1);\r\n    conf.setInputFormat(SequenceFileInputFormat.class);\r\n    FileInputFormat.setInputPaths(conf, inDir);\r\n    FileOutputFormat.setOutputPath(conf, outDir);\r\n    conf.setMapperClass(TextGen.class);\r\n    conf.setReducerClass(TextReduce.class);\r\n    conf.setOutputKeyClass(Text.class);\r\n    conf.setOutputValueClass(Text.class);\r\n    conf.set(MRConfig.FRAMEWORK_NAME, MRConfig.LOCAL_FRAMEWORK_NAME);\r\n    conf.setOutputFormat(SequenceFileOutputFormat.class);\r\n    if (!fs.mkdirs(testdir)) {\r\n        throw new IOException(\"Mkdirs failed to create \" + testdir.toString());\r\n    }\r\n    if (!fs.mkdirs(inDir)) {\r\n        throw new IOException(\"Mkdirs failed to create \" + inDir.toString());\r\n    }\r\n    Path inFile = new Path(inDir, \"part0\");\r\n    SequenceFile.Writer writer = SequenceFile.createWriter(fs, conf, inFile, Text.class, Text.class);\r\n    writer.append(new Text(\"rec: 1\"), new Text(\"Hello\"));\r\n    writer.close();\r\n    jc = new JobClient(conf);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "cleanup",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void cleanup()\n{\r\n    FileUtil.fullyDelete(TEST_DIR);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "testKeyMismatch",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void testKeyMismatch() throws Exception\n{\r\n    conf.setMapOutputKeyClass(IntWritable.class);\r\n    conf.setMapOutputValueClass(IntWritable.class);\r\n    RunningJob r_job = jc.submitJob(conf);\r\n    while (!r_job.isComplete()) {\r\n        Thread.sleep(1000);\r\n    }\r\n    if (r_job.isSuccessful()) {\r\n        fail(\"Oops! The job was supposed to break due to an exception\");\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "testValueMismatch",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void testValueMismatch() throws Exception\n{\r\n    conf.setMapOutputKeyClass(Text.class);\r\n    conf.setMapOutputValueClass(IntWritable.class);\r\n    RunningJob r_job = jc.submitJob(conf);\r\n    while (!r_job.isComplete()) {\r\n        Thread.sleep(1000);\r\n    }\r\n    if (r_job.isSuccessful()) {\r\n        fail(\"Oops! The job was supposed to break due to an exception\");\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "testNoMismatch",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void testNoMismatch() throws Exception\n{\r\n    conf.setMapOutputKeyClass(Text.class);\r\n    conf.setMapOutputValueClass(Text.class);\r\n    RunningJob r_job = jc.submitJob(conf);\r\n    while (!r_job.isComplete()) {\r\n        Thread.sleep(1000);\r\n    }\r\n    if (!r_job.isSuccessful()) {\r\n        fail(\"Oops! The job broke due to an unexpected error\");\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\partition",
  "methodName" : "testEmptyKey",
  "errType" : null,
  "containingMethodsNum" : 44,
  "sourceCodeText" : "void testEmptyKey() throws Exception\n{\r\n    int numReducers = 10;\r\n    KeyFieldBasedPartitioner<Text, Text> kfbp = new KeyFieldBasedPartitioner<Text, Text>();\r\n    Configuration conf = new Configuration();\r\n    conf.setInt(\"num.key.fields.for.partition\", 10);\r\n    kfbp.setConf(conf);\r\n    assertEquals(\"Empty key should map to 0th partition\", 0, kfbp.getPartition(new Text(), new Text(), numReducers));\r\n    kfbp = new KeyFieldBasedPartitioner<Text, Text>();\r\n    conf = new Configuration();\r\n    kfbp.setConf(conf);\r\n    String input = \"abc\\tdef\\txyz\";\r\n    int hashCode = input.hashCode();\r\n    int expectedPartition = kfbp.getPartition(hashCode, numReducers);\r\n    assertEquals(\"Partitioner doesnt work as expected\", expectedPartition, kfbp.getPartition(new Text(input), new Text(), numReducers));\r\n    kfbp = new KeyFieldBasedPartitioner<Text, Text>();\r\n    conf = new Configuration();\r\n    conf.set(KeyFieldBasedPartitioner.PARTITIONER_OPTIONS, \"-k2,2\");\r\n    kfbp.setConf(conf);\r\n    String expectedOutput = \"def\";\r\n    byte[] eBytes = expectedOutput.getBytes();\r\n    hashCode = kfbp.hashCode(eBytes, 0, eBytes.length - 1, 0);\r\n    expectedPartition = kfbp.getPartition(hashCode, numReducers);\r\n    assertEquals(\"Partitioner doesnt work as expected\", expectedPartition, kfbp.getPartition(new Text(input), new Text(), numReducers));\r\n    kfbp = new KeyFieldBasedPartitioner<Text, Text>();\r\n    conf = new Configuration();\r\n    conf.set(KeyFieldBasedPartitioner.PARTITIONER_OPTIONS, \"-k2,5\");\r\n    kfbp.setConf(conf);\r\n    expectedOutput = \"def\\txyz\";\r\n    eBytes = expectedOutput.getBytes();\r\n    hashCode = kfbp.hashCode(eBytes, 0, eBytes.length - 1, 0);\r\n    expectedPartition = kfbp.getPartition(hashCode, numReducers);\r\n    assertEquals(\"Partitioner doesnt work as expected\", expectedPartition, kfbp.getPartition(new Text(input), new Text(), numReducers));\r\n    kfbp = new KeyFieldBasedPartitioner<Text, Text>();\r\n    conf = new Configuration();\r\n    conf.set(KeyFieldBasedPartitioner.PARTITIONER_OPTIONS, \"-k2\");\r\n    kfbp.setConf(conf);\r\n    expectedOutput = \"def\\txyz\";\r\n    eBytes = expectedOutput.getBytes();\r\n    hashCode = kfbp.hashCode(eBytes, 0, eBytes.length - 1, 0);\r\n    expectedPartition = kfbp.getPartition(hashCode, numReducers);\r\n    assertEquals(\"Partitioner doesnt work as expected\", expectedPartition, kfbp.getPartition(new Text(input), new Text(), numReducers));\r\n    kfbp = new KeyFieldBasedPartitioner<Text, Text>();\r\n    conf = new Configuration();\r\n    conf.set(KeyFieldBasedPartitioner.PARTITIONER_OPTIONS, \"-k10\");\r\n    kfbp.setConf(conf);\r\n    assertEquals(\"Partitioner doesnt work as expected\", 0, kfbp.getPartition(new Text(input), new Text(), numReducers));\r\n    kfbp = new KeyFieldBasedPartitioner<Text, Text>();\r\n    conf = new Configuration();\r\n    conf.set(KeyFieldBasedPartitioner.PARTITIONER_OPTIONS, \"-k2,2 -k4,4\");\r\n    kfbp.setConf(conf);\r\n    input = \"abc\\tdef\\tpqr\\txyz\";\r\n    expectedOutput = \"def\";\r\n    eBytes = expectedOutput.getBytes();\r\n    hashCode = kfbp.hashCode(eBytes, 0, eBytes.length - 1, 0);\r\n    expectedOutput = \"xyz\";\r\n    eBytes = expectedOutput.getBytes();\r\n    hashCode = kfbp.hashCode(eBytes, 0, eBytes.length - 1, hashCode);\r\n    expectedPartition = kfbp.getPartition(hashCode, numReducers);\r\n    assertEquals(\"Partitioner doesnt work as expected\", expectedPartition, kfbp.getPartition(new Text(input), new Text(), numReducers));\r\n    kfbp = new KeyFieldBasedPartitioner<Text, Text>();\r\n    conf = new Configuration();\r\n    conf.set(KeyFieldBasedPartitioner.PARTITIONER_OPTIONS, \"-k2,2 -k30,21 -k4,4 -k5\");\r\n    kfbp.setConf(conf);\r\n    expectedOutput = \"def\";\r\n    eBytes = expectedOutput.getBytes();\r\n    hashCode = kfbp.hashCode(eBytes, 0, eBytes.length - 1, 0);\r\n    expectedOutput = \"xyz\";\r\n    eBytes = expectedOutput.getBytes();\r\n    hashCode = kfbp.hashCode(eBytes, 0, eBytes.length - 1, hashCode);\r\n    expectedPartition = kfbp.getPartition(hashCode, numReducers);\r\n    assertEquals(\"Partitioner doesnt work as expected\", expectedPartition, kfbp.getPartition(new Text(input), new Text(), numReducers));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\security",
  "methodName" : "testJobTokenRpc",
  "errType" : null,
  "containingMethodsNum" : 13,
  "sourceCodeText" : "void testJobTokenRpc() throws Exception\n{\r\n    TaskUmbilicalProtocol mockTT = mock(TaskUmbilicalProtocol.class);\r\n    doReturn(TaskUmbilicalProtocol.versionID).when(mockTT).getProtocolVersion(anyString(), anyLong());\r\n    doReturn(ProtocolSignature.getProtocolSignature(mockTT, TaskUmbilicalProtocol.class.getName(), TaskUmbilicalProtocol.versionID, 0)).when(mockTT).getProtocolSignature(anyString(), anyLong(), anyInt());\r\n    JobTokenSecretManager sm = new JobTokenSecretManager();\r\n    final Server server = new RPC.Builder(conf).setProtocol(TaskUmbilicalProtocol.class).setInstance(mockTT).setBindAddress(ADDRESS).setPort(0).setNumHandlers(5).setVerbose(true).setSecretManager(sm).build();\r\n    server.start();\r\n    final UserGroupInformation current = UserGroupInformation.getCurrentUser();\r\n    final InetSocketAddress addr = NetUtils.getConnectAddress(server);\r\n    String jobId = current.getUserName();\r\n    JobTokenIdentifier tokenId = new JobTokenIdentifier(new Text(jobId));\r\n    Token<JobTokenIdentifier> token = new Token<JobTokenIdentifier>(tokenId, sm);\r\n    sm.addTokenForJob(jobId, token);\r\n    SecurityUtil.setTokenService(token, addr);\r\n    LOG.info(\"Service address for token is \" + token.getService());\r\n    current.addToken(token);\r\n    current.doAs(new PrivilegedExceptionAction<Object>() {\r\n\r\n        @Override\r\n        public Object run() throws Exception {\r\n            TaskUmbilicalProtocol proxy = null;\r\n            try {\r\n                proxy = (TaskUmbilicalProtocol) RPC.getProxy(TaskUmbilicalProtocol.class, TaskUmbilicalProtocol.versionID, addr, conf);\r\n                proxy.statusUpdate(null, null);\r\n            } finally {\r\n                server.stop();\r\n                if (proxy != null) {\r\n                    RPC.stopProxy(proxy);\r\n                }\r\n            }\r\n            return null;\r\n        }\r\n    });\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\fs\\slive",
  "methodName" : "getSectionDelimiter",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getSectionDelimiter()\n{\r\n    return SECTION_DELIM;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\fs\\slive",
  "methodName" : "writeMessage",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void writeMessage(String msg, PrintWriter os)\n{\r\n    LOG.info(msg);\r\n    if (os != null) {\r\n        os.println(msg);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\fs\\slive",
  "methodName" : "basicReport",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void basicReport(List<OperationOutput> input, PrintWriter os)\n{\r\n    writeMessage(\"Default report for \" + input.size() + \" operations \", os);\r\n    writeMessage(getSectionDelimiter(), os);\r\n    for (OperationOutput data : input) {\r\n        writeMessage(\"Operation \\\"\" + data.getOperationType() + \"\\\" measuring \\\"\" + data.getMeasurementType() + \"\\\" = \" + data.getValue(), os);\r\n    }\r\n    writeMessage(getSectionDelimiter(), os);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\fs\\slive",
  "methodName" : "opReport",
  "errType" : null,
  "containingMethodsNum" : 25,
  "sourceCodeText" : "void opReport(String operation, List<OperationOutput> input, PrintWriter os)\n{\r\n    writeMessage(\"Basic report for operation type \" + operation, os);\r\n    writeMessage(getSectionDelimiter(), os);\r\n    for (OperationOutput data : input) {\r\n        writeMessage(\"Measurement \\\"\" + data.getMeasurementType() + \"\\\" = \" + data.getValue(), os);\r\n    }\r\n    Map<String, OperationOutput> combined = new TreeMap<String, OperationOutput>();\r\n    for (OperationOutput data : input) {\r\n        if (combined.containsKey(data.getMeasurementType())) {\r\n            OperationOutput curr = combined.get(data.getMeasurementType());\r\n            combined.put(data.getMeasurementType(), OperationOutput.merge(curr, data));\r\n        } else {\r\n            combined.put(data.getMeasurementType(), data);\r\n        }\r\n    }\r\n    OperationOutput timeTaken = combined.get(OK_TIME_TAKEN);\r\n    if (timeTaken != null) {\r\n        Long mTaken = Long.parseLong(timeTaken.getValue().toString());\r\n        if (mTaken > 0) {\r\n            NumberFormat formatter = Formatter.getDecimalFormatter();\r\n            for (String measurementType : combined.keySet()) {\r\n                Double rate = null;\r\n                String rateType = \"\";\r\n                if (measurementType.equals(BYTES_WRITTEN)) {\r\n                    Long mbWritten = Long.parseLong(combined.get(measurementType).getValue().toString()) / (Constants.MEGABYTES);\r\n                    rate = (double) mbWritten / (double) (mTaken / 1000.0d);\r\n                    rateType = \"MB/sec\";\r\n                } else if (measurementType.equals(SUCCESSES)) {\r\n                    Long succ = Long.parseLong(combined.get(measurementType).getValue().toString());\r\n                    rate = (double) succ / (double) (mTaken / 1000.0d);\r\n                    rateType = \"successes/sec\";\r\n                } else if (measurementType.equals(FILES_CREATED)) {\r\n                    Long filesCreated = Long.parseLong(combined.get(measurementType).getValue().toString());\r\n                    rate = (double) filesCreated / (double) (mTaken / 1000.0d);\r\n                    rateType = \"files created/sec\";\r\n                } else if (measurementType.equals(DIR_ENTRIES)) {\r\n                    Long entries = Long.parseLong(combined.get(measurementType).getValue().toString());\r\n                    rate = (double) entries / (double) (mTaken / 1000.0d);\r\n                    rateType = \"directory entries/sec\";\r\n                } else if (measurementType.equals(OP_COUNT)) {\r\n                    Long opCount = Long.parseLong(combined.get(measurementType).getValue().toString());\r\n                    rate = (double) opCount / (double) (mTaken / 1000.0d);\r\n                    rateType = \"operations/sec\";\r\n                } else if (measurementType.equals(BYTES_READ)) {\r\n                    Long mbRead = Long.parseLong(combined.get(measurementType).getValue().toString()) / (Constants.MEGABYTES);\r\n                    rate = (double) mbRead / (double) (mTaken / 1000.0d);\r\n                    rateType = \"MB/sec\";\r\n                }\r\n                if (rate != null) {\r\n                    writeMessage(\"Rate for measurement \\\"\" + measurementType + \"\\\" = \" + formatter.format(rate) + \" \" + rateType, os);\r\n                }\r\n            }\r\n        }\r\n    }\r\n    writeMessage(getSectionDelimiter(), os);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "testClusterWithYarnClientProtocolProvider",
  "errType" : [ "Exception", "IOException" ],
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void testClusterWithYarnClientProtocolProvider() throws Exception\n{\r\n    Configuration conf = new Configuration(false);\r\n    Cluster cluster = null;\r\n    try {\r\n        cluster = new Cluster(conf);\r\n    } catch (Exception e) {\r\n        throw new Exception(\"Failed to initialize a local runner w/o a cluster framework key\", e);\r\n    }\r\n    try {\r\n        assertTrue(\"client is not a LocalJobRunner\", cluster.getClient() instanceof LocalJobRunner);\r\n    } finally {\r\n        if (cluster != null) {\r\n            cluster.close();\r\n        }\r\n    }\r\n    try {\r\n        conf = new Configuration();\r\n        conf.set(MRConfig.FRAMEWORK_NAME, MRConfig.YARN_FRAMEWORK_NAME);\r\n        cluster = new Cluster(conf);\r\n        ClientProtocol client = cluster.getClient();\r\n        assertTrue(\"client is a YARNRunner\", client instanceof YARNRunner);\r\n    } catch (IOException e) {\r\n    } finally {\r\n        if (cluster != null) {\r\n            cluster.close();\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 3,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "testClusterGetDelegationToken",
  "errType" : null,
  "containingMethodsNum" : 19,
  "sourceCodeText" : "void testClusterGetDelegationToken() throws Exception\n{\r\n    Configuration conf = new Configuration(false);\r\n    Cluster cluster = null;\r\n    try {\r\n        conf = new Configuration();\r\n        conf.set(MRConfig.FRAMEWORK_NAME, MRConfig.YARN_FRAMEWORK_NAME);\r\n        cluster = new Cluster(conf);\r\n        YARNRunner yrunner = (YARNRunner) cluster.getClient();\r\n        GetDelegationTokenResponse getDTResponse = recordFactory.newRecordInstance(GetDelegationTokenResponse.class);\r\n        org.apache.hadoop.yarn.api.records.Token rmDTToken = recordFactory.newRecordInstance(org.apache.hadoop.yarn.api.records.Token.class);\r\n        rmDTToken.setIdentifier(ByteBuffer.wrap(new byte[2]));\r\n        rmDTToken.setKind(\"Testclusterkind\");\r\n        rmDTToken.setPassword(ByteBuffer.wrap(\"testcluster\".getBytes()));\r\n        rmDTToken.setService(\"0.0.0.0:8032\");\r\n        getDTResponse.setRMDelegationToken(rmDTToken);\r\n        final ApplicationClientProtocol cRMProtocol = mock(ApplicationClientProtocol.class);\r\n        when(cRMProtocol.getDelegationToken(any(GetDelegationTokenRequest.class))).thenReturn(getDTResponse);\r\n        ResourceMgrDelegate rmgrDelegate = new ResourceMgrDelegate(new YarnConfiguration(conf)) {\r\n\r\n            @Override\r\n            protected void serviceStart() throws Exception {\r\n                assertTrue(this.client instanceof YarnClientImpl);\r\n                this.client = spy(this.client);\r\n                doNothing().when(this.client).close();\r\n                ((YarnClientImpl) this.client).setRMClient(cRMProtocol);\r\n            }\r\n        };\r\n        yrunner.setResourceMgrDelegate(rmgrDelegate);\r\n        Token t = cluster.getDelegationToken(new Text(\" \"));\r\n        assertTrue(\"Token kind is instead \" + t.getKind().toString(), \"Testclusterkind\".equals(t.getKind().toString()));\r\n    } finally {\r\n        if (cluster != null) {\r\n            cluster.close();\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\fs\\slive",
  "methodName" : "now",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "long now()\n{\r\n    return System.currentTimeMillis();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\fs\\slive",
  "methodName" : "elapsed",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "long elapsed(long startTime)\n{\r\n    long elapsedTime = now() - startTime;\r\n    if (elapsedTime < 0) {\r\n        elapsedTime = 0;\r\n    }\r\n    return elapsedTime;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\chain",
  "methodName" : "testNoChain",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void testNoChain() throws Exception\n{\r\n    Path inDir = new Path(localPathRoot, \"testing/chain/input\");\r\n    Path outDir = new Path(localPathRoot, \"testing/chain/output\");\r\n    String input = \"a\\nb\\na\\n\";\r\n    String expectedOutput = \"a\\t2\\nb\\t1\\n\";\r\n    Configuration conf = createJobConf();\r\n    Job job = MapReduceTestUtil.createJob(conf, inDir, outDir, 1, 1, input);\r\n    job.setJobName(\"chain\");\r\n    ChainMapper.addMapper(job, TokenCounterMapper.class, Object.class, Text.class, Text.class, IntWritable.class, null);\r\n    ChainReducer.setReducer(job, IntSumReducer.class, Text.class, IntWritable.class, Text.class, IntWritable.class, null);\r\n    job.waitForCompletion(true);\r\n    assertTrue(\"Job failed\", job.isSuccessful());\r\n    assertEquals(\"Outputs doesn't match\", expectedOutput, MapReduceTestUtil.readOutput(outDir, conf));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred\\join",
  "methodName" : "testClassLoader",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void testClassLoader() throws Exception\n{\r\n    JobConf job = new JobConf();\r\n    Fake_ClassLoader classLoader = new Fake_ClassLoader();\r\n    job.setClassLoader(classLoader);\r\n    assertTrue(job.getClassLoader() instanceof Fake_ClassLoader);\r\n    FileSystem fs = FileSystem.get(job);\r\n    Path testdir = fs.makeQualified(new Path(System.getProperty(\"test.build.data\", \"/tmp\")));\r\n    Path base = new Path(testdir, \"/empty\");\r\n    Path[] src = { new Path(base, \"i0\"), new Path(\"i1\"), new Path(\"i2\") };\r\n    job.set(\"mapreduce.join.expr\", CompositeInputFormat.compose(\"outer\", IF_ClassLoaderChecker.class, src));\r\n    CompositeInputFormat<NullWritable> inputFormat = new CompositeInputFormat<NullWritable>();\r\n    inputFormat.getRecordReader(inputFormat.getSplits(job, 1)[0], job, Reporter.NULL);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\cli",
  "methodName" : "getExecutor",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "CommandExecutor getExecutor(String tag, Configuration conf) throws IllegalArgumentException\n{\r\n    throw new IllegalArgumentException(\"Method isn't supported\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "barrier",
  "errType" : [ "InterruptedException" ],
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void barrier()\n{\r\n    long sleepTime;\r\n    while ((sleepTime = startTime - System.currentTimeMillis()) > 0) {\r\n        try {\r\n            Thread.sleep(sleepTime);\r\n        } catch (InterruptedException ex) {\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "handleException",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void handleException(String operation, Throwable e, int singleFileExceptions)\n{\r\n    LOG.warn(\"Exception while \" + operation + \": \" + StringUtils.stringifyException(e));\r\n    if (singleFileExceptions >= maxExceptionsPerFile) {\r\n        throw new RuntimeException(singleFileExceptions + \" exceptions for a single file exceeds threshold. Aborting\");\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "createWrite",
  "errType" : [ "IOException", "IOException", "IOException" ],
  "containingMethodsNum" : 7,
  "sourceCodeText" : "int createWrite()\n{\r\n    int totalExceptions = 0;\r\n    FSDataOutputStream out = null;\r\n    boolean success;\r\n    for (int index = 0; index < numFiles; index++) {\r\n        int singleFileExceptions = 0;\r\n        do {\r\n            try {\r\n                out = fileSys.create(new Path(taskDir, \"\" + index), false, 512, (short) replicationFactorPerFile, bytesPerBlock);\r\n                success = true;\r\n            } catch (IOException ioe) {\r\n                success = false;\r\n                totalExceptions++;\r\n                handleException(\"creating file #\" + index, ioe, ++singleFileExceptions);\r\n            }\r\n        } while (!success);\r\n        long toBeWritten = bytesPerFile;\r\n        while (toBeWritten > 0) {\r\n            int nbytes = (int) Math.min(buffer.length, toBeWritten);\r\n            toBeWritten -= nbytes;\r\n            try {\r\n                out.write(buffer, 0, nbytes);\r\n            } catch (IOException ioe) {\r\n                totalExceptions++;\r\n                handleException(\"writing to file #\" + index, ioe, ++singleFileExceptions);\r\n            }\r\n        }\r\n        do {\r\n            try {\r\n                out.close();\r\n                success = true;\r\n            } catch (IOException ioe) {\r\n                success = false;\r\n                totalExceptions++;\r\n                handleException(\"closing file #\" + index, ioe, ++singleFileExceptions);\r\n            }\r\n        } while (!success);\r\n    }\r\n    return totalExceptions;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 3,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "openRead",
  "errType" : [ "IOException", "IOException" ],
  "containingMethodsNum" : 6,
  "sourceCodeText" : "int openRead()\n{\r\n    int totalExceptions = 0;\r\n    FSDataInputStream in;\r\n    for (int index = 0; index < numFiles; index++) {\r\n        int singleFileExceptions = 0;\r\n        try {\r\n            in = fileSys.open(new Path(taskDir, \"\" + index), 512);\r\n            long toBeRead = bytesPerFile;\r\n            while (toBeRead > 0) {\r\n                int nbytes = (int) Math.min(buffer.length, toBeRead);\r\n                toBeRead -= nbytes;\r\n                try {\r\n                    in.read(buffer, 0, nbytes);\r\n                } catch (IOException ioe) {\r\n                    totalExceptions++;\r\n                    handleException(\"reading from file #\" + index, ioe, ++singleFileExceptions);\r\n                }\r\n            }\r\n            in.close();\r\n        } catch (IOException ioe) {\r\n            totalExceptions++;\r\n            handleException(\"opening file #\" + index, ioe, ++singleFileExceptions);\r\n        }\r\n    }\r\n    return totalExceptions;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 2,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "rename",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 2,
  "sourceCodeText" : "int rename()\n{\r\n    int totalExceptions = 0;\r\n    boolean success;\r\n    for (int index = 0; index < numFiles; index++) {\r\n        int singleFileExceptions = 0;\r\n        do {\r\n            try {\r\n                fileSys.rename(new Path(taskDir, \"\" + index), new Path(taskDir, \"A\" + index));\r\n                success = true;\r\n            } catch (IOException ioe) {\r\n                success = false;\r\n                totalExceptions++;\r\n                handleException(\"creating file #\" + index, ioe, ++singleFileExceptions);\r\n            }\r\n        } while (!success);\r\n    }\r\n    return totalExceptions;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "delete",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 2,
  "sourceCodeText" : "int delete()\n{\r\n    int totalExceptions = 0;\r\n    boolean success;\r\n    for (int index = 0; index < numFiles; index++) {\r\n        int singleFileExceptions = 0;\r\n        do {\r\n            try {\r\n                fileSys.delete(new Path(taskDir, \"A\" + index), true);\r\n                success = true;\r\n            } catch (IOException ioe) {\r\n                success = false;\r\n                totalExceptions++;\r\n                handleException(\"creating file #\" + index, ioe, ++singleFileExceptions);\r\n            }\r\n        } while (!success);\r\n    }\r\n    return totalExceptions;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "main",
  "errType" : null,
  "containingMethodsNum" : 52,
  "sourceCodeText" : "void main(String[] args) throws IOException\n{\r\n    String version = \"NameNodeBenchmark.0.3\";\r\n    System.out.println(version);\r\n    int bytesPerChecksum = -1;\r\n    String usage = \"Usage: nnbench \" + \"  -operation <one of createWrite, openRead, rename, or delete>\\n \" + \"  -baseDir <base output/input DFS path>\\n \" + \"  -startTime <time to start, given in seconds from the epoch>\\n\" + \"  -numFiles <number of files to create>\\n \" + \"  -replicationFactorPerFile <Replication factor for the files, default is 1>\\n\" + \"  -blocksPerFile <number of blocks to create per file>\\n\" + \"  [-bytesPerBlock <number of bytes to write to each block, default is 1>]\\n\" + \"  [-bytesPerChecksum <value for io.bytes.per.checksum>]\\n\" + \"Note: bytesPerBlock MUST be a multiple of bytesPerChecksum\\n\";\r\n    String operation = null;\r\n    for (int i = 0; i < args.length; i++) {\r\n        if (args[i].equals(\"-baseDir\")) {\r\n            baseDir = new Path(args[++i]);\r\n        } else if (args[i].equals(\"-numFiles\")) {\r\n            numFiles = Integer.parseInt(args[++i]);\r\n        } else if (args[i].equals(\"-blocksPerFile\")) {\r\n            blocksPerFile = Integer.parseInt(args[++i]);\r\n        } else if (args[i].equals(\"-bytesPerBlock\")) {\r\n            bytesPerBlock = Long.parseLong(args[++i]);\r\n        } else if (args[i].equals(\"-bytesPerChecksum\")) {\r\n            bytesPerChecksum = Integer.parseInt(args[++i]);\r\n        } else if (args[i].equals(\"-replicationFactorPerFile\")) {\r\n            replicationFactorPerFile = Short.parseShort(args[++i]);\r\n        } else if (args[i].equals(\"-startTime\")) {\r\n            startTime = Long.parseLong(args[++i]) * 1000;\r\n        } else if (args[i].equals(\"-operation\")) {\r\n            operation = args[++i];\r\n        } else {\r\n            System.out.println(usage);\r\n            System.exit(-1);\r\n        }\r\n    }\r\n    bytesPerFile = bytesPerBlock * blocksPerFile;\r\n    JobConf jobConf = new JobConf(new Configuration(), NNBench.class);\r\n    if (bytesPerChecksum < 0) {\r\n        bytesPerChecksum = jobConf.getInt(\"io.bytes.per.checksum\", 512);\r\n    }\r\n    jobConf.set(\"io.bytes.per.checksum\", Integer.toString(bytesPerChecksum));\r\n    System.out.println(\"Inputs: \");\r\n    System.out.println(\"   operation: \" + operation);\r\n    System.out.println(\"   baseDir: \" + baseDir);\r\n    System.out.println(\"   startTime: \" + startTime);\r\n    System.out.println(\"   numFiles: \" + numFiles);\r\n    System.out.println(\"   replicationFactorPerFile: \" + replicationFactorPerFile);\r\n    System.out.println(\"   blocksPerFile: \" + blocksPerFile);\r\n    System.out.println(\"   bytesPerBlock: \" + bytesPerBlock);\r\n    System.out.println(\"   bytesPerChecksum: \" + bytesPerChecksum);\r\n    if (operation == null || baseDir == null || numFiles < 1 || blocksPerFile < 1 || bytesPerBlock < 0 || bytesPerBlock % bytesPerChecksum != 0) {\r\n        System.err.println(usage);\r\n        System.exit(-1);\r\n    }\r\n    fileSys = FileSystem.get(jobConf);\r\n    String uniqueId = java.net.InetAddress.getLocalHost().getHostName();\r\n    taskDir = new Path(baseDir, uniqueId);\r\n    buffer = new byte[(int) Math.min(bytesPerFile, 32768L)];\r\n    Date execTime;\r\n    Date endTime;\r\n    long duration;\r\n    int exceptions = 0;\r\n    barrier();\r\n    execTime = new Date();\r\n    System.out.println(\"Job started: \" + startTime);\r\n    if (operation.equals(\"createWrite\")) {\r\n        if (!fileSys.mkdirs(taskDir)) {\r\n            throw new IOException(\"Mkdirs failed to create \" + taskDir.toString());\r\n        }\r\n        exceptions = createWrite();\r\n    } else if (operation.equals(\"openRead\")) {\r\n        exceptions = openRead();\r\n    } else if (operation.equals(\"rename\")) {\r\n        exceptions = rename();\r\n    } else if (operation.equals(\"delete\")) {\r\n        exceptions = delete();\r\n    } else {\r\n        System.err.println(usage);\r\n        System.exit(-1);\r\n    }\r\n    endTime = new Date();\r\n    System.out.println(\"Job ended: \" + endTime);\r\n    duration = (endTime.getTime() - execTime.getTime()) / 1000;\r\n    System.out.println(\"The \" + operation + \" job took \" + duration + \" seconds.\");\r\n    System.out.println(\"The job recorded \" + exceptions + \" exceptions.\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "printUsage",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "int printUsage()\n{\r\n    System.out.println(\"randomtextwriter \" + \"[-outFormat <output format class>] \" + \"<output>\");\r\n    ToolRunner.printGenericCommandUsage(System.out);\r\n    return 2;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "generateSentenceWithRand",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "String generateSentenceWithRand(ThreadLocalRandom rand, int noWords)\n{\r\n    StringBuffer sentence = new StringBuffer(words[rand.nextInt(words.length)]);\r\n    for (int i = 1; i < noWords; i++) {\r\n        sentence.append(\" \").append(words[rand.nextInt(words.length)]);\r\n    }\r\n    return sentence.toString();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "run",
  "errType" : [ "ArrayIndexOutOfBoundsException" ],
  "containingMethodsNum" : 29,
  "sourceCodeText" : "int run(String[] args) throws Exception\n{\r\n    if (args.length == 0) {\r\n        return printUsage();\r\n    }\r\n    Configuration conf = getConf();\r\n    JobClient client = new JobClient(conf);\r\n    ClusterStatus cluster = client.getClusterStatus();\r\n    int numMapsPerHost = conf.getInt(MAPS_PER_HOST, 10);\r\n    long numBytesToWritePerMap = conf.getLong(BYTES_PER_MAP, 1 * 1024 * 1024 * 1024);\r\n    if (numBytesToWritePerMap == 0) {\r\n        System.err.println(\"Cannot have \" + BYTES_PER_MAP + \" set to 0\");\r\n        return -2;\r\n    }\r\n    long totalBytesToWrite = conf.getLong(TOTAL_BYTES, numMapsPerHost * numBytesToWritePerMap * cluster.getTaskTrackers());\r\n    int numMaps = (int) (totalBytesToWrite / numBytesToWritePerMap);\r\n    if (numMaps == 0 && totalBytesToWrite > 0) {\r\n        numMaps = 1;\r\n        conf.setLong(BYTES_PER_MAP, totalBytesToWrite);\r\n    }\r\n    conf.setInt(MRJobConfig.NUM_MAPS, numMaps);\r\n    Job job = Job.getInstance(conf);\r\n    job.setJarByClass(RandomTextWriter.class);\r\n    job.setJobName(\"random-text-writer\");\r\n    job.setOutputKeyClass(Text.class);\r\n    job.setOutputValueClass(Text.class);\r\n    job.setInputFormatClass(RandomWriter.RandomInputFormat.class);\r\n    job.setMapperClass(RandomTextMapper.class);\r\n    Class<? extends OutputFormat> outputFormatClass = SequenceFileOutputFormat.class;\r\n    List<String> otherArgs = new ArrayList<String>();\r\n    for (int i = 0; i < args.length; ++i) {\r\n        try {\r\n            if (\"-outFormat\".equals(args[i])) {\r\n                outputFormatClass = Class.forName(args[++i]).asSubclass(OutputFormat.class);\r\n            } else {\r\n                otherArgs.add(args[i]);\r\n            }\r\n        } catch (ArrayIndexOutOfBoundsException except) {\r\n            System.out.println(\"ERROR: Required parameter missing from \" + args[i - 1]);\r\n            return printUsage();\r\n        }\r\n    }\r\n    job.setOutputFormatClass(outputFormatClass);\r\n    FileOutputFormat.setOutputPath(job, new Path(otherArgs.get(0)));\r\n    System.out.println(\"Running \" + numMaps + \" maps.\");\r\n    job.setNumReduceTasks(0);\r\n    Date startTime = new Date();\r\n    System.out.println(\"Job started: \" + startTime);\r\n    int ret = job.waitForCompletion(true) ? 0 : 1;\r\n    Date endTime = new Date();\r\n    System.out.println(\"Job ended: \" + endTime);\r\n    System.out.println(\"The job took \" + (endTime.getTime() - startTime.getTime()) / 1000 + \" seconds.\");\r\n    return ret;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "main",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void main(String[] args) throws Exception\n{\r\n    int res = ToolRunner.run(new Configuration(), new RandomTextWriter(), args);\r\n    System.exit(res);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\partition",
  "methodName" : "testDefaultOffsets",
  "errType" : null,
  "containingMethodsNum" : 10,
  "sourceCodeText" : "void testDefaultOffsets()\n{\r\n    Configuration conf = new Configuration();\r\n    BinaryPartitioner<?> partitioner = ReflectionUtils.newInstance(BinaryPartitioner.class, conf);\r\n    BinaryComparable key1 = new BytesWritable(new byte[] { 1, 2, 3, 4, 5 });\r\n    BinaryComparable key2 = new BytesWritable(new byte[] { 1, 2, 3, 4, 5 });\r\n    int partition1 = partitioner.getPartition(key1, null, 10);\r\n    int partition2 = partitioner.getPartition(key2, null, 10);\r\n    assertEquals(partition1, partition2);\r\n    key1 = new BytesWritable(new byte[] { 1, 2, 3, 4, 5 });\r\n    key2 = new BytesWritable(new byte[] { 6, 2, 3, 4, 5 });\r\n    partition1 = partitioner.getPartition(key1, null, 10);\r\n    partition2 = partitioner.getPartition(key2, null, 10);\r\n    assertTrue(partition1 != partition2);\r\n    key1 = new BytesWritable(new byte[] { 1, 2, 3, 4, 5 });\r\n    key2 = new BytesWritable(new byte[] { 1, 2, 3, 4, 6 });\r\n    partition1 = partitioner.getPartition(key1, null, 10);\r\n    partition2 = partitioner.getPartition(key2, null, 10);\r\n    assertTrue(partition1 != partition2);\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\partition",
  "methodName" : "testCustomOffsets",
  "errType" : null,
  "containingMethodsNum" : 15,
  "sourceCodeText" : "void testCustomOffsets()\n{\r\n    Configuration conf = new Configuration();\r\n    BinaryComparable key1 = new BytesWritable(new byte[] { 1, 2, 3, 4, 5 });\r\n    BinaryComparable key2 = new BytesWritable(new byte[] { 6, 2, 3, 7, 8 });\r\n    BinaryPartitioner.setOffsets(conf, 1, -3);\r\n    BinaryPartitioner<?> partitioner = ReflectionUtils.newInstance(BinaryPartitioner.class, conf);\r\n    int partition1 = partitioner.getPartition(key1, null, 10);\r\n    int partition2 = partitioner.getPartition(key2, null, 10);\r\n    assertEquals(partition1, partition2);\r\n    BinaryPartitioner.setOffsets(conf, 1, 2);\r\n    partitioner = ReflectionUtils.newInstance(BinaryPartitioner.class, conf);\r\n    partition1 = partitioner.getPartition(key1, null, 10);\r\n    partition2 = partitioner.getPartition(key2, null, 10);\r\n    assertEquals(partition1, partition2);\r\n    BinaryPartitioner.setOffsets(conf, -4, -3);\r\n    partitioner = ReflectionUtils.newInstance(BinaryPartitioner.class, conf);\r\n    partition1 = partitioner.getPartition(key1, null, 10);\r\n    partition2 = partitioner.getPartition(key2, null, 10);\r\n    assertEquals(partition1, partition2);\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\partition",
  "methodName" : "testLowerBound",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void testLowerBound()\n{\r\n    Configuration conf = new Configuration();\r\n    BinaryPartitioner.setLeftOffset(conf, 0);\r\n    BinaryPartitioner<?> partitioner = ReflectionUtils.newInstance(BinaryPartitioner.class, conf);\r\n    BinaryComparable key1 = new BytesWritable(new byte[] { 1, 2, 3, 4, 5 });\r\n    BinaryComparable key2 = new BytesWritable(new byte[] { 6, 2, 3, 4, 5 });\r\n    int partition1 = partitioner.getPartition(key1, null, 10);\r\n    int partition2 = partitioner.getPartition(key2, null, 10);\r\n    assertTrue(partition1 != partition2);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\partition",
  "methodName" : "testUpperBound",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void testUpperBound()\n{\r\n    Configuration conf = new Configuration();\r\n    BinaryPartitioner.setRightOffset(conf, 4);\r\n    BinaryPartitioner<?> partitioner = ReflectionUtils.newInstance(BinaryPartitioner.class, conf);\r\n    BinaryComparable key1 = new BytesWritable(new byte[] { 1, 2, 3, 4, 5 });\r\n    BinaryComparable key2 = new BytesWritable(new byte[] { 1, 2, 3, 4, 6 });\r\n    int partition1 = partitioner.getPartition(key1, null, 10);\r\n    int partition2 = partitioner.getPartition(key2, null, 10);\r\n    assertTrue(partition1 != partition2);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "testFormat",
  "errType" : null,
  "containingMethodsNum" : 29,
  "sourceCodeText" : "void testFormat() throws Exception\n{\r\n    JobConf job = new JobConf(defaultConf);\r\n    Path file = new Path(workDir, \"test.txt\");\r\n    Reporter reporter = Reporter.NULL;\r\n    int seed = new Random().nextInt();\r\n    LOG.info(\"seed = \" + seed);\r\n    Random random = new Random(seed);\r\n    localFs.delete(workDir, true);\r\n    FileInputFormat.setInputPaths(job, workDir);\r\n    for (int length = 0; length < MAX_LENGTH; length += random.nextInt(MAX_LENGTH / 10) + 1) {\r\n        LOG.debug(\"creating; entries = \" + length);\r\n        Writer writer = new OutputStreamWriter(localFs.create(file));\r\n        try {\r\n            for (int i = 0; i < length; i++) {\r\n                writer.write(Integer.toString(i));\r\n                writer.write(\"\\n\");\r\n            }\r\n        } finally {\r\n            writer.close();\r\n        }\r\n        TextInputFormat format = new TextInputFormat();\r\n        format.configure(job);\r\n        LongWritable key = new LongWritable();\r\n        Text value = new Text();\r\n        for (int i = 0; i < 3; i++) {\r\n            int numSplits = random.nextInt(MAX_LENGTH / 20) + 1;\r\n            LOG.debug(\"splitting: requesting = \" + numSplits);\r\n            InputSplit[] splits = format.getSplits(job, numSplits);\r\n            LOG.debug(\"splitting: got =        \" + splits.length);\r\n            if (length == 0) {\r\n                assertEquals(\"Files of length 0 are not returned from FileInputFormat.getSplits().\", 1, splits.length);\r\n                assertEquals(\"Empty file length == 0\", 0, splits[0].getLength());\r\n            }\r\n            BitSet bits = new BitSet(length);\r\n            for (int j = 0; j < splits.length; j++) {\r\n                LOG.debug(\"split[\" + j + \"]= \" + splits[j]);\r\n                RecordReader<LongWritable, Text> reader = format.getRecordReader(splits[j], job, reporter);\r\n                try {\r\n                    int count = 0;\r\n                    while (reader.next(key, value)) {\r\n                        int v = Integer.parseInt(value.toString());\r\n                        LOG.debug(\"read \" + v);\r\n                        if (bits.get(v)) {\r\n                            LOG.warn(\"conflict with \" + v + \" in split \" + j + \" at position \" + reader.getPos());\r\n                        }\r\n                        assertFalse(\"Key in multiple partitions.\", bits.get(v));\r\n                        bits.set(v);\r\n                        count++;\r\n                    }\r\n                    LOG.debug(\"splits[\" + j + \"]=\" + splits[j] + \" count=\" + count);\r\n                } finally {\r\n                    reader.close();\r\n                }\r\n            }\r\n            assertEquals(\"Some keys in no partition.\", length, bits.cardinality());\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 2,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "testSplitableCodecs",
  "errType" : [ "ClassNotFoundException" ],
  "containingMethodsNum" : 17,
  "sourceCodeText" : "void testSplitableCodecs() throws IOException\n{\r\n    JobConf conf = new JobConf(defaultConf);\r\n    int seed = new Random().nextInt();\r\n    CompressionCodec codec = null;\r\n    try {\r\n        codec = (CompressionCodec) ReflectionUtils.newInstance(conf.getClassByName(\"org.apache.hadoop.io.compress.BZip2Codec\"), conf);\r\n    } catch (ClassNotFoundException cnfe) {\r\n        throw new IOException(\"Illegal codec!\");\r\n    }\r\n    Path file = new Path(workDir, \"test\" + codec.getDefaultExtension());\r\n    Reporter reporter = Reporter.NULL;\r\n    LOG.info(\"seed = \" + seed);\r\n    Random random = new Random(seed);\r\n    FileSystem localFs = FileSystem.getLocal(conf);\r\n    localFs.delete(workDir, true);\r\n    FileInputFormat.setInputPaths(conf, workDir);\r\n    final int MAX_LENGTH = 500000;\r\n    for (int length = MAX_LENGTH / 2; length < MAX_LENGTH; length += random.nextInt(MAX_LENGTH / 4) + 1) {\r\n        for (int i = 0; i < 3; i++) {\r\n            int numSplits = random.nextInt(MAX_LENGTH / 2000) + 1;\r\n            verifyPartitions(length, numSplits, file, codec, conf);\r\n        }\r\n    }\r\n    verifyPartitions(471507, 218, file, codec, conf);\r\n    verifyPartitions(473608, 110, file, codec, conf);\r\n    verifyPartitions(100, 20, file, codec, conf);\r\n    verifyPartitions(100, 25, file, codec, conf);\r\n    verifyPartitions(100, 30, file, codec, conf);\r\n    verifyPartitions(100, 50, file, codec, conf);\r\n    verifyPartitions(100, 100, file, codec, conf);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "testSplitableCodecs2",
  "errType" : [ "ClassNotFoundException" ],
  "containingMethodsNum" : 28,
  "sourceCodeText" : "void testSplitableCodecs2() throws IOException\n{\r\n    JobConf conf = new JobConf(defaultConf);\r\n    CompressionCodec codec = null;\r\n    try {\r\n        codec = (CompressionCodec) ReflectionUtils.newInstance(conf.getClassByName(\"org.apache.hadoop.io.compress.BZip2Codec\"), conf);\r\n    } catch (ClassNotFoundException cnfe) {\r\n        throw new IOException(\"Illegal codec!\");\r\n    }\r\n    Path file = new Path(workDir, \"test\" + codec.getDefaultExtension());\r\n    FileSystem localFs = FileSystem.getLocal(conf);\r\n    localFs.delete(workDir, true);\r\n    FileInputFormat.setInputPaths(conf, workDir);\r\n    int length = 250000;\r\n    LOG.info(\"creating; entries = \" + length);\r\n    Writer writer = new OutputStreamWriter(codec.createOutputStream(localFs.create(file)));\r\n    try {\r\n        for (int i = 0; i < length; i++) {\r\n            writer.write(Integer.toString(i));\r\n            writer.write(\"\\n\");\r\n        }\r\n    } finally {\r\n        writer.close();\r\n    }\r\n    for (long splitpos = 203418; splitpos < 203430; ++splitpos) {\r\n        TextInputFormat format = new TextInputFormat();\r\n        format.configure(conf);\r\n        LOG.info(\"setting block size of the input file to \" + splitpos);\r\n        conf.setLong(\"mapreduce.input.fileinputformat.split.minsize\", splitpos);\r\n        LongWritable key = new LongWritable();\r\n        Text value = new Text();\r\n        InputSplit[] splits = format.getSplits(conf, 2);\r\n        LOG.info(\"splitting: got =        \" + splits.length);\r\n        BitSet bits = new BitSet(length);\r\n        for (int j = 0; j < splits.length; j++) {\r\n            LOG.debug(\"split[\" + j + \"]= \" + splits[j]);\r\n            RecordReader<LongWritable, Text> reader = format.getRecordReader(splits[j], conf, Reporter.NULL);\r\n            try {\r\n                int counter = 0;\r\n                while (reader.next(key, value)) {\r\n                    int v = Integer.parseInt(value.toString());\r\n                    LOG.debug(\"read \" + v);\r\n                    if (bits.get(v)) {\r\n                        LOG.warn(\"conflict with \" + v + \" in split \" + j + \" at position \" + reader.getPos());\r\n                    }\r\n                    assertFalse(\"Key in multiple partitions.\", bits.get(v));\r\n                    bits.set(v);\r\n                    counter++;\r\n                }\r\n                if (counter > 0) {\r\n                    LOG.info(\"splits[\" + j + \"]=\" + splits[j] + \" count=\" + counter);\r\n                } else {\r\n                    LOG.debug(\"splits[\" + j + \"]=\" + splits[j] + \" count=\" + counter);\r\n                }\r\n            } finally {\r\n                reader.close();\r\n            }\r\n        }\r\n        assertEquals(\"Some keys in no partition.\", length, bits.cardinality());\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 3,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "verifyPartitions",
  "errType" : null,
  "containingMethodsNum" : 22,
  "sourceCodeText" : "void verifyPartitions(int length, int numSplits, Path file, CompressionCodec codec, JobConf conf) throws IOException\n{\r\n    LOG.info(\"creating; entries = \" + length);\r\n    Writer writer = new OutputStreamWriter(codec.createOutputStream(localFs.create(file)));\r\n    try {\r\n        for (int i = 0; i < length; i++) {\r\n            writer.write(Integer.toString(i));\r\n            writer.write(\"\\n\");\r\n        }\r\n    } finally {\r\n        writer.close();\r\n    }\r\n    TextInputFormat format = new TextInputFormat();\r\n    format.configure(conf);\r\n    LongWritable key = new LongWritable();\r\n    Text value = new Text();\r\n    LOG.info(\"splitting: requesting = \" + numSplits);\r\n    InputSplit[] splits = format.getSplits(conf, numSplits);\r\n    LOG.info(\"splitting: got =        \" + splits.length);\r\n    BitSet bits = new BitSet(length);\r\n    for (int j = 0; j < splits.length; j++) {\r\n        LOG.debug(\"split[\" + j + \"]= \" + splits[j]);\r\n        RecordReader<LongWritable, Text> reader = format.getRecordReader(splits[j], conf, Reporter.NULL);\r\n        try {\r\n            int counter = 0;\r\n            while (reader.next(key, value)) {\r\n                int v = Integer.parseInt(value.toString());\r\n                LOG.debug(\"read \" + v);\r\n                if (bits.get(v)) {\r\n                    LOG.warn(\"conflict with \" + v + \" in split \" + j + \" at position \" + reader.getPos());\r\n                }\r\n                assertFalse(\"Key in multiple partitions.\", bits.get(v));\r\n                bits.set(v);\r\n                counter++;\r\n            }\r\n            if (counter > 0) {\r\n                LOG.info(\"splits[\" + j + \"]=\" + splits[j] + \" count=\" + counter);\r\n            } else {\r\n                LOG.debug(\"splits[\" + j + \"]=\" + splits[j] + \" count=\" + counter);\r\n            }\r\n        } finally {\r\n            reader.close();\r\n        }\r\n    }\r\n    assertEquals(\"Some keys in no partition.\", length, bits.cardinality());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 2,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "makeStream",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "LineReader makeStream(String str) throws IOException\n{\r\n    return new LineReader(new ByteArrayInputStream(str.getBytes(\"UTF-8\")), defaultConf);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "makeStream",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "LineReader makeStream(String str, int bufsz) throws IOException\n{\r\n    return new LineReader(new ByteArrayInputStream(str.getBytes(\"UTF-8\")), bufsz);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "testUTF8",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void testUTF8() throws Exception\n{\r\n    LineReader in = makeStream(\"abcd\\u20acbdcd\\u20ac\");\r\n    Text line = new Text();\r\n    in.readLine(line);\r\n    assertEquals(\"readLine changed utf8 characters\", \"abcd\\u20acbdcd\\u20ac\", line.toString());\r\n    in = makeStream(\"abc\\u200axyz\");\r\n    in.readLine(line);\r\n    assertEquals(\"split on fake newline\", \"abc\\u200axyz\", line.toString());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "testNewLines",
  "errType" : null,
  "containingMethodsNum" : 22,
  "sourceCodeText" : "void testNewLines() throws Exception\n{\r\n    final String STR = \"a\\nbb\\n\\nccc\\rdddd\\r\\r\\r\\n\\r\\neeeee\";\r\n    final int STRLENBYTES = STR.getBytes().length;\r\n    Text out = new Text();\r\n    for (int bufsz = 1; bufsz < STRLENBYTES + 1; ++bufsz) {\r\n        LineReader in = makeStream(STR, bufsz);\r\n        int c = 0;\r\n        c += in.readLine(out);\r\n        assertEquals(\"line1 length, bufsz:\" + bufsz, 1, out.getLength());\r\n        c += in.readLine(out);\r\n        assertEquals(\"line2 length, bufsz:\" + bufsz, 2, out.getLength());\r\n        c += in.readLine(out);\r\n        assertEquals(\"line3 length, bufsz:\" + bufsz, 0, out.getLength());\r\n        c += in.readLine(out);\r\n        assertEquals(\"line4 length, bufsz:\" + bufsz, 3, out.getLength());\r\n        c += in.readLine(out);\r\n        assertEquals(\"line5 length, bufsz:\" + bufsz, 4, out.getLength());\r\n        c += in.readLine(out);\r\n        assertEquals(\"line6 length, bufsz:\" + bufsz, 0, out.getLength());\r\n        c += in.readLine(out);\r\n        assertEquals(\"line7 length, bufsz:\" + bufsz, 0, out.getLength());\r\n        c += in.readLine(out);\r\n        assertEquals(\"line8 length, bufsz:\" + bufsz, 0, out.getLength());\r\n        c += in.readLine(out);\r\n        assertEquals(\"line9 length, bufsz:\" + bufsz, 5, out.getLength());\r\n        assertEquals(\"end of file, bufsz: \" + bufsz, 0, in.readLine(out));\r\n        assertEquals(\"total bytes, bufsz: \" + bufsz, c, STRLENBYTES);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "testMaxLineLength",
  "errType" : null,
  "containingMethodsNum" : 16,
  "sourceCodeText" : "void testMaxLineLength() throws Exception\n{\r\n    final String STR = \"a\\nbb\\n\\nccc\\rdddd\\r\\neeeee\";\r\n    final int STRLENBYTES = STR.getBytes().length;\r\n    Text out = new Text();\r\n    for (int bufsz = 1; bufsz < STRLENBYTES + 1; ++bufsz) {\r\n        LineReader in = makeStream(STR, bufsz);\r\n        int c = 0;\r\n        c += in.readLine(out, 1);\r\n        assertEquals(\"line1 length, bufsz: \" + bufsz, 1, out.getLength());\r\n        c += in.readLine(out, 1);\r\n        assertEquals(\"line2 length, bufsz: \" + bufsz, 1, out.getLength());\r\n        c += in.readLine(out, 1);\r\n        assertEquals(\"line3 length, bufsz: \" + bufsz, 0, out.getLength());\r\n        c += in.readLine(out, 3);\r\n        assertEquals(\"line4 length, bufsz: \" + bufsz, 3, out.getLength());\r\n        c += in.readLine(out, 10);\r\n        assertEquals(\"line5 length, bufsz: \" + bufsz, 4, out.getLength());\r\n        c += in.readLine(out, 8);\r\n        assertEquals(\"line5 length, bufsz: \" + bufsz, 5, out.getLength());\r\n        assertEquals(\"end of file, bufsz: \" + bufsz, 0, in.readLine(out));\r\n        assertEquals(\"total bytes, bufsz: \" + bufsz, c, STRLENBYTES);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "testMRMaxLine",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void testMRMaxLine() throws Exception\n{\r\n    final int MAXPOS = 1024 * 1024;\r\n    final int MAXLINE = 10 * 1024;\r\n    final int BUF = 64 * 1024;\r\n    final InputStream infNull = new InputStream() {\r\n\r\n        int position = 0;\r\n\r\n        final int MAXPOSBUF = 1024 * 1024 + BUF;\r\n\r\n        @Override\r\n        public int read() {\r\n            ++position;\r\n            return 0;\r\n        }\r\n\r\n        @Override\r\n        public int read(byte[] b) {\r\n            assertTrue(\"Read too many bytes from the stream\", position < MAXPOSBUF);\r\n            Arrays.fill(b, (byte) 0);\r\n            position += b.length;\r\n            return b.length;\r\n        }\r\n\r\n        public void reset() {\r\n            position = 0;\r\n        }\r\n    };\r\n    final LongWritable key = new LongWritable();\r\n    final Text val = new Text();\r\n    LOG.info(\"Reading a line from /dev/null\");\r\n    final Configuration conf = new Configuration(false);\r\n    conf.setInt(org.apache.hadoop.mapreduce.lib.input.LineRecordReader.MAX_LINE_LENGTH, MAXLINE);\r\n    conf.setInt(\"io.file.buffer.size\", BUF);\r\n    LineRecordReader lrr = new LineRecordReader(infNull, 0, MAXPOS, conf);\r\n    assertFalse(\"Read a line from null\", lrr.next(key, val));\r\n    infNull.reset();\r\n    lrr = new LineRecordReader(infNull, 0L, MAXLINE, MAXPOS);\r\n    assertFalse(\"Read a line from null\", lrr.next(key, val));\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "writeFile",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void writeFile(FileSystem fs, Path name, CompressionCodec codec, String contents) throws IOException\n{\r\n    OutputStream stm;\r\n    if (codec == null) {\r\n        stm = fs.create(name);\r\n    } else {\r\n        stm = codec.createOutputStream(fs.create(name));\r\n    }\r\n    stm.write(contents.getBytes());\r\n    stm.close();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "readSplit",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "List<Text> readSplit(TextInputFormat format, InputSplit split, JobConf job) throws IOException\n{\r\n    List<Text> result = new ArrayList<Text>();\r\n    RecordReader<LongWritable, Text> reader = format.getRecordReader(split, job, voidReporter);\r\n    LongWritable key = reader.createKey();\r\n    Text value = reader.createValue();\r\n    while (reader.next(key, value)) {\r\n        result.add(value);\r\n        value = reader.createValue();\r\n    }\r\n    reader.close();\r\n    return result;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "testGzip",
  "errType" : null,
  "containingMethodsNum" : 16,
  "sourceCodeText" : "void testGzip() throws IOException\n{\r\n    JobConf job = new JobConf(defaultConf);\r\n    CompressionCodec gzip = new GzipCodec();\r\n    ReflectionUtils.setConf(gzip, job);\r\n    localFs.delete(workDir, true);\r\n    writeFile(localFs, new Path(workDir, \"part1.txt.gz\"), gzip, \"the quick\\nbrown\\nfox jumped\\nover\\n the lazy\\n dog\\n\");\r\n    writeFile(localFs, new Path(workDir, \"part2.txt.gz\"), gzip, \"this is a test\\nof gzip\\n\");\r\n    FileInputFormat.setInputPaths(job, workDir);\r\n    TextInputFormat format = new TextInputFormat();\r\n    format.configure(job);\r\n    InputSplit[] splits = format.getSplits(job, 100);\r\n    assertEquals(\"compressed splits == 2\", 2, splits.length);\r\n    FileSplit tmp = (FileSplit) splits[0];\r\n    if (tmp.getPath().getName().equals(\"part2.txt.gz\")) {\r\n        splits[0] = splits[1];\r\n        splits[1] = tmp;\r\n    }\r\n    List<Text> results = readSplit(format, splits[0], job);\r\n    assertEquals(\"splits[0] length\", 6, results.size());\r\n    assertEquals(\"splits[0][5]\", \" dog\", results.get(5).toString());\r\n    results = readSplit(format, splits[1], job);\r\n    assertEquals(\"splits[1] length\", 2, results.size());\r\n    assertEquals(\"splits[1][0]\", \"this is a test\", results.get(0).toString());\r\n    assertEquals(\"splits[1][1]\", \"of gzip\", results.get(1).toString());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "testGzipEmpty",
  "errType" : null,
  "containingMethodsNum" : 9,
  "sourceCodeText" : "void testGzipEmpty() throws IOException\n{\r\n    JobConf job = new JobConf(defaultConf);\r\n    CompressionCodec gzip = new GzipCodec();\r\n    ReflectionUtils.setConf(gzip, job);\r\n    localFs.delete(workDir, true);\r\n    writeFile(localFs, new Path(workDir, \"empty.gz\"), gzip, \"\");\r\n    FileInputFormat.setInputPaths(job, workDir);\r\n    TextInputFormat format = new TextInputFormat();\r\n    format.configure(job);\r\n    InputSplit[] splits = format.getSplits(job, 100);\r\n    assertEquals(\"Compressed files of length 0 are not returned from FileInputFormat.getSplits().\", 1, splits.length);\r\n    List<Text> results = readSplit(format, splits[0], job);\r\n    assertEquals(\"Compressed empty file length == 0\", 0, results.size());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "unquote",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "String unquote(String in)\n{\r\n    StringBuffer result = new StringBuffer();\r\n    for (int i = 0; i < in.length(); ++i) {\r\n        char ch = in.charAt(i);\r\n        if (ch == '\\\\') {\r\n            ch = in.charAt(++i);\r\n            switch(ch) {\r\n                case 'n':\r\n                    result.append('\\n');\r\n                    break;\r\n                case 'r':\r\n                    result.append('\\r');\r\n                    break;\r\n                default:\r\n                    result.append(ch);\r\n                    break;\r\n            }\r\n        } else {\r\n            result.append(ch);\r\n        }\r\n    }\r\n    return result.toString();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "main",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void main(String[] args) throws Exception\n{\r\n    for (String arg : args) {\r\n        System.out.println(\"Working on \" + arg);\r\n        LineReader reader = makeStream(unquote(arg));\r\n        Text line = new Text();\r\n        int size = reader.readLine(line);\r\n        while (size > 0) {\r\n            System.out.println(\"Got: \" + line.toString());\r\n            size = reader.readLine(line);\r\n        }\r\n        reader.close();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "startHttpServer",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void startHttpServer() throws Exception\n{\r\n    if (webServer != null) {\r\n        webServer.stop();\r\n        webServer = null;\r\n    }\r\n    webServer = new Server(0);\r\n    ServletContextHandler context = new ServletContextHandler(webServer, contextPath);\r\n    context.addServlet(new ServletHolder(new NotificationServlet()), servletPath);\r\n    webServer.start();\r\n    port = ((ServerConnector) webServer.getConnectors()[0]).getLocalPort();\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "stopHttpServer",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void stopHttpServer() throws Exception\n{\r\n    if (webServer != null) {\r\n        webServer.stop();\r\n        webServer.destroy();\r\n        webServer = null;\r\n    }\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getNotificationUrlTemplate",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getNotificationUrlTemplate()\n{\r\n    return \"http://localhost:\" + port + contextPath + servletPath + \"?jobId=$jobId&amp;jobStatus=$jobStatus\";\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "createJobConf",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "JobConf createJobConf()\n{\r\n    JobConf conf = super.createJobConf();\r\n    conf.setJobEndNotificationURI(getNotificationUrlTemplate());\r\n    conf.setInt(JobContext.MR_JOB_END_RETRY_ATTEMPTS, 3);\r\n    conf.setInt(JobContext.MR_JOB_END_RETRY_INTERVAL, 200);\r\n    return conf;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "setUp",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void setUp() throws Exception\n{\r\n    super.setUp();\r\n    startHttpServer();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "tearDown",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void tearDown() throws Exception\n{\r\n    stopHttpServer();\r\n    super.tearDown();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "testMR",
  "errType" : null,
  "containingMethodsNum" : 14,
  "sourceCodeText" : "void testMR() throws Exception\n{\r\n    System.out.println(launchWordCount(this.createJobConf(), \"a b c d e f g h\", 1, 1));\r\n    boolean keepTrying = true;\r\n    for (int tries = 0; tries < 30 && keepTrying; tries++) {\r\n        Thread.sleep(50);\r\n        keepTrying = !(NotificationServlet.counter == 2);\r\n    }\r\n    assertEquals(2, NotificationServlet.counter);\r\n    assertEquals(0, NotificationServlet.failureCounter);\r\n    Path inDir = new Path(\"notificationjob/input\");\r\n    Path outDir = new Path(\"notificationjob/output\");\r\n    if (isLocalFS()) {\r\n        String localPathRoot = System.getProperty(\"test.build.data\", \"/tmp\").toString().replace(' ', '+');\r\n        ;\r\n        inDir = new Path(localPathRoot, inDir);\r\n        outDir = new Path(localPathRoot, outDir);\r\n    }\r\n    System.out.println(UtilsForTests.runJobKill(this.createJobConf(), inDir, outDir).getID());\r\n    keepTrying = true;\r\n    for (int tries = 0; tries < 30 && keepTrying; tries++) {\r\n        Thread.sleep(50);\r\n        keepTrying = !(NotificationServlet.counter == 4);\r\n    }\r\n    assertEquals(4, NotificationServlet.counter);\r\n    assertEquals(0, NotificationServlet.failureCounter);\r\n    System.out.println(UtilsForTests.runJobFail(this.createJobConf(), inDir, outDir).getID());\r\n    keepTrying = true;\r\n    for (int tries = 0; tries < 30 && keepTrying; tries++) {\r\n        Thread.sleep(50);\r\n        keepTrying = !(NotificationServlet.counter == 6);\r\n    }\r\n    assertEquals(6, NotificationServlet.counter);\r\n    assertEquals(0, NotificationServlet.failureCounter);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "launchWordCount",
  "errType" : null,
  "containingMethodsNum" : 22,
  "sourceCodeText" : "String launchWordCount(JobConf conf, String input, int numMaps, int numReduces) throws IOException\n{\r\n    Path inDir = new Path(\"testing/wc/input\");\r\n    Path outDir = new Path(\"testing/wc/output\");\r\n    if (isLocalFS()) {\r\n        String localPathRoot = System.getProperty(\"test.build.data\", \"/tmp\").toString().replace(' ', '+');\r\n        ;\r\n        inDir = new Path(localPathRoot, inDir);\r\n        outDir = new Path(localPathRoot, outDir);\r\n    }\r\n    FileSystem fs = FileSystem.get(conf);\r\n    fs.delete(outDir, true);\r\n    if (!fs.mkdirs(inDir)) {\r\n        throw new IOException(\"Mkdirs failed to create \" + inDir.toString());\r\n    }\r\n    {\r\n        DataOutputStream file = fs.create(new Path(inDir, \"part-0\"));\r\n        file.writeBytes(input);\r\n        file.close();\r\n    }\r\n    conf.setJobName(\"wordcount\");\r\n    conf.setInputFormat(TextInputFormat.class);\r\n    conf.setOutputKeyClass(Text.class);\r\n    conf.setOutputValueClass(IntWritable.class);\r\n    conf.setMapperClass(WordCount.MapClass.class);\r\n    conf.setCombinerClass(WordCount.Reduce.class);\r\n    conf.setReducerClass(WordCount.Reduce.class);\r\n    FileInputFormat.setInputPaths(conf, inDir);\r\n    FileOutputFormat.setOutputPath(conf, outDir);\r\n    conf.setNumMapTasks(numMaps);\r\n    conf.setNumReduceTasks(numReduces);\r\n    JobClient.runJob(conf);\r\n    return MapReduceTestUtil.readOutput(outDir, conf);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "test0",
  "errType" : null,
  "containingMethodsNum" : 15,
  "sourceCodeText" : "int test0(IntWritable key, MarkableIterator<IntWritable> values) throws IOException\n{\r\n    int errors = 0;\r\n    IntWritable i;\r\n    ArrayList<IntWritable> expectedValues = new ArrayList<IntWritable>();\r\n    LOG.info(\"Executing TEST:0 for Key:\" + key.toString());\r\n    values.mark();\r\n    LOG.info(\"TEST:0. Marking\");\r\n    while (values.hasNext()) {\r\n        i = values.next();\r\n        expectedValues.add(i);\r\n        LOG.info(key + \":\" + i);\r\n    }\r\n    values.reset();\r\n    LOG.info(\"TEST:0. Reset\");\r\n    int count = 0;\r\n    while (values.hasNext()) {\r\n        i = values.next();\r\n        LOG.info(key + \":\" + i);\r\n        if (i != expectedValues.get(count)) {\r\n            LOG.info(\"TEST:0. Check:1 Expected: \" + expectedValues.get(count) + \", Got: \" + i);\r\n            errors++;\r\n            return errors;\r\n        }\r\n        count++;\r\n    }\r\n    LOG.info(\"TEST:0 Done\");\r\n    return errors;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "test1",
  "errType" : null,
  "containingMethodsNum" : 42,
  "sourceCodeText" : "int test1(IntWritable key, MarkableIterator<IntWritable> values) throws IOException\n{\r\n    IntWritable i;\r\n    int errors = 0;\r\n    int count = 0;\r\n    ArrayList<IntWritable> expectedValues = new ArrayList<IntWritable>();\r\n    ArrayList<IntWritable> expectedValues1 = new ArrayList<IntWritable>();\r\n    LOG.info(\"Executing TEST:1 for Key:\" + key);\r\n    values.mark();\r\n    LOG.info(\"TEST:1. Marking\");\r\n    while (values.hasNext()) {\r\n        i = values.next();\r\n        LOG.info(key + \":\" + i);\r\n        expectedValues.add(i);\r\n        if (count == 2) {\r\n            break;\r\n        }\r\n        count++;\r\n    }\r\n    values.reset();\r\n    LOG.info(\"TEST:1. Reset\");\r\n    count = 0;\r\n    while (values.hasNext()) {\r\n        i = values.next();\r\n        LOG.info(key + \":\" + i);\r\n        if (count < expectedValues.size()) {\r\n            if (i != expectedValues.get(count)) {\r\n                errors++;\r\n                LOG.info(\"TEST:1. Check:1 Expected: \" + expectedValues.get(count) + \", Got: \" + i);\r\n                return errors;\r\n            }\r\n        }\r\n        if (count == 3) {\r\n            values.mark();\r\n            LOG.info(\"TEST:1. Marking -- \" + key + \": \" + i);\r\n        }\r\n        if (count >= 3) {\r\n            expectedValues1.add(i);\r\n        }\r\n        if (count == 5) {\r\n            break;\r\n        }\r\n        count++;\r\n    }\r\n    if (count < expectedValues.size()) {\r\n        LOG.info((\"TEST:1 Check:2. Iterator returned lesser values\"));\r\n        errors++;\r\n        return errors;\r\n    }\r\n    values.reset();\r\n    count = 0;\r\n    LOG.info(\"TEST:1. Reset\");\r\n    expectedValues.clear();\r\n    while (values.hasNext()) {\r\n        i = values.next();\r\n        LOG.info(key + \":\" + i);\r\n        if (count < expectedValues1.size()) {\r\n            if (i != expectedValues1.get(count)) {\r\n                errors++;\r\n                LOG.info(\"TEST:1. Check:3 Expected: \" + expectedValues1.get(count) + \", Got: \" + i);\r\n                return errors;\r\n            }\r\n        }\r\n        if (count == 25) {\r\n            values.mark();\r\n            LOG.info(\"TEST:1. Marking -- \" + key + \":\" + i);\r\n        }\r\n        if (count >= 25) {\r\n            expectedValues.add(i);\r\n        }\r\n        count++;\r\n    }\r\n    if (count < expectedValues1.size()) {\r\n        LOG.info((\"TEST:1 Check:4. Iterator returned fewer values\"));\r\n        errors++;\r\n        return errors;\r\n    }\r\n    values.reset();\r\n    LOG.info(\"TEST:1. Reset\");\r\n    count = 0;\r\n    while (values.hasNext()) {\r\n        i = values.next();\r\n        LOG.info(key + \":\" + i);\r\n        if (i != expectedValues.get(count)) {\r\n            errors++;\r\n            LOG.info(\"TEST:1. Check:5 Expected: \" + expectedValues.get(count) + \", Got: \" + i);\r\n            return errors;\r\n        }\r\n    }\r\n    LOG.info(\"TEST:1 Done\");\r\n    return errors;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "test2",
  "errType" : null,
  "containingMethodsNum" : 38,
  "sourceCodeText" : "int test2(IntWritable key, MarkableIterator<IntWritable> values) throws IOException\n{\r\n    IntWritable i;\r\n    int errors = 0;\r\n    int count = 0;\r\n    ArrayList<IntWritable> expectedValues = new ArrayList<IntWritable>();\r\n    ArrayList<IntWritable> expectedValues1 = new ArrayList<IntWritable>();\r\n    LOG.info(\"Executing TEST:2 for Key:\" + key);\r\n    values.mark();\r\n    LOG.info(\"TEST:2 Marking\");\r\n    while (values.hasNext()) {\r\n        i = values.next();\r\n        LOG.info(key + \":\" + i);\r\n        expectedValues.add(i);\r\n        if (count == 8) {\r\n            break;\r\n        }\r\n        count++;\r\n    }\r\n    values.reset();\r\n    count = 0;\r\n    LOG.info(\"TEST:2 reset\");\r\n    while (values.hasNext()) {\r\n        i = values.next();\r\n        LOG.info(key + \":\" + i);\r\n        if (count < expectedValues.size()) {\r\n            if (i != expectedValues.get(count)) {\r\n                errors++;\r\n                LOG.info(\"TEST:2. Check:1 Expected: \" + expectedValues.get(count) + \", Got: \" + i);\r\n                return errors;\r\n            }\r\n        }\r\n        if (count == 3) {\r\n            values.mark();\r\n            LOG.info(\"TEST:2. Marking -- \" + key + \":\" + i);\r\n        }\r\n        if (count >= 3) {\r\n            expectedValues1.add(i);\r\n        }\r\n        count++;\r\n    }\r\n    values.reset();\r\n    LOG.info(\"TEST:2. Reset\");\r\n    expectedValues.clear();\r\n    count = 0;\r\n    while (values.hasNext()) {\r\n        i = values.next();\r\n        LOG.info(key + \":\" + i);\r\n        if (count < expectedValues1.size()) {\r\n            if (i != expectedValues1.get(count)) {\r\n                errors++;\r\n                LOG.info(\"TEST:2. Check:2 Expected: \" + expectedValues1.get(count) + \", Got: \" + i);\r\n                return errors;\r\n            }\r\n        }\r\n        if (count == 20) {\r\n            values.mark();\r\n            LOG.info(\"TEST:2. Marking -- \" + key + \":\" + i);\r\n        }\r\n        if (count >= 20) {\r\n            expectedValues.add(i);\r\n        }\r\n        count++;\r\n    }\r\n    values.reset();\r\n    count = 0;\r\n    LOG.info(\"TEST:2. Reset\");\r\n    while (values.hasNext()) {\r\n        i = values.next();\r\n        LOG.info(key + \":\" + i);\r\n        if (i != expectedValues.get(count)) {\r\n            errors++;\r\n            LOG.info(\"TEST:2. Check:1 Expected: \" + expectedValues.get(count) + \", Got: \" + i);\r\n            return errors;\r\n        }\r\n    }\r\n    LOG.info(\"TEST:2 Done\");\r\n    return errors;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "test3",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 27,
  "sourceCodeText" : "int test3(IntWritable key, MarkableIterator<IntWritable> values) throws IOException\n{\r\n    int errors = 0;\r\n    IntWritable i;\r\n    ArrayList<IntWritable> expectedValues = new ArrayList<IntWritable>();\r\n    LOG.info(\"Executing TEST:3 for Key:\" + key);\r\n    values.mark();\r\n    LOG.info(\"TEST:3. Marking\");\r\n    int count = 0;\r\n    while (values.hasNext()) {\r\n        i = values.next();\r\n        ;\r\n        LOG.info(key + \":\" + i);\r\n        if (count == 5) {\r\n            LOG.info(\"TEST:3. Clearing Mark\");\r\n            values.clearMark();\r\n        }\r\n        if (count == 8) {\r\n            LOG.info(\"TEST:3. Marking -- \" + key + \":\" + i);\r\n            values.mark();\r\n        }\r\n        if (count >= 8) {\r\n            expectedValues.add(i);\r\n        }\r\n        count++;\r\n    }\r\n    values.reset();\r\n    LOG.info(\"TEST:3. After reset\");\r\n    if (!values.hasNext()) {\r\n        errors++;\r\n        LOG.info(\"TEST:3, Check:1. HasNext returned false\");\r\n        return errors;\r\n    }\r\n    count = 0;\r\n    while (values.hasNext()) {\r\n        i = values.next();\r\n        LOG.info(key + \":\" + i);\r\n        if (count < expectedValues.size()) {\r\n            if (i != expectedValues.get(count)) {\r\n                errors++;\r\n                LOG.info(\"TEST:2. Check:1 Expected: \" + expectedValues.get(count) + \", Got: \" + i);\r\n                return errors;\r\n            }\r\n        }\r\n        if (count == 10) {\r\n            values.clearMark();\r\n            LOG.info(\"TEST:3. After clear mark\");\r\n        }\r\n        count++;\r\n    }\r\n    boolean successfulClearMark = false;\r\n    try {\r\n        LOG.info(\"TEST:3. Before Reset\");\r\n        values.reset();\r\n    } catch (IOException e) {\r\n        successfulClearMark = true;\r\n    }\r\n    if (!successfulClearMark) {\r\n        LOG.info(\"TEST:3 Check:4 reset was successfule even after clearMark\");\r\n        errors++;\r\n        return errors;\r\n    }\r\n    LOG.info(\"TEST:3 Done.\");\r\n    return errors;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "createInput",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void createInput() throws Exception\n{\r\n    for (int i = 0; i < NUM_MAPS; i++) {\r\n        Path file = new Path(TEST_ROOT_DIR + \"/in\", \"test\" + i + \".txt\");\r\n        localFs.delete(file, false);\r\n        OutputStream os = localFs.create(file);\r\n        Writer wr = new OutputStreamWriter(os);\r\n        wr.write(\"dummy\");\r\n        wr.close();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "testValueIterReset",
  "errType" : [ "Exception" ],
  "containingMethodsNum" : 20,
  "sourceCodeText" : "void testValueIterReset()\n{\r\n    try {\r\n        Configuration conf = new Configuration();\r\n        Job job = Job.getInstance(conf, \"TestValueIterReset\");\r\n        job.setJarByClass(TestValueIterReset.class);\r\n        job.setMapperClass(TestMapper.class);\r\n        job.setReducerClass(TestReducer.class);\r\n        job.setNumReduceTasks(NUM_TESTS);\r\n        job.setMapOutputKeyClass(IntWritable.class);\r\n        job.setMapOutputValueClass(IntWritable.class);\r\n        job.setOutputKeyClass(IntWritable.class);\r\n        job.setOutputValueClass(IntWritable.class);\r\n        job.getConfiguration().setInt(MRJobConfig.REDUCE_MARKRESET_BUFFER_SIZE, 128);\r\n        job.setInputFormatClass(TextInputFormat.class);\r\n        job.setOutputFormatClass(TextOutputFormat.class);\r\n        FileInputFormat.addInputPath(job, new Path(TEST_ROOT_DIR + \"/in\"));\r\n        Path output = new Path(TEST_ROOT_DIR + \"/out\");\r\n        localFs.delete(output, true);\r\n        FileOutputFormat.setOutputPath(job, output);\r\n        createInput();\r\n        assertTrue(job.waitForCompletion(true));\r\n        validateOutput();\r\n    } catch (Exception e) {\r\n        e.printStackTrace();\r\n        assertTrue(false);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "validateOutput",
  "errType" : null,
  "containingMethodsNum" : 10,
  "sourceCodeText" : "void validateOutput() throws IOException\n{\r\n    Path[] outputFiles = FileUtil.stat2Paths(localFs.listStatus(new Path(TEST_ROOT_DIR + \"/out\"), new Utils.OutputFileUtils.OutputFilesFilter()));\r\n    if (outputFiles.length > 0) {\r\n        InputStream is = localFs.open(outputFiles[0]);\r\n        BufferedReader reader = new BufferedReader(new InputStreamReader(is));\r\n        String line = reader.readLine();\r\n        while (line != null) {\r\n            StringTokenizer tokeniz = new StringTokenizer(line, \"\\t\");\r\n            String key = tokeniz.nextToken();\r\n            String value = tokeniz.nextToken();\r\n            LOG.info(\"Output: key: \" + key + \" value: \" + value);\r\n            int errors = Integer.parseInt(value);\r\n            assertTrue(errors == 0);\r\n            line = reader.readLine();\r\n        }\r\n        reader.close();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\fs",
  "methodName" : "testIOs",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testIOs() throws Exception\n{\r\n    testIOs(10, 10);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\fs",
  "methodName" : "testIOs",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void testIOs(int fileSize, int nrFiles) throws IOException\n{\r\n    FileSystem fs = FileSystem.get(fsConfig);\r\n    createControlFile(fs, fileSize, nrFiles);\r\n    writeTest(fs);\r\n    readTest(fs);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\fs",
  "methodName" : "createControlFile",
  "errType" : [ "Exception" ],
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void createControlFile(FileSystem fs, int fileSize, int nrFiles) throws IOException\n{\r\n    LOG.info(\"creating control file: \" + fileSize + \" mega bytes, \" + nrFiles + \" files\");\r\n    fs.delete(CONTROL_DIR, true);\r\n    for (int i = 0; i < nrFiles; i++) {\r\n        String name = getFileName(i);\r\n        Path controlFile = new Path(CONTROL_DIR, \"in_file_\" + name);\r\n        SequenceFile.Writer writer = null;\r\n        try {\r\n            writer = SequenceFile.createWriter(fs, fsConfig, controlFile, Text.class, LongWritable.class, CompressionType.NONE);\r\n            writer.append(new Text(name), new LongWritable(fileSize));\r\n        } catch (Exception e) {\r\n            throw new IOException(e.getLocalizedMessage());\r\n        } finally {\r\n            if (writer != null)\r\n                writer.close();\r\n            writer = null;\r\n        }\r\n    }\r\n    LOG.info(\"created control files for: \" + nrFiles + \" files\");\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\fs",
  "methodName" : "getFileName",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String getFileName(int fIdx)\n{\r\n    return BASE_FILE_NAME + Integer.toString(fIdx);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\fs",
  "methodName" : "writeTest",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void writeTest(FileSystem fs) throws IOException\n{\r\n    fs.delete(DATA_DIR, true);\r\n    fs.delete(WRITE_DIR, true);\r\n    runIOTest(WriteMapper.class, WRITE_DIR);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\fs",
  "methodName" : "runIOTest",
  "errType" : null,
  "containingMethodsNum" : 9,
  "sourceCodeText" : "void runIOTest(Class<? extends Mapper> mapperClass, Path outputDir) throws IOException\n{\r\n    JobConf job = new JobConf(fsConfig, DFSCIOTest.class);\r\n    FileInputFormat.setInputPaths(job, CONTROL_DIR);\r\n    job.setInputFormat(SequenceFileInputFormat.class);\r\n    job.setMapperClass(mapperClass);\r\n    job.setReducerClass(AccumulatingReducer.class);\r\n    FileOutputFormat.setOutputPath(job, outputDir);\r\n    job.setOutputKeyClass(Text.class);\r\n    job.setOutputValueClass(Text.class);\r\n    job.setNumReduceTasks(1);\r\n    JobClient.runJob(job);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\fs",
  "methodName" : "readTest",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void readTest(FileSystem fs) throws IOException\n{\r\n    fs.delete(READ_DIR, true);\r\n    runIOTest(ReadMapper.class, READ_DIR);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\fs",
  "methodName" : "sequentialTest",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void sequentialTest(FileSystem fs, int testType, int fileSize, int nrFiles) throws Exception\n{\r\n    IOStatMapper ioer = null;\r\n    if (testType == TEST_TYPE_READ)\r\n        ioer = new ReadMapper();\r\n    else if (testType == TEST_TYPE_WRITE)\r\n        ioer = new WriteMapper();\r\n    else\r\n        return;\r\n    for (int i = 0; i < nrFiles; i++) ioer.doIO(Reporter.NULL, BASE_FILE_NAME + Integer.toString(i), MEGA * fileSize);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\fs",
  "methodName" : "main",
  "errType" : [ "Exception" ],
  "containingMethodsNum" : 39,
  "sourceCodeText" : "void main(String[] args)\n{\r\n    int testType = TEST_TYPE_READ;\r\n    int bufferSize = DEFAULT_BUFFER_SIZE;\r\n    int fileSize = 1;\r\n    int nrFiles = 1;\r\n    String resFileName = DEFAULT_RES_FILE_NAME;\r\n    boolean isSequential = false;\r\n    String version = \"DFSCIOTest.0.0.1\";\r\n    String usage = \"Usage: DFSCIOTest -read | -write | -clean [-nrFiles N] [-fileSize MB] [-resFile resultFileName] [-bufferSize Bytes] \";\r\n    System.out.println(version);\r\n    if (args.length == 0) {\r\n        System.err.println(usage);\r\n        System.exit(-1);\r\n    }\r\n    for (int i = 0; i < args.length; i++) {\r\n        if (args[i].startsWith(\"-r\")) {\r\n            testType = TEST_TYPE_READ;\r\n        } else if (args[i].startsWith(\"-w\")) {\r\n            testType = TEST_TYPE_WRITE;\r\n        } else if (args[i].startsWith(\"-clean\")) {\r\n            testType = TEST_TYPE_CLEANUP;\r\n        } else if (args[i].startsWith(\"-seq\")) {\r\n            isSequential = true;\r\n        } else if (args[i].equals(\"-nrFiles\")) {\r\n            nrFiles = Integer.parseInt(args[++i]);\r\n        } else if (args[i].equals(\"-fileSize\")) {\r\n            fileSize = Integer.parseInt(args[++i]);\r\n        } else if (args[i].equals(\"-bufferSize\")) {\r\n            bufferSize = Integer.parseInt(args[++i]);\r\n        } else if (args[i].equals(\"-resFile\")) {\r\n            resFileName = args[++i];\r\n        }\r\n    }\r\n    LOG.info(\"nrFiles = \" + nrFiles);\r\n    LOG.info(\"fileSize (MB) = \" + fileSize);\r\n    LOG.info(\"bufferSize = \" + bufferSize);\r\n    try {\r\n        fsConfig.setInt(\"test.io.file.buffer.size\", bufferSize);\r\n        FileSystem fs = FileSystem.get(fsConfig);\r\n        if (testType != TEST_TYPE_CLEANUP) {\r\n            fs.delete(HDFS_TEST_DIR, true);\r\n            if (!fs.mkdirs(HDFS_TEST_DIR)) {\r\n                throw new IOException(\"Mkdirs failed to create \" + HDFS_TEST_DIR.toString());\r\n            }\r\n            String hadoopHome = System.getenv(\"HADOOP_HOME\");\r\n            fs.copyFromLocalFile(new Path(hadoopHome + \"/libhdfs/libhdfs.so.\" + HDFS_LIB_VERSION), HDFS_SHLIB);\r\n            fs.copyFromLocalFile(new Path(hadoopHome + \"/libhdfs/hdfs_read\"), HDFS_READ);\r\n            fs.copyFromLocalFile(new Path(hadoopHome + \"/libhdfs/hdfs_write\"), HDFS_WRITE);\r\n        }\r\n        if (isSequential) {\r\n            long tStart = System.currentTimeMillis();\r\n            sequentialTest(fs, testType, fileSize, nrFiles);\r\n            long execTime = System.currentTimeMillis() - tStart;\r\n            String resultLine = \"Seq Test exec time sec: \" + (float) execTime / 1000;\r\n            LOG.info(resultLine);\r\n            return;\r\n        }\r\n        if (testType == TEST_TYPE_CLEANUP) {\r\n            cleanup(fs);\r\n            return;\r\n        }\r\n        createControlFile(fs, fileSize, nrFiles);\r\n        long tStart = System.currentTimeMillis();\r\n        if (testType == TEST_TYPE_WRITE)\r\n            writeTest(fs);\r\n        if (testType == TEST_TYPE_READ)\r\n            readTest(fs);\r\n        long execTime = System.currentTimeMillis() - tStart;\r\n        analyzeResult(fs, testType, execTime, resFileName);\r\n    } catch (Exception e) {\r\n        System.err.print(e.getLocalizedMessage());\r\n        System.exit(-1);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\fs",
  "methodName" : "analyzeResult",
  "errType" : null,
  "containingMethodsNum" : 17,
  "sourceCodeText" : "void analyzeResult(FileSystem fs, int testType, long execTime, String resFileName) throws IOException\n{\r\n    Path reduceFile;\r\n    if (testType == TEST_TYPE_WRITE)\r\n        reduceFile = new Path(WRITE_DIR, \"part-00000\");\r\n    else\r\n        reduceFile = new Path(READ_DIR, \"part-00000\");\r\n    DataInputStream in;\r\n    in = new DataInputStream(fs.open(reduceFile));\r\n    BufferedReader lines;\r\n    lines = new BufferedReader(new InputStreamReader(in));\r\n    long tasks = 0;\r\n    long size = 0;\r\n    long time = 0;\r\n    float rate = 0;\r\n    float sqrate = 0;\r\n    String line;\r\n    while ((line = lines.readLine()) != null) {\r\n        StringTokenizer tokens = new StringTokenizer(line, \" \\t\\n\\r\\f%\");\r\n        String attr = tokens.nextToken();\r\n        if (attr.endsWith(\":tasks\"))\r\n            tasks = Long.parseLong(tokens.nextToken());\r\n        else if (attr.endsWith(\":size\"))\r\n            size = Long.parseLong(tokens.nextToken());\r\n        else if (attr.endsWith(\":time\"))\r\n            time = Long.parseLong(tokens.nextToken());\r\n        else if (attr.endsWith(\":rate\"))\r\n            rate = Float.parseFloat(tokens.nextToken());\r\n        else if (attr.endsWith(\":sqrate\"))\r\n            sqrate = Float.parseFloat(tokens.nextToken());\r\n    }\r\n    double med = rate / 1000 / tasks;\r\n    double stdDev = Math.sqrt(Math.abs(sqrate / 1000 / tasks - med * med));\r\n    String[] resultLines = { \"----- DFSCIOTest ----- : \" + ((testType == TEST_TYPE_WRITE) ? \"write\" : (testType == TEST_TYPE_READ) ? \"read\" : \"unknown\"), \"           Date & time: \" + new Date(System.currentTimeMillis()), \"       Number of files: \" + tasks, \"Total MBytes processed: \" + size / MEGA, \"     Throughput mb/sec: \" + size * 1000.0 / (time * MEGA), \"Average IO rate mb/sec: \" + med, \" Std IO rate deviation: \" + stdDev, \"    Test exec time sec: \" + (float) execTime / 1000, \"\" };\r\n    PrintStream res = new PrintStream(new FileOutputStream(new File(resFileName), true));\r\n    for (int i = 0; i < resultLines.length; i++) {\r\n        LOG.info(resultLines[i]);\r\n        res.println(resultLines[i]);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\fs",
  "methodName" : "cleanup",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void cleanup(FileSystem fs) throws Exception\n{\r\n    LOG.info(\"Cleaning up test files\");\r\n    fs.delete(new Path(TEST_ROOT_DIR), true);\r\n    fs.delete(HDFS_TEST_DIR, true);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "launchJob",
  "errType" : [ "NullPointerException" ],
  "containingMethodsNum" : 26,
  "sourceCodeText" : "boolean launchJob(URI fileSys, JobConf conf, int numMaps, int numReduces) throws IOException\n{\r\n    final Path inDir = new Path(\"/testing/input\");\r\n    final Path outDir = new Path(\"/testing/output\");\r\n    FileSystem fs = FileSystem.get(fileSys, conf);\r\n    fs.delete(outDir, true);\r\n    if (!fs.mkdirs(inDir)) {\r\n        LOG.warn(\"Can't create \" + inDir);\r\n        return false;\r\n    }\r\n    DataOutputStream file = fs.create(new Path(inDir, \"part-0\"));\r\n    file.writeBytes(\"foo foo2 foo3\");\r\n    file.close();\r\n    FileSystem.setDefaultUri(conf, fileSys);\r\n    conf.setJobName(\"foo\");\r\n    conf.setInputFormat(TextInputFormat.class);\r\n    conf.setOutputFormat(SpecialTextOutputFormat.class);\r\n    conf.setOutputKeyClass(LongWritable.class);\r\n    conf.setOutputValueClass(Text.class);\r\n    conf.setMapperClass(IdentityMapper.class);\r\n    conf.setReducerClass(IdentityReducer.class);\r\n    FileInputFormat.setInputPaths(conf, inDir);\r\n    FileOutputFormat.setOutputPath(conf, outDir);\r\n    conf.setNumMapTasks(numMaps);\r\n    conf.setNumReduceTasks(numReduces);\r\n    RunningJob runningJob = JobClient.runJob(conf);\r\n    try {\r\n        assertTrue(runningJob.isComplete());\r\n        assertTrue(runningJob.isSuccessful());\r\n        assertTrue(\"Output folder not found!\", fs.exists(new Path(\"/testing/output/\" + OUTPUT_FILENAME)));\r\n    } catch (NullPointerException npe) {\r\n        fail(\"A NPE should not have happened.\");\r\n    }\r\n    LOG.info(\"job is complete: \" + runningJob.isSuccessful());\r\n    return (runningJob.isSuccessful());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "testJobWithDFS",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void testJobWithDFS() throws IOException\n{\r\n    String namenode = null;\r\n    MiniDFSCluster dfs = null;\r\n    MiniMRCluster mr = null;\r\n    FileSystem fileSys = null;\r\n    try {\r\n        final int taskTrackers = 4;\r\n        final int jobTrackerPort = 60050;\r\n        Configuration conf = new Configuration();\r\n        dfs = new MiniDFSCluster.Builder(conf).build();\r\n        fileSys = dfs.getFileSystem();\r\n        namenode = fileSys.getUri().toString();\r\n        mr = new MiniMRCluster(taskTrackers, namenode, 2);\r\n        JobConf jobConf = new JobConf();\r\n        boolean result;\r\n        result = launchJob(fileSys.getUri(), jobConf, 3, 1);\r\n        assertTrue(result);\r\n    } finally {\r\n        if (dfs != null) {\r\n            dfs.shutdown();\r\n        }\r\n        if (mr != null) {\r\n            mr.shutdown();\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\fs\\slive",
  "methodName" : "getDirectory",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Path getDirectory()\n{\r\n    Path dir = getFinder().getDirectory();\r\n    return dir;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\fs\\slive",
  "methodName" : "run",
  "errType" : [ "FileNotFoundException", "IOException" ],
  "containingMethodsNum" : 13,
  "sourceCodeText" : "List<OperationOutput> run(FileSystem fs)\n{\r\n    List<OperationOutput> out = super.run(fs);\r\n    try {\r\n        Path dir = getDirectory();\r\n        long dirEntries = 0;\r\n        long timeTaken = 0;\r\n        {\r\n            long startTime = Timer.now();\r\n            FileStatus[] files = fs.listStatus(dir);\r\n            timeTaken = Timer.elapsed(startTime);\r\n            dirEntries = files.length;\r\n        }\r\n        out.add(new OperationOutput(OutputType.LONG, getType(), ReportWriter.OK_TIME_TAKEN, timeTaken));\r\n        out.add(new OperationOutput(OutputType.LONG, getType(), ReportWriter.SUCCESSES, 1L));\r\n        out.add(new OperationOutput(OutputType.LONG, getType(), ReportWriter.DIR_ENTRIES, dirEntries));\r\n        LOG.info(\"Directory \" + dir + \" has \" + dirEntries + \" entries\");\r\n    } catch (FileNotFoundException e) {\r\n        out.add(new OperationOutput(OutputType.LONG, getType(), ReportWriter.NOT_FOUND, 1L));\r\n        LOG.warn(\"Error with listing\", e);\r\n    } catch (IOException e) {\r\n        out.add(new OperationOutput(OutputType.LONG, getType(), ReportWriter.FAILURES, 1L));\r\n        LOG.warn(\"Error with listing\", e);\r\n    }\r\n    return out;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\security",
  "methodName" : "setUp",
  "errType" : null,
  "containingMethodsNum" : 13,
  "sourceCodeText" : "void setUp() throws Exception\n{\r\n    final Configuration conf = new Configuration();\r\n    conf.set(MRConfig.FRAMEWORK_NAME, MRConfig.YARN_FRAMEWORK_NAME);\r\n    conf.set(YarnConfiguration.RM_PRINCIPAL, \"jt_id/\" + SecurityUtil.HOSTNAME_PATTERN + \"@APACHE.ORG\");\r\n    final MiniDFSCluster.Builder builder = new MiniDFSCluster.Builder(conf);\r\n    builder.checkExitOnShutdown(true);\r\n    builder.numDataNodes(NUMWORKERS);\r\n    builder.format(true);\r\n    builder.racks(null);\r\n    dfsCluster = builder.build();\r\n    mrCluster = new MiniMRYarnCluster(TestBinaryTokenFile.class.getName(), noOfNMs);\r\n    mrCluster.init(conf);\r\n    mrCluster.start();\r\n    NameNodeAdapter.getDtSecretManager(dfsCluster.getNamesystem()).startThreads();\r\n    FileSystem fs = dfsCluster.getFileSystem();\r\n    p1 = new Path(\"file1\");\r\n    p1 = fs.makeQualified(p1);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\security",
  "methodName" : "tearDown",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void tearDown() throws Exception\n{\r\n    if (mrCluster != null) {\r\n        mrCluster.stop();\r\n        mrCluster = null;\r\n    }\r\n    if (dfsCluster != null) {\r\n        dfsCluster.shutdown();\r\n        dfsCluster = null;\r\n    }\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\security",
  "methodName" : "createBinaryTokenFile",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void createBinaryTokenFile(Configuration conf)\n{\r\n    try {\r\n        Credentials cred1 = new Credentials();\r\n        Credentials cred2 = new Credentials();\r\n        TokenCache.obtainTokensForNamenodesInternal(cred1, new Path[] { p1 }, conf);\r\n        for (Token<? extends TokenIdentifier> t : cred1.getAllTokens()) {\r\n            cred2.addToken(new Text(DELEGATION_TOKEN_KEY), t);\r\n        }\r\n        DataOutputStream os = new DataOutputStream(new FileOutputStream(binaryTokenFileName.toString()));\r\n        try {\r\n            cred2.writeTokenStorageToStream(os);\r\n        } finally {\r\n            os.close();\r\n        }\r\n    } catch (IOException e) {\r\n        Assert.fail(\"Exception \" + e);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 2,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\security",
  "methodName" : "testBinaryTokenFile",
  "errType" : [ "Exception" ],
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void testBinaryTokenFile() throws IOException\n{\r\n    Configuration conf = mrCluster.getConfig();\r\n    final String nnUri = dfsCluster.getURI(0).toString();\r\n    conf.set(MRJobConfig.JOB_NAMENODES, nnUri + \",\" + nnUri);\r\n    final String[] args = { \"-m\", \"1\", \"-r\", \"1\", \"-mt\", \"1\", \"-rt\", \"1\" };\r\n    int res = -1;\r\n    try {\r\n        res = ToolRunner.run(conf, new MySleepJob(), args);\r\n    } catch (Exception e) {\r\n        System.out.println(\"Job failed with \" + e.getLocalizedMessage());\r\n        e.printStackTrace(System.out);\r\n        fail(\"Job failed\");\r\n    }\r\n    assertEquals(\"dist job res is not 0:\", 0, res);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\security",
  "methodName" : "testTokenCacheFile",
  "errType" : [ "Exception" ],
  "containingMethodsNum" : 10,
  "sourceCodeText" : "void testTokenCacheFile() throws IOException\n{\r\n    Configuration conf = mrCluster.getConfig();\r\n    createBinaryTokenFile(conf);\r\n    final String nnUri = dfsCluster.getURI(0).toString();\r\n    conf.set(MRJobConfig.JOB_NAMENODES, nnUri + \",\" + nnUri);\r\n    final String[] args = { \"-tokenCacheFile\", binaryTokenFileName.toString(), \"-m\", \"1\", \"-r\", \"1\", \"-mt\", \"1\", \"-rt\", \"1\" };\r\n    int res = -1;\r\n    try {\r\n        res = ToolRunner.run(conf, new SleepJob(), args);\r\n    } catch (Exception e) {\r\n        System.out.println(\"Job failed with \" + e.getLocalizedMessage());\r\n        e.printStackTrace(System.out);\r\n        fail(\"Job failed\");\r\n    }\r\n    assertEquals(\"dist job res is not 0:\", 0, res);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\fs\\slive",
  "methodName" : "configure",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void configure(JobConf conf)\n{\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\fs\\slive",
  "methodName" : "getPartition",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "int getPartition(Text key, Text value, int numPartitions)\n{\r\n    OperationOutput oo = new OperationOutput(key, value);\r\n    return (oo.getOperationType().hashCode() & Integer.MAX_VALUE) % numPartitions;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\conf",
  "methodName" : "testNoDefaults",
  "errType" : null,
  "containingMethodsNum" : 26,
  "sourceCodeText" : "void testNoDefaults() throws Exception\n{\r\n    JobConf configuration = new JobConf();\r\n    assertTrue(configuration.get(\"hadoop.tmp.dir\", null) != null);\r\n    configuration = new JobConf(false);\r\n    assertTrue(configuration.get(\"hadoop.tmp.dir\", null) == null);\r\n    Path inDir = new Path(\"testing/jobconf/input\");\r\n    Path outDir = new Path(\"testing/jobconf/output\");\r\n    OutputStream os = getFileSystem().create(new Path(inDir, \"text.txt\"));\r\n    Writer wr = new OutputStreamWriter(os);\r\n    wr.write(\"hello\\n\");\r\n    wr.write(\"hello\\n\");\r\n    wr.close();\r\n    JobConf conf = new JobConf(false);\r\n    conf.set(\"fs.defaultFS\", createJobConf().get(\"fs.defaultFS\"));\r\n    conf.setJobName(\"mr\");\r\n    conf.setInputFormat(TextInputFormat.class);\r\n    conf.setMapOutputKeyClass(LongWritable.class);\r\n    conf.setMapOutputValueClass(Text.class);\r\n    conf.setOutputFormat(TextOutputFormat.class);\r\n    conf.setOutputKeyClass(LongWritable.class);\r\n    conf.setOutputValueClass(Text.class);\r\n    conf.setMapperClass(org.apache.hadoop.mapred.lib.IdentityMapper.class);\r\n    conf.setReducerClass(org.apache.hadoop.mapred.lib.IdentityReducer.class);\r\n    FileInputFormat.setInputPaths(conf, inDir);\r\n    FileOutputFormat.setOutputPath(conf, outDir);\r\n    JobClient.runJob(conf);\r\n    Path[] outputFiles = FileUtil.stat2Paths(getFileSystem().listStatus(outDir, new Utils.OutputFileUtils.OutputFilesFilter()));\r\n    if (outputFiles.length > 0) {\r\n        InputStream is = getFileSystem().open(outputFiles[0]);\r\n        BufferedReader reader = new BufferedReader(new InputStreamReader(is));\r\n        String line = reader.readLine();\r\n        int counter = 0;\r\n        while (line != null) {\r\n            counter++;\r\n            assertTrue(line.contains(\"hello\"));\r\n            line = reader.readLine();\r\n        }\r\n        reader.close();\r\n        assertEquals(2, counter);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "testFormat",
  "errType" : null,
  "containingMethodsNum" : 18,
  "sourceCodeText" : "void testFormat() throws Exception\n{\r\n    JobConf job = new JobConf(conf);\r\n    Reporter reporter = Reporter.NULL;\r\n    Random random = new Random();\r\n    long seed = random.nextLong();\r\n    LOG.info(\"seed = \" + seed);\r\n    random.setSeed(seed);\r\n    localFs.delete(workDir, true);\r\n    FileInputFormat.setInputPaths(job, workDir);\r\n    final int length = 10000;\r\n    final int numFiles = 10;\r\n    createFiles(length, numFiles, random);\r\n    InputFormat<IntWritable, BytesWritable> format = new CombineSequenceFileInputFormat<IntWritable, BytesWritable>();\r\n    IntWritable key = new IntWritable();\r\n    BytesWritable value = new BytesWritable();\r\n    for (int i = 0; i < 3; i++) {\r\n        int numSplits = random.nextInt(length / (SequenceFile.SYNC_INTERVAL / 20)) + 1;\r\n        LOG.info(\"splitting: requesting = \" + numSplits);\r\n        InputSplit[] splits = format.getSplits(job, numSplits);\r\n        LOG.info(\"splitting: got =        \" + splits.length);\r\n        assertEquals(\"We got more than one splits!\", 1, splits.length);\r\n        InputSplit split = splits[0];\r\n        assertEquals(\"It should be CombineFileSplit\", CombineFileSplit.class, split.getClass());\r\n        BitSet bits = new BitSet(length);\r\n        RecordReader<IntWritable, BytesWritable> reader = format.getRecordReader(split, job, reporter);\r\n        try {\r\n            while (reader.next(key, value)) {\r\n                assertFalse(\"Key in multiple partitions.\", bits.get(key.get()));\r\n                bits.set(key.get());\r\n            }\r\n        } finally {\r\n            reader.close();\r\n        }\r\n        assertEquals(\"Some keys in no partition.\", length, bits.cardinality());\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "createRanges",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Range[] createRanges(int length, int numFiles, Random random)\n{\r\n    Range[] ranges = new Range[numFiles];\r\n    for (int i = 0; i < numFiles; i++) {\r\n        int start = i == 0 ? 0 : ranges[i - 1].end;\r\n        int end = i == numFiles - 1 ? length : (length / numFiles) * (2 * i + 1) / 2 + random.nextInt(length / numFiles) + 1;\r\n        ranges[i] = new Range(start, end);\r\n    }\r\n    return ranges;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "createFiles",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void createFiles(int length, int numFiles, Random random) throws IOException\n{\r\n    Range[] ranges = createRanges(length, numFiles, random);\r\n    for (int i = 0; i < numFiles; i++) {\r\n        Path file = new Path(workDir, \"test_\" + i + \".seq\");\r\n        @SuppressWarnings(\"deprecation\")\r\n        SequenceFile.Writer writer = SequenceFile.createWriter(localFs, conf, file, IntWritable.class, BytesWritable.class);\r\n        Range range = ranges[i];\r\n        try {\r\n            for (int j = range.start; j < range.end; j++) {\r\n                IntWritable key = new IntWritable(j);\r\n                byte[] data = new byte[random.nextInt(10)];\r\n                random.nextBytes(data);\r\n                BytesWritable value = new BytesWritable(data);\r\n                writer.append(key, value);\r\n            }\r\n        } finally {\r\n            writer.close();\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2",
  "methodName" : "setup",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void setup() throws InterruptedException, IOException\n{\r\n    if (!(new File(MiniMRYarnCluster.APPJAR)).exists()) {\r\n        LOG.info(\"MRAppJar \" + MiniMRYarnCluster.APPJAR + \" not found. Not running test.\");\r\n        return;\r\n    }\r\n    if (mrCluster == null) {\r\n        mrCluster = new MiniMRYarnCluster(TestMRJobsWithProfiler.class.getName());\r\n        mrCluster.init(CONF);\r\n        mrCluster.start();\r\n    }\r\n    localFs.copyFromLocalFile(new Path(MiniMRYarnCluster.APPJAR), APP_JAR);\r\n    localFs.setPermission(APP_JAR, new FsPermission(\"700\"));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2",
  "methodName" : "tearDown",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void tearDown()\n{\r\n    if (!(new File(MiniMRYarnCluster.APPJAR)).exists()) {\r\n        LOG.info(\"MRAppJar \" + MiniMRYarnCluster.APPJAR + \" not found. Not running test.\");\r\n        return;\r\n    }\r\n    if (mrCluster != null) {\r\n        mrCluster.stop();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2",
  "methodName" : "testDefaultProfiler",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void testDefaultProfiler() throws Exception\n{\r\n    assumeFalse(\"The hprof agent has been removed since Java 9. Skipping.\", Shell.isJavaVersionAtLeast(9));\r\n    LOG.info(\"Starting testDefaultProfiler\");\r\n    testProfilerInternal(true);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2",
  "methodName" : "testDifferentProfilers",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void testDifferentProfilers() throws Exception\n{\r\n    LOG.info(\"Starting testDefaultProfiler\");\r\n    testProfilerInternal(false);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2",
  "methodName" : "testProfilerInternal",
  "errType" : null,
  "containingMethodsNum" : 58,
  "sourceCodeText" : "void testProfilerInternal(boolean useDefault) throws Exception\n{\r\n    if (!(new File(MiniMRYarnCluster.APPJAR)).exists()) {\r\n        LOG.info(\"MRAppJar \" + MiniMRYarnCluster.APPJAR + \" not found. Not running test.\");\r\n        return;\r\n    }\r\n    final SleepJob sleepJob = new SleepJob();\r\n    final JobConf sleepConf = new JobConf(mrCluster.getConfig());\r\n    sleepConf.setProfileEnabled(true);\r\n    sleepConf.setProfileTaskRange(true, String.valueOf(PROFILED_TASK_ID));\r\n    sleepConf.setProfileTaskRange(false, String.valueOf(PROFILED_TASK_ID));\r\n    if (!useDefault) {\r\n        if (Shell.isJavaVersionAtLeast(9)) {\r\n            sleepConf.set(MRJobConfig.TASK_MAP_PROFILE_PARAMS, \"-XX:StartFlightRecording=dumponexit=true,filename=%s\");\r\n            sleepConf.set(MRJobConfig.TASK_REDUCE_PROFILE_PARAMS, \"-XX:StartFlightRecording=dumponexit=true,filename=%s\");\r\n        } else {\r\n            sleepConf.set(MRJobConfig.TASK_MAP_PROFILE_PARAMS, \"-agentlib:hprof=cpu=samples,heap=sites,force=n,thread=y,verbose=n,\" + \"file=%s\");\r\n            sleepConf.set(MRJobConfig.TASK_REDUCE_PROFILE_PARAMS, \"-Xprof\");\r\n        }\r\n    }\r\n    sleepJob.setConf(sleepConf);\r\n    final Job job = sleepJob.createJob(2, 2, 500, 1, 500, 1);\r\n    job.setJarByClass(SleepJob.class);\r\n    job.addFileToClassPath(APP_JAR);\r\n    job.waitForCompletion(true);\r\n    final JobId jobId = TypeConverter.toYarn(job.getJobID());\r\n    final ApplicationId appID = jobId.getAppId();\r\n    int pollElapsed = 0;\r\n    while (true) {\r\n        Thread.sleep(1000);\r\n        pollElapsed += 1000;\r\n        if (TERMINAL_RM_APP_STATES.contains(mrCluster.getResourceManager().getRMContext().getRMApps().get(appID).getState())) {\r\n            break;\r\n        }\r\n        if (pollElapsed >= 60000) {\r\n            LOG.warn(\"application did not reach terminal state within 60 seconds\");\r\n            break;\r\n        }\r\n    }\r\n    Assert.assertEquals(RMAppState.FINISHED, mrCluster.getResourceManager().getRMContext().getRMApps().get(appID).getState());\r\n    final Configuration nmConf = mrCluster.getNodeManager(0).getConfig();\r\n    final String appIdStr = appID.toString();\r\n    final String appIdSuffix = appIdStr.substring(\"application_\".length(), appIdStr.length());\r\n    final String containerGlob = \"container_\" + appIdSuffix + \"_*_*\";\r\n    final Map<TaskAttemptID, Path> taLogDirs = new HashMap<TaskAttemptID, Path>();\r\n    final Pattern taskPattern = Pattern.compile(\".*Task:(attempt_\" + appIdSuffix + \"_[rm]_\" + \"[0-9]+_[0-9]+).*\");\r\n    for (String logDir : nmConf.getTrimmedStrings(YarnConfiguration.NM_LOG_DIRS)) {\r\n        for (FileStatus fileStatus : localFs.globStatus(new Path(logDir + Path.SEPARATOR + appIdStr + Path.SEPARATOR + containerGlob + Path.SEPARATOR + TaskLog.LogName.SYSLOG))) {\r\n            final BufferedReader br = new BufferedReader(new InputStreamReader(localFs.open(fileStatus.getPath())));\r\n            String line;\r\n            while ((line = br.readLine()) != null) {\r\n                final Matcher m = taskPattern.matcher(line);\r\n                if (m.matches()) {\r\n                    taLogDirs.put(TaskAttemptID.forName(m.group(1)), fileStatus.getPath().getParent());\r\n                    break;\r\n                }\r\n            }\r\n            br.close();\r\n        }\r\n    }\r\n    Assert.assertEquals(4, taLogDirs.size());\r\n    if (Shell.isJavaVersionAtLeast(9)) {\r\n        return;\r\n    }\r\n    for (Map.Entry<TaskAttemptID, Path> dirEntry : taLogDirs.entrySet()) {\r\n        final TaskAttemptID tid = dirEntry.getKey();\r\n        final Path profilePath = new Path(dirEntry.getValue(), TaskLog.LogName.PROFILE.toString());\r\n        final Path stdoutPath = new Path(dirEntry.getValue(), TaskLog.LogName.STDOUT.toString());\r\n        if (useDefault || tid.getTaskType() == TaskType.MAP) {\r\n            if (tid.getTaskID().getId() == PROFILED_TASK_ID) {\r\n                final BufferedReader br = new BufferedReader(new InputStreamReader(localFs.open(profilePath)));\r\n                final String line = br.readLine();\r\n                Assert.assertTrue(\"No hprof content found!\", line != null && line.startsWith(\"JAVA PROFILE\"));\r\n                br.close();\r\n                Assert.assertEquals(0L, localFs.getFileStatus(stdoutPath).getLen());\r\n            } else {\r\n                Assert.assertFalse(\"hprof file should not exist\", localFs.exists(profilePath));\r\n            }\r\n        } else {\r\n            Assert.assertFalse(\"hprof file should not exist\", localFs.exists(profilePath));\r\n            if (tid.getTaskID().getId() == PROFILED_TASK_ID) {\r\n                final BufferedReader br = new BufferedReader(new InputStreamReader(localFs.open(stdoutPath)));\r\n                boolean flatProfFound = false;\r\n                String line;\r\n                while ((line = br.readLine()) != null) {\r\n                    if (line.startsWith(\"Flat profile\")) {\r\n                        flatProfFound = true;\r\n                        break;\r\n                    }\r\n                }\r\n                br.close();\r\n                Assert.assertTrue(\"Xprof flat profile not found!\", flatProfFound);\r\n            } else {\r\n                Assert.assertEquals(0L, localFs.getFileStatus(stdoutPath).getLen());\r\n            }\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred\\join",
  "methodName" : "setUp",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setUp() throws Exception\n{\r\n    Configuration conf = new Configuration();\r\n    cluster = new MiniDFSCluster.Builder(conf).numDataNodes(2).build();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred\\join",
  "methodName" : "tearDown",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void tearDown() throws Exception\n{\r\n    if (cluster != null) {\r\n        cluster.shutdown();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred\\join",
  "methodName" : "createWriters",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "SequenceFile.Writer[] createWriters(Path testdir, Configuration conf, int srcs, Path[] src) throws IOException\n{\r\n    for (int i = 0; i < srcs; ++i) {\r\n        src[i] = new Path(testdir, Integer.toString(i + 10, 36));\r\n    }\r\n    SequenceFile.Writer[] out = new SequenceFile.Writer[srcs];\r\n    for (int i = 0; i < srcs; ++i) {\r\n        out[i] = new SequenceFile.Writer(testdir.getFileSystem(conf), conf, src[i], IntWritable.class, IntWritable.class);\r\n    }\r\n    return out;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred\\join",
  "methodName" : "writeSimpleSrc",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "Path[] writeSimpleSrc(Path testdir, Configuration conf, int srcs) throws IOException\n{\r\n    SequenceFile.Writer[] out = null;\r\n    Path[] src = new Path[srcs];\r\n    try {\r\n        out = createWriters(testdir, conf, srcs, src);\r\n        final int capacity = srcs * 2 + 1;\r\n        IntWritable key = new IntWritable();\r\n        IntWritable val = new IntWritable();\r\n        for (int k = 0; k < capacity; ++k) {\r\n            for (int i = 0; i < srcs; ++i) {\r\n                key.set(k % srcs == 0 ? k * srcs : k * srcs + i);\r\n                val.set(10 * k + i);\r\n                out[i].append(key, val);\r\n                if (i == k) {\r\n                    out[i].append(key, val);\r\n                }\r\n            }\r\n        }\r\n    } finally {\r\n        if (out != null) {\r\n            for (int i = 0; i < srcs; ++i) {\r\n                if (out[i] != null)\r\n                    out[i].close();\r\n            }\r\n        }\r\n    }\r\n    return src;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred\\join",
  "methodName" : "stringify",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "String stringify(IntWritable key, Writable val)\n{\r\n    StringBuilder sb = new StringBuilder();\r\n    sb.append(\"(\" + key);\r\n    sb.append(\",\" + val + \")\");\r\n    return sb.toString();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred\\join",
  "methodName" : "joinAs",
  "errType" : null,
  "containingMethodsNum" : 12,
  "sourceCodeText" : "void joinAs(String jointype, Class<? extends SimpleCheckerBase> c) throws Exception\n{\r\n    final int srcs = 4;\r\n    Configuration conf = new Configuration();\r\n    JobConf job = new JobConf(conf, c);\r\n    Path base = cluster.getFileSystem().makeQualified(new Path(\"/\" + jointype));\r\n    Path[] src = writeSimpleSrc(base, conf, srcs);\r\n    job.set(\"mapreduce.join.expr\", CompositeInputFormat.compose(jointype, SequenceFileInputFormat.class, src));\r\n    job.setInt(\"testdatamerge.sources\", srcs);\r\n    job.setInputFormat(CompositeInputFormat.class);\r\n    FileOutputFormat.setOutputPath(job, new Path(base, \"out\"));\r\n    job.setMapperClass(c);\r\n    job.setReducerClass(c);\r\n    job.setOutputKeyClass(IntWritable.class);\r\n    job.setOutputValueClass(IntWritable.class);\r\n    JobClient.runJob(job);\r\n    base.getFileSystem(job).delete(base, true);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred\\join",
  "methodName" : "testSimpleInnerJoin",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testSimpleInnerJoin() throws Exception\n{\r\n    joinAs(\"inner\", InnerJoinChecker.class);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred\\join",
  "methodName" : "testSimpleOuterJoin",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testSimpleOuterJoin() throws Exception\n{\r\n    joinAs(\"outer\", OuterJoinChecker.class);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred\\join",
  "methodName" : "testSimpleOverride",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testSimpleOverride() throws Exception\n{\r\n    joinAs(\"override\", OverrideChecker.class);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred\\join",
  "methodName" : "testNestedJoin",
  "errType" : null,
  "containingMethodsNum" : 42,
  "sourceCodeText" : "void testNestedJoin() throws Exception\n{\r\n    final int SOURCES = 3;\r\n    final int ITEMS = (SOURCES + 1) * (SOURCES + 1);\r\n    JobConf job = new JobConf();\r\n    Path base = cluster.getFileSystem().makeQualified(new Path(\"/nested\"));\r\n    int[][] source = new int[SOURCES][];\r\n    for (int i = 0; i < SOURCES; ++i) {\r\n        source[i] = new int[ITEMS];\r\n        for (int j = 0; j < ITEMS; ++j) {\r\n            source[i][j] = (i + 2) * (j + 1);\r\n        }\r\n    }\r\n    Path[] src = new Path[SOURCES];\r\n    SequenceFile.Writer[] out = createWriters(base, job, SOURCES, src);\r\n    IntWritable k = new IntWritable();\r\n    for (int i = 0; i < SOURCES; ++i) {\r\n        IntWritable v = new IntWritable();\r\n        v.set(i);\r\n        for (int j = 0; j < ITEMS; ++j) {\r\n            k.set(source[i][j]);\r\n            out[i].append(k, v);\r\n        }\r\n        out[i].close();\r\n    }\r\n    out = null;\r\n    StringBuilder sb = new StringBuilder();\r\n    sb.append(\"outer(inner(\");\r\n    for (int i = 0; i < SOURCES; ++i) {\r\n        sb.append(CompositeInputFormat.compose(SequenceFileInputFormat.class, src[i].toString()));\r\n        if (i + 1 != SOURCES)\r\n            sb.append(\",\");\r\n    }\r\n    sb.append(\"),outer(\");\r\n    sb.append(CompositeInputFormat.compose(Fake_IF.class, \"foobar\"));\r\n    sb.append(\",\");\r\n    for (int i = 0; i < SOURCES; ++i) {\r\n        sb.append(CompositeInputFormat.compose(SequenceFileInputFormat.class, src[i].toString()));\r\n        sb.append(\",\");\r\n    }\r\n    sb.append(CompositeInputFormat.compose(Fake_IF.class, \"raboof\") + \"))\");\r\n    job.set(\"mapreduce.join.expr\", sb.toString());\r\n    job.setInputFormat(CompositeInputFormat.class);\r\n    Path outf = new Path(base, \"out\");\r\n    FileOutputFormat.setOutputPath(job, outf);\r\n    Fake_IF.setKeyClass(job, IntWritable.class);\r\n    Fake_IF.setValClass(job, IntWritable.class);\r\n    job.setMapperClass(IdentityMapper.class);\r\n    job.setReducerClass(IdentityReducer.class);\r\n    job.setNumReduceTasks(0);\r\n    job.setOutputKeyClass(IntWritable.class);\r\n    job.setOutputValueClass(TupleWritable.class);\r\n    job.setOutputFormat(SequenceFileOutputFormat.class);\r\n    JobClient.runJob(job);\r\n    FileStatus[] outlist = cluster.getFileSystem().listStatus(outf, new Utils.OutputFileUtils.OutputFilesFilter());\r\n    assertEquals(1, outlist.length);\r\n    assertTrue(0 < outlist[0].getLen());\r\n    SequenceFile.Reader r = new SequenceFile.Reader(cluster.getFileSystem(), outlist[0].getPath(), job);\r\n    TupleWritable v = new TupleWritable();\r\n    while (r.next(k, v)) {\r\n        assertFalse(((TupleWritable) v.get(1)).has(0));\r\n        assertFalse(((TupleWritable) v.get(1)).has(SOURCES + 1));\r\n        boolean chk = true;\r\n        int ki = k.get();\r\n        for (int i = 2; i < SOURCES + 2; ++i) {\r\n            if ((ki % i) == 0 && ki <= i * ITEMS) {\r\n                assertEquals(i - 2, ((IntWritable) ((TupleWritable) v.get(1)).get((i - 1))).get());\r\n            } else\r\n                chk = false;\r\n        }\r\n        if (chk) {\r\n            assertTrue(v.has(0));\r\n            for (int i = 0; i < SOURCES; ++i) assertTrue(((TupleWritable) v.get(0)).has(i));\r\n        } else {\r\n            assertFalse(v.has(0));\r\n        }\r\n    }\r\n    r.close();\r\n    base.getFileSystem(job).delete(base, true);\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred\\join",
  "methodName" : "testEmptyJoin",
  "errType" : null,
  "containingMethodsNum" : 10,
  "sourceCodeText" : "void testEmptyJoin() throws Exception\n{\r\n    JobConf job = new JobConf();\r\n    Path base = cluster.getFileSystem().makeQualified(new Path(\"/empty\"));\r\n    Path[] src = { new Path(base, \"i0\"), new Path(\"i1\"), new Path(\"i2\") };\r\n    job.set(\"mapreduce.join.expr\", CompositeInputFormat.compose(\"outer\", Fake_IF.class, src));\r\n    job.setInputFormat(CompositeInputFormat.class);\r\n    FileOutputFormat.setOutputPath(job, new Path(base, \"out\"));\r\n    job.setMapperClass(IdentityMapper.class);\r\n    job.setReducerClass(IdentityReducer.class);\r\n    job.setOutputKeyClass(IncomparableKey.class);\r\n    job.setOutputValueClass(NullWritable.class);\r\n    JobClient.runJob(job);\r\n    base.getFileSystem(job).delete(base, true);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "printUsage",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "int printUsage()\n{\r\n    System.err.println(\"Usage: [-m <maps>] [-r <reduces>]\\n\" + \"       [-keepmap <percent>] [-keepred <percent>]\\n\" + \"       [-indir <path>] [-outdir <path]\\n\" + \"       [-inFormat[Indirect] <InputFormat>] [-outFormat <OutputFormat>]\\n\" + \"       [-outKey <WritableComparable>] [-outValue <Writable>]\\n\");\r\n    GenericOptionsParser.printGenericCommandUsage(System.err);\r\n    return -1;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "parseArgs",
  "errType" : [ "NumberFormatException", "Exception" ],
  "containingMethodsNum" : 31,
  "sourceCodeText" : "boolean parseArgs(String[] argv, JobConf job) throws IOException\n{\r\n    if (argv.length < 1) {\r\n        return 0 == printUsage();\r\n    }\r\n    for (int i = 0; i < argv.length; ++i) {\r\n        if (argv.length == i + 1) {\r\n            System.out.println(\"ERROR: Required parameter missing from \" + argv[i]);\r\n            return 0 == printUsage();\r\n        }\r\n        try {\r\n            if (\"-m\".equals(argv[i])) {\r\n                job.setNumMapTasks(Integer.parseInt(argv[++i]));\r\n            } else if (\"-r\".equals(argv[i])) {\r\n                job.setNumReduceTasks(Integer.parseInt(argv[++i]));\r\n            } else if (\"-inFormat\".equals(argv[i])) {\r\n                job.setInputFormat(Class.forName(argv[++i]).asSubclass(InputFormat.class));\r\n            } else if (\"-outFormat\".equals(argv[i])) {\r\n                job.setOutputFormat(Class.forName(argv[++i]).asSubclass(OutputFormat.class));\r\n            } else if (\"-outKey\".equals(argv[i])) {\r\n                job.setOutputKeyClass(Class.forName(argv[++i]).asSubclass(WritableComparable.class));\r\n            } else if (\"-outValue\".equals(argv[i])) {\r\n                job.setOutputValueClass(Class.forName(argv[++i]).asSubclass(Writable.class));\r\n            } else if (\"-keepmap\".equals(argv[i])) {\r\n                job.set(org.apache.hadoop.mapreduce.GenericMRLoadGenerator.MAP_PRESERVE_PERCENT, argv[++i]);\r\n            } else if (\"-keepred\".equals(argv[i])) {\r\n                job.set(org.apache.hadoop.mapreduce.GenericMRLoadGenerator.REDUCE_PRESERVE_PERCENT, argv[++i]);\r\n            } else if (\"-outdir\".equals(argv[i])) {\r\n                FileOutputFormat.setOutputPath(job, new Path(argv[++i]));\r\n            } else if (\"-indir\".equals(argv[i])) {\r\n                FileInputFormat.addInputPaths(job, argv[++i]);\r\n            } else if (\"-inFormatIndirect\".equals(argv[i])) {\r\n                job.setClass(org.apache.hadoop.mapreduce.GenericMRLoadGenerator.INDIRECT_INPUT_FORMAT, Class.forName(argv[++i]).asSubclass(InputFormat.class), InputFormat.class);\r\n                job.setInputFormat(IndirectInputFormat.class);\r\n            } else {\r\n                System.out.println(\"Unexpected argument: \" + argv[i]);\r\n                return 0 == printUsage();\r\n            }\r\n        } catch (NumberFormatException except) {\r\n            System.out.println(\"ERROR: Integer expected instead of \" + argv[i]);\r\n            return 0 == printUsage();\r\n        } catch (Exception e) {\r\n            throw (IOException) new IOException().initCause(e);\r\n        }\r\n    }\r\n    return true;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "run",
  "errType" : null,
  "containingMethodsNum" : 30,
  "sourceCodeText" : "int run(String[] argv) throws Exception\n{\r\n    JobConf job = new JobConf(getConf());\r\n    job.setJarByClass(GenericMRLoadGenerator.class);\r\n    job.setMapperClass(SampleMapper.class);\r\n    job.setReducerClass(SampleReducer.class);\r\n    if (!parseArgs(argv, job)) {\r\n        return -1;\r\n    }\r\n    if (null == FileOutputFormat.getOutputPath(job)) {\r\n        job.setOutputFormat(NullOutputFormat.class);\r\n    }\r\n    if (0 == FileInputFormat.getInputPaths(job).length) {\r\n        System.err.println(\"No input path; ignoring InputFormat\");\r\n        confRandom(job);\r\n    } else if (null != job.getClass(org.apache.hadoop.mapreduce.GenericMRLoadGenerator.INDIRECT_INPUT_FORMAT, null)) {\r\n        JobClient jClient = new JobClient(job);\r\n        Path tmpDir = new Path(jClient.getFs().getHomeDirectory(), \".staging\");\r\n        Random r = new Random();\r\n        Path indirInputFile = new Path(tmpDir, Integer.toString(r.nextInt(Integer.MAX_VALUE), 36) + \"_files\");\r\n        job.set(org.apache.hadoop.mapreduce.GenericMRLoadGenerator.INDIRECT_INPUT_FILE, indirInputFile.toString());\r\n        SequenceFile.Writer writer = SequenceFile.createWriter(tmpDir.getFileSystem(job), job, indirInputFile, LongWritable.class, Text.class, SequenceFile.CompressionType.NONE);\r\n        try {\r\n            for (Path p : FileInputFormat.getInputPaths(job)) {\r\n                FileSystem fs = p.getFileSystem(job);\r\n                Stack<Path> pathstack = new Stack<Path>();\r\n                pathstack.push(p);\r\n                while (!pathstack.empty()) {\r\n                    for (FileStatus stat : fs.listStatus(pathstack.pop())) {\r\n                        if (stat.isDirectory()) {\r\n                            if (!stat.getPath().getName().startsWith(\"_\")) {\r\n                                pathstack.push(stat.getPath());\r\n                            }\r\n                        } else {\r\n                            writer.sync();\r\n                            writer.append(new LongWritable(stat.getLen()), new Text(stat.getPath().toUri().toString()));\r\n                        }\r\n                    }\r\n                }\r\n            }\r\n        } finally {\r\n            writer.close();\r\n        }\r\n    }\r\n    Date startTime = new Date();\r\n    System.out.println(\"Job started: \" + startTime);\r\n    JobClient.runJob(job);\r\n    Date endTime = new Date();\r\n    System.out.println(\"Job ended: \" + endTime);\r\n    System.out.println(\"The job took \" + (endTime.getTime() - startTime.getTime()) / 1000 + \" seconds.\");\r\n    return 0;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "main",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void main(String[] argv) throws Exception\n{\r\n    int res = ToolRunner.run(new Configuration(), new GenericMRLoadGenerator(), argv);\r\n    System.exit(res);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "confRandom",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void confRandom(JobConf job) throws IOException\n{\r\n    job.setInputFormat(RandomInputFormat.class);\r\n    job.setMapperClass(RandomMapOutput.class);\r\n    final ClusterStatus cluster = new JobClient(job).getClusterStatus();\r\n    int numMapsPerHost = job.getInt(RandomTextWriter.MAPS_PER_HOST, 10);\r\n    long numBytesToWritePerMap = job.getLong(RandomTextWriter.BYTES_PER_MAP, 1 * 1024 * 1024 * 1024);\r\n    if (numBytesToWritePerMap == 0) {\r\n        throw new IOException(\"Cannot have \" + RandomTextWriter.BYTES_PER_MAP + \" set to 0\");\r\n    }\r\n    long totalBytesToWrite = job.getLong(RandomTextWriter.TOTAL_BYTES, numMapsPerHost * numBytesToWritePerMap * cluster.getTaskTrackers());\r\n    int numMaps = (int) (totalBytesToWrite / numBytesToWritePerMap);\r\n    if (numMaps == 0 && totalBytesToWrite > 0) {\r\n        numMaps = 1;\r\n        job.setLong(RandomTextWriter.BYTES_PER_MAP, totalBytesToWrite);\r\n    }\r\n    job.setNumMapTasks(numMaps);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "cleanup",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void cleanup()\n{\r\n    FileUtil.fullyDelete(TEST_DIR);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "testMapred",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testMapred() throws Exception\n{\r\n    launch();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "testNullKeys",
  "errType" : null,
  "containingMethodsNum" : 24,
  "sourceCodeText" : "void testNullKeys() throws Exception\n{\r\n    JobConf conf = new JobConf(TestMapRed.class);\r\n    FileSystem fs = FileSystem.getLocal(conf);\r\n    HashSet<String> values = new HashSet<String>();\r\n    String m = \"AAAAAAAAAAAAAA\";\r\n    for (int i = 1; i < 11; ++i) {\r\n        values.add(m);\r\n        m = m.replace((char) ('A' + i - 1), (char) ('A' + i));\r\n    }\r\n    Path testdir = fs.makeQualified(new Path(System.getProperty(\"test.build.data\", \"/tmp\")));\r\n    fs.delete(testdir, true);\r\n    Path inFile = new Path(testdir, \"nullin/blah\");\r\n    SequenceFile.Writer w = SequenceFile.createWriter(fs, conf, inFile, NullWritable.class, Text.class, SequenceFile.CompressionType.NONE);\r\n    Text t = new Text();\r\n    for (String s : values) {\r\n        t.set(s);\r\n        w.append(NullWritable.get(), t);\r\n    }\r\n    w.close();\r\n    FileInputFormat.setInputPaths(conf, inFile);\r\n    FileOutputFormat.setOutputPath(conf, new Path(testdir, \"nullout\"));\r\n    conf.setMapperClass(NullMapper.class);\r\n    conf.setReducerClass(IdentityReducer.class);\r\n    conf.setOutputKeyClass(NullWritable.class);\r\n    conf.setOutputValueClass(Text.class);\r\n    conf.setInputFormat(SequenceFileInputFormat.class);\r\n    conf.setOutputFormat(SequenceFileOutputFormat.class);\r\n    conf.setNumReduceTasks(1);\r\n    conf.set(MRConfig.FRAMEWORK_NAME, MRConfig.LOCAL_FRAMEWORK_NAME);\r\n    JobClient.runJob(conf);\r\n    SequenceFile.Reader r = new SequenceFile.Reader(fs, new Path(testdir, \"nullout/part-00000\"), conf);\r\n    m = \"AAAAAAAAAAAAAA\";\r\n    for (int i = 1; r.next(NullWritable.get(), t); ++i) {\r\n        assertTrue(\"Unexpected value: \" + t, values.remove(t.toString()));\r\n        m = m.replace((char) ('A' + i - 1), (char) ('A' + i));\r\n    }\r\n    assertTrue(\"Missing values: \" + values.toString(), values.isEmpty());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "checkCompression",
  "errType" : null,
  "containingMethodsNum" : 31,
  "sourceCodeText" : "void checkCompression(boolean compressMapOutputs, CompressionType redCompression, boolean includeCombine) throws Exception\n{\r\n    JobConf conf = new JobConf(TestMapRed.class);\r\n    Path testdir = new Path(TEST_DIR.getAbsolutePath());\r\n    Path inDir = new Path(testdir, \"in\");\r\n    Path outDir = new Path(testdir, \"out\");\r\n    FileSystem fs = FileSystem.get(conf);\r\n    fs.delete(testdir, true);\r\n    FileInputFormat.setInputPaths(conf, inDir);\r\n    FileOutputFormat.setOutputPath(conf, outDir);\r\n    conf.setMapperClass(MyMap.class);\r\n    conf.setReducerClass(MyReduce.class);\r\n    conf.setOutputKeyClass(Text.class);\r\n    conf.setOutputValueClass(Text.class);\r\n    conf.setOutputFormat(SequenceFileOutputFormat.class);\r\n    conf.set(MRConfig.FRAMEWORK_NAME, MRConfig.LOCAL_FRAMEWORK_NAME);\r\n    if (includeCombine) {\r\n        conf.setCombinerClass(IdentityReducer.class);\r\n    }\r\n    conf.setCompressMapOutput(compressMapOutputs);\r\n    SequenceFileOutputFormat.setOutputCompressionType(conf, redCompression);\r\n    try {\r\n        if (!fs.mkdirs(testdir)) {\r\n            throw new IOException(\"Mkdirs failed to create \" + testdir.toString());\r\n        }\r\n        if (!fs.mkdirs(inDir)) {\r\n            throw new IOException(\"Mkdirs failed to create \" + inDir.toString());\r\n        }\r\n        Path inFile = new Path(inDir, \"part0\");\r\n        DataOutputStream f = fs.create(inFile);\r\n        f.writeBytes(\"Owen was here\\n\");\r\n        f.writeBytes(\"Hadoop is fun\\n\");\r\n        f.writeBytes(\"Is this done, yet?\\n\");\r\n        f.close();\r\n        RunningJob rj = JobClient.runJob(conf);\r\n        assertTrue(\"job was complete\", rj.isComplete());\r\n        assertTrue(\"job was successful\", rj.isSuccessful());\r\n        Path output = new Path(outDir, Task.getOutputName(0));\r\n        assertTrue(\"reduce output exists \" + output, fs.exists(output));\r\n        SequenceFile.Reader rdr = new SequenceFile.Reader(fs, output, conf);\r\n        assertEquals(\"is reduce output compressed \" + output, redCompression != CompressionType.NONE, rdr.isCompressed());\r\n        rdr.close();\r\n    } finally {\r\n        fs.delete(testdir, true);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "testCompression",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void testCompression() throws Exception\n{\r\n    EnumSet<SequenceFile.CompressionType> seq = EnumSet.allOf(SequenceFile.CompressionType.class);\r\n    for (CompressionType redCompression : seq) {\r\n        for (int combine = 0; combine < 2; ++combine) {\r\n            checkCompression(false, redCompression, combine == 1);\r\n            checkCompression(true, redCompression, combine == 1);\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "launch",
  "errType" : null,
  "containingMethodsNum" : 64,
  "sourceCodeText" : "void launch() throws Exception\n{\r\n    JobConf conf;\r\n    if (getConf() == null) {\r\n        conf = new JobConf();\r\n    } else {\r\n        conf = new JobConf(getConf());\r\n    }\r\n    conf.set(MRConfig.FRAMEWORK_NAME, MRConfig.LOCAL_FRAMEWORK_NAME);\r\n    conf.setJarByClass(TestMapRed.class);\r\n    int countsToGo = counts;\r\n    int[] dist = new int[range];\r\n    for (int i = 0; i < range; i++) {\r\n        double avgInts = (1.0 * countsToGo) / (range - i);\r\n        dist[i] = (int) Math.max(0, Math.round(avgInts + (Math.sqrt(avgInts) * r.nextGaussian())));\r\n        countsToGo -= dist[i];\r\n    }\r\n    if (countsToGo > 0) {\r\n        dist[dist.length - 1] += countsToGo;\r\n    }\r\n    FileSystem fs = FileSystem.get(conf);\r\n    Path testdir = new Path(TEST_DIR.getAbsolutePath(), \"mapred.loadtest\");\r\n    if (!fs.mkdirs(testdir)) {\r\n        throw new IOException(\"Mkdirs failed to create \" + testdir.toString());\r\n    }\r\n    Path randomIns = new Path(testdir, \"genins\");\r\n    if (!fs.mkdirs(randomIns)) {\r\n        throw new IOException(\"Mkdirs failed to create \" + randomIns.toString());\r\n    }\r\n    Path answerkey = new Path(randomIns, \"answer.key\");\r\n    SequenceFile.Writer out = SequenceFile.createWriter(fs, conf, answerkey, IntWritable.class, IntWritable.class, SequenceFile.CompressionType.NONE);\r\n    try {\r\n        for (int i = 0; i < range; i++) {\r\n            out.append(new IntWritable(i), new IntWritable(dist[i]));\r\n        }\r\n    } finally {\r\n        out.close();\r\n    }\r\n    Path randomOuts = new Path(testdir, \"genouts\");\r\n    fs.delete(randomOuts, true);\r\n    JobConf genJob = new JobConf(conf, TestMapRed.class);\r\n    FileInputFormat.setInputPaths(genJob, randomIns);\r\n    genJob.setInputFormat(SequenceFileInputFormat.class);\r\n    genJob.setMapperClass(RandomGenMapper.class);\r\n    FileOutputFormat.setOutputPath(genJob, randomOuts);\r\n    genJob.setOutputKeyClass(IntWritable.class);\r\n    genJob.setOutputValueClass(IntWritable.class);\r\n    genJob.setOutputFormat(TextOutputFormat.class);\r\n    genJob.setReducerClass(RandomGenReducer.class);\r\n    genJob.setNumReduceTasks(1);\r\n    JobClient.runJob(genJob);\r\n    int intermediateReduces = 10;\r\n    Path intermediateOuts = new Path(testdir, \"intermediateouts\");\r\n    fs.delete(intermediateOuts, true);\r\n    JobConf checkJob = new JobConf(conf, TestMapRed.class);\r\n    FileInputFormat.setInputPaths(checkJob, randomOuts);\r\n    checkJob.setInputFormat(TextInputFormat.class);\r\n    checkJob.setMapperClass(RandomCheckMapper.class);\r\n    FileOutputFormat.setOutputPath(checkJob, intermediateOuts);\r\n    checkJob.setOutputKeyClass(IntWritable.class);\r\n    checkJob.setOutputValueClass(IntWritable.class);\r\n    checkJob.setOutputFormat(MapFileOutputFormat.class);\r\n    checkJob.setReducerClass(RandomCheckReducer.class);\r\n    checkJob.setNumReduceTasks(intermediateReduces);\r\n    JobClient.runJob(checkJob);\r\n    Path finalOuts = new Path(testdir, \"finalouts\");\r\n    fs.delete(finalOuts, true);\r\n    JobConf mergeJob = new JobConf(conf, TestMapRed.class);\r\n    FileInputFormat.setInputPaths(mergeJob, intermediateOuts);\r\n    mergeJob.setInputFormat(SequenceFileInputFormat.class);\r\n    mergeJob.setMapperClass(MergeMapper.class);\r\n    FileOutputFormat.setOutputPath(mergeJob, finalOuts);\r\n    mergeJob.setOutputKeyClass(IntWritable.class);\r\n    mergeJob.setOutputValueClass(IntWritable.class);\r\n    mergeJob.setOutputFormat(SequenceFileOutputFormat.class);\r\n    mergeJob.setReducerClass(MergeReducer.class);\r\n    mergeJob.setNumReduceTasks(1);\r\n    JobClient.runJob(mergeJob);\r\n    boolean success = true;\r\n    Path recomputedkey = new Path(finalOuts, \"part-00000\");\r\n    SequenceFile.Reader in = new SequenceFile.Reader(fs, recomputedkey, conf);\r\n    int totalseen = 0;\r\n    try {\r\n        IntWritable key = new IntWritable();\r\n        IntWritable val = new IntWritable();\r\n        for (int i = 0; i < range; i++) {\r\n            if (dist[i] == 0) {\r\n                continue;\r\n            }\r\n            if (!in.next(key, val)) {\r\n                System.err.println(\"Cannot read entry \" + i);\r\n                success = false;\r\n                break;\r\n            } else {\r\n                if (!((key.get() == i) && (val.get() == dist[i]))) {\r\n                    System.err.println(\"Mismatch!  Pos=\" + key.get() + \", i=\" + i + \", val=\" + val.get() + \", dist[i]=\" + dist[i]);\r\n                    success = false;\r\n                }\r\n                totalseen += val.get();\r\n            }\r\n        }\r\n        if (success) {\r\n            if (in.next(key, val)) {\r\n                System.err.println(\"Unnecessary lines in recomputed key!\");\r\n                success = false;\r\n            }\r\n        }\r\n    } finally {\r\n        in.close();\r\n    }\r\n    int originalTotal = 0;\r\n    for (int aDist : dist) {\r\n        originalTotal += aDist;\r\n    }\r\n    System.out.println(\"Original sum: \" + originalTotal);\r\n    System.out.println(\"Recomputed sum: \" + totalseen);\r\n    Path resultFile = new Path(testdir, \"results\");\r\n    BufferedWriter bw = new BufferedWriter(new OutputStreamWriter(fs.create(resultFile)));\r\n    try {\r\n        bw.write(\"Success=\" + success + \"\\n\");\r\n        System.out.println(\"Success=\" + success);\r\n    } finally {\r\n        bw.close();\r\n    }\r\n    assertTrue(\"testMapRed failed\", success);\r\n    fs.delete(testdir, true);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 3,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "printTextFile",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void printTextFile(FileSystem fs, Path p) throws IOException\n{\r\n    BufferedReader in = new BufferedReader(new InputStreamReader(fs.open(p)));\r\n    String line;\r\n    while ((line = in.readLine()) != null) {\r\n        System.out.println(\"  Row: \" + line);\r\n    }\r\n    in.close();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "printSequenceFile",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void printSequenceFile(FileSystem fs, Path p, Configuration conf) throws IOException\n{\r\n    SequenceFile.Reader r = new SequenceFile.Reader(fs, p, conf);\r\n    Object key = null;\r\n    Object value = null;\r\n    while ((key = r.next(key)) != null) {\r\n        value = r.getCurrentValue(value);\r\n        System.out.println(\"  Row: \" + key + \", \" + value);\r\n    }\r\n    r.close();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "isSequenceFile",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "boolean isSequenceFile(FileSystem fs, Path f) throws IOException\n{\r\n    DataInputStream in = fs.open(f);\r\n    byte[] seq = \"SEQ\".getBytes();\r\n    for (int i = 0; i < seq.length; ++i) {\r\n        if (seq[i] != in.read()) {\r\n            return false;\r\n        }\r\n    }\r\n    return true;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "printFiles",
  "errType" : null,
  "containingMethodsNum" : 11,
  "sourceCodeText" : "void printFiles(Path dir, Configuration conf) throws IOException\n{\r\n    FileSystem fs = dir.getFileSystem(conf);\r\n    for (FileStatus f : fs.listStatus(dir)) {\r\n        System.out.println(\"Reading \" + f.getPath() + \": \");\r\n        if (f.isDirectory()) {\r\n            System.out.println(\"  it is a map file.\");\r\n            printSequenceFile(fs, new Path(f.getPath(), \"data\"), conf);\r\n        } else if (isSequenceFile(fs, f.getPath())) {\r\n            System.out.println(\"  it is a sequence file.\");\r\n            printSequenceFile(fs, f.getPath(), conf);\r\n        } else {\r\n            System.out.println(\"  it is a text file.\");\r\n            printTextFile(fs, f.getPath());\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "main",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void main(String[] argv) throws Exception\n{\r\n    int res = ToolRunner.run(new TestMapRed(), argv);\r\n    System.exit(res);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "testSmallInput",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testSmallInput()\n{\r\n    runJob(100);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "testBiggerInput",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testBiggerInput()\n{\r\n    runJob(1000);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "runJob",
  "errType" : [ "Exception" ],
  "containingMethodsNum" : 24,
  "sourceCodeText" : "void runJob(int items)\n{\r\n    try {\r\n        JobConf conf = new JobConf(TestMapRed.class);\r\n        Path testdir = new Path(TEST_DIR.getAbsolutePath());\r\n        Path inDir = new Path(testdir, \"in\");\r\n        Path outDir = new Path(testdir, \"out\");\r\n        FileSystem fs = FileSystem.get(conf);\r\n        fs.delete(testdir, true);\r\n        conf.setInt(JobContext.IO_SORT_MB, 1);\r\n        conf.setInputFormat(SequenceFileInputFormat.class);\r\n        FileInputFormat.setInputPaths(conf, inDir);\r\n        FileOutputFormat.setOutputPath(conf, outDir);\r\n        conf.setMapperClass(IdentityMapper.class);\r\n        conf.setReducerClass(IdentityReducer.class);\r\n        conf.setOutputKeyClass(Text.class);\r\n        conf.setOutputValueClass(Text.class);\r\n        conf.setOutputFormat(SequenceFileOutputFormat.class);\r\n        conf.set(MRConfig.FRAMEWORK_NAME, MRConfig.LOCAL_FRAMEWORK_NAME);\r\n        if (!fs.mkdirs(testdir)) {\r\n            throw new IOException(\"Mkdirs failed to create \" + testdir.toString());\r\n        }\r\n        if (!fs.mkdirs(inDir)) {\r\n            throw new IOException(\"Mkdirs failed to create \" + inDir.toString());\r\n        }\r\n        Path inFile = new Path(inDir, \"part0\");\r\n        SequenceFile.Writer writer = SequenceFile.createWriter(fs, conf, inFile, Text.class, Text.class);\r\n        StringBuffer content = new StringBuffer();\r\n        for (int i = 0; i < 1000; i++) {\r\n            content.append(i).append(\": This is one more line of content\\n\");\r\n        }\r\n        Text text = new Text(content.toString());\r\n        for (int i = 0; i < items; i++) {\r\n            writer.append(new Text(\"rec:\" + i), text);\r\n        }\r\n        writer.close();\r\n        JobClient.runJob(conf);\r\n    } catch (Exception e) {\r\n        assertTrue(\"Threw exception:\" + e, false);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "run",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "int run(String[] argv) throws Exception\n{\r\n    if (argv.length < 2) {\r\n        System.err.println(\"Usage: TestMapRed <range> <counts>\");\r\n        System.err.println();\r\n        System.err.println(\"Note: a good test will have a \" + \"<counts> value that is substantially larger than the <range>\");\r\n        return -1;\r\n    }\r\n    int i = 0;\r\n    range = Integer.parseInt(argv[i++]);\r\n    counts = Integer.parseInt(argv[i++]);\r\n    launch();\r\n    return 0;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "serDeser",
  "errType" : null,
  "containingMethodsNum" : 9,
  "sourceCodeText" : "K serDeser(K conf) throws Exception\n{\r\n    SerializationFactory factory = new SerializationFactory(CONF);\r\n    Serializer<K> serializer = factory.getSerializer(GenericsUtil.getClass(conf));\r\n    Deserializer<K> deserializer = factory.getDeserializer(GenericsUtil.getClass(conf));\r\n    DataOutputBuffer out = new DataOutputBuffer();\r\n    serializer.open(out);\r\n    serializer.serialize(conf);\r\n    serializer.close();\r\n    DataInputBuffer in = new DataInputBuffer();\r\n    in.reset(out.getData(), out.getLength());\r\n    deserializer.open(in);\r\n    K after = deserializer.deserialize(null);\r\n    deserializer.close();\r\n    return after;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "assertEquals",
  "errType" : null,
  "containingMethodsNum" : 11,
  "sourceCodeText" : "void assertEquals(Configuration conf1, Configuration conf2)\n{\r\n    Iterator<Map.Entry<String, String>> iterator1 = conf1.iterator();\r\n    Map<String, String> map1 = new HashMap<String, String>();\r\n    while (iterator1.hasNext()) {\r\n        Map.Entry<String, String> entry = iterator1.next();\r\n        if (!Configuration.isDeprecated(entry.getKey())) {\r\n            map1.put(entry.getKey(), entry.getValue());\r\n        }\r\n    }\r\n    Iterator<Map.Entry<String, String>> iterator2 = conf2.iterator();\r\n    Map<String, String> map2 = new HashMap<String, String>();\r\n    while (iterator2.hasNext()) {\r\n        Map.Entry<String, String> entry = iterator2.next();\r\n        if (!Configuration.isDeprecated(entry.getKey())) {\r\n            map2.put(entry.getKey(), entry.getValue());\r\n        }\r\n    }\r\n    assertTrue(map1.equals(map2));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "testEmptyConfiguration",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void testEmptyConfiguration() throws Exception\n{\r\n    JobConf conf = new JobConf();\r\n    Configuration deser = serDeser(conf);\r\n    assertEquals(conf, deser);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "testNonEmptyConfiguration",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void testNonEmptyConfiguration() throws Exception\n{\r\n    JobConf conf = new JobConf();\r\n    conf.set(\"a\", \"A\");\r\n    conf.set(\"b\", \"B\");\r\n    Configuration deser = serDeser(conf);\r\n    assertEquals(conf, deser);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "testConfigurationWithDefaults",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void testConfigurationWithDefaults() throws Exception\n{\r\n    JobConf conf = new JobConf(false);\r\n    conf.set(\"a\", \"A\");\r\n    conf.set(\"b\", \"B\");\r\n    Configuration deser = serDeser(conf);\r\n    assertEquals(conf, deser);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred\\lib\\db",
  "methodName" : "testConstructQuery",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void testConstructQuery()\n{\r\n    String actual = format.constructQuery(\"hadoop_output\", fieldNames);\r\n    assertEquals(expected, actual);\r\n    actual = format.constructQuery(\"hadoop_output\", nullFieldNames);\r\n    assertEquals(nullExpected, actual);\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred\\lib\\db",
  "methodName" : "testSetOutput",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void testSetOutput() throws IOException\n{\r\n    JobConf job = new JobConf();\r\n    DBOutputFormat.setOutput(job, \"hadoop_output\", fieldNames);\r\n    DBConfiguration dbConf = new DBConfiguration(job);\r\n    String actual = format.constructQuery(dbConf.getOutputTableName(), dbConf.getOutputFieldNames());\r\n    assertEquals(expected, actual);\r\n    job = new JobConf();\r\n    dbConf = new DBConfiguration(job);\r\n    DBOutputFormat.setOutput(job, \"hadoop_output\", nullFieldNames.length);\r\n    assertNull(dbConf.getOutputFieldNames());\r\n    assertEquals(nullFieldNames.length, dbConf.getOutputFieldCount());\r\n    actual = format.constructQuery(dbConf.getOutputTableName(), new String[dbConf.getOutputFieldCount()]);\r\n    assertEquals(nullExpected, actual);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output",
  "methodName" : "setUp",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void setUp() throws Exception\n{\r\n    super.setUp();\r\n    conf = createJobConf();\r\n    fs = getFileSystem();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output",
  "methodName" : "tearDown",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void tearDown() throws Exception\n{\r\n    fs.delete(new Path(TEST_ROOT_DIR), true);\r\n    super.tearDown();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output",
  "methodName" : "getNewOutputDir",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Path getNewOutputDir()\n{\r\n    return new Path(TEST_ROOT_DIR, \"output-\" + outDirs++);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output",
  "methodName" : "testSuccessfulJob",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void testSuccessfulJob(String filename, Class<? extends OutputFormat> output, String[] exclude) throws Exception\n{\r\n    Path outDir = getNewOutputDir();\r\n    Job job = MapReduceTestUtil.createJob(conf, inDir, outDir, 1, 0);\r\n    job.setOutputFormatClass(output);\r\n    assertTrue(\"Job failed!\", job.waitForCompletion(true));\r\n    Path testFile = new Path(outDir, filename);\r\n    assertTrue(\"Done file missing for job \" + job.getJobID(), fs.exists(testFile));\r\n    for (String ex : exclude) {\r\n        Path file = new Path(outDir, ex);\r\n        assertFalse(\"File \" + file + \" should not be present for successful job \" + job.getJobID(), fs.exists(file));\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output",
  "methodName" : "testFailedJob",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void testFailedJob(String fileName, Class<? extends OutputFormat> output, String[] exclude) throws Exception\n{\r\n    Path outDir = getNewOutputDir();\r\n    Job job = MapReduceTestUtil.createFailJob(conf, outDir, inDir);\r\n    job.setOutputFormatClass(output);\r\n    assertFalse(\"Job did not fail!\", job.waitForCompletion(true));\r\n    if (fileName != null) {\r\n        Path testFile = new Path(outDir, fileName);\r\n        assertTrue(\"File \" + testFile + \" missing for failed job \" + job.getJobID(), fs.exists(testFile));\r\n    }\r\n    for (String ex : exclude) {\r\n        Path file = new Path(outDir, ex);\r\n        assertFalse(\"File \" + file + \" should not be present for failed job \" + job.getJobID(), fs.exists(file));\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output",
  "methodName" : "testKilledJob",
  "errType" : null,
  "containingMethodsNum" : 10,
  "sourceCodeText" : "void testKilledJob(String fileName, Class<? extends OutputFormat> output, String[] exclude) throws Exception\n{\r\n    Path outDir = getNewOutputDir();\r\n    Job job = MapReduceTestUtil.createKillJob(conf, outDir, inDir);\r\n    job.setOutputFormatClass(output);\r\n    job.submit();\r\n    while (job.setupProgress() != 1.0f) {\r\n        UtilsForTests.waitFor(100);\r\n    }\r\n    job.killJob();\r\n    assertFalse(\"Job did not get kill\", job.waitForCompletion(true));\r\n    if (fileName != null) {\r\n        Path testFile = new Path(outDir, fileName);\r\n        assertTrue(\"File \" + testFile + \" missing for job \" + job.getJobID(), fs.exists(testFile));\r\n    }\r\n    for (String ex : exclude) {\r\n        Path file = new Path(outDir, ex);\r\n        assertFalse(\"File \" + file + \" should not be present for killed job \" + job.getJobID(), fs.exists(file));\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output",
  "methodName" : "testDefaultCleanupAndAbort",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void testDefaultCleanupAndAbort() throws Exception\n{\r\n    testSuccessfulJob(FileOutputCommitter.SUCCEEDED_FILE_NAME, TextOutputFormat.class, new String[] {});\r\n    testFailedJob(null, TextOutputFormat.class, new String[] { FileOutputCommitter.SUCCEEDED_FILE_NAME });\r\n    testKilledJob(null, TextOutputFormat.class, new String[] { FileOutputCommitter.SUCCEEDED_FILE_NAME });\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output",
  "methodName" : "testCustomAbort",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void testCustomAbort() throws Exception\n{\r\n    testSuccessfulJob(FileOutputCommitter.SUCCEEDED_FILE_NAME, MyOutputFormatWithCustomAbort.class, new String[] { ABORT_FAILED_FILE_NAME, ABORT_KILLED_FILE_NAME });\r\n    testFailedJob(ABORT_FAILED_FILE_NAME, MyOutputFormatWithCustomAbort.class, new String[] { FileOutputCommitter.SUCCEEDED_FILE_NAME, ABORT_KILLED_FILE_NAME });\r\n    testKilledJob(ABORT_KILLED_FILE_NAME, MyOutputFormatWithCustomAbort.class, new String[] { FileOutputCommitter.SUCCEEDED_FILE_NAME, ABORT_FAILED_FILE_NAME });\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output",
  "methodName" : "testCustomCleanup",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void testCustomCleanup() throws Exception\n{\r\n    testSuccessfulJob(CUSTOM_CLEANUP_FILE_NAME, MyOutputFormatWithCustomCleanup.class, new String[] {});\r\n    testFailedJob(CUSTOM_CLEANUP_FILE_NAME, MyOutputFormatWithCustomCleanup.class, new String[] { FileOutputCommitter.SUCCEEDED_FILE_NAME });\r\n    testKilledJob(CUSTOM_CLEANUP_FILE_NAME, MyOutputFormatWithCustomCleanup.class, new String[] { FileOutputCommitter.SUCCEEDED_FILE_NAME });\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\security\\token\\delegation",
  "methodName" : "setup",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void setup() throws Exception\n{\r\n    user1 = UserGroupInformation.createUserForTesting(\"alice\", new String[] { \"users\" });\r\n    user2 = UserGroupInformation.createUserForTesting(\"bob\", new String[] { \"users\" });\r\n    cluster = new MiniMRCluster(0, 0, 1, \"file:///\", 1);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\security\\token\\delegation",
  "methodName" : "testDelegationToken",
  "errType" : [ "AccessControlException", "AccessControlException", "InvalidToken" ],
  "containingMethodsNum" : 19,
  "sourceCodeText" : "void testDelegationToken() throws Exception\n{\r\n    final JobClient client;\r\n    client = user1.doAs(new PrivilegedExceptionAction<JobClient>() {\r\n\r\n        @Override\r\n        public JobClient run() throws Exception {\r\n            return new JobClient(cluster.createJobConf());\r\n        }\r\n    });\r\n    final JobClient bobClient;\r\n    bobClient = user2.doAs(new PrivilegedExceptionAction<JobClient>() {\r\n\r\n        @Override\r\n        public JobClient run() throws Exception {\r\n            return new JobClient(cluster.createJobConf());\r\n        }\r\n    });\r\n    final Token<DelegationTokenIdentifier> token = client.getDelegationToken(new Text(user1.getUserName()));\r\n    DataInputBuffer inBuf = new DataInputBuffer();\r\n    byte[] bytes = token.getIdentifier();\r\n    inBuf.reset(bytes, bytes.length);\r\n    DelegationTokenIdentifier ident = new DelegationTokenIdentifier();\r\n    ident.readFields(inBuf);\r\n    assertEquals(\"alice\", ident.getUser().getUserName());\r\n    long createTime = ident.getIssueDate();\r\n    long maxTime = ident.getMaxDate();\r\n    long currentTime = System.currentTimeMillis();\r\n    System.out.println(\"create time: \" + createTime);\r\n    System.out.println(\"current time: \" + currentTime);\r\n    System.out.println(\"max time: \" + maxTime);\r\n    assertTrue(\"createTime < current\", createTime < currentTime);\r\n    assertTrue(\"current < maxTime\", currentTime < maxTime);\r\n    user1.doAs(new PrivilegedExceptionAction<Void>() {\r\n\r\n        @Override\r\n        public Void run() throws Exception {\r\n            client.renewDelegationToken(token);\r\n            client.renewDelegationToken(token);\r\n            return null;\r\n        }\r\n    });\r\n    user2.doAs(new PrivilegedExceptionAction<Void>() {\r\n\r\n        @Override\r\n        public Void run() throws Exception {\r\n            try {\r\n                bobClient.renewDelegationToken(token);\r\n                Assert.fail(\"bob renew\");\r\n            } catch (AccessControlException ace) {\r\n            }\r\n            return null;\r\n        }\r\n    });\r\n    user2.doAs(new PrivilegedExceptionAction<Void>() {\r\n\r\n        @Override\r\n        public Void run() throws Exception {\r\n            try {\r\n                bobClient.cancelDelegationToken(token);\r\n                Assert.fail(\"bob cancel\");\r\n            } catch (AccessControlException ace) {\r\n            }\r\n            return null;\r\n        }\r\n    });\r\n    user1.doAs(new PrivilegedExceptionAction<Void>() {\r\n\r\n        @Override\r\n        public Void run() throws Exception {\r\n            client.cancelDelegationToken(token);\r\n            try {\r\n                client.cancelDelegationToken(token);\r\n                Assert.fail(\"second alice cancel\");\r\n            } catch (InvalidToken it) {\r\n            }\r\n            return null;\r\n        }\r\n    });\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 3,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred\\pipes",
  "methodName" : "testRunner",
  "errType" : null,
  "containingMethodsNum" : 22,
  "sourceCodeText" : "void testRunner() throws Exception\n{\r\n    File[] psw = cleanTokenPasswordFile();\r\n    try {\r\n        RecordReader<FloatWritable, NullWritable> rReader = new ReaderPipesMapRunner();\r\n        JobConf conf = new JobConf();\r\n        conf.set(Submitter.IS_JAVA_RR, \"true\");\r\n        conf.set(MRJobConfig.TASK_ATTEMPT_ID, taskName);\r\n        CombineOutputCollector<IntWritable, Text> output = new CombineOutputCollector<IntWritable, Text>(new Counters.Counter(), new Progress());\r\n        FileSystem fs = new RawLocalFileSystem();\r\n        fs.initialize(FsConstants.LOCAL_FS_URI, conf);\r\n        Writer<IntWritable, Text> wr = new Writer<IntWritable, Text>(conf, fs.create(new Path(workSpace + File.separator + \"outfile\")), IntWritable.class, Text.class, null, null, true);\r\n        output.setWriter(wr);\r\n        File fCommand = getFileCommand(\"org.apache.hadoop.mapred.pipes.PipeApplicationRunnableStub\");\r\n        conf.set(MRJobConfig.CACHE_LOCALFILES, fCommand.getAbsolutePath());\r\n        Token<AMRMTokenIdentifier> token = new Token<AMRMTokenIdentifier>(\"user\".getBytes(), \"password\".getBytes(), new Text(\"kind\"), new Text(\"service\"));\r\n        TokenCache.setJobToken(token, conf.getCredentials());\r\n        conf.setBoolean(MRJobConfig.SKIP_RECORDS, true);\r\n        TestTaskReporter reporter = new TestTaskReporter();\r\n        PipesMapRunner<FloatWritable, NullWritable, IntWritable, Text> runner = new PipesMapRunner<FloatWritable, NullWritable, IntWritable, Text>();\r\n        initStdOut(conf);\r\n        runner.configure(conf);\r\n        runner.run(rReader, output, reporter);\r\n        String stdOut = readStdOut(conf);\r\n        assertTrue(stdOut.contains(\"CURRENT_PROTOCOL_VERSION:0\"));\r\n        assertTrue(stdOut.contains(\"Key class:org.apache.hadoop.io.FloatWritable\"));\r\n        assertTrue(stdOut.contains(\"Value class:org.apache.hadoop.io.NullWritable\"));\r\n        assertTrue(stdOut.contains(\"value:0.0\"));\r\n        assertTrue(stdOut.contains(\"value:9.0\"));\r\n    } finally {\r\n        if (psw != null) {\r\n            for (File file : psw) {\r\n                file.delete();\r\n            }\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred\\pipes",
  "methodName" : "testApplication",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 33,
  "sourceCodeText" : "void testApplication() throws Throwable\n{\r\n    JobConf conf = new JobConf();\r\n    RecordReader<FloatWritable, NullWritable> rReader = new Reader();\r\n    File fCommand = getFileCommand(\"org.apache.hadoop.mapred.pipes.PipeApplicationStub\");\r\n    TestTaskReporter reporter = new TestTaskReporter();\r\n    File[] psw = cleanTokenPasswordFile();\r\n    try {\r\n        conf.set(MRJobConfig.TASK_ATTEMPT_ID, taskName);\r\n        conf.set(MRJobConfig.CACHE_LOCALFILES, fCommand.getAbsolutePath());\r\n        Token<AMRMTokenIdentifier> token = new Token<AMRMTokenIdentifier>(\"user\".getBytes(), \"password\".getBytes(), new Text(\"kind\"), new Text(\"service\"));\r\n        TokenCache.setJobToken(token, conf.getCredentials());\r\n        FakeCollector output = new FakeCollector(new Counters.Counter(), new Progress());\r\n        FileSystem fs = new RawLocalFileSystem();\r\n        fs.initialize(FsConstants.LOCAL_FS_URI, conf);\r\n        Writer<IntWritable, Text> wr = new Writer<IntWritable, Text>(conf, fs.create(new Path(workSpace.getAbsolutePath() + File.separator + \"outfile\")), IntWritable.class, Text.class, null, null, true);\r\n        output.setWriter(wr);\r\n        conf.set(Submitter.PRESERVE_COMMANDFILE, \"true\");\r\n        initStdOut(conf);\r\n        Application<WritableComparable<IntWritable>, Writable, IntWritable, Text> application = new Application<WritableComparable<IntWritable>, Writable, IntWritable, Text>(conf, rReader, output, reporter, IntWritable.class, Text.class);\r\n        application.getDownlink().flush();\r\n        application.getDownlink().mapItem(new IntWritable(3), new Text(\"txt\"));\r\n        application.getDownlink().flush();\r\n        application.waitForFinish();\r\n        wr.close();\r\n        String stdOut = readStdOut(conf);\r\n        assertTrue(stdOut.contains(\"key:3\"));\r\n        assertTrue(stdOut.contains(\"value:txt\"));\r\n        assertEquals(1.0, reporter.getProgress(), 0.01);\r\n        assertNotNull(reporter.getCounter(\"group\", \"name\"));\r\n        assertEquals(reporter.getStatus(), \"PROGRESS\");\r\n        stdOut = readFile(new File(workSpace.getAbsolutePath() + File.separator + \"outfile\"));\r\n        assertEquals(0.55f, rReader.getProgress(), 0.001);\r\n        application.getDownlink().close();\r\n        Entry<IntWritable, Text> entry = output.getCollect().entrySet().iterator().next();\r\n        assertEquals(123, entry.getKey().get());\r\n        assertEquals(\"value\", entry.getValue().toString());\r\n        try {\r\n            application.abort(new Throwable());\r\n            fail();\r\n        } catch (IOException e) {\r\n            assertEquals(\"pipe child exception\", e.getMessage());\r\n        }\r\n    } finally {\r\n        if (psw != null) {\r\n            for (File file : psw) {\r\n                file.delete();\r\n            }\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 2,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred\\pipes",
  "methodName" : "testSubmitter",
  "errType" : [ "ExitUtil.ExitException", "ExitUtil.ExitException" ],
  "containingMethodsNum" : 51,
  "sourceCodeText" : "void testSubmitter() throws Exception\n{\r\n    JobConf conf = new JobConf();\r\n    File[] psw = cleanTokenPasswordFile();\r\n    System.setProperty(\"test.build.data\", \"target/tmp/build/TEST_SUBMITTER_MAPPER/data\");\r\n    conf.set(\"hadoop.log.dir\", \"target/tmp\");\r\n    Submitter.setIsJavaMapper(conf, false);\r\n    Submitter.setIsJavaReducer(conf, false);\r\n    Submitter.setKeepCommandFile(conf, false);\r\n    Submitter.setIsJavaRecordReader(conf, false);\r\n    Submitter.setIsJavaRecordWriter(conf, false);\r\n    PipesPartitioner<IntWritable, Text> partitioner = new PipesPartitioner<IntWritable, Text>();\r\n    partitioner.configure(conf);\r\n    Submitter.setJavaPartitioner(conf, partitioner.getClass());\r\n    assertEquals(PipesPartitioner.class, (Submitter.getJavaPartitioner(conf)));\r\n    SecurityManager securityManager = System.getSecurityManager();\r\n    PrintStream oldps = System.out;\r\n    ByteArrayOutputStream out = new ByteArrayOutputStream();\r\n    ExitUtil.disableSystemExit();\r\n    try {\r\n        System.setOut(new PrintStream(out));\r\n        Submitter.main(new String[0]);\r\n        fail();\r\n    } catch (ExitUtil.ExitException e) {\r\n        assertTrue(out.toString().contains(\"\"));\r\n        assertTrue(out.toString(), out.toString().contains(\"pipes\"));\r\n        assertTrue(out.toString().contains(\"[-input <path>] // Input directory\"));\r\n        assertTrue(out.toString().contains(\"[-output <path>] // Output directory\"));\r\n        assertTrue(out.toString().contains(\"[-jar <jar file> // jar filename\"));\r\n        assertTrue(out.toString().contains(\"[-inputformat <class>] // InputFormat class\"));\r\n        assertTrue(out.toString().contains(\"[-map <class>] // Java Map class\"));\r\n        assertTrue(out.toString().contains(\"[-partitioner <class>] // Java Partitioner\"));\r\n        assertTrue(out.toString().contains(\"[-reduce <class>] // Java Reduce class\"));\r\n        assertTrue(out.toString().contains(\"[-writer <class>] // Java RecordWriter\"));\r\n        assertTrue(out.toString().contains(\"[-program <executable>] // executable URI\"));\r\n        assertTrue(out.toString().contains(\"[-reduces <num>] // number of reduces\"));\r\n        assertTrue(out.toString().contains(\"[-lazyOutput <true/false>] // createOutputLazily\"));\r\n        assertTrue(out.toString().contains(\"-conf <configuration file>        specify an application \" + \"configuration file\"));\r\n        assertTrue(out.toString().contains(\"-D <property=value>               define a value for a given \" + \"property\"));\r\n        assertTrue(out.toString().contains(\"-fs <file:///|hdfs://namenode:port> \" + \"specify default filesystem URL to use, overrides \" + \"'fs.defaultFS' property from configurations.\"));\r\n        assertTrue(out.toString().contains(\"-jt <local|resourcemanager:port>  specify a ResourceManager\"));\r\n        assertTrue(out.toString().contains(\"-files <file1,...>                specify a comma-separated list of \" + \"files to be copied to the map reduce cluster\"));\r\n        assertTrue(out.toString().contains(\"-libjars <jar1,...>               specify a comma-separated list of \" + \"jar files to be included in the classpath\"));\r\n        assertTrue(out.toString().contains(\"-archives <archive1,...>          specify a comma-separated list of \" + \"archives to be unarchived on the compute machines\"));\r\n    } finally {\r\n        System.setOut(oldps);\r\n        System.setSecurityManager(securityManager);\r\n        if (psw != null) {\r\n            for (File file : psw) {\r\n                file.delete();\r\n            }\r\n        }\r\n    }\r\n    try {\r\n        File fCommand = getFileCommand(null);\r\n        String[] args = new String[22];\r\n        File input = new File(workSpace + File.separator + \"input\");\r\n        if (!input.exists()) {\r\n            Assert.assertTrue(input.createNewFile());\r\n        }\r\n        File outPut = new File(workSpace + File.separator + \"output\");\r\n        FileUtil.fullyDelete(outPut);\r\n        args[0] = \"-input\";\r\n        args[1] = input.getAbsolutePath();\r\n        args[2] = \"-output\";\r\n        args[3] = outPut.getAbsolutePath();\r\n        args[4] = \"-inputformat\";\r\n        args[5] = \"org.apache.hadoop.mapred.TextInputFormat\";\r\n        args[6] = \"-map\";\r\n        args[7] = \"org.apache.hadoop.mapred.lib.IdentityMapper\";\r\n        args[8] = \"-partitioner\";\r\n        args[9] = \"org.apache.hadoop.mapred.pipes.PipesPartitioner\";\r\n        args[10] = \"-reduce\";\r\n        args[11] = \"org.apache.hadoop.mapred.lib.IdentityReducer\";\r\n        args[12] = \"-writer\";\r\n        args[13] = \"org.apache.hadoop.mapred.TextOutputFormat\";\r\n        args[14] = \"-program\";\r\n        args[15] = fCommand.getAbsolutePath();\r\n        args[16] = \"-reduces\";\r\n        args[17] = \"2\";\r\n        args[18] = \"-lazyOutput\";\r\n        args[19] = \"lazyOutput\";\r\n        args[20] = \"-jobconf\";\r\n        args[21] = \"mapreduce.pipes.isjavarecordwriter=false,mapreduce.pipes.isjavarecordreader=false\";\r\n        Submitter.main(args);\r\n        fail();\r\n    } catch (ExitUtil.ExitException e) {\r\n        assertEquals(e.status, 0);\r\n    } finally {\r\n        System.setOut(oldps);\r\n        System.setSecurityManager(securityManager);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 2,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred\\pipes",
  "methodName" : "testPipesReduser",
  "errType" : null,
  "containingMethodsNum" : 21,
  "sourceCodeText" : "void testPipesReduser() throws Exception\n{\r\n    File[] psw = cleanTokenPasswordFile();\r\n    JobConf conf = new JobConf();\r\n    try {\r\n        Token<AMRMTokenIdentifier> token = new Token<AMRMTokenIdentifier>(\"user\".getBytes(), \"password\".getBytes(), new Text(\"kind\"), new Text(\"service\"));\r\n        TokenCache.setJobToken(token, conf.getCredentials());\r\n        File fCommand = getFileCommand(\"org.apache.hadoop.mapred.pipes.PipeReducerStub\");\r\n        conf.set(MRJobConfig.CACHE_LOCALFILES, fCommand.getAbsolutePath());\r\n        PipesReducer<BooleanWritable, Text, IntWritable, Text> reducer = new PipesReducer<BooleanWritable, Text, IntWritable, Text>();\r\n        reducer.configure(conf);\r\n        BooleanWritable bw = new BooleanWritable(true);\r\n        conf.set(MRJobConfig.TASK_ATTEMPT_ID, taskName);\r\n        initStdOut(conf);\r\n        conf.setBoolean(MRJobConfig.SKIP_RECORDS, true);\r\n        CombineOutputCollector<IntWritable, Text> output = new CombineOutputCollector<IntWritable, Text>(new Counters.Counter(), new Progress());\r\n        Reporter reporter = new TestTaskReporter();\r\n        List<Text> texts = new ArrayList<Text>();\r\n        texts.add(new Text(\"first\"));\r\n        texts.add(new Text(\"second\"));\r\n        texts.add(new Text(\"third\"));\r\n        reducer.reduce(bw, texts.iterator(), output, reporter);\r\n        reducer.close();\r\n        String stdOut = readStdOut(conf);\r\n        assertTrue(stdOut.contains(\"reducer key :true\"));\r\n        assertTrue(stdOut.contains(\"reduce value  :first\"));\r\n        assertTrue(stdOut.contains(\"reduce value  :second\"));\r\n        assertTrue(stdOut.contains(\"reduce value  :third\"));\r\n    } finally {\r\n        if (psw != null) {\r\n            for (File file : psw) {\r\n                file.delete();\r\n            }\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred\\pipes",
  "methodName" : "testPipesPartitioner",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void testPipesPartitioner()\n{\r\n    PipesPartitioner<IntWritable, Text> partitioner = new PipesPartitioner<IntWritable, Text>();\r\n    JobConf configuration = new JobConf();\r\n    Submitter.getJavaPartitioner(configuration);\r\n    partitioner.configure(new JobConf());\r\n    IntWritable iw = new IntWritable(4);\r\n    assertEquals(0, partitioner.getPartition(iw, new Text(\"test\"), 2));\r\n    PipesPartitioner.setNextPartition(3);\r\n    assertEquals(3, partitioner.getPartition(iw, new Text(\"test\"), 2));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred\\pipes",
  "methodName" : "testSocketCleaner",
  "errType" : [ "Exception" ],
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void testSocketCleaner() throws Exception\n{\r\n    ServerSocket serverSocket = setupServerSocket();\r\n    SocketCleaner cleaner = setupCleaner(serverSocket);\r\n    int expectedClosedCount = 5;\r\n    for (int i = 0; i < expectedClosedCount; i++) {\r\n        try {\r\n            Thread.sleep(1000);\r\n            Socket clientSocket = new Socket(serverSocket.getInetAddress(), serverSocket.getLocalPort());\r\n            clientSocket.close();\r\n        } catch (Exception exception) {\r\n            exception.printStackTrace();\r\n        }\r\n    }\r\n    GenericTestUtils.waitFor(() -> expectedClosedCount == cleaner.getCloseSocketCount(), 100, 5000);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred\\pipes",
  "methodName" : "testSocketTimeout",
  "errType" : [ "Exception" ],
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void testSocketTimeout() throws Exception\n{\r\n    ServerSocket serverSocket = setupServerSocket();\r\n    SocketCleaner cleaner = setupCleaner(serverSocket, 100);\r\n    try {\r\n        new Socket(serverSocket.getInetAddress(), serverSocket.getLocalPort());\r\n        Thread.sleep(1000);\r\n    } catch (Exception exception) {\r\n    }\r\n    GenericTestUtils.waitFor(() -> 1 == cleaner.getCloseSocketCount(), 100, 5000);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred\\pipes",
  "methodName" : "setupCleaner",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "SocketCleaner setupCleaner(ServerSocket serverSocket)\n{\r\n    return setupCleaner(serverSocket, CommonConfigurationKeys.IPC_PING_INTERVAL_DEFAULT);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred\\pipes",
  "methodName" : "setupCleaner",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "SocketCleaner setupCleaner(ServerSocket serverSocket, int soTimeout)\n{\r\n    SocketCleaner cleaner = new SocketCleaner(\"test-ping-socket-cleaner\", serverSocket, soTimeout);\r\n    cleaner.setDaemon(true);\r\n    cleaner.start();\r\n    return cleaner;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred\\pipes",
  "methodName" : "setupServerSocket",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "ServerSocket setupServerSocket() throws Exception\n{\r\n    return new ServerSocket(0, 1);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred\\pipes",
  "methodName" : "initStdOut",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void initStdOut(JobConf configuration)\n{\r\n    TaskAttemptID taskId = TaskAttemptID.forName(configuration.get(MRJobConfig.TASK_ATTEMPT_ID));\r\n    File stdOut = TaskLog.getTaskLogFile(taskId, false, TaskLog.LogName.STDOUT);\r\n    File stdErr = TaskLog.getTaskLogFile(taskId, false, TaskLog.LogName.STDERR);\r\n    if (!stdOut.getParentFile().exists()) {\r\n        stdOut.getParentFile().mkdirs();\r\n    } else {\r\n        stdOut.deleteOnExit();\r\n        stdErr.deleteOnExit();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred\\pipes",
  "methodName" : "readStdOut",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "String readStdOut(JobConf conf) throws Exception\n{\r\n    TaskAttemptID taskId = TaskAttemptID.forName(conf.get(MRJobConfig.TASK_ATTEMPT_ID));\r\n    File stdOut = TaskLog.getTaskLogFile(taskId, false, TaskLog.LogName.STDOUT);\r\n    return readFile(stdOut);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred\\pipes",
  "methodName" : "readFile",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "String readFile(File file) throws Exception\n{\r\n    ByteArrayOutputStream out = new ByteArrayOutputStream();\r\n    InputStream is = new FileInputStream(file);\r\n    byte[] buffer = new byte[1024];\r\n    int counter = 0;\r\n    while ((counter = is.read(buffer)) >= 0) {\r\n        out.write(buffer, 0, counter);\r\n    }\r\n    is.close();\r\n    return out.toString();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred\\pipes",
  "methodName" : "cleanTokenPasswordFile",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "File[] cleanTokenPasswordFile() throws Exception\n{\r\n    File[] result = new File[2];\r\n    result[0] = new File(\"./jobTokenPassword\");\r\n    if (result[0].exists()) {\r\n        FileUtil.chmod(result[0].getAbsolutePath(), \"700\");\r\n        assertTrue(result[0].delete());\r\n    }\r\n    result[1] = new File(\"./.jobTokenPassword.crc\");\r\n    if (result[1].exists()) {\r\n        FileUtil.chmod(result[1].getAbsolutePath(), \"700\");\r\n        result[1].delete();\r\n    }\r\n    return result;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred\\pipes",
  "methodName" : "getFileCommand",
  "errType" : null,
  "containingMethodsNum" : 11,
  "sourceCodeText" : "File getFileCommand(String clazz) throws Exception\n{\r\n    String classpath = System.getProperty(\"java.class.path\");\r\n    File fCommand = new File(workSpace + File.separator + \"cache.sh\");\r\n    fCommand.deleteOnExit();\r\n    if (!fCommand.getParentFile().exists()) {\r\n        fCommand.getParentFile().mkdirs();\r\n    }\r\n    fCommand.createNewFile();\r\n    OutputStream os = new FileOutputStream(fCommand);\r\n    os.write(\"#!/bin/sh \\n\".getBytes());\r\n    if (clazz == null) {\r\n        os.write((\"ls \").getBytes());\r\n    } else {\r\n        os.write((\"java -cp \" + classpath + \" \" + clazz).getBytes());\r\n    }\r\n    os.flush();\r\n    os.close();\r\n    FileUtil.chmod(fCommand.getAbsolutePath(), \"700\");\r\n    return fCommand;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getRack",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "String getRack(int id, int level)\n{\r\n    StringBuilder rack = new StringBuilder();\r\n    char alpha = 'a';\r\n    int length = level + 1;\r\n    while (length > level) {\r\n        rack.append(\"/\");\r\n        rack.append(alpha);\r\n        ++alpha;\r\n        --length;\r\n    }\r\n    while (length > 0) {\r\n        rack.append(\"/\");\r\n        rack.append(alpha);\r\n        rack.append(id);\r\n        ++alpha;\r\n        --length;\r\n    }\r\n    return rack.toString();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "testMultiLevelCaching",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testMultiLevelCaching() throws Exception\n{\r\n    for (int i = 1; i <= MAX_LEVEL; ++i) {\r\n        testCachingAtLevel(i);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "testCachingAtLevel",
  "errType" : null,
  "containingMethodsNum" : 16,
  "sourceCodeText" : "void testCachingAtLevel(int level) throws Exception\n{\r\n    String namenode = null;\r\n    MiniDFSCluster dfs = null;\r\n    MiniMRCluster mr = null;\r\n    FileSystem fileSys = null;\r\n    String testName = \"TestMultiLevelCaching\";\r\n    try {\r\n        final int taskTrackers = 1;\r\n        String rack1 = getRack(0, level);\r\n        String rack2 = getRack(1, level);\r\n        Configuration conf = new Configuration();\r\n        dfs = new MiniDFSCluster.Builder(conf).racks(new String[] { rack1 }).hosts(new String[] { \"host1.com\" }).build();\r\n        dfs.waitActive();\r\n        fileSys = dfs.getFileSystem();\r\n        if (!fileSys.mkdirs(inDir)) {\r\n            throw new IOException(\"Mkdirs failed to create \" + inDir.toString());\r\n        }\r\n        UtilsForTests.writeFile(dfs.getNameNode(), conf, new Path(inDir + \"/file\"), (short) 1);\r\n        namenode = (dfs.getFileSystem()).getUri().getHost() + \":\" + (dfs.getFileSystem()).getUri().getPort();\r\n        JobConf jc = new JobConf();\r\n        jc.setInt(JTConfig.JT_TASKCACHE_LEVELS, level + 2);\r\n        mr = new MiniMRCluster(taskTrackers, namenode, 1, new String[] { rack2 }, new String[] { \"host2.com\" }, jc);\r\n        launchJobAndTestCounters(testName, mr, fileSys, inDir, outputPath, 1, 1, 0, 0);\r\n        mr.shutdown();\r\n    } finally {\r\n        if (null != fileSys) {\r\n            fileSys.delete(inDir, true);\r\n            fileSys.delete(outputPath, true);\r\n        }\r\n        if (dfs != null) {\r\n            dfs.shutdown();\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "launchJobAndTestCounters",
  "errType" : null,
  "containingMethodsNum" : 10,
  "sourceCodeText" : "void launchJobAndTestCounters(String jobName, MiniMRCluster mr, FileSystem fileSys, Path in, Path out, int numMaps, int otherLocalMaps, int dataLocalMaps, int rackLocalMaps) throws IOException\n{\r\n    JobConf jobConf = mr.createJobConf();\r\n    if (fileSys.exists(out)) {\r\n        fileSys.delete(out, true);\r\n    }\r\n    RunningJob job = launchJob(jobConf, in, out, numMaps, jobName);\r\n    Counters counters = job.getCounters();\r\n    assertEquals(\"Number of local maps\", counters.getCounter(JobCounter.OTHER_LOCAL_MAPS), otherLocalMaps);\r\n    assertEquals(\"Number of Data-local maps\", counters.getCounter(JobCounter.DATA_LOCAL_MAPS), dataLocalMaps);\r\n    assertEquals(\"Number of Rack-local maps\", counters.getCounter(JobCounter.RACK_LOCAL_MAPS), rackLocalMaps);\r\n    mr.waitUntilIdle();\r\n    mr.shutdown();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "launchJob",
  "errType" : null,
  "containingMethodsNum" : 13,
  "sourceCodeText" : "RunningJob launchJob(JobConf jobConf, Path inDir, Path outputPath, int numMaps, String jobName) throws IOException\n{\r\n    jobConf.setJobName(jobName);\r\n    jobConf.setInputFormat(NonSplitableSequenceFileInputFormat.class);\r\n    jobConf.setOutputFormat(SequenceFileOutputFormat.class);\r\n    FileInputFormat.setInputPaths(jobConf, inDir);\r\n    FileOutputFormat.setOutputPath(jobConf, outputPath);\r\n    jobConf.setMapperClass(IdentityMapper.class);\r\n    jobConf.setReducerClass(IdentityReducer.class);\r\n    jobConf.setOutputKeyClass(BytesWritable.class);\r\n    jobConf.setOutputValueClass(BytesWritable.class);\r\n    jobConf.setNumMapTasks(numMaps);\r\n    jobConf.setNumReduceTasks(0);\r\n    jobConf.setJar(\"build/test/mapred/testjar/testjob.jar\");\r\n    return JobClient.runJob(jobConf);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2",
  "methodName" : "setUp",
  "errType" : null,
  "containingMethodsNum" : 19,
  "sourceCodeText" : "void setUp() throws Exception\n{\r\n    if (System.getProperty(\"hadoop.log.dir\") == null) {\r\n        System.setProperty(\"hadoop.log.dir\", \"/tmp\");\r\n    }\r\n    int taskTrackers = 2;\r\n    int dataNodes = 2;\r\n    String proxyUser = System.getProperty(\"user.name\");\r\n    String proxyGroup = \"g\";\r\n    StringBuilder sb = new StringBuilder();\r\n    sb.append(\"127.0.0.1,localhost\");\r\n    for (InetAddress i : InetAddress.getAllByName(InetAddress.getLocalHost().getHostName())) {\r\n        sb.append(\",\").append(i.getCanonicalHostName());\r\n    }\r\n    JobConf conf = new JobConf();\r\n    conf.set(\"dfs.block.access.token.enable\", \"false\");\r\n    conf.set(\"dfs.permissions\", \"true\");\r\n    conf.set(\"hadoop.security.authentication\", \"simple\");\r\n    dfsCluster = new MiniDFSCluster.Builder(conf).numDataNodes(dataNodes).build();\r\n    FileSystem fileSystem = dfsCluster.getFileSystem();\r\n    fileSystem.mkdirs(new Path(\"/tmp\"));\r\n    fileSystem.mkdirs(new Path(\"/user\"));\r\n    fileSystem.mkdirs(new Path(\"/hadoop/mapred/system\"));\r\n    fileSystem.setPermission(new Path(\"/tmp\"), FsPermission.valueOf(\"-rwxrwxrwx\"));\r\n    fileSystem.setPermission(new Path(\"/user\"), FsPermission.valueOf(\"-rwxrwxrwx\"));\r\n    fileSystem.setPermission(new Path(\"/hadoop/mapred/system\"), FsPermission.valueOf(\"-rwx------\"));\r\n    String nnURI = fileSystem.getUri().toString();\r\n    int numDirs = 1;\r\n    String[] racks = null;\r\n    String[] hosts = null;\r\n    mrCluster = new MiniMRCluster(0, 0, taskTrackers, nnURI, numDirs, racks, hosts, null, conf);\r\n    ProxyUsers.refreshSuperUserGroupsConfiguration(conf);\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2",
  "methodName" : "getJobConf",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "JobConf getJobConf()\n{\r\n    return mrCluster.createJobConf();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2",
  "methodName" : "tearDown",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void tearDown() throws Exception\n{\r\n    if (mrCluster != null) {\r\n        mrCluster.shutdown();\r\n    }\r\n    if (dfsCluster != null) {\r\n        dfsCluster.shutdown();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2",
  "methodName" : "testGetInvalidJob",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void testGetInvalidJob() throws Exception\n{\r\n    RunningJob runJob = new JobClient(getJobConf()).getJob(JobID.forName(\"job_0_0\"));\r\n    assertNull(runJob);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "testPeriodStatsets",
  "errType" : null,
  "containingMethodsNum" : 29,
  "sourceCodeText" : "void testPeriodStatsets()\n{\r\n    PeriodicStatsAccumulator cumulative = new CumulativePeriodicStats(8);\r\n    PeriodicStatsAccumulator status = new StatePeriodicStats(8);\r\n    cumulative.extend(0.0D, 0);\r\n    cumulative.extend(0.4375D, 700);\r\n    cumulative.extend(0.5625D, 1100);\r\n    cumulative.extend(0.625D, 1300);\r\n    cumulative.extend(1.0D, 7901);\r\n    int total = 0;\r\n    int[] results = cumulative.getValues();\r\n    for (int i = 0; i < 8; ++i) {\r\n        System.err.println(\"segment i = \" + results[i]);\r\n    }\r\n    assertEquals(\"Bad interpolation in cumulative segment 0\", 200, results[0]);\r\n    assertEquals(\"Bad interpolation in cumulative segment 1\", 200, results[1]);\r\n    assertEquals(\"Bad interpolation in cumulative segment 2\", 200, results[2]);\r\n    assertEquals(\"Bad interpolation in cumulative segment 3\", 300, results[3]);\r\n    assertEquals(\"Bad interpolation in cumulative segment 4\", 400, results[4]);\r\n    assertEquals(\"Bad interpolation in cumulative segment 5\", 2200, results[5]);\r\n    assertEquals(\"Bad interpolation in cumulative segment 6\", 2200, results[6]);\r\n    assertEquals(\"Bad interpolation in cumulative segment 7\", 2201, results[7]);\r\n    status.extend(0.0D, 0);\r\n    status.extend(1.0D / 16.0D, 300);\r\n    status.extend(3.0D / 16.0D, 700);\r\n    status.extend(7.0D / 16.0D, 2300);\r\n    status.extend(1.0D, 1400);\r\n    ;\r\n    results = status.getValues();\r\n    assertEquals(\"Bad interpolation in status segment 0\", 275, results[0]);\r\n    assertEquals(\"Bad interpolation in status segment 1\", 750, results[1]);\r\n    assertEquals(\"Bad interpolation in status segment 2\", 1500, results[2]);\r\n    assertEquals(\"Bad interpolation in status segment 3\", 2175, results[3]);\r\n    assertEquals(\"Bad interpolation in status segment 4\", 2100, results[4]);\r\n    assertEquals(\"Bad interpolation in status segment 5\", 1900, results[5]);\r\n    assertEquals(\"Bad interpolation in status segment 6\", 1700, results[6]);\r\n    assertEquals(\"Bad interpolation in status segment 7\", 1500, results[7]);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop",
  "methodName" : "map",
  "errType" : [ "InterruptedException" ],
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void map(Text key, Text value, Context context) throws IOException, InterruptedException\n{\r\n    new Thread() {\r\n\r\n        @Override\r\n        public void run() {\r\n            synchronized (this) {\r\n                try {\r\n                    wait();\r\n                } catch (InterruptedException e) {\r\n                }\r\n            }\r\n        }\r\n    }.start();\r\n    if (context.getTaskAttemptID().getId() == 0) {\r\n        System.out.println(\"Attempt:\" + context.getTaskAttemptID() + \" Failing mapper throwing exception\");\r\n        throw new IOException(\"Attempt:\" + context.getTaskAttemptID() + \" Failing mapper throwing exception\");\r\n    } else {\r\n        System.out.println(\"Attempt:\" + context.getTaskAttemptID() + \" Exiting\");\r\n        System.exit(-1);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\fs\\slive",
  "methodName" : "getBaseConfig",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Configuration getBaseConfig()\n{\r\n    Configuration conf = new Configuration();\r\n    return conf;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\fs\\slive",
  "methodName" : "getWriteLoc",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "File getWriteLoc()\n{\r\n    String writeLoc = System.getProperty(TEST_DATA_PROP, \"build/test/data/\");\r\n    File writeDir = new File(writeLoc, \"slive\");\r\n    writeDir.mkdirs();\r\n    return writeDir;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\fs\\slive",
  "methodName" : "getFlowLocation",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "File getFlowLocation()\n{\r\n    return new File(getWriteLoc(), \"flow\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\fs\\slive",
  "methodName" : "getTestDir",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "File getTestDir()\n{\r\n    return new File(getWriteLoc(), \"slivedir\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\fs\\slive",
  "methodName" : "getTestFile",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "File getTestFile()\n{\r\n    return new File(getWriteLoc(), \"slivefile\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\fs\\slive",
  "methodName" : "getTestRenameFile",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "File getTestRenameFile()\n{\r\n    return new File(getWriteLoc(), \"slivefile1\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\fs\\slive",
  "methodName" : "getResultFile",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "File getResultFile()\n{\r\n    return new File(getWriteLoc(), \"sliveresfile\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\fs\\slive",
  "methodName" : "getImaginaryFile",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "File getImaginaryFile()\n{\r\n    return new File(getWriteLoc(), \"slivenofile\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\fs\\slive",
  "methodName" : "getTestArgs",
  "errType" : null,
  "containingMethodsNum" : 29,
  "sourceCodeText" : "String[] getTestArgs(boolean sleep)\n{\r\n    List<String> args = new LinkedList<String>();\r\n    {\r\n        args.add(\"-\" + ConfigOption.WRITE_SIZE.getOpt());\r\n        args.add(\"1M,2M\");\r\n        args.add(\"-\" + ConfigOption.OPS.getOpt());\r\n        args.add(Constants.OperationType.values().length + \"\");\r\n        args.add(\"-\" + ConfigOption.MAPS.getOpt());\r\n        args.add(\"2\");\r\n        args.add(\"-\" + ConfigOption.REDUCES.getOpt());\r\n        args.add(\"2\");\r\n        args.add(\"-\" + ConfigOption.APPEND_SIZE.getOpt());\r\n        args.add(\"1M,2M\");\r\n        args.add(\"-\" + ConfigOption.BLOCK_SIZE.getOpt());\r\n        args.add(\"1M,2M\");\r\n        args.add(\"-\" + ConfigOption.REPLICATION_AM.getOpt());\r\n        args.add(\"1,1\");\r\n        if (sleep) {\r\n            args.add(\"-\" + ConfigOption.SLEEP_TIME.getOpt());\r\n            args.add(\"10,10\");\r\n        }\r\n        args.add(\"-\" + ConfigOption.RESULT_FILE.getOpt());\r\n        args.add(getResultFile().toString());\r\n        args.add(\"-\" + ConfigOption.BASE_DIR.getOpt());\r\n        args.add(getFlowLocation().toString());\r\n        args.add(\"-\" + ConfigOption.DURATION.getOpt());\r\n        args.add(\"10\");\r\n        args.add(\"-\" + ConfigOption.DIR_SIZE.getOpt());\r\n        args.add(\"10\");\r\n        args.add(\"-\" + ConfigOption.FILES.getOpt());\r\n        args.add(\"10\");\r\n        args.add(\"-\" + ConfigOption.TRUNCATE_SIZE.getOpt());\r\n        args.add(\"0,1M\");\r\n    }\r\n    return args.toArray(new String[args.size()]);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\fs\\slive",
  "methodName" : "testFinder",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void testFinder() throws Exception\n{\r\n    ConfigExtractor extractor = getTestConfig(false);\r\n    PathFinder fr = new PathFinder(extractor, rnd);\r\n    int maxIterations = 10000;\r\n    Set<Path> files = new HashSet<Path>();\r\n    for (int i = 0; i < maxIterations; i++) {\r\n        files.add(fr.getFile());\r\n    }\r\n    assertTrue(files.size() == 10);\r\n    Set<Path> dirs = new HashSet<Path>();\r\n    for (int i = 0; i < maxIterations; i++) {\r\n        dirs.add(fr.getDirectory());\r\n    }\r\n    assertTrue(dirs.size() == 10);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\fs\\slive",
  "methodName" : "testSelection",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void testSelection() throws Exception\n{\r\n    ConfigExtractor extractor = getTestConfig(false);\r\n    WeightSelector selector = new WeightSelector(extractor, rnd);\r\n    int expected = OperationType.values().length;\r\n    Operation op = null;\r\n    Set<String> types = new HashSet<String>();\r\n    FileSystem fs = FileSystem.get(extractor.getConfig());\r\n    while (true) {\r\n        op = selector.select(1, 1);\r\n        if (op == null) {\r\n            break;\r\n        }\r\n        op.run(fs);\r\n        types.add(op.getType());\r\n    }\r\n    assertThat(types.size()).isEqualTo(expected);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\fs\\slive",
  "methodName" : "getTestConfig",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "ConfigExtractor getTestConfig(boolean sleep) throws Exception\n{\r\n    ArgumentParser parser = new ArgumentParser(getTestArgs(sleep));\r\n    ParsedOutput out = parser.parse();\r\n    assertTrue(!out.shouldOutputHelp());\r\n    ConfigMerger merge = new ConfigMerger();\r\n    Configuration cfg = merge.getMerged(out, getBaseConfig());\r\n    ConfigExtractor extractor = new ConfigExtractor(cfg);\r\n    return extractor;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\fs\\slive",
  "methodName" : "ensureDeleted",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void ensureDeleted() throws Exception\n{\r\n    rDelete(getTestFile());\r\n    rDelete(getTestDir());\r\n    rDelete(getTestRenameFile());\r\n    rDelete(getResultFile());\r\n    rDelete(getFlowLocation());\r\n    rDelete(getImaginaryFile());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\fs\\slive",
  "methodName" : "rDelete",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void rDelete(File place) throws Exception\n{\r\n    if (place.isFile()) {\r\n        LOG.info(\"Deleting file \" + place);\r\n        assertTrue(place.delete());\r\n    } else if (place.isDirectory()) {\r\n        deleteDir(place);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\fs\\slive",
  "methodName" : "deleteDir",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void deleteDir(File dir) throws Exception\n{\r\n    String[] fns = dir.list();\r\n    for (String afn : fns) {\r\n        File fn = new File(dir, afn);\r\n        rDelete(fn);\r\n    }\r\n    LOG.info(\"Deleting directory \" + dir);\r\n    assertTrue(dir.delete());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\fs\\slive",
  "methodName" : "testArguments",
  "errType" : null,
  "containingMethodsNum" : 20,
  "sourceCodeText" : "void testArguments() throws Exception\n{\r\n    ConfigExtractor extractor = getTestConfig(true);\r\n    assertEquals(extractor.getOpCount().intValue(), Constants.OperationType.values().length);\r\n    assertThat(extractor.getMapAmount().intValue()).isEqualTo(2);\r\n    assertThat(extractor.getReducerAmount().intValue()).isEqualTo(2);\r\n    Range<Long> apRange = extractor.getAppendSize();\r\n    assertThat(apRange.getLower().intValue()).isEqualTo(Constants.MEGABYTES * 1);\r\n    assertThat(apRange.getUpper().intValue()).isEqualTo(Constants.MEGABYTES * 2);\r\n    Range<Long> wRange = extractor.getWriteSize();\r\n    assertThat(wRange.getLower().intValue()).isEqualTo(Constants.MEGABYTES * 1);\r\n    assertThat(wRange.getUpper().intValue()).isEqualTo(Constants.MEGABYTES * 2);\r\n    Range<Long> trRange = extractor.getTruncateSize();\r\n    assertThat(trRange.getLower().intValue()).isZero();\r\n    assertThat(trRange.getUpper().intValue()).isEqualTo(Constants.MEGABYTES * 1);\r\n    Range<Long> bRange = extractor.getBlockSize();\r\n    assertThat(bRange.getLower().intValue()).isEqualTo(Constants.MEGABYTES * 1);\r\n    assertThat(bRange.getUpper().intValue()).isEqualTo(Constants.MEGABYTES * 2);\r\n    String resfile = extractor.getResultFile();\r\n    assertEquals(resfile, getResultFile().toString());\r\n    int durationMs = extractor.getDurationMilliseconds();\r\n    assertThat(durationMs).isEqualTo(10 * 1000);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\fs\\slive",
  "methodName" : "testDataWriting",
  "errType" : null,
  "containingMethodsNum" : 10,
  "sourceCodeText" : "void testDataWriting() throws Exception\n{\r\n    long byteAm = 100;\r\n    File fn = getTestFile();\r\n    DataWriter writer = new DataWriter(rnd);\r\n    FileOutputStream fs = new FileOutputStream(fn);\r\n    GenerateOutput ostat = writer.writeSegment(byteAm, fs);\r\n    LOG.info(ostat.toString());\r\n    fs.close();\r\n    assertTrue(ostat.getBytesWritten() == byteAm);\r\n    DataVerifier vf = new DataVerifier();\r\n    FileInputStream fin = new FileInputStream(fn);\r\n    VerifyOutput vfout = vf.verifyFile(byteAm, new DataInputStream(fin));\r\n    LOG.info(vfout.toString());\r\n    fin.close();\r\n    assertEquals(vfout.getBytesRead(), byteAm);\r\n    assertTrue(vfout.getChunksDifferent() == 0);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\fs\\slive",
  "methodName" : "testRange",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void testRange()\n{\r\n    Range<Long> r = new Range<Long>(10L, 20L);\r\n    assertThat(r.getLower().longValue()).isEqualTo(10L);\r\n    assertThat(r.getUpper().longValue()).isEqualTo(20L);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\fs\\slive",
  "methodName" : "testCreateOp",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void testCreateOp() throws Exception\n{\r\n    ConfigExtractor extractor = getTestConfig(false);\r\n    final Path fn = new Path(getTestFile().getCanonicalPath());\r\n    CreateOp op = new CreateOp(extractor, rnd) {\r\n\r\n        protected Path getCreateFile() {\r\n            return fn;\r\n        }\r\n    };\r\n    runOperationOk(extractor, op, true);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\fs\\slive",
  "methodName" : "testOpFailures",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void testOpFailures() throws Exception\n{\r\n    ConfigExtractor extractor = getTestConfig(false);\r\n    final Path fn = new Path(getImaginaryFile().getCanonicalPath());\r\n    ReadOp rop = new ReadOp(extractor, rnd) {\r\n\r\n        protected Path getReadFile() {\r\n            return fn;\r\n        }\r\n    };\r\n    runOperationBad(extractor, rop);\r\n    DeleteOp dop = new DeleteOp(extractor, rnd) {\r\n\r\n        protected Path getDeleteFile() {\r\n            return fn;\r\n        }\r\n    };\r\n    runOperationBad(extractor, dop);\r\n    RenameOp reop = new RenameOp(extractor, rnd) {\r\n\r\n        protected SrcTarget getRenames() {\r\n            return new SrcTarget(fn, fn);\r\n        }\r\n    };\r\n    runOperationBad(extractor, reop);\r\n    AppendOp aop = new AppendOp(extractor, rnd) {\r\n\r\n        protected Path getAppendFile() {\r\n            return fn;\r\n        }\r\n    };\r\n    runOperationBad(extractor, aop);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\fs\\slive",
  "methodName" : "runOperationBad",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void runOperationBad(ConfigExtractor cfg, Operation op) throws Exception\n{\r\n    FileSystem fs = FileSystem.get(cfg.getConfig());\r\n    List<OperationOutput> data = op.run(fs);\r\n    assertTrue(!data.isEmpty());\r\n    boolean foundFail = false;\r\n    for (OperationOutput d : data) {\r\n        if (d.getMeasurementType().equals(ReportWriter.FAILURES)) {\r\n            foundFail = true;\r\n        }\r\n        if (d.getMeasurementType().equals(ReportWriter.NOT_FOUND)) {\r\n            foundFail = true;\r\n        }\r\n    }\r\n    assertTrue(foundFail);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\fs\\slive",
  "methodName" : "runOperationOk",
  "errType" : null,
  "containingMethodsNum" : 10,
  "sourceCodeText" : "void runOperationOk(ConfigExtractor cfg, Operation op, boolean checkOk) throws Exception\n{\r\n    FileSystem fs = FileSystem.get(cfg.getConfig());\r\n    List<OperationOutput> data = op.run(fs);\r\n    assertTrue(!data.isEmpty());\r\n    if (checkOk) {\r\n        boolean foundSuc = false;\r\n        boolean foundOpCount = false;\r\n        boolean foundTime = false;\r\n        for (OperationOutput d : data) {\r\n            assertTrue(!d.getMeasurementType().equals(ReportWriter.FAILURES));\r\n            if (d.getMeasurementType().equals(ReportWriter.SUCCESSES)) {\r\n                foundSuc = true;\r\n            }\r\n            if (d.getMeasurementType().equals(ReportWriter.OP_COUNT)) {\r\n                foundOpCount = true;\r\n            }\r\n            if (d.getMeasurementType().equals(ReportWriter.OK_TIME_TAKEN)) {\r\n                foundTime = true;\r\n            }\r\n        }\r\n        assertTrue(foundSuc);\r\n        assertTrue(foundOpCount);\r\n        assertTrue(foundTime);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\fs\\slive",
  "methodName" : "testDelete",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void testDelete() throws Exception\n{\r\n    ConfigExtractor extractor = getTestConfig(false);\r\n    final Path fn = new Path(getTestFile().getCanonicalPath());\r\n    CreateOp op = new CreateOp(extractor, rnd) {\r\n\r\n        protected Path getCreateFile() {\r\n            return fn;\r\n        }\r\n    };\r\n    runOperationOk(extractor, op, true);\r\n    DeleteOp dop = new DeleteOp(extractor, rnd) {\r\n\r\n        protected Path getDeleteFile() {\r\n            return fn;\r\n        }\r\n    };\r\n    runOperationOk(extractor, dop, true);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\fs\\slive",
  "methodName" : "testRename",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void testRename() throws Exception\n{\r\n    ConfigExtractor extractor = getTestConfig(false);\r\n    final Path src = new Path(getTestFile().getCanonicalPath());\r\n    final Path tgt = new Path(getTestRenameFile().getCanonicalPath());\r\n    CreateOp op = new CreateOp(extractor, rnd) {\r\n\r\n        protected Path getCreateFile() {\r\n            return src;\r\n        }\r\n    };\r\n    runOperationOk(extractor, op, true);\r\n    RenameOp rop = new RenameOp(extractor, rnd) {\r\n\r\n        protected SrcTarget getRenames() {\r\n            return new SrcTarget(src, tgt);\r\n        }\r\n    };\r\n    runOperationOk(extractor, rop, true);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\fs\\slive",
  "methodName" : "testMRFlow",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void testMRFlow() throws Exception\n{\r\n    ConfigExtractor extractor = getTestConfig(false);\r\n    SliveTest s = new SliveTest(getBaseConfig());\r\n    int ec = ToolRunner.run(s, getTestArgs(false));\r\n    assertTrue(ec == 0);\r\n    String resFile = extractor.getResultFile();\r\n    File fn = new File(resFile);\r\n    assertTrue(fn.exists());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\fs\\slive",
  "methodName" : "testRead",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void testRead() throws Exception\n{\r\n    ConfigExtractor extractor = getTestConfig(false);\r\n    final Path fn = new Path(getTestFile().getCanonicalPath());\r\n    CreateOp op = new CreateOp(extractor, rnd) {\r\n\r\n        protected Path getCreateFile() {\r\n            return fn;\r\n        }\r\n    };\r\n    runOperationOk(extractor, op, true);\r\n    ReadOp rop = new ReadOp(extractor, rnd) {\r\n\r\n        protected Path getReadFile() {\r\n            return fn;\r\n        }\r\n    };\r\n    runOperationOk(extractor, rop, true);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\fs\\slive",
  "methodName" : "testSleep",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void testSleep() throws Exception\n{\r\n    ConfigExtractor extractor = getTestConfig(true);\r\n    SleepOp op = new SleepOp(extractor, rnd);\r\n    runOperationOk(extractor, op, true);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\fs\\slive",
  "methodName" : "testList",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void testList() throws Exception\n{\r\n    ConfigExtractor extractor = getTestConfig(false);\r\n    final Path dir = new Path(getTestDir().getCanonicalPath());\r\n    MkdirOp op = new MkdirOp(extractor, rnd) {\r\n\r\n        protected Path getDirectory() {\r\n            return dir;\r\n        }\r\n    };\r\n    runOperationOk(extractor, op, true);\r\n    ListOp lop = new ListOp(extractor, rnd) {\r\n\r\n        protected Path getDirectory() {\r\n            return dir;\r\n        }\r\n    };\r\n    runOperationOk(extractor, lop, true);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\fs\\slive",
  "methodName" : "testBadChunks",
  "errType" : [ "Exception" ],
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void testBadChunks() throws Exception\n{\r\n    File fn = getTestFile();\r\n    int byteAm = 10000;\r\n    FileOutputStream fout = new FileOutputStream(fn);\r\n    byte[] bytes = new byte[byteAm];\r\n    rnd.nextBytes(bytes);\r\n    fout.write(bytes);\r\n    fout.close();\r\n    DataVerifier vf = new DataVerifier();\r\n    VerifyOutput vout = new VerifyOutput(0, 0, 0, 0);\r\n    DataInputStream in = null;\r\n    try {\r\n        in = new DataInputStream(new FileInputStream(fn));\r\n        vout = vf.verifyFile(byteAm, in);\r\n    } catch (Exception e) {\r\n    } finally {\r\n        if (in != null)\r\n            in.close();\r\n    }\r\n    assertTrue(vout.getChunksSame() == 0);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\fs\\slive",
  "methodName" : "testMkdir",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void testMkdir() throws Exception\n{\r\n    ConfigExtractor extractor = getTestConfig(false);\r\n    final Path dir = new Path(getTestDir().getCanonicalPath());\r\n    MkdirOp op = new MkdirOp(extractor, rnd) {\r\n\r\n        protected Path getDirectory() {\r\n            return dir;\r\n        }\r\n    };\r\n    runOperationOk(extractor, op, true);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\fs\\slive",
  "methodName" : "testSelector",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void testSelector() throws Exception\n{\r\n    ConfigExtractor extractor = getTestConfig(false);\r\n    RouletteSelector selector = new RouletteSelector(rnd);\r\n    List<OperationWeight> sList = new LinkedList<OperationWeight>();\r\n    Operation op = selector.select(sList);\r\n    assertTrue(op == null);\r\n    CreateOp cop = new CreateOp(extractor, rnd);\r\n    sList.add(new OperationWeight(cop, 1.0d));\r\n    AppendOp aop = new AppendOp(extractor, rnd);\r\n    sList.add(new OperationWeight(aop, 0.01d));\r\n    op = selector.select(sList);\r\n    assertTrue(op == cop);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\fs\\slive",
  "methodName" : "testAppendOp",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void testAppendOp() throws Exception\n{\r\n    ConfigExtractor extractor = getTestConfig(false);\r\n    final Path fn = new Path(getTestFile().getCanonicalPath());\r\n    CreateOp op = new CreateOp(extractor, rnd) {\r\n\r\n        protected Path getCreateFile() {\r\n            return fn;\r\n        }\r\n    };\r\n    runOperationOk(extractor, op, true);\r\n    AppendOp aop = new AppendOp(extractor, rnd) {\r\n\r\n        protected Path getAppendFile() {\r\n            return fn;\r\n        }\r\n    };\r\n    runOperationOk(extractor, aop, false);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\fs\\slive",
  "methodName" : "testTruncateOp",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void testTruncateOp() throws Exception\n{\r\n    ConfigExtractor extractor = getTestConfig(false);\r\n    final Path fn = new Path(getTestFile().getCanonicalPath());\r\n    CreateOp op = new CreateOp(extractor, rnd) {\r\n\r\n        protected Path getCreateFile() {\r\n            return fn;\r\n        }\r\n    };\r\n    runOperationOk(extractor, op, true);\r\n    TruncateOp top = new TruncateOp(extractor, rnd) {\r\n\r\n        protected Path getTruncateFile() {\r\n            return fn;\r\n        }\r\n    };\r\n    runOperationOk(extractor, top, false);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\util",
  "methodName" : "setUp",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setUp()\n{\r\n    FileUtil.fullyDelete(new File(TEST_ROOT_DIR));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\util",
  "methodName" : "relativeToWorking",
  "errType" : null,
  "containingMethodsNum" : 17,
  "sourceCodeText" : "String relativeToWorking(String pathname)\n{\r\n    String cwd = System.getProperty(\"user.dir\", \"/\");\r\n    pathname = (new Path(pathname)).toUri().getPath();\r\n    cwd = (new Path(cwd)).toUri().getPath();\r\n    String[] cwdParts = cwd.split(Path.SEPARATOR);\r\n    String[] pathParts = pathname.split(Path.SEPARATOR);\r\n    if (cwd.equals(pathname)) {\r\n        LOG.info(\"relative to working: \" + pathname + \" -> .\");\r\n        return \".\";\r\n    }\r\n    int common = 0;\r\n    for (int i = 0; i < Math.min(cwdParts.length, pathParts.length); i++) {\r\n        if (cwdParts[i].equals(pathParts[i])) {\r\n            common++;\r\n        } else {\r\n            break;\r\n        }\r\n    }\r\n    StringBuilder sb = new StringBuilder();\r\n    int parentDirsRequired = cwdParts.length - common;\r\n    for (int i = 0; i < parentDirsRequired; i++) {\r\n        sb.append(\"..\");\r\n        sb.append(Path.SEPARATOR);\r\n    }\r\n    for (int i = common; i < pathParts.length; i++) {\r\n        sb.append(pathParts[i]);\r\n        sb.append(Path.SEPARATOR);\r\n    }\r\n    String s = sb.toString();\r\n    if (s.endsWith(Path.SEPARATOR)) {\r\n        s = s.substring(0, s.length() - 1);\r\n    }\r\n    LOG.info(\"relative to working: \" + pathname + \" -> \" + s);\r\n    return s;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\util",
  "methodName" : "testRelativeToWorking",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void testRelativeToWorking()\n{\r\n    assertEquals(\".\", relativeToWorking(System.getProperty(\"user.dir\", \".\")));\r\n    String cwd = System.getProperty(\"user.dir\", \".\");\r\n    Path cwdPath = new Path(cwd);\r\n    Path subdir = new Path(cwdPath, \"foo\");\r\n    assertEquals(\"foo\", relativeToWorking(subdir.toUri().getPath()));\r\n    Path subsubdir = new Path(subdir, \"bar\");\r\n    assertEquals(\"foo/bar\", relativeToWorking(subsubdir.toUri().getPath()));\r\n    Path parent = new Path(cwdPath, \"..\");\r\n    assertEquals(\"..\", relativeToWorking(parent.toUri().getPath()));\r\n    Path sideways = new Path(parent, \"baz\");\r\n    assertEquals(\"../baz\", relativeToWorking(sideways.toUri().getPath()));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\util",
  "methodName" : "testVolumeNormalization",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void testVolumeNormalization() throws Throwable\n{\r\n    LOG.info(\"TEST_ROOT_DIR is \" + TEST_ROOT_DIR);\r\n    String relativeTestRoot = relativeToWorking(TEST_ROOT_DIR);\r\n    FileSystem localFileSystem = FileSystem.getLocal(new Configuration());\r\n    String[] vols = new String[] { relativeTestRoot + \"/0\", relativeTestRoot + \"/1\" };\r\n    Path delDir = new Path(vols[0], MRAsyncDiskService.TOBEDELETED);\r\n    localFileSystem.mkdirs(delDir);\r\n    localFileSystem.create(new Path(delDir, \"foo\")).close();\r\n    MRAsyncDiskService service = new MRAsyncDiskService(localFileSystem, vols);\r\n    makeSureCleanedUp(vols, service);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\util",
  "methodName" : "testMRAsyncDiskService",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 19,
  "sourceCodeText" : "void testMRAsyncDiskService() throws Throwable\n{\r\n    FileSystem localFileSystem = FileSystem.getLocal(new Configuration());\r\n    String[] vols = new String[] { TEST_ROOT_DIR + \"/0\", TEST_ROOT_DIR + \"/1\" };\r\n    MRAsyncDiskService service = new MRAsyncDiskService(localFileSystem, vols);\r\n    String a = \"a\";\r\n    String b = \"b\";\r\n    String c = \"b/c\";\r\n    String d = \"d\";\r\n    File fa = new File(vols[0], a);\r\n    File fb = new File(vols[1], b);\r\n    File fc = new File(vols[1], c);\r\n    File fd = new File(vols[1], d);\r\n    fa.mkdirs();\r\n    fb.mkdirs();\r\n    fc.mkdirs();\r\n    fd.mkdirs();\r\n    assertTrue(fa.exists());\r\n    assertTrue(fb.exists());\r\n    assertTrue(fc.exists());\r\n    assertTrue(fd.exists());\r\n    service.moveAndDeleteRelativePath(vols[0], a);\r\n    assertFalse(fa.exists());\r\n    service.moveAndDeleteRelativePath(vols[1], b);\r\n    assertFalse(fb.exists());\r\n    assertFalse(fc.exists());\r\n    assertFalse(service.moveAndDeleteRelativePath(vols[1], \"not_exists\"));\r\n    IOException ee = null;\r\n    try {\r\n        service.moveAndDeleteAbsolutePath(TEST_ROOT_DIR + \"/2\");\r\n    } catch (IOException e) {\r\n        ee = e;\r\n    }\r\n    assertNotNull(\"asyncDiskService should not be able to delete files \" + \"outside all volumes\", ee);\r\n    assertTrue(service.moveAndDeleteAbsolutePath(vols[1] + Path.SEPARATOR_CHAR + d));\r\n    makeSureCleanedUp(vols, service);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\util",
  "methodName" : "testMRAsyncDiskServiceMoveAndDeleteAllVolumes",
  "errType" : null,
  "containingMethodsNum" : 15,
  "sourceCodeText" : "void testMRAsyncDiskServiceMoveAndDeleteAllVolumes() throws Throwable\n{\r\n    FileSystem localFileSystem = FileSystem.getLocal(new Configuration());\r\n    String[] vols = new String[] { TEST_ROOT_DIR + \"/0\", TEST_ROOT_DIR + \"/1\" };\r\n    MRAsyncDiskService service = new MRAsyncDiskService(localFileSystem, vols);\r\n    String a = \"a\";\r\n    String b = \"b\";\r\n    String c = \"b/c\";\r\n    String d = \"d\";\r\n    File fa = new File(vols[0], a);\r\n    File fb = new File(vols[1], b);\r\n    File fc = new File(vols[1], c);\r\n    File fd = new File(vols[1], d);\r\n    fa.mkdirs();\r\n    fb.mkdirs();\r\n    fc.mkdirs();\r\n    fd.mkdirs();\r\n    assertTrue(fa.exists());\r\n    assertTrue(fb.exists());\r\n    assertTrue(fc.exists());\r\n    assertTrue(fd.exists());\r\n    service.cleanupAllVolumes();\r\n    assertFalse(fa.exists());\r\n    assertFalse(fb.exists());\r\n    assertFalse(fc.exists());\r\n    assertFalse(fd.exists());\r\n    makeSureCleanedUp(vols, service);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\util",
  "methodName" : "testMRAsyncDiskServiceStartupCleaning",
  "errType" : null,
  "containingMethodsNum" : 10,
  "sourceCodeText" : "void testMRAsyncDiskServiceStartupCleaning() throws Throwable\n{\r\n    FileSystem localFileSystem = FileSystem.getLocal(new Configuration());\r\n    String[] vols = new String[] { TEST_ROOT_DIR + \"/0\", TEST_ROOT_DIR + \"/1\" };\r\n    String a = \"a\";\r\n    String b = \"b\";\r\n    String c = \"b/c\";\r\n    String d = \"d\";\r\n    String suffix = Path.SEPARATOR_CHAR + MRAsyncDiskService.TOBEDELETED;\r\n    File fa = new File(vols[0] + suffix, a);\r\n    File fb = new File(vols[1] + suffix, b);\r\n    File fc = new File(vols[1] + suffix, c);\r\n    File fd = new File(vols[1] + suffix, d);\r\n    fa.mkdirs();\r\n    fb.mkdirs();\r\n    fc.mkdirs();\r\n    fd.mkdirs();\r\n    assertTrue(fa.exists());\r\n    assertTrue(fb.exists());\r\n    assertTrue(fc.exists());\r\n    assertTrue(fd.exists());\r\n    MRAsyncDiskService service = new MRAsyncDiskService(localFileSystem, vols);\r\n    makeSureCleanedUp(vols, service);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\util",
  "methodName" : "makeSureCleanedUp",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void makeSureCleanedUp(String[] vols, MRAsyncDiskService service) throws Throwable\n{\r\n    service.shutdown();\r\n    if (!service.awaitTermination(5000)) {\r\n        fail(\"MRAsyncDiskService is still not shutdown in 5 seconds!\");\r\n    }\r\n    for (int i = 0; i < vols.length; i++) {\r\n        File subDir = new File(vols[0]);\r\n        String[] subDirContent = subDir.list();\r\n        assertEquals(\"Volume should contain a single child: \" + MRAsyncDiskService.TOBEDELETED, 1, subDirContent.length);\r\n        File toBeDeletedDir = new File(vols[0], MRAsyncDiskService.TOBEDELETED);\r\n        String[] content = toBeDeletedDir.list();\r\n        assertNotNull(\"Cannot find \" + toBeDeletedDir, content);\r\n        assertThat(content).withFailMessage(toBeDeletedDir.toString() + \" should be empty now.\").isEmpty();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\util",
  "methodName" : "testToleratesSomeUnwritableVolumes",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void testToleratesSomeUnwritableVolumes() throws Throwable\n{\r\n    FileSystem localFileSystem = FileSystem.getLocal(new Configuration());\r\n    String[] vols = new String[] { TEST_ROOT_DIR + \"/0\", TEST_ROOT_DIR + \"/1\" };\r\n    assertTrue(new File(vols[0]).mkdirs());\r\n    assertEquals(0, FileUtil.chmod(vols[0], \"400\"));\r\n    try {\r\n        new MRAsyncDiskService(localFileSystem, vols);\r\n    } finally {\r\n        FileUtil.chmod(vols[0], \"755\");\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\testshell",
  "methodName" : "configure",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void configure(JobConf job)\n{\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\testshell",
  "methodName" : "close",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void close() throws IOException\n{\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\testshell",
  "methodName" : "run",
  "errType" : [ "ClassNotFoundException" ],
  "containingMethodsNum" : 11,
  "sourceCodeText" : "int run(String[] argv) throws IOException\n{\r\n    if (argv.length < 2) {\r\n        System.out.println(\"ExternalMapReduce <input> <output>\");\r\n        return -1;\r\n    }\r\n    Path outDir = new Path(argv[1]);\r\n    Path input = new Path(argv[0]);\r\n    JobConf testConf = new JobConf(getConf(), ExternalMapReduce.class);\r\n    try {\r\n        testConf.getClassByName(\"testjar.ClassWordCount\");\r\n    } catch (ClassNotFoundException e) {\r\n        System.out.println(\"Could not find class from libjar\");\r\n        return -1;\r\n    }\r\n    testConf.setJobName(\"external job\");\r\n    FileInputFormat.setInputPaths(testConf, input);\r\n    FileOutputFormat.setOutputPath(testConf, outDir);\r\n    testConf.setMapperClass(MapClass.class);\r\n    testConf.setReducerClass(Reduce.class);\r\n    testConf.setNumReduceTasks(1);\r\n    JobClient.runJob(testConf);\r\n    return 0;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\testshell",
  "methodName" : "main",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void main(String[] args) throws Exception\n{\r\n    int res = ToolRunner.run(new Configuration(), new ExternalMapReduce(), args);\r\n    System.exit(res);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "data",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Collection<Object[]> data()\n{\r\n    Object[][] data = new Object[][] { { true }, { false } };\r\n    return Arrays.asList(data);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "testUnknownAppInRM",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void testUnknownAppInRM() throws Exception\n{\r\n    MRClientProtocol historyServerProxy = mock(MRClientProtocol.class);\r\n    when(historyServerProxy.getJobReport(getJobReportRequest())).thenReturn(getJobReportResponse());\r\n    ClientServiceDelegate clientServiceDelegate = getClientServiceDelegate(historyServerProxy, getRMDelegate());\r\n    JobStatus jobStatus = clientServiceDelegate.getJobStatus(oldJobId);\r\n    Assert.assertNotNull(jobStatus);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "testRemoteExceptionFromHistoryServer",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void testRemoteExceptionFromHistoryServer() throws Exception\n{\r\n    MRClientProtocol historyServerProxy = mock(MRClientProtocol.class);\r\n    when(historyServerProxy.getJobReport(getJobReportRequest())).thenThrow(new IOException(\"Job ID doesnot Exist\"));\r\n    ResourceMgrDelegate rm = mock(ResourceMgrDelegate.class);\r\n    when(rm.getApplicationReport(TypeConverter.toYarn(oldJobId).getAppId())).thenReturn(null);\r\n    ClientServiceDelegate clientServiceDelegate = getClientServiceDelegate(historyServerProxy, rm);\r\n    try {\r\n        clientServiceDelegate.getJobStatus(oldJobId);\r\n        Assert.fail(\"Invoke should throw exception after retries.\");\r\n    } catch (IOException e) {\r\n        Assert.assertTrue(e.getMessage().contains(\"Job ID doesnot Exist\"));\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "testRetriesOnConnectionFailure",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void testRetriesOnConnectionFailure() throws Exception\n{\r\n    MRClientProtocol historyServerProxy = mock(MRClientProtocol.class);\r\n    when(historyServerProxy.getJobReport(getJobReportRequest())).thenThrow(new RuntimeException(\"1\")).thenThrow(new RuntimeException(\"2\")).thenReturn(getJobReportResponse());\r\n    ResourceMgrDelegate rm = mock(ResourceMgrDelegate.class);\r\n    when(rm.getApplicationReport(TypeConverter.toYarn(oldJobId).getAppId())).thenReturn(null);\r\n    ClientServiceDelegate clientServiceDelegate = getClientServiceDelegate(historyServerProxy, rm);\r\n    JobStatus jobStatus = clientServiceDelegate.getJobStatus(oldJobId);\r\n    Assert.assertNotNull(jobStatus);\r\n    verify(historyServerProxy, times(3)).getJobReport(any(GetJobReportRequest.class));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "testRetriesOnAMConnectionFailures",
  "errType" : null,
  "containingMethodsNum" : 11,
  "sourceCodeText" : "void testRetriesOnAMConnectionFailures() throws Exception\n{\r\n    if (!isAMReachableFromClient) {\r\n        return;\r\n    }\r\n    ResourceMgrDelegate rm = mock(ResourceMgrDelegate.class);\r\n    when(rm.getApplicationReport(TypeConverter.toYarn(oldJobId).getAppId())).thenReturn(getRunningApplicationReport(\"am1\", 78));\r\n    final MRClientProtocol amProxy = mock(MRClientProtocol.class);\r\n    when(amProxy.getJobReport(any(GetJobReportRequest.class))).thenThrow(new RuntimeException(\"11\")).thenThrow(new RuntimeException(\"22\")).thenThrow(new RuntimeException(\"33\")).thenThrow(new RuntimeException(\"44\")).thenReturn(getJobReportResponse());\r\n    Configuration conf = new YarnConfiguration();\r\n    conf.set(MRConfig.FRAMEWORK_NAME, MRConfig.YARN_FRAMEWORK_NAME);\r\n    conf.setBoolean(MRJobConfig.JOB_AM_ACCESS_DISABLED, !isAMReachableFromClient);\r\n    ClientServiceDelegate clientServiceDelegate = new ClientServiceDelegate(conf, rm, oldJobId, null) {\r\n\r\n        @Override\r\n        MRClientProtocol instantiateAMProxy(final InetSocketAddress serviceAddr) throws IOException {\r\n            super.instantiateAMProxy(serviceAddr);\r\n            return amProxy;\r\n        }\r\n    };\r\n    JobStatus jobStatus = clientServiceDelegate.getJobStatus(oldJobId);\r\n    Assert.assertNotNull(jobStatus);\r\n    Assert.assertEquals(conf.getInt(MRJobConfig.MR_CLIENT_MAX_RETRIES, MRJobConfig.DEFAULT_MR_CLIENT_MAX_RETRIES), clientServiceDelegate.getMaxClientRetry());\r\n    verify(amProxy, times(5)).getJobReport(any(GetJobReportRequest.class));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "testNoRetryOnAMAuthorizationException",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 12,
  "sourceCodeText" : "void testNoRetryOnAMAuthorizationException() throws Exception\n{\r\n    if (!isAMReachableFromClient) {\r\n        return;\r\n    }\r\n    ResourceMgrDelegate rm = mock(ResourceMgrDelegate.class);\r\n    when(rm.getApplicationReport(TypeConverter.toYarn(oldJobId).getAppId())).thenReturn(getRunningApplicationReport(\"am1\", 78));\r\n    final MRClientProtocol amProxy = mock(MRClientProtocol.class);\r\n    when(amProxy.getJobReport(any(GetJobReportRequest.class))).thenThrow(new AuthorizationException(\"Denied\"));\r\n    Configuration conf = new YarnConfiguration();\r\n    conf.set(MRConfig.FRAMEWORK_NAME, MRConfig.YARN_FRAMEWORK_NAME);\r\n    conf.setBoolean(MRJobConfig.JOB_AM_ACCESS_DISABLED, !isAMReachableFromClient);\r\n    ClientServiceDelegate clientServiceDelegate = new ClientServiceDelegate(conf, rm, oldJobId, null) {\r\n\r\n        @Override\r\n        MRClientProtocol instantiateAMProxy(final InetSocketAddress serviceAddr) throws IOException {\r\n            super.instantiateAMProxy(serviceAddr);\r\n            return amProxy;\r\n        }\r\n    };\r\n    try {\r\n        clientServiceDelegate.getJobStatus(oldJobId);\r\n        Assert.fail(\"Exception should be thrown upon AuthorizationException\");\r\n    } catch (IOException e) {\r\n        Assert.assertEquals(AuthorizationException.class.getName() + \": Denied\", e.getMessage());\r\n    }\r\n    Assert.assertEquals(conf.getInt(MRJobConfig.MR_CLIENT_MAX_RETRIES, MRJobConfig.DEFAULT_MR_CLIENT_MAX_RETRIES), clientServiceDelegate.getMaxClientRetry());\r\n    verify(amProxy, times(1)).getJobReport(any(GetJobReportRequest.class));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "testHistoryServerNotConfigured",
  "errType" : null,
  "containingMethodsNum" : 11,
  "sourceCodeText" : "void testHistoryServerNotConfigured() throws Exception\n{\r\n    ClientServiceDelegate clientServiceDelegate = getClientServiceDelegate(null, getRMDelegate());\r\n    JobStatus jobStatus = clientServiceDelegate.getJobStatus(oldJobId);\r\n    Assert.assertEquals(\"N/A\", jobStatus.getUsername());\r\n    Assert.assertEquals(JobStatus.State.PREP, jobStatus.getState());\r\n    ResourceMgrDelegate rm = mock(ResourceMgrDelegate.class);\r\n    ApplicationReport applicationReport = getFinishedApplicationReport();\r\n    when(rm.getApplicationReport(jobId.getAppId())).thenReturn(applicationReport);\r\n    clientServiceDelegate = getClientServiceDelegate(null, rm);\r\n    jobStatus = clientServiceDelegate.getJobStatus(oldJobId);\r\n    Assert.assertEquals(applicationReport.getUser(), jobStatus.getUsername());\r\n    Assert.assertEquals(JobStatus.State.SUCCEEDED, jobStatus.getState());\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "testJobReportFromHistoryServer",
  "errType" : null,
  "containingMethodsNum" : 11,
  "sourceCodeText" : "void testJobReportFromHistoryServer() throws Exception\n{\r\n    MRClientProtocol historyServerProxy = mock(MRClientProtocol.class);\r\n    when(historyServerProxy.getJobReport(getJobReportRequest())).thenReturn(getJobReportResponseFromHistoryServer());\r\n    ResourceMgrDelegate rm = mock(ResourceMgrDelegate.class);\r\n    when(rm.getApplicationReport(TypeConverter.toYarn(oldJobId).getAppId())).thenReturn(null);\r\n    ClientServiceDelegate clientServiceDelegate = getClientServiceDelegate(historyServerProxy, rm);\r\n    JobStatus jobStatus = clientServiceDelegate.getJobStatus(oldJobId);\r\n    Assert.assertNotNull(jobStatus);\r\n    Assert.assertEquals(\"TestJobFilePath\", jobStatus.getJobFile());\r\n    Assert.assertEquals(\"http://TestTrackingUrl\", jobStatus.getTrackingUrl());\r\n    Assert.assertEquals(1.0f, jobStatus.getMapProgress(), 0.0f);\r\n    Assert.assertEquals(1.0f, jobStatus.getReduceProgress(), 0.0f);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "testCountersFromHistoryServer",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void testCountersFromHistoryServer() throws Exception\n{\r\n    MRClientProtocol historyServerProxy = mock(MRClientProtocol.class);\r\n    when(historyServerProxy.getCounters(getCountersRequest())).thenReturn(getCountersResponseFromHistoryServer());\r\n    ResourceMgrDelegate rm = mock(ResourceMgrDelegate.class);\r\n    when(rm.getApplicationReport(TypeConverter.toYarn(oldJobId).getAppId())).thenReturn(null);\r\n    ClientServiceDelegate clientServiceDelegate = getClientServiceDelegate(historyServerProxy, rm);\r\n    Counters counters = TypeConverter.toYarn(clientServiceDelegate.getJobCounters(oldJobId));\r\n    Assert.assertNotNull(counters);\r\n    Assert.assertEquals(1001, counters.getCounterGroup(\"dummyCounters\").getCounter(\"dummyCounter\").getValue());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "testReconnectOnAMRestart",
  "errType" : [ "YarnException" ],
  "containingMethodsNum" : 23,
  "sourceCodeText" : "void testReconnectOnAMRestart() throws IOException\n{\r\n    if (!isAMReachableFromClient) {\r\n        return;\r\n    }\r\n    MRClientProtocol historyServerProxy = mock(MRClientProtocol.class);\r\n    ResourceMgrDelegate rmDelegate = mock(ResourceMgrDelegate.class);\r\n    try {\r\n        when(rmDelegate.getApplicationReport(jobId.getAppId())).thenReturn(getRunningApplicationReport(\"am1\", 78)).thenReturn(getRunningApplicationReport(null, 0)).thenReturn(getRunningApplicationReport(null, 0)).thenReturn(getRunningApplicationReport(\"am2\", 90));\r\n    } catch (YarnException e) {\r\n        throw new IOException(e);\r\n    }\r\n    GetJobReportResponse jobReportResponse1 = mock(GetJobReportResponse.class);\r\n    when(jobReportResponse1.getJobReport()).thenReturn(MRBuilderUtils.newJobReport(jobId, \"jobName-firstGen\", \"user\", JobState.RUNNING, 0, 0, 0, 0, 0, 0, 0, \"anything\", null, false, \"\"));\r\n    MRClientProtocol firstGenAMProxy = mock(MRClientProtocol.class);\r\n    when(firstGenAMProxy.getJobReport(any(GetJobReportRequest.class))).thenReturn(jobReportResponse1).thenThrow(new RuntimeException(\"AM is down!\"));\r\n    GetJobReportResponse jobReportResponse2 = mock(GetJobReportResponse.class);\r\n    when(jobReportResponse2.getJobReport()).thenReturn(MRBuilderUtils.newJobReport(jobId, \"jobName-secondGen\", \"user\", JobState.RUNNING, 0, 0, 0, 0, 0, 0, 0, \"anything\", null, false, \"\"));\r\n    MRClientProtocol secondGenAMProxy = mock(MRClientProtocol.class);\r\n    when(secondGenAMProxy.getJobReport(any(GetJobReportRequest.class))).thenReturn(jobReportResponse2);\r\n    ClientServiceDelegate clientServiceDelegate = spy(getClientServiceDelegate(historyServerProxy, rmDelegate));\r\n    doReturn(firstGenAMProxy).doReturn(secondGenAMProxy).when(clientServiceDelegate).instantiateAMProxy(any(InetSocketAddress.class));\r\n    JobStatus jobStatus = clientServiceDelegate.getJobStatus(oldJobId);\r\n    Assert.assertNotNull(jobStatus);\r\n    Assert.assertEquals(\"jobName-firstGen\", jobStatus.getJobName());\r\n    jobStatus = clientServiceDelegate.getJobStatus(oldJobId);\r\n    Assert.assertNotNull(jobStatus);\r\n    Assert.assertEquals(\"jobName-secondGen\", jobStatus.getJobName());\r\n    jobStatus = clientServiceDelegate.getJobStatus(oldJobId);\r\n    Assert.assertNotNull(jobStatus);\r\n    Assert.assertEquals(\"jobName-secondGen\", jobStatus.getJobName());\r\n    verify(clientServiceDelegate, times(2)).instantiateAMProxy(any(InetSocketAddress.class));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "testAMAccessDisabled",
  "errType" : [ "YarnException" ],
  "containingMethodsNum" : 24,
  "sourceCodeText" : "void testAMAccessDisabled() throws IOException\n{\r\n    if (isAMReachableFromClient) {\r\n        return;\r\n    }\r\n    MRClientProtocol historyServerProxy = mock(MRClientProtocol.class);\r\n    when(historyServerProxy.getJobReport(getJobReportRequest())).thenReturn(getJobReportResponseFromHistoryServer());\r\n    ResourceMgrDelegate rmDelegate = mock(ResourceMgrDelegate.class);\r\n    try {\r\n        when(rmDelegate.getApplicationReport(jobId.getAppId())).thenReturn(getRunningApplicationReport(\"am1\", 78)).thenReturn(getRunningApplicationReport(\"am1\", 78)).thenReturn(getRunningApplicationReport(\"am1\", 78)).thenReturn(getFinishedApplicationReport());\r\n    } catch (YarnException e) {\r\n        throw new IOException(e);\r\n    }\r\n    ClientServiceDelegate clientServiceDelegate = spy(getClientServiceDelegate(historyServerProxy, rmDelegate));\r\n    JobStatus jobStatus = clientServiceDelegate.getJobStatus(oldJobId);\r\n    Assert.assertNotNull(jobStatus);\r\n    Assert.assertEquals(\"N/A\", jobStatus.getJobName());\r\n    verify(clientServiceDelegate, times(0)).instantiateAMProxy(any(InetSocketAddress.class));\r\n    jobStatus = clientServiceDelegate.getJobStatus(oldJobId);\r\n    Assert.assertNotNull(jobStatus);\r\n    Assert.assertEquals(\"N/A\", jobStatus.getJobName());\r\n    verify(clientServiceDelegate, times(0)).instantiateAMProxy(any(InetSocketAddress.class));\r\n    jobStatus = clientServiceDelegate.getJobStatus(oldJobId);\r\n    Assert.assertNotNull(jobStatus);\r\n    Assert.assertEquals(\"N/A\", jobStatus.getJobName());\r\n    verify(clientServiceDelegate, times(0)).instantiateAMProxy(any(InetSocketAddress.class));\r\n    JobStatus jobStatus1 = clientServiceDelegate.getJobStatus(oldJobId);\r\n    Assert.assertNotNull(jobStatus1);\r\n    Assert.assertEquals(\"TestJobFilePath\", jobStatus1.getJobFile());\r\n    Assert.assertEquals(\"http://TestTrackingUrl\", jobStatus1.getTrackingUrl());\r\n    Assert.assertEquals(1.0f, jobStatus1.getMapProgress(), 0.0f);\r\n    Assert.assertEquals(1.0f, jobStatus1.getReduceProgress(), 0.0f);\r\n    verify(clientServiceDelegate, times(0)).instantiateAMProxy(any(InetSocketAddress.class));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "testRMDownForJobStatusBeforeGetAMReport",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testRMDownForJobStatusBeforeGetAMReport() throws IOException\n{\r\n    Configuration conf = new YarnConfiguration();\r\n    testRMDownForJobStatusBeforeGetAMReport(conf, MRJobConfig.DEFAULT_MR_CLIENT_MAX_RETRIES);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "testRMDownForJobStatusBeforeGetAMReportWithRetryTimes",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void testRMDownForJobStatusBeforeGetAMReportWithRetryTimes() throws IOException\n{\r\n    Configuration conf = new YarnConfiguration();\r\n    conf.setInt(MRJobConfig.MR_CLIENT_MAX_RETRIES, 2);\r\n    testRMDownForJobStatusBeforeGetAMReport(conf, conf.getInt(MRJobConfig.MR_CLIENT_MAX_RETRIES, MRJobConfig.DEFAULT_MR_CLIENT_MAX_RETRIES));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "testRMDownRestoreForJobStatusBeforeGetAMReport",
  "errType" : [ "YarnException" ],
  "containingMethodsNum" : 10,
  "sourceCodeText" : "void testRMDownRestoreForJobStatusBeforeGetAMReport() throws IOException\n{\r\n    Configuration conf = new YarnConfiguration();\r\n    conf.setInt(MRJobConfig.MR_CLIENT_MAX_RETRIES, 3);\r\n    conf.set(MRConfig.FRAMEWORK_NAME, MRConfig.YARN_FRAMEWORK_NAME);\r\n    conf.setBoolean(MRJobConfig.JOB_AM_ACCESS_DISABLED, !isAMReachableFromClient);\r\n    MRClientProtocol historyServerProxy = mock(MRClientProtocol.class);\r\n    when(historyServerProxy.getJobReport(any(GetJobReportRequest.class))).thenReturn(getJobReportResponse());\r\n    ResourceMgrDelegate rmDelegate = mock(ResourceMgrDelegate.class);\r\n    try {\r\n        when(rmDelegate.getApplicationReport(jobId.getAppId())).thenThrow(new java.lang.reflect.UndeclaredThrowableException(new IOException(\"Connection refuced1\"))).thenThrow(new java.lang.reflect.UndeclaredThrowableException(new IOException(\"Connection refuced2\"))).thenReturn(getFinishedApplicationReport());\r\n        ClientServiceDelegate clientServiceDelegate = new ClientServiceDelegate(conf, rmDelegate, oldJobId, historyServerProxy);\r\n        JobStatus jobStatus = clientServiceDelegate.getJobStatus(oldJobId);\r\n        verify(rmDelegate, times(3)).getApplicationReport(any(ApplicationId.class));\r\n        Assert.assertNotNull(jobStatus);\r\n    } catch (YarnException e) {\r\n        throw new IOException(e);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "testRMDownForJobStatusBeforeGetAMReport",
  "errType" : [ "YarnException", "IOException" ],
  "containingMethodsNum" : 9,
  "sourceCodeText" : "void testRMDownForJobStatusBeforeGetAMReport(Configuration conf, int noOfRetries) throws IOException\n{\r\n    conf.set(MRConfig.FRAMEWORK_NAME, MRConfig.YARN_FRAMEWORK_NAME);\r\n    conf.setBoolean(MRJobConfig.JOB_AM_ACCESS_DISABLED, !isAMReachableFromClient);\r\n    MRClientProtocol historyServerProxy = mock(MRClientProtocol.class);\r\n    ResourceMgrDelegate rmDelegate = mock(ResourceMgrDelegate.class);\r\n    try {\r\n        when(rmDelegate.getApplicationReport(jobId.getAppId())).thenThrow(new java.lang.reflect.UndeclaredThrowableException(new IOException(\"Connection refuced1\"))).thenThrow(new java.lang.reflect.UndeclaredThrowableException(new IOException(\"Connection refuced2\"))).thenThrow(new java.lang.reflect.UndeclaredThrowableException(new IOException(\"Connection refuced3\")));\r\n        ClientServiceDelegate clientServiceDelegate = new ClientServiceDelegate(conf, rmDelegate, oldJobId, historyServerProxy);\r\n        try {\r\n            clientServiceDelegate.getJobStatus(oldJobId);\r\n            Assert.fail(\"It should throw exception after retries\");\r\n        } catch (IOException e) {\r\n            System.out.println(\"fail to get job status,and e=\" + e.toString());\r\n        }\r\n        verify(rmDelegate, times(noOfRetries)).getApplicationReport(any(ApplicationId.class));\r\n    } catch (YarnException e) {\r\n        throw new IOException(e);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 2,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getJobReportRequest",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "GetJobReportRequest getJobReportRequest()\n{\r\n    GetJobReportRequest request = Records.newRecord(GetJobReportRequest.class);\r\n    request.setJobId(jobId);\r\n    return request;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getJobReportResponse",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "GetJobReportResponse getJobReportResponse()\n{\r\n    GetJobReportResponse jobReportResponse = Records.newRecord(GetJobReportResponse.class);\r\n    JobReport jobReport = Records.newRecord(JobReport.class);\r\n    jobReport.setJobId(jobId);\r\n    jobReport.setJobState(JobState.SUCCEEDED);\r\n    jobReportResponse.setJobReport(jobReport);\r\n    return jobReportResponse;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getCountersRequest",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "GetCountersRequest getCountersRequest()\n{\r\n    GetCountersRequest request = Records.newRecord(GetCountersRequest.class);\r\n    request.setJobId(jobId);\r\n    return request;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getFinishedApplicationReport",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "ApplicationReport getFinishedApplicationReport()\n{\r\n    ApplicationId appId = ApplicationId.newInstance(1234, 5);\r\n    ApplicationAttemptId attemptId = ApplicationAttemptId.newInstance(appId, 0);\r\n    return ApplicationReport.newInstance(appId, attemptId, \"user\", \"queue\", \"appname\", \"host\", 124, null, YarnApplicationState.FINISHED, \"diagnostics\", \"url\", 0, 0, 0, FinalApplicationStatus.SUCCEEDED, null, \"N/A\", 0.0f, YarnConfiguration.DEFAULT_APPLICATION_TYPE, null);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getRunningApplicationReport",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "ApplicationReport getRunningApplicationReport(String host, int port)\n{\r\n    ApplicationId appId = ApplicationId.newInstance(1234, 5);\r\n    ApplicationAttemptId attemptId = ApplicationAttemptId.newInstance(appId, 0);\r\n    return ApplicationReport.newInstance(appId, attemptId, \"user\", \"queue\", \"appname\", host, port, null, YarnApplicationState.RUNNING, \"diagnostics\", \"url\", 0, 0, 0, FinalApplicationStatus.UNDEFINED, null, \"N/A\", 0.0f, YarnConfiguration.DEFAULT_APPLICATION_TYPE, null);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getRMDelegate",
  "errType" : [ "YarnException" ],
  "containingMethodsNum" : 3,
  "sourceCodeText" : "ResourceMgrDelegate getRMDelegate() throws IOException\n{\r\n    ResourceMgrDelegate rm = mock(ResourceMgrDelegate.class);\r\n    try {\r\n        ApplicationId appId = jobId.getAppId();\r\n        when(rm.getApplicationReport(appId)).thenThrow(new ApplicationNotFoundException(appId + \" not found\"));\r\n    } catch (YarnException e) {\r\n        throw new IOException(e);\r\n    }\r\n    return rm;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getClientServiceDelegate",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "ClientServiceDelegate getClientServiceDelegate(MRClientProtocol historyServerProxy, ResourceMgrDelegate rm)\n{\r\n    Configuration conf = new YarnConfiguration();\r\n    conf.set(MRConfig.FRAMEWORK_NAME, MRConfig.YARN_FRAMEWORK_NAME);\r\n    conf.setBoolean(MRJobConfig.JOB_AM_ACCESS_DISABLED, !isAMReachableFromClient);\r\n    ClientServiceDelegate clientServiceDelegate = new ClientServiceDelegate(conf, rm, oldJobId, historyServerProxy);\r\n    return clientServiceDelegate;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getJobReportResponseFromHistoryServer",
  "errType" : null,
  "containingMethodsNum" : 9,
  "sourceCodeText" : "GetJobReportResponse getJobReportResponseFromHistoryServer()\n{\r\n    GetJobReportResponse jobReportResponse = Records.newRecord(GetJobReportResponse.class);\r\n    JobReport jobReport = Records.newRecord(JobReport.class);\r\n    jobReport.setJobId(jobId);\r\n    jobReport.setJobState(JobState.SUCCEEDED);\r\n    jobReport.setMapProgress(1.0f);\r\n    jobReport.setReduceProgress(1.0f);\r\n    jobReport.setJobFile(\"TestJobFilePath\");\r\n    jobReport.setTrackingUrl(\"http://TestTrackingUrl\");\r\n    jobReportResponse.setJobReport(jobReport);\r\n    return jobReportResponse;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getCountersResponseFromHistoryServer",
  "errType" : null,
  "containingMethodsNum" : 12,
  "sourceCodeText" : "GetCountersResponse getCountersResponseFromHistoryServer()\n{\r\n    GetCountersResponse countersResponse = Records.newRecord(GetCountersResponse.class);\r\n    Counter counter = Records.newRecord(Counter.class);\r\n    CounterGroup counterGroup = Records.newRecord(CounterGroup.class);\r\n    Counters counters = Records.newRecord(Counters.class);\r\n    counter.setDisplayName(\"dummyCounter\");\r\n    counter.setName(\"dummyCounter\");\r\n    counter.setValue(1001);\r\n    counterGroup.setName(\"dummyCounters\");\r\n    counterGroup.setDisplayName(\"dummyCounters\");\r\n    counterGroup.setCounter(\"dummyCounter\", counter);\r\n    counters.setCounterGroup(\"dummyCounters\", counterGroup);\r\n    countersResponse.setCounters(counters);\r\n    return countersResponse;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\input",
  "methodName" : "testFormat",
  "errType" : null,
  "containingMethodsNum" : 30,
  "sourceCodeText" : "void testFormat() throws IOException, InterruptedException\n{\r\n    Job job = Job.getInstance(conf);\r\n    Random random = new Random();\r\n    long seed = random.nextLong();\r\n    random.setSeed(seed);\r\n    localFs.delete(workDir, true);\r\n    FileInputFormat.setInputPaths(job, workDir);\r\n    final int length = 10000;\r\n    final int numFiles = 10;\r\n    createFiles(length, numFiles, random, job);\r\n    TaskAttemptContext context = MapReduceTestUtil.createDummyMapTaskAttemptContext(job.getConfiguration());\r\n    InputFormat<IntWritable, BytesWritable> format = new CombineSequenceFileInputFormat<IntWritable, BytesWritable>();\r\n    for (int i = 0; i < 3; i++) {\r\n        int numSplits = random.nextInt(length / (SequenceFile.SYNC_INTERVAL / 20)) + 1;\r\n        LOG.info(\"splitting: requesting = \" + numSplits);\r\n        List<InputSplit> splits = format.getSplits(job);\r\n        LOG.info(\"splitting: got =        \" + splits.size());\r\n        assertEquals(\"We got more than one splits!\", 1, splits.size());\r\n        InputSplit split = splits.get(0);\r\n        assertEquals(\"It should be CombineFileSplit\", CombineFileSplit.class, split.getClass());\r\n        BitSet bits = new BitSet(length);\r\n        RecordReader<IntWritable, BytesWritable> reader = format.createRecordReader(split, context);\r\n        MapContext<IntWritable, BytesWritable, IntWritable, BytesWritable> mcontext = new MapContextImpl<IntWritable, BytesWritable, IntWritable, BytesWritable>(job.getConfiguration(), context.getTaskAttemptID(), reader, null, null, MapReduceTestUtil.createDummyReporter(), split);\r\n        reader.initialize(split, mcontext);\r\n        assertEquals(\"reader class is CombineFileRecordReader.\", CombineFileRecordReader.class, reader.getClass());\r\n        try {\r\n            while (reader.nextKeyValue()) {\r\n                IntWritable key = reader.getCurrentKey();\r\n                BytesWritable value = reader.getCurrentValue();\r\n                assertNotNull(\"Value should not be null.\", value);\r\n                final int k = key.get();\r\n                LOG.debug(\"read \" + k);\r\n                assertFalse(\"Key in multiple partitions.\", bits.get(k));\r\n                bits.set(k);\r\n            }\r\n        } finally {\r\n            reader.close();\r\n        }\r\n        assertEquals(\"Some keys in no partition.\", length, bits.cardinality());\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\input",
  "methodName" : "createRanges",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Range[] createRanges(int length, int numFiles, Random random)\n{\r\n    Range[] ranges = new Range[numFiles];\r\n    for (int i = 0; i < numFiles; i++) {\r\n        int start = i == 0 ? 0 : ranges[i - 1].end;\r\n        int end = i == numFiles - 1 ? length : (length / numFiles) * (2 * i + 1) / 2 + random.nextInt(length / numFiles) + 1;\r\n        ranges[i] = new Range(start, end);\r\n    }\r\n    return ranges;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\input",
  "methodName" : "createFiles",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void createFiles(int length, int numFiles, Random random, Job job) throws IOException\n{\r\n    Range[] ranges = createRanges(length, numFiles, random);\r\n    for (int i = 0; i < numFiles; i++) {\r\n        Path file = new Path(workDir, \"test_\" + i + \".seq\");\r\n        @SuppressWarnings(\"deprecation\")\r\n        SequenceFile.Writer writer = SequenceFile.createWriter(localFs, job.getConfiguration(), file, IntWritable.class, BytesWritable.class);\r\n        Range range = ranges[i];\r\n        try {\r\n            for (int j = range.start; j < range.end; j++) {\r\n                IntWritable key = new IntWritable(j);\r\n                byte[] data = new byte[random.nextInt(10)];\r\n                random.nextBytes(data);\r\n                BytesWritable value = new BytesWritable(data);\r\n                writer.append(key, value);\r\n            }\r\n        } finally {\r\n            writer.close();\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "cleanAndCreateInput",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void cleanAndCreateInput(FileSystem fs) throws IOException\n{\r\n    fs.delete(INPUT_DIR, true);\r\n    fs.delete(OUTPUT_DIR, true);\r\n    OutputStream os = fs.create(INPUT_FILE);\r\n    Writer wr = new OutputStreamWriter(os);\r\n    wr.write(\"hello1\\n\");\r\n    wr.write(\"hello2\\n\");\r\n    wr.write(\"hello3\\n\");\r\n    wr.write(\"hello4\\n\");\r\n    wr.close();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "testMapReduceJob",
  "errType" : null,
  "containingMethodsNum" : 22,
  "sourceCodeText" : "void testMapReduceJob() throws Exception\n{\r\n    JobConf conf = new JobConf(TestUserDefinedCounters.class);\r\n    conf.setJobName(\"UserDefinedCounters\");\r\n    FileSystem fs = FileSystem.get(conf);\r\n    cleanAndCreateInput(fs);\r\n    conf.setInputFormat(TextInputFormat.class);\r\n    conf.setMapOutputKeyClass(LongWritable.class);\r\n    conf.setMapOutputValueClass(Text.class);\r\n    conf.setOutputFormat(TextOutputFormat.class);\r\n    conf.setOutputKeyClass(LongWritable.class);\r\n    conf.setOutputValueClass(Text.class);\r\n    conf.setMapperClass(CountingMapper.class);\r\n    conf.setReducerClass(IdentityReducer.class);\r\n    FileInputFormat.setInputPaths(conf, INPUT_DIR);\r\n    FileOutputFormat.setOutputPath(conf, OUTPUT_DIR);\r\n    RunningJob runningJob = JobClient.runJob(conf);\r\n    Path[] outputFiles = FileUtil.stat2Paths(fs.listStatus(OUTPUT_DIR, new Utils.OutputFileUtils.OutputFilesFilter()));\r\n    if (outputFiles.length > 0) {\r\n        InputStream is = fs.open(outputFiles[0]);\r\n        BufferedReader reader = new BufferedReader(new InputStreamReader(is));\r\n        String line = reader.readLine();\r\n        int counter = 0;\r\n        while (line != null) {\r\n            counter++;\r\n            assertTrue(line.contains(\"hello\"));\r\n            line = reader.readLine();\r\n        }\r\n        reader.close();\r\n        assertEquals(4, counter);\r\n    }\r\n    verifyCounters(runningJob, 4);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "verifyCounters",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void verifyCounters(RunningJob runningJob, int expected) throws IOException\n{\r\n    assertEquals(expected, runningJob.getCounters().getCounter(EnumCounter.MAP_RECORDS));\r\n    assertEquals(expected, runningJob.getCounters().getGroup(\"StringCounter\").getCounter(\"MapRecords\"));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "testWithDFS",
  "errType" : null,
  "containingMethodsNum" : 11,
  "sourceCodeText" : "void testWithDFS() throws IOException\n{\r\n    MiniMRCluster mr = null;\r\n    MiniDFSCluster dfs = null;\r\n    FileSystem fileSys = null;\r\n    try {\r\n        JobConf conf = new JobConf();\r\n        dfs = new MiniDFSCluster.Builder(conf).build();\r\n        fileSys = dfs.getFileSystem();\r\n        mr = new MiniMRCluster(2, fileSys.getUri().toString(), 4);\r\n        MRCaching.setupCache(\"/cachedir\", fileSys);\r\n        TestResult ret = MRCaching.launchMRCache(\"/testing/wc/input\", \"/testing/wc/output\", \"/cachedir\", mr.createJobConf(), \"The quick brown fox\\nhas many silly\\n\" + \"red fox sox\\n\");\r\n        assertTrue(\"Archives not matching\", ret.isOutputOk);\r\n        ret = MRCaching.launchMRCache(\"/testing/wc/input\", \"/testing/wc/output\", \"/cachedir\", mr.createJobConf(), \"The quick brown fox\\nhas many silly\\n\" + \"red fox sox\\n\");\r\n        assertTrue(\"Archives not matching\", ret.isOutputOk);\r\n    } finally {\r\n        if (fileSys != null) {\r\n            fileSys.close();\r\n        }\r\n        if (dfs != null) {\r\n            dfs.shutdown();\r\n        }\r\n        if (mr != null) {\r\n            mr.shutdown();\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2",
  "methodName" : "getTestParameters",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "Collection<Object[]> getTestParameters()\n{\r\n    List<String> ignoredTests = Arrays.asList(new String[] { \"stalled_run\", \"slowing_run\", \"step_stalled_run\" });\r\n    return Arrays.asList(new Object[][] { { SimpleExponentialTaskRuntimeEstimator.class, ignoredTests, NUM_MAP_DEFAULT, NUM_REDUCE_DEFAULT }, { LegacyTaskRuntimeEstimator.class, ignoredTests, NUM_MAP_DEFAULT, NUM_REDUCE_DEFAULT } });\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2",
  "methodName" : "setup",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void setup() throws IOException\n{\r\n    if (!(new File(MiniMRYarnCluster.APPJAR)).exists()) {\r\n        LOG.info(\"MRAppJar \" + MiniMRYarnCluster.APPJAR + \" not found. Not running test.\");\r\n        return;\r\n    }\r\n    if (mrCluster == null) {\r\n        mrCluster = new MiniMRYarnCluster(TestSpeculativeExecution.class.getName(), NODE_MANAGERS_COUNT);\r\n        Configuration conf = new Configuration();\r\n        mrCluster.init(conf);\r\n        mrCluster.start();\r\n    }\r\n    localFs.copyFromLocalFile(new Path(MiniMRYarnCluster.APPJAR), APP_JAR);\r\n    localFs.setPermission(APP_JAR, new FsPermission(\"700\"));\r\n    myMapSleepTime = MAP_SLEEP_TIME_DEFAULT;\r\n    myReduceSleepTime = REDUCE_SLEEP_TIME_DEFAULT;\r\n    myMapSleepCount = MAP_SLEEP_COUNT_DEFAULT;\r\n    myReduceSleepCount = REDUCE_SLEEP_COUNT_DEFAULT;\r\n    chosenSleepCalc = MAP_SLEEP_CALCULATOR_TYPE_DEFAULT;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2",
  "methodName" : "tearDown",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void tearDown()\n{\r\n    if (mrCluster != null) {\r\n        mrCluster.stop();\r\n        mrCluster = null;\r\n    }\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2",
  "methodName" : "testExecDynamicSlowingSpeculative",
  "errType" : null,
  "containingMethodsNum" : 12,
  "sourceCodeText" : "void testExecDynamicSlowingSpeculative() throws Exception\n{\r\n    chosenSleepCalc = \"dynamic_slowing_run\";\r\n    if (ignoredTests.contains(chosenSleepCalc)) {\r\n        return;\r\n    }\r\n    EstimatorMetricsPair[] estimatorPairs = new EstimatorMetricsPair[] { new EstimatorMetricsPair(SimpleExponentialTaskRuntimeEstimator.class, myNumMapper, myNumReduce, true), new EstimatorMetricsPair(LegacyTaskRuntimeEstimator.class, myNumMapper, myNumReduce, true), new EstimatorMetricsPair(ExponentiallySmoothedTaskRuntimeEstimator.class, myNumMapper, myNumReduce, true) };\r\n    for (EstimatorMetricsPair specEstimator : estimatorPairs) {\r\n        if (!estimatorClass.equals(specEstimator.estimatorClass)) {\r\n            continue;\r\n        }\r\n        LOG.info(\"+++ Dynamic Slow Progress testing against \" + estimatorClass.getName() + \" +++\");\r\n        Job job = runSpecTest();\r\n        boolean succeeded = job.waitForCompletion(true);\r\n        Assert.assertTrue(\"Job expected to succeed with estimator \" + estimatorClass.getName(), succeeded);\r\n        Assert.assertEquals(\"Job expected to succeed with estimator \" + estimatorClass.getName(), JobStatus.State.SUCCEEDED, job.getJobState());\r\n        Counters counters = job.getCounters();\r\n        String errorMessage = specEstimator.getErrorMessage(counters);\r\n        boolean didSpeculate = specEstimator.didSpeculate(counters);\r\n        Assert.assertEquals(errorMessage, didSpeculate, specEstimator.speculativeEstimator);\r\n        Assert.assertEquals(\"Failed maps higher than 0 \" + estimatorClass.getName(), 0, counters.findCounter(JobCounter.NUM_FAILED_MAPS).getValue());\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2",
  "methodName" : "testExecSlowNonSpeculative",
  "errType" : null,
  "containingMethodsNum" : 12,
  "sourceCodeText" : "void testExecSlowNonSpeculative() throws Exception\n{\r\n    chosenSleepCalc = \"slowing_run\";\r\n    if (ignoredTests.contains(chosenSleepCalc)) {\r\n        return;\r\n    }\r\n    EstimatorMetricsPair[] estimatorPairs = new EstimatorMetricsPair[] { new EstimatorMetricsPair(SimpleExponentialTaskRuntimeEstimator.class, myNumMapper, myNumReduce, false), new EstimatorMetricsPair(LegacyTaskRuntimeEstimator.class, myNumMapper, myNumReduce, true), new EstimatorMetricsPair(ExponentiallySmoothedTaskRuntimeEstimator.class, myNumMapper, myNumReduce, true) };\r\n    for (EstimatorMetricsPair specEstimator : estimatorPairs) {\r\n        if (!estimatorClass.equals(specEstimator.estimatorClass)) {\r\n            continue;\r\n        }\r\n        LOG.info(\"+++ Linear Slow Progress Non Speculative testing against \" + estimatorClass.getName() + \" +++\");\r\n        Job job = runSpecTest();\r\n        boolean succeeded = job.waitForCompletion(true);\r\n        Assert.assertTrue(\"Job expected to succeed with estimator \" + estimatorClass.getName(), succeeded);\r\n        Assert.assertEquals(\"Job expected to succeed with estimator \" + estimatorClass.getName(), JobStatus.State.SUCCEEDED, job.getJobState());\r\n        Counters counters = job.getCounters();\r\n        String errorMessage = specEstimator.getErrorMessage(counters);\r\n        boolean didSpeculate = specEstimator.didSpeculate(counters);\r\n        Assert.assertEquals(errorMessage, didSpeculate, specEstimator.speculativeEstimator);\r\n        Assert.assertEquals(\"Failed maps higher than 0 \" + estimatorClass.getName(), 0, counters.findCounter(JobCounter.NUM_FAILED_MAPS).getValue());\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2",
  "methodName" : "testExecStepStalledSpeculative",
  "errType" : null,
  "containingMethodsNum" : 12,
  "sourceCodeText" : "void testExecStepStalledSpeculative() throws Exception\n{\r\n    chosenSleepCalc = \"step_stalled_run\";\r\n    if (ignoredTests.contains(chosenSleepCalc)) {\r\n        return;\r\n    }\r\n    EstimatorMetricsPair[] estimatorPairs = new EstimatorMetricsPair[] { new EstimatorMetricsPair(SimpleExponentialTaskRuntimeEstimator.class, myNumMapper, myNumReduce, true), new EstimatorMetricsPair(LegacyTaskRuntimeEstimator.class, myNumMapper, myNumReduce, true), new EstimatorMetricsPair(ExponentiallySmoothedTaskRuntimeEstimator.class, myNumMapper, myNumReduce, true) };\r\n    for (EstimatorMetricsPair specEstimator : estimatorPairs) {\r\n        if (!estimatorClass.equals(specEstimator.estimatorClass)) {\r\n            continue;\r\n        }\r\n        LOG.info(\"+++ Stalled Progress testing against \" + estimatorClass.getName() + \" +++\");\r\n        Job job = runSpecTest();\r\n        boolean succeeded = job.waitForCompletion(true);\r\n        Assert.assertTrue(\"Job expected to succeed with estimator \" + estimatorClass.getName(), succeeded);\r\n        Assert.assertEquals(\"Job expected to succeed with estimator \" + estimatorClass.getName(), JobStatus.State.SUCCEEDED, job.getJobState());\r\n        Counters counters = job.getCounters();\r\n        String errorMessage = specEstimator.getErrorMessage(counters);\r\n        boolean didSpeculate = specEstimator.didSpeculate(counters);\r\n        Assert.assertEquals(errorMessage, didSpeculate, specEstimator.speculativeEstimator);\r\n        Assert.assertEquals(\"Failed maps higher than 0 \" + estimatorClass.getName(), 0, counters.findCounter(JobCounter.NUM_FAILED_MAPS).getValue());\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2",
  "methodName" : "testExecStalledSpeculative",
  "errType" : null,
  "containingMethodsNum" : 12,
  "sourceCodeText" : "void testExecStalledSpeculative() throws Exception\n{\r\n    chosenSleepCalc = \"stalled_run\";\r\n    if (ignoredTests.contains(chosenSleepCalc)) {\r\n        return;\r\n    }\r\n    EstimatorMetricsPair[] estimatorPairs = new EstimatorMetricsPair[] { new EstimatorMetricsPair(SimpleExponentialTaskRuntimeEstimator.class, myNumMapper, myNumReduce, true), new EstimatorMetricsPair(LegacyTaskRuntimeEstimator.class, myNumMapper, myNumReduce, true), new EstimatorMetricsPair(ExponentiallySmoothedTaskRuntimeEstimator.class, myNumMapper, myNumReduce, true) };\r\n    for (EstimatorMetricsPair specEstimator : estimatorPairs) {\r\n        if (!estimatorClass.equals(specEstimator.estimatorClass)) {\r\n            continue;\r\n        }\r\n        LOG.info(\"+++ Stalled Progress testing against \" + estimatorClass.getName() + \" +++\");\r\n        Job job = runSpecTest();\r\n        boolean succeeded = job.waitForCompletion(true);\r\n        Assert.assertTrue(\"Job expected to succeed with estimator \" + estimatorClass.getName(), succeeded);\r\n        Assert.assertEquals(\"Job expected to succeed with estimator \" + estimatorClass.getName(), JobStatus.State.SUCCEEDED, job.getJobState());\r\n        Counters counters = job.getCounters();\r\n        String errorMessage = specEstimator.getErrorMessage(counters);\r\n        boolean didSpeculate = specEstimator.didSpeculate(counters);\r\n        Assert.assertEquals(errorMessage, didSpeculate, specEstimator.speculativeEstimator);\r\n        Assert.assertEquals(\"Failed maps higher than 0 \" + estimatorClass.getName(), 0, counters.findCounter(JobCounter.NUM_FAILED_MAPS).getValue());\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2",
  "methodName" : "testExecNonSpeculative",
  "errType" : null,
  "containingMethodsNum" : 13,
  "sourceCodeText" : "void testExecNonSpeculative() throws Exception\n{\r\n    if (!(new File(MiniMRYarnCluster.APPJAR)).exists()) {\r\n        LOG.info(\"MRAppJar \" + MiniMRYarnCluster.APPJAR + \" not found. Not running test.\");\r\n        return;\r\n    }\r\n    if (ignoredTests.contains(chosenSleepCalc)) {\r\n        return;\r\n    }\r\n    EstimatorMetricsPair[] estimatorPairs = new EstimatorMetricsPair[] { new EstimatorMetricsPair(LegacyTaskRuntimeEstimator.class, myNumMapper, myNumReduce, true), new EstimatorMetricsPair(SimpleExponentialTaskRuntimeEstimator.class, myNumMapper, myNumReduce, false), new EstimatorMetricsPair(ExponentiallySmoothedTaskRuntimeEstimator.class, myNumMapper, myNumReduce, true) };\r\n    for (EstimatorMetricsPair specEstimator : estimatorPairs) {\r\n        if (!estimatorClass.equals(specEstimator.estimatorClass)) {\r\n            continue;\r\n        }\r\n        LOG.info(\"+++ No Speculation testing against \" + estimatorClass.getName() + \" +++\");\r\n        Job job = runSpecTest();\r\n        boolean succeeded = job.waitForCompletion(true);\r\n        Assert.assertTrue(\"Job expected to succeed with estimator \" + estimatorClass.getName(), succeeded);\r\n        Assert.assertEquals(\"Job expected to succeed with estimator \" + estimatorClass.getName(), JobStatus.State.SUCCEEDED, job.getJobState());\r\n        Counters counters = job.getCounters();\r\n        String errorMessage = specEstimator.getErrorMessage(counters);\r\n        boolean didSpeculate = specEstimator.didSpeculate(counters);\r\n        Assert.assertEquals(errorMessage, didSpeculate, specEstimator.speculativeEstimator);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2",
  "methodName" : "runSpecTest",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 27,
  "sourceCodeText" : "Job runSpecTest() throws IOException, ClassNotFoundException, InterruptedException\n{\r\n    Configuration conf = mrCluster.getConfig();\r\n    conf.setBoolean(MRJobConfig.MAP_SPECULATIVE, ENABLE_SPECULATIVE_MAP);\r\n    conf.setBoolean(MRJobConfig.REDUCE_SPECULATIVE, ENABLE_SPECULATIVE_REDUCE);\r\n    conf.setClass(MRJobConfig.MR_AM_TASK_ESTIMATOR, estimatorClass, TaskRuntimeEstimator.class);\r\n    conf.setLong(MAP_SLEEP_TIME, myMapSleepTime);\r\n    conf.setLong(REDUCE_SLEEP_TIME, myReduceSleepTime);\r\n    conf.setInt(MAP_SLEEP_COUNT, myMapSleepCount);\r\n    conf.setInt(REDUCE_SLEEP_COUNT, myReduceSleepCount);\r\n    conf.setFloat(MRJobConfig.COMPLETED_MAPS_FOR_REDUCE_SLOWSTART, 1.0F);\r\n    conf.setInt(MRJobConfig.NUM_MAPS, myNumMapper);\r\n    conf.set(MAP_SLEEP_CALCULATOR_TYPE, chosenSleepCalc);\r\n    Job job = Job.getInstance(conf);\r\n    job.setJarByClass(TestSpeculativeExecution.class);\r\n    job.setMapperClass(SpeculativeSleepMapper.class);\r\n    job.setMapOutputKeyClass(IntWritable.class);\r\n    job.setMapOutputValueClass(NullWritable.class);\r\n    job.setReducerClass(SpeculativeSleepReducer.class);\r\n    job.setOutputFormatClass(NullOutputFormat.class);\r\n    job.setInputFormatClass(SpeculativeSleepInputFormat.class);\r\n    job.setPartitionerClass(SpeculativeSleepJobPartitioner.class);\r\n    job.setNumReduceTasks(myNumReduce);\r\n    FileInputFormat.addInputPath(job, new Path(\"ignored\"));\r\n    try {\r\n        localFs.delete(TEST_OUT_DIR, true);\r\n    } catch (IOException e) {\r\n    }\r\n    FileOutputFormat.setOutputPath(job, TEST_OUT_DIR);\r\n    job.addFileToClassPath(APP_JAR);\r\n    job.setMaxMapAttempts(2);\r\n    job.submit();\r\n    return job;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2",
  "methodName" : "setup",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void setup() throws IOException\n{\r\n    TestMRJobs.setup();\r\n    if (mrCluster != null) {\r\n        mrCluster.getConfig().setBoolean(MRJobConfig.JOB_UBERTASK_ENABLE, true);\r\n        mrCluster.getConfig().setInt(MRJobConfig.JOB_UBERTASK_MAXREDUCES, 3);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2",
  "methodName" : "testSleepJob",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testSleepJob() throws Exception\n{\r\n    numSleepReducers = 1;\r\n    super.testSleepJob();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2",
  "methodName" : "testSleepJobWithMultipleReducers",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testSleepJobWithMultipleReducers() throws Exception\n{\r\n    numSleepReducers = 3;\r\n    super.testSleepJob();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2",
  "methodName" : "verifySleepJobCounters",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void verifySleepJobCounters(Job job) throws InterruptedException, IOException\n{\r\n    Counters counters = job.getCounters();\r\n    super.verifySleepJobCounters(job);\r\n    Assert.assertEquals(3, counters.findCounter(JobCounter.NUM_UBER_SUBMAPS).getValue());\r\n    Assert.assertEquals(numSleepReducers, counters.findCounter(JobCounter.NUM_UBER_SUBREDUCES).getValue());\r\n    Assert.assertEquals(3 + numSleepReducers, counters.findCounter(JobCounter.TOTAL_LAUNCHED_UBERTASKS).getValue());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2",
  "methodName" : "testRandomWriter",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testRandomWriter() throws IOException, InterruptedException, ClassNotFoundException\n{\r\n    super.testRandomWriter();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2",
  "methodName" : "verifyRandomWriterCounters",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void verifyRandomWriterCounters(Job job) throws InterruptedException, IOException\n{\r\n    super.verifyRandomWriterCounters(job);\r\n    Counters counters = job.getCounters();\r\n    Assert.assertEquals(3, counters.findCounter(JobCounter.NUM_UBER_SUBMAPS).getValue());\r\n    Assert.assertEquals(3, counters.findCounter(JobCounter.TOTAL_LAUNCHED_UBERTASKS).getValue());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2",
  "methodName" : "testFailingMapper",
  "errType" : [ "Exception" ],
  "containingMethodsNum" : 17,
  "sourceCodeText" : "void testFailingMapper() throws IOException, InterruptedException, ClassNotFoundException\n{\r\n    LOG.info(\"\\n\\n\\nStarting uberized testFailingMapper().\");\r\n    if (!(new File(MiniMRYarnCluster.APPJAR)).exists()) {\r\n        LOG.info(\"MRAppJar \" + MiniMRYarnCluster.APPJAR + \" not found. Not running test.\");\r\n        return;\r\n    }\r\n    Job job = runFailingMapperJob();\r\n    TaskID taskID = new TaskID(job.getJobID(), TaskType.MAP, 0);\r\n    TaskAttemptID aId = new TaskAttemptID(taskID, 0);\r\n    System.out.println(\"Diagnostics for \" + aId + \" :\");\r\n    for (String diag : job.getTaskDiagnostics(aId)) {\r\n        System.out.println(diag);\r\n    }\r\n    boolean secondTaskAttemptExists = true;\r\n    try {\r\n        aId = new TaskAttemptID(taskID, 1);\r\n        System.out.println(\"Diagnostics for \" + aId + \" :\");\r\n        for (String diag : job.getTaskDiagnostics(aId)) {\r\n            System.out.println(diag);\r\n        }\r\n    } catch (Exception e) {\r\n        secondTaskAttemptExists = false;\r\n    }\r\n    assertThat(secondTaskAttemptExists).isFalse();\r\n    TaskCompletionEvent[] events = job.getTaskCompletionEvents(0, 2);\r\n    Assert.assertEquals(1, events.length);\r\n    TaskCompletionEvent.Status status = events[0].getStatus();\r\n    Assert.assertTrue(status == TaskCompletionEvent.Status.FAILED || status == TaskCompletionEvent.Status.TIPFAILED);\r\n    Assert.assertEquals(JobStatus.State.FAILED, job.getJobState());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2",
  "methodName" : "verifyFailingMapperCounters",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void verifyFailingMapperCounters(Job job) throws InterruptedException, IOException\n{\r\n    Counters counters = job.getCounters();\r\n    super.verifyFailingMapperCounters(job);\r\n    Assert.assertEquals(2, counters.findCounter(JobCounter.TOTAL_LAUNCHED_UBERTASKS).getValue());\r\n    Assert.assertEquals(2, counters.findCounter(JobCounter.NUM_UBER_SUBMAPS).getValue());\r\n    Assert.assertEquals(2, counters.findCounter(JobCounter.NUM_FAILED_UBERTASKS).getValue());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2",
  "methodName" : "testSleepJobWithSecurityOn",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testSleepJobWithSecurityOn() throws IOException, InterruptedException, ClassNotFoundException\n{\r\n    super.testSleepJobWithSecurityOn();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\partition",
  "methodName" : "testSplitSampler",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void testSplitSampler() throws Exception\n{\r\n    final int TOT_SPLITS = 15;\r\n    final int NUM_SPLITS = 5;\r\n    final int STEP_SAMPLE = 5;\r\n    final int NUM_SAMPLES = NUM_SPLITS * STEP_SAMPLE;\r\n    InputSampler.Sampler<IntWritable, NullWritable> sampler = new InputSampler.SplitSampler<IntWritable, NullWritable>(NUM_SAMPLES, NUM_SPLITS);\r\n    int[] inits = new int[TOT_SPLITS];\r\n    for (int i = 0; i < TOT_SPLITS; ++i) {\r\n        inits[i] = i * STEP_SAMPLE;\r\n    }\r\n    Job ignored = Job.getInstance();\r\n    Object[] samples = sampler.getSample(new TestInputSamplerIF(100000, TOT_SPLITS, inits), ignored);\r\n    assertEquals(NUM_SAMPLES, samples.length);\r\n    Arrays.sort(samples, new IntWritable.Comparator());\r\n    for (int i = 0; i < NUM_SAMPLES; ++i) {\r\n        assertEquals(i, ((IntWritable) samples[i]).get());\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\partition",
  "methodName" : "testMapredSplitSampler",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void testMapredSplitSampler() throws Exception\n{\r\n    final int TOT_SPLITS = 15;\r\n    final int NUM_SPLITS = 5;\r\n    final int STEP_SAMPLE = 5;\r\n    final int NUM_SAMPLES = NUM_SPLITS * STEP_SAMPLE;\r\n    org.apache.hadoop.mapred.lib.InputSampler.Sampler<IntWritable, NullWritable> sampler = new org.apache.hadoop.mapred.lib.InputSampler.SplitSampler<IntWritable, NullWritable>(NUM_SAMPLES, NUM_SPLITS);\r\n    int[] inits = new int[TOT_SPLITS];\r\n    for (int i = 0; i < TOT_SPLITS; ++i) {\r\n        inits[i] = i * STEP_SAMPLE;\r\n    }\r\n    Object[] samples = sampler.getSample(new TestMapredInputSamplerIF(100000, TOT_SPLITS, inits), new JobConf());\r\n    assertEquals(NUM_SAMPLES, samples.length);\r\n    Arrays.sort(samples, new IntWritable.Comparator());\r\n    for (int i = 0; i < NUM_SAMPLES; ++i) {\r\n        assertEquals(i % STEP_SAMPLE + TOT_SPLITS * (i / STEP_SAMPLE), ((IntWritable) samples[i]).get());\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\partition",
  "methodName" : "testIntervalSampler",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void testIntervalSampler() throws Exception\n{\r\n    final int TOT_SPLITS = 16;\r\n    final int PER_SPLIT_SAMPLE = 4;\r\n    final int NUM_SAMPLES = TOT_SPLITS * PER_SPLIT_SAMPLE;\r\n    final double FREQ = 1.0 / TOT_SPLITS;\r\n    InputSampler.Sampler<IntWritable, NullWritable> sampler = new InputSampler.IntervalSampler<IntWritable, NullWritable>(FREQ, NUM_SAMPLES);\r\n    int[] inits = new int[TOT_SPLITS];\r\n    for (int i = 0; i < TOT_SPLITS; ++i) {\r\n        inits[i] = i;\r\n    }\r\n    Job ignored = Job.getInstance();\r\n    Object[] samples = sampler.getSample(new TestInputSamplerIF(NUM_SAMPLES, TOT_SPLITS, inits), ignored);\r\n    assertEquals(NUM_SAMPLES, samples.length);\r\n    Arrays.sort(samples, new IntWritable.Comparator());\r\n    for (int i = 0; i < NUM_SAMPLES; ++i) {\r\n        assertEquals(i, ((IntWritable) samples[i]).get());\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\partition",
  "methodName" : "testMapredIntervalSampler",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void testMapredIntervalSampler() throws Exception\n{\r\n    final int TOT_SPLITS = 16;\r\n    final int PER_SPLIT_SAMPLE = 4;\r\n    final int NUM_SAMPLES = TOT_SPLITS * PER_SPLIT_SAMPLE;\r\n    final double FREQ = 1.0 / TOT_SPLITS;\r\n    org.apache.hadoop.mapred.lib.InputSampler.Sampler<IntWritable, NullWritable> sampler = new org.apache.hadoop.mapred.lib.InputSampler.IntervalSampler<IntWritable, NullWritable>(FREQ, NUM_SAMPLES);\r\n    int[] inits = new int[TOT_SPLITS];\r\n    for (int i = 0; i < TOT_SPLITS; ++i) {\r\n        inits[i] = i;\r\n    }\r\n    Job ignored = Job.getInstance();\r\n    Object[] samples = sampler.getSample(new TestInputSamplerIF(NUM_SAMPLES, TOT_SPLITS, inits), ignored);\r\n    assertEquals(NUM_SAMPLES, samples.length);\r\n    Arrays.sort(samples, new IntWritable.Comparator());\r\n    for (int i = 0; i < NUM_SAMPLES; ++i) {\r\n        assertEquals(i, ((IntWritable) samples[i]).get());\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "printUsage",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "int printUsage()\n{\r\n    System.err.println(\"Usage: [-m <maps>] number of mappers (default: \" + NUM_MAPS_DEFAULT + \")\\n\" + \"     [-v] timeline service version (default: \" + TIMELINE_SERVICE_VERSION_1 + \")\\n\" + \"          1. version 1.x\\n\" + \"          2. version 2.x\\n\" + \"     [-mtype <mapper type in integer>] (default: \" + SIMPLE_ENTITY_WRITER + \")\\n\" + \"          1. simple entity write mapper\\n\" + \"          2. jobhistory files replay mapper\\n\" + \"     [-s <(KBs)test>] number of KB per put (mtype=1, default: \" + SimpleEntityWriterConstants.KBS_SENT_DEFAULT + \" KB)\\n\" + \"     [-t] package sending iterations per mapper (mtype=1, default: \" + SimpleEntityWriterConstants.TEST_TIMES_DEFAULT + \")\\n\" + \"     [-d <path>] hdfs root path of job history files (mtype=2)\\n\" + \"     [-r <replay mode>] (mtype=2)\\n\" + \"          1. write all entities for a job in one put (default)\\n\" + \"          2. write one entity at a time\\n\");\r\n    GenericOptionsParser.printGenericCommandUsage(System.err);\r\n    return -1;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "parseArgs",
  "errType" : [ "NumberFormatException", "Exception" ],
  "containingMethodsNum" : 35,
  "sourceCodeText" : "boolean parseArgs(String[] args, Job job) throws IOException\n{\r\n    Configuration conf = job.getConfiguration();\r\n    conf.setInt(MRJobConfig.NUM_MAPS, NUM_MAPS_DEFAULT);\r\n    for (int i = 0; i < args.length; i++) {\r\n        if (args.length == i + 1) {\r\n            System.out.println(\"ERROR: Required parameter missing from \" + args[i]);\r\n            return printUsage() == 0;\r\n        }\r\n        try {\r\n            if (\"-v\".equals(args[i])) {\r\n                timeline_service_version = Integer.parseInt(args[++i]);\r\n            } else if (\"-m\".equals(args[i])) {\r\n                if (Integer.parseInt(args[++i]) > 0) {\r\n                    job.getConfiguration().setInt(MRJobConfig.NUM_MAPS, Integer.parseInt(args[i]));\r\n                }\r\n            } else if (\"-mtype\".equals(args[i])) {\r\n                mapperType = Integer.parseInt(args[++i]);\r\n            } else if (\"-s\".equals(args[i])) {\r\n                if (Integer.parseInt(args[++i]) > 0) {\r\n                    conf.setInt(SimpleEntityWriterConstants.KBS_SENT, Integer.parseInt(args[i]));\r\n                }\r\n            } else if (\"-t\".equals(args[i])) {\r\n                if (Integer.parseInt(args[++i]) > 0) {\r\n                    conf.setInt(SimpleEntityWriterConstants.TEST_TIMES, Integer.parseInt(args[i]));\r\n                }\r\n            } else if (\"-d\".equals(args[i])) {\r\n                conf.set(JobHistoryFileReplayHelper.PROCESSING_PATH, args[++i]);\r\n            } else if (\"-r\".equals(args[i])) {\r\n                conf.setInt(JobHistoryFileReplayHelper.REPLAY_MODE, Integer.parseInt(args[++i]));\r\n            } else {\r\n                System.out.println(\"Unexpected argument: \" + args[i]);\r\n                return printUsage() == 0;\r\n            }\r\n        } catch (NumberFormatException except) {\r\n            System.out.println(\"ERROR: Integer expected instead of \" + args[i]);\r\n            return printUsage() == 0;\r\n        } catch (Exception e) {\r\n            throw (IOException) new IOException().initCause(e);\r\n        }\r\n    }\r\n    switch(mapperType) {\r\n        case JOB_HISTORY_FILE_REPLAY_MAPPER:\r\n            String processingPath = conf.get(JobHistoryFileReplayHelper.PROCESSING_PATH);\r\n            if (processingPath == null || processingPath.isEmpty()) {\r\n                System.out.println(\"processing path is missing while mtype = 2\");\r\n                return printUsage() == 0;\r\n            }\r\n            switch(timeline_service_version) {\r\n                case TIMELINE_SERVICE_VERSION_2:\r\n                    job.setMapperClass(JobHistoryFileReplayMapperV2.class);\r\n                    break;\r\n                case TIMELINE_SERVICE_VERSION_1:\r\n                default:\r\n                    job.setMapperClass(JobHistoryFileReplayMapperV1.class);\r\n                    break;\r\n            }\r\n            break;\r\n        case SIMPLE_ENTITY_WRITER:\r\n        default:\r\n            conf.setLong(SimpleEntityWriterConstants.TIMELINE_SERVICE_PERFORMANCE_RUN_ID, System.currentTimeMillis());\r\n            switch(timeline_service_version) {\r\n                case TIMELINE_SERVICE_VERSION_2:\r\n                    job.setMapperClass(SimpleEntityWriterV2.class);\r\n                    break;\r\n                case TIMELINE_SERVICE_VERSION_1:\r\n                default:\r\n                    job.setMapperClass(SimpleEntityWriterV1.class);\r\n                    break;\r\n            }\r\n            break;\r\n    }\r\n    return true;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "run",
  "errType" : null,
  "containingMethodsNum" : 23,
  "sourceCodeText" : "int run(String[] args) throws Exception\n{\r\n    Job job = Job.getInstance(getConf());\r\n    job.setJarByClass(TimelineServicePerformance.class);\r\n    job.setMapperClass(SimpleEntityWriterV1.class);\r\n    job.setInputFormatClass(SleepInputFormat.class);\r\n    job.setOutputFormatClass(NullOutputFormat.class);\r\n    job.setNumReduceTasks(0);\r\n    if (!parseArgs(args, job)) {\r\n        return -1;\r\n    }\r\n    Date startTime = new Date();\r\n    System.out.println(\"Job started: \" + startTime);\r\n    int ret = job.waitForCompletion(true) ? 0 : 1;\r\n    if (job.isSuccessful()) {\r\n        org.apache.hadoop.mapreduce.Counters counters = job.getCounters();\r\n        long writecounts = counters.findCounter(PerfCounters.TIMELINE_SERVICE_WRITE_COUNTER).getValue();\r\n        long writefailures = counters.findCounter(PerfCounters.TIMELINE_SERVICE_WRITE_FAILURES).getValue();\r\n        if (writefailures > 0 && writefailures == writecounts) {\r\n            System.out.println(\"Job failed: all writes failed!\");\r\n        } else {\r\n            long writetime = counters.findCounter(PerfCounters.TIMELINE_SERVICE_WRITE_TIME).getValue();\r\n            long writesize = counters.findCounter(PerfCounters.TIMELINE_SERVICE_WRITE_KBS).getValue();\r\n            if (writetime == 0L) {\r\n                System.out.println(\"Job failed: write time is 0!\");\r\n            } else {\r\n                double transacrate = writecounts * 1000 / (double) writetime;\r\n                double iorate = writesize * 1000 / (double) writetime;\r\n                int numMaps = Integer.parseInt(job.getConfiguration().get(MRJobConfig.NUM_MAPS));\r\n                System.out.println(\"TRANSACTION RATE (per mapper): \" + transacrate + \" ops/s\");\r\n                System.out.println(\"IO RATE (per mapper): \" + iorate + \" KB/s\");\r\n                System.out.println(\"TRANSACTION RATE (total): \" + transacrate * numMaps + \" ops/s\");\r\n                System.out.println(\"IO RATE (total): \" + iorate * numMaps + \" KB/s\");\r\n            }\r\n        }\r\n    } else {\r\n        System.out.println(\"Job failed: \" + job.getStatus().getFailureInfo());\r\n    }\r\n    return ret;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "main",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void main(String[] args) throws Exception\n{\r\n    int res = ToolRunner.run(new Configuration(), new TimelineServicePerformance(), args);\r\n    System.exit(res);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred\\pipes",
  "methodName" : "main",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void main(String[] args)\n{\r\n    PipeReducerStub client = new PipeReducerStub();\r\n    client.binaryProtocolStub();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred\\pipes",
  "methodName" : "binaryProtocolStub",
  "errType" : [ "Exception" ],
  "containingMethodsNum" : 16,
  "sourceCodeText" : "void binaryProtocolStub()\n{\r\n    try {\r\n        initSoket();\r\n        WritableUtils.readVInt(dataInput);\r\n        WritableUtils.readVInt(dataInput);\r\n        int intValue = WritableUtils.readVInt(dataInput);\r\n        System.out.println(\"getIsJavaRecordWriter:\" + intValue);\r\n        WritableUtils.readVInt(dataInput);\r\n        BooleanWritable value = new BooleanWritable();\r\n        readObject(value, dataInput);\r\n        System.out.println(\"reducer key :\" + value);\r\n        while ((intValue = WritableUtils.readVInt(dataInput)) == 7) {\r\n            Text txt = new Text();\r\n            readObject(txt, dataInput);\r\n            System.out.println(\"reduce value  :\" + txt);\r\n        }\r\n        WritableUtils.writeVInt(dataOut, 54);\r\n        dataOut.flush();\r\n        dataOut.close();\r\n    } catch (Exception x) {\r\n        x.printStackTrace();\r\n    } finally {\r\n        closeSoket();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "runValueIterator",
  "errType" : null,
  "containingMethodsNum" : 20,
  "sourceCodeText" : "void runValueIterator(Path tmpDir, Pair[] vals, Configuration conf, CompressionCodec codec) throws IOException\n{\r\n    FileSystem localFs = FileSystem.getLocal(conf);\r\n    FileSystem rfs = ((LocalFileSystem) localFs).getRaw();\r\n    Path path = new Path(tmpDir, \"data.in\");\r\n    IFile.Writer<Text, Text> writer = new IFile.Writer<Text, Text>(conf, rfs.create(path), Text.class, Text.class, codec, null);\r\n    for (Pair p : vals) {\r\n        writer.append(new Text(p.key), new Text(p.value));\r\n    }\r\n    writer.close();\r\n    @SuppressWarnings(\"unchecked\")\r\n    RawKeyValueIterator rawItr = Merger.merge(conf, rfs, Text.class, Text.class, codec, new Path[] { path }, false, conf.getInt(JobContext.IO_SORT_FACTOR, 100), tmpDir, new Text.Comparator(), new NullProgress(), null, null, null);\r\n    @SuppressWarnings(\"unchecked\")\r\n    ReduceTask.ValuesIterator valItr = new ReduceTask.ValuesIterator<Text, Text>(rawItr, WritableComparator.get(Text.class), Text.class, Text.class, conf, new NullProgress());\r\n    int i = 0;\r\n    while (valItr.more()) {\r\n        Object key = valItr.getKey();\r\n        String keyString = key.toString();\r\n        assertEquals(vals[i].key, keyString);\r\n        assertTrue(valItr.hasNext());\r\n        while (valItr.hasNext()) {\r\n            String valueString = valItr.next().toString();\r\n            assertEquals(vals[i].value, valueString);\r\n            assertEquals(vals[i].key, valItr.getKey().toString());\r\n            i += 1;\r\n        }\r\n        assertEquals(keyString, valItr.getKey().toString());\r\n        valItr.nextKey();\r\n    }\r\n    assertEquals(vals.length, i);\r\n    assertEquals(1.0f, rawItr.getProgress().get(), 0.0000);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "testValueIterator",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testValueIterator() throws Exception\n{\r\n    Path tmpDir = new Path(\"build/test/test.reduce.task\");\r\n    Configuration conf = new Configuration();\r\n    for (Pair[] testCase : testCases) {\r\n        runValueIterator(tmpDir, testCase, conf, null);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "testValueIteratorWithCompression",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void testValueIteratorWithCompression() throws Exception\n{\r\n    Path tmpDir = new Path(\"build/test/test.reduce.task.compression\");\r\n    Configuration conf = new Configuration();\r\n    DefaultCodec codec = new DefaultCodec();\r\n    codec.setConf(conf);\r\n    for (Pair[] testCase : testCases) {\r\n        runValueIterator(tmpDir, testCase, conf, codec);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\fs",
  "methodName" : "getBaseDir",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String getBaseDir(Configuration conf)\n{\r\n    return conf.get(\"test.build.data\", \"/benchmarks/TestDFSIO\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\fs",
  "methodName" : "getControlDir",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Path getControlDir(Configuration conf)\n{\r\n    return new Path(getBaseDir(conf), \"io_control\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\fs",
  "methodName" : "getWriteDir",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Path getWriteDir(Configuration conf)\n{\r\n    return new Path(getBaseDir(conf), \"io_write\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\fs",
  "methodName" : "getReadDir",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Path getReadDir(Configuration conf)\n{\r\n    return new Path(getBaseDir(conf), \"io_read\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\fs",
  "methodName" : "getAppendDir",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Path getAppendDir(Configuration conf)\n{\r\n    return new Path(getBaseDir(conf), \"io_append\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\fs",
  "methodName" : "getRandomReadDir",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Path getRandomReadDir(Configuration conf)\n{\r\n    return new Path(getBaseDir(conf), \"io_random_read\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\fs",
  "methodName" : "getTruncateDir",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Path getTruncateDir(Configuration conf)\n{\r\n    return new Path(getBaseDir(conf), \"io_truncate\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\fs",
  "methodName" : "getDataDir",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Path getDataDir(Configuration conf)\n{\r\n    return new Path(getBaseDir(conf), \"io_data\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\fs",
  "methodName" : "beforeClass",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void beforeClass() throws Exception\n{\r\n    bench = new TestDFSIO();\r\n    bench.getConf().setInt(DFSConfigKeys.DFS_HEARTBEAT_INTERVAL_KEY, 1);\r\n    cluster = new MiniDFSCluster.Builder(bench.getConf()).numDataNodes(2).format(true).build();\r\n    FileSystem fs = cluster.getFileSystem();\r\n    bench.createControlFile(fs, DEFAULT_NR_BYTES, DEFAULT_NR_FILES);\r\n    testWrite();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\fs",
  "methodName" : "afterClass",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void afterClass() throws Exception\n{\r\n    if (cluster == null)\r\n        return;\r\n    FileSystem fs = cluster.getFileSystem();\r\n    bench.cleanup(fs);\r\n    cluster.shutdown();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\fs",
  "methodName" : "testWrite",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void testWrite() throws Exception\n{\r\n    FileSystem fs = cluster.getFileSystem();\r\n    long execTime = bench.writeTest(fs);\r\n    bench.analyzeResult(fs, TestType.TEST_TYPE_WRITE, execTime);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\fs",
  "methodName" : "testRead",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void testRead() throws Exception\n{\r\n    FileSystem fs = cluster.getFileSystem();\r\n    long execTime = bench.readTest(fs);\r\n    bench.analyzeResult(fs, TestType.TEST_TYPE_READ, execTime);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\fs",
  "methodName" : "testReadRandom",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void testReadRandom() throws Exception\n{\r\n    FileSystem fs = cluster.getFileSystem();\r\n    bench.getConf().setLong(\"test.io.skip.size\", 0);\r\n    long execTime = bench.randomReadTest(fs);\r\n    bench.analyzeResult(fs, TestType.TEST_TYPE_READ_RANDOM, execTime);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\fs",
  "methodName" : "testReadBackward",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void testReadBackward() throws Exception\n{\r\n    FileSystem fs = cluster.getFileSystem();\r\n    bench.getConf().setLong(\"test.io.skip.size\", -DEFAULT_BUFFER_SIZE);\r\n    long execTime = bench.randomReadTest(fs);\r\n    bench.analyzeResult(fs, TestType.TEST_TYPE_READ_BACKWARD, execTime);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\fs",
  "methodName" : "testReadSkip",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void testReadSkip() throws Exception\n{\r\n    FileSystem fs = cluster.getFileSystem();\r\n    bench.getConf().setLong(\"test.io.skip.size\", 1);\r\n    long execTime = bench.randomReadTest(fs);\r\n    bench.analyzeResult(fs, TestType.TEST_TYPE_READ_SKIP, execTime);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\fs",
  "methodName" : "testAppend",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void testAppend() throws Exception\n{\r\n    FileSystem fs = cluster.getFileSystem();\r\n    long execTime = bench.appendTest(fs);\r\n    bench.analyzeResult(fs, TestType.TEST_TYPE_APPEND, execTime);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\fs",
  "methodName" : "testTruncate",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void testTruncate() throws Exception\n{\r\n    FileSystem fs = cluster.getFileSystem();\r\n    bench.createControlFile(fs, DEFAULT_NR_BYTES / 2, DEFAULT_NR_FILES);\r\n    long execTime = bench.truncateTest(fs);\r\n    bench.analyzeResult(fs, TestType.TEST_TYPE_TRUNCATE, execTime);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\fs",
  "methodName" : "createControlFile",
  "errType" : [ "Exception" ],
  "containingMethodsNum" : 10,
  "sourceCodeText" : "void createControlFile(FileSystem fs, long nrBytes, int nrFiles) throws IOException\n{\r\n    LOG.info(\"creating control file: \" + nrBytes + \" bytes, \" + nrFiles + \" files\");\r\n    final int maxDirItems = config.getInt(DFSConfigKeys.DFS_NAMENODE_MAX_DIRECTORY_ITEMS_KEY, DFSConfigKeys.DFS_NAMENODE_MAX_DIRECTORY_ITEMS_DEFAULT);\r\n    Path controlDir = getControlDir(config);\r\n    if (nrFiles > maxDirItems) {\r\n        final String message = \"The directory item limit of \" + controlDir + \" is exceeded: limit=\" + maxDirItems + \" items=\" + nrFiles;\r\n        throw new IOException(message);\r\n    }\r\n    fs.delete(controlDir, true);\r\n    for (int i = 0; i < nrFiles; i++) {\r\n        String name = getFileName(i);\r\n        Path controlFile = new Path(controlDir, \"in_file_\" + name);\r\n        SequenceFile.Writer writer = null;\r\n        try {\r\n            writer = SequenceFile.createWriter(fs, config, controlFile, Text.class, LongWritable.class, CompressionType.NONE);\r\n            writer.append(new Text(name), new LongWritable(nrBytes));\r\n        } catch (Exception e) {\r\n            throw new IOException(e.getLocalizedMessage());\r\n        } finally {\r\n            if (writer != null) {\r\n                writer.close();\r\n            }\r\n            writer = null;\r\n        }\r\n    }\r\n    LOG.info(\"created control files for: \" + nrFiles + \" files\");\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\fs",
  "methodName" : "getFileName",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String getFileName(int fIdx)\n{\r\n    return BASE_FILE_NAME + Integer.toString(fIdx);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\fs",
  "methodName" : "writeTest",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "long writeTest(FileSystem fs) throws IOException\n{\r\n    Path writeDir = getWriteDir(config);\r\n    fs.delete(getDataDir(config), true);\r\n    fs.delete(writeDir, true);\r\n    long tStart = System.currentTimeMillis();\r\n    if (isECEnabled()) {\r\n        createAndEnableECOnPath(fs, getDataDir(config));\r\n    }\r\n    runIOTest(WriteMapper.class, writeDir);\r\n    long execTime = System.currentTimeMillis() - tStart;\r\n    return execTime;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\fs",
  "methodName" : "runIOTest",
  "errType" : null,
  "containingMethodsNum" : 10,
  "sourceCodeText" : "void runIOTest(Class<? extends Mapper<Text, LongWritable, Text, Text>> mapperClass, Path outputDir) throws IOException\n{\r\n    JobConf job = new JobConf(config, TestDFSIO.class);\r\n    FileInputFormat.setInputPaths(job, getControlDir(config));\r\n    job.setInputFormat(SequenceFileInputFormat.class);\r\n    job.setMapperClass(mapperClass);\r\n    job.setReducerClass(AccumulatingReducer.class);\r\n    FileOutputFormat.setOutputPath(job, outputDir);\r\n    job.setOutputKeyClass(Text.class);\r\n    job.setOutputValueClass(Text.class);\r\n    job.setNumReduceTasks(1);\r\n    job.setSpeculativeExecution(false);\r\n    JobClient.runJob(job);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\fs",
  "methodName" : "appendTest",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "long appendTest(FileSystem fs) throws IOException\n{\r\n    Path appendDir = getAppendDir(config);\r\n    fs.delete(appendDir, true);\r\n    long tStart = System.currentTimeMillis();\r\n    runIOTest(AppendMapper.class, appendDir);\r\n    long execTime = System.currentTimeMillis() - tStart;\r\n    return execTime;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\fs",
  "methodName" : "readTest",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "long readTest(FileSystem fs) throws IOException\n{\r\n    Path readDir = getReadDir(config);\r\n    fs.delete(readDir, true);\r\n    long tStart = System.currentTimeMillis();\r\n    runIOTest(ReadMapper.class, readDir);\r\n    long execTime = System.currentTimeMillis() - tStart;\r\n    return execTime;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\fs",
  "methodName" : "randomReadTest",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "long randomReadTest(FileSystem fs) throws IOException\n{\r\n    Path readDir = getRandomReadDir(config);\r\n    fs.delete(readDir, true);\r\n    long tStart = System.currentTimeMillis();\r\n    runIOTest(RandomReadMapper.class, readDir);\r\n    long execTime = System.currentTimeMillis() - tStart;\r\n    return execTime;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\fs",
  "methodName" : "truncateTest",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "long truncateTest(FileSystem fs) throws IOException\n{\r\n    Path TruncateDir = getTruncateDir(config);\r\n    fs.delete(TruncateDir, true);\r\n    long tStart = System.currentTimeMillis();\r\n    runIOTest(TruncateMapper.class, TruncateDir);\r\n    long execTime = System.currentTimeMillis() - tStart;\r\n    return execTime;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\fs",
  "methodName" : "sequentialTest",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void sequentialTest(FileSystem fs, TestType testType, long fileSize, int nrFiles) throws IOException\n{\r\n    IOStatMapper ioer = null;\r\n    switch(testType) {\r\n        case TEST_TYPE_READ:\r\n            ioer = new ReadMapper();\r\n            break;\r\n        case TEST_TYPE_WRITE:\r\n            ioer = new WriteMapper();\r\n            break;\r\n        case TEST_TYPE_APPEND:\r\n            ioer = new AppendMapper();\r\n            break;\r\n        case TEST_TYPE_READ_RANDOM:\r\n        case TEST_TYPE_READ_BACKWARD:\r\n        case TEST_TYPE_READ_SKIP:\r\n            ioer = new RandomReadMapper();\r\n            break;\r\n        case TEST_TYPE_TRUNCATE:\r\n            ioer = new TruncateMapper();\r\n            break;\r\n        default:\r\n            return;\r\n    }\r\n    for (int i = 0; i < nrFiles; i++) ioer.doIO(Reporter.NULL, BASE_FILE_NAME + Integer.toString(i), fileSize);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\fs",
  "methodName" : "main",
  "errType" : [ "Exception" ],
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void main(String[] args)\n{\r\n    TestDFSIO bench = new TestDFSIO();\r\n    int res = -1;\r\n    try {\r\n        res = ToolRunner.run(bench, args);\r\n    } catch (Exception e) {\r\n        System.err.print(StringUtils.stringifyException(e));\r\n        res = -2;\r\n    }\r\n    if (res == -1) {\r\n        System.err.println(USAGE);\r\n    }\r\n    System.exit(res);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\fs",
  "methodName" : "run",
  "errType" : null,
  "containingMethodsNum" : 53,
  "sourceCodeText" : "int run(String[] args) throws IOException\n{\r\n    TestType testType = null;\r\n    int bufferSize = DEFAULT_BUFFER_SIZE;\r\n    long nrBytes = 1 * MEGA;\r\n    String erasureCodePolicyName = null;\r\n    int nrFiles = 1;\r\n    long skipSize = 0;\r\n    String resFileName = DEFAULT_RES_FILE_NAME;\r\n    String compressionClass = null;\r\n    String storagePolicy = null;\r\n    boolean isSequential = false;\r\n    String version = TestDFSIO.class.getSimpleName() + \".1.8\";\r\n    LOG.info(version);\r\n    if (args.length == 0) {\r\n        System.err.println(\"Missing arguments.\");\r\n        return -1;\r\n    }\r\n    for (int i = 0; i < args.length; i++) {\r\n        if (StringUtils.toLowerCase(args[i]).startsWith(\"-read\")) {\r\n            testType = TestType.TEST_TYPE_READ;\r\n        } else if (args[i].equalsIgnoreCase(\"-write\")) {\r\n            testType = TestType.TEST_TYPE_WRITE;\r\n        } else if (args[i].equalsIgnoreCase(\"-append\")) {\r\n            testType = TestType.TEST_TYPE_APPEND;\r\n        } else if (args[i].equalsIgnoreCase(\"-random\")) {\r\n            if (testType != TestType.TEST_TYPE_READ)\r\n                return -1;\r\n            testType = TestType.TEST_TYPE_READ_RANDOM;\r\n        } else if (args[i].equalsIgnoreCase(\"-backward\")) {\r\n            if (testType != TestType.TEST_TYPE_READ)\r\n                return -1;\r\n            testType = TestType.TEST_TYPE_READ_BACKWARD;\r\n        } else if (args[i].equalsIgnoreCase(\"-skip\")) {\r\n            if (testType != TestType.TEST_TYPE_READ)\r\n                return -1;\r\n            testType = TestType.TEST_TYPE_READ_SKIP;\r\n        } else if (args[i].equalsIgnoreCase(\"-truncate\")) {\r\n            testType = TestType.TEST_TYPE_TRUNCATE;\r\n        } else if (args[i].equalsIgnoreCase(\"-clean\")) {\r\n            testType = TestType.TEST_TYPE_CLEANUP;\r\n        } else if (StringUtils.toLowerCase(args[i]).startsWith(\"-seq\")) {\r\n            isSequential = true;\r\n        } else if (StringUtils.toLowerCase(args[i]).startsWith(\"-compression\")) {\r\n            compressionClass = args[++i];\r\n        } else if (args[i].equalsIgnoreCase(\"-nrfiles\")) {\r\n            nrFiles = Integer.parseInt(args[++i]);\r\n        } else if (args[i].equalsIgnoreCase(\"-filesize\") || args[i].equalsIgnoreCase(\"-size\")) {\r\n            nrBytes = parseSize(args[++i]);\r\n        } else if (args[i].equalsIgnoreCase(\"-skipsize\")) {\r\n            skipSize = parseSize(args[++i]);\r\n        } else if (args[i].equalsIgnoreCase(\"-buffersize\")) {\r\n            bufferSize = Integer.parseInt(args[++i]);\r\n        } else if (args[i].equalsIgnoreCase(\"-resfile\")) {\r\n            resFileName = args[++i];\r\n        } else if (args[i].equalsIgnoreCase(\"-storagePolicy\")) {\r\n            storagePolicy = args[++i];\r\n        } else if (args[i].equalsIgnoreCase(\"-erasureCodePolicy\")) {\r\n            erasureCodePolicyName = args[++i];\r\n        } else {\r\n            System.err.println(\"Illegal argument: \" + args[i]);\r\n            return -1;\r\n        }\r\n    }\r\n    if (testType == null) {\r\n        return -1;\r\n    }\r\n    if (testType == TestType.TEST_TYPE_READ_BACKWARD) {\r\n        skipSize = -bufferSize;\r\n    } else if (testType == TestType.TEST_TYPE_READ_SKIP && skipSize == 0) {\r\n        skipSize = bufferSize;\r\n    }\r\n    LOG.info(\"nrFiles = \" + nrFiles);\r\n    LOG.info(\"nrBytes (MB) = \" + toMB(nrBytes));\r\n    LOG.info(\"bufferSize = \" + bufferSize);\r\n    if (skipSize > 0) {\r\n        LOG.info(\"skipSize = \" + skipSize);\r\n    }\r\n    LOG.info(\"baseDir = \" + getBaseDir(config));\r\n    if (compressionClass != null) {\r\n        config.set(\"test.io.compression.class\", compressionClass);\r\n        LOG.info(\"compressionClass = \" + compressionClass);\r\n    }\r\n    config.setInt(\"test.io.file.buffer.size\", bufferSize);\r\n    config.setLong(\"test.io.skip.size\", skipSize);\r\n    FileSystem fs = FileSystem.get(config);\r\n    if (erasureCodePolicyName != null) {\r\n        if (!checkErasureCodePolicy(erasureCodePolicyName, fs, testType)) {\r\n            return -1;\r\n        }\r\n    }\r\n    if (storagePolicy != null) {\r\n        if (!checkStoragePolicy(storagePolicy, fs)) {\r\n            return -1;\r\n        }\r\n    }\r\n    if (isSequential) {\r\n        long tStart = System.currentTimeMillis();\r\n        sequentialTest(fs, testType, nrBytes, nrFiles);\r\n        long execTime = System.currentTimeMillis() - tStart;\r\n        String resultLine = \"Seq Test exec time sec: \" + msToSecs(execTime);\r\n        LOG.info(resultLine);\r\n        return 0;\r\n    }\r\n    if (testType == TestType.TEST_TYPE_CLEANUP) {\r\n        cleanup(fs);\r\n        return 0;\r\n    }\r\n    createControlFile(fs, nrBytes, nrFiles);\r\n    long tStart = System.currentTimeMillis();\r\n    switch(testType) {\r\n        case TEST_TYPE_WRITE:\r\n            writeTest(fs);\r\n            break;\r\n        case TEST_TYPE_READ:\r\n            readTest(fs);\r\n            break;\r\n        case TEST_TYPE_APPEND:\r\n            appendTest(fs);\r\n            break;\r\n        case TEST_TYPE_READ_RANDOM:\r\n        case TEST_TYPE_READ_BACKWARD:\r\n        case TEST_TYPE_READ_SKIP:\r\n            randomReadTest(fs);\r\n            break;\r\n        case TEST_TYPE_TRUNCATE:\r\n            truncateTest(fs);\r\n            break;\r\n        default:\r\n    }\r\n    long execTime = System.currentTimeMillis() - tStart;\r\n    analyzeResult(fs, testType, execTime, resFileName);\r\n    return 0;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\fs",
  "methodName" : "getConf",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Configuration getConf()\n{\r\n    return this.config;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\fs",
  "methodName" : "setConf",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setConf(Configuration conf)\n{\r\n    this.config = conf;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\fs",
  "methodName" : "parseSize",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "long parseSize(String arg)\n{\r\n    String[] args = arg.split(\"\\\\D\", 2);\r\n    assert args.length <= 2;\r\n    long nrBytes = Long.parseLong(args[0]);\r\n    String bytesMult = arg.substring(args[0].length());\r\n    return nrBytes * ByteMultiple.parseString(bytesMult).value();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\fs",
  "methodName" : "toMB",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "float toMB(long bytes)\n{\r\n    return ((float) bytes) / MEGA;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\fs",
  "methodName" : "msToSecs",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "float msToSecs(long timeMillis)\n{\r\n    return timeMillis / 1000.0f;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\fs",
  "methodName" : "checkErasureCodePolicy",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "boolean checkErasureCodePolicy(String erasureCodePolicyName, FileSystem fs, TestType testType) throws IOException\n{\r\n    Collection<ErasureCodingPolicyInfo> list = ((DistributedFileSystem) fs).getAllErasureCodingPolicies();\r\n    boolean isValid = false;\r\n    for (ErasureCodingPolicyInfo ec : list) {\r\n        if (erasureCodePolicyName.equals(ec.getPolicy().getName())) {\r\n            isValid = true;\r\n            break;\r\n        }\r\n    }\r\n    if (!isValid) {\r\n        System.out.println(\"Invalid erasure code policy: \" + erasureCodePolicyName);\r\n        System.out.println(\"Current supported erasure code policy list: \");\r\n        for (ErasureCodingPolicyInfo ec : list) {\r\n            System.out.println(ec.getPolicy().getName());\r\n        }\r\n        return false;\r\n    }\r\n    if (testType == TestType.TEST_TYPE_APPEND || testType == TestType.TEST_TYPE_TRUNCATE) {\r\n        System.out.println(\"So far append or truncate operation\" + \" does not support erasureCodePolicy\");\r\n        return false;\r\n    }\r\n    config.set(ERASURE_CODE_POLICY_NAME_KEY, erasureCodePolicyName);\r\n    LOG.info(\"erasureCodePolicy = \" + erasureCodePolicyName);\r\n    return true;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\fs",
  "methodName" : "checkStoragePolicy",
  "errType" : [ "Exception" ],
  "containingMethodsNum" : 7,
  "sourceCodeText" : "boolean checkStoragePolicy(String storagePolicy, FileSystem fs) throws IOException\n{\r\n    boolean isValid = false;\r\n    Collection<BlockStoragePolicy> storagePolicies = ((DistributedFileSystem) fs).getAllStoragePolicies();\r\n    try {\r\n        for (BlockStoragePolicy policy : storagePolicies) {\r\n            if (policy.getName().equals(storagePolicy)) {\r\n                isValid = true;\r\n                break;\r\n            }\r\n        }\r\n    } catch (Exception e) {\r\n        throw new IOException(\"Get block storage policies error: \", e);\r\n    }\r\n    if (!isValid) {\r\n        System.out.println(\"Invalid block storage policy: \" + storagePolicy);\r\n        System.out.println(\"Current supported storage policy list: \");\r\n        for (BlockStoragePolicy policy : storagePolicies) {\r\n            System.out.println(policy.getName());\r\n        }\r\n        return false;\r\n    }\r\n    config.set(STORAGE_POLICY_NAME_KEY, storagePolicy);\r\n    LOG.info(\"storagePolicy = \" + storagePolicy);\r\n    return true;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\fs",
  "methodName" : "isECEnabled",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "boolean isECEnabled()\n{\r\n    String erasureCodePolicyName = getConf().get(ERASURE_CODE_POLICY_NAME_KEY, null);\r\n    return erasureCodePolicyName != null ? true : false;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\fs",
  "methodName" : "createAndEnableECOnPath",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void createAndEnableECOnPath(FileSystem fs, Path path) throws IOException\n{\r\n    String erasureCodePolicyName = getConf().get(ERASURE_CODE_POLICY_NAME_KEY, null);\r\n    fs.mkdirs(path);\r\n    Collection<ErasureCodingPolicyInfo> list = ((DistributedFileSystem) fs).getAllErasureCodingPolicies();\r\n    for (ErasureCodingPolicyInfo info : list) {\r\n        final ErasureCodingPolicy ec = info.getPolicy();\r\n        if (erasureCodePolicyName.equals(ec.getName())) {\r\n            ((DistributedFileSystem) fs).setErasureCodingPolicy(path, ec.getName());\r\n            LOG.info(\"enable erasureCodePolicy = \" + erasureCodePolicyName + \" on \" + path.toString());\r\n            break;\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\fs",
  "methodName" : "analyzeResult",
  "errType" : null,
  "containingMethodsNum" : 26,
  "sourceCodeText" : "void analyzeResult(FileSystem fs, TestType testType, long execTime, String resFileName) throws IOException\n{\r\n    Path reduceFile = getReduceFilePath(testType);\r\n    long tasks = 0;\r\n    long size = 0;\r\n    long time = 0;\r\n    float rate = 0;\r\n    float sqrate = 0;\r\n    DataInputStream in = null;\r\n    BufferedReader lines = null;\r\n    try {\r\n        in = new DataInputStream(fs.open(reduceFile));\r\n        lines = new BufferedReader(new InputStreamReader(in));\r\n        String line;\r\n        while ((line = lines.readLine()) != null) {\r\n            StringTokenizer tokens = new StringTokenizer(line, \" \\t\\n\\r\\f%\");\r\n            String attr = tokens.nextToken();\r\n            if (attr.endsWith(\":tasks\"))\r\n                tasks = Long.parseLong(tokens.nextToken());\r\n            else if (attr.endsWith(\":size\"))\r\n                size = Long.parseLong(tokens.nextToken());\r\n            else if (attr.endsWith(\":time\"))\r\n                time = Long.parseLong(tokens.nextToken());\r\n            else if (attr.endsWith(\":rate\"))\r\n                rate = Float.parseFloat(tokens.nextToken());\r\n            else if (attr.endsWith(\":sqrate\"))\r\n                sqrate = Float.parseFloat(tokens.nextToken());\r\n        }\r\n    } finally {\r\n        if (in != null)\r\n            in.close();\r\n        if (lines != null)\r\n            lines.close();\r\n    }\r\n    double med = rate / 1000 / tasks;\r\n    double stdDev = Math.sqrt(Math.abs(sqrate / 1000 / tasks - med * med));\r\n    DecimalFormat df = new DecimalFormat(\"#.##\");\r\n    String[] resultLines = { \"----- TestDFSIO ----- : \" + testType, \"            Date & time: \" + new Date(System.currentTimeMillis()), \"        Number of files: \" + tasks, \" Total MBytes processed: \" + df.format(toMB(size)), \"      Throughput mb/sec: \" + df.format(toMB(size) / msToSecs(time)), \" Average IO rate mb/sec: \" + df.format(med), \"  IO rate std deviation: \" + df.format(stdDev), \"     Test exec time sec: \" + df.format(msToSecs(execTime)), \"\" };\r\n    PrintStream res = null;\r\n    try {\r\n        res = new PrintStream(new FileOutputStream(new File(resFileName), true));\r\n        for (int i = 0; i < resultLines.length; i++) {\r\n            LOG.info(resultLines[i]);\r\n            res.println(resultLines[i]);\r\n        }\r\n    } finally {\r\n        if (res != null)\r\n            res.close();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 2,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\fs",
  "methodName" : "getReduceFilePath",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "Path getReduceFilePath(TestType testType)\n{\r\n    switch(testType) {\r\n        case TEST_TYPE_WRITE:\r\n            return new Path(getWriteDir(config), \"part-00000\");\r\n        case TEST_TYPE_APPEND:\r\n            return new Path(getAppendDir(config), \"part-00000\");\r\n        case TEST_TYPE_READ:\r\n            return new Path(getReadDir(config), \"part-00000\");\r\n        case TEST_TYPE_READ_RANDOM:\r\n        case TEST_TYPE_READ_BACKWARD:\r\n        case TEST_TYPE_READ_SKIP:\r\n            return new Path(getRandomReadDir(config), \"part-00000\");\r\n        case TEST_TYPE_TRUNCATE:\r\n            return new Path(getTruncateDir(config), \"part-00000\");\r\n        default:\r\n    }\r\n    return null;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\fs",
  "methodName" : "analyzeResult",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void analyzeResult(FileSystem fs, TestType testType, long execTime) throws IOException\n{\r\n    String dir = System.getProperty(\"test.build.dir\", \"target/test-dir\");\r\n    analyzeResult(fs, testType, execTime, dir + \"/\" + DEFAULT_RES_FILE_NAME);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\fs",
  "methodName" : "cleanup",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void cleanup(FileSystem fs) throws IOException\n{\r\n    LOG.info(\"Cleaning up test files\");\r\n    fs.delete(new Path(getBaseDir(config)), true);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\testjar",
  "methodName" : "configure",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void configure(JobConf job)\n{\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\testjar",
  "methodName" : "close",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void close() throws IOException\n{\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\testjar",
  "methodName" : "reduce",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void reduce(WritableComparable key, Iterator<Writable> values, OutputCollector<WritableComparable, Writable> output, Reporter reporter) throws IOException\n{\r\n    while (values.hasNext()) {\r\n        output.collect(key, values.next());\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output",
  "methodName" : "testBinary",
  "errType" : null,
  "containingMethodsNum" : 44,
  "sourceCodeText" : "void testBinary() throws IOException, InterruptedException\n{\r\n    Configuration conf = new Configuration();\r\n    Job job = Job.getInstance(conf);\r\n    Path outdir = new Path(System.getProperty(\"test.build.data\", \"/tmp\"), \"outseq\");\r\n    Random r = new Random();\r\n    long seed = r.nextLong();\r\n    r.setSeed(seed);\r\n    FileOutputFormat.setOutputPath(job, outdir);\r\n    SequenceFileAsBinaryOutputFormat.setSequenceFileOutputKeyClass(job, IntWritable.class);\r\n    SequenceFileAsBinaryOutputFormat.setSequenceFileOutputValueClass(job, DoubleWritable.class);\r\n    SequenceFileAsBinaryOutputFormat.setCompressOutput(job, true);\r\n    SequenceFileAsBinaryOutputFormat.setOutputCompressionType(job, CompressionType.BLOCK);\r\n    BytesWritable bkey = new BytesWritable();\r\n    BytesWritable bval = new BytesWritable();\r\n    TaskAttemptContext context = MapReduceTestUtil.createDummyMapTaskAttemptContext(job.getConfiguration());\r\n    OutputFormat<BytesWritable, BytesWritable> outputFormat = new SequenceFileAsBinaryOutputFormat();\r\n    OutputCommitter committer = outputFormat.getOutputCommitter(context);\r\n    committer.setupJob(job);\r\n    RecordWriter<BytesWritable, BytesWritable> writer = outputFormat.getRecordWriter(context);\r\n    IntWritable iwritable = new IntWritable();\r\n    DoubleWritable dwritable = new DoubleWritable();\r\n    DataOutputBuffer outbuf = new DataOutputBuffer();\r\n    LOG.info(\"Creating data by SequenceFileAsBinaryOutputFormat\");\r\n    try {\r\n        for (int i = 0; i < RECORDS; ++i) {\r\n            iwritable = new IntWritable(r.nextInt());\r\n            iwritable.write(outbuf);\r\n            bkey.set(outbuf.getData(), 0, outbuf.getLength());\r\n            outbuf.reset();\r\n            dwritable = new DoubleWritable(r.nextDouble());\r\n            dwritable.write(outbuf);\r\n            bval.set(outbuf.getData(), 0, outbuf.getLength());\r\n            outbuf.reset();\r\n            writer.write(bkey, bval);\r\n        }\r\n    } finally {\r\n        writer.close(context);\r\n    }\r\n    committer.commitTask(context);\r\n    committer.commitJob(job);\r\n    InputFormat<IntWritable, DoubleWritable> iformat = new SequenceFileInputFormat<IntWritable, DoubleWritable>();\r\n    int count = 0;\r\n    r.setSeed(seed);\r\n    SequenceFileInputFormat.setInputPaths(job, outdir);\r\n    LOG.info(\"Reading data by SequenceFileInputFormat\");\r\n    for (InputSplit split : iformat.getSplits(job)) {\r\n        RecordReader<IntWritable, DoubleWritable> reader = iformat.createRecordReader(split, context);\r\n        MapContext<IntWritable, DoubleWritable, BytesWritable, BytesWritable> mcontext = new MapContextImpl<IntWritable, DoubleWritable, BytesWritable, BytesWritable>(job.getConfiguration(), context.getTaskAttemptID(), reader, null, null, MapReduceTestUtil.createDummyReporter(), split);\r\n        reader.initialize(split, mcontext);\r\n        try {\r\n            int sourceInt;\r\n            double sourceDouble;\r\n            while (reader.nextKeyValue()) {\r\n                sourceInt = r.nextInt();\r\n                sourceDouble = r.nextDouble();\r\n                iwritable = reader.getCurrentKey();\r\n                dwritable = reader.getCurrentValue();\r\n                assertEquals(\"Keys don't match: \" + \"*\" + iwritable.get() + \":\" + sourceInt + \"*\", sourceInt, iwritable.get());\r\n                assertThat(dwritable.get()).withFailMessage(\"Vals don't match: \" + \"*\" + dwritable.get() + \":\" + sourceDouble + \"*\").isEqualTo(sourceDouble);\r\n                ++count;\r\n            }\r\n        } finally {\r\n            reader.close();\r\n        }\r\n    }\r\n    assertEquals(\"Some records not found\", RECORDS, count);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 2,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output",
  "methodName" : "testSequenceOutputClassDefaultsToMapRedOutputClass",
  "errType" : null,
  "containingMethodsNum" : 9,
  "sourceCodeText" : "void testSequenceOutputClassDefaultsToMapRedOutputClass() throws IOException\n{\r\n    Job job = Job.getInstance();\r\n    job.setOutputKeyClass(FloatWritable.class);\r\n    job.setOutputValueClass(BooleanWritable.class);\r\n    assertEquals(\"SequenceFileOutputKeyClass should default to ouputKeyClass\", FloatWritable.class, SequenceFileAsBinaryOutputFormat.getSequenceFileOutputKeyClass(job));\r\n    assertEquals(\"SequenceFileOutputValueClass should default to \" + \"ouputValueClass\", BooleanWritable.class, SequenceFileAsBinaryOutputFormat.getSequenceFileOutputValueClass(job));\r\n    SequenceFileAsBinaryOutputFormat.setSequenceFileOutputKeyClass(job, IntWritable.class);\r\n    SequenceFileAsBinaryOutputFormat.setSequenceFileOutputValueClass(job, DoubleWritable.class);\r\n    assertEquals(\"SequenceFileOutputKeyClass not updated\", IntWritable.class, SequenceFileAsBinaryOutputFormat.getSequenceFileOutputKeyClass(job));\r\n    assertEquals(\"SequenceFileOutputValueClass not updated\", DoubleWritable.class, SequenceFileAsBinaryOutputFormat.getSequenceFileOutputValueClass(job));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output",
  "methodName" : "testcheckOutputSpecsForbidRecordCompression",
  "errType" : [ "Exception", "InvalidJobConfException", "Exception" ],
  "containingMethodsNum" : 13,
  "sourceCodeText" : "void testcheckOutputSpecsForbidRecordCompression() throws IOException\n{\r\n    Job job = Job.getInstance();\r\n    FileSystem fs = FileSystem.getLocal(job.getConfiguration());\r\n    Path outputdir = new Path(System.getProperty(\"test.build.data\", \"/tmp\") + \"/output\");\r\n    fs.delete(outputdir, true);\r\n    FileOutputFormat.setOutputPath(job, outputdir);\r\n    SequenceFileAsBinaryOutputFormat.setCompressOutput(job, true);\r\n    SequenceFileAsBinaryOutputFormat.setOutputCompressionType(job, CompressionType.BLOCK);\r\n    try {\r\n        new SequenceFileAsBinaryOutputFormat().checkOutputSpecs(job);\r\n    } catch (Exception e) {\r\n        fail(\"Block compression should be allowed for \" + \"SequenceFileAsBinaryOutputFormat:Caught \" + e.getClass().getName());\r\n    }\r\n    SequenceFileAsBinaryOutputFormat.setOutputCompressionType(job, CompressionType.RECORD);\r\n    try {\r\n        new SequenceFileAsBinaryOutputFormat().checkOutputSpecs(job);\r\n        fail(\"Record compression should not be allowed for \" + \"SequenceFileAsBinaryOutputFormat\");\r\n    } catch (InvalidJobConfException ie) {\r\n    } catch (Exception e) {\r\n        fail(\"Expected \" + InvalidJobConfException.class.getName() + \"but caught \" + e.getClass().getName());\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 2,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\chain",
  "methodName" : "testChainSubmission",
  "errType" : [ "IllegalArgumentException", "IllegalArgumentException" ],
  "containingMethodsNum" : 9,
  "sourceCodeText" : "void testChainSubmission() throws Exception\n{\r\n    Configuration conf = createJobConf();\r\n    Job job = MapReduceTestUtil.createJob(conf, inDir, outDir, 0, 0, input);\r\n    job.setJobName(\"chain\");\r\n    Throwable th = null;\r\n    try {\r\n        ChainMapper.addMapper(job, Mapper.class, LongWritable.class, Text.class, IntWritable.class, Text.class, null);\r\n        ChainMapper.addMapper(job, Mapper.class, LongWritable.class, Text.class, LongWritable.class, Text.class, null);\r\n    } catch (IllegalArgumentException iae) {\r\n        th = iae;\r\n    }\r\n    assertTrue(th != null);\r\n    th = null;\r\n    try {\r\n        ChainReducer.setReducer(job, Reducer.class, LongWritable.class, Text.class, IntWritable.class, Text.class, null);\r\n        ChainMapper.addMapper(job, Mapper.class, LongWritable.class, Text.class, LongWritable.class, Text.class, null);\r\n    } catch (IllegalArgumentException iae) {\r\n        th = iae;\r\n    }\r\n    assertTrue(th != null);\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 2,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\chain",
  "methodName" : "testChainFail",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void testChainFail() throws Exception\n{\r\n    Configuration conf = createJobConf();\r\n    Job job = MapReduceTestUtil.createJob(conf, inDir, outDir, 1, 0, input);\r\n    job.setJobName(\"chain\");\r\n    ChainMapper.addMapper(job, Mapper.class, LongWritable.class, Text.class, LongWritable.class, Text.class, null);\r\n    ChainMapper.addMapper(job, FailMap.class, LongWritable.class, Text.class, IntWritable.class, Text.class, null);\r\n    ChainMapper.addMapper(job, Mapper.class, IntWritable.class, Text.class, LongWritable.class, Text.class, null);\r\n    job.waitForCompletion(true);\r\n    assertTrue(\"Job Not failed\", !job.isSuccessful());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\chain",
  "methodName" : "testReducerFail",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void testReducerFail() throws Exception\n{\r\n    Configuration conf = createJobConf();\r\n    Job job = MapReduceTestUtil.createJob(conf, inDir, outDir, 1, 1, input);\r\n    job.setJobName(\"chain\");\r\n    ChainMapper.addMapper(job, Mapper.class, LongWritable.class, Text.class, LongWritable.class, Text.class, null);\r\n    ChainReducer.setReducer(job, FailReduce.class, LongWritable.class, Text.class, LongWritable.class, Text.class, null);\r\n    ChainReducer.addMapper(job, Mapper.class, LongWritable.class, Text.class, LongWritable.class, Text.class, null);\r\n    job.waitForCompletion(true);\r\n    assertTrue(\"Job Not failed\", !job.isSuccessful());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\chain",
  "methodName" : "testChainMapNoOuptut",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void testChainMapNoOuptut() throws Exception\n{\r\n    Configuration conf = createJobConf();\r\n    String expectedOutput = \"\";\r\n    Job job = MapReduceTestUtil.createJob(conf, inDir, outDir, 1, 0, input);\r\n    job.setJobName(\"chain\");\r\n    ChainMapper.addMapper(job, ConsumeMap.class, IntWritable.class, Text.class, LongWritable.class, Text.class, null);\r\n    ChainMapper.addMapper(job, Mapper.class, LongWritable.class, Text.class, LongWritable.class, Text.class, null);\r\n    job.waitForCompletion(true);\r\n    assertTrue(\"Job failed\", job.isSuccessful());\r\n    assertEquals(\"Outputs doesn't match\", expectedOutput, MapReduceTestUtil.readOutput(outDir, conf));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\chain",
  "methodName" : "testChainReduceNoOuptut",
  "errType" : null,
  "containingMethodsNum" : 9,
  "sourceCodeText" : "void testChainReduceNoOuptut() throws Exception\n{\r\n    Configuration conf = createJobConf();\r\n    String expectedOutput = \"\";\r\n    Job job = MapReduceTestUtil.createJob(conf, inDir, outDir, 1, 1, input);\r\n    job.setJobName(\"chain\");\r\n    ChainMapper.addMapper(job, Mapper.class, IntWritable.class, Text.class, LongWritable.class, Text.class, null);\r\n    ChainReducer.setReducer(job, ConsumeReduce.class, LongWritable.class, Text.class, LongWritable.class, Text.class, null);\r\n    ChainReducer.addMapper(job, Mapper.class, LongWritable.class, Text.class, LongWritable.class, Text.class, null);\r\n    job.waitForCompletion(true);\r\n    assertTrue(\"Job failed\", job.isSuccessful());\r\n    assertEquals(\"Outputs doesn't match\", expectedOutput, MapReduceTestUtil.readOutput(outDir, conf));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2",
  "methodName" : "setup",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void setup() throws IOException\n{\r\n    if (!(new File(MiniMRYarnCluster.APPJAR)).exists()) {\r\n        LOG.info(\"MRAppJar \" + MiniMRYarnCluster.APPJAR + \" not found. Not running test.\");\r\n        return;\r\n    }\r\n    if (mrCluster == null) {\r\n        mrCluster = new MiniMRYarnCluster(TestSpeculativeExecution.class.getName(), 4);\r\n        Configuration conf = new Configuration();\r\n        mrCluster.init(conf);\r\n        mrCluster.start();\r\n    }\r\n    localFs.copyFromLocalFile(new Path(MiniMRYarnCluster.APPJAR), APP_JAR);\r\n    localFs.setPermission(APP_JAR, new FsPermission(\"700\"));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2",
  "methodName" : "tearDown",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void tearDown()\n{\r\n    if (mrCluster != null) {\r\n        mrCluster.stop();\r\n        mrCluster = null;\r\n    }\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2",
  "methodName" : "testSpeculativeExecution",
  "errType" : null,
  "containingMethodsNum" : 35,
  "sourceCodeText" : "void testSpeculativeExecution() throws Exception\n{\r\n    if (!(new File(MiniMRYarnCluster.APPJAR)).exists()) {\r\n        LOG.info(\"MRAppJar \" + MiniMRYarnCluster.APPJAR + \" not found. Not running test.\");\r\n        return;\r\n    }\r\n    Job job = runSpecTest(false, false);\r\n    boolean succeeded = job.waitForCompletion(true);\r\n    Assert.assertTrue(succeeded);\r\n    Assert.assertEquals(JobStatus.State.SUCCEEDED, job.getJobState());\r\n    Counters counters = job.getCounters();\r\n    Assert.assertEquals(2, counters.findCounter(JobCounter.TOTAL_LAUNCHED_MAPS).getValue());\r\n    Assert.assertEquals(2, counters.findCounter(JobCounter.TOTAL_LAUNCHED_REDUCES).getValue());\r\n    Assert.assertEquals(0, counters.findCounter(JobCounter.NUM_FAILED_MAPS).getValue());\r\n    job = runNonSpecFailOnceTest();\r\n    succeeded = job.waitForCompletion(true);\r\n    Assert.assertTrue(succeeded);\r\n    Assert.assertEquals(JobStatus.State.SUCCEEDED, job.getJobState());\r\n    counters = job.getCounters();\r\n    Assert.assertEquals(4, counters.findCounter(JobCounter.TOTAL_LAUNCHED_MAPS).getValue());\r\n    Assert.assertEquals(4, counters.findCounter(JobCounter.TOTAL_LAUNCHED_REDUCES).getValue());\r\n    Assert.assertEquals(0, counters.findCounter(JobCounter.NUM_KILLED_MAPS).getValue());\r\n    Assert.assertEquals(0, counters.findCounter(JobCounter.NUM_KILLED_REDUCES).getValue());\r\n    job = runSpecTest(true, false);\r\n    succeeded = job.waitForCompletion(true);\r\n    Assert.assertTrue(succeeded);\r\n    Assert.assertEquals(JobStatus.State.SUCCEEDED, job.getJobState());\r\n    counters = job.getCounters();\r\n    Assert.assertEquals(3, counters.findCounter(JobCounter.TOTAL_LAUNCHED_MAPS).getValue());\r\n    Assert.assertEquals(2, counters.findCounter(JobCounter.TOTAL_LAUNCHED_REDUCES).getValue());\r\n    Assert.assertEquals(0, counters.findCounter(JobCounter.NUM_FAILED_MAPS).getValue());\r\n    Assert.assertEquals(1, counters.findCounter(JobCounter.NUM_KILLED_MAPS).getValue());\r\n    job = runSpecTest(false, true);\r\n    succeeded = job.waitForCompletion(true);\r\n    Assert.assertTrue(succeeded);\r\n    Assert.assertEquals(JobStatus.State.SUCCEEDED, job.getJobState());\r\n    counters = job.getCounters();\r\n    Assert.assertEquals(2, counters.findCounter(JobCounter.TOTAL_LAUNCHED_MAPS).getValue());\r\n    Assert.assertEquals(3, counters.findCounter(JobCounter.TOTAL_LAUNCHED_REDUCES).getValue());\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2",
  "methodName" : "createTempFile",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "Path createTempFile(String filename, String contents) throws IOException\n{\r\n    Path path = new Path(TEST_ROOT_DIR, filename);\r\n    FSDataOutputStream os = localFs.create(path);\r\n    os.writeBytes(contents);\r\n    os.close();\r\n    localFs.setPermission(path, new FsPermission(\"700\"));\r\n    return path;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2",
  "methodName" : "runSpecTest",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 20,
  "sourceCodeText" : "Job runSpecTest(boolean mapspec, boolean redspec) throws IOException, ClassNotFoundException, InterruptedException\n{\r\n    Path first = createTempFile(\"specexec_map_input1\", \"a\\nz\");\r\n    Path secnd = createTempFile(\"specexec_map_input2\", \"a\\nz\");\r\n    Configuration conf = mrCluster.getConfig();\r\n    conf.setBoolean(MRJobConfig.MAP_SPECULATIVE, mapspec);\r\n    conf.setBoolean(MRJobConfig.REDUCE_SPECULATIVE, redspec);\r\n    conf.setClass(MRJobConfig.MR_AM_TASK_ESTIMATOR, TestSpecEstimator.class, TaskRuntimeEstimator.class);\r\n    Job job = Job.getInstance(conf);\r\n    job.setJarByClass(TestSpeculativeExecution.class);\r\n    job.setMapperClass(SpeculativeMapper.class);\r\n    job.setReducerClass(SpeculativeReducer.class);\r\n    job.setOutputKeyClass(Text.class);\r\n    job.setOutputValueClass(IntWritable.class);\r\n    job.setNumReduceTasks(2);\r\n    FileInputFormat.setInputPaths(job, first);\r\n    FileInputFormat.addInputPath(job, secnd);\r\n    FileOutputFormat.setOutputPath(job, TEST_OUT_DIR);\r\n    try {\r\n        localFs.delete(TEST_OUT_DIR, true);\r\n    } catch (IOException e) {\r\n    }\r\n    job.addFileToClassPath(APP_JAR);\r\n    job.setMaxMapAttempts(2);\r\n    job.submit();\r\n    return job;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2",
  "methodName" : "runNonSpecFailOnceTest",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 22,
  "sourceCodeText" : "Job runNonSpecFailOnceTest() throws IOException, ClassNotFoundException, InterruptedException\n{\r\n    Path first = createTempFile(\"specexec_map_input1\", \"a\\nz\");\r\n    Path secnd = createTempFile(\"specexec_map_input2\", \"a\\nz\");\r\n    Configuration conf = mrCluster.getConfig();\r\n    conf.setBoolean(MRJobConfig.MAP_SPECULATIVE, false);\r\n    conf.setBoolean(MRJobConfig.REDUCE_SPECULATIVE, false);\r\n    conf.setBoolean(MRJobConfig.MR_AM_JOB_NODE_BLACKLISTING_ENABLE, false);\r\n    conf.setInt(MRJobConfig.TASK_EXIT_TIMEOUT, 20);\r\n    conf.setInt(MRJobConfig.TASK_EXIT_TIMEOUT_CHECK_INTERVAL_MS, 10);\r\n    Job job = Job.getInstance(conf);\r\n    job.setJarByClass(TestSpeculativeExecution.class);\r\n    job.setMapperClass(FailOnceMapper.class);\r\n    job.setReducerClass(FailOnceReducer.class);\r\n    job.setOutputKeyClass(Text.class);\r\n    job.setOutputValueClass(IntWritable.class);\r\n    job.setNumReduceTasks(2);\r\n    FileInputFormat.setInputPaths(job, first);\r\n    FileInputFormat.addInputPath(job, secnd);\r\n    FileOutputFormat.setOutputPath(job, TEST_OUT_DIR);\r\n    try {\r\n        localFs.delete(TEST_OUT_DIR, true);\r\n    } catch (IOException e) {\r\n    }\r\n    job.addFileToClassPath(APP_JAR);\r\n    job.setMaxMapAttempts(2);\r\n    job.submit();\r\n    return job;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getConfig",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Configuration getConfig()\n{\r\n    return miniMRYarnCluster.getConfig();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "start",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void start()\n{\r\n    miniMRYarnCluster.start();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "stop",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void stop()\n{\r\n    miniMRYarnCluster.stop();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "restart",
  "errType" : null,
  "containingMethodsNum" : 10,
  "sourceCodeText" : "void restart()\n{\r\n    if (!miniMRYarnCluster.getServiceState().equals(STATE.STARTED)) {\r\n        LOG.warn(\"Cannot restart the mini cluster, start it first\");\r\n        return;\r\n    }\r\n    Configuration oldConf = new Configuration(getConfig());\r\n    String callerName = oldConf.get(\"minimrclientcluster.caller.name\", this.getClass().getName());\r\n    int noOfNMs = oldConf.getInt(\"minimrclientcluster.nodemanagers.number\", 1);\r\n    oldConf.setBoolean(YarnConfiguration.YARN_MINICLUSTER_FIXED_PORTS, true);\r\n    oldConf.setBoolean(JHAdminConfig.MR_HISTORY_MINICLUSTER_FIXED_PORTS, true);\r\n    stop();\r\n    miniMRYarnCluster = new MiniMRYarnCluster(callerName, noOfNMs);\r\n    miniMRYarnCluster.init(oldConf);\r\n    miniMRYarnCluster.start();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getMiniMRYarnCluster",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "MiniMRYarnCluster getMiniMRYarnCluster()\n{\r\n    return miniMRYarnCluster;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "testReduceFromDisk",
  "errType" : null,
  "containingMethodsNum" : 13,
  "sourceCodeText" : "void testReduceFromDisk() throws Exception\n{\r\n    final int MAP_TASKS = 8;\r\n    JobConf job = mrCluster.createJobConf();\r\n    job.set(JobContext.REDUCE_INPUT_BUFFER_PERCENT, \"0.0\");\r\n    job.setNumMapTasks(MAP_TASKS);\r\n    job.set(JobConf.MAPRED_REDUCE_TASK_JAVA_OPTS, \"-Xmx128m\");\r\n    job.setLong(JobContext.REDUCE_MEMORY_TOTAL_BYTES, 128 << 20);\r\n    job.set(JobContext.SHUFFLE_INPUT_BUFFER_PERCENT, \"0.05\");\r\n    job.setInt(JobContext.IO_SORT_FACTOR, 2);\r\n    job.setInt(JobContext.REDUCE_MERGE_INMEM_THRESHOLD, 4);\r\n    Counters c = runJob(job);\r\n    final long spill = c.findCounter(TaskCounter.SPILLED_RECORDS).getCounter();\r\n    final long out = c.findCounter(TaskCounter.MAP_OUTPUT_RECORDS).getCounter();\r\n    assertTrue(\"Expected all records spilled during reduce (\" + spill + \")\", spill >= 2 * out);\r\n    assertTrue(\"Expected intermediate merges (\" + spill + \")\", spill >= 2 * out + (out / MAP_TASKS));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "testReduceFromMem",
  "errType" : null,
  "containingMethodsNum" : 9,
  "sourceCodeText" : "void testReduceFromMem() throws Exception\n{\r\n    final int MAP_TASKS = 3;\r\n    JobConf job = mrCluster.createJobConf();\r\n    job.set(JobContext.REDUCE_INPUT_BUFFER_PERCENT, \"1.0\");\r\n    job.set(JobContext.SHUFFLE_INPUT_BUFFER_PERCENT, \"1.0\");\r\n    job.setLong(JobContext.REDUCE_MEMORY_TOTAL_BYTES, 128 << 20);\r\n    job.setNumMapTasks(MAP_TASKS);\r\n    Counters c = runJob(job);\r\n    final long spill = c.findCounter(TaskCounter.SPILLED_RECORDS).getCounter();\r\n    final long out = c.findCounter(TaskCounter.MAP_OUTPUT_RECORDS).getCounter();\r\n    assertEquals(\"Spilled records: \" + spill, out, spill);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "main",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void main(String[] args) throws Exception\n{\r\n    int res = ToolRunner.run(new Configuration(), new SleepJob(), args);\r\n    System.exit(res);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "createJob",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Job createJob(int numMapper, int numReducer, long mapSleepTime, int mapSleepCount, long reduceSleepTime, int reduceSleepCount) throws IOException\n{\r\n    return createJob(numMapper, numReducer, mapSleepTime, mapSleepCount, reduceSleepTime, reduceSleepCount, SLEEP_JOB_NAME);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "createJob",
  "errType" : null,
  "containingMethodsNum" : 21,
  "sourceCodeText" : "Job createJob(int numMapper, int numReducer, long mapSleepTime, int mapSleepCount, long reduceSleepTime, int reduceSleepCount, String name) throws IOException\n{\r\n    Configuration conf = getConf();\r\n    conf.setLong(MAP_SLEEP_TIME, mapSleepTime);\r\n    conf.setLong(REDUCE_SLEEP_TIME, reduceSleepTime);\r\n    conf.setInt(MAP_SLEEP_COUNT, mapSleepCount);\r\n    conf.setInt(REDUCE_SLEEP_COUNT, reduceSleepCount);\r\n    conf.setInt(MRJobConfig.NUM_MAPS, numMapper);\r\n    Job job = Job.getInstance(conf);\r\n    job.setNumReduceTasks(numReducer);\r\n    job.setJarByClass(SleepJob.class);\r\n    job.setMapperClass(SleepMapper.class);\r\n    job.setMapOutputKeyClass(IntWritable.class);\r\n    job.setMapOutputValueClass(NullWritable.class);\r\n    job.setReducerClass(SleepReducer.class);\r\n    job.setOutputFormatClass(NullOutputFormat.class);\r\n    job.setInputFormatClass(SleepInputFormat.class);\r\n    job.setPartitionerClass(SleepJobPartitioner.class);\r\n    job.setSpeculativeExecution(false);\r\n    if (SLEEP_JOB_NAME.equals(name)) {\r\n        job.setJobName(SLEEP_JOB_NAME);\r\n    } else {\r\n        job.setJobName(SLEEP_JOB_NAME + \" - \" + name);\r\n    }\r\n    FileInputFormat.addInputPath(job, new Path(\"ignored\"));\r\n    return job;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "run",
  "errType" : null,
  "containingMethodsNum" : 21,
  "sourceCodeText" : "int run(String[] args) throws Exception\n{\r\n    if (args.length < 1) {\r\n        return printUsage(\"number of arguments must be > 0\");\r\n    }\r\n    int numMapper = 1, numReducer = 1;\r\n    long mapSleepTime = 100, reduceSleepTime = 100, recSleepTime = 100;\r\n    int mapSleepCount = 1, reduceSleepCount = 1;\r\n    String name = SLEEP_JOB_NAME;\r\n    for (int i = 0; i < args.length; i++) {\r\n        if (args[i].equals(\"-m\")) {\r\n            numMapper = Integer.parseInt(args[++i]);\r\n            if (numMapper < 0) {\r\n                return printUsage(numMapper + \": numMapper must be >= 0\");\r\n            }\r\n        } else if (args[i].equals(\"-r\")) {\r\n            numReducer = Integer.parseInt(args[++i]);\r\n            if (numReducer < 0) {\r\n                return printUsage(numReducer + \": numReducer must be >= 0\");\r\n            }\r\n        } else if (args[i].equals(\"-mt\")) {\r\n            mapSleepTime = Long.parseLong(args[++i]);\r\n            if (mapSleepTime < 0) {\r\n                return printUsage(mapSleepTime + \": mapSleepTime must be >= 0\");\r\n            }\r\n        } else if (args[i].equals(\"-rt\")) {\r\n            reduceSleepTime = Long.parseLong(args[++i]);\r\n            if (reduceSleepTime < 0) {\r\n                return printUsage(reduceSleepTime + \": reduceSleepTime must be >= 0\");\r\n            }\r\n        } else if (args[i].equals(\"-recordt\")) {\r\n            recSleepTime = Long.parseLong(args[++i]);\r\n            if (recSleepTime < 0) {\r\n                return printUsage(recSleepTime + \": recordSleepTime must be >= 0\");\r\n            }\r\n        } else if (args[i].equals(\"-name\")) {\r\n            name = args[++i];\r\n        }\r\n    }\r\n    mapSleepCount = (int) Math.ceil(mapSleepTime / ((double) recSleepTime));\r\n    reduceSleepCount = (int) Math.ceil(reduceSleepTime / ((double) recSleepTime));\r\n    Job job = createJob(numMapper, numReducer, mapSleepTime, mapSleepCount, reduceSleepTime, reduceSleepCount, name);\r\n    return job.waitForCompletion(true) ? 0 : 1;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "printUsage",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "int printUsage(String error)\n{\r\n    if (error != null) {\r\n        System.err.println(\"ERROR: \" + error);\r\n    }\r\n    System.err.println(\"SleepJob [-m numMapper] [-r numReducer]\" + \" [-mt mapSleepTime (msec)] [-rt reduceSleepTime (msec)]\" + \" [-recordt recordSleepTime (msec)] [-name]\");\r\n    ToolRunner.printGenericCommandUsage(System.err);\r\n    return 2;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "setup",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void setup() throws IOException\n{\r\n    final Configuration conf = new Configuration();\r\n    final Path TEST_ROOT_DIR = new Path(System.getProperty(\"test.build.data\", \"/tmp\"));\r\n    testdir = new Path(TEST_ROOT_DIR, \"TestMiniMRClientCluster\");\r\n    inDir = new Path(testdir, \"in\");\r\n    outDir = new Path(testdir, \"out\");\r\n    FileSystem fs = FileSystem.getLocal(conf);\r\n    if (fs.exists(testdir) && !fs.delete(testdir, true)) {\r\n        throw new IOException(\"Could not delete \" + testdir);\r\n    }\r\n    if (!fs.mkdirs(inDir)) {\r\n        throw new IOException(\"Mkdirs failed to create \" + inDir);\r\n    }\r\n    for (int i = 0; i < inFiles.length; i++) {\r\n        inFiles[i] = new Path(inDir, \"part_\" + i);\r\n        createFile(inFiles[i], conf);\r\n    }\r\n    mrCluster = MiniMRClientClusterFactory.create(InternalClass.class, 1, new Configuration());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "cleanup",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void cleanup() throws IOException\n{\r\n    final Configuration conf = new Configuration();\r\n    final FileSystem fs = testdir.getFileSystem(conf);\r\n    if (fs.exists(testdir)) {\r\n        fs.delete(testdir, true);\r\n    }\r\n    mrCluster.stop();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "testRestart",
  "errType" : null,
  "containingMethodsNum" : 22,
  "sourceCodeText" : "void testRestart() throws Exception\n{\r\n    String rmAddress1 = mrCluster.getConfig().get(YarnConfiguration.RM_ADDRESS);\r\n    String rmAdminAddress1 = mrCluster.getConfig().get(YarnConfiguration.RM_ADMIN_ADDRESS);\r\n    String rmSchedAddress1 = mrCluster.getConfig().get(YarnConfiguration.RM_SCHEDULER_ADDRESS);\r\n    String rmRstrackerAddress1 = mrCluster.getConfig().get(YarnConfiguration.RM_RESOURCE_TRACKER_ADDRESS);\r\n    String rmWebAppAddress1 = mrCluster.getConfig().get(YarnConfiguration.RM_WEBAPP_ADDRESS);\r\n    String mrHistAddress1 = mrCluster.getConfig().get(JHAdminConfig.MR_HISTORY_ADDRESS);\r\n    String mrHistWebAppAddress1 = mrCluster.getConfig().get(JHAdminConfig.MR_HISTORY_WEBAPP_ADDRESS);\r\n    mrCluster.restart();\r\n    String rmAddress2 = mrCluster.getConfig().get(YarnConfiguration.RM_ADDRESS);\r\n    String rmAdminAddress2 = mrCluster.getConfig().get(YarnConfiguration.RM_ADMIN_ADDRESS);\r\n    String rmSchedAddress2 = mrCluster.getConfig().get(YarnConfiguration.RM_SCHEDULER_ADDRESS);\r\n    String rmRstrackerAddress2 = mrCluster.getConfig().get(YarnConfiguration.RM_RESOURCE_TRACKER_ADDRESS);\r\n    String rmWebAppAddress2 = mrCluster.getConfig().get(YarnConfiguration.RM_WEBAPP_ADDRESS);\r\n    String mrHistAddress2 = mrCluster.getConfig().get(JHAdminConfig.MR_HISTORY_ADDRESS);\r\n    String mrHistWebAppAddress2 = mrCluster.getConfig().get(JHAdminConfig.MR_HISTORY_WEBAPP_ADDRESS);\r\n    assertEquals(\"Address before restart: \" + rmAddress1 + \" is different from new address: \" + rmAddress2, rmAddress1, rmAddress2);\r\n    assertEquals(\"Address before restart: \" + rmAdminAddress1 + \" is different from new address: \" + rmAdminAddress2, rmAdminAddress1, rmAdminAddress2);\r\n    assertEquals(\"Address before restart: \" + rmSchedAddress1 + \" is different from new address: \" + rmSchedAddress2, rmSchedAddress1, rmSchedAddress2);\r\n    assertEquals(\"Address before restart: \" + rmRstrackerAddress1 + \" is different from new address: \" + rmRstrackerAddress2, rmRstrackerAddress1, rmRstrackerAddress2);\r\n    assertEquals(\"Address before restart: \" + rmWebAppAddress1 + \" is different from new address: \" + rmWebAppAddress2, rmWebAppAddress1, rmWebAppAddress2);\r\n    assertEquals(\"Address before restart: \" + mrHistAddress1 + \" is different from new address: \" + mrHistAddress2, mrHistAddress1, mrHistAddress2);\r\n    assertEquals(\"Address before restart: \" + mrHistWebAppAddress1 + \" is different from new address: \" + mrHistWebAppAddress2, mrHistWebAppAddress1, mrHistWebAppAddress2);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "testJob",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void testJob() throws Exception\n{\r\n    final Job job = createJob();\r\n    org.apache.hadoop.mapreduce.lib.input.FileInputFormat.setInputPaths(job, inDir);\r\n    org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setOutputPath(job, new Path(outDir, \"testJob\"));\r\n    assertTrue(job.waitForCompletion(true));\r\n    validateCounters(job.getCounters(), 5, 25, 5, 5);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "validateCounters",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void validateCounters(Counters counters, long mapInputRecords, long mapOutputRecords, long reduceInputGroups, long reduceOutputRecords)\n{\r\n    assertEquals(\"MapInputRecords\", mapInputRecords, counters.findCounter(\"MyCounterGroup\", \"MAP_INPUT_RECORDS\").getValue());\r\n    assertEquals(\"MapOutputRecords\", mapOutputRecords, counters.findCounter(\"MyCounterGroup\", \"MAP_OUTPUT_RECORDS\").getValue());\r\n    assertEquals(\"ReduceInputGroups\", reduceInputGroups, counters.findCounter(\"MyCounterGroup\", \"REDUCE_INPUT_GROUPS\").getValue());\r\n    assertEquals(\"ReduceOutputRecords\", reduceOutputRecords, counters.findCounter(\"MyCounterGroup\", \"REDUCE_OUTPUT_RECORDS\").getValue());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "createFile",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void createFile(Path inFile, Configuration conf) throws IOException\n{\r\n    final FileSystem fs = inFile.getFileSystem(conf);\r\n    if (fs.exists(inFile)) {\r\n        return;\r\n    }\r\n    FSDataOutputStream out = fs.create(inFile);\r\n    out.writeBytes(\"This is a test file\");\r\n    out.close();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "createJob",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "Job createJob() throws IOException\n{\r\n    final Job baseJob = Job.getInstance(mrCluster.getConfig());\r\n    baseJob.setOutputKeyClass(Text.class);\r\n    baseJob.setOutputValueClass(IntWritable.class);\r\n    baseJob.setMapperClass(MyMapper.class);\r\n    baseJob.setReducerClass(MyReducer.class);\r\n    baseJob.setNumReduceTasks(1);\r\n    return baseJob;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\input",
  "methodName" : "initMocks",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void initMocks()\n{\r\n    MockitoAnnotations.initMocks(this);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\input",
  "methodName" : "testRecordReaderInit",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void testRecordReaderInit() throws InterruptedException, IOException\n{\r\n    TaskAttemptID taskId = new TaskAttemptID(\"jt\", 0, TaskType.MAP, 0, 0);\r\n    Configuration conf1 = new Configuration();\r\n    conf1.set(DUMMY_KEY, \"STATE1\");\r\n    TaskAttemptContext context1 = new TaskAttemptContextImpl(conf1, taskId);\r\n    InputFormat inputFormat = new ChildRRInputFormat();\r\n    Path[] files = { new Path(\"file1\") };\r\n    long[] lengths = { 1 };\r\n    CombineFileSplit split = new CombineFileSplit(files, lengths);\r\n    RecordReader rr = inputFormat.createRecordReader(split, context1);\r\n    assertTrue(\"Unexpected RR type!\", rr instanceof CombineFileRecordReader);\r\n    assertEquals(\"Invalid initial dummy key value\", \"STATE1\", rr.getCurrentKey().toString());\r\n    Configuration conf2 = new Configuration();\r\n    conf2.set(DUMMY_KEY, \"STATE2\");\r\n    TaskAttemptContext context2 = new TaskAttemptContextImpl(conf2, taskId);\r\n    rr.initialize(split, context2);\r\n    assertEquals(\"Invalid secondary dummy key value\", \"STATE2\", rr.getCurrentKey().toString());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\input",
  "methodName" : "testReinit",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void testReinit() throws Exception\n{\r\n    TaskAttemptID taskId = new TaskAttemptID(\"jt\", 0, TaskType.MAP, 0, 0);\r\n    Configuration conf = new Configuration();\r\n    TaskAttemptContext context = new TaskAttemptContextImpl(conf, taskId);\r\n    InputFormat inputFormat = new ChildRRInputFormat();\r\n    Path[] files = { new Path(\"file1\"), new Path(\"file2\") };\r\n    long[] lengths = { 1, 1 };\r\n    CombineFileSplit split = new CombineFileSplit(files, lengths);\r\n    RecordReader rr = inputFormat.createRecordReader(split, context);\r\n    assertTrue(\"Unexpected RR type!\", rr instanceof CombineFileRecordReader);\r\n    rr.initialize(split, context);\r\n    assertTrue(rr.nextKeyValue());\r\n    assertEquals(\"file1\", rr.getCurrentValue().toString());\r\n    assertTrue(rr.nextKeyValue());\r\n    assertEquals(\"file2\", rr.getCurrentValue().toString());\r\n    assertFalse(rr.nextKeyValue());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\input",
  "methodName" : "testSplitPlacement",
  "errType" : null,
  "containingMethodsNum" : 342,
  "sourceCodeText" : "void testSplitPlacement() throws Exception\n{\r\n    MiniDFSCluster dfs = null;\r\n    FileSystem fileSys = null;\r\n    try {\r\n        Configuration conf = new Configuration();\r\n        conf.setBoolean(\"dfs.replication.considerLoad\", false);\r\n        dfs = new MiniDFSCluster.Builder(conf).racks(rack1).hosts(hosts1).build();\r\n        dfs.waitActive();\r\n        fileSys = dfs.getFileSystem();\r\n        if (!fileSys.mkdirs(inDir)) {\r\n            throw new IOException(\"Mkdirs failed to create \" + inDir.toString());\r\n        }\r\n        Path file1 = new Path(dir1 + \"/file1\");\r\n        writeFile(conf, file1, (short) 1, 1);\r\n        Path file5 = new Path(dir5 + \"/file5\");\r\n        writeFile(conf, file5, (short) 1, 1);\r\n        DummyInputFormat inFormat = new DummyInputFormat();\r\n        Job job = Job.getInstance(conf);\r\n        FileInputFormat.setInputPaths(job, dir1 + \",\" + dir5);\r\n        List<InputSplit> splits = inFormat.getSplits(job);\r\n        System.out.println(\"Made splits(Test0): \" + splits.size());\r\n        for (InputSplit split : splits) {\r\n            System.out.println(\"File split(Test0): \" + split);\r\n        }\r\n        assertEquals(1, splits.size());\r\n        CombineFileSplit fileSplit = (CombineFileSplit) splits.get(0);\r\n        assertEquals(2, fileSplit.getNumPaths());\r\n        assertEquals(1, fileSplit.getLocations().length);\r\n        assertEquals(file1.getName(), fileSplit.getPath(0).getName());\r\n        assertEquals(0, fileSplit.getOffset(0));\r\n        assertEquals(BLOCKSIZE, fileSplit.getLength(0));\r\n        assertEquals(file5.getName(), fileSplit.getPath(1).getName());\r\n        assertEquals(0, fileSplit.getOffset(1));\r\n        assertEquals(BLOCKSIZE, fileSplit.getLength(1));\r\n        assertEquals(hosts1[0], fileSplit.getLocations()[0]);\r\n        dfs.startDataNodes(conf, 1, true, null, rack2, hosts2, null);\r\n        dfs.waitActive();\r\n        Path file2 = new Path(dir2 + \"/file2\");\r\n        writeFile(conf, file2, (short) 2, 2);\r\n        inFormat = new DummyInputFormat();\r\n        FileInputFormat.setInputPaths(job, dir1 + \",\" + dir2);\r\n        inFormat.setMinSplitSizeRack(BLOCKSIZE);\r\n        splits = inFormat.getSplits(job);\r\n        System.out.println(\"Made splits(Test1): \" + splits.size());\r\n        for (InputSplit split : splits) {\r\n            System.out.println(\"File split(Test1): \" + split);\r\n        }\r\n        for (InputSplit split : splits) {\r\n            fileSplit = (CombineFileSplit) split;\r\n            if (splits.size() == 2) {\r\n                if (split.equals(splits.get(0))) {\r\n                    assertEquals(2, fileSplit.getNumPaths());\r\n                    assertEquals(1, fileSplit.getLocations().length);\r\n                    assertEquals(file2.getName(), fileSplit.getPath(0).getName());\r\n                    assertEquals(0, fileSplit.getOffset(0));\r\n                    assertEquals(BLOCKSIZE, fileSplit.getLength(0));\r\n                    assertEquals(file2.getName(), fileSplit.getPath(1).getName());\r\n                    assertEquals(BLOCKSIZE, fileSplit.getOffset(1));\r\n                    assertEquals(BLOCKSIZE, fileSplit.getLength(1));\r\n                    assertEquals(hosts2[0], fileSplit.getLocations()[0]);\r\n                }\r\n                if (split.equals(splits.get(1))) {\r\n                    assertEquals(1, fileSplit.getNumPaths());\r\n                    assertEquals(1, fileSplit.getLocations().length);\r\n                    assertEquals(file1.getName(), fileSplit.getPath(0).getName());\r\n                    assertEquals(0, fileSplit.getOffset(0));\r\n                    assertEquals(BLOCKSIZE, fileSplit.getLength(0));\r\n                    assertEquals(hosts1[0], fileSplit.getLocations()[0]);\r\n                }\r\n            } else if (splits.size() == 1) {\r\n                assertEquals(3, fileSplit.getNumPaths());\r\n                Set<Split> expected = new HashSet<>();\r\n                expected.add(new Split(file1.getName(), BLOCKSIZE, 0));\r\n                expected.add(new Split(file2.getName(), BLOCKSIZE, 0));\r\n                expected.add(new Split(file2.getName(), BLOCKSIZE, BLOCKSIZE));\r\n                List<Split> actual = new ArrayList<>();\r\n                for (int i = 0; i < 3; i++) {\r\n                    String name = fileSplit.getPath(i).getName();\r\n                    long length = fileSplit.getLength(i);\r\n                    long offset = fileSplit.getOffset(i);\r\n                    actual.add(new Split(name, length, offset));\r\n                }\r\n                assertTrue(actual.containsAll(expected));\r\n                assertEquals(1, fileSplit.getLocations().length);\r\n                assertEquals(hosts1[0], fileSplit.getLocations()[0]);\r\n            } else {\r\n                fail(\"Expected split size is 1 or 2, but actual size is \" + splits.size());\r\n            }\r\n        }\r\n        dfs.startDataNodes(conf, 1, true, null, rack3, hosts3, null);\r\n        dfs.waitActive();\r\n        Path file3 = new Path(dir3 + \"/file3\");\r\n        writeFile(conf, new Path(dir3 + \"/file3\"), (short) 3, 3);\r\n        inFormat = new DummyInputFormat();\r\n        FileInputFormat.setInputPaths(job, dir1 + \",\" + dir2 + \",\" + dir3);\r\n        inFormat.setMinSplitSizeRack(BLOCKSIZE);\r\n        splits = inFormat.getSplits(job);\r\n        for (InputSplit split : splits) {\r\n            System.out.println(\"File split(Test2): \" + split);\r\n        }\r\n        Set<Split> expected = new HashSet<>();\r\n        expected.add(new Split(file1.getName(), BLOCKSIZE, 0));\r\n        expected.add(new Split(file2.getName(), BLOCKSIZE, 0));\r\n        expected.add(new Split(file2.getName(), BLOCKSIZE, BLOCKSIZE));\r\n        expected.add(new Split(file3.getName(), BLOCKSIZE, 0));\r\n        expected.add(new Split(file3.getName(), BLOCKSIZE, BLOCKSIZE));\r\n        expected.add(new Split(file3.getName(), BLOCKSIZE, BLOCKSIZE * 2));\r\n        List<Split> actual = new ArrayList<>();\r\n        for (InputSplit split : splits) {\r\n            fileSplit = (CombineFileSplit) split;\r\n            if (splits.size() == 3) {\r\n                if (split.equals(splits.get(0))) {\r\n                    assertEquals(3, fileSplit.getNumPaths());\r\n                    assertEquals(1, fileSplit.getLocations().length);\r\n                    assertEquals(file3.getName(), fileSplit.getPath(0).getName());\r\n                    assertEquals(0, fileSplit.getOffset(0));\r\n                    assertEquals(BLOCKSIZE, fileSplit.getLength(0));\r\n                    assertEquals(file3.getName(), fileSplit.getPath(1).getName());\r\n                    assertEquals(BLOCKSIZE, fileSplit.getOffset(1));\r\n                    assertEquals(BLOCKSIZE, fileSplit.getLength(1));\r\n                    assertEquals(file3.getName(), fileSplit.getPath(2).getName());\r\n                    assertEquals(2 * BLOCKSIZE, fileSplit.getOffset(2));\r\n                    assertEquals(BLOCKSIZE, fileSplit.getLength(2));\r\n                    assertEquals(hosts3[0], fileSplit.getLocations()[0]);\r\n                }\r\n                if (split.equals(splits.get(1))) {\r\n                    assertEquals(2, fileSplit.getNumPaths());\r\n                    assertEquals(1, fileSplit.getLocations().length);\r\n                    assertEquals(file2.getName(), fileSplit.getPath(0).getName());\r\n                    assertEquals(0, fileSplit.getOffset(0));\r\n                    assertEquals(BLOCKSIZE, fileSplit.getLength(0));\r\n                    assertEquals(file2.getName(), fileSplit.getPath(1).getName());\r\n                    assertEquals(BLOCKSIZE, fileSplit.getOffset(1));\r\n                    assertEquals(BLOCKSIZE, fileSplit.getLength(1));\r\n                    assertEquals(hosts2[0], fileSplit.getLocations()[0]);\r\n                }\r\n                if (split.equals(splits.get(2))) {\r\n                    assertEquals(1, fileSplit.getNumPaths());\r\n                    assertEquals(1, fileSplit.getLocations().length);\r\n                    assertEquals(file1.getName(), fileSplit.getPath(0).getName());\r\n                    assertEquals(0, fileSplit.getOffset(0));\r\n                    assertEquals(BLOCKSIZE, fileSplit.getLength(0));\r\n                    assertEquals(hosts1[0], fileSplit.getLocations()[0]);\r\n                }\r\n            } else if (splits.size() == 2) {\r\n                if (split.equals(splits.get(0))) {\r\n                    assertEquals(1, fileSplit.getLocations().length);\r\n                    if (fileSplit.getLocations()[0].equals(hosts2[0])) {\r\n                        assertEquals(2, fileSplit.getNumPaths());\r\n                    } else if (fileSplit.getLocations()[0].equals(hosts3[0])) {\r\n                        assertEquals(3, fileSplit.getNumPaths());\r\n                    } else {\r\n                        fail(\"First split should be on rack2 or rack3.\");\r\n                    }\r\n                }\r\n                if (split.equals(splits.get(1))) {\r\n                    assertEquals(1, fileSplit.getLocations().length);\r\n                    assertEquals(hosts1[0], fileSplit.getLocations()[0]);\r\n                }\r\n            } else if (splits.size() == 1) {\r\n                assertEquals(1, fileSplit.getLocations().length);\r\n                assertEquals(6, fileSplit.getNumPaths());\r\n                assertEquals(hosts1[0], fileSplit.getLocations()[0]);\r\n            } else {\r\n                fail(\"Split size should be 1, 2, or 3.\");\r\n            }\r\n            for (int i = 0; i < fileSplit.getNumPaths(); i++) {\r\n                String name = fileSplit.getPath(i).getName();\r\n                long length = fileSplit.getLength(i);\r\n                long offset = fileSplit.getOffset(i);\r\n                actual.add(new Split(name, length, offset));\r\n            }\r\n        }\r\n        assertEquals(6, actual.size());\r\n        assertTrue(actual.containsAll(expected));\r\n        Path file4 = new Path(dir4 + \"/file4\");\r\n        writeFile(conf, file4, (short) 3, 3);\r\n        inFormat = new DummyInputFormat();\r\n        FileInputFormat.setInputPaths(job, dir1 + \",\" + dir2 + \",\" + dir3 + \",\" + dir4);\r\n        inFormat.setMinSplitSizeRack(BLOCKSIZE);\r\n        splits = inFormat.getSplits(job);\r\n        for (InputSplit split : splits) {\r\n            System.out.println(\"File split(Test3): \" + split);\r\n        }\r\n        expected.add(new Split(file4.getName(), BLOCKSIZE, 0));\r\n        expected.add(new Split(file4.getName(), BLOCKSIZE, BLOCKSIZE));\r\n        expected.add(new Split(file4.getName(), BLOCKSIZE, BLOCKSIZE * 2));\r\n        actual.clear();\r\n        for (InputSplit split : splits) {\r\n            fileSplit = (CombineFileSplit) split;\r\n            if (splits.size() == 3) {\r\n                if (split.equals(splits.get(0))) {\r\n                    assertEquals(6, fileSplit.getNumPaths());\r\n                    assertEquals(1, fileSplit.getLocations().length);\r\n                    assertEquals(hosts3[0], fileSplit.getLocations()[0]);\r\n                }\r\n                if (split.equals(splits.get(1))) {\r\n                    assertEquals(2, fileSplit.getNumPaths());\r\n                    assertEquals(1, fileSplit.getLocations().length);\r\n                    assertEquals(file2.getName(), fileSplit.getPath(0).getName());\r\n                    assertEquals(0, fileSplit.getOffset(0));\r\n                    assertEquals(BLOCKSIZE, fileSplit.getLength(0));\r\n                    assertEquals(file2.getName(), fileSplit.getPath(1).getName());\r\n                    assertEquals(BLOCKSIZE, fileSplit.getOffset(1));\r\n                    assertEquals(BLOCKSIZE, fileSplit.getLength(1));\r\n                    assertEquals(hosts2[0], fileSplit.getLocations()[0]);\r\n                }\r\n                if (split.equals(splits.get(2))) {\r\n                    assertEquals(1, fileSplit.getNumPaths());\r\n                    assertEquals(1, fileSplit.getLocations().length);\r\n                    assertEquals(file1.getName(), fileSplit.getPath(0).getName());\r\n                    assertEquals(0, fileSplit.getOffset(0));\r\n                    assertEquals(BLOCKSIZE, fileSplit.getLength(0));\r\n                    assertEquals(hosts1[0], fileSplit.getLocations()[0]);\r\n                }\r\n            } else if (splits.size() == 2) {\r\n                if (split.equals(splits.get(0))) {\r\n                    assertEquals(1, fileSplit.getLocations().length);\r\n                    if (fileSplit.getLocations()[0].equals(hosts2[0])) {\r\n                        assertEquals(5, fileSplit.getNumPaths());\r\n                    } else if (fileSplit.getLocations()[0].equals(hosts3[0])) {\r\n                        assertEquals(6, fileSplit.getNumPaths());\r\n                    } else {\r\n                        fail(\"First split should be on rack2 or rack3.\");\r\n                    }\r\n                }\r\n                if (split.equals(splits.get(1))) {\r\n                    assertEquals(1, fileSplit.getLocations().length);\r\n                    assertEquals(hosts1[0], fileSplit.getLocations()[0]);\r\n                }\r\n            } else if (splits.size() == 1) {\r\n                assertEquals(1, fileSplit.getLocations().length);\r\n                assertEquals(9, fileSplit.getNumPaths());\r\n                assertEquals(hosts1[0], fileSplit.getLocations()[0]);\r\n            } else {\r\n                fail(\"Split size should be 1, 2, or 3.\");\r\n            }\r\n            for (int i = 0; i < fileSplit.getNumPaths(); i++) {\r\n                String name = fileSplit.getPath(i).getName();\r\n                long length = fileSplit.getLength(i);\r\n                long offset = fileSplit.getOffset(i);\r\n                actual.add(new Split(name, length, offset));\r\n            }\r\n        }\r\n        assertEquals(9, actual.size());\r\n        assertTrue(actual.containsAll(expected));\r\n        inFormat = new DummyInputFormat();\r\n        inFormat.setMinSplitSizeNode(BLOCKSIZE);\r\n        inFormat.setMaxSplitSize(2 * BLOCKSIZE);\r\n        FileInputFormat.setInputPaths(job, dir1 + \",\" + dir2 + \",\" + dir3 + \",\" + dir4);\r\n        splits = inFormat.getSplits(job);\r\n        for (InputSplit split : splits) {\r\n            System.out.println(\"File split(Test4): \" + split);\r\n        }\r\n        assertEquals(5, splits.size());\r\n        actual.clear();\r\n        reset(mockList);\r\n        for (InputSplit split : splits) {\r\n            fileSplit = (CombineFileSplit) split;\r\n            for (int i = 0; i < fileSplit.getNumPaths(); i++) {\r\n                String name = fileSplit.getPath(i).getName();\r\n                long length = fileSplit.getLength(i);\r\n                long offset = fileSplit.getOffset(i);\r\n                actual.add(new Split(name, length, offset));\r\n            }\r\n            mockList.add(fileSplit.getLocations()[0]);\r\n        }\r\n        assertEquals(9, actual.size());\r\n        assertTrue(actual.containsAll(expected));\r\n        verify(mockList, atLeastOnce()).add(hosts1[0]);\r\n        verify(mockList, atLeastOnce()).add(hosts2[0]);\r\n        verify(mockList, atLeastOnce()).add(hosts3[0]);\r\n        inFormat = new DummyInputFormat();\r\n        inFormat.setMinSplitSizeNode(BLOCKSIZE);\r\n        inFormat.setMaxSplitSize(3 * BLOCKSIZE);\r\n        FileInputFormat.setInputPaths(job, dir1 + \",\" + dir2 + \",\" + dir3 + \",\" + dir4);\r\n        splits = inFormat.getSplits(job);\r\n        for (InputSplit split : splits) {\r\n            System.out.println(\"File split(Test5): \" + split);\r\n        }\r\n        assertEquals(3, splits.size());\r\n        actual.clear();\r\n        reset(mockList);\r\n        for (InputSplit split : splits) {\r\n            fileSplit = (CombineFileSplit) split;\r\n            for (int i = 0; i < fileSplit.getNumPaths(); i++) {\r\n                String name = fileSplit.getPath(i).getName();\r\n                long length = fileSplit.getLength(i);\r\n                long offset = fileSplit.getOffset(i);\r\n                actual.add(new Split(name, length, offset));\r\n            }\r\n            mockList.add(fileSplit.getLocations()[0]);\r\n        }\r\n        assertEquals(9, actual.size());\r\n        assertTrue(actual.containsAll(expected));\r\n        verify(mockList, atLeastOnce()).add(hosts1[0]);\r\n        verify(mockList, atLeastOnce()).add(hosts2[0]);\r\n        inFormat = new DummyInputFormat();\r\n        inFormat.setMaxSplitSize(4 * BLOCKSIZE);\r\n        FileInputFormat.setInputPaths(job, dir1 + \",\" + dir2 + \",\" + dir3 + \",\" + dir4);\r\n        splits = inFormat.getSplits(job);\r\n        for (InputSplit split : splits) {\r\n            System.out.println(\"File split(Test6): \" + split);\r\n        }\r\n        assertEquals(3, splits.size());\r\n        actual.clear();\r\n        reset(mockList);\r\n        for (InputSplit split : splits) {\r\n            fileSplit = (CombineFileSplit) split;\r\n            for (int i = 0; i < fileSplit.getNumPaths(); i++) {\r\n                String name = fileSplit.getPath(i).getName();\r\n                long length = fileSplit.getLength(i);\r\n                long offset = fileSplit.getOffset(i);\r\n                actual.add(new Split(name, length, offset));\r\n            }\r\n            mockList.add(fileSplit.getLocations()[0]);\r\n        }\r\n        assertEquals(9, actual.size());\r\n        assertTrue(actual.containsAll(expected));\r\n        verify(mockList, atLeastOnce()).add(hosts1[0]);\r\n        inFormat = new DummyInputFormat();\r\n        inFormat.setMaxSplitSize(7 * BLOCKSIZE);\r\n        inFormat.setMinSplitSizeNode(3 * BLOCKSIZE);\r\n        inFormat.setMinSplitSizeRack(3 * BLOCKSIZE);\r\n        FileInputFormat.setInputPaths(job, dir1 + \",\" + dir2 + \",\" + dir3 + \",\" + dir4);\r\n        splits = inFormat.getSplits(job);\r\n        for (InputSplit split : splits) {\r\n            System.out.println(\"File split(Test7): \" + split);\r\n        }\r\n        assertEquals(2, splits.size());\r\n        actual.clear();\r\n        reset(mockList);\r\n        for (InputSplit split : splits) {\r\n            fileSplit = (CombineFileSplit) split;\r\n            for (int i = 0; i < fileSplit.getNumPaths(); i++) {\r\n                String name = fileSplit.getPath(i).getName();\r\n                long length = fileSplit.getLength(i);\r\n                long offset = fileSplit.getOffset(i);\r\n                actual.add(new Split(name, length, offset));\r\n            }\r\n            mockList.add(fileSplit.getLocations()[0]);\r\n        }\r\n        assertEquals(9, actual.size());\r\n        assertTrue(actual.containsAll(expected));\r\n        verify(mockList, atLeastOnce()).add(hosts1[0]);\r\n        inFormat = new DummyInputFormat();\r\n        FileInputFormat.addInputPath(job, inDir);\r\n        inFormat.setMinSplitSizeRack(1);\r\n        inFormat.createPool(new TestFilter(dir1), new TestFilter(dir2));\r\n        splits = inFormat.getSplits(job);\r\n        for (InputSplit split : splits) {\r\n            System.out.println(\"File split(Test1): \" + split);\r\n        }\r\n        for (InputSplit split : splits) {\r\n            fileSplit = (CombineFileSplit) split;\r\n            if (splits.size() == 2) {\r\n                if (split.equals(splits.get(0))) {\r\n                    assertEquals(3, fileSplit.getNumPaths());\r\n                    expected.clear();\r\n                    expected.add(new Split(file1.getName(), BLOCKSIZE, 0));\r\n                    expected.add(new Split(file2.getName(), BLOCKSIZE, 0));\r\n                    expected.add(new Split(file2.getName(), BLOCKSIZE, BLOCKSIZE));\r\n                    actual.clear();\r\n                    for (int i = 0; i < 3; i++) {\r\n                        String name = fileSplit.getPath(i).getName();\r\n                        long length = fileSplit.getLength(i);\r\n                        long offset = fileSplit.getOffset(i);\r\n                        actual.add(new Split(name, length, offset));\r\n                    }\r\n                    assertTrue(actual.containsAll(expected));\r\n                    assertEquals(1, fileSplit.getLocations().length);\r\n                    assertEquals(hosts1[0], fileSplit.getLocations()[0]);\r\n                }\r\n                if (split.equals(splits.get(1))) {\r\n                    assertEquals(6, fileSplit.getNumPaths());\r\n                    expected.clear();\r\n                    expected.add(new Split(file3.getName(), BLOCKSIZE, 0));\r\n                    expected.add(new Split(file3.getName(), BLOCKSIZE, BLOCKSIZE));\r\n                    expected.add(new Split(file3.getName(), BLOCKSIZE, BLOCKSIZE * 2));\r\n                    expected.add(new Split(file4.getName(), BLOCKSIZE, 0));\r\n                    expected.add(new Split(file4.getName(), BLOCKSIZE, BLOCKSIZE));\r\n                    expected.add(new Split(file4.getName(), BLOCKSIZE, BLOCKSIZE * 2));\r\n                    actual.clear();\r\n                    for (int i = 0; i < 6; i++) {\r\n                        String name = fileSplit.getPath(i).getName();\r\n                        long length = fileSplit.getLength(i);\r\n                        long offset = fileSplit.getOffset(i);\r\n                        actual.add(new Split(name, length, offset));\r\n                    }\r\n                    assertTrue(actual.containsAll(expected));\r\n                    assertEquals(1, fileSplit.getLocations().length);\r\n                }\r\n            } else if (splits.size() == 3) {\r\n                if (split.equals(splits.get(0))) {\r\n                    assertEquals(2, fileSplit.getNumPaths());\r\n                    expected.clear();\r\n                    expected.add(new Split(file2.getName(), BLOCKSIZE, 0));\r\n                    expected.add(new Split(file2.getName(), BLOCKSIZE, BLOCKSIZE));\r\n                    actual.clear();\r\n                    for (int i = 0; i < 2; i++) {\r\n                        String name = fileSplit.getPath(i).getName();\r\n                        long length = fileSplit.getLength(i);\r\n                        long offset = fileSplit.getOffset(i);\r\n                        actual.add(new Split(name, length, offset));\r\n                    }\r\n                    assertTrue(actual.containsAll(expected));\r\n                    assertEquals(1, fileSplit.getLocations().length);\r\n                    assertEquals(hosts2[0], fileSplit.getLocations()[0]);\r\n                }\r\n                if (split.equals(splits.get(1))) {\r\n                    assertEquals(1, fileSplit.getNumPaths());\r\n                    assertEquals(file1.getName(), fileSplit.getPath(0).getName());\r\n                    assertEquals(BLOCKSIZE, fileSplit.getLength(0));\r\n                    assertEquals(0, fileSplit.getOffset(0));\r\n                    assertEquals(1, fileSplit.getLocations().length);\r\n                    assertEquals(hosts1[0], fileSplit.getLocations()[0]);\r\n                }\r\n                if (split.equals(splits.get(2))) {\r\n                    assertEquals(6, fileSplit.getNumPaths());\r\n                    expected.clear();\r\n                    expected.add(new Split(file3.getName(), BLOCKSIZE, 0));\r\n                    expected.add(new Split(file3.getName(), BLOCKSIZE, BLOCKSIZE));\r\n                    expected.add(new Split(file3.getName(), BLOCKSIZE, BLOCKSIZE * 2));\r\n                    expected.add(new Split(file4.getName(), BLOCKSIZE, 0));\r\n                    expected.add(new Split(file4.getName(), BLOCKSIZE, BLOCKSIZE));\r\n                    expected.add(new Split(file4.getName(), BLOCKSIZE, BLOCKSIZE * 2));\r\n                    actual.clear();\r\n                    for (int i = 0; i < 6; i++) {\r\n                        String name = fileSplit.getPath(i).getName();\r\n                        long length = fileSplit.getLength(i);\r\n                        long offset = fileSplit.getOffset(i);\r\n                        actual.add(new Split(name, length, offset));\r\n                    }\r\n                    assertTrue(actual.containsAll(expected));\r\n                    assertEquals(1, fileSplit.getLocations().length);\r\n                }\r\n            } else {\r\n                fail(\"Split size should be 2 or 3.\");\r\n            }\r\n        }\r\n        int numPools = 100;\r\n        int numFiles = 1000;\r\n        DummyInputFormat1 inFormat1 = new DummyInputFormat1();\r\n        for (int i = 0; i < numFiles; i++) {\r\n            FileInputFormat.setInputPaths(job, file1);\r\n        }\r\n        inFormat1.setMinSplitSizeRack(1);\r\n        final Path dirNoMatch1 = new Path(inDir, \"/dirxx\");\r\n        final Path dirNoMatch2 = new Path(inDir, \"/diryy\");\r\n        for (int i = 0; i < numPools; i++) {\r\n            inFormat1.createPool(new TestFilter(dirNoMatch1), new TestFilter(dirNoMatch2));\r\n        }\r\n        long start = System.currentTimeMillis();\r\n        splits = inFormat1.getSplits(job);\r\n        long end = System.currentTimeMillis();\r\n        System.out.println(\"Elapsed time for \" + numPools + \" pools \" + \" and \" + numFiles + \" files is \" + ((end - start) / 1000) + \" seconds.\");\r\n        inFormat = new DummyInputFormat();\r\n        inFormat.setMaxSplitSize(BLOCKSIZE / 2);\r\n        FileInputFormat.setInputPaths(job, dir3);\r\n        splits = inFormat.getSplits(job);\r\n        for (InputSplit split : splits) {\r\n            System.out.println(\"File split(Test8): \" + split);\r\n        }\r\n        assertEquals(splits.size(), 6);\r\n    } finally {\r\n        if (dfs != null) {\r\n            dfs.shutdown();\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\input",
  "methodName" : "writeFile",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void writeFile(Configuration conf, Path name, short replication, int numBlocks) throws IOException, TimeoutException, InterruptedException\n{\r\n    FileSystem fileSys = FileSystem.get(conf);\r\n    FSDataOutputStream stm = fileSys.create(name, true, conf.getInt(\"io.file.buffer.size\", 4096), replication, (long) BLOCKSIZE);\r\n    writeDataAndSetReplication(fileSys, name, stm, replication, numBlocks);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\input",
  "methodName" : "writeGzipFile",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "FileStatus writeGzipFile(Configuration conf, Path name, short replication, int numBlocks) throws IOException, TimeoutException, InterruptedException\n{\r\n    FileSystem fileSys = FileSystem.get(conf);\r\n    GZIPOutputStream out = new GZIPOutputStream(fileSys.create(name, true, conf.getInt(\"io.file.buffer.size\", 4096), replication, (long) BLOCKSIZE));\r\n    writeDataAndSetReplication(fileSys, name, out, replication, numBlocks);\r\n    return fileSys.getFileStatus(name);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\input",
  "methodName" : "writeDataAndSetReplication",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void writeDataAndSetReplication(FileSystem fileSys, Path name, OutputStream out, short replication, int numBlocks) throws IOException, TimeoutException, InterruptedException\n{\r\n    for (int i = 0; i < numBlocks; i++) {\r\n        out.write(databuf);\r\n    }\r\n    out.close();\r\n    DFSTestUtil.waitReplication(fileSys, name, replication);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\input",
  "methodName" : "testNodeDistribution",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void testNodeDistribution() throws IOException, InterruptedException\n{\r\n    DummyInputFormat inFormat = new DummyInputFormat();\r\n    int numBlocks = 60;\r\n    long totLength = 0;\r\n    long blockSize = 100;\r\n    int numNodes = 10;\r\n    long minSizeNode = 50;\r\n    long minSizeRack = 50;\r\n    int maxSplitSize = 200;\r\n    String[] locations = new String[numNodes];\r\n    for (int i = 0; i < numNodes; i++) {\r\n        locations[i] = \"h\" + i;\r\n    }\r\n    String[] racks = new String[0];\r\n    Path path = new Path(\"hdfs://file\");\r\n    OneBlockInfo[] blocks = new OneBlockInfo[numBlocks];\r\n    int hostCountBase = 0;\r\n    for (int i = 0; i < numBlocks; i++) {\r\n        int localHostCount = hostCountBase;\r\n        String[] blockHosts = new String[3];\r\n        for (int j = 0; j < 3; j++) {\r\n            int hostNum = localHostCount % numNodes;\r\n            blockHosts[j] = \"h\" + hostNum;\r\n            localHostCount++;\r\n        }\r\n        hostCountBase++;\r\n        blocks[i] = new OneBlockInfo(path, i * blockSize, blockSize, blockHosts, racks);\r\n        totLength += blockSize;\r\n    }\r\n    List<InputSplit> splits = new ArrayList<InputSplit>();\r\n    HashMap<String, Set<String>> rackToNodes = new HashMap<String, Set<String>>();\r\n    HashMap<String, List<OneBlockInfo>> rackToBlocks = new HashMap<String, List<OneBlockInfo>>();\r\n    HashMap<OneBlockInfo, String[]> blockToNodes = new HashMap<OneBlockInfo, String[]>();\r\n    Map<String, Set<OneBlockInfo>> nodeToBlocks = new TreeMap<String, Set<OneBlockInfo>>();\r\n    OneFileInfo.populateBlockInfo(blocks, rackToBlocks, blockToNodes, nodeToBlocks, rackToNodes);\r\n    inFormat.createSplits(nodeToBlocks, blockToNodes, rackToBlocks, totLength, maxSplitSize, minSizeNode, minSizeRack, splits);\r\n    int expectedSplitCount = (int) (totLength / maxSplitSize);\r\n    assertEquals(expectedSplitCount, splits.size());\r\n    int numLocalSplits = 0;\r\n    for (InputSplit inputSplit : splits) {\r\n        assertEquals(maxSplitSize, inputSplit.getLength());\r\n        if (inputSplit.getLocations().length == 1) {\r\n            numLocalSplits++;\r\n        }\r\n    }\r\n    assertTrue(numLocalSplits >= 0.9 * splits.size());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\input",
  "methodName" : "testNodeInputSplit",
  "errType" : null,
  "containingMethodsNum" : 10,
  "sourceCodeText" : "void testNodeInputSplit() throws IOException, InterruptedException\n{\r\n    DummyInputFormat inFormat = new DummyInputFormat();\r\n    int numBlocks = 12;\r\n    long totLength = 0;\r\n    long blockSize = 100;\r\n    long maxSize = 200;\r\n    long minSizeNode = 50;\r\n    long minSizeRack = 50;\r\n    String[] locations = { \"h1\", \"h2\" };\r\n    String[] racks = new String[0];\r\n    Path path = new Path(\"hdfs://file\");\r\n    OneBlockInfo[] blocks = new OneBlockInfo[numBlocks];\r\n    for (int i = 0; i < numBlocks; ++i) {\r\n        blocks[i] = new OneBlockInfo(path, i * blockSize, blockSize, locations, racks);\r\n        totLength += blockSize;\r\n    }\r\n    List<InputSplit> splits = new ArrayList<InputSplit>();\r\n    HashMap<String, Set<String>> rackToNodes = new HashMap<String, Set<String>>();\r\n    HashMap<String, List<OneBlockInfo>> rackToBlocks = new HashMap<String, List<OneBlockInfo>>();\r\n    HashMap<OneBlockInfo, String[]> blockToNodes = new HashMap<OneBlockInfo, String[]>();\r\n    HashMap<String, Set<OneBlockInfo>> nodeToBlocks = new HashMap<String, Set<OneBlockInfo>>();\r\n    OneFileInfo.populateBlockInfo(blocks, rackToBlocks, blockToNodes, nodeToBlocks, rackToNodes);\r\n    inFormat.createSplits(nodeToBlocks, blockToNodes, rackToBlocks, totLength, maxSize, minSizeNode, minSizeRack, splits);\r\n    int expectedSplitCount = (int) (totLength / maxSize);\r\n    assertEquals(expectedSplitCount, splits.size());\r\n    HashMultiset<String> nodeSplits = HashMultiset.create();\r\n    for (int i = 0; i < expectedSplitCount; ++i) {\r\n        InputSplit inSplit = splits.get(i);\r\n        assertEquals(maxSize, inSplit.getLength());\r\n        assertEquals(1, inSplit.getLocations().length);\r\n        nodeSplits.add(inSplit.getLocations()[0]);\r\n    }\r\n    assertEquals(3, nodeSplits.count(locations[0]));\r\n    assertEquals(3, nodeSplits.count(locations[1]));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\input",
  "methodName" : "testSplitPlacementForCompressedFiles",
  "errType" : null,
  "containingMethodsNum" : 283,
  "sourceCodeText" : "void testSplitPlacementForCompressedFiles() throws Exception\n{\r\n    MiniDFSCluster dfs = null;\r\n    FileSystem fileSys = null;\r\n    try {\r\n        Configuration conf = new Configuration();\r\n        conf.setBoolean(\"dfs.replication.considerLoad\", false);\r\n        dfs = new MiniDFSCluster.Builder(conf).racks(rack1).hosts(hosts1).build();\r\n        dfs.waitActive();\r\n        fileSys = dfs.getFileSystem();\r\n        if (!fileSys.mkdirs(inDir)) {\r\n            throw new IOException(\"Mkdirs failed to create \" + inDir.toString());\r\n        }\r\n        Path file1 = new Path(dir1 + \"/file1.gz\");\r\n        FileStatus f1 = writeGzipFile(conf, file1, (short) 1, 1);\r\n        Path file5 = new Path(dir5 + \"/file5.gz\");\r\n        FileStatus f5 = writeGzipFile(conf, file5, (short) 1, 1);\r\n        DummyInputFormat inFormat = new DummyInputFormat();\r\n        Job job = Job.getInstance(conf);\r\n        FileInputFormat.setInputPaths(job, dir1 + \",\" + dir5);\r\n        List<InputSplit> splits = inFormat.getSplits(job);\r\n        System.out.println(\"Made splits(Test0): \" + splits.size());\r\n        for (InputSplit split : splits) {\r\n            System.out.println(\"File split(Test0): \" + split);\r\n        }\r\n        assertEquals(1, splits.size());\r\n        CombineFileSplit fileSplit = (CombineFileSplit) splits.get(0);\r\n        assertEquals(2, fileSplit.getNumPaths());\r\n        assertEquals(1, fileSplit.getLocations().length);\r\n        assertEquals(file1.getName(), fileSplit.getPath(0).getName());\r\n        assertEquals(0, fileSplit.getOffset(0));\r\n        assertEquals(f1.getLen(), fileSplit.getLength(0));\r\n        assertEquals(file5.getName(), fileSplit.getPath(1).getName());\r\n        assertEquals(0, fileSplit.getOffset(1));\r\n        assertEquals(f5.getLen(), fileSplit.getLength(1));\r\n        assertEquals(hosts1[0], fileSplit.getLocations()[0]);\r\n        dfs.startDataNodes(conf, 1, true, null, rack2, hosts2, null);\r\n        dfs.waitActive();\r\n        Path file2 = new Path(dir2 + \"/file2.gz\");\r\n        FileStatus f2 = writeGzipFile(conf, file2, (short) 2, 2);\r\n        inFormat = new DummyInputFormat();\r\n        FileInputFormat.setInputPaths(job, dir1 + \",\" + dir2);\r\n        inFormat.setMinSplitSizeRack(f1.getLen());\r\n        splits = inFormat.getSplits(job);\r\n        System.out.println(\"Made splits(Test1): \" + splits.size());\r\n        for (InputSplit split : splits) {\r\n            System.out.println(\"File split(Test1): \" + split);\r\n        }\r\n        Set<Split> expected = new HashSet<>();\r\n        expected.add(new Split(file1.getName(), f1.getLen(), 0));\r\n        expected.add(new Split(file2.getName(), f2.getLen(), 0));\r\n        List<Split> actual = new ArrayList<>();\r\n        for (InputSplit split : splits) {\r\n            fileSplit = (CombineFileSplit) split;\r\n            if (splits.size() == 2) {\r\n                if (split.equals(splits.get(0))) {\r\n                    assertEquals(1, fileSplit.getNumPaths());\r\n                    assertEquals(1, fileSplit.getLocations().length);\r\n                    assertEquals(file2.getName(), fileSplit.getPath(0).getName());\r\n                    assertEquals(0, fileSplit.getOffset(0));\r\n                    assertEquals(f2.getLen(), fileSplit.getLength(0));\r\n                    assertEquals(hosts2[0], fileSplit.getLocations()[0]);\r\n                }\r\n                if (split.equals(splits.get(1))) {\r\n                    assertEquals(1, fileSplit.getNumPaths());\r\n                    assertEquals(1, fileSplit.getLocations().length);\r\n                    assertEquals(file1.getName(), fileSplit.getPath(0).getName());\r\n                    assertEquals(0, fileSplit.getOffset(0));\r\n                    assertEquals(f1.getLen(), fileSplit.getLength(0));\r\n                    assertEquals(hosts1[0], fileSplit.getLocations()[0]);\r\n                }\r\n            } else if (splits.size() == 1) {\r\n                assertEquals(2, fileSplit.getNumPaths());\r\n                assertEquals(1, fileSplit.getLocations().length);\r\n                assertEquals(hosts1[0], fileSplit.getLocations()[0]);\r\n            } else {\r\n                fail(\"Split size should be 1 or 2.\");\r\n            }\r\n            for (int i = 0; i < fileSplit.getNumPaths(); i++) {\r\n                String name = fileSplit.getPath(i).getName();\r\n                long length = fileSplit.getLength(i);\r\n                long offset = fileSplit.getOffset(i);\r\n                actual.add(new Split(name, length, offset));\r\n            }\r\n        }\r\n        assertEquals(2, actual.size());\r\n        assertTrue(actual.containsAll(expected));\r\n        dfs.startDataNodes(conf, 1, true, null, rack3, hosts3, null);\r\n        dfs.waitActive();\r\n        Path file3 = new Path(dir3 + \"/file3.gz\");\r\n        FileStatus f3 = writeGzipFile(conf, file3, (short) 3, 3);\r\n        inFormat = new DummyInputFormat();\r\n        FileInputFormat.setInputPaths(job, dir1 + \",\" + dir2 + \",\" + dir3);\r\n        inFormat.setMinSplitSizeRack(f1.getLen());\r\n        splits = inFormat.getSplits(job);\r\n        for (InputSplit split : splits) {\r\n            System.out.println(\"File split(Test2): \" + split);\r\n        }\r\n        expected.add(new Split(file3.getName(), f3.getLen(), 0));\r\n        actual.clear();\r\n        for (InputSplit split : splits) {\r\n            fileSplit = (CombineFileSplit) split;\r\n            if (splits.size() == 3) {\r\n                if (split.equals(splits.get(0))) {\r\n                    assertEquals(1, fileSplit.getNumPaths());\r\n                    assertEquals(file3.getName(), fileSplit.getPath(0).getName());\r\n                    assertEquals(f3.getLen(), fileSplit.getLength(0));\r\n                    assertEquals(0, fileSplit.getOffset(0));\r\n                    assertEquals(1, fileSplit.getLocations().length);\r\n                    assertEquals(hosts3[0], fileSplit.getLocations()[0]);\r\n                }\r\n                if (split.equals(splits.get(1))) {\r\n                    assertEquals(1, fileSplit.getNumPaths());\r\n                    assertEquals(file2.getName(), fileSplit.getPath(0).getName());\r\n                    assertEquals(f2.getLen(), fileSplit.getLength(0));\r\n                    assertEquals(0, fileSplit.getOffset(0));\r\n                    assertEquals(1, fileSplit.getLocations().length);\r\n                    assertEquals(hosts2[0], fileSplit.getLocations()[0]);\r\n                }\r\n                if (split.equals(splits.get(2))) {\r\n                    assertEquals(1, fileSplit.getNumPaths());\r\n                    assertEquals(file1.getName(), fileSplit.getPath(0).getName());\r\n                    assertEquals(f1.getLen(), fileSplit.getLength(0));\r\n                    assertEquals(0, fileSplit.getOffset(0));\r\n                    assertEquals(1, fileSplit.getLocations().length);\r\n                    assertEquals(hosts1[0], fileSplit.getLocations()[0]);\r\n                }\r\n            } else if (splits.size() == 2) {\r\n                if (split.equals(splits.get(0))) {\r\n                    assertEquals(1, fileSplit.getLocations().length);\r\n                    if (fileSplit.getLocations()[0].equals(hosts2[0])) {\r\n                        assertEquals(2, fileSplit.getNumPaths());\r\n                    } else if (fileSplit.getLocations()[0].equals(hosts3[0])) {\r\n                        assertEquals(1, fileSplit.getNumPaths());\r\n                    } else {\r\n                        fail(\"First split should be on rack2 or rack3.\");\r\n                    }\r\n                }\r\n                if (split.equals(splits.get(1))) {\r\n                    assertEquals(1, fileSplit.getLocations().length);\r\n                    assertEquals(hosts1[0], fileSplit.getLocations()[0]);\r\n                }\r\n            } else if (splits.size() == 1) {\r\n                assertEquals(1, fileSplit.getLocations().length);\r\n                assertEquals(3, fileSplit.getNumPaths());\r\n                assertEquals(hosts1[0], fileSplit.getLocations()[0]);\r\n            } else {\r\n                fail(\"Split size should be 1, 2, or 3.\");\r\n            }\r\n            for (int i = 0; i < fileSplit.getNumPaths(); i++) {\r\n                String name = fileSplit.getPath(i).getName();\r\n                long length = fileSplit.getLength(i);\r\n                long offset = fileSplit.getOffset(i);\r\n                actual.add(new Split(name, length, offset));\r\n            }\r\n        }\r\n        assertEquals(3, actual.size());\r\n        assertTrue(actual.containsAll(expected));\r\n        Path file4 = new Path(dir4 + \"/file4.gz\");\r\n        FileStatus f4 = writeGzipFile(conf, file4, (short) 3, 3);\r\n        inFormat = new DummyInputFormat();\r\n        FileInputFormat.setInputPaths(job, dir1 + \",\" + dir2 + \",\" + dir3 + \",\" + dir4);\r\n        inFormat.setMinSplitSizeRack(f1.getLen());\r\n        splits = inFormat.getSplits(job);\r\n        for (InputSplit split : splits) {\r\n            System.out.println(\"File split(Test3): \" + split);\r\n        }\r\n        expected.add(new Split(file3.getName(), f3.getLen(), 0));\r\n        actual.clear();\r\n        for (InputSplit split : splits) {\r\n            fileSplit = (CombineFileSplit) split;\r\n            if (splits.size() == 3) {\r\n                if (split.equals(splits.get(0))) {\r\n                    assertEquals(2, fileSplit.getNumPaths());\r\n                    assertEquals(hosts3[0], fileSplit.getLocations()[0]);\r\n                }\r\n                if (split.equals(splits.get(1))) {\r\n                    assertEquals(1, fileSplit.getNumPaths());\r\n                    assertEquals(file2.getName(), fileSplit.getPath(0).getName());\r\n                    assertEquals(f2.getLen(), fileSplit.getLength(0));\r\n                    assertEquals(0, fileSplit.getOffset(0));\r\n                    assertEquals(1, fileSplit.getLocations().length);\r\n                    assertEquals(hosts2[0], fileSplit.getLocations()[0]);\r\n                }\r\n                if (split.equals(splits.get(2))) {\r\n                    assertEquals(1, fileSplit.getNumPaths());\r\n                    assertEquals(file1.getName(), fileSplit.getPath(0).getName());\r\n                    assertEquals(f1.getLen(), fileSplit.getLength(0));\r\n                    assertEquals(0, fileSplit.getOffset(0));\r\n                    assertEquals(1, fileSplit.getLocations().length);\r\n                    assertEquals(hosts1[0], fileSplit.getLocations()[0]);\r\n                }\r\n            } else if (splits.size() == 2) {\r\n                if (split.equals(splits.get(0))) {\r\n                    assertEquals(1, fileSplit.getLocations().length);\r\n                    if (fileSplit.getLocations()[0].equals(hosts2[0])) {\r\n                        assertEquals(3, fileSplit.getNumPaths());\r\n                    } else if (fileSplit.getLocations()[0].equals(hosts3[0])) {\r\n                        assertEquals(2, fileSplit.getNumPaths());\r\n                    } else {\r\n                        fail(\"First split should be on rack2 or rack3.\");\r\n                    }\r\n                }\r\n                if (split.equals(splits.get(1))) {\r\n                    assertEquals(1, fileSplit.getLocations().length);\r\n                    assertEquals(hosts1[0], fileSplit.getLocations()[0]);\r\n                }\r\n            } else if (splits.size() == 1) {\r\n                assertEquals(1, fileSplit.getLocations().length);\r\n                assertEquals(4, fileSplit.getNumPaths());\r\n                assertEquals(hosts1[0], fileSplit.getLocations()[0]);\r\n            } else {\r\n                fail(\"Split size should be 1, 2, or 3.\");\r\n            }\r\n            for (int i = 0; i < fileSplit.getNumPaths(); i++) {\r\n                String name = fileSplit.getPath(i).getName();\r\n                long length = fileSplit.getLength(i);\r\n                long offset = fileSplit.getOffset(i);\r\n                actual.add(new Split(name, length, offset));\r\n            }\r\n        }\r\n        assertEquals(4, actual.size());\r\n        assertTrue(actual.containsAll(expected));\r\n        inFormat = new DummyInputFormat();\r\n        inFormat.setMinSplitSizeNode(f1.getLen());\r\n        inFormat.setMaxSplitSize(f1.getLen());\r\n        FileInputFormat.setInputPaths(job, dir1 + \",\" + dir2 + \",\" + dir3 + \",\" + dir4);\r\n        splits = inFormat.getSplits(job);\r\n        for (InputSplit split : splits) {\r\n            System.out.println(\"File split(Test4): \" + split);\r\n        }\r\n        assertEquals(4, splits.size());\r\n        actual.clear();\r\n        for (InputSplit split : splits) {\r\n            fileSplit = (CombineFileSplit) split;\r\n            for (int i = 0; i < fileSplit.getNumPaths(); i++) {\r\n                String name = fileSplit.getPath(i).getName();\r\n                long length = fileSplit.getLength(i);\r\n                long offset = fileSplit.getOffset(i);\r\n                actual.add(new Split(name, length, offset));\r\n            }\r\n            mockList.add(fileSplit.getLocations()[0]);\r\n        }\r\n        assertEquals(4, actual.size());\r\n        assertTrue(actual.containsAll(expected));\r\n        verify(mockList, atLeastOnce()).add(hosts1[0]);\r\n        verify(mockList, atLeastOnce()).add(hosts2[0]);\r\n        verify(mockList, atLeastOnce()).add(hosts3[0]);\r\n        inFormat = new DummyInputFormat();\r\n        inFormat.setMinSplitSizeNode(f1.getLen());\r\n        inFormat.setMaxSplitSize(2 * f1.getLen());\r\n        FileInputFormat.setInputPaths(job, dir1 + \",\" + dir2 + \",\" + dir3 + \",\" + dir4);\r\n        splits = inFormat.getSplits(job);\r\n        for (InputSplit split : splits) {\r\n            System.out.println(\"File split(Test5): \" + split);\r\n        }\r\n        actual.clear();\r\n        reset(mockList);\r\n        for (InputSplit split : splits) {\r\n            fileSplit = (CombineFileSplit) split;\r\n            for (int i = 0; i < fileSplit.getNumPaths(); i++) {\r\n                String name = fileSplit.getPath(i).getName();\r\n                long length = fileSplit.getLength(i);\r\n                long offset = fileSplit.getOffset(i);\r\n                actual.add(new Split(name, length, offset));\r\n            }\r\n            mockList.add(fileSplit.getLocations()[0]);\r\n        }\r\n        assertEquals(4, actual.size());\r\n        assertTrue(actual.containsAll(expected));\r\n        if (splits.size() == 3) {\r\n            verify(mockList, times(1)).add(hosts1[0]);\r\n            verify(mockList, times(1)).add(hosts2[0]);\r\n            verify(mockList, times(1)).add(hosts3[0]);\r\n        } else if (splits.size() == 2) {\r\n            verify(mockList, times(1)).add(hosts1[0]);\r\n        } else {\r\n            fail(\"Split size should be 2 or 3.\");\r\n        }\r\n        inFormat = new DummyInputFormat();\r\n        inFormat.setMinSplitSizeNode(2 * f1.getLen());\r\n        inFormat.setMaxSplitSize(4 * f1.getLen());\r\n        FileInputFormat.setInputPaths(job, dir1 + \",\" + dir2 + \",\" + dir3 + \",\" + dir4);\r\n        splits = inFormat.getSplits(job);\r\n        for (InputSplit split : splits) {\r\n            System.out.println(\"File split(Test6): \" + split);\r\n        }\r\n        assertTrue(\"Split size should be 1 or 2.\", splits.size() == 1 || splits.size() == 2);\r\n        actual.clear();\r\n        reset(mockList);\r\n        for (InputSplit split : splits) {\r\n            fileSplit = (CombineFileSplit) split;\r\n            for (int i = 0; i < fileSplit.getNumPaths(); i++) {\r\n                String name = fileSplit.getPath(i).getName();\r\n                long length = fileSplit.getLength(i);\r\n                long offset = fileSplit.getOffset(i);\r\n                actual.add(new Split(name, length, offset));\r\n            }\r\n            mockList.add(fileSplit.getLocations()[0]);\r\n        }\r\n        assertEquals(4, actual.size());\r\n        assertTrue(actual.containsAll(expected));\r\n        verify(mockList, times(1)).add(hosts1[0]);\r\n        inFormat = new DummyInputFormat();\r\n        inFormat.setMaxSplitSize(4 * f1.getLen());\r\n        inFormat.setMinSplitSizeRack(4 * f1.getLen());\r\n        FileInputFormat.setInputPaths(job, dir1 + \",\" + dir2 + \",\" + dir3 + \",\" + dir4);\r\n        splits = inFormat.getSplits(job);\r\n        for (InputSplit split : splits) {\r\n            System.out.println(\"File split(Test7): \" + split);\r\n        }\r\n        assertEquals(1, splits.size());\r\n        fileSplit = (CombineFileSplit) splits.get(0);\r\n        assertEquals(4, fileSplit.getNumPaths());\r\n        assertEquals(1, fileSplit.getLocations().length);\r\n        assertEquals(hosts1[0], fileSplit.getLocations()[0]);\r\n        inFormat = new DummyInputFormat();\r\n        inFormat.setMinSplitSizeNode(4 * f1.getLen());\r\n        FileInputFormat.setInputPaths(job, dir1 + \",\" + dir2 + \",\" + dir3 + \",\" + dir4);\r\n        splits = inFormat.getSplits(job);\r\n        for (InputSplit split : splits) {\r\n            System.out.println(\"File split(Test8): \" + split);\r\n        }\r\n        assertEquals(1, splits.size());\r\n        fileSplit = (CombineFileSplit) splits.get(0);\r\n        assertEquals(4, fileSplit.getNumPaths());\r\n        assertEquals(1, fileSplit.getLocations().length);\r\n        assertEquals(hosts1[0], fileSplit.getLocations()[0]);\r\n        inFormat = new DummyInputFormat();\r\n        FileInputFormat.addInputPath(job, inDir);\r\n        inFormat.setMinSplitSizeRack(1);\r\n        inFormat.createPool(new TestFilter(dir1), new TestFilter(dir2));\r\n        splits = inFormat.getSplits(job);\r\n        for (InputSplit split : splits) {\r\n            System.out.println(\"File split(Test9): \" + split);\r\n        }\r\n        actual.clear();\r\n        for (InputSplit split : splits) {\r\n            fileSplit = (CombineFileSplit) split;\r\n            if (splits.size() == 3) {\r\n                if (split.equals(splits.get(0))) {\r\n                    assertEquals(1, fileSplit.getNumPaths());\r\n                    assertEquals(1, fileSplit.getLocations().length);\r\n                    assertEquals(hosts2[0], fileSplit.getLocations()[0]);\r\n                }\r\n                if (split.equals(splits.get(1))) {\r\n                    assertEquals(1, fileSplit.getNumPaths());\r\n                    assertEquals(1, fileSplit.getLocations().length);\r\n                    assertEquals(hosts1[0], fileSplit.getLocations()[0]);\r\n                }\r\n                if (split.equals(splits.get(2))) {\r\n                    assertEquals(2, fileSplit.getNumPaths());\r\n                    assertEquals(1, fileSplit.getLocations().length);\r\n                    assertEquals(hosts3[0], fileSplit.getLocations()[0]);\r\n                }\r\n            } else if (splits.size() == 2) {\r\n                if (split.equals(splits.get(0))) {\r\n                    assertEquals(2, fileSplit.getNumPaths());\r\n                    assertEquals(1, fileSplit.getLocations().length);\r\n                    assertEquals(hosts1[0], fileSplit.getLocations()[0]);\r\n                }\r\n                if (split.equals(splits.get(1))) {\r\n                    assertEquals(2, fileSplit.getNumPaths());\r\n                    assertEquals(1, fileSplit.getLocations().length);\r\n                    assertEquals(hosts3[0], fileSplit.getLocations()[0]);\r\n                }\r\n            } else {\r\n                fail(\"Split size should be 2 or 3.\");\r\n            }\r\n            for (int i = 0; i < fileSplit.getNumPaths(); i++) {\r\n                String name = fileSplit.getPath(i).getName();\r\n                long length = fileSplit.getLength(i);\r\n                long offset = fileSplit.getOffset(i);\r\n                actual.add(new Split(name, length, offset));\r\n            }\r\n        }\r\n        assertEquals(4, actual.size());\r\n        assertTrue(actual.containsAll(expected));\r\n        int numPools = 100;\r\n        int numFiles = 1000;\r\n        DummyInputFormat1 inFormat1 = new DummyInputFormat1();\r\n        for (int i = 0; i < numFiles; i++) {\r\n            FileInputFormat.setInputPaths(job, file1);\r\n        }\r\n        inFormat1.setMinSplitSizeRack(1);\r\n        final Path dirNoMatch1 = new Path(inDir, \"/dirxx\");\r\n        final Path dirNoMatch2 = new Path(inDir, \"/diryy\");\r\n        for (int i = 0; i < numPools; i++) {\r\n            inFormat1.createPool(new TestFilter(dirNoMatch1), new TestFilter(dirNoMatch2));\r\n        }\r\n        long start = System.currentTimeMillis();\r\n        splits = inFormat1.getSplits(job);\r\n        long end = System.currentTimeMillis();\r\n        System.out.println(\"Elapsed time for \" + numPools + \" pools \" + \" and \" + numFiles + \" files is \" + ((end - start)) + \" milli seconds.\");\r\n    } finally {\r\n        if (dfs != null) {\r\n            dfs.shutdown();\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\input",
  "methodName" : "testMissingBlocks",
  "errType" : null,
  "containingMethodsNum" : 26,
  "sourceCodeText" : "void testMissingBlocks() throws Exception\n{\r\n    final Configuration conf = new Configuration();\r\n    conf.set(\"fs.hdfs.impl\", MissingBlockFileSystem.class.getName());\r\n    conf.setBoolean(\"dfs.replication.considerLoad\", false);\r\n    try (MiniDFSCluster dfs = new MiniDFSCluster.Builder(conf).racks(rack1).hosts(hosts1).build()) {\r\n        dfs.waitActive();\r\n        final FileSystem fileSys = MissingBlockFileSystem.newInstance(dfs.getURI(), conf);\r\n        if (!fileSys.mkdirs(inDir)) {\r\n            throw new IOException(\"Mkdirs failed to create \" + inDir.toString());\r\n        }\r\n        Path file1 = new Path(dir1 + \"/file1\");\r\n        writeFile(conf, file1, (short) 1, 1);\r\n        Path file5 = new Path(dir5 + \"/file5\");\r\n        writeFile(conf, file5, (short) 1, 1);\r\n        ((MissingBlockFileSystem) fileSys).setFileWithMissingBlocks(file1.toUri().getPath());\r\n        DummyInputFormat inFormat = new DummyInputFormat();\r\n        Job job = Job.getInstance(conf);\r\n        FileInputFormat.setInputPaths(job, dir1 + \",\" + dir5);\r\n        List<InputSplit> splits = inFormat.getSplits(job);\r\n        System.out.println(\"Made splits(Test0): \" + splits.size());\r\n        for (InputSplit split : splits) {\r\n            System.out.println(\"File split(Test0): \" + split);\r\n        }\r\n        assertThat(splits.size()).isEqualTo(1);\r\n        CombineFileSplit fileSplit = (CombineFileSplit) splits.get(0);\r\n        assertEquals(2, fileSplit.getNumPaths());\r\n        assertEquals(1, fileSplit.getLocations().length);\r\n        assertEquals(file1.getName(), fileSplit.getPath(0).getName());\r\n        assertEquals(0, fileSplit.getOffset(0));\r\n        assertEquals(BLOCKSIZE, fileSplit.getLength(0));\r\n        assertEquals(file5.getName(), fileSplit.getPath(1).getName());\r\n        assertEquals(0, fileSplit.getOffset(1));\r\n        assertEquals(BLOCKSIZE, fileSplit.getLength(1));\r\n        assertEquals(hosts1[0], fileSplit.getLocations()[0]);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\input",
  "methodName" : "testForEmptyFile",
  "errType" : null,
  "containingMethodsNum" : 14,
  "sourceCodeText" : "void testForEmptyFile() throws Exception\n{\r\n    Configuration conf = new Configuration();\r\n    FileSystem fileSys = FileSystem.get(conf);\r\n    Path file = new Path(\"test\" + \"/file\");\r\n    FSDataOutputStream out = fileSys.create(file, true, conf.getInt(\"io.file.buffer.size\", 4096), (short) 1, (long) BLOCKSIZE);\r\n    out.write(new byte[0]);\r\n    out.close();\r\n    DummyInputFormat inFormat = new DummyInputFormat();\r\n    Job job = Job.getInstance(conf);\r\n    FileInputFormat.setInputPaths(job, \"test\");\r\n    List<InputSplit> splits = inFormat.getSplits(job);\r\n    assertEquals(1, splits.size());\r\n    CombineFileSplit fileSplit = (CombineFileSplit) splits.get(0);\r\n    assertEquals(1, fileSplit.getNumPaths());\r\n    assertEquals(file.getName(), fileSplit.getPath(0).getName());\r\n    assertEquals(0, fileSplit.getOffset(0));\r\n    assertEquals(0, fileSplit.getLength(0));\r\n    fileSys.delete(file.getParent(), true);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\input",
  "methodName" : "testGetSplitsWithDirectory",
  "errType" : null,
  "containingMethodsNum" : 20,
  "sourceCodeText" : "void testGetSplitsWithDirectory() throws Exception\n{\r\n    MiniDFSCluster dfs = null;\r\n    try {\r\n        Configuration conf = new Configuration();\r\n        dfs = new MiniDFSCluster.Builder(conf).racks(rack1).hosts(hosts1).build();\r\n        dfs.waitActive();\r\n        FileSystem fileSys = dfs.getFileSystem();\r\n        Path dir1 = new Path(\"/dir1\");\r\n        Path file = new Path(\"/dir1/file1\");\r\n        Path dir2 = new Path(\"/dir1/dir2\");\r\n        if (!fileSys.mkdirs(dir1)) {\r\n            throw new IOException(\"Mkdirs failed to create \" + dir1.toString());\r\n        }\r\n        FSDataOutputStream out = fileSys.create(file);\r\n        out.write(new byte[0]);\r\n        out.close();\r\n        if (!fileSys.mkdirs(dir2)) {\r\n            throw new IOException(\"Mkdirs failed to create \" + dir2.toString());\r\n        }\r\n        DummyInputFormat inFormat = new DummyInputFormat();\r\n        Job job = Job.getInstance(conf);\r\n        FileInputFormat.setInputPaths(job, \"/dir1\");\r\n        List<InputSplit> splits = inFormat.getSplits(job);\r\n        assertEquals(1, splits.size());\r\n        CombineFileSplit fileSplit = (CombineFileSplit) splits.get(0);\r\n        assertEquals(1, fileSplit.getNumPaths());\r\n        assertEquals(file.getName(), fileSplit.getPath(0).getName());\r\n        assertEquals(0, fileSplit.getOffset(0));\r\n        assertEquals(0, fileSplit.getLength(0));\r\n    } finally {\r\n        if (dfs != null) {\r\n            dfs.shutdown();\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\input",
  "methodName" : "testForNonDefaultFileSystem",
  "errType" : null,
  "containingMethodsNum" : 13,
  "sourceCodeText" : "void testForNonDefaultFileSystem() throws Throwable\n{\r\n    Configuration conf = new Configuration();\r\n    conf.set(CommonConfigurationKeys.FS_DEFAULT_NAME_KEY, DUMMY_FS_URI);\r\n    assertEquals(DUMMY_FS_URI, FileSystem.getDefaultUri(conf).toString());\r\n    String localPathRoot = System.getProperty(\"test.build.data\", \"build/test/data\");\r\n    Path localPath = new Path(localPathRoot, \"testFile1\");\r\n    FileSystem lfs = FileSystem.getLocal(conf);\r\n    FSDataOutputStream dos = lfs.create(localPath);\r\n    dos.writeChars(\"Local file for CFIF\");\r\n    dos.close();\r\n    Job job = Job.getInstance(conf);\r\n    FileInputFormat.setInputPaths(job, lfs.makeQualified(localPath));\r\n    DummyInputFormat inFormat = new DummyInputFormat();\r\n    List<InputSplit> splits = inFormat.getSplits(job);\r\n    assertTrue(splits.size() > 0);\r\n    for (InputSplit s : splits) {\r\n        CombineFileSplit cfs = (CombineFileSplit) s;\r\n        for (Path p : cfs.getPaths()) {\r\n            assertThat(p.toUri().getScheme()).isEqualTo(\"file\");\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\input",
  "methodName" : "splitRealFiles",
  "errType" : null,
  "containingMethodsNum" : 12,
  "sourceCodeText" : "void splitRealFiles(String[] args) throws IOException\n{\r\n    Configuration conf = new Configuration();\r\n    Job job = Job.getInstance();\r\n    FileSystem fs = FileSystem.get(conf);\r\n    if (!(fs instanceof DistributedFileSystem)) {\r\n        throw new IOException(\"Wrong file system: \" + fs.getClass().getName());\r\n    }\r\n    long blockSize = fs.getDefaultBlockSize();\r\n    DummyInputFormat inFormat = new DummyInputFormat();\r\n    for (int i = 0; i < args.length; i++) {\r\n        FileInputFormat.addInputPaths(job, args[i]);\r\n    }\r\n    inFormat.setMinSplitSizeRack(blockSize);\r\n    inFormat.setMaxSplitSize(10 * blockSize);\r\n    List<InputSplit> splits = inFormat.getSplits(job);\r\n    System.out.println(\"Total number of splits \" + splits.size());\r\n    for (int i = 0; i < splits.size(); ++i) {\r\n        CombineFileSplit fileSplit = (CombineFileSplit) splits.get(i);\r\n        System.out.println(\"Split[\" + i + \"] \" + fileSplit);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\input",
  "methodName" : "main",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void main(String[] args) throws Exception\n{\r\n    if (args.length != 0) {\r\n        TestCombineFileInputFormat test = new TestCombineFileInputFormat();\r\n        test.splitRealFiles(args);\r\n    } else {\r\n        TestCombineFileInputFormat test = new TestCombineFileInputFormat();\r\n        test.testSplitPlacement();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\fs\\slive",
  "methodName" : "getSelector",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "RouletteSelector getSelector()\n{\r\n    return selector;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\fs\\slive",
  "methodName" : "configureWeights",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void configureWeights(ConfigExtractor e)\n{\r\n    weights = new HashMap<Distribution, Weightable>();\r\n    weights.put(Distribution.UNIFORM, new UniformWeight());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\fs\\slive",
  "methodName" : "determineHowMany",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "int determineHowMany(int totalAm, OperationData opData, OperationType type)\n{\r\n    if (totalAm <= 0) {\r\n        return 0;\r\n    }\r\n    int amLeft = (int) Math.floor(opData.getPercent() * totalAm);\r\n    if (amLeft < 0) {\r\n        throw new IllegalArgumentException(\"Invalid amount \" + amLeft + \" determined for operation type \" + type.name());\r\n    }\r\n    return amLeft;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\fs\\slive",
  "methodName" : "configureOperations",
  "errType" : null,
  "containingMethodsNum" : 12,
  "sourceCodeText" : "void configureOperations(ConfigExtractor cfg)\n{\r\n    operations = new TreeMap<OperationType, OperationInfo>();\r\n    Map<OperationType, OperationData> opinfo = cfg.getOperations();\r\n    int totalAm = cfg.getOpCount();\r\n    int opsLeft = totalAm;\r\n    NumberFormat formatter = Formatter.getPercentFormatter();\r\n    for (final OperationType type : opinfo.keySet()) {\r\n        OperationData opData = opinfo.get(type);\r\n        OperationInfo info = new OperationInfo();\r\n        info.distribution = opData.getDistribution();\r\n        int amLeft = determineHowMany(totalAm, opData, type);\r\n        opsLeft -= amLeft;\r\n        LOG.info(type.name() + \" has \" + amLeft + \" initial operations out of \" + totalAm + \" for its ratio \" + formatter.format(opData.getPercent()));\r\n        info.amountLeft = amLeft;\r\n        Operation op = factory.getOperation(type);\r\n        if (op != null) {\r\n            Observer fn = new Observer() {\r\n\r\n                public void notifyFinished(Operation op) {\r\n                    OperationInfo opInfo = operations.get(type);\r\n                    if (opInfo != null) {\r\n                        --opInfo.amountLeft;\r\n                    }\r\n                }\r\n\r\n                public void notifyStarting(Operation op) {\r\n                }\r\n            };\r\n            info.operation = new ObserveableOp(op, fn);\r\n            operations.put(type, info);\r\n        }\r\n    }\r\n    if (opsLeft > 0) {\r\n        LOG.info(opsLeft + \" left over operations found (due to inability to support partial operations)\");\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\fs\\slive",
  "methodName" : "select",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "Operation select(int elapsed, int duration)\n{\r\n    List<OperationWeight> validOps = new ArrayList<OperationWeight>(operations.size());\r\n    for (OperationType type : operations.keySet()) {\r\n        OperationInfo opinfo = operations.get(type);\r\n        if (opinfo == null || opinfo.amountLeft <= 0) {\r\n            continue;\r\n        }\r\n        Weightable weighter = weights.get(opinfo.distribution);\r\n        if (weighter != null) {\r\n            OperationWeight weightOp = new OperationWeight(opinfo.operation, weighter.weight(elapsed, duration));\r\n            validOps.add(weightOp);\r\n        } else {\r\n            throw new RuntimeException(\"Unable to get weight for distribution \" + opinfo.distribution);\r\n        }\r\n    }\r\n    if (validOps.isEmpty()) {\r\n        return null;\r\n    }\r\n    return getSelector().select(validOps);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\fs\\slive",
  "methodName" : "getDecimalFormatter",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "NumberFormat getDecimalFormatter()\n{\r\n    if (decFormatter == null) {\r\n        decFormatter = new DecimalFormat(NUMBER_FORMAT);\r\n    }\r\n    return decFormatter;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\fs\\slive",
  "methodName" : "getPercentFormatter",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "NumberFormat getPercentFormatter()\n{\r\n    if (percFormatter == null) {\r\n        percFormatter = NumberFormat.getPercentInstance();\r\n        percFormatter.setMaximumFractionDigits(3);\r\n    }\r\n    return percFormatter;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred\\pipes",
  "methodName" : "testFormat",
  "errType" : null,
  "containingMethodsNum" : 19,
  "sourceCodeText" : "void testFormat() throws IOException\n{\r\n    PipesNonJavaInputFormat inputFormat = new PipesNonJavaInputFormat();\r\n    JobConf conf = new JobConf();\r\n    Reporter reporter = mock(Reporter.class);\r\n    RecordReader<FloatWritable, NullWritable> reader = inputFormat.getRecordReader(new FakeSplit(), conf, reporter);\r\n    assertEquals(0.0f, reader.getProgress(), 0.001);\r\n    File input1 = new File(workSpace + File.separator + \"input1\");\r\n    if (!input1.getParentFile().exists()) {\r\n        Assert.assertTrue(input1.getParentFile().mkdirs());\r\n    }\r\n    if (!input1.exists()) {\r\n        Assert.assertTrue(input1.createNewFile());\r\n    }\r\n    File input2 = new File(workSpace + File.separator + \"input2\");\r\n    if (!input2.exists()) {\r\n        Assert.assertTrue(input2.createNewFile());\r\n    }\r\n    conf.set(org.apache.hadoop.mapreduce.lib.input.FileInputFormat.INPUT_DIR, StringUtils.escapeString(input1.getAbsolutePath()) + \",\" + StringUtils.escapeString(input2.getAbsolutePath()));\r\n    InputSplit[] splits = inputFormat.getSplits(conf, 2);\r\n    assertEquals(2, splits.length);\r\n    PipesNonJavaInputFormat.PipesDummyRecordReader dummyRecordReader = new PipesNonJavaInputFormat.PipesDummyRecordReader(conf, splits[0]);\r\n    assertNull(dummyRecordReader.createKey());\r\n    assertNull(dummyRecordReader.createValue());\r\n    assertEquals(0, dummyRecordReader.getPos());\r\n    assertEquals(0.0, dummyRecordReader.getProgress(), 0.001);\r\n    assertTrue(dummyRecordReader.next(new FloatWritable(2.0f), NullWritable.get()));\r\n    assertEquals(2.0, dummyRecordReader.getProgress(), 0.001);\r\n    dummyRecordReader.close();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\aggregate",
  "methodName" : "testAggregates",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testAggregates() throws Exception\n{\r\n    launch();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\aggregate",
  "methodName" : "launch",
  "errType" : null,
  "containingMethodsNum" : 45,
  "sourceCodeText" : "void launch() throws Exception\n{\r\n    Configuration conf = new Configuration();\r\n    FileSystem fs = FileSystem.get(conf);\r\n    int numOfInputLines = 20;\r\n    String baseDir = System.getProperty(\"test.build.data\", \"build/test/data\");\r\n    Path OUTPUT_DIR = new Path(baseDir + \"/output_for_aggregates_test\");\r\n    Path INPUT_DIR = new Path(baseDir + \"/input_for_aggregates_test\");\r\n    String inputFile = \"input.txt\";\r\n    fs.delete(INPUT_DIR, true);\r\n    fs.mkdirs(INPUT_DIR);\r\n    fs.delete(OUTPUT_DIR, true);\r\n    StringBuffer inputData = new StringBuffer();\r\n    StringBuffer expectedOutput = new StringBuffer();\r\n    expectedOutput.append(\"max\\t19\\n\");\r\n    expectedOutput.append(\"min\\t1\\n\");\r\n    FSDataOutputStream fileOut = fs.create(new Path(INPUT_DIR, inputFile));\r\n    for (int i = 1; i < numOfInputLines; i++) {\r\n        expectedOutput.append(\"count_\").append(idFormat.format(i));\r\n        expectedOutput.append(\"\\t\").append(i).append(\"\\n\");\r\n        inputData.append(idFormat.format(i));\r\n        for (int j = 1; j < i; j++) {\r\n            inputData.append(\" \").append(idFormat.format(i));\r\n        }\r\n        inputData.append(\"\\n\");\r\n    }\r\n    expectedOutput.append(\"value_as_string_max\\t9\\n\");\r\n    expectedOutput.append(\"value_as_string_min\\t1\\n\");\r\n    expectedOutput.append(\"uniq_count\\t15\\n\");\r\n    fileOut.write(inputData.toString().getBytes(\"utf-8\"));\r\n    fileOut.close();\r\n    System.out.println(\"inputData:\");\r\n    System.out.println(inputData.toString());\r\n    conf.setInt(ValueAggregatorJobBase.DESCRIPTOR_NUM, 1);\r\n    conf.set(ValueAggregatorJobBase.DESCRIPTOR + \".0\", \"UserDefined,org.apache.hadoop.mapreduce.lib.aggregate.AggregatorTests\");\r\n    conf.setLong(UniqValueCount.MAX_NUM_UNIQUE_VALUES, 14);\r\n    Job job = Job.getInstance(conf);\r\n    FileInputFormat.setInputPaths(job, INPUT_DIR);\r\n    job.setInputFormatClass(TextInputFormat.class);\r\n    FileOutputFormat.setOutputPath(job, OUTPUT_DIR);\r\n    job.setOutputFormatClass(TextOutputFormat.class);\r\n    job.setMapOutputKeyClass(Text.class);\r\n    job.setMapOutputValueClass(Text.class);\r\n    job.setOutputKeyClass(Text.class);\r\n    job.setOutputValueClass(Text.class);\r\n    job.setNumReduceTasks(1);\r\n    job.setMapperClass(ValueAggregatorMapper.class);\r\n    job.setReducerClass(ValueAggregatorReducer.class);\r\n    job.setCombinerClass(ValueAggregatorCombiner.class);\r\n    job.waitForCompletion(true);\r\n    assertTrue(job.isSuccessful());\r\n    String outdata = MapReduceTestUtil.readOutput(OUTPUT_DIR, conf);\r\n    System.out.println(\"full out data:\");\r\n    System.out.println(outdata.toString());\r\n    outdata = outdata.substring(0, expectedOutput.toString().length());\r\n    assertEquals(expectedOutput.toString(), outdata);\r\n    fs.delete(OUTPUT_DIR, true);\r\n    fs.delete(INPUT_DIR, true);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2",
  "methodName" : "setup",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void setup() throws Exception\n{\r\n    if (!(new File(MiniMRYarnCluster.APPJAR)).exists()) {\r\n        LOG.info(\"MRAppJar \" + MiniMRYarnCluster.APPJAR + \" not found. Not running test.\");\r\n        return;\r\n    }\r\n    if (mrCluster == null) {\r\n        mrCluster = new MiniMRYarnCluster(getClass().getSimpleName());\r\n        mrCluster.init(new Configuration());\r\n        mrCluster.start();\r\n    }\r\n    localFs.copyFromLocalFile(new Path(MiniMRYarnCluster.APPJAR), APP_JAR);\r\n    localFs.setPermission(APP_JAR, new FsPermission(\"700\"));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2",
  "methodName" : "testJobWithNonNormalizedCapabilities",
  "errType" : null,
  "containingMethodsNum" : 13,
  "sourceCodeText" : "void testJobWithNonNormalizedCapabilities() throws Exception\n{\r\n    if (!(new File(MiniMRYarnCluster.APPJAR)).exists()) {\r\n        LOG.info(\"MRAppJar \" + MiniMRYarnCluster.APPJAR + \" not found. Not running test.\");\r\n        return;\r\n    }\r\n    JobConf jobConf = new JobConf(mrCluster.getConfig());\r\n    jobConf.setInt(\"mapreduce.map.memory.mb\", 700);\r\n    jobConf.setInt(\"mapred.reduce.memory.mb\", 1500);\r\n    SleepJob sleepJob = new SleepJob();\r\n    sleepJob.setConf(jobConf);\r\n    Job job = sleepJob.createJob(3, 2, 1000, 1, 500, 1);\r\n    job.setJarByClass(SleepJob.class);\r\n    job.addFileToClassPath(APP_JAR);\r\n    job.submit();\r\n    boolean completed = job.waitForCompletion(true);\r\n    Assert.assertTrue(\"Job should be completed\", completed);\r\n    Assert.assertEquals(\"Job should be finished successfully\", JobStatus.State.SUCCEEDED, job.getJobState());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2",
  "methodName" : "tearDown",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void tearDown()\n{\r\n    if (!(new File(MiniMRYarnCluster.APPJAR)).exists()) {\r\n        LOG.info(\"MRAppJar \" + MiniMRYarnCluster.APPJAR + \" not found. Not running test.\");\r\n        return;\r\n    }\r\n    if (mrCluster != null) {\r\n        mrCluster.stop();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "setupBeforeClass",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setupBeforeClass()\n{\r\n    ResourceUtils.resetResourceTypes(new Configuration());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "setUp",
  "errType" : null,
  "containingMethodsNum" : 11,
  "sourceCodeText" : "void setUp() throws Exception\n{\r\n    resourceMgrDelegate = mock(ResourceMgrDelegate.class);\r\n    conf = new YarnConfiguration();\r\n    conf.set(YarnConfiguration.RM_PRINCIPAL, \"mapred/host@REALM\");\r\n    clientCache = new ClientCache(conf, resourceMgrDelegate);\r\n    clientCache = spy(clientCache);\r\n    yarnRunner = new YARNRunner(conf, resourceMgrDelegate, clientCache);\r\n    yarnRunner = spy(yarnRunner);\r\n    submissionContext = mock(ApplicationSubmissionContext.class);\r\n    doAnswer(new Answer<ApplicationSubmissionContext>() {\r\n\r\n        @Override\r\n        public ApplicationSubmissionContext answer(InvocationOnMock invocation) throws Throwable {\r\n            return submissionContext;\r\n        }\r\n    }).when(yarnRunner).createApplicationSubmissionContext(any(Configuration.class), any(String.class), any(Credentials.class));\r\n    appId = ApplicationId.newInstance(System.currentTimeMillis(), 1);\r\n    jobId = TypeConverter.fromYarn(appId);\r\n    if (testWorkDir.exists()) {\r\n        FileContext.getLocalFSFileContext().delete(new Path(testWorkDir.toString()), true);\r\n    }\r\n    testWorkDir.mkdirs();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "cleanup",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void cleanup()\n{\r\n    FileUtil.fullyDelete(testWorkDir);\r\n    ResourceUtils.resetResourceTypes(new Configuration());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "testJobKill",
  "errType" : null,
  "containingMethodsNum" : 13,
  "sourceCodeText" : "void testJobKill() throws Exception\n{\r\n    clientDelegate = mock(ClientServiceDelegate.class);\r\n    when(clientDelegate.getJobStatus(any(JobID.class))).thenReturn(new org.apache.hadoop.mapreduce.JobStatus(jobId, 0f, 0f, 0f, 0f, State.PREP, JobPriority.HIGH, \"tmp\", \"tmp\", \"tmp\", \"tmp\"));\r\n    when(clientDelegate.killJob(any(JobID.class))).thenReturn(true);\r\n    doAnswer(new Answer<ClientServiceDelegate>() {\r\n\r\n        @Override\r\n        public ClientServiceDelegate answer(InvocationOnMock invocation) throws Throwable {\r\n            return clientDelegate;\r\n        }\r\n    }).when(clientCache).getClient(any(JobID.class));\r\n    yarnRunner.killJob(jobId);\r\n    verify(resourceMgrDelegate).killApplication(appId);\r\n    when(clientDelegate.getJobStatus(any(JobID.class))).thenReturn(new org.apache.hadoop.mapreduce.JobStatus(jobId, 0f, 0f, 0f, 0f, State.RUNNING, JobPriority.HIGH, \"tmp\", \"tmp\", \"tmp\", \"tmp\"));\r\n    yarnRunner.killJob(jobId);\r\n    verify(clientDelegate).killJob(jobId);\r\n    when(clientDelegate.getJobStatus(any(JobID.class))).thenReturn(null);\r\n    when(resourceMgrDelegate.getApplicationReport(any(ApplicationId.class))).thenReturn(ApplicationReport.newInstance(appId, null, \"tmp\", \"tmp\", \"tmp\", \"tmp\", 0, null, YarnApplicationState.FINISHED, \"tmp\", \"tmp\", 0L, 0L, 0L, FinalApplicationStatus.SUCCEEDED, null, null, 0f, \"tmp\", null));\r\n    yarnRunner.killJob(jobId);\r\n    verify(clientDelegate).killJob(jobId);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "testJobKillTimeout",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void testJobKillTimeout() throws Exception\n{\r\n    long timeToWaitBeforeHardKill = 10000 + MRJobConfig.DEFAULT_MR_AM_HARD_KILL_TIMEOUT_MS;\r\n    conf.setLong(MRJobConfig.MR_AM_HARD_KILL_TIMEOUT_MS, timeToWaitBeforeHardKill);\r\n    clientDelegate = mock(ClientServiceDelegate.class);\r\n    doAnswer(new Answer<ClientServiceDelegate>() {\r\n\r\n        @Override\r\n        public ClientServiceDelegate answer(InvocationOnMock invocation) throws Throwable {\r\n            return clientDelegate;\r\n        }\r\n    }).when(clientCache).getClient(any(JobID.class));\r\n    when(clientDelegate.getJobStatus(any(JobID.class))).thenReturn(new org.apache.hadoop.mapreduce.JobStatus(jobId, 0f, 0f, 0f, 0f, State.RUNNING, JobPriority.HIGH, \"tmp\", \"tmp\", \"tmp\", \"tmp\"));\r\n    long startTimeMillis = System.currentTimeMillis();\r\n    yarnRunner.killJob(jobId);\r\n    assertTrue(\"killJob should have waited at least \" + timeToWaitBeforeHardKill + \" ms.\", System.currentTimeMillis() - startTimeMillis >= timeToWaitBeforeHardKill);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "testJobSubmissionFailure",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 11,
  "sourceCodeText" : "void testJobSubmissionFailure() throws Exception\n{\r\n    when(resourceMgrDelegate.submitApplication(any(ApplicationSubmissionContext.class))).thenReturn(appId);\r\n    ApplicationReport report = mock(ApplicationReport.class);\r\n    when(report.getApplicationId()).thenReturn(appId);\r\n    when(report.getDiagnostics()).thenReturn(failString);\r\n    when(report.getYarnApplicationState()).thenReturn(YarnApplicationState.FAILED);\r\n    when(resourceMgrDelegate.getApplicationReport(appId)).thenReturn(report);\r\n    Credentials credentials = new Credentials();\r\n    File jobxml = new File(testWorkDir, \"job.xml\");\r\n    OutputStream out = new FileOutputStream(jobxml);\r\n    conf.writeXml(out);\r\n    out.close();\r\n    try {\r\n        yarnRunner.submitJob(jobId, testWorkDir.getAbsolutePath().toString(), credentials);\r\n    } catch (IOException io) {\r\n        LOG.info(\"Logging exception:\", io);\r\n        assertTrue(io.getLocalizedMessage().contains(failString));\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "testResourceMgrDelegate",
  "errType" : null,
  "containingMethodsNum" : 34,
  "sourceCodeText" : "void testResourceMgrDelegate() throws Exception\n{\r\n    final ApplicationClientProtocol clientRMProtocol = mock(ApplicationClientProtocol.class);\r\n    ResourceMgrDelegate delegate = new ResourceMgrDelegate(conf) {\r\n\r\n        @Override\r\n        protected void serviceStart() throws Exception {\r\n            assertTrue(this.client instanceof YarnClientImpl);\r\n            ((YarnClientImpl) this.client).setRMClient(clientRMProtocol);\r\n        }\r\n    };\r\n    when(clientRMProtocol.forceKillApplication(any(KillApplicationRequest.class))).thenReturn(KillApplicationResponse.newInstance(true));\r\n    delegate.killApplication(appId);\r\n    verify(clientRMProtocol).forceKillApplication(any(KillApplicationRequest.class));\r\n    when(clientRMProtocol.getApplications(any(GetApplicationsRequest.class))).thenReturn(recordFactory.newRecordInstance(GetApplicationsResponse.class));\r\n    delegate.getAllJobs();\r\n    verify(clientRMProtocol).getApplications(any(GetApplicationsRequest.class));\r\n    when(clientRMProtocol.getApplicationReport(any(GetApplicationReportRequest.class))).thenReturn(recordFactory.newRecordInstance(GetApplicationReportResponse.class));\r\n    delegate.getApplicationReport(appId);\r\n    verify(clientRMProtocol).getApplicationReport(any(GetApplicationReportRequest.class));\r\n    GetClusterMetricsResponse clusterMetricsResponse = recordFactory.newRecordInstance(GetClusterMetricsResponse.class);\r\n    clusterMetricsResponse.setClusterMetrics(recordFactory.newRecordInstance(YarnClusterMetrics.class));\r\n    when(clientRMProtocol.getClusterMetrics(any(GetClusterMetricsRequest.class))).thenReturn(clusterMetricsResponse);\r\n    delegate.getClusterMetrics();\r\n    verify(clientRMProtocol).getClusterMetrics(any(GetClusterMetricsRequest.class));\r\n    when(clientRMProtocol.getClusterNodes(any(GetClusterNodesRequest.class))).thenReturn(recordFactory.newRecordInstance(GetClusterNodesResponse.class));\r\n    delegate.getActiveTrackers();\r\n    verify(clientRMProtocol).getClusterNodes(any(GetClusterNodesRequest.class));\r\n    GetNewApplicationResponse newAppResponse = recordFactory.newRecordInstance(GetNewApplicationResponse.class);\r\n    newAppResponse.setApplicationId(appId);\r\n    when(clientRMProtocol.getNewApplication(any(GetNewApplicationRequest.class))).thenReturn(newAppResponse);\r\n    delegate.getNewJobID();\r\n    verify(clientRMProtocol).getNewApplication(any(GetNewApplicationRequest.class));\r\n    GetQueueInfoResponse queueInfoResponse = recordFactory.newRecordInstance(GetQueueInfoResponse.class);\r\n    queueInfoResponse.setQueueInfo(recordFactory.newRecordInstance(QueueInfo.class));\r\n    when(clientRMProtocol.getQueueInfo(any(GetQueueInfoRequest.class))).thenReturn(queueInfoResponse);\r\n    delegate.getQueues();\r\n    verify(clientRMProtocol).getQueueInfo(any(GetQueueInfoRequest.class));\r\n    GetQueueUserAclsInfoResponse aclResponse = recordFactory.newRecordInstance(GetQueueUserAclsInfoResponse.class);\r\n    when(clientRMProtocol.getQueueUserAcls(any(GetQueueUserAclsInfoRequest.class))).thenReturn(aclResponse);\r\n    delegate.getQueueAclsForCurrentUser();\r\n    verify(clientRMProtocol).getQueueUserAcls(any(GetQueueUserAclsInfoRequest.class));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "testGetHSDelegationToken",
  "errType" : null,
  "containingMethodsNum" : 29,
  "sourceCodeText" : "void testGetHSDelegationToken() throws Exception\n{\r\n    try {\r\n        Configuration conf = new Configuration();\r\n        InetSocketAddress mockRmAddress = new InetSocketAddress(\"localhost\", 4444);\r\n        Text rmTokenSevice = SecurityUtil.buildTokenService(mockRmAddress);\r\n        InetSocketAddress mockHsAddress = new InetSocketAddress(\"localhost\", 9200);\r\n        Text hsTokenSevice = SecurityUtil.buildTokenService(mockHsAddress);\r\n        RMDelegationTokenIdentifier tokenIdentifier = new RMDelegationTokenIdentifier(new Text(\"owner\"), new Text(\"renewer\"), new Text(\"real\"));\r\n        Token<RMDelegationTokenIdentifier> token = new Token<RMDelegationTokenIdentifier>(new byte[0], new byte[0], tokenIdentifier.getKind(), rmTokenSevice);\r\n        token.setKind(RMDelegationTokenIdentifier.KIND_NAME);\r\n        org.apache.hadoop.yarn.api.records.Token historyToken = org.apache.hadoop.yarn.api.records.Token.newInstance(new byte[0], MRDelegationTokenIdentifier.KIND_NAME.toString(), new byte[0], hsTokenSevice.toString());\r\n        GetDelegationTokenResponse getDtResponse = Records.newRecord(GetDelegationTokenResponse.class);\r\n        getDtResponse.setDelegationToken(historyToken);\r\n        MRClientProtocol mockHsProxy = mock(MRClientProtocol.class);\r\n        doReturn(mockHsAddress).when(mockHsProxy).getConnectAddress();\r\n        doReturn(getDtResponse).when(mockHsProxy).getDelegationToken(any(GetDelegationTokenRequest.class));\r\n        ResourceMgrDelegate rmDelegate = mock(ResourceMgrDelegate.class);\r\n        doReturn(rmTokenSevice).when(rmDelegate).getRMDelegationTokenService();\r\n        ClientCache clientCache = mock(ClientCache.class);\r\n        doReturn(mockHsProxy).when(clientCache).getInitializedHSProxy();\r\n        Credentials creds = new Credentials();\r\n        YARNRunner yarnRunner = new YARNRunner(conf, rmDelegate, clientCache);\r\n        yarnRunner.addHistoryToken(creds);\r\n        verify(mockHsProxy, times(0)).getDelegationToken(any(GetDelegationTokenRequest.class));\r\n        creds.addToken(new Text(\"rmdt\"), token);\r\n        yarnRunner.addHistoryToken(creds);\r\n        verify(mockHsProxy, times(0)).getDelegationToken(any(GetDelegationTokenRequest.class));\r\n        conf.set(CommonConfigurationKeys.HADOOP_SECURITY_AUTHENTICATION, \"kerberos\");\r\n        UserGroupInformation.setConfiguration(conf);\r\n        creds = new Credentials();\r\n        yarnRunner.addHistoryToken(creds);\r\n        verify(mockHsProxy, times(0)).getDelegationToken(any(GetDelegationTokenRequest.class));\r\n        creds.addToken(new Text(\"rmdt\"), token);\r\n        yarnRunner.addHistoryToken(creds);\r\n        verify(mockHsProxy, times(1)).getDelegationToken(any(GetDelegationTokenRequest.class));\r\n        yarnRunner.addHistoryToken(creds);\r\n        verify(mockHsProxy, times(1)).getDelegationToken(any(GetDelegationTokenRequest.class));\r\n    } finally {\r\n        UserGroupInformation.setConfiguration(new Configuration());\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "testHistoryServerToken",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void testHistoryServerToken() throws Exception\n{\r\n    conf.set(YarnConfiguration.RM_PRINCIPAL, \"foo@LOCAL\");\r\n    final String masterPrincipal = Master.getMasterPrincipal(conf);\r\n    final MRClientProtocol hsProxy = mock(MRClientProtocol.class);\r\n    when(hsProxy.getDelegationToken(any(GetDelegationTokenRequest.class))).thenAnswer(new Answer<GetDelegationTokenResponse>() {\r\n\r\n        public GetDelegationTokenResponse answer(InvocationOnMock invocation) {\r\n            GetDelegationTokenRequest request = (GetDelegationTokenRequest) invocation.getArguments()[0];\r\n            assertEquals(masterPrincipal, request.getRenewer());\r\n            org.apache.hadoop.yarn.api.records.Token token = recordFactory.newRecordInstance(org.apache.hadoop.yarn.api.records.Token.class);\r\n            token.setKind(\"\");\r\n            token.setService(\"\");\r\n            token.setIdentifier(ByteBuffer.allocate(0));\r\n            token.setPassword(ByteBuffer.allocate(0));\r\n            GetDelegationTokenResponse tokenResponse = recordFactory.newRecordInstance(GetDelegationTokenResponse.class);\r\n            tokenResponse.setDelegationToken(token);\r\n            return tokenResponse;\r\n        }\r\n    });\r\n    UserGroupInformation.createRemoteUser(\"someone\").doAs(new PrivilegedExceptionAction<Void>() {\r\n\r\n        @Override\r\n        public Void run() throws Exception {\r\n            yarnRunner = new YARNRunner(conf, null, null);\r\n            yarnRunner.getDelegationTokenFromHS(hsProxy);\r\n            verify(hsProxy).getDelegationToken(any(GetDelegationTokenRequest.class));\r\n            return null;\r\n        }\r\n    });\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "testAMAdminCommandOpts",
  "errType" : null,
  "containingMethodsNum" : 14,
  "sourceCodeText" : "void testAMAdminCommandOpts() throws Exception\n{\r\n    JobConf jobConf = new JobConf();\r\n    jobConf.set(MRJobConfig.MR_AM_ADMIN_COMMAND_OPTS, \"-Djava.net.preferIPv4Stack=true\");\r\n    jobConf.set(MRJobConfig.MR_AM_COMMAND_OPTS, \"-Xmx1024m\");\r\n    YARNRunner yarnRunner = new YARNRunner(jobConf);\r\n    ApplicationSubmissionContext submissionContext = buildSubmitContext(yarnRunner, jobConf);\r\n    ContainerLaunchContext containerSpec = submissionContext.getAMContainerSpec();\r\n    List<String> commands = containerSpec.getCommands();\r\n    int index = 0;\r\n    int adminIndex = 0;\r\n    int adminPos = -1;\r\n    int userIndex = 0;\r\n    int userPos = -1;\r\n    int tmpDirPos = -1;\r\n    for (String command : commands) {\r\n        if (command != null) {\r\n            assertFalse(\"Profiler should be disabled by default\", command.contains(PROFILE_PARAMS));\r\n            adminPos = command.indexOf(\"-Djava.net.preferIPv4Stack=true\");\r\n            if (adminPos >= 0)\r\n                adminIndex = index;\r\n            userPos = command.indexOf(\"-Xmx1024m\");\r\n            if (userPos >= 0)\r\n                userIndex = index;\r\n            tmpDirPos = command.indexOf(\"-Djava.io.tmpdir=\");\r\n        }\r\n        index++;\r\n    }\r\n    assertTrue(\"java.io.tmpdir is not set for AM\", tmpDirPos > 0);\r\n    assertTrue(\"AM admin command opts not in the commands.\", adminPos > 0);\r\n    assertTrue(\"AM user command opts not in the commands.\", userPos > 0);\r\n    if (adminIndex == userIndex) {\r\n        assertTrue(\"AM admin command opts is after user command opts.\", adminPos < userPos);\r\n    } else {\r\n        assertTrue(\"AM admin command opts is after user command opts.\", adminIndex < userIndex);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "testWarnCommandOpts",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void testWarnCommandOpts() throws Exception\n{\r\n    org.apache.log4j.Logger logger = org.apache.log4j.Logger.getLogger(YARNRunner.class);\r\n    ByteArrayOutputStream bout = new ByteArrayOutputStream();\r\n    Layout layout = new SimpleLayout();\r\n    Appender appender = new WriterAppender(layout, bout);\r\n    logger.addAppender(appender);\r\n    JobConf jobConf = new JobConf();\r\n    jobConf.set(MRJobConfig.MR_AM_ADMIN_COMMAND_OPTS, \"-Djava.net.preferIPv4Stack=true -Djava.library.path=foo\");\r\n    jobConf.set(MRJobConfig.MR_AM_COMMAND_OPTS, \"-Xmx1024m -Djava.library.path=bar\");\r\n    YARNRunner yarnRunner = new YARNRunner(jobConf);\r\n    @SuppressWarnings(\"unused\")\r\n    ApplicationSubmissionContext submissionContext = buildSubmitContext(yarnRunner, jobConf);\r\n    String logMsg = bout.toString();\r\n    assertTrue(logMsg.contains(\"WARN - Usage of -Djava.library.path in \" + \"yarn.app.mapreduce.am.admin-command-opts can cause programs to no \" + \"longer function if hadoop native libraries are used. These values \" + \"should be set as part of the LD_LIBRARY_PATH in the app master JVM \" + \"env using yarn.app.mapreduce.am.admin.user.env config settings.\"));\r\n    assertTrue(logMsg.contains(\"WARN - Usage of -Djava.library.path in \" + \"yarn.app.mapreduce.am.command-opts can cause programs to no longer \" + \"function if hadoop native libraries are used. These values should \" + \"be set as part of the LD_LIBRARY_PATH in the app master JVM env \" + \"using yarn.app.mapreduce.am.env config settings.\"));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "testAMProfiler",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void testAMProfiler() throws Exception\n{\r\n    JobConf jobConf = new JobConf();\r\n    jobConf.setBoolean(MRJobConfig.MR_AM_PROFILE, true);\r\n    YARNRunner yarnRunner = new YARNRunner(jobConf);\r\n    ApplicationSubmissionContext submissionContext = buildSubmitContext(yarnRunner, jobConf);\r\n    ContainerLaunchContext containerSpec = submissionContext.getAMContainerSpec();\r\n    List<String> commands = containerSpec.getCommands();\r\n    for (String command : commands) {\r\n        if (command != null) {\r\n            if (command.contains(PROFILE_PARAMS)) {\r\n                return;\r\n            }\r\n        }\r\n    }\r\n    throw new IllegalStateException(\"Profiler opts not found!\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "testNodeLabelExp",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void testNodeLabelExp() throws Exception\n{\r\n    JobConf jobConf = new JobConf();\r\n    jobConf.set(MRJobConfig.JOB_NODE_LABEL_EXP, \"GPU\");\r\n    jobConf.set(MRJobConfig.AM_NODE_LABEL_EXP, \"highMem\");\r\n    YARNRunner yarnRunner = new YARNRunner(jobConf);\r\n    ApplicationSubmissionContext appSubCtx = buildSubmitContext(yarnRunner, jobConf);\r\n    assertThat(appSubCtx.getNodeLabelExpression()).isEqualTo(\"GPU\");\r\n    assertThat(appSubCtx.getAMContainerResourceRequests().get(0).getNodeLabelExpression()).isEqualTo(\"highMem\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "testResourceRequestLocalityAny",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void testResourceRequestLocalityAny() throws Exception\n{\r\n    ResourceRequest amAnyResourceRequest = createResourceRequest(ResourceRequest.ANY, true);\r\n    verifyResourceRequestLocality(null, null, amAnyResourceRequest);\r\n    verifyResourceRequestLocality(null, \"label1\", amAnyResourceRequest);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "testResourceRequestLocalityRack",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void testResourceRequestLocalityRack() throws Exception\n{\r\n    ResourceRequest amAnyResourceRequest = createResourceRequest(ResourceRequest.ANY, false);\r\n    ResourceRequest amRackResourceRequest = createResourceRequest(\"/rack1\", true);\r\n    verifyResourceRequestLocality(\"/rack1\", null, amAnyResourceRequest, amRackResourceRequest);\r\n    verifyResourceRequestLocality(\"/rack1\", \"label1\", amAnyResourceRequest, amRackResourceRequest);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "testResourceRequestLocalityNode",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void testResourceRequestLocalityNode() throws Exception\n{\r\n    ResourceRequest amAnyResourceRequest = createResourceRequest(ResourceRequest.ANY, false);\r\n    ResourceRequest amRackResourceRequest = createResourceRequest(\"/rack1\", false);\r\n    ResourceRequest amNodeResourceRequest = createResourceRequest(\"node1\", true);\r\n    verifyResourceRequestLocality(\"/rack1/node1\", null, amAnyResourceRequest, amRackResourceRequest, amNodeResourceRequest);\r\n    verifyResourceRequestLocality(\"/rack1/node1\", \"label1\", amAnyResourceRequest, amRackResourceRequest, amNodeResourceRequest);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "testResourceRequestLocalityNodeDefaultRack",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void testResourceRequestLocalityNodeDefaultRack() throws Exception\n{\r\n    ResourceRequest amAnyResourceRequest = createResourceRequest(ResourceRequest.ANY, false);\r\n    ResourceRequest amRackResourceRequest = createResourceRequest(\"/default-rack\", false);\r\n    ResourceRequest amNodeResourceRequest = createResourceRequest(\"node1\", true);\r\n    verifyResourceRequestLocality(\"node1\", null, amAnyResourceRequest, amRackResourceRequest, amNodeResourceRequest);\r\n    verifyResourceRequestLocality(\"node1\", \"label1\", amAnyResourceRequest, amRackResourceRequest, amNodeResourceRequest);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "testResourceRequestLocalityMultipleNodes",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void testResourceRequestLocalityMultipleNodes() throws Exception\n{\r\n    ResourceRequest amAnyResourceRequest = createResourceRequest(ResourceRequest.ANY, false);\r\n    ResourceRequest amRackResourceRequest = createResourceRequest(\"/rack1\", false);\r\n    ResourceRequest amNodeResourceRequest = createResourceRequest(\"node1\", true);\r\n    ResourceRequest amNode2ResourceRequest = createResourceRequest(\"node2\", true);\r\n    verifyResourceRequestLocality(\"/rack1/node1,/rack1/node2\", null, amAnyResourceRequest, amRackResourceRequest, amNodeResourceRequest, amNode2ResourceRequest);\r\n    verifyResourceRequestLocality(\"/rack1/node1,/rack1/node2\", \"label1\", amAnyResourceRequest, amRackResourceRequest, amNodeResourceRequest, amNode2ResourceRequest);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "testResourceRequestLocalityMultipleNodesDifferentRack",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void testResourceRequestLocalityMultipleNodesDifferentRack() throws Exception\n{\r\n    ResourceRequest amAnyResourceRequest = createResourceRequest(ResourceRequest.ANY, false);\r\n    ResourceRequest amRackResourceRequest = createResourceRequest(\"/rack1\", false);\r\n    ResourceRequest amNodeResourceRequest = createResourceRequest(\"node1\", true);\r\n    ResourceRequest amRack2ResourceRequest = createResourceRequest(\"/rack2\", false);\r\n    ResourceRequest amNode2ResourceRequest = createResourceRequest(\"node2\", true);\r\n    verifyResourceRequestLocality(\"/rack1/node1,/rack2/node2\", null, amAnyResourceRequest, amRackResourceRequest, amNodeResourceRequest, amRack2ResourceRequest, amNode2ResourceRequest);\r\n    verifyResourceRequestLocality(\"/rack1/node1,/rack2/node2\", \"label1\", amAnyResourceRequest, amRackResourceRequest, amNodeResourceRequest, amRack2ResourceRequest, amNode2ResourceRequest);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "testResourceRequestLocalityMultipleNodesDefaultRack",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void testResourceRequestLocalityMultipleNodesDefaultRack() throws Exception\n{\r\n    ResourceRequest amAnyResourceRequest = createResourceRequest(ResourceRequest.ANY, false);\r\n    ResourceRequest amRackResourceRequest = createResourceRequest(\"/rack1\", false);\r\n    ResourceRequest amNodeResourceRequest = createResourceRequest(\"node1\", true);\r\n    ResourceRequest amRack2ResourceRequest = createResourceRequest(\"/default-rack\", false);\r\n    ResourceRequest amNode2ResourceRequest = createResourceRequest(\"node2\", true);\r\n    verifyResourceRequestLocality(\"/rack1/node1,node2\", null, amAnyResourceRequest, amRackResourceRequest, amNodeResourceRequest, amRack2ResourceRequest, amNode2ResourceRequest);\r\n    verifyResourceRequestLocality(\"/rack1/node1,node2\", \"label1\", amAnyResourceRequest, amRackResourceRequest, amNodeResourceRequest, amRack2ResourceRequest, amNode2ResourceRequest);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "testResourceRequestLocalityInvalid",
  "errType" : [ "IOException", "IOException" ],
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void testResourceRequestLocalityInvalid() throws Exception\n{\r\n    try {\r\n        verifyResourceRequestLocality(\"rack/node1\", null, new ResourceRequest[] {});\r\n        fail(\"Should have failed due to invalid resource but did not\");\r\n    } catch (IOException ioe) {\r\n        assertTrue(ioe.getMessage().contains(\"Invalid resource name\"));\r\n    }\r\n    try {\r\n        verifyResourceRequestLocality(\"/rack/node1/blah\", null, new ResourceRequest[] {});\r\n        fail(\"Should have failed due to invalid resource but did not\");\r\n    } catch (IOException ioe) {\r\n        assertTrue(ioe.getMessage().contains(\"Invalid resource name\"));\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 2,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "verifyResourceRequestLocality",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void verifyResourceRequestLocality(String strictResource, String label, ResourceRequest... expectedReqs) throws Exception\n{\r\n    JobConf jobConf = new JobConf();\r\n    if (strictResource != null) {\r\n        jobConf.set(MRJobConfig.AM_STRICT_LOCALITY, strictResource);\r\n    }\r\n    if (label != null) {\r\n        jobConf.set(MRJobConfig.AM_NODE_LABEL_EXP, label);\r\n        for (ResourceRequest expectedReq : expectedReqs) {\r\n            expectedReq.setNodeLabelExpression(label);\r\n        }\r\n    }\r\n    YARNRunner yarnRunner = new YARNRunner(jobConf);\r\n    ApplicationSubmissionContext appSubCtx = buildSubmitContext(yarnRunner, jobConf);\r\n    assertEquals(Arrays.asList(expectedReqs), appSubCtx.getAMContainerResourceRequests());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "createResourceRequest",
  "errType" : null,
  "containingMethodsNum" : 9,
  "sourceCodeText" : "ResourceRequest createResourceRequest(String name, boolean relaxLocality)\n{\r\n    Resource capability = recordFactory.newRecordInstance(Resource.class);\r\n    capability.setMemorySize(MRJobConfig.DEFAULT_MR_AM_VMEM_MB);\r\n    capability.setVirtualCores(MRJobConfig.DEFAULT_MR_AM_CPU_VCORES);\r\n    ResourceRequest req = recordFactory.newRecordInstance(ResourceRequest.class);\r\n    req.setPriority(YARNRunner.AM_CONTAINER_PRIORITY);\r\n    req.setResourceName(name);\r\n    req.setCapability(capability);\r\n    req.setNumContainers(1);\r\n    req.setRelaxLocality(relaxLocality);\r\n    return req;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "testAMStandardEnvWithDefaultLibPath",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testAMStandardEnvWithDefaultLibPath() throws Exception\n{\r\n    testAMStandardEnv(false, false);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "testAMStandardEnvWithCustomLibPath",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testAMStandardEnvWithCustomLibPath() throws Exception\n{\r\n    testAMStandardEnv(true, false);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "testAMStandardEnvWithCustomLibPathWithSeparateEnvProps",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testAMStandardEnvWithCustomLibPathWithSeparateEnvProps() throws Exception\n{\r\n    testAMStandardEnv(true, true);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "testAMStandardEnv",
  "errType" : null,
  "containingMethodsNum" : 19,
  "sourceCodeText" : "void testAMStandardEnv(boolean customLibPath, boolean useSeparateEnvProps) throws Exception\n{\r\n    assumeNotWindows();\r\n    final String ADMIN_LIB_PATH = \"foo\";\r\n    final String USER_LIB_PATH = \"bar\";\r\n    final String USER_SHELL = \"shell\";\r\n    JobConf jobConf = new JobConf();\r\n    String pathKey = Environment.LD_LIBRARY_PATH.name();\r\n    if (customLibPath) {\r\n        if (useSeparateEnvProps) {\r\n            jobConf.set(MRJobConfig.MR_AM_ADMIN_USER_ENV + \".\" + pathKey, ADMIN_LIB_PATH);\r\n            jobConf.set(MRJobConfig.MR_AM_ENV + \".\" + pathKey, USER_LIB_PATH);\r\n        } else {\r\n            jobConf.set(MRJobConfig.MR_AM_ADMIN_USER_ENV, pathKey + \"=\" + ADMIN_LIB_PATH);\r\n            jobConf.set(MRJobConfig.MR_AM_ENV, pathKey + \"=\" + USER_LIB_PATH);\r\n        }\r\n    }\r\n    jobConf.set(MRJobConfig.MAPRED_ADMIN_USER_SHELL, USER_SHELL);\r\n    YARNRunner yarnRunner = new YARNRunner(jobConf);\r\n    ApplicationSubmissionContext appSubCtx = buildSubmitContext(yarnRunner, jobConf);\r\n    ContainerLaunchContext clc = appSubCtx.getAMContainerSpec();\r\n    Map<String, String> env = clc.getEnvironment();\r\n    String libPath = env.get(pathKey);\r\n    assertNotNull(pathKey + \" not set\", libPath);\r\n    String cps = jobConf.getBoolean(MRConfig.MAPREDUCE_APP_SUBMISSION_CROSS_PLATFORM, MRConfig.DEFAULT_MAPREDUCE_APP_SUBMISSION_CROSS_PLATFORM) ? ApplicationConstants.CLASS_PATH_SEPARATOR : File.pathSeparator;\r\n    String expectedLibPath = MRApps.crossPlatformifyMREnv(conf, Environment.PWD);\r\n    if (customLibPath) {\r\n        expectedLibPath += cps + ADMIN_LIB_PATH + cps + USER_LIB_PATH;\r\n    } else {\r\n        expectedLibPath += cps + MRJobConfig.DEFAULT_MR_AM_ADMIN_USER_ENV.substring(pathKey.length() + 1);\r\n    }\r\n    assertEquals(\"Bad AM \" + pathKey + \" setting\", expectedLibPath, libPath);\r\n    String shell = env.get(Environment.SHELL.name());\r\n    assertNotNull(\"SHELL not set\", shell);\r\n    assertEquals(\"Bad SHELL setting\", USER_SHELL, shell);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "testJobPriority",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void testJobPriority() throws Exception\n{\r\n    JobConf jobConf = new JobConf();\r\n    jobConf.set(MRJobConfig.PRIORITY, \"LOW\");\r\n    YARNRunner yarnRunner = new YARNRunner(jobConf);\r\n    ApplicationSubmissionContext appSubCtx = buildSubmitContext(yarnRunner, jobConf);\r\n    assertEquals(appSubCtx.getPriority(), Priority.newInstance(2));\r\n    jobConf.set(MRJobConfig.PRIORITY, \"12\");\r\n    yarnRunner = new YARNRunner(jobConf);\r\n    appSubCtx = buildSubmitContext(yarnRunner, jobConf);\r\n    assertEquals(appSubCtx.getPriority(), Priority.newInstance(12));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "buildSubmitContext",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "ApplicationSubmissionContext buildSubmitContext(YARNRunner yarnRunner, JobConf jobConf) throws IOException\n{\r\n    File jobxml = new File(testWorkDir, MRJobConfig.JOB_CONF_FILE);\r\n    OutputStream out = new FileOutputStream(jobxml);\r\n    conf.writeXml(out);\r\n    out.close();\r\n    File jobsplit = new File(testWorkDir, MRJobConfig.JOB_SPLIT);\r\n    out = new FileOutputStream(jobsplit);\r\n    out.close();\r\n    File jobsplitmetainfo = new File(testWorkDir, MRJobConfig.JOB_SPLIT_METAINFO);\r\n    out = new FileOutputStream(jobsplitmetainfo);\r\n    out.close();\r\n    return yarnRunner.createApplicationSubmissionContext(jobConf, testWorkDir.toString(), new Credentials());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "testSendJobConf",
  "errType" : null,
  "containingMethodsNum" : 15,
  "sourceCodeText" : "void testSendJobConf() throws IOException\n{\r\n    JobConf jobConf = new JobConf();\r\n    jobConf.set(\"dfs.nameservices\", \"mycluster1,mycluster2\");\r\n    jobConf.set(\"dfs.namenode.rpc-address.mycluster2.nn1\", \"123.0.0.1\");\r\n    jobConf.set(\"dfs.namenode.rpc-address.mycluster2.nn2\", \"123.0.0.2\");\r\n    jobConf.set(\"dfs.ha.namenodes.mycluster2\", \"nn1,nn2\");\r\n    jobConf.set(\"dfs.client.failover.proxy.provider.mycluster2\", \"provider\");\r\n    jobConf.set(\"hadoop.tmp.dir\", \"testconfdir\");\r\n    jobConf.set(CommonConfigurationKeysPublic.HADOOP_SECURITY_AUTHENTICATION, \"kerberos\");\r\n    jobConf.set(\"mapreduce.job.send-token-conf\", \"dfs.nameservices|^dfs.namenode.rpc-address.*$|^dfs.ha.namenodes.*$\" + \"|^dfs.client.failover.proxy.provider.*$\" + \"|dfs.namenode.kerberos.principal\");\r\n    UserGroupInformation.setConfiguration(jobConf);\r\n    YARNRunner yarnRunner = new YARNRunner(jobConf);\r\n    ApplicationSubmissionContext submissionContext = buildSubmitContext(yarnRunner, jobConf);\r\n    Configuration confSent = BuilderUtils.parseTokensConf(submissionContext);\r\n    Assert.assertEquals(\"123.0.0.1\", confSent.get(\"dfs.namenode.rpc-address.mycluster2.nn1\"));\r\n    Assert.assertEquals(\"123.0.0.2\", confSent.get(\"dfs.namenode.rpc-address.mycluster2.nn2\"));\r\n    Assert.assertTrue(confSent.get(\"hadoop.tmp.dir\") == null || !confSent.get(\"hadoop.tmp.dir\").equals(\"testconfdir\"));\r\n    UserGroupInformation.reset();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "testCustomAMRMResourceType",
  "errType" : null,
  "containingMethodsNum" : 11,
  "sourceCodeText" : "void testCustomAMRMResourceType() throws Exception\n{\r\n    initResourceTypes();\r\n    JobConf jobConf = new JobConf();\r\n    jobConf.setInt(MRJobConfig.MR_AM_RESOURCE_PREFIX + CUSTOM_RESOURCE_NAME, 5);\r\n    jobConf.setInt(MRJobConfig.MR_AM_CPU_VCORES, 3);\r\n    yarnRunner = new YARNRunner(jobConf);\r\n    submissionContext = buildSubmitContext(yarnRunner, jobConf);\r\n    List<ResourceRequest> resourceRequests = submissionContext.getAMContainerResourceRequests();\r\n    Assert.assertEquals(1, resourceRequests.size());\r\n    ResourceRequest resourceRequest = resourceRequests.get(0);\r\n    ResourceInformation resourceInformation = resourceRequest.getCapability().getResourceInformation(CUSTOM_RESOURCE_NAME);\r\n    Assert.assertEquals(\"Expecting the default unit (G)\", \"G\", resourceInformation.getUnits());\r\n    Assert.assertEquals(5L, resourceInformation.getValue());\r\n    Assert.assertEquals(3, resourceRequest.getCapability().getVirtualCores());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "testAMRMemoryRequest",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void testAMRMemoryRequest() throws Exception\n{\r\n    for (String memoryName : ImmutableList.of(MRJobConfig.RESOURCE_TYPE_NAME_MEMORY, MRJobConfig.RESOURCE_TYPE_ALTERNATIVE_NAME_MEMORY)) {\r\n        JobConf jobConf = new JobConf();\r\n        jobConf.set(MRJobConfig.MR_AM_RESOURCE_PREFIX + memoryName, \"3 Gi\");\r\n        yarnRunner = new YARNRunner(jobConf);\r\n        submissionContext = buildSubmitContext(yarnRunner, jobConf);\r\n        List<ResourceRequest> resourceRequests = submissionContext.getAMContainerResourceRequests();\r\n        Assert.assertEquals(1, resourceRequests.size());\r\n        ResourceRequest resourceRequest = resourceRequests.get(0);\r\n        long memorySize = resourceRequest.getCapability().getMemorySize();\r\n        Assert.assertEquals(3072, memorySize);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "testAMRMemoryRequestOverriding",
  "errType" : null,
  "containingMethodsNum" : 13,
  "sourceCodeText" : "void testAMRMemoryRequestOverriding() throws Exception\n{\r\n    for (String memoryName : ImmutableList.of(MRJobConfig.RESOURCE_TYPE_NAME_MEMORY, MRJobConfig.RESOURCE_TYPE_ALTERNATIVE_NAME_MEMORY)) {\r\n        TestAppender testAppender = new TestAppender();\r\n        org.apache.log4j.Logger logger = org.apache.log4j.Logger.getLogger(YARNRunner.class);\r\n        logger.addAppender(testAppender);\r\n        try {\r\n            JobConf jobConf = new JobConf();\r\n            jobConf.set(MRJobConfig.MR_AM_RESOURCE_PREFIX + memoryName, \"3 Gi\");\r\n            jobConf.setInt(MRJobConfig.MR_AM_VMEM_MB, 2048);\r\n            yarnRunner = new YARNRunner(jobConf);\r\n            submissionContext = buildSubmitContext(yarnRunner, jobConf);\r\n            List<ResourceRequest> resourceRequests = submissionContext.getAMContainerResourceRequests();\r\n            Assert.assertEquals(1, resourceRequests.size());\r\n            ResourceRequest resourceRequest = resourceRequests.get(0);\r\n            long memorySize = resourceRequest.getCapability().getMemorySize();\r\n            Assert.assertEquals(3072, memorySize);\r\n            assertTrue(testAppender.getLogEvents().stream().anyMatch(e -> e.getLevel() == Level.WARN && (\"Configuration \" + \"yarn.app.mapreduce.am.resource.\" + memoryName + \"=3Gi is \" + \"overriding the yarn.app.mapreduce.am.resource.mb=2048 \" + \"configuration\").equals(e.getMessage())));\r\n        } finally {\r\n            logger.removeAppender(testAppender);\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "initResourceTypes",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void initResourceTypes()\n{\r\n    CustomResourceTypesConfigurationProvider.initResourceTypes(ImmutableMap.<String, String>builder().put(CUSTOM_RESOURCE_NAME, \"G\").build());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "cleanupBeforeTestrun",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void cleanupBeforeTestrun() throws IOException\n{\r\n    FileSystem tempFS = FileSystem.get(new Path(baseDir).toUri(), getConf());\r\n    if (operation.equals(OP_CREATE_WRITE)) {\r\n        LOG.info(\"Deleting data directory\");\r\n        tempFS.delete(new Path(baseDir, DATA_DIR_NAME), true);\r\n    }\r\n    tempFS.delete(new Path(baseDir, CONTROL_DIR_NAME), true);\r\n    tempFS.delete(new Path(baseDir, OUTPUT_DIR_NAME), true);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "createControlFiles",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void createControlFiles() throws IOException\n{\r\n    LOG.info(\"Creating \" + numberOfMaps + \" control files\");\r\n    for (int i = 0; i < numberOfMaps; i++) {\r\n        String strFileName = \"NNBench_Controlfile_\" + i;\r\n        Path filePath = new Path(new Path(baseDir, CONTROL_DIR_NAME), strFileName);\r\n        SequenceFile.Writer writer = null;\r\n        try {\r\n            writer = SequenceFile.createWriter(getConf(), Writer.file(filePath), Writer.keyClass(Text.class), Writer.valueClass(LongWritable.class), Writer.compression(CompressionType.NONE));\r\n            writer.append(new Text(strFileName), new LongWritable(i));\r\n        } finally {\r\n            if (writer != null) {\r\n                writer.close();\r\n            }\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "displayVersion",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void displayVersion()\n{\r\n    System.out.println(NNBENCH_VERSION);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "displayUsage",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void displayUsage()\n{\r\n    String usage = \"Usage: nnbench <options>\\n\" + \"Options:\\n\" + \"\\t-operation <Available operations are \" + OP_CREATE_WRITE + \" \" + OP_OPEN_READ + \" \" + OP_RENAME + \" \" + OP_DELETE + \". \" + \"This option is mandatory>\\n\" + \"\\t * NOTE: The open_read, rename and delete operations assume \" + \"that the files they operate on, are already available. \" + \"The create_write operation must be run before running the \" + \"other operations.\\n\" + \"\\t-maps <number of maps. default is 1. This is not mandatory>\\n\" + \"\\t-reduces <number of reduces. default is 1. This is not mandatory>\\n\" + \"\\t-startTime <time to start, given in seconds from the epoch. \" + \"Make sure this is far enough into the future, so all maps \" + \"(operations) will start at the same time. \" + \"default is launch time + 2 mins. This is not mandatory>\\n\" + \"\\t-blockSize <Block size in bytes. default is 1. \" + \"This is not mandatory>\\n\" + \"\\t-bytesToWrite <Bytes to write. default is 0. \" + \"This is not mandatory>\\n\" + \"\\t-bytesPerChecksum <Bytes per checksum for the files. default is 1. \" + \"This is not mandatory>\\n\" + \"\\t-numberOfFiles <number of files to create. default is 1. \" + \"This is not mandatory>\\n\" + \"\\t-replicationFactorPerFile <Replication factor for the files.\" + \" default is 1. This is not mandatory>\\n\" + \"\\t-baseDir <base DFS path. default is /benchmarks/NNBench. \" + \"Supports cross-cluster access by using full path with schema and \" + \"cluster. This is not mandatory>\\n\" + \"\\t-readFileAfterOpen <true or false. if true, it reads the file and \" + \"reports the average time to read. This is valid with the open_read \" + \"operation. default is false. This is not mandatory>\\n\" + \"\\t-help: Display the help statement\\n\";\r\n    System.out.println(usage);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "checkArgs",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void checkArgs(final int index, final int length)\n{\r\n    if (index == length) {\r\n        displayUsage();\r\n        throw new HadoopIllegalArgumentException(\"Not enough arguments\");\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "parseInputs",
  "errType" : null,
  "containingMethodsNum" : 59,
  "sourceCodeText" : "void parseInputs(final String[] args)\n{\r\n    if (args.length == 0) {\r\n        displayUsage();\r\n        throw new HadoopIllegalArgumentException(\"Give valid inputs\");\r\n    }\r\n    for (int i = 0; i < args.length; i++) {\r\n        if (args[i].equals(\"-operation\")) {\r\n            operation = args[++i];\r\n        } else if (args[i].equals(\"-maps\")) {\r\n            checkArgs(i + 1, args.length);\r\n            numberOfMaps = Long.parseLong(args[++i]);\r\n        } else if (args[i].equals(\"-reduces\")) {\r\n            checkArgs(i + 1, args.length);\r\n            numberOfReduces = Long.parseLong(args[++i]);\r\n        } else if (args[i].equals(\"-startTime\")) {\r\n            checkArgs(i + 1, args.length);\r\n            startTime = Long.parseLong(args[++i]) * 1000;\r\n        } else if (args[i].equals(\"-blockSize\")) {\r\n            checkArgs(i + 1, args.length);\r\n            blockSize = Long.parseLong(args[++i]);\r\n        } else if (args[i].equals(\"-bytesToWrite\")) {\r\n            checkArgs(i + 1, args.length);\r\n            bytesToWrite = Integer.parseInt(args[++i]);\r\n        } else if (args[i].equals(\"-bytesPerChecksum\")) {\r\n            checkArgs(i + 1, args.length);\r\n            bytesPerChecksum = Long.parseLong(args[++i]);\r\n        } else if (args[i].equals(\"-numberOfFiles\")) {\r\n            checkArgs(i + 1, args.length);\r\n            numberOfFiles = Long.parseLong(args[++i]);\r\n        } else if (args[i].equals(\"-replicationFactorPerFile\")) {\r\n            checkArgs(i + 1, args.length);\r\n            replicationFactorPerFile = Short.parseShort(args[++i]);\r\n        } else if (args[i].equals(\"-baseDir\")) {\r\n            checkArgs(i + 1, args.length);\r\n            baseDir = args[++i];\r\n        } else if (args[i].equals(\"-readFileAfterOpen\")) {\r\n            checkArgs(i + 1, args.length);\r\n            readFileAfterOpen = Boolean.parseBoolean(args[++i]);\r\n        } else if (args[i].equals(\"-help\")) {\r\n            displayUsage();\r\n            isHelpMessage = true;\r\n        }\r\n    }\r\n    LOG.info(\"Test Inputs: \");\r\n    LOG.info(\"           Test Operation: \" + operation);\r\n    LOG.info(\"               Start time: \" + sdf.format(new Date(startTime)));\r\n    LOG.info(\"           Number of maps: \" + numberOfMaps);\r\n    LOG.info(\"        Number of reduces: \" + numberOfReduces);\r\n    LOG.info(\"               Block Size: \" + blockSize);\r\n    LOG.info(\"           Bytes to write: \" + bytesToWrite);\r\n    LOG.info(\"       Bytes per checksum: \" + bytesPerChecksum);\r\n    LOG.info(\"          Number of files: \" + numberOfFiles);\r\n    LOG.info(\"       Replication factor: \" + replicationFactorPerFile);\r\n    LOG.info(\"                 Base dir: \" + baseDir);\r\n    LOG.info(\"     Read file after open: \" + readFileAfterOpen);\r\n    getConf().set(\"test.nnbench.operation\", operation);\r\n    getConf().setLong(\"test.nnbench.maps\", numberOfMaps);\r\n    getConf().setLong(\"test.nnbench.reduces\", numberOfReduces);\r\n    getConf().setLong(\"test.nnbench.starttime\", startTime);\r\n    getConf().setLong(\"test.nnbench.blocksize\", blockSize);\r\n    getConf().setInt(\"test.nnbench.bytestowrite\", bytesToWrite);\r\n    getConf().setLong(\"test.nnbench.bytesperchecksum\", bytesPerChecksum);\r\n    getConf().setLong(\"test.nnbench.numberoffiles\", numberOfFiles);\r\n    getConf().setInt(\"test.nnbench.replicationfactor\", (int) replicationFactorPerFile);\r\n    getConf().set(\"test.nnbench.basedir\", baseDir);\r\n    getConf().setBoolean(\"test.nnbench.readFileAfterOpen\", readFileAfterOpen);\r\n    getConf().set(\"test.nnbench.datadir.name\", DATA_DIR_NAME);\r\n    getConf().set(\"test.nnbench.outputdir.name\", OUTPUT_DIR_NAME);\r\n    getConf().set(\"test.nnbench.controldir.name\", CONTROL_DIR_NAME);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "analyzeResults",
  "errType" : null,
  "containingMethodsNum" : 30,
  "sourceCodeText" : "int analyzeResults() throws IOException\n{\r\n    final FileSystem fs = FileSystem.get(new Path(baseDir).toUri(), getConf());\r\n    Path reduceDir = new Path(baseDir, OUTPUT_DIR_NAME);\r\n    long totalTimeAL1 = 0l;\r\n    long totalTimeAL2 = 0l;\r\n    long totalTimeTPmS = 0l;\r\n    long lateMaps = 0l;\r\n    long numOfExceptions = 0l;\r\n    long successfulFileOps = 0l;\r\n    long mapStartTimeTPmS = 0l;\r\n    long mapEndTimeTPmS = 0l;\r\n    FileStatus[] fss = fs.listStatus(reduceDir);\r\n    for (FileStatus status : fss) {\r\n        Path reduceFile = status.getPath();\r\n        try (DataInputStream in = new DataInputStream(fs.open(reduceFile));\r\n            BufferedReader lines = new BufferedReader(new InputStreamReader(in))) {\r\n            String line;\r\n            while ((line = lines.readLine()) != null) {\r\n                StringTokenizer tokens = new StringTokenizer(line, \" \\t\\n\\r\\f%;\");\r\n                String attr = tokens.nextToken();\r\n                if (attr.endsWith(\":totalTimeAL1\")) {\r\n                    totalTimeAL1 = Long.parseLong(tokens.nextToken());\r\n                } else if (attr.endsWith(\":totalTimeAL2\")) {\r\n                    totalTimeAL2 = Long.parseLong(tokens.nextToken());\r\n                } else if (attr.endsWith(\":totalTimeTPmS\")) {\r\n                    totalTimeTPmS = Long.parseLong(tokens.nextToken());\r\n                } else if (attr.endsWith(\":latemaps\")) {\r\n                    lateMaps = Long.parseLong(tokens.nextToken());\r\n                } else if (attr.endsWith(\":numOfExceptions\")) {\r\n                    numOfExceptions = Long.parseLong(tokens.nextToken());\r\n                } else if (attr.endsWith(\":successfulFileOps\")) {\r\n                    successfulFileOps = Long.parseLong(tokens.nextToken());\r\n                } else if (attr.endsWith(\":mapStartTimeTPmS\")) {\r\n                    mapStartTimeTPmS = Long.parseLong(tokens.nextToken());\r\n                } else if (attr.endsWith(\":mapEndTimeTPmS\")) {\r\n                    mapEndTimeTPmS = Long.parseLong(tokens.nextToken());\r\n                }\r\n            }\r\n        }\r\n    }\r\n    double avgLatency1 = (double) totalTimeAL1 / successfulFileOps;\r\n    double avgLatency2 = (double) totalTimeAL2 / successfulFileOps;\r\n    double longestMapTimeTPmS = (double) (mapEndTimeTPmS - mapStartTimeTPmS);\r\n    double totalTimeTPS = (longestMapTimeTPmS == 0) ? (1000 * successfulFileOps) : (double) (1000 * successfulFileOps) / longestMapTimeTPmS;\r\n    double AverageExecutionTime = (totalTimeTPmS == 0) ? (double) successfulFileOps : (double) totalTimeTPmS / successfulFileOps;\r\n    String resultTPSLine1 = null;\r\n    String resultTPSLine2 = null;\r\n    String resultALLine1 = null;\r\n    String resultALLine2 = null;\r\n    if (operation.equals(OP_CREATE_WRITE)) {\r\n        resultTPSLine1 = \"               TPS: Create/Write/Close: \" + (int) (totalTimeTPS * 2);\r\n        resultTPSLine2 = \"Avg exec time (ms): Create/Write/Close: \" + AverageExecutionTime;\r\n        resultALLine1 = \"            Avg Lat (ms): Create/Write: \" + avgLatency1;\r\n        resultALLine2 = \"                   Avg Lat (ms): Close: \" + avgLatency2;\r\n    } else if (operation.equals(OP_OPEN_READ)) {\r\n        resultTPSLine1 = \"                        TPS: Open/Read: \" + (int) totalTimeTPS;\r\n        resultTPSLine2 = \"         Avg Exec time (ms): Open/Read: \" + AverageExecutionTime;\r\n        resultALLine1 = \"                    Avg Lat (ms): Open: \" + avgLatency1;\r\n        if (readFileAfterOpen) {\r\n            resultALLine2 = \"                  Avg Lat (ms): Read: \" + avgLatency2;\r\n        }\r\n    } else if (operation.equals(OP_RENAME)) {\r\n        resultTPSLine1 = \"                           TPS: Rename: \" + (int) totalTimeTPS;\r\n        resultTPSLine2 = \"            Avg Exec time (ms): Rename: \" + AverageExecutionTime;\r\n        resultALLine1 = \"                  Avg Lat (ms): Rename: \" + avgLatency1;\r\n    } else if (operation.equals(OP_DELETE)) {\r\n        resultTPSLine1 = \"                           TPS: Delete: \" + (int) totalTimeTPS;\r\n        resultTPSLine2 = \"            Avg Exec time (ms): Delete: \" + AverageExecutionTime;\r\n        resultALLine1 = \"                  Avg Lat (ms): Delete: \" + avgLatency1;\r\n    }\r\n    String[] resultLines = { \"-------------- NNBench -------------- : \", \"                               Version: \" + NNBENCH_VERSION, \"                           Date & time: \" + sdf.format(new Date(System.currentTimeMillis())), \"\", \"                        Test Operation: \" + operation, \"                            Start time: \" + sdf.format(new Date(startTime)), \"                           Maps to run: \" + numberOfMaps, \"                        Reduces to run: \" + numberOfReduces, \"                    Block Size (bytes): \" + blockSize, \"                        Bytes to write: \" + bytesToWrite, \"                    Bytes per checksum: \" + bytesPerChecksum, \"                       Number of files: \" + numberOfFiles, \"                    Replication factor: \" + replicationFactorPerFile, \"            Successful file operations: \" + successfulFileOps, \"\", \"        # maps that missed the barrier: \" + lateMaps, \"                          # exceptions: \" + numOfExceptions, \"\", resultTPSLine1, resultTPSLine2, resultALLine1, resultALLine2, \"\", \"                 RAW DATA: AL Total #1: \" + totalTimeAL1, \"                 RAW DATA: AL Total #2: \" + totalTimeAL2, \"              RAW DATA: TPS Total (ms): \" + totalTimeTPmS, \"       RAW DATA: Longest Map Time (ms): \" + longestMapTimeTPmS, \"                   RAW DATA: Late maps: \" + lateMaps, \"             RAW DATA: # of exceptions: \" + numOfExceptions, \"\" };\r\n    try (PrintStream res = new PrintStream(new FileOutputStream(new File(DEFAULT_RES_FILE_NAME), true))) {\r\n        for (String resultLine : resultLines) {\r\n            LOG.info(resultLine);\r\n            res.println(resultLine);\r\n        }\r\n    }\r\n    if (numOfExceptions >= MAX_OPERATION_EXCEPTIONS) {\r\n        return -1;\r\n    }\r\n    return 0;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 2,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "runTests",
  "errType" : null,
  "containingMethodsNum" : 14,
  "sourceCodeText" : "void runTests() throws IOException\n{\r\n    getConf().setLong(\"io.bytes.per.checksum\", bytesPerChecksum);\r\n    JobConf job = new JobConf(getConf(), NNBench.class);\r\n    job.setJobName(\"NNBench-\" + operation);\r\n    FileInputFormat.setInputPaths(job, new Path(baseDir, CONTROL_DIR_NAME));\r\n    job.setInputFormat(SequenceFileInputFormat.class);\r\n    job.setMaxMapAttempts(1);\r\n    job.setSpeculativeExecution(false);\r\n    job.setMapperClass(NNBenchMapper.class);\r\n    job.setReducerClass(NNBenchReducer.class);\r\n    FileOutputFormat.setOutputPath(job, new Path(baseDir, OUTPUT_DIR_NAME));\r\n    job.setOutputKeyClass(Text.class);\r\n    job.setOutputValueClass(Text.class);\r\n    job.setNumReduceTasks((int) numberOfReduces);\r\n    JobClient.runJob(job);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "validateInputs",
  "errType" : null,
  "containingMethodsNum" : 22,
  "sourceCodeText" : "void validateInputs()\n{\r\n    if (!operation.equals(OP_CREATE_WRITE) && !operation.equals(OP_OPEN_READ) && !operation.equals(OP_RENAME) && !operation.equals(OP_DELETE)) {\r\n        System.err.println(\"Error: Unknown operation: \" + operation);\r\n        displayUsage();\r\n        throw new HadoopIllegalArgumentException(\"Error: Unknown operation: \" + operation);\r\n    }\r\n    if (numberOfMaps < 0) {\r\n        System.err.println(\"Error: Number of maps must be a positive number\");\r\n        displayUsage();\r\n        throw new HadoopIllegalArgumentException(\"Error: Number of maps must be a positive number\");\r\n    }\r\n    if (numberOfReduces <= 0) {\r\n        System.err.println(\"Error: Number of reduces must be a positive number\");\r\n        displayUsage();\r\n        throw new HadoopIllegalArgumentException(\"Error: Number of reduces must be a positive number\");\r\n    }\r\n    if (blockSize <= 0) {\r\n        System.err.println(\"Error: Block size must be a positive number\");\r\n        displayUsage();\r\n        throw new HadoopIllegalArgumentException(\"Error: Block size must be a positive number\");\r\n    }\r\n    if (bytesToWrite < 0) {\r\n        System.err.println(\"Error: Bytes to write must be a positive number\");\r\n        displayUsage();\r\n        throw new HadoopIllegalArgumentException(\"Error: Bytes to write must be a positive number\");\r\n    }\r\n    if (bytesPerChecksum < 0) {\r\n        System.err.println(\"Error: Bytes per checksum must be a positive number\");\r\n        displayUsage();\r\n        throw new HadoopIllegalArgumentException(\"Error: Bytes per checksum must be a positive number\");\r\n    }\r\n    if (numberOfFiles < 0) {\r\n        System.err.println(\"Error: Number of files must be a positive number\");\r\n        displayUsage();\r\n        throw new HadoopIllegalArgumentException(\"Error: Number of files must be a positive number\");\r\n    }\r\n    if (replicationFactorPerFile < 0) {\r\n        System.err.println(\"Error: Replication factor must be a positive number\");\r\n        displayUsage();\r\n        throw new HadoopIllegalArgumentException(\"Error: Replication factor must be a positive number\");\r\n    }\r\n    if (blockSize % bytesPerChecksum != 0) {\r\n        System.err.println(\"Error: Block Size in bytes must be a multiple of \" + \"bytes per checksum: \");\r\n        displayUsage();\r\n        throw new HadoopIllegalArgumentException(\"Error: Block Size in bytes must be a multiple of \" + \"bytes per checksum:\");\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "main",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void main(String[] args) throws Exception\n{\r\n    int res = ToolRunner.run(new NNBench(), args);\r\n    System.exit(res);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\hdfs",
  "methodName" : "run",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "int run(String[] args) throws Exception\n{\r\n    displayVersion();\r\n    parseInputs(args);\r\n    if (isHelpMessage) {\r\n        return 0;\r\n    }\r\n    validateInputs();\r\n    cleanupBeforeTestrun();\r\n    createControlFiles();\r\n    runTests();\r\n    return analyzeResults();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\fi",
  "methodName" : "injectCriteria",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "boolean injectCriteria(String klassName)\n{\r\n    boolean trigger = false;\r\n    if (generator.nextFloat() < getProbability(klassName)) {\r\n        trigger = true;\r\n    }\r\n    return trigger;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\fi",
  "methodName" : "getProbability",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "float getProbability(final String klass)\n{\r\n    String newProbName = FPROB_NAME + klass;\r\n    String newValue = System.getProperty(newProbName, conf.get(ALL_PROBABILITIES));\r\n    if (newValue != null && !newValue.equals(conf.get(newProbName)))\r\n        conf.set(newProbName, newValue);\r\n    float ret = conf.getFloat(newProbName, conf.getFloat(ALL_PROBABILITIES, DEFAULT_PROB));\r\n    LOG.debug(\"Request for \" + newProbName + \" returns=\" + ret);\r\n    if (ret < DEFAULT_PROB || ret > MAX_PROB)\r\n        ret = conf.getFloat(ALL_PROBABILITIES, DEFAULT_PROB);\r\n    return ret;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "testMapTaskStatusStartAndFinishTimes",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testMapTaskStatusStartAndFinishTimes()\n{\r\n    checkTaskStatues(true);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "testReduceTaskStatusStartAndFinishTimes",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testReduceTaskStatusStartAndFinishTimes()\n{\r\n    checkTaskStatues(false);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "checkTaskStatues",
  "errType" : null,
  "containingMethodsNum" : 20,
  "sourceCodeText" : "void checkTaskStatues(boolean isMap)\n{\r\n    TaskStatus status = null;\r\n    if (isMap) {\r\n        status = new MapTaskStatus();\r\n    } else {\r\n        status = new ReduceTaskStatus();\r\n    }\r\n    long currentTime = System.currentTimeMillis();\r\n    status.setFinishTime(currentTime);\r\n    assertEquals(\"Finish time of the task status set without start time\", 0, status.getFinishTime());\r\n    status.setStartTime(currentTime);\r\n    assertEquals(\"Start time of the task status not set correctly.\", currentTime, status.getStartTime());\r\n    long wrongTime = -1;\r\n    status.setStartTime(wrongTime);\r\n    assertEquals(\"Start time of the task status is set to wrong negative value\", currentTime, status.getStartTime());\r\n    status.setFinishTime(wrongTime);\r\n    assertEquals(\"Finish time of task status is set to wrong negative value\", 0, status.getFinishTime());\r\n    status.setFinishTime(currentTime);\r\n    assertEquals(\"Finish time of the task status not set correctly.\", currentTime, status.getFinishTime());\r\n    TaskStatus ts = ((TaskStatus) status.clone());\r\n    ts.setDiagnosticInfo(null);\r\n    ts.setDiagnosticInfo(\"\");\r\n    ts.setStateString(null);\r\n    ts.setStateString(\"\");\r\n    ((TaskStatus) status.clone()).statusUpdate(ts);\r\n    ((TaskStatus) status.clone()).statusUpdate(0, null, null);\r\n    ((TaskStatus) status.clone()).statusUpdate(0, \"\", null);\r\n    ((TaskStatus) status.clone()).statusUpdate(null, 0, \"\", null, 1);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "testTaskDiagnosticsAndStateString",
  "errType" : null,
  "containingMethodsNum" : 32,
  "sourceCodeText" : "void testTaskDiagnosticsAndStateString()\n{\r\n    String test = \"hi\";\r\n    final int maxSize = 16;\r\n    TaskStatus status = new TaskStatus(null, 0, 0, null, test, test, null, null, null) {\r\n\r\n        @Override\r\n        protected int getMaxStringSize() {\r\n            return maxSize;\r\n        }\r\n\r\n        @Override\r\n        public void addFetchFailedMap(TaskAttemptID mapTaskId) {\r\n        }\r\n\r\n        @Override\r\n        public boolean getIsMap() {\r\n            return false;\r\n        }\r\n    };\r\n    assertEquals(\"Small diagnostic info test failed\", status.getDiagnosticInfo(), test);\r\n    assertEquals(\"Small state string test failed\", status.getStateString(), test);\r\n    String newDInfo = test.concat(test);\r\n    status.setDiagnosticInfo(test);\r\n    status.setStateString(newDInfo);\r\n    assertEquals(\"Small diagnostic info append failed\", newDInfo, status.getDiagnosticInfo());\r\n    assertEquals(\"Small state-string append failed\", newDInfo, status.getStateString());\r\n    TaskStatus newStatus = (TaskStatus) status.clone();\r\n    String newSInfo = \"hi1\";\r\n    newStatus.setStateString(newSInfo);\r\n    status.statusUpdate(newStatus);\r\n    newDInfo = newDInfo.concat(newStatus.getDiagnosticInfo());\r\n    assertEquals(\"Status-update on diagnostic-info failed\", newDInfo, status.getDiagnosticInfo());\r\n    assertEquals(\"Status-update on state-string failed\", newSInfo, status.getStateString());\r\n    newSInfo = \"hi2\";\r\n    status.statusUpdate(0, newSInfo, null);\r\n    assertEquals(\"Status-update on state-string failed\", newSInfo, status.getStateString());\r\n    newSInfo = \"hi3\";\r\n    status.statusUpdate(null, 0, newSInfo, null, 0);\r\n    assertEquals(\"Status-update on state-string failed\", newSInfo, status.getStateString());\r\n    String large = \"hihihihihihihihihihi\";\r\n    status.setDiagnosticInfo(large);\r\n    status.setStateString(large);\r\n    assertEquals(\"Large diagnostic info append test failed\", maxSize, status.getDiagnosticInfo().length());\r\n    assertEquals(\"Large state-string append test failed\", maxSize, status.getStateString().length());\r\n    newStatus.setDiagnosticInfo(large + \"0\");\r\n    newStatus.setStateString(large + \"1\");\r\n    status.statusUpdate(newStatus);\r\n    assertEquals(\"Status-update on diagnostic info failed\", maxSize, status.getDiagnosticInfo().length());\r\n    assertEquals(\"Status-update on state-string failed\", maxSize, status.getStateString().length());\r\n    status.statusUpdate(0, large + \"2\", null);\r\n    assertEquals(\"Status-update on state-string failed\", maxSize, status.getStateString().length());\r\n    status.statusUpdate(null, 0, large + \"3\", null, 0);\r\n    assertEquals(\"Status-update on state-string failed\", maxSize, status.getStateString().length());\r\n    status = new TaskStatus(null, 0, 0, null, large, large, null, null, null) {\r\n\r\n        @Override\r\n        protected int getMaxStringSize() {\r\n            return maxSize;\r\n        }\r\n\r\n        @Override\r\n        public void addFetchFailedMap(TaskAttemptID mapTaskId) {\r\n        }\r\n\r\n        @Override\r\n        public boolean getIsMap() {\r\n            return false;\r\n        }\r\n    };\r\n    assertEquals(\"Large diagnostic info test failed\", maxSize, status.getDiagnosticInfo().length());\r\n    assertEquals(\"Large state-string test failed\", maxSize, status.getStateString().length());\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "cleanup",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void cleanup()\n{\r\n    FileUtil.fullyDelete(TEST_DIR);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "testMapred",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testMapred() throws Exception\n{\r\n    launch();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "launch",
  "errType" : null,
  "containingMethodsNum" : 64,
  "sourceCodeText" : "void launch() throws Exception\n{\r\n    Configuration conf = new Configuration();\r\n    int countsToGo = counts;\r\n    int[] dist = new int[range];\r\n    for (int i = 0; i < range; i++) {\r\n        double avgInts = (1.0 * countsToGo) / (range - i);\r\n        dist[i] = (int) Math.max(0, Math.round(avgInts + (Math.sqrt(avgInts) * r.nextGaussian())));\r\n        countsToGo -= dist[i];\r\n    }\r\n    if (countsToGo > 0) {\r\n        dist[dist.length - 1] += countsToGo;\r\n    }\r\n    Path testdir = new Path(TEST_DIR.getAbsolutePath());\r\n    if (!fs.mkdirs(testdir)) {\r\n        throw new IOException(\"Mkdirs failed to create \" + testdir.toString());\r\n    }\r\n    Path randomIns = new Path(testdir, \"genins\");\r\n    if (!fs.mkdirs(randomIns)) {\r\n        throw new IOException(\"Mkdirs failed to create \" + randomIns.toString());\r\n    }\r\n    Path answerkey = new Path(randomIns, \"answer.key\");\r\n    SequenceFile.Writer out = SequenceFile.createWriter(fs, conf, answerkey, IntWritable.class, IntWritable.class, SequenceFile.CompressionType.NONE);\r\n    try {\r\n        for (int i = 0; i < range; i++) {\r\n            out.append(new IntWritable(i), new IntWritable(dist[i]));\r\n        }\r\n    } finally {\r\n        out.close();\r\n    }\r\n    printFiles(randomIns, conf);\r\n    Path randomOuts = new Path(testdir, \"genouts\");\r\n    fs.delete(randomOuts, true);\r\n    Job genJob = Job.getInstance(conf);\r\n    FileInputFormat.setInputPaths(genJob, randomIns);\r\n    genJob.setInputFormatClass(SequenceFileInputFormat.class);\r\n    genJob.setMapperClass(RandomGenMapper.class);\r\n    FileOutputFormat.setOutputPath(genJob, randomOuts);\r\n    genJob.setOutputKeyClass(IntWritable.class);\r\n    genJob.setOutputValueClass(IntWritable.class);\r\n    genJob.setReducerClass(RandomGenReducer.class);\r\n    genJob.setNumReduceTasks(1);\r\n    genJob.waitForCompletion(true);\r\n    printFiles(randomOuts, conf);\r\n    int intermediateReduces = 10;\r\n    Path intermediateOuts = new Path(testdir, \"intermediateouts\");\r\n    fs.delete(intermediateOuts, true);\r\n    Job checkJob = Job.getInstance(conf);\r\n    FileInputFormat.setInputPaths(checkJob, randomOuts);\r\n    checkJob.setMapperClass(RandomCheckMapper.class);\r\n    FileOutputFormat.setOutputPath(checkJob, intermediateOuts);\r\n    checkJob.setOutputKeyClass(IntWritable.class);\r\n    checkJob.setOutputValueClass(IntWritable.class);\r\n    checkJob.setOutputFormatClass(MapFileOutputFormat.class);\r\n    checkJob.setReducerClass(RandomCheckReducer.class);\r\n    checkJob.setNumReduceTasks(intermediateReduces);\r\n    checkJob.waitForCompletion(true);\r\n    printFiles(intermediateOuts, conf);\r\n    Path finalOuts = new Path(testdir, \"finalouts\");\r\n    fs.delete(finalOuts, true);\r\n    Job mergeJob = Job.getInstance(conf);\r\n    FileInputFormat.setInputPaths(mergeJob, intermediateOuts);\r\n    mergeJob.setInputFormatClass(SequenceFileInputFormat.class);\r\n    mergeJob.setMapperClass(MergeMapper.class);\r\n    FileOutputFormat.setOutputPath(mergeJob, finalOuts);\r\n    mergeJob.setOutputKeyClass(IntWritable.class);\r\n    mergeJob.setOutputValueClass(IntWritable.class);\r\n    mergeJob.setOutputFormatClass(SequenceFileOutputFormat.class);\r\n    mergeJob.setReducerClass(MergeReducer.class);\r\n    mergeJob.setNumReduceTasks(1);\r\n    mergeJob.waitForCompletion(true);\r\n    printFiles(finalOuts, conf);\r\n    boolean success = true;\r\n    Path recomputedkey = new Path(finalOuts, \"part-r-00000\");\r\n    SequenceFile.Reader in = new SequenceFile.Reader(fs, recomputedkey, conf);\r\n    int totalseen = 0;\r\n    try {\r\n        IntWritable key = new IntWritable();\r\n        IntWritable val = new IntWritable();\r\n        for (int i = 0; i < range; i++) {\r\n            if (dist[i] == 0) {\r\n                continue;\r\n            }\r\n            if (!in.next(key, val)) {\r\n                System.err.println(\"Cannot read entry \" + i);\r\n                success = false;\r\n                break;\r\n            } else {\r\n                if (!((key.get() == i) && (val.get() == dist[i]))) {\r\n                    System.err.println(\"Mismatch!  Pos=\" + key.get() + \", i=\" + i + \", val=\" + val.get() + \", dist[i]=\" + dist[i]);\r\n                    success = false;\r\n                }\r\n                totalseen += val.get();\r\n            }\r\n        }\r\n        if (success) {\r\n            if (in.next(key, val)) {\r\n                System.err.println(\"Unnecessary lines in recomputed key!\");\r\n                success = false;\r\n            }\r\n        }\r\n    } finally {\r\n        in.close();\r\n    }\r\n    int originalTotal = 0;\r\n    for (int i = 0; i < dist.length; i++) {\r\n        originalTotal += dist[i];\r\n    }\r\n    System.out.println(\"Original sum: \" + originalTotal);\r\n    System.out.println(\"Recomputed sum: \" + totalseen);\r\n    Path resultFile = new Path(testdir, \"results\");\r\n    BufferedWriter bw = new BufferedWriter(new OutputStreamWriter(fs.create(resultFile)));\r\n    try {\r\n        bw.write(\"Success=\" + success + \"\\n\");\r\n        System.out.println(\"Success=\" + success);\r\n    } finally {\r\n        bw.close();\r\n    }\r\n    assertTrue(\"testMapRed failed\", success);\r\n    fs.delete(testdir, true);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 3,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "printTextFile",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void printTextFile(FileSystem fs, Path p) throws IOException\n{\r\n    BufferedReader in = new BufferedReader(new InputStreamReader(fs.open(p)));\r\n    String line;\r\n    while ((line = in.readLine()) != null) {\r\n        System.out.println(\"  Row: \" + line);\r\n    }\r\n    in.close();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "printSequenceFile",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void printSequenceFile(FileSystem fs, Path p, Configuration conf) throws IOException\n{\r\n    SequenceFile.Reader r = new SequenceFile.Reader(fs, p, conf);\r\n    Object key = null;\r\n    Object value = null;\r\n    while ((key = r.next(key)) != null) {\r\n        value = r.getCurrentValue(value);\r\n        System.out.println(\"  Row: \" + key + \", \" + value);\r\n    }\r\n    r.close();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "isSequenceFile",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "boolean isSequenceFile(FileSystem fs, Path f) throws IOException\n{\r\n    DataInputStream in = fs.open(f);\r\n    try {\r\n        byte[] seq = \"SEQ\".getBytes();\r\n        for (int i = 0; i < seq.length; ++i) {\r\n            if (seq[i] != in.read()) {\r\n                return false;\r\n            }\r\n        }\r\n    } finally {\r\n        in.close();\r\n    }\r\n    return true;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "printFiles",
  "errType" : null,
  "containingMethodsNum" : 11,
  "sourceCodeText" : "void printFiles(Path dir, Configuration conf) throws IOException\n{\r\n    FileSystem fs = dir.getFileSystem(conf);\r\n    for (FileStatus f : fs.listStatus(dir)) {\r\n        System.out.println(\"Reading \" + f.getPath() + \": \");\r\n        if (f.isDirectory()) {\r\n            System.out.println(\"  it is a map file.\");\r\n            printSequenceFile(fs, new Path(f.getPath(), \"data\"), conf);\r\n        } else if (isSequenceFile(fs, f.getPath())) {\r\n            System.out.println(\"  it is a sequence file.\");\r\n            printSequenceFile(fs, f.getPath(), conf);\r\n        } else {\r\n            System.out.println(\"  it is a text file.\");\r\n            printTextFile(fs, f.getPath());\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "main",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void main(String[] argv) throws Exception\n{\r\n    if (argv.length < 2) {\r\n        System.err.println(\"Usage: TestMapReduce <range> <counts>\");\r\n        System.err.println();\r\n        System.err.println(\"Note: a good test will have a <counts> value\" + \" that is substantially larger than the <range>\");\r\n        return;\r\n    }\r\n    int i = 0;\r\n    range = Integer.parseInt(argv[i++]);\r\n    counts = Integer.parseInt(argv[i++]);\r\n    try {\r\n        launch();\r\n    } finally {\r\n        FileUtil.fullyDelete(TEST_DIR);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\fs\\slive",
  "methodName" : "logAndSetStatus",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void logAndSetStatus(Reporter r, String msg)\n{\r\n    r.setStatus(msg);\r\n    LOG.info(msg);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\fs\\slive",
  "methodName" : "getConfig",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "ConfigExtractor getConfig()\n{\r\n    return config;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\fs\\slive",
  "methodName" : "reduce",
  "errType" : [ "Exception" ],
  "containingMethodsNum" : 10,
  "sourceCodeText" : "void reduce(Text key, Iterator<Text> values, OutputCollector<Text, Text> output, Reporter reporter) throws IOException\n{\r\n    OperationOutput collector = null;\r\n    int reduceAm = 0;\r\n    int errorAm = 0;\r\n    logAndSetStatus(reporter, \"Iterating over reduction values for key \" + key);\r\n    while (values.hasNext()) {\r\n        Text value = values.next();\r\n        try {\r\n            OperationOutput val = new OperationOutput(key, value);\r\n            if (collector == null) {\r\n                collector = val;\r\n            } else {\r\n                collector = OperationOutput.merge(collector, val);\r\n            }\r\n            LOG.info(\"Combined \" + val + \" into/with \" + collector);\r\n            ++reduceAm;\r\n        } catch (Exception e) {\r\n            ++errorAm;\r\n            logAndSetStatus(reporter, \"Error iterating over reduction input \" + value + \" due to : \" + StringUtils.stringifyException(e));\r\n            if (getConfig().shouldExitOnFirstError()) {\r\n                break;\r\n            }\r\n        }\r\n    }\r\n    logAndSetStatus(reporter, \"Reduced \" + reduceAm + \" values with \" + errorAm + \" errors\");\r\n    if (collector != null) {\r\n        logAndSetStatus(reporter, \"Writing output \" + collector.getKey() + \" : \" + collector.getOutputValue());\r\n        output.collect(collector.getKey(), collector.getOutputValue());\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\fs\\slive",
  "methodName" : "configure",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void configure(JobConf conf)\n{\r\n    config = new ConfigExtractor(conf);\r\n    ConfigExtractor.dumpOptions(config);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\fs\\slive",
  "methodName" : "determineBlockSize",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "long determineBlockSize()\n{\r\n    Range<Long> blockSizeRange = getConfig().getBlockSize();\r\n    long blockSize = Range.betweenPositive(getRandom(), blockSizeRange);\r\n    Long byteChecksum = getConfig().getByteCheckSum();\r\n    if (byteChecksum == null) {\r\n        return blockSize;\r\n    }\r\n    long full = (blockSize / byteChecksum) * byteChecksum;\r\n    long toFull = blockSize - full;\r\n    if (toFull >= (byteChecksum / 2)) {\r\n        full += byteChecksum;\r\n    }\r\n    if (full > blockSizeRange.getUpper()) {\r\n        full = blockSizeRange.getUpper();\r\n    }\r\n    if (full < blockSizeRange.getLower()) {\r\n        full = blockSizeRange.getLower();\r\n    }\r\n    return full;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\fs\\slive",
  "methodName" : "determineReplication",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "short determineReplication()\n{\r\n    Range<Short> replicationAmountRange = getConfig().getReplication();\r\n    Range<Long> repRange = new Range<Long>(replicationAmountRange.getLower().longValue(), replicationAmountRange.getUpper().longValue());\r\n    short replicationAmount = (short) Range.betweenPositive(getRandom(), repRange);\r\n    return replicationAmount;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\fs\\slive",
  "methodName" : "getBufferSize",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "int getBufferSize()\n{\r\n    return getConfig().getConfig().getInt(IO_BUF_CONFIG, DEF_IO_BUFFER_SIZE);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\fs\\slive",
  "methodName" : "getCreateFile",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Path getCreateFile()\n{\r\n    Path fn = getFinder().getFile();\r\n    return fn;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\fs\\slive",
  "methodName" : "run",
  "errType" : [ "IOException", "IOException" ],
  "containingMethodsNum" : 28,
  "sourceCodeText" : "List<OperationOutput> run(FileSystem fs)\n{\r\n    List<OperationOutput> out = super.run(fs);\r\n    FSDataOutputStream os = null;\r\n    try {\r\n        Path fn = getCreateFile();\r\n        Range<Long> writeSizeRange = getConfig().getWriteSize();\r\n        long writeSize = 0;\r\n        long blockSize = determineBlockSize();\r\n        short replicationAmount = determineReplication();\r\n        if (getConfig().shouldWriteUseBlockSize()) {\r\n            writeSizeRange = getConfig().getBlockSize();\r\n        }\r\n        writeSize = Range.betweenPositive(getRandom(), writeSizeRange);\r\n        long bytesWritten = 0;\r\n        long timeTaken = 0;\r\n        int bufSize = getBufferSize();\r\n        boolean overWrite = false;\r\n        DataWriter writer = new DataWriter(getRandom());\r\n        LOG.info(\"Attempting to create file at \" + fn + \" of size \" + Helper.toByteInfo(writeSize) + \" using blocksize \" + Helper.toByteInfo(blockSize) + \" and replication amount \" + replicationAmount);\r\n        {\r\n            long startTime = Timer.now();\r\n            os = fs.create(fn, overWrite, bufSize, replicationAmount, blockSize);\r\n            timeTaken += Timer.elapsed(startTime);\r\n            GenerateOutput stats = writer.writeSegment(writeSize, os);\r\n            bytesWritten += stats.getBytesWritten();\r\n            timeTaken += stats.getTimeTaken();\r\n            startTime = Timer.now();\r\n            os.close();\r\n            os = null;\r\n            timeTaken += Timer.elapsed(startTime);\r\n        }\r\n        LOG.info(\"Created file at \" + fn + \" of size \" + Helper.toByteInfo(bytesWritten) + \" bytes using blocksize \" + Helper.toByteInfo(blockSize) + \" and replication amount \" + replicationAmount + \" in \" + timeTaken + \" milliseconds\");\r\n        out.add(new OperationOutput(OutputType.LONG, getType(), ReportWriter.OK_TIME_TAKEN, timeTaken));\r\n        out.add(new OperationOutput(OutputType.LONG, getType(), ReportWriter.BYTES_WRITTEN, bytesWritten));\r\n        out.add(new OperationOutput(OutputType.LONG, getType(), ReportWriter.SUCCESSES, 1L));\r\n    } catch (IOException e) {\r\n        out.add(new OperationOutput(OutputType.LONG, getType(), ReportWriter.FAILURES, 1L));\r\n        LOG.warn(\"Error with creating\", e);\r\n    } finally {\r\n        if (os != null) {\r\n            try {\r\n                os.close();\r\n            } catch (IOException e) {\r\n                LOG.warn(\"Error closing create stream\", e);\r\n            }\r\n        }\r\n    }\r\n    return out;\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 2,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "setUp",
  "errType" : null,
  "containingMethodsNum" : 12,
  "sourceCodeText" : "void setUp() throws IOException\n{\r\n    JobConf conf = new JobConf();\r\n    fileSys = FileSystem.get(conf);\r\n    fileSys.delete(new Path(TEST_ROOT_DIR), true);\r\n    conf.set(\"mapred.job.tracker.handler.count\", \"1\");\r\n    conf.set(\"mapred.job.tracker\", \"127.0.0.1:0\");\r\n    conf.set(\"mapred.job.tracker.http.address\", \"127.0.0.1:0\");\r\n    conf.set(\"mapred.task.tracker.http.address\", \"127.0.0.1:0\");\r\n    conf.set(JHAdminConfig.MR_HISTORY_INTERMEDIATE_DONE_DIR, TEST_ROOT_DIR + \"/intermediate\");\r\n    conf.set(org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.SUCCESSFUL_JOB_OUTPUT_DIR_MARKER, \"true\");\r\n    mr = new MiniMRCluster(1, \"file:///\", 1, null, null, conf);\r\n    inDir = new Path(TEST_ROOT_DIR, \"test-input\");\r\n    String input = \"The quick brown fox\\n\" + \"has many silly\\n\" + \"red fox sox\\n\";\r\n    DataOutputStream file = fileSys.create(new Path(inDir, \"part-\" + 0));\r\n    file.writeBytes(input);\r\n    file.close();\r\n    emptyInDir = new Path(TEST_ROOT_DIR, \"empty-input\");\r\n    fileSys.mkdirs(emptyInDir);\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "tearDown",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void tearDown() throws Exception\n{\r\n    if (fileSys != null) {\r\n        fileSys.close();\r\n    }\r\n    if (mr != null) {\r\n        mr.shutdown();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getNewOutputDir",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Path getNewOutputDir()\n{\r\n    return new Path(TEST_ROOT_DIR, \"output-\" + outDirs++);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "configureJob",
  "errType" : null,
  "containingMethodsNum" : 10,
  "sourceCodeText" : "void configureJob(JobConf jc, String jobName, int maps, int reds, Path outDir)\n{\r\n    jc.setJobName(jobName);\r\n    jc.setInputFormat(TextInputFormat.class);\r\n    jc.setOutputKeyClass(LongWritable.class);\r\n    jc.setOutputValueClass(Text.class);\r\n    FileInputFormat.setInputPaths(jc, inDir);\r\n    FileOutputFormat.setOutputPath(jc, outDir);\r\n    jc.setMapperClass(IdentityMapper.class);\r\n    jc.setReducerClass(IdentityReducer.class);\r\n    jc.setNumMapTasks(maps);\r\n    jc.setNumReduceTasks(reds);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "testSuccessfulJob",
  "errType" : null,
  "containingMethodsNum" : 10,
  "sourceCodeText" : "void testSuccessfulJob(String filename, Class<? extends OutputCommitter> committer, String[] exclude) throws IOException\n{\r\n    JobConf jc = mr.createJobConf();\r\n    Path outDir = getNewOutputDir();\r\n    configureJob(jc, \"job with cleanup()\", 1, 0, outDir);\r\n    jc.setOutputCommitter(committer);\r\n    JobClient jobClient = new JobClient(jc);\r\n    RunningJob job = jobClient.submitJob(jc);\r\n    JobID id = job.getID();\r\n    job.waitForCompletion();\r\n    LOG.info(\"Job finished : \" + job.isComplete());\r\n    Path testFile = new Path(outDir, filename);\r\n    assertTrue(\"Done file \\\"\" + testFile + \"\\\" missing for job \" + id, fileSys.exists(testFile));\r\n    for (String ex : exclude) {\r\n        Path file = new Path(outDir, ex);\r\n        assertFalse(\"File \" + file + \" should not be present for successful job \" + id, fileSys.exists(file));\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "testFailedJob",
  "errType" : null,
  "containingMethodsNum" : 12,
  "sourceCodeText" : "void testFailedJob(String fileName, Class<? extends OutputCommitter> committer, String[] exclude) throws IOException\n{\r\n    JobConf jc = mr.createJobConf();\r\n    Path outDir = getNewOutputDir();\r\n    configureJob(jc, \"fail job with abort()\", 1, 0, outDir);\r\n    jc.setMaxMapAttempts(1);\r\n    jc.setMapperClass(UtilsForTests.FailMapper.class);\r\n    jc.setOutputCommitter(committer);\r\n    JobClient jobClient = new JobClient(jc);\r\n    RunningJob job = jobClient.submitJob(jc);\r\n    JobID id = job.getID();\r\n    job.waitForCompletion();\r\n    assertEquals(\"Job did not fail\", JobStatus.FAILED, job.getJobState());\r\n    if (fileName != null) {\r\n        Path testFile = new Path(outDir, fileName);\r\n        assertTrue(\"File \" + testFile + \" missing for failed job \" + id, fileSys.exists(testFile));\r\n    }\r\n    for (String ex : exclude) {\r\n        Path file = new Path(outDir, ex);\r\n        assertFalse(\"File \" + file + \" should not be present for failed job \" + id, fileSys.exists(file));\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "testKilledJob",
  "errType" : null,
  "containingMethodsNum" : 17,
  "sourceCodeText" : "void testKilledJob(String fileName, Class<? extends OutputCommitter> committer, String[] exclude) throws IOException\n{\r\n    JobConf jc = mr.createJobConf();\r\n    Path outDir = getNewOutputDir();\r\n    configureJob(jc, \"kill job with abort()\", 1, 0, outDir);\r\n    jc.setMapperClass(UtilsForTests.KillMapper.class);\r\n    jc.setOutputCommitter(committer);\r\n    JobClient jobClient = new JobClient(jc);\r\n    RunningJob job = jobClient.submitJob(jc);\r\n    JobID id = job.getID();\r\n    Counters counters = job.getCounters();\r\n    while (true) {\r\n        if (counters.getCounter(JobCounter.TOTAL_LAUNCHED_MAPS) == 1) {\r\n            break;\r\n        }\r\n        LOG.info(\"Waiting for a map task to be launched\");\r\n        UtilsForTests.waitFor(100);\r\n        counters = job.getCounters();\r\n    }\r\n    job.killJob();\r\n    job.waitForCompletion();\r\n    assertEquals(\"Job was not killed\", JobStatus.KILLED, job.getJobState());\r\n    if (fileName != null) {\r\n        Path testFile = new Path(outDir, fileName);\r\n        assertTrue(\"File \" + testFile + \" missing for job \" + id, fileSys.exists(testFile));\r\n    }\r\n    for (String ex : exclude) {\r\n        Path file = new Path(outDir, ex);\r\n        assertFalse(\"File \" + file + \" should not be present for killed job \" + id, fileSys.exists(file));\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "testDefaultCleanupAndAbort",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void testDefaultCleanupAndAbort() throws IOException\n{\r\n    testSuccessfulJob(FileOutputCommitter.SUCCEEDED_FILE_NAME, FileOutputCommitter.class, new String[] {});\r\n    testFailedJob(null, FileOutputCommitter.class, new String[] { FileOutputCommitter.SUCCEEDED_FILE_NAME });\r\n    testKilledJob(null, FileOutputCommitter.class, new String[] { FileOutputCommitter.SUCCEEDED_FILE_NAME });\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "testCustomAbort",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void testCustomAbort() throws IOException\n{\r\n    testSuccessfulJob(FileOutputCommitter.SUCCEEDED_FILE_NAME, CommitterWithCustomAbort.class, new String[] { ABORT_FAILED_FILE_NAME, ABORT_KILLED_FILE_NAME });\r\n    testFailedJob(ABORT_FAILED_FILE_NAME, CommitterWithCustomAbort.class, new String[] { FileOutputCommitter.SUCCEEDED_FILE_NAME, ABORT_KILLED_FILE_NAME });\r\n    testKilledJob(ABORT_KILLED_FILE_NAME, CommitterWithCustomAbort.class, new String[] { FileOutputCommitter.SUCCEEDED_FILE_NAME, ABORT_FAILED_FILE_NAME });\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "testCustomCleanup",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void testCustomCleanup() throws IOException\n{\r\n    testSuccessfulJob(CUSTOM_CLEANUP_FILE_NAME, CommitterWithCustomDeprecatedCleanup.class, new String[] {});\r\n    testFailedJob(CUSTOM_CLEANUP_FILE_NAME, CommitterWithCustomDeprecatedCleanup.class, new String[] { FileOutputCommitter.SUCCEEDED_FILE_NAME });\r\n    testKilledJob(TestJobCleanup.CUSTOM_CLEANUP_FILE_NAME, CommitterWithCustomDeprecatedCleanup.class, new String[] { FileOutputCommitter.SUCCEEDED_FILE_NAME });\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "testAdd",
  "errType" : null,
  "containingMethodsNum" : 22,
  "sourceCodeText" : "void testAdd()\n{\r\n    SortedRanges sr = new SortedRanges();\r\n    sr.add(new Range(2, 9));\r\n    assertEquals(9, sr.getIndicesCount());\r\n    sr.add(new SortedRanges.Range(3, 5));\r\n    assertEquals(9, sr.getIndicesCount());\r\n    sr.add(new SortedRanges.Range(7, 1));\r\n    assertEquals(9, sr.getIndicesCount());\r\n    sr.add(new Range(1, 12));\r\n    assertEquals(12, sr.getIndicesCount());\r\n    sr.add(new Range(7, 9));\r\n    assertEquals(15, sr.getIndicesCount());\r\n    sr.add(new Range(31, 10));\r\n    sr.add(new Range(51, 10));\r\n    sr.add(new Range(66, 10));\r\n    assertEquals(45, sr.getIndicesCount());\r\n    sr.add(new Range(21, 50));\r\n    assertEquals(70, sr.getIndicesCount());\r\n    LOG.debug(sr.toString());\r\n    Iterator<Long> it = sr.skipRangeIterator();\r\n    int i = 0;\r\n    assertEquals(i, it.next().longValue());\r\n    for (i = 16; i < 21; i++) {\r\n        assertEquals(i, it.next().longValue());\r\n    }\r\n    assertEquals(76, it.next().longValue());\r\n    assertEquals(77, it.next().longValue());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "testRemove",
  "errType" : null,
  "containingMethodsNum" : 17,
  "sourceCodeText" : "void testRemove()\n{\r\n    SortedRanges sr = new SortedRanges();\r\n    sr.add(new Range(2, 19));\r\n    assertEquals(19, sr.getIndicesCount());\r\n    sr.remove(new SortedRanges.Range(15, 8));\r\n    assertEquals(13, sr.getIndicesCount());\r\n    sr.remove(new SortedRanges.Range(6, 5));\r\n    assertEquals(8, sr.getIndicesCount());\r\n    sr.remove(new SortedRanges.Range(8, 4));\r\n    assertEquals(7, sr.getIndicesCount());\r\n    sr.add(new Range(18, 5));\r\n    assertEquals(12, sr.getIndicesCount());\r\n    sr.add(new Range(25, 1));\r\n    assertEquals(13, sr.getIndicesCount());\r\n    sr.remove(new SortedRanges.Range(7, 24));\r\n    assertEquals(4, sr.getIndicesCount());\r\n    sr.remove(new SortedRanges.Range(5, 1));\r\n    assertEquals(3, sr.getIndicesCount());\r\n    LOG.debug(sr.toString());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\fs\\slive",
  "methodName" : "getDistribution",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Distribution getDistribution()\n{\r\n    return distribution;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\fs\\slive",
  "methodName" : "getPercent",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Double getPercent()\n{\r\n    return percent;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\fs\\slive",
  "methodName" : "toString",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "String toString()\n{\r\n    StringBuilder str = new StringBuilder();\r\n    if (getPercent() != null) {\r\n        str.append(getPercent() * 100.0d);\r\n    } else {\r\n        str.append(Double.NaN);\r\n    }\r\n    str.append(SEP);\r\n    str.append(getDistribution().lowerName());\r\n    return str.toString();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "setUp",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void setUp() throws Exception\n{\r\n    tearDown();\r\n    localFs.mkdirs(workDir);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "tearDown",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void tearDown() throws Exception\n{\r\n    if (localFs.exists(workDir)) {\r\n        localFs.delete(workDir, true);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "createFile",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "Path createFile(String fileName) throws IOException\n{\r\n    Path file = new Path(workDir, fileName);\r\n    Writer writer = new OutputStreamWriter(localFs.create(file));\r\n    writer.write(\"\");\r\n    writer.close();\r\n    return localFs.makeQualified(file);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "createFiles",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "Set<Path> createFiles() throws IOException\n{\r\n    Set<Path> files = new HashSet<Path>();\r\n    files.add(createFile(\"a\"));\r\n    files.add(createFile(\"b\"));\r\n    files.add(createFile(\"aa\"));\r\n    files.add(createFile(\"bb\"));\r\n    files.add(createFile(\"_hello\"));\r\n    files.add(createFile(\".hello\"));\r\n    return files;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "_testInputFiles",
  "errType" : null,
  "containingMethodsNum" : 14,
  "sourceCodeText" : "void _testInputFiles(boolean withFilter, boolean withGlob) throws Exception\n{\r\n    Set<Path> createdFiles = createFiles();\r\n    JobConf conf = new JobConf();\r\n    Path inputDir = (withGlob) ? new Path(workDir, \"a*\") : workDir;\r\n    FileInputFormat.setInputPaths(conf, inputDir);\r\n    conf.setInputFormat(DummyFileInputFormat.class);\r\n    if (withFilter) {\r\n        FileInputFormat.setInputPathFilter(conf, TestPathFilter.class);\r\n    }\r\n    DummyFileInputFormat inputFormat = (DummyFileInputFormat) conf.getInputFormat();\r\n    Set<Path> computedFiles = new HashSet<Path>();\r\n    for (FileStatus file : inputFormat.listStatus(conf)) {\r\n        computedFiles.add(file.getPath());\r\n    }\r\n    createdFiles.remove(localFs.makeQualified(new Path(workDir, \"_hello\")));\r\n    createdFiles.remove(localFs.makeQualified(new Path(workDir, \".hello\")));\r\n    if (withFilter) {\r\n        createdFiles.remove(localFs.makeQualified(new Path(workDir, \"aa\")));\r\n        createdFiles.remove(localFs.makeQualified(new Path(workDir, \"bb\")));\r\n    }\r\n    if (withGlob) {\r\n        createdFiles.remove(localFs.makeQualified(new Path(workDir, \"b\")));\r\n        createdFiles.remove(localFs.makeQualified(new Path(workDir, \"bb\")));\r\n    }\r\n    assertEquals(createdFiles, computedFiles);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "testWithoutPathFilterWithoutGlob",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testWithoutPathFilterWithoutGlob() throws Exception\n{\r\n    _testInputFiles(false, false);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "testWithoutPathFilterWithGlob",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testWithoutPathFilterWithGlob() throws Exception\n{\r\n    _testInputFiles(false, true);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "testWithPathFilterWithoutGlob",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testWithPathFilterWithoutGlob() throws Exception\n{\r\n    _testInputFiles(true, false);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "testWithPathFilterWithGlob",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testWithPathFilterWithGlob() throws Exception\n{\r\n    _testInputFiles(true, true);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "submitAndValidateJob",
  "errType" : null,
  "containingMethodsNum" : 18,
  "sourceCodeText" : "Job submitAndValidateJob(JobConf conf, int numMaps, int numReds, boolean oldConfigs) throws IOException, InterruptedException, ClassNotFoundException\n{\r\n    conf.setBoolean(OLD_CONFIGS, oldConfigs);\r\n    if (oldConfigs) {\r\n        conf.set(JobConf.MAPRED_TASK_JAVA_OPTS, TASK_OPTS_VAL);\r\n    } else {\r\n        conf.set(JobConf.MAPRED_MAP_TASK_JAVA_OPTS, MAP_OPTS_VAL);\r\n        conf.set(JobConf.MAPRED_REDUCE_TASK_JAVA_OPTS, REDUCE_OPTS_VAL);\r\n    }\r\n    conf.set(JobConf.MAPRED_MAP_TASK_LOG_LEVEL, Level.OFF.toString());\r\n    conf.set(JobConf.MAPRED_REDUCE_TASK_LOG_LEVEL, Level.OFF.toString());\r\n    Job job = MapReduceTestUtil.createJob(conf, inDir, outDir, numMaps, numReds);\r\n    job.setMapperClass(MyMapper.class);\r\n    job.setReducerClass(MyReducer.class);\r\n    assertFalse(\"Job already has a job tracker connection, before it's submitted\", job.isConnected());\r\n    job.submit();\r\n    assertTrue(\"Job doesn't have a job tracker connection, even though it's been submitted\", job.isConnected());\r\n    job.waitForCompletion(true);\r\n    assertTrue(job.isSuccessful());\r\n    FileSystem fs = FileSystem.get(conf);\r\n    assertTrue(\"Job output directory doesn't exit!\", fs.exists(outDir));\r\n    FileStatus[] list = fs.listStatus(outDir, new OutputFilter());\r\n    int numPartFiles = numReds == 0 ? numMaps : numReds;\r\n    assertTrue(\"Number of part-files is \" + list.length + \" and not \" + numPartFiles, list.length == numPartFiles);\r\n    return job;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "testChild",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void testChild() throws Exception\n{\r\n    try {\r\n        submitAndValidateJob(createJobConf(), 1, 1, true);\r\n        submitAndValidateJob(createJobConf(), 1, 1, false);\r\n    } finally {\r\n        tearDown();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "launchWordCount",
  "errType" : null,
  "containingMethodsNum" : 25,
  "sourceCodeText" : "TestResult launchWordCount(JobConf conf, Path inDir, Path outDir, String input, int numMaps, int numReduces, String sysDir) throws IOException\n{\r\n    FileSystem inFs = inDir.getFileSystem(conf);\r\n    FileSystem outFs = outDir.getFileSystem(conf);\r\n    outFs.delete(outDir, true);\r\n    if (!inFs.mkdirs(inDir)) {\r\n        throw new IOException(\"Mkdirs failed to create \" + inDir.toString());\r\n    }\r\n    {\r\n        DataOutputStream file = inFs.create(new Path(inDir, \"part-0\"));\r\n        file.writeBytes(input);\r\n        file.close();\r\n    }\r\n    conf.setJobName(\"wordcount\");\r\n    conf.setInputFormat(TextInputFormat.class);\r\n    conf.setOutputKeyClass(Text.class);\r\n    conf.setOutputValueClass(IntWritable.class);\r\n    conf.setMapperClass(WordCount.MapClass.class);\r\n    conf.setCombinerClass(WordCount.Reduce.class);\r\n    conf.setReducerClass(WordCount.Reduce.class);\r\n    FileInputFormat.setInputPaths(conf, inDir);\r\n    FileOutputFormat.setOutputPath(conf, outDir);\r\n    conf.setNumMapTasks(numMaps);\r\n    conf.setNumReduceTasks(numReduces);\r\n    conf.set(JTConfig.JT_SYSTEM_DIR, \"/tmp/subru/mapred/system\");\r\n    JobClient jobClient = new JobClient(conf);\r\n    RunningJob job = jobClient.runJob(conf);\r\n    assertFalse(FileSystem.get(conf).exists(new Path(conf.get(JTConfig.JT_SYSTEM_DIR))));\r\n    assertFalse(sysDir.contains(\"/tmp/subru/mapred/system\"));\r\n    assertTrue(sysDir.contains(\"custom\"));\r\n    return new TestResult(job, MapReduceTestUtil.readOutput(outDir, conf));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "runWordCount",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void runWordCount(MiniMRCluster mr, JobConf jobConf, String sysDir) throws IOException\n{\r\n    LOG.info(\"runWordCount\");\r\n    TestResult result;\r\n    final Path inDir = new Path(\"./wc/input\");\r\n    final Path outDir = new Path(\"./wc/output\");\r\n    result = launchWordCount(jobConf, inDir, outDir, \"The quick brown fox\\nhas many silly\\n\" + \"red fox sox\\n\", 3, 1, sysDir);\r\n    assertEquals(\"The\\t1\\nbrown\\t1\\nfox\\t2\\nhas\\t1\\nmany\\t1\\n\" + \"quick\\t1\\nred\\t1\\nsilly\\t1\\nsox\\t1\\n\", result.output);\r\n    assertTrue(result.job.isSuccessful());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "testWithDFS",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void testWithDFS() throws IOException\n{\r\n    MiniDFSCluster dfs = null;\r\n    MiniMRCluster mr = null;\r\n    FileSystem fileSys = null;\r\n    try {\r\n        final int taskTrackers = 4;\r\n        JobConf conf = new JobConf();\r\n        conf.set(JTConfig.JT_SYSTEM_DIR, \"/tmp/custom/mapred/system\");\r\n        dfs = new MiniDFSCluster.Builder(conf).numDataNodes(4).build();\r\n        fileSys = dfs.getFileSystem();\r\n        mr = new MiniMRCluster(taskTrackers, fileSys.getUri().toString(), 1, null, null, conf);\r\n        runWordCount(mr, mr.createJobConf(), conf.get(\"mapred.system.dir\"));\r\n    } finally {\r\n        if (dfs != null) {\r\n            dfs.shutdown();\r\n        }\r\n        if (mr != null) {\r\n            mr.shutdown();\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\input",
  "methodName" : "testFormat",
  "errType" : null,
  "containingMethodsNum" : 9,
  "sourceCodeText" : "void testFormat() throws Exception\n{\r\n    Job job = Job.getInstance(conf);\r\n    Path file = new Path(workDir, \"test.txt\");\r\n    localFs.delete(workDir, true);\r\n    FileInputFormat.setInputPaths(job, workDir);\r\n    int numLinesPerMap = 5;\r\n    NLineInputFormat.setNumLinesPerSplit(job, numLinesPerMap);\r\n    for (int length = 0; length < MAX_LENGTH; length += 1) {\r\n        Writer writer = new OutputStreamWriter(localFs.create(file));\r\n        try {\r\n            for (int i = 0; i < length; i++) {\r\n                writer.write(Integer.toString(i) + \" some more text\");\r\n                writer.write(\"\\n\");\r\n            }\r\n        } finally {\r\n            writer.close();\r\n        }\r\n        int lastN = 0;\r\n        if (length != 0) {\r\n            lastN = length % 5;\r\n            if (lastN == 0) {\r\n                lastN = 5;\r\n            }\r\n        }\r\n        checkFormat(job, numLinesPerMap, lastN);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\input",
  "methodName" : "checkFormat",
  "errType" : null,
  "containingMethodsNum" : 17,
  "sourceCodeText" : "void checkFormat(Job job, int expectedN, int lastN) throws IOException, InterruptedException\n{\r\n    NLineInputFormat format = new NLineInputFormat();\r\n    List<InputSplit> splits = format.getSplits(job);\r\n    int count = 0;\r\n    for (int i = 0; i < splits.size(); i++) {\r\n        assertEquals(\"There are no split locations\", 0, splits.get(i).getLocations().length);\r\n        TaskAttemptContext context = MapReduceTestUtil.createDummyMapTaskAttemptContext(job.getConfiguration());\r\n        RecordReader<LongWritable, Text> reader = format.createRecordReader(splits.get(i), context);\r\n        Class<?> clazz = reader.getClass();\r\n        assertEquals(\"reader class is LineRecordReader.\", LineRecordReader.class, clazz);\r\n        MapContext<LongWritable, Text, LongWritable, Text> mcontext = new MapContextImpl<LongWritable, Text, LongWritable, Text>(job.getConfiguration(), context.getTaskAttemptID(), reader, null, null, MapReduceTestUtil.createDummyReporter(), splits.get(i));\r\n        reader.initialize(splits.get(i), mcontext);\r\n        try {\r\n            count = 0;\r\n            while (reader.nextKeyValue()) {\r\n                count++;\r\n            }\r\n        } finally {\r\n            reader.close();\r\n        }\r\n        if (i == splits.size() - 1) {\r\n            assertEquals(\"number of lines in split(\" + i + \") is wrong\", lastN, count);\r\n        } else {\r\n            assertEquals(\"number of lines in split(\" + i + \") is wrong\", expectedN, count);\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "makeOptions",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Options makeOptions()\n{\r\n    Options options = new Options();\r\n    options.addOption(\"nodfs\", false, \"Don't start a mini DFS cluster\").addOption(\"nomr\", false, \"Don't start a mini MR cluster\").addOption(\"nodemanagers\", true, \"How many nodemanagers to start (default 1)\").addOption(\"datanodes\", true, \"How many datanodes to start (default 1)\").addOption(\"format\", false, \"Format the DFS (default false)\").addOption(\"nnport\", true, \"NameNode port (default 0--we choose)\").addOption(\"nnhttpport\", true, \"NameNode HTTP port (default 0--we choose)\").addOption(\"namenode\", true, \"URL of the namenode (default \" + \"is either the DFS cluster or a temporary dir)\").addOption(\"rmport\", true, \"ResourceManager port (default 0--we choose)\").addOption(\"jhsport\", true, \"JobHistoryServer port (default 0--we choose)\").addOption(OptionBuilder.hasArgs().withArgName(\"property=value\").withDescription(\"Options to pass into configuration object\").create(\"D\")).addOption(OptionBuilder.hasArg().withArgName(\"path\").withDescription(\"Save configuration to this XML file.\").create(\"writeConfig\")).addOption(OptionBuilder.hasArg().withArgName(\"path\").withDescription(\"Write basic information to this JSON file.\").create(\"writeDetails\")).addOption(OptionBuilder.withDescription(\"Prints option help.\").create(\"help\"));\r\n    return options;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "run",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void run(String[] args) throws IOException, URISyntaxException\n{\r\n    if (!parseArguments(args)) {\r\n        return;\r\n    }\r\n    start();\r\n    sleepForever();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "sleepForever",
  "errType" : [ "InterruptedException" ],
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void sleepForever()\n{\r\n    while (true) {\r\n        try {\r\n            Thread.sleep(1000 * 60);\r\n        } catch (InterruptedException e) {\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "start",
  "errType" : null,
  "containingMethodsNum" : 17,
  "sourceCodeText" : "void start() throws IOException, FileNotFoundException, URISyntaxException\n{\r\n    if (!noDFS) {\r\n        dfs = new MiniDFSCluster.Builder(conf).nameNodePort(nnPort).nameNodeHttpPort(nnHttpPort).numDataNodes(numDataNodes).format(dfsOpts == StartupOption.FORMAT).startupOption(dfsOpts).build();\r\n        LOG.info(\"Started MiniDFSCluster -- namenode on port \" + dfs.getNameNodePort());\r\n    }\r\n    if (!noMR) {\r\n        if (fs == null && dfs != null) {\r\n            fs = dfs.getFileSystem().getUri().toString();\r\n        } else if (fs == null) {\r\n            fs = \"file:///tmp/minimr-\" + System.nanoTime();\r\n        }\r\n        FileSystem.setDefaultUri(conf, new URI(fs));\r\n        conf.setBoolean(YarnConfiguration.YARN_MINICLUSTER_FIXED_PORTS, true);\r\n        conf.setBoolean(JHAdminConfig.MR_HISTORY_MINICLUSTER_FIXED_PORTS, true);\r\n        conf.set(YarnConfiguration.RM_ADDRESS, MiniYARNCluster.getHostname() + \":\" + this.rmPort);\r\n        conf.set(JHAdminConfig.MR_HISTORY_ADDRESS, MiniYARNCluster.getHostname() + \":\" + this.jhsPort);\r\n        mr = MiniMRClientClusterFactory.create(this.getClass(), numNodeManagers, conf);\r\n        LOG.info(\"Started MiniMRCluster\");\r\n    }\r\n    if (writeConfig != null) {\r\n        FileOutputStream fos = new FileOutputStream(new File(writeConfig));\r\n        conf.writeXml(fos);\r\n        fos.close();\r\n    }\r\n    if (writeDetails != null) {\r\n        Map<String, Object> map = new TreeMap<String, Object>();\r\n        if (dfs != null) {\r\n            map.put(\"namenode_port\", dfs.getNameNodePort());\r\n        }\r\n        if (mr != null) {\r\n            map.put(\"resourcemanager_port\", mr.getConfig().get(YarnConfiguration.RM_ADDRESS).split(\":\")[1]);\r\n        }\r\n        FileWriter fw = new FileWriter(new File(writeDetails));\r\n        fw.write(new JSON().toJSON(map));\r\n        fw.close();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "stop",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void stop() throws IOException\n{\r\n    if (mr != null) {\r\n        mr.stop();\r\n    }\r\n    if (dfs != null) {\r\n        dfs.shutdown();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "parseArguments",
  "errType" : [ "ParseException" ],
  "containingMethodsNum" : 23,
  "sourceCodeText" : "boolean parseArguments(String[] args)\n{\r\n    Options options = makeOptions();\r\n    CommandLine cli;\r\n    try {\r\n        CommandLineParser parser = new GnuParser();\r\n        cli = parser.parse(options, args);\r\n    } catch (ParseException e) {\r\n        LOG.warn(\"options parsing failed:  \" + e.getMessage());\r\n        new HelpFormatter().printHelp(\"...\", options);\r\n        return false;\r\n    }\r\n    if (cli.hasOption(\"help\")) {\r\n        new HelpFormatter().printHelp(\"...\", options);\r\n        return false;\r\n    }\r\n    if (cli.getArgs().length > 0) {\r\n        for (String arg : cli.getArgs()) {\r\n            System.err.println(\"Unrecognized option: \" + arg);\r\n            new HelpFormatter().printHelp(\"...\", options);\r\n            return false;\r\n        }\r\n    }\r\n    noMR = cli.hasOption(\"nomr\");\r\n    numNodeManagers = intArgument(cli, \"nodemanagers\", 1);\r\n    rmPort = intArgument(cli, \"rmport\", 0);\r\n    jhsPort = intArgument(cli, \"jhsport\", 0);\r\n    fs = cli.getOptionValue(\"namenode\");\r\n    noDFS = cli.hasOption(\"nodfs\");\r\n    numDataNodes = intArgument(cli, \"datanodes\", 1);\r\n    nnPort = intArgument(cli, \"nnport\", 0);\r\n    nnHttpPort = intArgument(cli, \"nnhttpport\", 0);\r\n    dfsOpts = cli.hasOption(\"format\") ? StartupOption.FORMAT : StartupOption.REGULAR;\r\n    writeDetails = cli.getOptionValue(\"writeDetails\");\r\n    writeConfig = cli.getOptionValue(\"writeConfig\");\r\n    conf = new JobConf();\r\n    updateConfiguration(conf, cli.getOptionValues(\"D\"));\r\n    return true;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "updateConfiguration",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void updateConfiguration(JobConf conf, String[] keyvalues)\n{\r\n    int num_confs_updated = 0;\r\n    if (keyvalues != null) {\r\n        for (String prop : keyvalues) {\r\n            String[] keyval = prop.split(\"=\", 2);\r\n            if (keyval.length == 2) {\r\n                conf.set(keyval[0], keyval[1]);\r\n                num_confs_updated++;\r\n            } else {\r\n                LOG.warn(\"Ignoring -D option \" + prop);\r\n            }\r\n        }\r\n    }\r\n    LOG.info(\"Updated \" + num_confs_updated + \" configuration settings from command line.\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "intArgument",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "int intArgument(CommandLine cli, String argName, int default_)\n{\r\n    String o = cli.getOptionValue(argName);\r\n    if (o == null) {\r\n        return default_;\r\n    } else {\r\n        return Integer.parseInt(o);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "main",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void main(String[] args) throws IOException, URISyntaxException\n{\r\n    new MiniHadoopClusterManager().run(args);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\input",
  "methodName" : "onlyOnce",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void onlyOnce()\n{\r\n    try {\r\n        defaultConf = new Configuration();\r\n        defaultConf.set(\"fs.defaultFS\", \"file:///\");\r\n        localFs = FileSystem.getLocal(defaultConf);\r\n        chars = (\"abcdefghijklmnopqrstuvABCDEFGHIJKLMN OPQRSTUVWXYZ1234567890)\" + \"(*&^%$#@!-=><?:\\\"{}][';/.,']\").toCharArray();\r\n        workDir = new Path(new Path(System.getProperty(\"test.build.data\", \".\"), \"data\"), \"TestKeyValueFixedLengthInputFormat\");\r\n        charRand = new Random();\r\n    } catch (IOException e) {\r\n        throw new RuntimeException(\"init failure\", e);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\input",
  "methodName" : "testFormat",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testFormat() throws Exception\n{\r\n    runRandomTests(null);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\input",
  "methodName" : "testFormatCompressedIn",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testFormatCompressedIn() throws Exception\n{\r\n    runRandomTests(new GzipCodec());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\input",
  "methodName" : "testNoRecordLength",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 13,
  "sourceCodeText" : "void testNoRecordLength() throws Exception\n{\r\n    localFs.delete(workDir, true);\r\n    Path file = new Path(workDir, new String(\"testFormat.txt\"));\r\n    createFile(file, null, 10, 10);\r\n    Job job = Job.getInstance(defaultConf);\r\n    FileInputFormat.setInputPaths(job, workDir);\r\n    FixedLengthInputFormat format = new FixedLengthInputFormat();\r\n    List<InputSplit> splits = format.getSplits(job);\r\n    boolean exceptionThrown = false;\r\n    for (InputSplit split : splits) {\r\n        try {\r\n            TaskAttemptContext context = MapReduceTestUtil.createDummyMapTaskAttemptContext(job.getConfiguration());\r\n            RecordReader<LongWritable, BytesWritable> reader = format.createRecordReader(split, context);\r\n            MapContext<LongWritable, BytesWritable, LongWritable, BytesWritable> mcontext = new MapContextImpl<LongWritable, BytesWritable, LongWritable, BytesWritable>(job.getConfiguration(), context.getTaskAttemptID(), reader, null, null, MapReduceTestUtil.createDummyReporter(), split);\r\n            reader.initialize(split, mcontext);\r\n        } catch (IOException ioe) {\r\n            exceptionThrown = true;\r\n            LOG.info(\"Exception message:\" + ioe.getMessage());\r\n        }\r\n    }\r\n    assertTrue(\"Exception for not setting record length:\", exceptionThrown);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\input",
  "methodName" : "testZeroRecordLength",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 14,
  "sourceCodeText" : "void testZeroRecordLength() throws Exception\n{\r\n    localFs.delete(workDir, true);\r\n    Path file = new Path(workDir, new String(\"testFormat.txt\"));\r\n    createFile(file, null, 10, 10);\r\n    Job job = Job.getInstance(defaultConf);\r\n    FixedLengthInputFormat format = new FixedLengthInputFormat();\r\n    format.setRecordLength(job.getConfiguration(), 0);\r\n    FileInputFormat.setInputPaths(job, workDir);\r\n    List<InputSplit> splits = format.getSplits(job);\r\n    boolean exceptionThrown = false;\r\n    for (InputSplit split : splits) {\r\n        try {\r\n            TaskAttemptContext context = MapReduceTestUtil.createDummyMapTaskAttemptContext(job.getConfiguration());\r\n            RecordReader<LongWritable, BytesWritable> reader = format.createRecordReader(split, context);\r\n            MapContext<LongWritable, BytesWritable, LongWritable, BytesWritable> mcontext = new MapContextImpl<LongWritable, BytesWritable, LongWritable, BytesWritable>(job.getConfiguration(), context.getTaskAttemptID(), reader, null, null, MapReduceTestUtil.createDummyReporter(), split);\r\n            reader.initialize(split, mcontext);\r\n        } catch (IOException ioe) {\r\n            exceptionThrown = true;\r\n            LOG.info(\"Exception message:\" + ioe.getMessage());\r\n        }\r\n    }\r\n    assertTrue(\"Exception for zero record length:\", exceptionThrown);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\input",
  "methodName" : "testNegativeRecordLength",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 14,
  "sourceCodeText" : "void testNegativeRecordLength() throws Exception\n{\r\n    localFs.delete(workDir, true);\r\n    Path file = new Path(workDir, new String(\"testFormat.txt\"));\r\n    createFile(file, null, 10, 10);\r\n    Job job = Job.getInstance(defaultConf);\r\n    FixedLengthInputFormat format = new FixedLengthInputFormat();\r\n    format.setRecordLength(job.getConfiguration(), -10);\r\n    FileInputFormat.setInputPaths(job, workDir);\r\n    List<InputSplit> splits = format.getSplits(job);\r\n    boolean exceptionThrown = false;\r\n    for (InputSplit split : splits) {\r\n        try {\r\n            TaskAttemptContext context = MapReduceTestUtil.createDummyMapTaskAttemptContext(job.getConfiguration());\r\n            RecordReader<LongWritable, BytesWritable> reader = format.createRecordReader(split, context);\r\n            MapContext<LongWritable, BytesWritable, LongWritable, BytesWritable> mcontext = new MapContextImpl<LongWritable, BytesWritable, LongWritable, BytesWritable>(job.getConfiguration(), context.getTaskAttemptID(), reader, null, null, MapReduceTestUtil.createDummyReporter(), split);\r\n            reader.initialize(split, mcontext);\r\n        } catch (IOException ioe) {\r\n            exceptionThrown = true;\r\n            LOG.info(\"Exception message:\" + ioe.getMessage());\r\n        }\r\n    }\r\n    assertTrue(\"Exception for negative record length:\", exceptionThrown);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\input",
  "methodName" : "testPartialRecordCompressedIn",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testPartialRecordCompressedIn() throws Exception\n{\r\n    CompressionCodec gzip = new GzipCodec();\r\n    runPartialRecordTest(gzip);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\input",
  "methodName" : "testPartialRecordUncompressedIn",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testPartialRecordUncompressedIn() throws Exception\n{\r\n    runPartialRecordTest(null);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\input",
  "methodName" : "testGzipWithTwoInputs",
  "errType" : null,
  "containingMethodsNum" : 20,
  "sourceCodeText" : "void testGzipWithTwoInputs() throws Exception\n{\r\n    CompressionCodec gzip = new GzipCodec();\r\n    localFs.delete(workDir, true);\r\n    Job job = Job.getInstance(defaultConf);\r\n    FixedLengthInputFormat format = new FixedLengthInputFormat();\r\n    format.setRecordLength(job.getConfiguration(), 5);\r\n    ReflectionUtils.setConf(gzip, job.getConfiguration());\r\n    FileInputFormat.setInputPaths(job, workDir);\r\n    writeFile(localFs, new Path(workDir, \"part1.txt.gz\"), gzip, \"one  two  threefour five six  seveneightnine ten  \");\r\n    writeFile(localFs, new Path(workDir, \"part2.txt.gz\"), gzip, \"ten  nine eightsevensix  five four threetwo  one  \");\r\n    List<InputSplit> splits = format.getSplits(job);\r\n    assertEquals(\"compressed splits == 2\", 2, splits.size());\r\n    FileSplit tmp = (FileSplit) splits.get(0);\r\n    if (tmp.getPath().getName().equals(\"part2.txt.gz\")) {\r\n        splits.set(0, splits.get(1));\r\n        splits.set(1, tmp);\r\n    }\r\n    List<String> results = readSplit(format, splits.get(0), job);\r\n    assertEquals(\"splits[0] length\", 10, results.size());\r\n    assertEquals(\"splits[0][5]\", \"six  \", results.get(5));\r\n    results = readSplit(format, splits.get(1), job);\r\n    assertEquals(\"splits[1] length\", 10, results.size());\r\n    assertEquals(\"splits[1][0]\", \"ten  \", results.get(0));\r\n    assertEquals(\"splits[1][1]\", \"nine \", results.get(1));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\input",
  "methodName" : "createFile",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "ArrayList<String> createFile(Path targetFile, CompressionCodec codec, int recordLen, int numRecords) throws IOException\n{\r\n    ArrayList<String> recordList = new ArrayList<String>(numRecords);\r\n    OutputStream ostream = localFs.create(targetFile);\r\n    if (codec != null) {\r\n        ostream = codec.createOutputStream(ostream);\r\n    }\r\n    Writer writer = new OutputStreamWriter(ostream);\r\n    try {\r\n        StringBuffer sb = new StringBuffer();\r\n        for (int i = 0; i < numRecords; i++) {\r\n            for (int j = 0; j < recordLen; j++) {\r\n                sb.append(chars[charRand.nextInt(chars.length)]);\r\n            }\r\n            String recordData = sb.toString();\r\n            recordList.add(recordData);\r\n            writer.write(recordData);\r\n            sb.setLength(0);\r\n        }\r\n    } finally {\r\n        writer.close();\r\n    }\r\n    return recordList;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\input",
  "methodName" : "runRandomTests",
  "errType" : null,
  "containingMethodsNum" : 42,
  "sourceCodeText" : "void runRandomTests(CompressionCodec codec) throws Exception\n{\r\n    StringBuilder fileName = new StringBuilder(\"testFormat.txt\");\r\n    if (codec != null) {\r\n        fileName.append(\".gz\");\r\n    }\r\n    localFs.delete(workDir, true);\r\n    Path file = new Path(workDir, fileName.toString());\r\n    int seed = new Random().nextInt();\r\n    LOG.info(\"Seed = \" + seed);\r\n    Random random = new Random(seed);\r\n    int MAX_TESTS = 20;\r\n    LongWritable key;\r\n    BytesWritable value;\r\n    for (int i = 0; i < MAX_TESTS; i++) {\r\n        LOG.info(\"----------------------------------------------------------\");\r\n        int totalRecords = random.nextInt(999) + 1;\r\n        if (i == 8) {\r\n            totalRecords = 0;\r\n        }\r\n        int recordLength = random.nextInt(1024 * 100) + 1;\r\n        if (i == 10) {\r\n            recordLength = 1;\r\n        }\r\n        int fileSize = (totalRecords * recordLength);\r\n        LOG.info(\"totalRecords=\" + totalRecords + \" recordLength=\" + recordLength);\r\n        Job job = Job.getInstance(defaultConf);\r\n        if (codec != null) {\r\n            ReflectionUtils.setConf(codec, job.getConfiguration());\r\n        }\r\n        ArrayList<String> recordList = createFile(file, codec, recordLength, totalRecords);\r\n        assertTrue(localFs.exists(file));\r\n        FixedLengthInputFormat.setRecordLength(job.getConfiguration(), recordLength);\r\n        int numSplits = 1;\r\n        if (i > 0) {\r\n            if (i == (MAX_TESTS - 1)) {\r\n                numSplits = (int) (fileSize / Math.floor(recordLength / 2));\r\n            } else {\r\n                if (MAX_TESTS % i == 0) {\r\n                    numSplits = fileSize / (fileSize - random.nextInt(fileSize));\r\n                } else {\r\n                    numSplits = Math.max(1, fileSize / random.nextInt(Integer.MAX_VALUE));\r\n                }\r\n            }\r\n            LOG.info(\"Number of splits set to: \" + numSplits);\r\n        }\r\n        job.getConfiguration().setLong(\"mapreduce.input.fileinputformat.split.maxsize\", (long) (fileSize / numSplits));\r\n        FileInputFormat.setInputPaths(job, workDir);\r\n        FixedLengthInputFormat format = new FixedLengthInputFormat();\r\n        List<InputSplit> splits = format.getSplits(job);\r\n        LOG.info(\"Actual number of splits = \" + splits.size());\r\n        long recordOffset = 0;\r\n        int recordNumber = 0;\r\n        for (InputSplit split : splits) {\r\n            TaskAttemptContext context = MapReduceTestUtil.createDummyMapTaskAttemptContext(job.getConfiguration());\r\n            RecordReader<LongWritable, BytesWritable> reader = format.createRecordReader(split, context);\r\n            MapContext<LongWritable, BytesWritable, LongWritable, BytesWritable> mcontext = new MapContextImpl<LongWritable, BytesWritable, LongWritable, BytesWritable>(job.getConfiguration(), context.getTaskAttemptID(), reader, null, null, MapReduceTestUtil.createDummyReporter(), split);\r\n            reader.initialize(split, mcontext);\r\n            Class<?> clazz = reader.getClass();\r\n            assertEquals(\"RecordReader class should be FixedLengthRecordReader:\", FixedLengthRecordReader.class, clazz);\r\n            while (reader.nextKeyValue()) {\r\n                key = reader.getCurrentKey();\r\n                value = reader.getCurrentValue();\r\n                assertEquals(\"Checking key\", (long) (recordNumber * recordLength), key.get());\r\n                String valueString = new String(value.getBytes(), 0, value.getLength());\r\n                assertEquals(\"Checking record length:\", recordLength, value.getLength());\r\n                assertTrue(\"Checking for more records than expected:\", recordNumber < totalRecords);\r\n                String origRecord = recordList.get(recordNumber);\r\n                assertEquals(\"Checking record content:\", origRecord, valueString);\r\n                recordNumber++;\r\n            }\r\n            reader.close();\r\n        }\r\n        assertEquals(\"Total original records should be total read records:\", recordList.size(), recordNumber);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\input",
  "methodName" : "writeFile",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void writeFile(FileSystem fs, Path name, CompressionCodec codec, String contents) throws IOException\n{\r\n    OutputStream stm;\r\n    if (codec == null) {\r\n        stm = fs.create(name);\r\n    } else {\r\n        stm = codec.createOutputStream(fs.create(name));\r\n    }\r\n    stm.write(contents.getBytes());\r\n    stm.close();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\input",
  "methodName" : "readSplit",
  "errType" : null,
  "containingMethodsNum" : 11,
  "sourceCodeText" : "List<String> readSplit(FixedLengthInputFormat format, InputSplit split, Job job) throws Exception\n{\r\n    List<String> result = new ArrayList<String>();\r\n    TaskAttemptContext context = MapReduceTestUtil.createDummyMapTaskAttemptContext(job.getConfiguration());\r\n    RecordReader<LongWritable, BytesWritable> reader = format.createRecordReader(split, context);\r\n    MapContext<LongWritable, BytesWritable, LongWritable, BytesWritable> mcontext = new MapContextImpl<LongWritable, BytesWritable, LongWritable, BytesWritable>(job.getConfiguration(), context.getTaskAttemptID(), reader, null, null, MapReduceTestUtil.createDummyReporter(), split);\r\n    LongWritable key;\r\n    BytesWritable value;\r\n    try {\r\n        reader.initialize(split, mcontext);\r\n        while (reader.nextKeyValue()) {\r\n            key = reader.getCurrentKey();\r\n            value = reader.getCurrentValue();\r\n            result.add(new String(value.getBytes(), 0, value.getLength()));\r\n        }\r\n    } finally {\r\n        reader.close();\r\n    }\r\n    return result;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\input",
  "methodName" : "runPartialRecordTest",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 12,
  "sourceCodeText" : "void runPartialRecordTest(CompressionCodec codec) throws Exception\n{\r\n    localFs.delete(workDir, true);\r\n    Job job = Job.getInstance(defaultConf);\r\n    StringBuilder fileName = new StringBuilder(\"testFormat.txt\");\r\n    if (codec != null) {\r\n        fileName.append(\".gz\");\r\n        ReflectionUtils.setConf(codec, job.getConfiguration());\r\n    }\r\n    writeFile(localFs, new Path(workDir, fileName.toString()), codec, \"one  two  threefour five six  seveneightnine ten\");\r\n    FixedLengthInputFormat format = new FixedLengthInputFormat();\r\n    format.setRecordLength(job.getConfiguration(), 5);\r\n    FileInputFormat.setInputPaths(job, workDir);\r\n    List<InputSplit> splits = format.getSplits(job);\r\n    if (codec != null) {\r\n        assertEquals(\"compressed splits == 1\", 1, splits.size());\r\n    }\r\n    boolean exceptionThrown = false;\r\n    for (InputSplit split : splits) {\r\n        try {\r\n            List<String> results = readSplit(format, split, job);\r\n        } catch (IOException ioe) {\r\n            exceptionThrown = true;\r\n            LOG.info(\"Exception message:\" + ioe.getMessage());\r\n        }\r\n    }\r\n    assertTrue(\"Exception for partial record:\", exceptionThrown);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\jobcontrol",
  "methodName" : "testAddingDependingJobToRunningJobFails",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void testAddingDependingJobToRunningJobFails() throws Exception\n{\r\n    Configuration conf = new Configuration();\r\n    ControlledJob job1 = new ControlledJob(conf);\r\n    job1.setJobState(ControlledJob.State.RUNNING);\r\n    assertFalse(job1.addDependingJob(new ControlledJob(conf)));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\jobcontrol",
  "methodName" : "testAddingDependingJobToCompletedJobFails",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void testAddingDependingJobToCompletedJobFails() throws Exception\n{\r\n    Configuration conf = new Configuration();\r\n    ControlledJob job1 = new ControlledJob(conf);\r\n    job1.setJobState(ControlledJob.State.SUCCESS);\r\n    assertFalse(job1.addDependingJob(new ControlledJob(conf)));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred\\pipes",
  "methodName" : "createDigest",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "String createDigest(byte[] password, String data) throws IOException\n{\r\n    SecretKey key = JobTokenSecretManager.createSecretKey(password);\r\n    return SecureShuffleUtils.hashFromString(data, key);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred\\pipes",
  "methodName" : "readObject",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void readObject(Writable obj, DataInputStream inStream) throws IOException\n{\r\n    int numBytes = WritableUtils.readVInt(inStream);\r\n    byte[] buffer;\r\n    if (obj instanceof BytesWritable) {\r\n        buffer = new byte[numBytes];\r\n        inStream.readFully(buffer);\r\n        ((BytesWritable) obj).set(buffer, 0, numBytes);\r\n    } else if (obj instanceof Text) {\r\n        buffer = new byte[numBytes];\r\n        inStream.readFully(buffer);\r\n        ((Text) obj).set(buffer);\r\n    } else {\r\n        obj.readFields(inStream);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred\\pipes",
  "methodName" : "writeObject",
  "errType" : null,
  "containingMethodsNum" : 14,
  "sourceCodeText" : "void writeObject(Writable obj, DataOutputStream stream) throws IOException\n{\r\n    DataOutputBuffer buffer = new DataOutputBuffer();\r\n    if (obj instanceof Text) {\r\n        Text t = (Text) obj;\r\n        int len = t.getLength();\r\n        WritableUtils.writeVLong(stream, len);\r\n        stream.flush();\r\n        stream.write(t.getBytes(), 0, len);\r\n        stream.flush();\r\n    } else if (obj instanceof BytesWritable) {\r\n        BytesWritable b = (BytesWritable) obj;\r\n        int len = b.getLength();\r\n        WritableUtils.writeVLong(stream, len);\r\n        stream.write(b.getBytes(), 0, len);\r\n    } else {\r\n        buffer.reset();\r\n        obj.write(buffer);\r\n        int length = buffer.getLength();\r\n        WritableUtils.writeVInt(stream, length);\r\n        stream.write(buffer.getData(), 0, length);\r\n    }\r\n    stream.flush();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred\\pipes",
  "methodName" : "initSoket",
  "errType" : null,
  "containingMethodsNum" : 18,
  "sourceCodeText" : "void initSoket() throws Exception\n{\r\n    int port = Integer.parseInt(System.getenv(\"mapreduce.pipes.command.port\"));\r\n    java.net.InetAddress address = java.net.InetAddress.getLocalHost();\r\n    socket = new Socket(address.getHostName(), port);\r\n    InputStream input = socket.getInputStream();\r\n    OutputStream output = socket.getOutputStream();\r\n    dataInput = new DataInputStream(input);\r\n    WritableUtils.readVInt(dataInput);\r\n    String str = Text.readString(dataInput);\r\n    Text.readString(dataInput);\r\n    dataOut = new DataOutputStream(output);\r\n    WritableUtils.writeVInt(dataOut, 57);\r\n    String s = createDigest(\"password\".getBytes(), str);\r\n    Text.writeString(dataOut, s);\r\n    WritableUtils.readVInt(dataInput);\r\n    int cuttentAnswer = WritableUtils.readVInt(dataInput);\r\n    System.out.println(\"CURRENT_PROTOCOL_VERSION:\" + cuttentAnswer);\r\n    WritableUtils.readVInt(dataInput);\r\n    int j = WritableUtils.readVInt(dataInput);\r\n    for (int i = 0; i < j; i++) {\r\n        Text.readString(dataInput);\r\n        i++;\r\n        Text.readString(dataInput);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred\\pipes",
  "methodName" : "closeSoket",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void closeSoket()\n{\r\n    if (socket != null) {\r\n        try {\r\n            socket.close();\r\n        } catch (IOException e) {\r\n            e.printStackTrace();\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\partition",
  "methodName" : "testparseOption",
  "errType" : null,
  "containingMethodsNum" : 82,
  "sourceCodeText" : "void testparseOption() throws Exception\n{\r\n    KeyFieldHelper helper = new KeyFieldHelper();\r\n    helper.setKeyFieldSeparator(\"\\t\");\r\n    String keySpecs = \"-k1.2,3.4\";\r\n    String eKeySpecs = keySpecs;\r\n    helper.parseOption(keySpecs);\r\n    String actKeySpecs = helper.keySpecs().get(0).toString();\r\n    assertEquals(\"KeyFieldHelper's parsing is garbled\", eKeySpecs, actKeySpecs);\r\n    keySpecs = \"-k 1.2\";\r\n    eKeySpecs = \"-k1.2,0.0\";\r\n    helper = new KeyFieldHelper();\r\n    helper.parseOption(keySpecs);\r\n    actKeySpecs = helper.keySpecs().get(0).toString();\r\n    assertEquals(\"KeyFieldHelper's parsing is garbled\", eKeySpecs, actKeySpecs);\r\n    keySpecs = \"-nr -k1.2,3.4\";\r\n    eKeySpecs = \"-k1.2,3.4nr\";\r\n    helper = new KeyFieldHelper();\r\n    helper.parseOption(keySpecs);\r\n    actKeySpecs = helper.keySpecs().get(0).toString();\r\n    assertEquals(\"KeyFieldHelper's parsing is garbled\", eKeySpecs, actKeySpecs);\r\n    keySpecs = \"-nr -k1.2,3.4n\";\r\n    eKeySpecs = \"-k1.2,3.4n\";\r\n    helper = new KeyFieldHelper();\r\n    helper.parseOption(keySpecs);\r\n    actKeySpecs = helper.keySpecs().get(0).toString();\r\n    assertEquals(\"KeyFieldHelper's parsing is garbled\", eKeySpecs, actKeySpecs);\r\n    keySpecs = \"-nr -k1.2,3.4r\";\r\n    eKeySpecs = \"-k1.2,3.4r\";\r\n    helper = new KeyFieldHelper();\r\n    helper.parseOption(keySpecs);\r\n    actKeySpecs = helper.keySpecs().get(0).toString();\r\n    assertEquals(\"KeyFieldHelper's parsing is garbled\", eKeySpecs, actKeySpecs);\r\n    keySpecs = \"-nr -k1.2,3.4 -k5.6,7.8n -k9.10,11.12r -k13.14,15.16nr\";\r\n    eKeySpecs = \"-k1.2,3.4nr\";\r\n    helper = new KeyFieldHelper();\r\n    helper.parseOption(keySpecs);\r\n    actKeySpecs = helper.keySpecs().get(0).toString();\r\n    assertEquals(\"KeyFieldHelper's parsing is garbled\", eKeySpecs, actKeySpecs);\r\n    eKeySpecs = \"-k5.6,7.8n\";\r\n    actKeySpecs = helper.keySpecs().get(1).toString();\r\n    assertEquals(\"KeyFieldHelper's parsing is garbled\", eKeySpecs, actKeySpecs);\r\n    eKeySpecs = \"-k9.10,11.12r\";\r\n    actKeySpecs = helper.keySpecs().get(2).toString();\r\n    assertEquals(\"KeyFieldHelper's parsing is garbled\", eKeySpecs, actKeySpecs);\r\n    eKeySpecs = \"-k13.14,15.16nr\";\r\n    actKeySpecs = helper.keySpecs().get(3).toString();\r\n    assertEquals(\"KeyFieldHelper's parsing is garbled\", eKeySpecs, actKeySpecs);\r\n    keySpecs = \"-k1.2n,3.4\";\r\n    eKeySpecs = \"-k1.2,3.4n\";\r\n    helper = new KeyFieldHelper();\r\n    helper.parseOption(keySpecs);\r\n    actKeySpecs = helper.keySpecs().get(0).toString();\r\n    assertEquals(\"KeyFieldHelper's parsing is garbled\", eKeySpecs, actKeySpecs);\r\n    keySpecs = \"-k1.2r,3.4\";\r\n    eKeySpecs = \"-k1.2,3.4r\";\r\n    helper = new KeyFieldHelper();\r\n    helper.parseOption(keySpecs);\r\n    actKeySpecs = helper.keySpecs().get(0).toString();\r\n    assertEquals(\"KeyFieldHelper's parsing is garbled\", eKeySpecs, actKeySpecs);\r\n    keySpecs = \"-k1.2nr,3.4\";\r\n    eKeySpecs = \"-k1.2,3.4nr\";\r\n    helper = new KeyFieldHelper();\r\n    helper.parseOption(keySpecs);\r\n    actKeySpecs = helper.keySpecs().get(0).toString();\r\n    assertEquals(\"KeyFieldHelper's parsing is garbled\", eKeySpecs, actKeySpecs);\r\n    keySpecs = \"-k1.2,3.4n\";\r\n    eKeySpecs = \"-k1.2,3.4n\";\r\n    helper = new KeyFieldHelper();\r\n    helper.parseOption(keySpecs);\r\n    actKeySpecs = helper.keySpecs().get(0).toString();\r\n    assertEquals(\"KeyFieldHelper's parsing is garbled\", eKeySpecs, actKeySpecs);\r\n    keySpecs = \"-k1.2,3.4r\";\r\n    eKeySpecs = \"-k1.2,3.4r\";\r\n    helper = new KeyFieldHelper();\r\n    helper.parseOption(keySpecs);\r\n    actKeySpecs = helper.keySpecs().get(0).toString();\r\n    assertEquals(\"KeyFieldHelper's parsing is garbled\", eKeySpecs, actKeySpecs);\r\n    keySpecs = \"-k1.2,3.4nr\";\r\n    eKeySpecs = \"-k1.2,3.4nr\";\r\n    helper = new KeyFieldHelper();\r\n    helper.parseOption(keySpecs);\r\n    actKeySpecs = helper.keySpecs().get(0).toString();\r\n    assertEquals(\"KeyFieldHelper's parsing is garbled\", eKeySpecs, actKeySpecs);\r\n    keySpecs = \"-nr -k1.2,3.4 -k5.6,7.8\";\r\n    eKeySpecs = \"-k1.2,3.4nr\";\r\n    helper = new KeyFieldHelper();\r\n    helper.parseOption(keySpecs);\r\n    actKeySpecs = helper.keySpecs().get(0).toString();\r\n    assertEquals(\"KeyFieldHelper's parsing is garbled\", eKeySpecs, actKeySpecs);\r\n    eKeySpecs = \"-k5.6,7.8nr\";\r\n    actKeySpecs = helper.keySpecs().get(1).toString();\r\n    assertEquals(\"KeyFieldHelper's parsing is garbled\", eKeySpecs, actKeySpecs);\r\n    keySpecs = \"-n -k1.2,3.4 -k5.6,7.8\";\r\n    eKeySpecs = \"-k1.2,3.4n\";\r\n    helper = new KeyFieldHelper();\r\n    helper.parseOption(keySpecs);\r\n    actKeySpecs = helper.keySpecs().get(0).toString();\r\n    assertEquals(\"KeyFieldHelper's parsing is garbled\", eKeySpecs, actKeySpecs);\r\n    eKeySpecs = \"-k5.6,7.8n\";\r\n    actKeySpecs = helper.keySpecs().get(1).toString();\r\n    assertEquals(\"KeyFieldHelper's parsing is garbled\", eKeySpecs, actKeySpecs);\r\n    keySpecs = \"-r -k1.2,3.4 -k5.6,7.8\";\r\n    eKeySpecs = \"-k1.2,3.4r\";\r\n    helper = new KeyFieldHelper();\r\n    helper.parseOption(keySpecs);\r\n    actKeySpecs = helper.keySpecs().get(0).toString();\r\n    assertEquals(\"KeyFieldHelper's parsing is garbled\", eKeySpecs, actKeySpecs);\r\n    eKeySpecs = \"-k5.6,7.8r\";\r\n    actKeySpecs = helper.keySpecs().get(1).toString();\r\n    assertEquals(\"KeyFieldHelper's parsing is garbled\", eKeySpecs, actKeySpecs);\r\n    keySpecs = \"-k1.2,3.4n -k5.6,7.8\";\r\n    eKeySpecs = \"-k1.2,3.4n\";\r\n    helper = new KeyFieldHelper();\r\n    helper.parseOption(keySpecs);\r\n    actKeySpecs = helper.keySpecs().get(0).toString();\r\n    assertEquals(\"KeyFieldHelper's parsing is garbled\", eKeySpecs, actKeySpecs);\r\n    eKeySpecs = \"-k5.6,7.8\";\r\n    actKeySpecs = helper.keySpecs().get(1).toString();\r\n    assertEquals(\"KeyFieldHelper's parsing is garbled\", eKeySpecs, actKeySpecs);\r\n    keySpecs = \"-k1.2,3.4r -k5.6,7.8\";\r\n    eKeySpecs = \"-k1.2,3.4r\";\r\n    helper = new KeyFieldHelper();\r\n    helper.parseOption(keySpecs);\r\n    actKeySpecs = helper.keySpecs().get(0).toString();\r\n    assertEquals(\"KeyFieldHelper's parsing is garbled\", eKeySpecs, actKeySpecs);\r\n    eKeySpecs = \"-k5.6,7.8\";\r\n    actKeySpecs = helper.keySpecs().get(1).toString();\r\n    assertEquals(\"KeyFieldHelper's parsing is garbled\", eKeySpecs, actKeySpecs);\r\n    keySpecs = \"-k1.2,3.4nr -k5.6,7.8\";\r\n    eKeySpecs = \"-k1.2,3.4nr\";\r\n    helper = new KeyFieldHelper();\r\n    helper.parseOption(keySpecs);\r\n    actKeySpecs = helper.keySpecs().get(0).toString();\r\n    assertEquals(\"KeyFieldHelper's parsing is garbled\", eKeySpecs, actKeySpecs);\r\n    eKeySpecs = \"-k5.6,7.8\";\r\n    actKeySpecs = helper.keySpecs().get(1).toString();\r\n    assertEquals(\"KeyFieldHelper's parsing is garbled\", eKeySpecs, actKeySpecs);\r\n    keySpecs = \"-n\";\r\n    eKeySpecs = \"-k1.1,0.0n\";\r\n    helper = new KeyFieldHelper();\r\n    helper.parseOption(keySpecs);\r\n    actKeySpecs = helper.keySpecs().get(0).toString();\r\n    assertEquals(\"KeyFieldHelper's parsing is garbled\", eKeySpecs, actKeySpecs);\r\n    keySpecs = \"-r\";\r\n    eKeySpecs = \"-k1.1,0.0r\";\r\n    helper = new KeyFieldHelper();\r\n    helper.parseOption(keySpecs);\r\n    actKeySpecs = helper.keySpecs().get(0).toString();\r\n    assertEquals(\"KeyFieldHelper's parsing is garbled\", eKeySpecs, actKeySpecs);\r\n    keySpecs = \"-nr\";\r\n    eKeySpecs = \"-k1.1,0.0nr\";\r\n    helper = new KeyFieldHelper();\r\n    helper.parseOption(keySpecs);\r\n    actKeySpecs = helper.keySpecs().get(0).toString();\r\n    assertEquals(\"KeyFieldHelper's parsing is garbled\", eKeySpecs, actKeySpecs);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\partition",
  "methodName" : "testGetWordLengths",
  "errType" : null,
  "containingMethodsNum" : 24,
  "sourceCodeText" : "void testGetWordLengths() throws Exception\n{\r\n    KeyFieldHelper helper = new KeyFieldHelper();\r\n    helper.setKeyFieldSeparator(\"\\t\");\r\n    String input = \"hi\";\r\n    int[] result = helper.getWordLengths(input.getBytes(), 0, 2);\r\n    assertTrue(equals(result, new int[] { 1 }));\r\n    helper.setKeyFieldSpec(1, 2);\r\n    input = \"hi\\thello there\";\r\n    result = helper.getWordLengths(input.getBytes(), 0, input.length());\r\n    assertTrue(equals(result, new int[] { 2, 2, 11 }));\r\n    helper.setKeyFieldSeparator(\" \");\r\n    input = \"hi hello\\tthere you\";\r\n    result = helper.getWordLengths(input.getBytes(), 0, input.length());\r\n    assertTrue(equals(result, new int[] { 3, 2, 11, 3 }));\r\n    input = \"hi hello there you where me there\";\r\n    result = helper.getWordLengths(input.getBytes(), 10, 33);\r\n    assertTrue(equals(result, new int[] { 5, 4, 3, 5, 2, 3 }));\r\n    input = \"hi hello there you where me \";\r\n    result = helper.getWordLengths(input.getBytes(), 10, input.length());\r\n    assertTrue(equals(result, new int[] { 5, 4, 3, 5, 2, 0 }));\r\n    input = \"\";\r\n    result = helper.getWordLengths(input.getBytes(), 0, 0);\r\n    assertTrue(equals(result, new int[] { 1, 0 }));\r\n    input = \"  abc\";\r\n    result = helper.getWordLengths(input.getBytes(), 0, 5);\r\n    assertTrue(equals(result, new int[] { 3, 0, 0, 3 }));\r\n    input = \"  abc\";\r\n    result = helper.getWordLengths(input.getBytes(), 0, 2);\r\n    assertTrue(equals(result, new int[] { 3, 0, 0, 0 }));\r\n    input = \" abc \";\r\n    result = helper.getWordLengths(input.getBytes(), 0, 2);\r\n    assertTrue(equals(result, new int[] { 2, 0, 1 }));\r\n    helper.setKeyFieldSeparator(\"abcd\");\r\n    input = \"abc\";\r\n    result = helper.getWordLengths(input.getBytes(), 0, 3);\r\n    assertTrue(equals(result, new int[] { 1, 3 }));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\partition",
  "methodName" : "testgetStartEndOffset",
  "errType" : null,
  "containingMethodsNum" : 39,
  "sourceCodeText" : "void testgetStartEndOffset() throws Exception\n{\r\n    KeyFieldHelper helper = new KeyFieldHelper();\r\n    helper.setKeyFieldSeparator(\"\\t\");\r\n    helper.setKeyFieldSpec(1, 2);\r\n    String input = \"hi\\thello\";\r\n    String expectedOutput = input;\r\n    testKeySpecs(input, expectedOutput, helper);\r\n    helper = new KeyFieldHelper();\r\n    helper.setKeyFieldSeparator(\"\\t\");\r\n    helper.parseOption(\"-k1.0,0\");\r\n    testKeySpecs(input, null, helper);\r\n    helper = new KeyFieldHelper();\r\n    helper.setKeyFieldSeparator(\"\\t\");\r\n    helper.parseOption(\"-k1,0\");\r\n    expectedOutput = input;\r\n    testKeySpecs(input, expectedOutput, helper);\r\n    helper = new KeyFieldHelper();\r\n    helper.setKeyFieldSeparator(\"\\t\");\r\n    helper.parseOption(\"-k1.2,0\");\r\n    expectedOutput = \"i\\thello\";\r\n    testKeySpecs(input, expectedOutput, helper);\r\n    helper = new KeyFieldHelper();\r\n    helper.setKeyFieldSeparator(\"\\t\");\r\n    helper.parseOption(\"-k1.1,2.3\");\r\n    expectedOutput = \"hi\\thel\";\r\n    testKeySpecs(input, expectedOutput, helper);\r\n    helper = new KeyFieldHelper();\r\n    helper.setKeyFieldSeparator(\"\\t\");\r\n    helper.parseOption(\"-k1.2,2.3\");\r\n    expectedOutput = \"i\\thel\";\r\n    testKeySpecs(input, expectedOutput, helper);\r\n    helper = new KeyFieldHelper();\r\n    helper.setKeyFieldSeparator(\"\\t\");\r\n    helper.parseOption(\"-k1.2,3.0\");\r\n    expectedOutput = \"i\\thello\";\r\n    testKeySpecs(input, expectedOutput, helper);\r\n    helper = new KeyFieldHelper();\r\n    helper.setKeyFieldSeparator(\"\\t\");\r\n    helper.parseOption(\"-k2,2\");\r\n    expectedOutput = \"hello\";\r\n    testKeySpecs(input, expectedOutput, helper);\r\n    helper = new KeyFieldHelper();\r\n    helper.setKeyFieldSeparator(\"\\t\");\r\n    helper.parseOption(\"-k3.1,4.0\");\r\n    testKeySpecs(input, null, helper);\r\n    helper = new KeyFieldHelper();\r\n    input = \"123123123123123hi\\thello\\thow\";\r\n    helper.setKeyFieldSeparator(\"\\t\");\r\n    helper.parseOption(\"-k2.1\");\r\n    expectedOutput = \"hello\\thow\";\r\n    testKeySpecs(input, expectedOutput, helper, 15, input.length());\r\n    helper = new KeyFieldHelper();\r\n    input = \"123123123123123hi\\thello\\t\\thow\\tare\";\r\n    helper.setKeyFieldSeparator(\"\\t\");\r\n    helper.parseOption(\"-k2.1,3\");\r\n    expectedOutput = \"hello\\t\";\r\n    testKeySpecs(input, expectedOutput, helper, 17, input.length());\r\n    helper = new KeyFieldHelper();\r\n    input = \"123123123123123hi\\thello\\thow\\tare\";\r\n    helper.setKeyFieldSeparator(\"\\t\");\r\n    helper.parseOption(\"-k2.1\");\r\n    expectedOutput = \"hello\\thow\\t\";\r\n    testKeySpecs(input, expectedOutput, helper, 17, 28);\r\n    helper = new KeyFieldHelper();\r\n    input = \"123123123123123hi\\thello\\thow\";\r\n    helper.setKeyFieldSeparator(\"\\t\");\r\n    helper.parseOption(\"-k2.1,3\");\r\n    expectedOutput = \"hello\";\r\n    testKeySpecs(input, expectedOutput, helper, 15, 23);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\partition",
  "methodName" : "testKeySpecs",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testKeySpecs(String input, String expectedOutput, KeyFieldHelper helper)\n{\r\n    testKeySpecs(input, expectedOutput, helper, 0, -1);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\partition",
  "methodName" : "testKeySpecs",
  "errType" : null,
  "containingMethodsNum" : 16,
  "sourceCodeText" : "void testKeySpecs(String input, String expectedOutput, KeyFieldHelper helper, int s1, int e1)\n{\r\n    LOG.info(\"input : \" + input);\r\n    String keySpecs = helper.keySpecs().get(0).toString();\r\n    LOG.info(\"keyspecs : \" + keySpecs);\r\n    byte[] inputBytes = input.getBytes();\r\n    if (e1 == -1) {\r\n        e1 = inputBytes.length;\r\n    }\r\n    LOG.info(\"length : \" + e1);\r\n    int[] indices = helper.getWordLengths(inputBytes, s1, e1);\r\n    int start = helper.getStartOffset(inputBytes, s1, e1, indices, helper.keySpecs().get(0));\r\n    LOG.info(\"start : \" + start);\r\n    if (expectedOutput == null) {\r\n        assertEquals(\"Expected -1 when the start index is invalid\", -1, start);\r\n        return;\r\n    }\r\n    int end = helper.getEndOffset(inputBytes, s1, e1, indices, helper.keySpecs().get(0));\r\n    LOG.info(\"end : \" + end);\r\n    end = (end >= inputBytes.length) ? inputBytes.length - 1 : end;\r\n    int length = end + 1 - start;\r\n    LOG.info(\"length : \" + length);\r\n    byte[] outputBytes = new byte[length];\r\n    System.arraycopy(inputBytes, start, outputBytes, 0, length);\r\n    String output = new String(outputBytes);\r\n    LOG.info(\"output : \" + output);\r\n    LOG.info(\"expected-output : \" + expectedOutput);\r\n    assertEquals(keySpecs + \" failed on input '\" + input + \"'\", expectedOutput, output);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\partition",
  "methodName" : "equals",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean equals(int[] test, int[] expected)\n{\r\n    if (test[0] != expected[0]) {\r\n        return false;\r\n    }\r\n    for (int i = 0; i < test[0] && i < expected[0]; ++i) {\r\n        if (test[i] != expected[i]) {\r\n            return false;\r\n        }\r\n    }\r\n    return true;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\util",
  "methodName" : "main",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void main(String[] args)\n{\r\n    try {\r\n        System.out.println(\"Creating file\" + args[0]);\r\n        FileOutputStream fstream = new FileOutputStream(args[0]);\r\n        fstream.write(\"Hello Hadoopers\".getBytes());\r\n        fstream.close();\r\n    } catch (IOException e) {\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "configure",
  "errType" : null,
  "containingMethodsNum" : 11,
  "sourceCodeText" : "void configure(JobConf conf) throws IOException\n{\r\n    conf.setJobName(\"TestCollect\");\r\n    conf.setJarByClass(TestCollect.class);\r\n    conf.setInputFormat(RandomInputFormat.class);\r\n    conf.setOutputKeyClass(IntWritable.class);\r\n    conf.setOutputValueClass(IntWritable.class);\r\n    FileOutputFormat.setOutputPath(conf, OUTPUT_DIR);\r\n    conf.set(MRConfig.FRAMEWORK_NAME, MRConfig.LOCAL_FRAMEWORK_NAME);\r\n    conf.setMapperClass(Map.class);\r\n    conf.setReducerClass(Reduce.class);\r\n    conf.setNumMapTasks(1);\r\n    conf.setNumReduceTasks(1);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "testCollect",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void testCollect() throws IOException\n{\r\n    JobConf conf = new JobConf();\r\n    configure(conf);\r\n    try {\r\n        JobClient.runJob(conf);\r\n        if (Reduce.numSeen != (NUM_COLLECTS_PER_THREAD * NUM_FEEDERS)) {\r\n            throw new IOException(\"Collect test failed!! Total does not match.\");\r\n        }\r\n    } catch (IOException ioe) {\r\n        throw ioe;\r\n    } finally {\r\n        FileSystem fs = FileSystem.get(conf);\r\n        fs.delete(OUTPUT_DIR, true);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\input",
  "methodName" : "testFormat",
  "errType" : null,
  "containingMethodsNum" : 45,
  "sourceCodeText" : "void testFormat() throws Exception\n{\r\n    Job job = Job.getInstance(new Configuration(defaultConf));\r\n    Path file = new Path(workDir, \"test.txt\");\r\n    int seed = new Random().nextInt();\r\n    LOG.info(\"seed = \" + seed);\r\n    Random random = new Random(seed);\r\n    localFs.delete(workDir, true);\r\n    FileInputFormat.setInputPaths(job, workDir);\r\n    final int MAX_LENGTH = 10000;\r\n    for (int length = 0; length < MAX_LENGTH; length += random.nextInt(MAX_LENGTH / 10) + 1) {\r\n        LOG.debug(\"creating; entries = \" + length);\r\n        Writer writer = new OutputStreamWriter(localFs.create(file));\r\n        try {\r\n            for (int i = 0; i < length; i++) {\r\n                writer.write(Integer.toString(i * 2));\r\n                writer.write(\"\\t\");\r\n                writer.write(Integer.toString(i));\r\n                writer.write(\"\\n\");\r\n            }\r\n        } finally {\r\n            writer.close();\r\n        }\r\n        KeyValueTextInputFormat format = new KeyValueTextInputFormat();\r\n        for (int i = 0; i < 3; i++) {\r\n            int numSplits = random.nextInt(MAX_LENGTH / 20) + 1;\r\n            LOG.debug(\"splitting: requesting = \" + numSplits);\r\n            List<InputSplit> splits = format.getSplits(job);\r\n            LOG.debug(\"splitting: got =        \" + splits.size());\r\n            BitSet bits = new BitSet(length);\r\n            for (int j = 0; j < splits.size(); j++) {\r\n                LOG.debug(\"split[\" + j + \"]= \" + splits.get(j));\r\n                TaskAttemptContext context = MapReduceTestUtil.createDummyMapTaskAttemptContext(job.getConfiguration());\r\n                RecordReader<Text, Text> reader = format.createRecordReader(splits.get(j), context);\r\n                Class<?> clazz = reader.getClass();\r\n                assertEquals(\"reader class is KeyValueLineRecordReader.\", KeyValueLineRecordReader.class, clazz);\r\n                MapContext<Text, Text, Text, Text> mcontext = new MapContextImpl<Text, Text, Text, Text>(job.getConfiguration(), context.getTaskAttemptID(), reader, null, null, MapReduceTestUtil.createDummyReporter(), splits.get(j));\r\n                reader.initialize(splits.get(j), mcontext);\r\n                Text key = null;\r\n                Text value = null;\r\n                try {\r\n                    int count = 0;\r\n                    while (reader.nextKeyValue()) {\r\n                        key = reader.getCurrentKey();\r\n                        clazz = key.getClass();\r\n                        assertEquals(\"Key class is Text.\", Text.class, clazz);\r\n                        value = reader.getCurrentValue();\r\n                        clazz = value.getClass();\r\n                        assertEquals(\"Value class is Text.\", Text.class, clazz);\r\n                        final int k = Integer.parseInt(key.toString());\r\n                        final int v = Integer.parseInt(value.toString());\r\n                        assertEquals(\"Bad key\", 0, k % 2);\r\n                        assertEquals(\"Mismatched key/value\", k / 2, v);\r\n                        LOG.debug(\"read \" + v);\r\n                        assertFalse(\"Key in multiple partitions.\", bits.get(v));\r\n                        bits.set(v);\r\n                        count++;\r\n                    }\r\n                    LOG.debug(\"splits[\" + j + \"]=\" + splits.get(j) + \" count=\" + count);\r\n                } finally {\r\n                    reader.close();\r\n                }\r\n            }\r\n            assertEquals(\"Some keys in no partition.\", length, bits.cardinality());\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 2,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\input",
  "methodName" : "testSplitableCodecs",
  "errType" : [ "ClassNotFoundException" ],
  "containingMethodsNum" : 46,
  "sourceCodeText" : "void testSplitableCodecs() throws Exception\n{\r\n    final Job job = Job.getInstance(defaultConf);\r\n    final Configuration conf = job.getConfiguration();\r\n    CompressionCodec codec = null;\r\n    try {\r\n        codec = (CompressionCodec) ReflectionUtils.newInstance(conf.getClassByName(\"org.apache.hadoop.io.compress.BZip2Codec\"), conf);\r\n    } catch (ClassNotFoundException cnfe) {\r\n        throw new IOException(\"Illegal codec!\");\r\n    }\r\n    Path file = new Path(workDir, \"test\" + codec.getDefaultExtension());\r\n    int seed = new Random().nextInt();\r\n    LOG.info(\"seed = \" + seed);\r\n    Random random = new Random(seed);\r\n    localFs.delete(workDir, true);\r\n    FileInputFormat.setInputPaths(job, workDir);\r\n    final int MAX_LENGTH = 500000;\r\n    FileInputFormat.setMaxInputSplitSize(job, MAX_LENGTH / 20);\r\n    for (int length = 0; length < MAX_LENGTH; length += random.nextInt(MAX_LENGTH / 4) + 1) {\r\n        LOG.info(\"creating; entries = \" + length);\r\n        Writer writer = new OutputStreamWriter(codec.createOutputStream(localFs.create(file)));\r\n        try {\r\n            for (int i = 0; i < length; i++) {\r\n                writer.write(Integer.toString(i * 2));\r\n                writer.write(\"\\t\");\r\n                writer.write(Integer.toString(i));\r\n                writer.write(\"\\n\");\r\n            }\r\n        } finally {\r\n            writer.close();\r\n        }\r\n        KeyValueTextInputFormat format = new KeyValueTextInputFormat();\r\n        assertTrue(\"KVTIF claims not splittable\", format.isSplitable(job, file));\r\n        for (int i = 0; i < 3; i++) {\r\n            int numSplits = random.nextInt(MAX_LENGTH / 2000) + 1;\r\n            LOG.info(\"splitting: requesting = \" + numSplits);\r\n            List<InputSplit> splits = format.getSplits(job);\r\n            LOG.info(\"splitting: got =        \" + splits.size());\r\n            BitSet bits = new BitSet(length);\r\n            for (int j = 0; j < splits.size(); j++) {\r\n                LOG.debug(\"split[\" + j + \"]= \" + splits.get(j));\r\n                TaskAttemptContext context = MapReduceTestUtil.createDummyMapTaskAttemptContext(job.getConfiguration());\r\n                RecordReader<Text, Text> reader = format.createRecordReader(splits.get(j), context);\r\n                Class<?> clazz = reader.getClass();\r\n                MapContext<Text, Text, Text, Text> mcontext = new MapContextImpl<Text, Text, Text, Text>(job.getConfiguration(), context.getTaskAttemptID(), reader, null, null, MapReduceTestUtil.createDummyReporter(), splits.get(j));\r\n                reader.initialize(splits.get(j), mcontext);\r\n                Text key = null;\r\n                Text value = null;\r\n                try {\r\n                    int count = 0;\r\n                    while (reader.nextKeyValue()) {\r\n                        key = reader.getCurrentKey();\r\n                        value = reader.getCurrentValue();\r\n                        final int k = Integer.parseInt(key.toString());\r\n                        final int v = Integer.parseInt(value.toString());\r\n                        assertEquals(\"Bad key\", 0, k % 2);\r\n                        assertEquals(\"Mismatched key/value\", k / 2, v);\r\n                        LOG.debug(\"read \" + k + \",\" + v);\r\n                        assertFalse(k + \",\" + v + \" in multiple partitions.\", bits.get(v));\r\n                        bits.set(v);\r\n                        count++;\r\n                    }\r\n                    if (count > 0) {\r\n                        LOG.info(\"splits[\" + j + \"]=\" + splits.get(j) + \" count=\" + count);\r\n                    } else {\r\n                        LOG.debug(\"splits[\" + j + \"]=\" + splits.get(j) + \" count=\" + count);\r\n                    }\r\n                } finally {\r\n                    reader.close();\r\n                }\r\n            }\r\n            assertEquals(\"Some keys in no partition.\", length, bits.cardinality());\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 3,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\input",
  "methodName" : "makeStream",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "LineReader makeStream(String str) throws IOException\n{\r\n    return new LineReader(new ByteArrayInputStream(str.getBytes(\"UTF-8\")), defaultConf);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\input",
  "methodName" : "testUTF8",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void testUTF8() throws Exception\n{\r\n    LineReader in = makeStream(\"abcd\\u20acbdcd\\u20ac\");\r\n    Text line = new Text();\r\n    in.readLine(line);\r\n    assertEquals(\"readLine changed utf8 characters\", \"abcd\\u20acbdcd\\u20ac\", line.toString());\r\n    in = makeStream(\"abc\\u200axyz\");\r\n    in.readLine(line);\r\n    assertEquals(\"split on fake newline\", \"abc\\u200axyz\", line.toString());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\input",
  "methodName" : "testNewLines",
  "errType" : null,
  "containingMethodsNum" : 14,
  "sourceCodeText" : "void testNewLines() throws Exception\n{\r\n    LineReader in = makeStream(\"a\\nbb\\n\\nccc\\rdddd\\r\\neeeee\");\r\n    Text out = new Text();\r\n    in.readLine(out);\r\n    assertEquals(\"line1 length\", 1, out.getLength());\r\n    in.readLine(out);\r\n    assertEquals(\"line2 length\", 2, out.getLength());\r\n    in.readLine(out);\r\n    assertEquals(\"line3 length\", 0, out.getLength());\r\n    in.readLine(out);\r\n    assertEquals(\"line4 length\", 3, out.getLength());\r\n    in.readLine(out);\r\n    assertEquals(\"line5 length\", 4, out.getLength());\r\n    in.readLine(out);\r\n    assertEquals(\"line5 length\", 5, out.getLength());\r\n    assertEquals(\"end of file\", 0, in.readLine(out));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\input",
  "methodName" : "writeFile",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void writeFile(FileSystem fs, Path name, CompressionCodec codec, String contents) throws IOException\n{\r\n    OutputStream stm;\r\n    if (codec == null) {\r\n        stm = fs.create(name);\r\n    } else {\r\n        stm = codec.createOutputStream(fs.create(name));\r\n    }\r\n    stm.write(contents.getBytes());\r\n    stm.close();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\input",
  "methodName" : "readSplit",
  "errType" : null,
  "containingMethodsNum" : 9,
  "sourceCodeText" : "List<Text> readSplit(KeyValueTextInputFormat format, InputSplit split, Job job) throws IOException, InterruptedException\n{\r\n    List<Text> result = new ArrayList<Text>();\r\n    Configuration conf = job.getConfiguration();\r\n    TaskAttemptContext context = MapReduceTestUtil.createDummyMapTaskAttemptContext(conf);\r\n    RecordReader<Text, Text> reader = format.createRecordReader(split, MapReduceTestUtil.createDummyMapTaskAttemptContext(conf));\r\n    MapContext<Text, Text, Text, Text> mcontext = new MapContextImpl<Text, Text, Text, Text>(conf, context.getTaskAttemptID(), reader, null, null, MapReduceTestUtil.createDummyReporter(), split);\r\n    reader.initialize(split, mcontext);\r\n    while (reader.nextKeyValue()) {\r\n        result.add(new Text(reader.getCurrentValue()));\r\n    }\r\n    reader.close();\r\n    return result;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\input",
  "methodName" : "testGzip",
  "errType" : null,
  "containingMethodsNum" : 24,
  "sourceCodeText" : "void testGzip() throws IOException, InterruptedException\n{\r\n    Configuration conf = new Configuration(defaultConf);\r\n    CompressionCodec gzip = new GzipCodec();\r\n    ReflectionUtils.setConf(gzip, conf);\r\n    localFs.delete(workDir, true);\r\n    writeFile(localFs, new Path(workDir, \"part1.txt.gz\"), gzip, \"line-1\\tthe quick\\nline-2\\tbrown\\nline-3\\t\" + \"fox jumped\\nline-4\\tover\\nline-5\\t the lazy\\nline-6\\t dog\\n\");\r\n    writeFile(localFs, new Path(workDir, \"part2.txt.gz\"), gzip, \"line-1\\tthis is a test\\nline-1\\tof gzip\\n\");\r\n    Job job = Job.getInstance(conf);\r\n    FileInputFormat.setInputPaths(job, workDir);\r\n    KeyValueTextInputFormat format = new KeyValueTextInputFormat();\r\n    List<InputSplit> splits = format.getSplits(job);\r\n    assertEquals(\"compressed splits == 2\", 2, splits.size());\r\n    FileSplit tmp = (FileSplit) splits.get(0);\r\n    if (tmp.getPath().getName().equals(\"part2.txt.gz\")) {\r\n        splits.set(0, splits.get(1));\r\n        splits.set(1, tmp);\r\n    }\r\n    List<Text> results = readSplit(format, splits.get(0), job);\r\n    assertEquals(\"splits[0] length\", 6, results.size());\r\n    assertEquals(\"splits[0][0]\", \"the quick\", results.get(0).toString());\r\n    assertEquals(\"splits[0][1]\", \"brown\", results.get(1).toString());\r\n    assertEquals(\"splits[0][2]\", \"fox jumped\", results.get(2).toString());\r\n    assertEquals(\"splits[0][3]\", \"over\", results.get(3).toString());\r\n    assertEquals(\"splits[0][4]\", \" the lazy\", results.get(4).toString());\r\n    assertEquals(\"splits[0][5]\", \" dog\", results.get(5).toString());\r\n    results = readSplit(format, splits.get(1), job);\r\n    assertEquals(\"splits[1] length\", 2, results.size());\r\n    assertEquals(\"splits[1][0]\", \"this is a test\", results.get(0).toString());\r\n    assertEquals(\"splits[1][1]\", \"of gzip\", results.get(1).toString());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\input",
  "methodName" : "main",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void main(String[] args) throws Exception\n{\r\n    new TestMRKeyValueTextInputFormat().testFormat();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "testJobSubmission",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void testJobSubmission() throws Exception\n{\r\n    JobConf conf = new JobConf();\r\n    Job job = new Job(conf);\r\n    job.setInputFormatClass(TestInputFormat.class);\r\n    job.setMapperClass(TestMapper.class);\r\n    job.setOutputFormatClass(TestOutputFormat.class);\r\n    job.setOutputKeyClass(IntWritable.class);\r\n    job.setOutputValueClass(IntWritable.class);\r\n    job.waitForCompletion(true);\r\n    assertTrue(job.isSuccessful());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "createRecordReader",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "RecordReader<IntWritable, IntWritable> createRecordReader(InputSplit split, TaskAttemptContext context) throws IOException, InterruptedException\n{\r\n    return new RecordReader<IntWritable, IntWritable>() {\r\n\r\n        private boolean done = false;\r\n\r\n        @Override\r\n        public void close() throws IOException {\r\n        }\r\n\r\n        @Override\r\n        public IntWritable getCurrentKey() throws IOException, InterruptedException {\r\n            return new IntWritable(0);\r\n        }\r\n\r\n        @Override\r\n        public IntWritable getCurrentValue() throws IOException, InterruptedException {\r\n            return new IntWritable(0);\r\n        }\r\n\r\n        @Override\r\n        public float getProgress() throws IOException, InterruptedException {\r\n            return done ? 0 : 1;\r\n        }\r\n\r\n        @Override\r\n        public void initialize(InputSplit split, TaskAttemptContext context) throws IOException, InterruptedException {\r\n        }\r\n\r\n        @Override\r\n        public boolean nextKeyValue() throws IOException, InterruptedException {\r\n            if (!done) {\r\n                done = true;\r\n                return true;\r\n            }\r\n            return false;\r\n        }\r\n    };\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "getSplits",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "List<InputSplit> getSplits(JobContext context) throws IOException, InterruptedException\n{\r\n    List<InputSplit> list = new ArrayList<InputSplit>();\r\n    list.add(new TestInputSplit());\r\n    return list;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "getLength",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getLength() throws IOException, InterruptedException\n{\r\n    return 1;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "getLocations",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String[] getLocations() throws IOException, InterruptedException\n{\r\n    String[] hosts = { \"localhost\" };\r\n    return hosts;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "readFields",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void readFields(DataInput in) throws IOException\n{\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "write",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void write(DataOutput out) throws IOException\n{\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "checkOutputSpecs",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void checkOutputSpecs(JobContext context) throws IOException, InterruptedException\n{\r\n    conf.setBoolean(TEST_CONFIG_NAME, true);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "getOutputCommitter",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "OutputCommitter getOutputCommitter(TaskAttemptContext context) throws IOException, InterruptedException\n{\r\n    return new OutputCommitter() {\r\n\r\n        @Override\r\n        public void abortTask(TaskAttemptContext taskContext) throws IOException {\r\n        }\r\n\r\n        @Override\r\n        public void commitTask(TaskAttemptContext taskContext) throws IOException {\r\n        }\r\n\r\n        @Override\r\n        public boolean needsTaskCommit(TaskAttemptContext taskContext) throws IOException {\r\n            return false;\r\n        }\r\n\r\n        @Override\r\n        public void setupJob(JobContext jobContext) throws IOException {\r\n        }\r\n\r\n        @Override\r\n        public void setupTask(TaskAttemptContext taskContext) throws IOException {\r\n        }\r\n    };\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "getRecordWriter",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "RecordWriter<IntWritable, IntWritable> getRecordWriter(TaskAttemptContext context) throws IOException, InterruptedException\n{\r\n    assertTrue(context.getConfiguration().getBoolean(TEST_CONFIG_NAME, false));\r\n    return new RecordWriter<IntWritable, IntWritable>() {\r\n\r\n        @Override\r\n        public void close(TaskAttemptContext context) throws IOException, InterruptedException {\r\n        }\r\n\r\n        @Override\r\n        public void write(IntWritable key, IntWritable value) throws IOException, InterruptedException {\r\n        }\r\n    };\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "getConf",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Configuration getConf()\n{\r\n    return conf;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "setConf",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setConf(Configuration conf)\n{\r\n    this.conf = conf;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "main",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void main(String[] args) throws Exception\n{\r\n    int res = ToolRunner.run(new Configuration(), new GrowingSleepJob(), args);\r\n    System.exit(res);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "createJob",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "Job createJob(int numMapper, int numReducer, long mapSleepTime, int mapSleepCount, long reduceSleepTime, int reduceSleepCount) throws IOException\n{\r\n    Job job = super.createJob(numMapper, numReducer, mapSleepTime, mapSleepCount, reduceSleepTime, reduceSleepCount);\r\n    job.setMapperClass(GrowingSleepMapper.class);\r\n    job.setJobName(\"Growing sleep job\");\r\n    return job;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\fs",
  "methodName" : "configure",
  "errType" : [ "Exception", "Exception" ],
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void configure(JobConf conf)\n{\r\n    setConf(conf);\r\n    try {\r\n        fs = FileSystem.get(conf);\r\n    } catch (Exception e) {\r\n        throw new RuntimeException(\"Cannot create file system.\", e);\r\n    }\r\n    bufferSize = conf.getInt(\"test.io.file.buffer.size\", 4096);\r\n    buffer = new byte[bufferSize];\r\n    try {\r\n        hostName = InetAddress.getLocalHost().getHostName();\r\n    } catch (Exception e) {\r\n        hostName = \"localhost\";\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 2,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\fs",
  "methodName" : "close",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void close() throws IOException\n{\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\fs",
  "methodName" : "doIO",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "T doIO(Reporter reporter, String name, long value) throws IOException",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\fs",
  "methodName" : "getIOStream",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Closeable getIOStream(String name) throws IOException\n{\r\n    return null;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\fs",
  "methodName" : "collectStats",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void collectStats(OutputCollector<Text, Text> output, String name, long execTime, T doIOReturnValue) throws IOException",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\fs",
  "methodName" : "map",
  "errType" : null,
  "containingMethodsNum" : 10,
  "sourceCodeText" : "void map(Text key, LongWritable value, OutputCollector<Text, Text> output, Reporter reporter) throws IOException\n{\r\n    String name = key.toString();\r\n    long longValue = value.get();\r\n    reporter.setStatus(\"starting \" + name + \" ::host = \" + hostName);\r\n    this.stream = getIOStream(name);\r\n    T statValue = null;\r\n    long tStart = System.currentTimeMillis();\r\n    try {\r\n        statValue = doIO(reporter, name, longValue);\r\n    } finally {\r\n        if (stream != null)\r\n            stream.close();\r\n    }\r\n    long tEnd = System.currentTimeMillis();\r\n    long execTime = tEnd - tStart;\r\n    collectStats(output, name, execTime, statValue);\r\n    reporter.setStatus(\"finished \" + name + \" ::host = \" + hostName);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred\\lib\\aggregate",
  "methodName" : "testAggregates",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testAggregates() throws Exception\n{\r\n    launch();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred\\lib\\aggregate",
  "methodName" : "launch",
  "errType" : null,
  "containingMethodsNum" : 43,
  "sourceCodeText" : "void launch() throws Exception\n{\r\n    JobConf conf = new JobConf(TestAggregates.class);\r\n    FileSystem fs = FileSystem.get(conf);\r\n    int numOfInputLines = 20;\r\n    String baseDir = System.getProperty(\"test.build.data\", \"build/test/data\");\r\n    Path OUTPUT_DIR = new Path(baseDir + \"/output_for_aggregates_test\");\r\n    Path INPUT_DIR = new Path(baseDir + \"/input_for_aggregates_test\");\r\n    String inputFile = \"input.txt\";\r\n    fs.delete(INPUT_DIR, true);\r\n    fs.mkdirs(INPUT_DIR);\r\n    fs.delete(OUTPUT_DIR, true);\r\n    StringBuffer inputData = new StringBuffer();\r\n    StringBuffer expectedOutput = new StringBuffer();\r\n    expectedOutput.append(\"max\\t19\\n\");\r\n    expectedOutput.append(\"min\\t1\\n\");\r\n    FSDataOutputStream fileOut = fs.create(new Path(INPUT_DIR, inputFile));\r\n    for (int i = 1; i < numOfInputLines; i++) {\r\n        expectedOutput.append(\"count_\").append(idFormat.format(i));\r\n        expectedOutput.append(\"\\t\").append(i).append(\"\\n\");\r\n        inputData.append(idFormat.format(i));\r\n        for (int j = 1; j < i; j++) {\r\n            inputData.append(\" \").append(idFormat.format(i));\r\n        }\r\n        inputData.append(\"\\n\");\r\n    }\r\n    expectedOutput.append(\"value_as_string_max\\t9\\n\");\r\n    expectedOutput.append(\"value_as_string_min\\t1\\n\");\r\n    expectedOutput.append(\"uniq_count\\t15\\n\");\r\n    fileOut.write(inputData.toString().getBytes(\"utf-8\"));\r\n    fileOut.close();\r\n    System.out.println(\"inputData:\");\r\n    System.out.println(inputData.toString());\r\n    JobConf job = new JobConf(conf, TestAggregates.class);\r\n    FileInputFormat.setInputPaths(job, INPUT_DIR);\r\n    job.setInputFormat(TextInputFormat.class);\r\n    FileOutputFormat.setOutputPath(job, OUTPUT_DIR);\r\n    job.setOutputFormat(TextOutputFormat.class);\r\n    job.setMapOutputKeyClass(Text.class);\r\n    job.setMapOutputValueClass(Text.class);\r\n    job.setOutputKeyClass(Text.class);\r\n    job.setOutputValueClass(Text.class);\r\n    job.setNumReduceTasks(1);\r\n    job.setMapperClass(ValueAggregatorMapper.class);\r\n    job.setReducerClass(ValueAggregatorReducer.class);\r\n    job.setCombinerClass(ValueAggregatorCombiner.class);\r\n    job.setInt(\"aggregator.descriptor.num\", 1);\r\n    job.set(\"aggregator.descriptor.0\", \"UserDefined,org.apache.hadoop.mapred.lib.aggregate.AggregatorTests\");\r\n    job.setLong(\"aggregate.max.num.unique.values\", 14);\r\n    JobClient.runJob(job);\r\n    boolean success = true;\r\n    Path outPath = new Path(OUTPUT_DIR, \"part-00000\");\r\n    String outdata = MapReduceTestUtil.readOutput(outPath, job);\r\n    System.out.println(\"full out data:\");\r\n    System.out.println(outdata.toString());\r\n    outdata = outdata.substring(0, expectedOutput.toString().length());\r\n    assertEquals(expectedOutput.toString(), outdata);\r\n    fs.delete(OUTPUT_DIR, true);\r\n    fs.delete(INPUT_DIR, true);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred\\lib\\aggregate",
  "methodName" : "main",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void main(String[] argv) throws Exception\n{\r\n    launch();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\input",
  "methodName" : "testSplitting",
  "errType" : null,
  "containingMethodsNum" : 20,
  "sourceCodeText" : "void testSplitting() throws Exception\n{\r\n    Job job = Job.getInstance();\r\n    MiniDFSCluster dfs = null;\r\n    try {\r\n        dfs = new MiniDFSCluster.Builder(job.getConfiguration()).numDataNodes(4).racks(new String[] { \"/rack0\", \"/rack0\", \"/rack1\", \"/rack1\" }).hosts(new String[] { \"host0\", \"host1\", \"host2\", \"host3\" }).build();\r\n        FileSystem fs = dfs.getFileSystem();\r\n        Path path = getPath(\"/foo/bar\", fs);\r\n        Path path2 = getPath(\"/foo/baz\", fs);\r\n        Path path3 = getPath(\"/bar/bar\", fs);\r\n        Path path4 = getPath(\"/bar/baz\", fs);\r\n        final int numSplits = 100;\r\n        FileInputFormat.setMaxInputSplitSize(job, fs.getFileStatus(path).getLen() / numSplits);\r\n        MultipleInputs.addInputPath(job, path, TextInputFormat.class, MapClass.class);\r\n        MultipleInputs.addInputPath(job, path2, TextInputFormat.class, MapClass2.class);\r\n        MultipleInputs.addInputPath(job, path3, KeyValueTextInputFormat.class, MapClass.class);\r\n        MultipleInputs.addInputPath(job, path4, TextInputFormat.class, MapClass2.class);\r\n        DelegatingInputFormat inFormat = new DelegatingInputFormat();\r\n        int[] bins = new int[3];\r\n        for (InputSplit split : (List<InputSplit>) inFormat.getSplits(job)) {\r\n            assertTrue(split instanceof TaggedInputSplit);\r\n            final TaggedInputSplit tis = (TaggedInputSplit) split;\r\n            int index = -1;\r\n            if (tis.getInputFormatClass().equals(KeyValueTextInputFormat.class)) {\r\n                index = 0;\r\n            } else if (tis.getMapperClass().equals(MapClass.class)) {\r\n                index = 1;\r\n            } else {\r\n                index = 2;\r\n            }\r\n            bins[index]++;\r\n        }\r\n        assertEquals(\"count is not equal to num splits\", numSplits, bins[0]);\r\n        assertEquals(\"count is not equal to num splits\", numSplits, bins[1]);\r\n        assertEquals(\"count is not equal to 2 * num splits\", numSplits * 2, bins[2]);\r\n    } finally {\r\n        if (dfs != null) {\r\n            dfs.shutdown();\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\input",
  "methodName" : "getPath",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "Path getPath(final String location, final FileSystem fs) throws IOException\n{\r\n    Path path = new Path(location);\r\n    DataOutputStream out = fs.create(path, true, 4096, (short) 2, 512, null);\r\n    for (int i = 0; i < 1000; ++i) {\r\n        out.writeChars(\"Hello\\n\");\r\n    }\r\n    out.close();\r\n    return path;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "printUsage",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "int printUsage()\n{\r\n    System.out.println(\"wordcount [-m <maps>] [-r <reduces>] <input> <output>\");\r\n    ToolRunner.printGenericCommandUsage(System.out);\r\n    return -1;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "run",
  "errType" : [ "NumberFormatException", "ArrayIndexOutOfBoundsException" ],
  "containingMethodsNum" : 22,
  "sourceCodeText" : "int run(String[] args) throws Exception\n{\r\n    JobConf conf = new JobConf(getConf(), WordCount.class);\r\n    conf.setJobName(\"wordcount\");\r\n    conf.setOutputKeyClass(Text.class);\r\n    conf.setOutputValueClass(IntWritable.class);\r\n    conf.setMapperClass(MapClass.class);\r\n    conf.setCombinerClass(Reduce.class);\r\n    conf.setReducerClass(Reduce.class);\r\n    List<String> other_args = new ArrayList<String>();\r\n    for (int i = 0; i < args.length; ++i) {\r\n        try {\r\n            if (\"-m\".equals(args[i])) {\r\n                conf.setNumMapTasks(Integer.parseInt(args[++i]));\r\n            } else if (\"-r\".equals(args[i])) {\r\n                conf.setNumReduceTasks(Integer.parseInt(args[++i]));\r\n            } else {\r\n                other_args.add(args[i]);\r\n            }\r\n        } catch (NumberFormatException except) {\r\n            System.out.println(\"ERROR: Integer expected instead of \" + args[i]);\r\n            return printUsage();\r\n        } catch (ArrayIndexOutOfBoundsException except) {\r\n            System.out.println(\"ERROR: Required parameter missing from \" + args[i - 1]);\r\n            return printUsage();\r\n        }\r\n    }\r\n    if (other_args.size() != 2) {\r\n        System.out.println(\"ERROR: Wrong number of parameters: \" + other_args.size() + \" instead of 2.\");\r\n        return printUsage();\r\n    }\r\n    FileInputFormat.setInputPaths(conf, other_args.get(0));\r\n    FileOutputFormat.setOutputPath(conf, new Path(other_args.get(1)));\r\n    JobClient.runJob(conf);\r\n    return 0;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "main",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void main(String[] args) throws Exception\n{\r\n    int res = ToolRunner.run(new Configuration(), new WordCount(), args);\r\n    System.exit(res);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2",
  "methodName" : "setUp",
  "errType" : null,
  "containingMethodsNum" : 24,
  "sourceCodeText" : "void setUp() throws Exception\n{\r\n    if (System.getProperty(\"hadoop.log.dir\") == null) {\r\n        System.setProperty(\"hadoop.log.dir\", \"/tmp\");\r\n    }\r\n    int taskTrackers = 2;\r\n    int dataNodes = 2;\r\n    String proxyUser = System.getProperty(\"user.name\");\r\n    String proxyGroup = \"g\";\r\n    StringBuilder sb = new StringBuilder();\r\n    sb.append(\"127.0.0.1,localhost\");\r\n    for (InetAddress i : InetAddress.getAllByName(InetAddress.getLocalHost().getHostName())) {\r\n        sb.append(\",\").append(i.getCanonicalHostName());\r\n    }\r\n    JobConf conf = new JobConf();\r\n    conf.set(\"dfs.block.access.token.enable\", \"false\");\r\n    conf.set(\"dfs.permissions\", \"true\");\r\n    conf.set(\"hadoop.security.authentication\", \"simple\");\r\n    conf.set(\"hadoop.proxyuser.\" + proxyUser + \".hosts\", sb.toString());\r\n    conf.set(\"hadoop.proxyuser.\" + proxyUser + \".groups\", proxyGroup);\r\n    String[] userGroups = new String[] { proxyGroup };\r\n    UserGroupInformation.createUserForTesting(proxyUser, userGroups);\r\n    UserGroupInformation.createUserForTesting(\"u1\", userGroups);\r\n    UserGroupInformation.createUserForTesting(\"u2\", new String[] { \"gg\" });\r\n    dfsCluster = new MiniDFSCluster.Builder(conf).numDataNodes(dataNodes).build();\r\n    FileSystem fileSystem = dfsCluster.getFileSystem();\r\n    fileSystem.mkdirs(new Path(\"/tmp\"));\r\n    fileSystem.mkdirs(new Path(\"/user\"));\r\n    fileSystem.mkdirs(new Path(\"/hadoop/mapred/system\"));\r\n    fileSystem.setPermission(new Path(\"/tmp\"), FsPermission.valueOf(\"-rwxrwxrwx\"));\r\n    fileSystem.setPermission(new Path(\"/user\"), FsPermission.valueOf(\"-rwxrwxrwx\"));\r\n    fileSystem.setPermission(new Path(\"/hadoop/mapred/system\"), FsPermission.valueOf(\"-rwx------\"));\r\n    String nnURI = fileSystem.getUri().toString();\r\n    int numDirs = 1;\r\n    String[] racks = null;\r\n    String[] hosts = null;\r\n    mrCluster = new MiniMRCluster(0, 0, taskTrackers, nnURI, numDirs, racks, hosts, null, conf);\r\n    ProxyUsers.refreshSuperUserGroupsConfiguration(conf);\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2",
  "methodName" : "getJobConf",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "JobConf getJobConf()\n{\r\n    return mrCluster.createJobConf();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2",
  "methodName" : "tearDown",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void tearDown() throws Exception\n{\r\n    if (mrCluster != null) {\r\n        mrCluster.shutdown();\r\n    }\r\n    if (dfsCluster != null) {\r\n        dfsCluster.shutdown();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2",
  "methodName" : "mrRun",
  "errType" : null,
  "containingMethodsNum" : 15,
  "sourceCodeText" : "void mrRun() throws Exception\n{\r\n    FileSystem fs = FileSystem.get(getJobConf());\r\n    Path inputDir = new Path(\"input\");\r\n    fs.mkdirs(inputDir);\r\n    Writer writer = new OutputStreamWriter(fs.create(new Path(inputDir, \"data.txt\")));\r\n    writer.write(\"hello\");\r\n    writer.close();\r\n    Path outputDir = new Path(\"output\", \"output\");\r\n    JobConf jobConf = new JobConf(getJobConf());\r\n    jobConf.setInt(\"mapred.map.tasks\", 1);\r\n    jobConf.setInt(\"mapred.map.max.attempts\", 1);\r\n    jobConf.setInt(\"mapred.reduce.max.attempts\", 1);\r\n    jobConf.set(\"mapred.input.dir\", inputDir.toString());\r\n    jobConf.set(\"mapred.output.dir\", outputDir.toString());\r\n    JobClient jobClient = new JobClient(jobConf);\r\n    RunningJob runJob = jobClient.submitJob(jobConf);\r\n    runJob.waitForCompletion();\r\n    assertTrue(runJob.isComplete());\r\n    assertTrue(runJob.isSuccessful());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2",
  "methodName" : "__testCurrentUser",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void __testCurrentUser() throws Exception\n{\r\n    mrRun();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2",
  "methodName" : "testValidProxyUser",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void testValidProxyUser() throws Exception\n{\r\n    UserGroupInformation ugi = UserGroupInformation.createProxyUser(\"u1\", UserGroupInformation.getLoginUser());\r\n    ugi.doAs(new PrivilegedExceptionAction<Void>() {\r\n\r\n        public Void run() throws Exception {\r\n            mrRun();\r\n            return null;\r\n        }\r\n    });\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2",
  "methodName" : "___testInvalidProxyUser",
  "errType" : [ "RemoteException", "Exception" ],
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void ___testInvalidProxyUser() throws Exception\n{\r\n    UserGroupInformation ugi = UserGroupInformation.createProxyUser(\"u2\", UserGroupInformation.getLoginUser());\r\n    ugi.doAs(new PrivilegedExceptionAction<Void>() {\r\n\r\n        public Void run() throws Exception {\r\n            try {\r\n                mrRun();\r\n                fail();\r\n            } catch (RemoteException ex) {\r\n            } catch (Exception ex) {\r\n                fail();\r\n            }\r\n            return null;\r\n        }\r\n    });\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\fs\\slive",
  "methodName" : "getLower",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "T getLower()\n{\r\n    return min;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\fs\\slive",
  "methodName" : "getUpper",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "T getUpper()\n{\r\n    return max;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\fs\\slive",
  "methodName" : "toString",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String toString()\n{\r\n    return min + SEP + max;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\fs\\slive",
  "methodName" : "betweenPositive",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "long betweenPositive(Random rnd, Range<Long> range)\n{\r\n    if (range.getLower().equals(range.getUpper())) {\r\n        return range.getLower();\r\n    }\r\n    long nextRnd = rnd.nextLong();\r\n    long normRange = (range.getUpper() - range.getLower() + 1);\r\n    return Math.abs(nextRnd % normRange) + range.getLower();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "validateFileCounters",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void validateFileCounters(Counters counter, long fileBytesRead, long fileBytesWritten, long mapOutputBytes, long mapOutputMaterializedBytes)\n{\r\n    assertTrue(counter.findCounter(FileInputFormatCounter.BYTES_READ).getValue() != 0);\r\n    assertEquals(fileBytesRead, counter.findCounter(FileInputFormatCounter.BYTES_READ).getValue());\r\n    assertTrue(counter.findCounter(FileOutputFormatCounter.BYTES_WRITTEN).getValue() != 0);\r\n    if (mapOutputBytes >= 0) {\r\n        assertTrue(counter.findCounter(TaskCounter.MAP_OUTPUT_BYTES).getValue() != 0);\r\n    }\r\n    if (mapOutputMaterializedBytes >= 0) {\r\n        assertTrue(counter.findCounter(TaskCounter.MAP_OUTPUT_MATERIALIZED_BYTES).getValue() != 0);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "validateOldFileCounters",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void validateOldFileCounters(Counters counter, long fileBytesRead, long fileBytesWritten, long mapOutputBytes, long mapOutputMaterializedBytes)\n{\r\n    assertEquals(fileBytesRead, counter.findCounter(FileInputFormat.Counter.BYTES_READ).getValue());\r\n    assertEquals(fileBytesRead, counter.findCounter(org.apache.hadoop.mapreduce.lib.input.FileInputFormat.Counter.BYTES_READ).getValue());\r\n    assertEquals(fileBytesWritten, counter.findCounter(FileOutputFormat.Counter.BYTES_WRITTEN).getValue());\r\n    assertEquals(fileBytesWritten, counter.findCounter(org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.Counter.BYTES_WRITTEN).getValue());\r\n    if (mapOutputBytes >= 0) {\r\n        assertTrue(counter.findCounter(TaskCounter.MAP_OUTPUT_BYTES).getValue() != 0);\r\n    }\r\n    if (mapOutputMaterializedBytes >= 0) {\r\n        assertTrue(counter.findCounter(TaskCounter.MAP_OUTPUT_MATERIALIZED_BYTES).getValue() != 0);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "validateCounters",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void validateCounters(Counters counter, long spillRecCnt, long mapInputRecords, long mapOutputRecords)\n{\r\n    assertEquals(spillRecCnt, counter.findCounter(TaskCounter.SPILLED_RECORDS).getCounter());\r\n    assertEquals(mapInputRecords, counter.findCounter(TaskCounter.MAP_INPUT_RECORDS).getCounter());\r\n    assertEquals(mapOutputRecords, counter.findCounter(TaskCounter.MAP_OUTPUT_RECORDS).getCounter());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "removeWordsFile",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void removeWordsFile(Path inpFile, Configuration conf) throws IOException\n{\r\n    final FileSystem fs = inpFile.getFileSystem(conf);\r\n    if (fs.exists(inpFile) && !fs.delete(inpFile, false)) {\r\n        throw new IOException(\"Failed to delete \" + inpFile);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "createWordsFile",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void createWordsFile(Path inpFile, Configuration conf) throws IOException\n{\r\n    final FileSystem fs = inpFile.getFileSystem(conf);\r\n    if (fs.exists(inpFile)) {\r\n        return;\r\n    }\r\n    FSDataOutputStream out = fs.create(inpFile);\r\n    try {\r\n        int REPLICAS = 5, NUMLINES = 1024, NUMWORDSPERLINE = 4;\r\n        final String WORD = \"zymurgy\";\r\n        final Formatter fmt = new Formatter(new StringBuilder());\r\n        for (int i = 0; i < REPLICAS; i++) {\r\n            for (int j = 1; j <= NUMLINES * NUMWORDSPERLINE; j += NUMWORDSPERLINE) {\r\n                ((StringBuilder) fmt.out()).setLength(0);\r\n                for (int k = 0; k < NUMWORDSPERLINE; ++k) {\r\n                    fmt.format(\"%s%04d \", WORD, j + k);\r\n                }\r\n                ((StringBuilder) fmt.out()).append(\"\\n\");\r\n                out.writeBytes(fmt.toString());\r\n            }\r\n        }\r\n    } finally {\r\n        out.close();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getFileSize",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "long getFileSize(Path path) throws IOException\n{\r\n    FileSystem fs = FileSystem.getLocal(new Configuration());\r\n    long len = 0;\r\n    len += fs.getFileStatus(path).getLen();\r\n    Path crcPath = new Path(path.getParent(), \".\" + path.getName() + \".crc\");\r\n    if (fs.exists(crcPath)) {\r\n        len += fs.getFileStatus(crcPath).getLen();\r\n    }\r\n    return len;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "initPaths",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void initPaths() throws IOException\n{\r\n    final Configuration conf = new Configuration();\r\n    final Path TEST_ROOT_DIR = new Path(System.getProperty(\"test.build.data\", \"/tmp\"));\r\n    testdir = new Path(TEST_ROOT_DIR, \"spilledRecords.countertest\");\r\n    IN_DIR = new Path(testdir, \"in\");\r\n    OUT_DIR = new Path(testdir, \"out\");\r\n    FileSystem fs = FileSystem.getLocal(conf);\r\n    testdir = new Path(TEST_ROOT_DIR, \"spilledRecords.countertest\");\r\n    if (fs.exists(testdir) && !fs.delete(testdir, true)) {\r\n        throw new IOException(\"Could not delete \" + testdir);\r\n    }\r\n    if (!fs.mkdirs(IN_DIR)) {\r\n        throw new IOException(\"Mkdirs failed to create \" + IN_DIR);\r\n    }\r\n    for (int i = 0; i < inFiles.length; i++) {\r\n        inFiles[i] = new Path(IN_DIR, \"input5_2k_\" + i);\r\n    }\r\n    createWordsFile(inFiles[0], conf);\r\n    createWordsFile(inFiles[1], conf);\r\n    createWordsFile(inFiles[2], conf);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "cleanup",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void cleanup() throws IOException\n{\r\n    final Configuration conf = new Configuration();\r\n    final FileSystem fs = testdir.getFileSystem(conf);\r\n    if (fs.exists(testdir)) {\r\n        fs.delete(testdir, true);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "createConfiguration",
  "errType" : null,
  "containingMethodsNum" : 9,
  "sourceCodeText" : "JobConf createConfiguration() throws IOException\n{\r\n    JobConf baseConf = new JobConf(TestJobCounters.class);\r\n    baseConf.setOutputKeyClass(Text.class);\r\n    baseConf.setOutputValueClass(IntWritable.class);\r\n    baseConf.setMapperClass(WordCount.MapClass.class);\r\n    baseConf.setCombinerClass(WordCount.Reduce.class);\r\n    baseConf.setReducerClass(WordCount.Reduce.class);\r\n    baseConf.setNumReduceTasks(1);\r\n    baseConf.setInt(JobContext.IO_SORT_MB, 1);\r\n    baseConf.set(JobContext.MAP_SORT_SPILL_PERCENT, \"0.50\");\r\n    baseConf.setInt(JobContext.MAP_COMBINE_MIN_SPILLS, 3);\r\n    return baseConf;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "createJob",
  "errType" : null,
  "containingMethodsNum" : 11,
  "sourceCodeText" : "Job createJob() throws IOException\n{\r\n    final Configuration conf = new Configuration();\r\n    final Job baseJob = Job.getInstance(conf);\r\n    baseJob.setOutputKeyClass(Text.class);\r\n    baseJob.setOutputValueClass(IntWritable.class);\r\n    baseJob.setMapperClass(NewMapTokenizer.class);\r\n    baseJob.setCombinerClass(NewSummer.class);\r\n    baseJob.setReducerClass(NewSummer.class);\r\n    baseJob.setNumReduceTasks(1);\r\n    baseJob.getConfiguration().setInt(JobContext.IO_SORT_MB, 1);\r\n    baseJob.getConfiguration().set(JobContext.MAP_SORT_SPILL_PERCENT, \"0.50\");\r\n    baseJob.getConfiguration().setInt(JobContext.MAP_COMBINE_MIN_SPILLS, 3);\r\n    org.apache.hadoop.mapreduce.lib.input.FileInputFormat.setMinInputSplitSize(baseJob, Long.MAX_VALUE);\r\n    return baseJob;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "testOldCounterA",
  "errType" : null,
  "containingMethodsNum" : 15,
  "sourceCodeText" : "void testOldCounterA() throws Exception\n{\r\n    JobConf conf = createConfiguration();\r\n    conf.setNumMapTasks(3);\r\n    conf.setInt(JobContext.IO_SORT_FACTOR, 2);\r\n    removeWordsFile(inFiles[3], conf);\r\n    removeWordsFile(inFiles[4], conf);\r\n    long inputSize = 0;\r\n    inputSize += getFileSize(inFiles[0]);\r\n    inputSize += getFileSize(inFiles[1]);\r\n    inputSize += getFileSize(inFiles[2]);\r\n    FileInputFormat.setInputPaths(conf, IN_DIR);\r\n    FileOutputFormat.setOutputPath(conf, new Path(OUT_DIR, \"outputO0\"));\r\n    RunningJob myJob = JobClient.runJob(conf);\r\n    Counters c1 = myJob.getCounters();\r\n    validateCounters(c1, 73728, 15360, 61440);\r\n    validateFileCounters(c1, inputSize, 0, 0, 0);\r\n    validateOldFileCounters(c1, inputSize, 61928, 0, 0);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "testOldCounterB",
  "errType" : null,
  "containingMethodsNum" : 15,
  "sourceCodeText" : "void testOldCounterB() throws Exception\n{\r\n    JobConf conf = createConfiguration();\r\n    createWordsFile(inFiles[3], conf);\r\n    removeWordsFile(inFiles[4], conf);\r\n    long inputSize = 0;\r\n    inputSize += getFileSize(inFiles[0]);\r\n    inputSize += getFileSize(inFiles[1]);\r\n    inputSize += getFileSize(inFiles[2]);\r\n    inputSize += getFileSize(inFiles[3]);\r\n    conf.setNumMapTasks(4);\r\n    conf.setInt(JobContext.IO_SORT_FACTOR, 2);\r\n    FileInputFormat.setInputPaths(conf, IN_DIR);\r\n    FileOutputFormat.setOutputPath(conf, new Path(OUT_DIR, \"outputO1\"));\r\n    RunningJob myJob = JobClient.runJob(conf);\r\n    Counters c1 = myJob.getCounters();\r\n    validateCounters(c1, 98304, 20480, 81920);\r\n    validateFileCounters(c1, inputSize, 0, 0, 0);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "testOldCounterC",
  "errType" : null,
  "containingMethodsNum" : 16,
  "sourceCodeText" : "void testOldCounterC() throws Exception\n{\r\n    JobConf conf = createConfiguration();\r\n    createWordsFile(inFiles[3], conf);\r\n    createWordsFile(inFiles[4], conf);\r\n    long inputSize = 0;\r\n    inputSize += getFileSize(inFiles[0]);\r\n    inputSize += getFileSize(inFiles[1]);\r\n    inputSize += getFileSize(inFiles[2]);\r\n    inputSize += getFileSize(inFiles[3]);\r\n    inputSize += getFileSize(inFiles[4]);\r\n    conf.setNumMapTasks(4);\r\n    conf.setInt(JobContext.IO_SORT_FACTOR, 3);\r\n    FileInputFormat.setInputPaths(conf, IN_DIR);\r\n    FileOutputFormat.setOutputPath(conf, new Path(OUT_DIR, \"outputO2\"));\r\n    RunningJob myJob = JobClient.runJob(conf);\r\n    Counters c1 = myJob.getCounters();\r\n    validateCounters(c1, 122880, 25600, 102400);\r\n    validateFileCounters(c1, inputSize, 0, 0, 0);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "testOldCounterD",
  "errType" : null,
  "containingMethodsNum" : 15,
  "sourceCodeText" : "void testOldCounterD() throws Exception\n{\r\n    JobConf conf = createConfiguration();\r\n    conf.setNumMapTasks(3);\r\n    conf.setInt(JobContext.IO_SORT_FACTOR, 2);\r\n    conf.setNumReduceTasks(0);\r\n    removeWordsFile(inFiles[3], conf);\r\n    removeWordsFile(inFiles[4], conf);\r\n    long inputSize = 0;\r\n    inputSize += getFileSize(inFiles[0]);\r\n    inputSize += getFileSize(inFiles[1]);\r\n    inputSize += getFileSize(inFiles[2]);\r\n    FileInputFormat.setInputPaths(conf, IN_DIR);\r\n    FileOutputFormat.setOutputPath(conf, new Path(OUT_DIR, \"outputO3\"));\r\n    RunningJob myJob = JobClient.runJob(conf);\r\n    Counters c1 = myJob.getCounters();\r\n    validateCounters(c1, 0, 15360, 61440);\r\n    validateFileCounters(c1, inputSize, 0, -1, -1);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "testNewCounterA",
  "errType" : null,
  "containingMethodsNum" : 14,
  "sourceCodeText" : "void testNewCounterA() throws Exception\n{\r\n    final Job job = createJob();\r\n    final Configuration conf = job.getConfiguration();\r\n    conf.setInt(JobContext.IO_SORT_FACTOR, 2);\r\n    removeWordsFile(inFiles[3], conf);\r\n    removeWordsFile(inFiles[4], conf);\r\n    long inputSize = 0;\r\n    inputSize += getFileSize(inFiles[0]);\r\n    inputSize += getFileSize(inFiles[1]);\r\n    inputSize += getFileSize(inFiles[2]);\r\n    org.apache.hadoop.mapreduce.lib.input.FileInputFormat.setInputPaths(job, IN_DIR);\r\n    org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setOutputPath(job, new Path(OUT_DIR, \"outputN0\"));\r\n    assertTrue(job.waitForCompletion(true));\r\n    final Counters c1 = Counters.downgrade(job.getCounters());\r\n    validateCounters(c1, 73728, 15360, 61440);\r\n    validateFileCounters(c1, inputSize, 0, 0, 0);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "testNewCounterB",
  "errType" : null,
  "containingMethodsNum" : 15,
  "sourceCodeText" : "void testNewCounterB() throws Exception\n{\r\n    final Job job = createJob();\r\n    final Configuration conf = job.getConfiguration();\r\n    conf.setInt(JobContext.IO_SORT_FACTOR, 2);\r\n    createWordsFile(inFiles[3], conf);\r\n    removeWordsFile(inFiles[4], conf);\r\n    long inputSize = 0;\r\n    inputSize += getFileSize(inFiles[0]);\r\n    inputSize += getFileSize(inFiles[1]);\r\n    inputSize += getFileSize(inFiles[2]);\r\n    inputSize += getFileSize(inFiles[3]);\r\n    org.apache.hadoop.mapreduce.lib.input.FileInputFormat.setInputPaths(job, IN_DIR);\r\n    org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setOutputPath(job, new Path(OUT_DIR, \"outputN1\"));\r\n    assertTrue(job.waitForCompletion(true));\r\n    final Counters c1 = Counters.downgrade(job.getCounters());\r\n    validateCounters(c1, 98304, 20480, 81920);\r\n    validateFileCounters(c1, inputSize, 0, 0, 0);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "testNewCounterC",
  "errType" : null,
  "containingMethodsNum" : 16,
  "sourceCodeText" : "void testNewCounterC() throws Exception\n{\r\n    final Job job = createJob();\r\n    final Configuration conf = job.getConfiguration();\r\n    conf.setInt(JobContext.IO_SORT_FACTOR, 3);\r\n    createWordsFile(inFiles[3], conf);\r\n    createWordsFile(inFiles[4], conf);\r\n    long inputSize = 0;\r\n    inputSize += getFileSize(inFiles[0]);\r\n    inputSize += getFileSize(inFiles[1]);\r\n    inputSize += getFileSize(inFiles[2]);\r\n    inputSize += getFileSize(inFiles[3]);\r\n    inputSize += getFileSize(inFiles[4]);\r\n    org.apache.hadoop.mapreduce.lib.input.FileInputFormat.setInputPaths(job, IN_DIR);\r\n    org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setOutputPath(job, new Path(OUT_DIR, \"outputN2\"));\r\n    assertTrue(job.waitForCompletion(true));\r\n    final Counters c1 = Counters.downgrade(job.getCounters());\r\n    validateCounters(c1, 122880, 25600, 102400);\r\n    validateFileCounters(c1, inputSize, 0, 0, 0);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "testNewCounterD",
  "errType" : null,
  "containingMethodsNum" : 15,
  "sourceCodeText" : "void testNewCounterD() throws Exception\n{\r\n    final Job job = createJob();\r\n    final Configuration conf = job.getConfiguration();\r\n    conf.setInt(JobContext.IO_SORT_FACTOR, 2);\r\n    job.setNumReduceTasks(0);\r\n    removeWordsFile(inFiles[3], conf);\r\n    removeWordsFile(inFiles[4], conf);\r\n    long inputSize = 0;\r\n    inputSize += getFileSize(inFiles[0]);\r\n    inputSize += getFileSize(inFiles[1]);\r\n    inputSize += getFileSize(inFiles[2]);\r\n    org.apache.hadoop.mapreduce.lib.input.FileInputFormat.setInputPaths(job, IN_DIR);\r\n    org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setOutputPath(job, new Path(OUT_DIR, \"outputN3\"));\r\n    assertTrue(job.waitForCompletion(true));\r\n    final Counters c1 = Counters.downgrade(job.getCounters());\r\n    validateCounters(c1, 0, 15360, 61440);\r\n    validateFileCounters(c1, inputSize, 0, -1, -1);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "testOldCounters",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void testOldCounters() throws Exception\n{\r\n    Counters c1 = new Counters();\r\n    c1.incrCounter(FileInputFormat.Counter.BYTES_READ, 100);\r\n    c1.incrCounter(FileOutputFormat.Counter.BYTES_WRITTEN, 200);\r\n    c1.incrCounter(TaskCounter.MAP_OUTPUT_BYTES, 100);\r\n    c1.incrCounter(TaskCounter.MAP_OUTPUT_MATERIALIZED_BYTES, 100);\r\n    validateFileCounters(c1, 100, 200, 100, 100);\r\n    validateOldFileCounters(c1, 100, 200, 100, 100);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getTaskCounterUsage",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "long getTaskCounterUsage(JobClient client, JobID id, int numReports, int taskId, TaskType type) throws Exception\n{\r\n    TaskReport[] reports = null;\r\n    if (TaskType.MAP.equals(type)) {\r\n        reports = client.getMapTaskReports(id);\r\n    } else if (TaskType.REDUCE.equals(type)) {\r\n        reports = client.getReduceTaskReports(id);\r\n    }\r\n    assertNotNull(\"No reports found for task type '\" + type.name() + \"' in job \" + id, reports);\r\n    assertEquals(\"Mismatch in task id\", numReports, reports.length);\r\n    Counters counters = reports[taskId].getCounters();\r\n    return counters.getCounter(TaskCounter.COMMITTED_HEAP_BYTES);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "runHeapUsageTestJob",
  "errType" : null,
  "containingMethodsNum" : 20,
  "sourceCodeText" : "RunningJob runHeapUsageTestJob(JobConf conf, Path testRootDir, String heapOptions, long targetMapValue, long targetReduceValue, FileSystem fs, JobClient client, Path inDir) throws IOException\n{\r\n    JobConf jobConf = new JobConf(conf);\r\n    jobConf.setNumMapTasks(1);\r\n    jobConf.setNumReduceTasks(1);\r\n    jobConf.setMapperClass(MemoryLoaderMapper.class);\r\n    jobConf.setReducerClass(MemoryLoaderReducer.class);\r\n    jobConf.setInputFormat(TextInputFormat.class);\r\n    jobConf.setOutputKeyClass(LongWritable.class);\r\n    jobConf.setOutputValueClass(Text.class);\r\n    jobConf.setMaxMapAttempts(1);\r\n    jobConf.setMaxReduceAttempts(1);\r\n    jobConf.set(JobConf.MAPRED_MAP_TASK_JAVA_OPTS, heapOptions);\r\n    jobConf.set(JobConf.MAPRED_REDUCE_TASK_JAVA_OPTS, heapOptions);\r\n    jobConf.setLong(MemoryLoaderMapper.TARGET_VALUE, targetMapValue);\r\n    jobConf.setLong(MemoryLoaderReducer.TARGET_VALUE, targetReduceValue);\r\n    FileInputFormat.setInputPaths(jobConf, inDir);\r\n    Path outDir = new Path(testRootDir, \"out\");\r\n    fs.delete(outDir, true);\r\n    FileOutputFormat.setOutputPath(jobConf, outDir);\r\n    RunningJob job = client.submitJob(jobConf);\r\n    job.waitForCompletion();\r\n    JobID jobID = job.getID();\r\n    assertTrue(\"Job \" + jobID + \" failed!\", job.isSuccessful());\r\n    return job;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "testHeapUsageCounter",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 24,
  "sourceCodeText" : "void testHeapUsageCounter() throws Exception\n{\r\n    JobConf conf = new JobConf();\r\n    FileSystem fileSystem = FileSystem.getLocal(conf);\r\n    Path rootDir = new Path(System.getProperty(\"test.build.data\", \"/tmp\"));\r\n    Path testRootDir = new Path(rootDir, \"testHeapUsageCounter\");\r\n    fileSystem.delete(testRootDir, true);\r\n    fileSystem.setWorkingDirectory(testRootDir);\r\n    fileSystem.deleteOnExit(testRootDir);\r\n    MiniMRCluster mrCluster = new MiniMRCluster(1, fileSystem.getUri().toString(), 1);\r\n    try {\r\n        conf = mrCluster.createJobConf();\r\n        JobClient jobClient = new JobClient(conf);\r\n        Path inDir = new Path(testRootDir, \"in\");\r\n        createWordsFile(inDir, conf);\r\n        RunningJob lowMemJob = runHeapUsageTestJob(conf, testRootDir, \"-Xms32m -Xmx1G\", 0, 0, fileSystem, jobClient, inDir);\r\n        JobID lowMemJobID = lowMemJob.getID();\r\n        long lowMemJobMapHeapUsage = getTaskCounterUsage(jobClient, lowMemJobID, 1, 0, TaskType.MAP);\r\n        System.out.println(\"Job1 (low memory job) map task heap usage: \" + lowMemJobMapHeapUsage);\r\n        long lowMemJobReduceHeapUsage = getTaskCounterUsage(jobClient, lowMemJobID, 1, 0, TaskType.REDUCE);\r\n        System.out.println(\"Job1 (low memory job) reduce task heap usage: \" + lowMemJobReduceHeapUsage);\r\n        RunningJob highMemJob = runHeapUsageTestJob(conf, testRootDir, \"-Xms32m -Xmx1G\", lowMemJobMapHeapUsage + 256 * 1024 * 1024, lowMemJobReduceHeapUsage + 256 * 1024 * 1024, fileSystem, jobClient, inDir);\r\n        JobID highMemJobID = highMemJob.getID();\r\n        long highMemJobMapHeapUsage = getTaskCounterUsage(jobClient, highMemJobID, 1, 0, TaskType.MAP);\r\n        System.out.println(\"Job2 (high memory job) map task heap usage: \" + highMemJobMapHeapUsage);\r\n        long highMemJobReduceHeapUsage = getTaskCounterUsage(jobClient, highMemJobID, 1, 0, TaskType.REDUCE);\r\n        System.out.println(\"Job2 (high memory job) reduce task heap usage: \" + highMemJobReduceHeapUsage);\r\n        assertTrue(\"Incorrect map heap usage reported by the map task\", lowMemJobMapHeapUsage < highMemJobMapHeapUsage);\r\n        assertTrue(\"Incorrect reduce heap usage reported by the reduce task\", lowMemJobReduceHeapUsage < highMemJobReduceHeapUsage);\r\n    } finally {\r\n        mrCluster.shutdown();\r\n        try {\r\n            fileSystem.delete(testRootDir, true);\r\n        } catch (IOException ioe) {\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 2,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "testMockResourceCalculatorProcessTree",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void testMockResourceCalculatorProcessTree()\n{\r\n    ResourceCalculatorProcessTree tree;\r\n    tree = ResourceCalculatorProcessTree.getResourceCalculatorProcessTree(\"1\", TestJobCounters.MockResourceCalculatorProcessTree.class, new Configuration());\r\n    assertNotNull(tree);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "testMaxCounter",
  "errType" : null,
  "containingMethodsNum" : 38,
  "sourceCodeText" : "void testMaxCounter() throws IOException, ClassNotFoundException, InterruptedException\n{\r\n    MiniMRClientCluster mrCluster = MiniMRClientClusterFactory.create(this.getClass(), 2, new Configuration());\r\n    try {\r\n        Path rootDir = new Path(System.getProperty(\"test.build.data\", \"/tmp\"));\r\n        Path testRootDir = new Path(rootDir, \"testMaxCounter\");\r\n        Path testInputDir = new Path(testRootDir, \"input\");\r\n        Path testOutputDir = new Path(testRootDir, \"output\");\r\n        FileSystem fs = FileSystem.getLocal(new Configuration());\r\n        fs.mkdirs(testInputDir);\r\n        Path testInputFile = new Path(testInputDir, \"file01\");\r\n        FSDataOutputStream stream = fs.create(testInputFile);\r\n        stream.writeChars(\"foo\");\r\n        stream.writeChars(\"bar\");\r\n        stream.close();\r\n        fs.delete(testOutputDir, true);\r\n        Configuration conf = new Configuration();\r\n        conf.setClass(MRConfig.RESOURCE_CALCULATOR_PROCESS_TREE, MockResourceCalculatorProcessTree.class, ResourceCalculatorProcessTree.class);\r\n        Job job = Job.getInstance(conf, \"word count\");\r\n        job.setJarByClass(WordCount.class);\r\n        job.setMapperClass(TokenizerMapper.class);\r\n        job.setCombinerClass(IntSumReducer.class);\r\n        job.setReducerClass(IntSumReducer.class);\r\n        job.setOutputKeyClass(Text.class);\r\n        job.setOutputValueClass(IntWritable.class);\r\n        job.setNumReduceTasks(2);\r\n        org.apache.hadoop.mapreduce.lib.input.FileInputFormat.addInputPath(job, testInputDir);\r\n        org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setOutputPath(job, testOutputDir);\r\n        assertTrue(job.waitForCompletion(true));\r\n        org.apache.hadoop.mapreduce.Counter maxMap = job.getCounters().findCounter(TaskCounter.MAP_PHYSICAL_MEMORY_BYTES_MAX);\r\n        org.apache.hadoop.mapreduce.Counter maxReduce = job.getCounters().findCounter(TaskCounter.REDUCE_PHYSICAL_MEMORY_BYTES_MAX);\r\n        org.apache.hadoop.mapreduce.Counter allP = job.getCounters().findCounter(TaskCounter.PHYSICAL_MEMORY_BYTES);\r\n        assertEquals(1024, maxMap.getValue());\r\n        assertEquals(1024, maxReduce.getValue());\r\n        assertEquals(3072, allP.getValue());\r\n        org.apache.hadoop.mapreduce.Counter maxMapV = job.getCounters().findCounter(TaskCounter.MAP_VIRTUAL_MEMORY_BYTES_MAX);\r\n        org.apache.hadoop.mapreduce.Counter maxReduceV = job.getCounters().findCounter(TaskCounter.REDUCE_VIRTUAL_MEMORY_BYTES_MAX);\r\n        org.apache.hadoop.mapreduce.Counter allV = job.getCounters().findCounter(TaskCounter.VIRTUAL_MEMORY_BYTES);\r\n        assertEquals(2000, maxMapV.getValue());\r\n        assertEquals(2000, maxReduceV.getValue());\r\n        assertEquals(6000, allV.getValue());\r\n        org.apache.hadoop.mapreduce.Counter customerCounter = job.getCounters().findCounter(IntSumReducer.Counters.MY_COUNTER_MAX);\r\n        assertEquals(200, customerCounter.getValue());\r\n        fs.delete(testInputDir, true);\r\n        fs.delete(testOutputDir, true);\r\n    } finally {\r\n        mrCluster.stop();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2",
  "methodName" : "setup",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void setup() throws InterruptedException, IOException\n{\r\n    if (!(new File(MiniMRYarnCluster.APPJAR)).exists()) {\r\n        LOG.info(\"MRAppJar \" + MiniMRYarnCluster.APPJAR + \" not found. Not running test.\");\r\n        return;\r\n    }\r\n    if (mrCluster == null) {\r\n        mrCluster = new MiniMRYarnCluster(getClass().getName());\r\n        mrCluster.init(new Configuration());\r\n        mrCluster.start();\r\n    }\r\n    localFs.copyFromLocalFile(new Path(MiniMRYarnCluster.APPJAR), APP_JAR);\r\n    localFs.setPermission(APP_JAR, new FsPermission(\"700\"));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2",
  "methodName" : "tearDown",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void tearDown()\n{\r\n    if (!(new File(MiniMRYarnCluster.APPJAR)).exists()) {\r\n        LOG.info(\"MRAppJar \" + MiniMRYarnCluster.APPJAR + \" not found. Not running test.\");\r\n        return;\r\n    }\r\n    if (mrCluster != null) {\r\n        mrCluster.stop();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2",
  "methodName" : "testJobHistoryData",
  "errType" : null,
  "containingMethodsNum" : 23,
  "sourceCodeText" : "void testJobHistoryData() throws IOException, InterruptedException, AvroRemoteException, ClassNotFoundException\n{\r\n    if (!(new File(MiniMRYarnCluster.APPJAR)).exists()) {\r\n        LOG.info(\"MRAppJar \" + MiniMRYarnCluster.APPJAR + \" not found. Not running test.\");\r\n        return;\r\n    }\r\n    SleepJob sleepJob = new SleepJob();\r\n    sleepJob.setConf(mrCluster.getConfig());\r\n    Job job = sleepJob.createJob(3, 2, 1000, 1, 500, 1);\r\n    job.setJarByClass(SleepJob.class);\r\n    job.addFileToClassPath(APP_JAR);\r\n    job.waitForCompletion(true);\r\n    Counters counterMR = job.getCounters();\r\n    JobId jobId = TypeConverter.toYarn(job.getJobID());\r\n    ApplicationId appID = jobId.getAppId();\r\n    int pollElapsed = 0;\r\n    while (true) {\r\n        Thread.sleep(1000);\r\n        pollElapsed += 1000;\r\n        if (TERMINAL_RM_APP_STATES.contains(mrCluster.getResourceManager().getRMContext().getRMApps().get(appID).getState())) {\r\n            break;\r\n        }\r\n        if (pollElapsed >= 60000) {\r\n            LOG.warn(\"application did not reach terminal state within 60 seconds\");\r\n            break;\r\n        }\r\n    }\r\n    Assert.assertEquals(RMAppState.FINISHED, mrCluster.getResourceManager().getRMContext().getRMApps().get(appID).getState());\r\n    Counters counterHS = job.getCounters();\r\n    LOG.info(\"CounterHS \" + counterHS);\r\n    LOG.info(\"CounterMR \" + counterMR);\r\n    Assert.assertEquals(counterHS, counterMR);\r\n    HSClientProtocol historyClient = instantiateHistoryProxy();\r\n    GetJobReportRequest gjReq = Records.newRecord(GetJobReportRequest.class);\r\n    gjReq.setJobId(jobId);\r\n    JobReport jobReport = historyClient.getJobReport(gjReq).getJobReport();\r\n    verifyJobReport(jobReport, jobId);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2",
  "methodName" : "verifyJobReport",
  "errType" : null,
  "containingMethodsNum" : 10,
  "sourceCodeText" : "void verifyJobReport(JobReport jobReport, JobId jobId)\n{\r\n    List<AMInfo> amInfos = jobReport.getAMInfos();\r\n    Assert.assertEquals(1, amInfos.size());\r\n    AMInfo amInfo = amInfos.get(0);\r\n    ApplicationAttemptId appAttemptId = ApplicationAttemptId.newInstance(jobId.getAppId(), 1);\r\n    ContainerId amContainerId = ContainerId.newContainerId(appAttemptId, 1);\r\n    Assert.assertEquals(appAttemptId, amInfo.getAppAttemptId());\r\n    Assert.assertEquals(amContainerId, amInfo.getContainerId());\r\n    Assert.assertTrue(jobReport.getSubmitTime() > 0);\r\n    Assert.assertTrue(jobReport.getStartTime() > 0 && jobReport.getStartTime() >= jobReport.getSubmitTime());\r\n    Assert.assertTrue(jobReport.getFinishTime() > 0 && jobReport.getFinishTime() >= jobReport.getStartTime());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2",
  "methodName" : "instantiateHistoryProxy",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "HSClientProtocol instantiateHistoryProxy()\n{\r\n    final String serviceAddr = mrCluster.getConfig().get(JHAdminConfig.MR_HISTORY_ADDRESS);\r\n    final YarnRPC rpc = YarnRPC.create(conf);\r\n    HSClientProtocol historyClient = (HSClientProtocol) rpc.getProxy(HSClientProtocol.class, NetUtils.createSocketAddr(serviceAddr), mrCluster.getConfig());\r\n    return historyClient;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\fs\\slive",
  "methodName" : "getSleepTime",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "long getSleepTime(Range<Long> sleepTime)\n{\r\n    long sleepMs = Range.betweenPositive(getRandom(), sleepTime);\r\n    return sleepMs;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\fs\\slive",
  "methodName" : "run",
  "errType" : [ "InterruptedException" ],
  "containingMethodsNum" : 9,
  "sourceCodeText" : "List<OperationOutput> run(Range<Long> sleepTime)\n{\r\n    List<OperationOutput> out = super.run(null);\r\n    try {\r\n        if (sleepTime != null) {\r\n            long sleepMs = getSleepTime(sleepTime);\r\n            long startTime = Timer.now();\r\n            sleep(sleepMs);\r\n            long elapsedTime = Timer.elapsed(startTime);\r\n            out.add(new OperationOutput(OutputType.LONG, getType(), ReportWriter.OK_TIME_TAKEN, elapsedTime));\r\n            out.add(new OperationOutput(OutputType.LONG, getType(), ReportWriter.SUCCESSES, 1L));\r\n        }\r\n    } catch (InterruptedException e) {\r\n        out.add(new OperationOutput(OutputType.LONG, getType(), ReportWriter.FAILURES, 1L));\r\n        LOG.warn(\"Error with sleeping\", e);\r\n    }\r\n    return out;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\fs\\slive",
  "methodName" : "run",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "List<OperationOutput> run(FileSystem fs)\n{\r\n    Range<Long> sleepTime = getConfig().getSleepRange();\r\n    return run(sleepTime);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\fs\\slive",
  "methodName" : "sleep",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void sleep(long ms) throws InterruptedException\n{\r\n    if (ms <= 0) {\r\n        return;\r\n    }\r\n    Thread.sleep(ms);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "setupClass",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setupClass() throws Exception\n{\r\n    setupClassBase(TestJobName.class);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "testComplexName",
  "errType" : null,
  "containingMethodsNum" : 18,
  "sourceCodeText" : "void testComplexName() throws Exception\n{\r\n    OutputStream os = getFileSystem().create(new Path(getInputDir(), \"text.txt\"));\r\n    Writer wr = new OutputStreamWriter(os);\r\n    wr.write(\"b a\\n\");\r\n    wr.close();\r\n    JobConf conf = createJobConf();\r\n    conf.setJobName(\"[name][some other value that gets truncated internally that this test attempts to aggravate]\");\r\n    conf.setInputFormat(TextInputFormat.class);\r\n    conf.setOutputKeyClass(LongWritable.class);\r\n    conf.setOutputValueClass(Text.class);\r\n    conf.setMapperClass(IdentityMapper.class);\r\n    FileInputFormat.setInputPaths(conf, getInputDir());\r\n    FileOutputFormat.setOutputPath(conf, getOutputDir());\r\n    JobClient.runJob(conf);\r\n    Path[] outputFiles = FileUtil.stat2Paths(getFileSystem().listStatus(getOutputDir(), new Utils.OutputFileUtils.OutputFilesFilter()));\r\n    assertEquals(1, outputFiles.length);\r\n    InputStream is = getFileSystem().open(outputFiles[0]);\r\n    BufferedReader reader = new BufferedReader(new InputStreamReader(is));\r\n    assertEquals(\"0\\tb a\", reader.readLine());\r\n    assertNull(reader.readLine());\r\n    reader.close();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "testComplexNameWithRegex",
  "errType" : null,
  "containingMethodsNum" : 18,
  "sourceCodeText" : "void testComplexNameWithRegex() throws Exception\n{\r\n    OutputStream os = getFileSystem().create(new Path(getInputDir(), \"text.txt\"));\r\n    Writer wr = new OutputStreamWriter(os);\r\n    wr.write(\"b a\\n\");\r\n    wr.close();\r\n    JobConf conf = createJobConf();\r\n    conf.setJobName(\"name \\\\Evalue]\");\r\n    conf.setInputFormat(TextInputFormat.class);\r\n    conf.setOutputKeyClass(LongWritable.class);\r\n    conf.setOutputValueClass(Text.class);\r\n    conf.setMapperClass(IdentityMapper.class);\r\n    FileInputFormat.setInputPaths(conf, getInputDir());\r\n    FileOutputFormat.setOutputPath(conf, getOutputDir());\r\n    JobClient.runJob(conf);\r\n    Path[] outputFiles = FileUtil.stat2Paths(getFileSystem().listStatus(getOutputDir(), new Utils.OutputFileUtils.OutputFilesFilter()));\r\n    assertEquals(1, outputFiles.length);\r\n    InputStream is = getFileSystem().open(outputFiles[0]);\r\n    BufferedReader reader = new BufferedReader(new InputStreamReader(is));\r\n    assertEquals(\"0\\tb a\", reader.readLine());\r\n    assertNull(reader.readLine());\r\n    reader.close();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\testjar",
  "methodName" : "configure",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void configure(JobConf job)\n{\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\testjar",
  "methodName" : "close",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void close() throws IOException\n{\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\testjar",
  "methodName" : "map",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void map(WritableComparable key, Writable value, OutputCollector<ExternalWritable, IntWritable> output, Reporter reporter) throws IOException\n{\r\n    if (value instanceof Text) {\r\n        Text text = (Text) value;\r\n        ExternalWritable ext = new ExternalWritable(text.toString());\r\n        output.collect(ext, new IntWritable(1));\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\testjar",
  "methodName" : "reduce",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void reduce(WritableComparable key, Iterator<Writable> values, OutputCollector<WritableComparable, IntWritable> output, Reporter reporter) throws IOException\n{\r\n    int count = 0;\r\n    while (values.hasNext()) {\r\n        count++;\r\n        values.next();\r\n    }\r\n    output.collect(key, new IntWritable(count));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop",
  "methodName" : "createJob",
  "errType" : null,
  "containingMethodsNum" : 13,
  "sourceCodeText" : "Job createJob(Configuration conf) throws IOException\n{\r\n    long numBytesToWritePerMap = conf.getLong(BYTES_PER_MAP, 10 * 1024);\r\n    long totalBytesToWrite = conf.getLong(TOTAL_BYTES, numBytesToWritePerMap);\r\n    int numMaps = (int) (totalBytesToWrite / numBytesToWritePerMap);\r\n    if (numMaps == 0 && totalBytesToWrite > 0) {\r\n        numMaps = 1;\r\n        conf.setLong(BYTES_PER_MAP, totalBytesToWrite);\r\n    }\r\n    conf.setInt(MRJobConfig.NUM_MAPS, numMaps);\r\n    Job job = Job.getInstance(conf);\r\n    job.setJarByClass(RandomTextWriterJob.class);\r\n    job.setJobName(\"random-text-writer\");\r\n    job.setOutputKeyClass(Text.class);\r\n    job.setOutputValueClass(Text.class);\r\n    job.setInputFormatClass(RandomInputFormat.class);\r\n    job.setMapperClass(RandomTextMapper.class);\r\n    job.setOutputFormatClass(SequenceFileOutputFormat.class);\r\n    job.setNumReduceTasks(0);\r\n    return job;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop",
  "methodName" : "run",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "int run(String[] args) throws Exception\n{\r\n    if (args.length == 0) {\r\n        return printUsage();\r\n    }\r\n    Job job = createJob(getConf());\r\n    FileOutputFormat.setOutputPath(job, new Path(args[0]));\r\n    Date startTime = new Date();\r\n    System.out.println(\"Job started: \" + startTime);\r\n    int ret = job.waitForCompletion(true) ? 0 : 1;\r\n    Date endTime = new Date();\r\n    System.out.println(\"Job ended: \" + endTime);\r\n    System.out.println(\"The job took \" + (endTime.getTime() - startTime.getTime()) / 1000 + \" seconds.\");\r\n    return ret;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop",
  "methodName" : "printUsage",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "int printUsage()\n{\r\n    System.out.println(\"randomtextwriter \" + \"[-outFormat <output format class>] \" + \"<output>\");\r\n    ToolRunner.printGenericCommandUsage(System.out);\r\n    return 2;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop",
  "methodName" : "main",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void main(String[] args) throws Exception\n{\r\n    int res = ToolRunner.run(new Configuration(), new RandomTextWriterJob(), args);\r\n    System.exit(res);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred\\pipes",
  "methodName" : "main",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void main(String[] args)\n{\r\n    PipeApplicationRunnableStub client = new PipeApplicationRunnableStub();\r\n    client.binaryProtocolStub();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred\\pipes",
  "methodName" : "binaryProtocolStub",
  "errType" : [ "Exception" ],
  "containingMethodsNum" : 22,
  "sourceCodeText" : "void binaryProtocolStub()\n{\r\n    try {\r\n        initSoket();\r\n        System.out.println(\"start OK\");\r\n        int answer = WritableUtils.readVInt(dataInput);\r\n        System.out.println(\"RunMap:\" + answer);\r\n        TestPipeApplication.FakeSplit split = new TestPipeApplication.FakeSplit();\r\n        readObject(split, dataInput);\r\n        WritableUtils.readVInt(dataInput);\r\n        WritableUtils.readVInt(dataInput);\r\n        WritableUtils.readVInt(dataInput);\r\n        String inText = Text.readString(dataInput);\r\n        System.out.println(\"Key class:\" + inText);\r\n        inText = Text.readString(dataInput);\r\n        System.out.println(\"Value class:\" + inText);\r\n        @SuppressWarnings(\"unused\")\r\n        int inCode = 0;\r\n        while ((inCode = WritableUtils.readVInt(dataInput)) == 4) {\r\n            FloatWritable key = new FloatWritable();\r\n            NullWritable value = NullWritable.get();\r\n            readObject(key, dataInput);\r\n            System.out.println(\"value:\" + key.get());\r\n            readObject(value, dataInput);\r\n        }\r\n        WritableUtils.writeVInt(dataOut, 54);\r\n        dataOut.flush();\r\n        dataOut.close();\r\n    } catch (Exception x) {\r\n        x.printStackTrace();\r\n    } finally {\r\n        closeSoket();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "testMerge",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void testMerge() throws Exception\n{\r\n    MiniDFSCluster dfsCluster = null;\r\n    MiniMRClientCluster mrCluster = null;\r\n    FileSystem fileSystem = null;\r\n    try {\r\n        Configuration conf = new Configuration();\r\n        dfsCluster = new MiniDFSCluster.Builder(conf).numDataNodes(NUM_HADOOP_DATA_NODES).build();\r\n        fileSystem = dfsCluster.getFileSystem();\r\n        mrCluster = MiniMRClientClusterFactory.create(this.getClass(), NUM_HADOOP_DATA_NODES, conf);\r\n        createInput(fileSystem);\r\n        runMergeTest(new JobConf(mrCluster.getConfig()), fileSystem);\r\n    } finally {\r\n        if (mrCluster != null) {\r\n            mrCluster.stop();\r\n        }\r\n        if (dfsCluster != null) {\r\n            dfsCluster.shutdown();\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "createInput",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void createInput(FileSystem fs) throws Exception\n{\r\n    fs.delete(INPUT_DIR, true);\r\n    for (int i = 0; i < NUM_MAPPERS; i++) {\r\n        OutputStream os = fs.create(new Path(INPUT_DIR, \"input_\" + i + \".txt\"));\r\n        Writer writer = new OutputStreamWriter(os);\r\n        for (int j = 0; j < NUM_LINES; j++) {\r\n            int k = j + 1;\r\n            String formattedNumber = String.format(\"%09d\", k);\r\n            writer.write(formattedNumber + \" \" + formattedNumber + \"\\n\");\r\n        }\r\n        writer.close();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "runMergeTest",
  "errType" : [ "IOException", "InterruptedException" ],
  "containingMethodsNum" : 20,
  "sourceCodeText" : "void runMergeTest(JobConf job, FileSystem fileSystem) throws Exception\n{\r\n    fileSystem.delete(OUTPUT, true);\r\n    job.setJobName(\"MergeTest\");\r\n    JobClient client = new JobClient(job);\r\n    RunningJob submittedJob = null;\r\n    FileInputFormat.setInputPaths(job, INPUT_DIR);\r\n    FileOutputFormat.setOutputPath(job, OUTPUT);\r\n    job.set(\"mapreduce.output.textoutputformat.separator\", \" \");\r\n    job.setInputFormat(TextInputFormat.class);\r\n    job.setMapOutputKeyClass(Text.class);\r\n    job.setMapOutputValueClass(Text.class);\r\n    job.setOutputKeyClass(Text.class);\r\n    job.setOutputValueClass(Text.class);\r\n    job.setMapperClass(MyMapper.class);\r\n    job.setPartitionerClass(MyPartitioner.class);\r\n    job.setOutputFormat(TextOutputFormat.class);\r\n    job.setNumReduceTasks(NUM_REDUCERS);\r\n    job.set(JobContext.MAP_OUTPUT_COLLECTOR_CLASS_ATTR, MapOutputCopier.class.getName());\r\n    try {\r\n        submittedJob = client.submitJob(job);\r\n        try {\r\n            if (!client.monitorAndPrintJob(job, submittedJob)) {\r\n                throw new IOException(\"Job failed!\");\r\n            }\r\n        } catch (InterruptedException ie) {\r\n            Thread.currentThread().interrupt();\r\n        }\r\n    } catch (IOException ioe) {\r\n        System.err.println(\"Job failed with: \" + ioe);\r\n    } finally {\r\n        verifyOutput(submittedJob, fileSystem);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 2,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "verifyOutput",
  "errType" : null,
  "containingMethodsNum" : 11,
  "sourceCodeText" : "void verifyOutput(RunningJob submittedJob, FileSystem fileSystem) throws Exception\n{\r\n    FSDataInputStream dis = null;\r\n    long numValidRecords = 0;\r\n    long numInvalidRecords = 0;\r\n    long numMappersLaunched = NUM_MAPPERS;\r\n    String prevKeyValue = \"000000000\";\r\n    Path[] fileList = FileUtil.stat2Paths(fileSystem.listStatus(OUTPUT, new Utils.OutputFileUtils.OutputFilesFilter()));\r\n    for (Path outFile : fileList) {\r\n        try {\r\n            dis = fileSystem.open(outFile);\r\n            String record;\r\n            while ((record = dis.readLine()) != null) {\r\n                int blankPos = record.indexOf(\" \");\r\n                String keyString = record.substring(0, blankPos);\r\n                String valueString = record.substring(blankPos + 1);\r\n                if (keyString.compareTo(prevKeyValue) >= 0 && keyString.equals(valueString)) {\r\n                    prevKeyValue = keyString;\r\n                    numValidRecords++;\r\n                } else {\r\n                    numInvalidRecords++;\r\n                }\r\n            }\r\n        } finally {\r\n            if (dis != null) {\r\n                dis.close();\r\n                dis = null;\r\n            }\r\n        }\r\n    }\r\n    assertEquals((long) (NUM_MAPPERS * NUM_LINES), numValidRecords);\r\n    assertEquals(0, numInvalidRecords);\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\testjar",
  "methodName" : "main",
  "errType" : null,
  "containingMethodsNum" : 12,
  "sourceCodeText" : "void main(String[] args) throws Exception\n{\r\n    Path outDir = new Path(\"output\");\r\n    Configuration conf = new Configuration();\r\n    Job job = Job.getInstance(conf, \"user name check\");\r\n    job.setJarByClass(UserNamePermission.class);\r\n    job.setMapperClass(UserNamePermission.UserNameMapper.class);\r\n    job.setCombinerClass(UserNamePermission.UserNameReducer.class);\r\n    job.setMapOutputKeyClass(Text.class);\r\n    job.setMapOutputValueClass(Text.class);\r\n    job.setReducerClass(UserNamePermission.UserNameReducer.class);\r\n    job.setNumReduceTasks(1);\r\n    job.setInputFormatClass(TextInputFormat.class);\r\n    TextInputFormat.addInputPath(job, new Path(\"input\"));\r\n    FileOutputFormat.setOutputPath(job, outDir);\r\n    System.exit(job.waitForCompletion(true) ? 0 : 1);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "testBinary",
  "errType" : null,
  "containingMethodsNum" : 38,
  "sourceCodeText" : "void testBinary() throws IOException\n{\r\n    JobConf job = new JobConf();\r\n    FileSystem fs = FileSystem.getLocal(job);\r\n    Path dir = new Path(new Path(new Path(System.getProperty(\"test.build.data\", \".\")), FileOutputCommitter.TEMP_DIR_NAME), \"_\" + attempt);\r\n    Path file = new Path(dir, \"testbinary.seq\");\r\n    Random r = new Random();\r\n    long seed = r.nextLong();\r\n    r.setSeed(seed);\r\n    fs.delete(dir, true);\r\n    if (!fs.mkdirs(dir)) {\r\n        fail(\"Failed to create output directory\");\r\n    }\r\n    job.set(JobContext.TASK_ATTEMPT_ID, attempt);\r\n    FileOutputFormat.setOutputPath(job, dir.getParent().getParent());\r\n    FileOutputFormat.setWorkOutputPath(job, dir);\r\n    SequenceFileAsBinaryOutputFormat.setSequenceFileOutputKeyClass(job, IntWritable.class);\r\n    SequenceFileAsBinaryOutputFormat.setSequenceFileOutputValueClass(job, DoubleWritable.class);\r\n    SequenceFileAsBinaryOutputFormat.setCompressOutput(job, true);\r\n    SequenceFileAsBinaryOutputFormat.setOutputCompressionType(job, CompressionType.BLOCK);\r\n    BytesWritable bkey = new BytesWritable();\r\n    BytesWritable bval = new BytesWritable();\r\n    RecordWriter<BytesWritable, BytesWritable> writer = new SequenceFileAsBinaryOutputFormat().getRecordWriter(fs, job, file.toString(), Reporter.NULL);\r\n    IntWritable iwritable = new IntWritable();\r\n    DoubleWritable dwritable = new DoubleWritable();\r\n    DataOutputBuffer outbuf = new DataOutputBuffer();\r\n    LOG.info(\"Creating data by SequenceFileAsBinaryOutputFormat\");\r\n    try {\r\n        for (int i = 0; i < RECORDS; ++i) {\r\n            iwritable = new IntWritable(r.nextInt());\r\n            iwritable.write(outbuf);\r\n            bkey.set(outbuf.getData(), 0, outbuf.getLength());\r\n            outbuf.reset();\r\n            dwritable = new DoubleWritable(r.nextDouble());\r\n            dwritable.write(outbuf);\r\n            bval.set(outbuf.getData(), 0, outbuf.getLength());\r\n            outbuf.reset();\r\n            writer.write(bkey, bval);\r\n        }\r\n    } finally {\r\n        writer.close(Reporter.NULL);\r\n    }\r\n    InputFormat<IntWritable, DoubleWritable> iformat = new SequenceFileInputFormat<IntWritable, DoubleWritable>();\r\n    int count = 0;\r\n    r.setSeed(seed);\r\n    DataInputBuffer buf = new DataInputBuffer();\r\n    final int NUM_SPLITS = 3;\r\n    SequenceFileInputFormat.addInputPath(job, file);\r\n    LOG.info(\"Reading data by SequenceFileInputFormat\");\r\n    for (InputSplit split : iformat.getSplits(job, NUM_SPLITS)) {\r\n        RecordReader<IntWritable, DoubleWritable> reader = iformat.getRecordReader(split, job, Reporter.NULL);\r\n        try {\r\n            int sourceInt;\r\n            double sourceDouble;\r\n            while (reader.next(iwritable, dwritable)) {\r\n                sourceInt = r.nextInt();\r\n                sourceDouble = r.nextDouble();\r\n                assertEquals(\"Keys don't match: \" + \"*\" + iwritable.get() + \":\" + sourceInt + \"*\", sourceInt, iwritable.get());\r\n                assertThat(dwritable.get()).withFailMessage(\"Vals don't match: \" + \"*\" + dwritable.get() + \":\" + sourceDouble + \"*\").isEqualTo(sourceDouble);\r\n                ++count;\r\n            }\r\n        } finally {\r\n            reader.close();\r\n        }\r\n    }\r\n    assertEquals(\"Some records not found\", RECORDS, count);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 2,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "testSequenceOutputClassDefaultsToMapRedOutputClass",
  "errType" : null,
  "containingMethodsNum" : 9,
  "sourceCodeText" : "void testSequenceOutputClassDefaultsToMapRedOutputClass() throws IOException\n{\r\n    JobConf job = new JobConf();\r\n    FileSystem fs = FileSystem.getLocal(job);\r\n    job.setOutputKeyClass(FloatWritable.class);\r\n    job.setOutputValueClass(BooleanWritable.class);\r\n    assertEquals(\"SequenceFileOutputKeyClass should default to ouputKeyClass\", FloatWritable.class, SequenceFileAsBinaryOutputFormat.getSequenceFileOutputKeyClass(job));\r\n    assertEquals(\"SequenceFileOutputValueClass should default to \" + \"ouputValueClass\", BooleanWritable.class, SequenceFileAsBinaryOutputFormat.getSequenceFileOutputValueClass(job));\r\n    SequenceFileAsBinaryOutputFormat.setSequenceFileOutputKeyClass(job, IntWritable.class);\r\n    SequenceFileAsBinaryOutputFormat.setSequenceFileOutputValueClass(job, DoubleWritable.class);\r\n    assertEquals(\"SequenceFileOutputKeyClass not updated\", IntWritable.class, SequenceFileAsBinaryOutputFormat.getSequenceFileOutputKeyClass(job));\r\n    assertEquals(\"SequenceFileOutputValueClass not updated\", DoubleWritable.class, SequenceFileAsBinaryOutputFormat.getSequenceFileOutputValueClass(job));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "testcheckOutputSpecsForbidRecordCompression",
  "errType" : [ "Exception", "InvalidJobConfException", "Exception" ],
  "containingMethodsNum" : 17,
  "sourceCodeText" : "void testcheckOutputSpecsForbidRecordCompression() throws IOException\n{\r\n    JobConf job = new JobConf();\r\n    FileSystem fs = FileSystem.getLocal(job);\r\n    Path dir = new Path(System.getProperty(\"test.build.data\", \".\") + \"/mapred\");\r\n    Path outputdir = new Path(System.getProperty(\"test.build.data\", \".\") + \"/output\");\r\n    fs.delete(dir, true);\r\n    fs.delete(outputdir, true);\r\n    if (!fs.mkdirs(dir)) {\r\n        fail(\"Failed to create output directory\");\r\n    }\r\n    FileOutputFormat.setWorkOutputPath(job, dir);\r\n    FileOutputFormat.setOutputPath(job, outputdir);\r\n    SequenceFileAsBinaryOutputFormat.setCompressOutput(job, true);\r\n    SequenceFileAsBinaryOutputFormat.setOutputCompressionType(job, CompressionType.BLOCK);\r\n    try {\r\n        new SequenceFileAsBinaryOutputFormat().checkOutputSpecs(fs, job);\r\n    } catch (Exception e) {\r\n        fail(\"Block compression should be allowed for \" + \"SequenceFileAsBinaryOutputFormat:\" + \"Caught \" + e.getClass().getName());\r\n    }\r\n    SequenceFileAsBinaryOutputFormat.setOutputCompressionType(job, CompressionType.RECORD);\r\n    try {\r\n        new SequenceFileAsBinaryOutputFormat().checkOutputSpecs(fs, job);\r\n        fail(\"Record compression should not be allowed for \" + \"SequenceFileAsBinaryOutputFormat\");\r\n    } catch (InvalidJobConfException ie) {\r\n    } catch (Exception e) {\r\n        fail(\"Expected \" + InvalidJobConfException.class.getName() + \"but caught \" + e.getClass().getName());\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 2,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "run",
  "errType" : null,
  "containingMethodsNum" : 26,
  "sourceCodeText" : "int run(String[] args) throws Exception\n{\r\n    if (args.length == 0) {\r\n        System.out.println(\"Usage: writer <out-dir>\");\r\n        ToolRunner.printGenericCommandUsage(System.out);\r\n        return 2;\r\n    }\r\n    Path outDir = new Path(args[0]);\r\n    Configuration conf = getConf();\r\n    JobClient client = new JobClient(conf);\r\n    ClusterStatus cluster = client.getClusterStatus();\r\n    int numMapsPerHost = conf.getInt(MAPS_PER_HOST, 10);\r\n    long numBytesToWritePerMap = conf.getLong(BYTES_PER_MAP, 1 * 1024 * 1024 * 1024);\r\n    if (numBytesToWritePerMap == 0) {\r\n        System.err.println(\"Cannot have\" + BYTES_PER_MAP + \" set to 0\");\r\n        return -2;\r\n    }\r\n    long totalBytesToWrite = conf.getLong(TOTAL_BYTES, numMapsPerHost * numBytesToWritePerMap * cluster.getTaskTrackers());\r\n    int numMaps = (int) (totalBytesToWrite / numBytesToWritePerMap);\r\n    if (numMaps == 0 && totalBytesToWrite > 0) {\r\n        numMaps = 1;\r\n        conf.setLong(BYTES_PER_MAP, totalBytesToWrite);\r\n    }\r\n    conf.setInt(MRJobConfig.NUM_MAPS, numMaps);\r\n    Job job = Job.getInstance(conf);\r\n    job.setJarByClass(RandomWriter.class);\r\n    job.setJobName(\"random-writer\");\r\n    FileOutputFormat.setOutputPath(job, outDir);\r\n    job.setOutputKeyClass(BytesWritable.class);\r\n    job.setOutputValueClass(BytesWritable.class);\r\n    job.setInputFormatClass(RandomInputFormat.class);\r\n    job.setMapperClass(RandomMapper.class);\r\n    job.setReducerClass(Reducer.class);\r\n    job.setOutputFormatClass(SequenceFileOutputFormat.class);\r\n    System.out.println(\"Running \" + numMaps + \" maps.\");\r\n    job.setNumReduceTasks(0);\r\n    Date startTime = new Date();\r\n    System.out.println(\"Job started: \" + startTime);\r\n    int ret = job.waitForCompletion(true) ? 0 : 1;\r\n    Date endTime = new Date();\r\n    System.out.println(\"Job ended: \" + endTime);\r\n    System.out.println(\"The job took \" + (endTime.getTime() - startTime.getTime()) / 1000 + \" seconds.\");\r\n    return ret;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "main",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void main(String[] args) throws Exception\n{\r\n    int res = ToolRunner.run(new Configuration(), new RandomWriter(), args);\r\n    System.exit(res);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred\\lib",
  "methodName" : "testSplitting",
  "errType" : null,
  "containingMethodsNum" : 17,
  "sourceCodeText" : "void testSplitting() throws Exception\n{\r\n    JobConf conf = new JobConf();\r\n    MiniDFSCluster dfs = null;\r\n    try {\r\n        dfs = new MiniDFSCluster.Builder(conf).numDataNodes(4).racks(new String[] { \"/rack0\", \"/rack0\", \"/rack1\", \"/rack1\" }).hosts(new String[] { \"host0\", \"host1\", \"host2\", \"host3\" }).build();\r\n        FileSystem fs = dfs.getFileSystem();\r\n        Path path = getPath(\"/foo/bar\", fs);\r\n        Path path2 = getPath(\"/foo/baz\", fs);\r\n        Path path3 = getPath(\"/bar/bar\", fs);\r\n        Path path4 = getPath(\"/bar/baz\", fs);\r\n        final int numSplits = 100;\r\n        MultipleInputs.addInputPath(conf, path, TextInputFormat.class, MapClass.class);\r\n        MultipleInputs.addInputPath(conf, path2, TextInputFormat.class, MapClass2.class);\r\n        MultipleInputs.addInputPath(conf, path3, KeyValueTextInputFormat.class, MapClass.class);\r\n        MultipleInputs.addInputPath(conf, path4, TextInputFormat.class, MapClass2.class);\r\n        DelegatingInputFormat inFormat = new DelegatingInputFormat();\r\n        InputSplit[] splits = inFormat.getSplits(conf, numSplits);\r\n        int[] bins = new int[3];\r\n        for (InputSplit split : splits) {\r\n            assertTrue(split instanceof TaggedInputSplit);\r\n            final TaggedInputSplit tis = (TaggedInputSplit) split;\r\n            int index = -1;\r\n            if (tis.getInputFormatClass().equals(KeyValueTextInputFormat.class)) {\r\n                index = 0;\r\n            } else if (tis.getMapperClass().equals(MapClass.class)) {\r\n                index = 1;\r\n            } else {\r\n                index = 2;\r\n            }\r\n            bins[index]++;\r\n        }\r\n        for (int count : bins) {\r\n            assertEquals(numSplits, count);\r\n        }\r\n        assertTrue(true);\r\n    } finally {\r\n        if (dfs != null) {\r\n            dfs.shutdown();\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred\\lib",
  "methodName" : "getPath",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "Path getPath(final String location, final FileSystem fs) throws IOException\n{\r\n    Path path = new Path(location);\r\n    DataOutputStream out = fs.create(path, true, 4096, (short) 2, 512, null);\r\n    for (int i = 0; i < 1000; ++i) {\r\n        out.writeChars(\"Hello\\n\");\r\n    }\r\n    out.close();\r\n    return path;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "printUsage",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "int printUsage()\n{\r\n    System.err.println(\"Usage: [-m <maps>] [-r <reduces>]\\n\" + \"       [-keepmap <percent>] [-keepred <percent>]\\n\" + \"       [-indir <path>] [-outdir <path]\\n\" + \"       [-inFormat[Indirect] <InputFormat>] [-outFormat <OutputFormat>]\\n\" + \"       [-outKey <WritableComparable>] [-outValue <Writable>]\\n\");\r\n    GenericOptionsParser.printGenericCommandUsage(System.err);\r\n    return -1;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "parseArgs",
  "errType" : [ "NumberFormatException", "Exception" ],
  "containingMethodsNum" : 29,
  "sourceCodeText" : "boolean parseArgs(String[] argv, Job job) throws IOException\n{\r\n    if (argv.length < 1) {\r\n        return 0 == printUsage();\r\n    }\r\n    for (int i = 0; i < argv.length; ++i) {\r\n        if (argv.length == i + 1) {\r\n            System.out.println(\"ERROR: Required parameter missing from \" + argv[i]);\r\n            return 0 == printUsage();\r\n        }\r\n        try {\r\n            if (\"-r\".equals(argv[i])) {\r\n                job.setNumReduceTasks(Integer.parseInt(argv[++i]));\r\n            } else if (\"-inFormat\".equals(argv[i])) {\r\n                job.setInputFormatClass(Class.forName(argv[++i]).asSubclass(InputFormat.class));\r\n            } else if (\"-outFormat\".equals(argv[i])) {\r\n                job.setOutputFormatClass(Class.forName(argv[++i]).asSubclass(OutputFormat.class));\r\n            } else if (\"-outKey\".equals(argv[i])) {\r\n                job.setOutputKeyClass(Class.forName(argv[++i]).asSubclass(WritableComparable.class));\r\n            } else if (\"-outValue\".equals(argv[i])) {\r\n                job.setOutputValueClass(Class.forName(argv[++i]).asSubclass(Writable.class));\r\n            } else if (\"-keepmap\".equals(argv[i])) {\r\n                job.getConfiguration().set(MAP_PRESERVE_PERCENT, argv[++i]);\r\n            } else if (\"-keepred\".equals(argv[i])) {\r\n                job.getConfiguration().set(REDUCE_PRESERVE_PERCENT, argv[++i]);\r\n            } else if (\"-outdir\".equals(argv[i])) {\r\n                FileOutputFormat.setOutputPath(job, new Path(argv[++i]));\r\n            } else if (\"-indir\".equals(argv[i])) {\r\n                FileInputFormat.addInputPaths(job, argv[++i]);\r\n            } else if (\"-inFormatIndirect\".equals(argv[i])) {\r\n                job.getConfiguration().setClass(INDIRECT_INPUT_FORMAT, Class.forName(argv[++i]).asSubclass(InputFormat.class), InputFormat.class);\r\n                job.setInputFormatClass(IndirectInputFormat.class);\r\n            } else {\r\n                System.out.println(\"Unexpected argument: \" + argv[i]);\r\n                return 0 == printUsage();\r\n            }\r\n        } catch (NumberFormatException except) {\r\n            System.out.println(\"ERROR: Integer expected instead of \" + argv[i]);\r\n            return 0 == printUsage();\r\n        } catch (Exception e) {\r\n            throw (IOException) new IOException().initCause(e);\r\n        }\r\n    }\r\n    return true;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "run",
  "errType" : null,
  "containingMethodsNum" : 30,
  "sourceCodeText" : "int run(String[] argv) throws Exception\n{\r\n    Job job = Job.getInstance(getConf());\r\n    job.setJarByClass(GenericMRLoadGenerator.class);\r\n    job.setMapperClass(SampleMapper.class);\r\n    job.setReducerClass(SampleReducer.class);\r\n    if (!parseArgs(argv, job)) {\r\n        return -1;\r\n    }\r\n    Configuration conf = job.getConfiguration();\r\n    if (null == FileOutputFormat.getOutputPath(job)) {\r\n        job.setOutputFormatClass(NullOutputFormat.class);\r\n    }\r\n    if (0 == FileInputFormat.getInputPaths(job).length) {\r\n        System.err.println(\"No input path; ignoring InputFormat\");\r\n        confRandom(job);\r\n    } else if (null != conf.getClass(INDIRECT_INPUT_FORMAT, null)) {\r\n        JobClient jClient = new JobClient(conf);\r\n        Path tmpDir = new Path(\"/tmp\");\r\n        Random r = new Random();\r\n        Path indirInputFile = new Path(tmpDir, Integer.toString(r.nextInt(Integer.MAX_VALUE), 36) + \"_files\");\r\n        conf.set(INDIRECT_INPUT_FILE, indirInputFile.toString());\r\n        SequenceFile.Writer writer = SequenceFile.createWriter(tmpDir.getFileSystem(conf), conf, indirInputFile, LongWritable.class, Text.class, SequenceFile.CompressionType.NONE);\r\n        try {\r\n            for (Path p : FileInputFormat.getInputPaths(job)) {\r\n                FileSystem fs = p.getFileSystem(conf);\r\n                Stack<Path> pathstack = new Stack<Path>();\r\n                pathstack.push(p);\r\n                while (!pathstack.empty()) {\r\n                    for (FileStatus stat : fs.listStatus(pathstack.pop())) {\r\n                        if (stat.isDirectory()) {\r\n                            if (!stat.getPath().getName().startsWith(\"_\")) {\r\n                                pathstack.push(stat.getPath());\r\n                            }\r\n                        } else {\r\n                            writer.sync();\r\n                            writer.append(new LongWritable(stat.getLen()), new Text(stat.getPath().toUri().toString()));\r\n                        }\r\n                    }\r\n                }\r\n            }\r\n        } finally {\r\n            writer.close();\r\n        }\r\n    }\r\n    Date startTime = new Date();\r\n    System.out.println(\"Job started: \" + startTime);\r\n    int ret = job.waitForCompletion(true) ? 0 : 1;\r\n    Date endTime = new Date();\r\n    System.out.println(\"Job ended: \" + endTime);\r\n    System.out.println(\"The job took \" + (endTime.getTime() - startTime.getTime()) / 1000 + \" seconds.\");\r\n    return ret;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "main",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void main(String[] argv) throws Exception\n{\r\n    int res = ToolRunner.run(new Configuration(), new GenericMRLoadGenerator(), argv);\r\n    System.exit(res);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "confRandom",
  "errType" : null,
  "containingMethodsNum" : 9,
  "sourceCodeText" : "void confRandom(Job job) throws IOException\n{\r\n    job.setInputFormatClass(RandomInputFormat.class);\r\n    job.setMapperClass(RandomMapOutput.class);\r\n    Configuration conf = job.getConfiguration();\r\n    final ClusterStatus cluster = new JobClient(conf).getClusterStatus();\r\n    int numMapsPerHost = conf.getInt(RandomTextWriter.MAPS_PER_HOST, 10);\r\n    long numBytesToWritePerMap = conf.getLong(RandomTextWriter.BYTES_PER_MAP, 1 * 1024 * 1024 * 1024);\r\n    if (numBytesToWritePerMap == 0) {\r\n        throw new IOException(\"Cannot have \" + RandomTextWriter.BYTES_PER_MAP + \" set to 0\");\r\n    }\r\n    long totalBytesToWrite = conf.getLong(RandomTextWriter.TOTAL_BYTES, numMapsPerHost * numBytesToWritePerMap * cluster.getTaskTrackers());\r\n    int numMaps = (int) (totalBytesToWrite / numBytesToWritePerMap);\r\n    if (numMaps == 0 && totalBytesToWrite > 0) {\r\n        numMaps = 1;\r\n        conf.setLong(RandomTextWriter.BYTES_PER_MAP, totalBytesToWrite);\r\n    }\r\n    conf.setInt(MRJobConfig.NUM_MAPS, numMaps);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\test",
  "methodName" : "run",
  "errType" : [ "Throwable" ],
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void run(String[] argv)\n{\r\n    int exitCode = -1;\r\n    try {\r\n        exitCode = pgd.run(argv);\r\n    } catch (Throwable e) {\r\n        e.printStackTrace();\r\n    }\r\n    System.exit(exitCode);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\test",
  "methodName" : "main",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void main(String[] argv)\n{\r\n    new MapredTestDriver().run(argv);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "writeFile",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void writeFile(FileSystem fs, Path name, String contents) throws IOException\n{\r\n    OutputStream stm;\r\n    stm = fs.create(name);\r\n    stm.write(contents.getBytes());\r\n    stm.close();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "testSplits",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void testSplits() throws IOException\n{\r\n    JobConf job = new JobConf(defaultConf);\r\n    localFs.delete(workDir, true);\r\n    writeFile(localFs, new Path(workDir, \"test.txt\"), \"the quick\\nbrown\\nfox jumped\\nover\\n the lazy\\n dog\\n\");\r\n    FileInputFormat.setInputPaths(job, workDir);\r\n    CombineFileInputFormat format = new CombineFileInputFormat() {\r\n\r\n        @Override\r\n        public RecordReader getRecordReader(InputSplit split, JobConf job, Reporter reporter) throws IOException {\r\n            return new CombineFileRecordReader(job, (CombineFileSplit) split, reporter, CombineFileRecordReader.class);\r\n        }\r\n    };\r\n    final int SIZE_SPLITS = 1;\r\n    LOG.info(\"Trying to getSplits with splits = \" + SIZE_SPLITS);\r\n    InputSplit[] splits = format.getSplits(job, SIZE_SPLITS);\r\n    LOG.info(\"Got getSplits = \" + splits.length);\r\n    assertEquals(\"splits == \" + SIZE_SPLITS, SIZE_SPLITS, splits.length);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\db",
  "methodName" : "toLongArray",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "long[] toLongArray(List<Long> in)\n{\r\n    long[] out = new long[in.size()];\r\n    for (int i = 0; i < in.size(); i++) {\r\n        out[i] = in.get(i).longValue();\r\n    }\r\n    return out;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\db",
  "methodName" : "formatLongArray",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "String formatLongArray(long[] ar)\n{\r\n    StringBuilder sb = new StringBuilder();\r\n    sb.append(\"[\");\r\n    boolean first = true;\r\n    for (long val : ar) {\r\n        if (!first) {\r\n            sb.append(\", \");\r\n        }\r\n        sb.append(Long.toString(val));\r\n        first = false;\r\n    }\r\n    sb.append(\"]\");\r\n    return sb.toString();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\db",
  "methodName" : "assertLongArrayEquals",
  "errType" : [ "ArrayIndexOutOfBoundsException" ],
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void assertLongArrayEquals(long[] expected, long[] actual)\n{\r\n    for (int i = 0; i < expected.length; i++) {\r\n        try {\r\n            assertEquals(\"Failure at position \" + i + \"; got \" + actual[i] + \" instead of \" + expected[i] + \"; actual array is \" + formatLongArray(actual), expected[i], actual[i]);\r\n        } catch (ArrayIndexOutOfBoundsException oob) {\r\n            fail(\"Expected array with \" + expected.length + \" elements; got \" + actual.length + \". Actual array is \" + formatLongArray(actual));\r\n        }\r\n    }\r\n    if (actual.length > expected.length) {\r\n        fail(\"Actual array has \" + actual.length + \" elements; expected \" + expected.length + \". ACtual array is \" + formatLongArray(actual));\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\db",
  "methodName" : "testEvenSplits",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void testEvenSplits() throws SQLException\n{\r\n    List<Long> splits = new IntegerSplitter().split(10, 0, 100);\r\n    long[] expected = { 0, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100 };\r\n    assertLongArrayEquals(expected, toLongArray(splits));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\db",
  "methodName" : "testOddSplits",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void testOddSplits() throws SQLException\n{\r\n    List<Long> splits = new IntegerSplitter().split(10, 0, 95);\r\n    long[] expected = { 0, 9, 18, 27, 36, 45, 54, 63, 72, 81, 90, 95 };\r\n    assertLongArrayEquals(expected, toLongArray(splits));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\db",
  "methodName" : "testSingletonSplit",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void testSingletonSplit() throws SQLException\n{\r\n    List<Long> splits = new IntegerSplitter().split(1, 5, 5);\r\n    long[] expected = { 5, 5 };\r\n    assertLongArrayEquals(expected, toLongArray(splits));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\db",
  "methodName" : "testSingletonSplit2",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void testSingletonSplit2() throws SQLException\n{\r\n    List<Long> splits = new IntegerSplitter().split(5, 5, 5);\r\n    long[] expected = { 5, 5 };\r\n    assertLongArrayEquals(expected, toLongArray(splits));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\db",
  "methodName" : "testTooManySplits",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void testTooManySplits() throws SQLException\n{\r\n    List<Long> splits = new IntegerSplitter().split(5, 3, 5);\r\n    long[] expected = { 3, 4, 5 };\r\n    assertLongArrayEquals(expected, toLongArray(splits));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\input",
  "methodName" : "testBinary",
  "errType" : null,
  "containingMethodsNum" : 34,
  "sourceCodeText" : "void testBinary() throws IOException, InterruptedException\n{\r\n    Job job = Job.getInstance();\r\n    FileSystem fs = FileSystem.getLocal(job.getConfiguration());\r\n    Path dir = new Path(System.getProperty(\"test.build.data\", \".\") + \"/mapred\");\r\n    Path file = new Path(dir, \"testbinary.seq\");\r\n    Random r = new Random();\r\n    long seed = r.nextLong();\r\n    r.setSeed(seed);\r\n    fs.delete(dir, true);\r\n    FileInputFormat.setInputPaths(job, dir);\r\n    Text tkey = new Text();\r\n    Text tval = new Text();\r\n    SequenceFile.Writer writer = new SequenceFile.Writer(fs, job.getConfiguration(), file, Text.class, Text.class);\r\n    try {\r\n        for (int i = 0; i < RECORDS; ++i) {\r\n            tkey.set(Integer.toString(r.nextInt(), 36));\r\n            tval.set(Long.toString(r.nextLong(), 36));\r\n            writer.append(tkey, tval);\r\n        }\r\n    } finally {\r\n        writer.close();\r\n    }\r\n    TaskAttemptContext context = MapReduceTestUtil.createDummyMapTaskAttemptContext(job.getConfiguration());\r\n    InputFormat<BytesWritable, BytesWritable> bformat = new SequenceFileAsBinaryInputFormat();\r\n    int count = 0;\r\n    r.setSeed(seed);\r\n    BytesWritable bkey = new BytesWritable();\r\n    BytesWritable bval = new BytesWritable();\r\n    Text cmpkey = new Text();\r\n    Text cmpval = new Text();\r\n    DataInputBuffer buf = new DataInputBuffer();\r\n    FileInputFormat.setInputPaths(job, file);\r\n    for (InputSplit split : bformat.getSplits(job)) {\r\n        RecordReader<BytesWritable, BytesWritable> reader = bformat.createRecordReader(split, context);\r\n        MapContext<BytesWritable, BytesWritable, BytesWritable, BytesWritable> mcontext = new MapContextImpl<BytesWritable, BytesWritable, BytesWritable, BytesWritable>(job.getConfiguration(), context.getTaskAttemptID(), reader, null, null, MapReduceTestUtil.createDummyReporter(), split);\r\n        reader.initialize(split, mcontext);\r\n        try {\r\n            while (reader.nextKeyValue()) {\r\n                bkey = reader.getCurrentKey();\r\n                bval = reader.getCurrentValue();\r\n                tkey.set(Integer.toString(r.nextInt(), 36));\r\n                tval.set(Long.toString(r.nextLong(), 36));\r\n                buf.reset(bkey.getBytes(), bkey.getLength());\r\n                cmpkey.readFields(buf);\r\n                buf.reset(bval.getBytes(), bval.getLength());\r\n                cmpval.readFields(buf);\r\n                assertTrue(\"Keys don't match: \" + \"*\" + cmpkey.toString() + \":\" + tkey.toString() + \"*\", cmpkey.toString().equals(tkey.toString()));\r\n                assertTrue(\"Vals don't match: \" + \"*\" + cmpval.toString() + \":\" + tval.toString() + \"*\", cmpval.toString().equals(tval.toString()));\r\n                ++count;\r\n            }\r\n        } finally {\r\n            reader.close();\r\n        }\r\n    }\r\n    assertEquals(\"Some records not found\", RECORDS, count);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 2,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "setupClass",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setupClass() throws Exception\n{\r\n    testRootDir = GenericTestUtils.setupTestRootDir(TestLocalJobSubmission.class);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "setup",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void setup() throws IOException\n{\r\n    unitTestDir = new File(testRootDir, unitTestName.getMethodName());\r\n    unitTestDir.mkdirs();\r\n    config = createConfig();\r\n    jarPath = makeJar(new Path(unitTestDir.getAbsolutePath(), \"test.jar\"));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "createConfig",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "Configuration createConfig()\n{\r\n    Configuration conf = MRJobConfUtil.setLocalDirectoriesConfigForTesting(null, unitTestDir);\r\n    conf.set(MRConfig.FRAMEWORK_NAME, \"local\");\r\n    return conf;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "testLocalJobLibjarsOption",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void testLocalJobLibjarsOption() throws IOException\n{\r\n    testLocalJobLibjarsOption(config);\r\n    config.setBoolean(Job.USE_WILDCARD_FOR_LIBJARS, false);\r\n    testLocalJobLibjarsOption(config);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "testLocalJobLibjarsOption",
  "errType" : [ "Exception" ],
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void testLocalJobLibjarsOption(Configuration conf) throws IOException\n{\r\n    conf.set(FileSystem.FS_DEFAULT_NAME_KEY, \"hdfs://localhost:9000\");\r\n    conf.set(MRConfig.FRAMEWORK_NAME, \"local\");\r\n    final String[] args = { \"-jt\", \"local\", \"-libjars\", jarPath.toString(), \"-m\", \"1\", \"-r\", \"1\", \"-mt\", \"1\", \"-rt\", \"1\" };\r\n    int res = -1;\r\n    try {\r\n        res = ToolRunner.run(conf, new SleepJob(), args);\r\n    } catch (Exception e) {\r\n        LOG.error(\"Job failed with {}\", e.getLocalizedMessage(), e);\r\n        fail(\"Job failed\");\r\n    }\r\n    assertEquals(\"dist job res is not 0:\", 0, res);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "testLocalJobEncryptedIntermediateData",
  "errType" : [ "Exception" ],
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void testLocalJobEncryptedIntermediateData() throws IOException\n{\r\n    config = MRJobConfUtil.initEncryptedIntermediateConfigsForTesting(config);\r\n    final String[] args = { \"-m\", \"1\", \"-r\", \"1\", \"-mt\", \"1\", \"-rt\", \"1\" };\r\n    int res = -1;\r\n    try {\r\n        SpillCallBackPathsFinder spillInjector = (SpillCallBackPathsFinder) IntermediateEncryptedStream.setSpillCBInjector(new SpillCallBackPathsFinder());\r\n        res = ToolRunner.run(config, new SleepJob(), args);\r\n        Assert.assertTrue(\"No spill occurred\", spillInjector.getEncryptedSpilledFiles().size() > 0);\r\n    } catch (Exception e) {\r\n        LOG.error(\"Job failed with {}\", e.getLocalizedMessage(), e);\r\n        fail(\"Job failed\");\r\n    }\r\n    assertEquals(\"dist job res is not 0:\", 0, res);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "testJobMaxMapConfig",
  "errType" : [ "IllegalArgumentException" ],
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void testJobMaxMapConfig() throws Exception\n{\r\n    config.setInt(MRJobConfig.JOB_MAX_MAP, 0);\r\n    final String[] args = { \"-m\", \"1\", \"-r\", \"1\", \"-mt\", \"1\", \"-rt\", \"1\" };\r\n    int res = -1;\r\n    try {\r\n        res = ToolRunner.run(config, new SleepJob(), args);\r\n        fail(\"Job should fail\");\r\n    } catch (IllegalArgumentException e) {\r\n        assertTrue(e.getLocalizedMessage().contains(\"The number of map tasks 1 exceeded limit\"));\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "testLocalJobFilesOption",
  "errType" : [ "Exception" ],
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void testLocalJobFilesOption() throws IOException\n{\r\n    config.set(FileSystem.FS_DEFAULT_NAME_KEY, \"hdfs://localhost:9000\");\r\n    final String[] args = { \"-jt\", \"local\", \"-files\", jarPath.toString(), \"-m\", \"1\", \"-r\", \"1\", \"-mt\", \"1\", \"-rt\", \"1\" };\r\n    int res = -1;\r\n    try {\r\n        res = ToolRunner.run(config, new SleepJob(), args);\r\n    } catch (Exception e) {\r\n        LOG.error(\"Job failed with {}\", e.getLocalizedMessage(), e);\r\n        fail(\"Job failed\");\r\n    }\r\n    assertEquals(\"dist job res is not 0:\", 0, res);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "testLocalJobArchivesOption",
  "errType" : [ "Exception" ],
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void testLocalJobArchivesOption() throws IOException\n{\r\n    config.set(FileSystem.FS_DEFAULT_NAME_KEY, \"hdfs://localhost:9000\");\r\n    final String[] args = { \"-jt\", \"local\", \"-archives\", jarPath.toString(), \"-m\", \"1\", \"-r\", \"1\", \"-mt\", \"1\", \"-rt\", \"1\" };\r\n    int res = -1;\r\n    try {\r\n        res = ToolRunner.run(config, new SleepJob(), args);\r\n    } catch (Exception e) {\r\n        LOG.error(\"Job failed with {}\" + e.getLocalizedMessage(), e);\r\n        fail(\"Job failed\");\r\n    }\r\n    assertEquals(\"dist job res is not 0:\", 0, res);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "makeJar",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "Path makeJar(Path p) throws IOException\n{\r\n    FileOutputStream fos = new FileOutputStream(p.toString());\r\n    JarOutputStream jos = new JarOutputStream(fos);\r\n    ZipEntry ze = new ZipEntry(\"test.jar.inside\");\r\n    jos.putNextEntry(ze);\r\n    jos.write((\"inside the jar!\").getBytes());\r\n    jos.closeEntry();\r\n    jos.close();\r\n    return p;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "testBinary",
  "errType" : null,
  "containingMethodsNum" : 25,
  "sourceCodeText" : "void testBinary() throws IOException\n{\r\n    JobConf job = new JobConf();\r\n    FileSystem fs = FileSystem.getLocal(job);\r\n    Path dir = new Path(System.getProperty(\"test.build.data\", \".\") + \"/mapred\");\r\n    Path file = new Path(dir, \"testbinary.seq\");\r\n    Random r = new Random();\r\n    long seed = r.nextLong();\r\n    r.setSeed(seed);\r\n    fs.delete(dir, true);\r\n    FileInputFormat.setInputPaths(job, dir);\r\n    Text tkey = new Text();\r\n    Text tval = new Text();\r\n    SequenceFile.Writer writer = new SequenceFile.Writer(fs, job, file, Text.class, Text.class);\r\n    try {\r\n        for (int i = 0; i < RECORDS; ++i) {\r\n            tkey.set(Integer.toString(r.nextInt(), 36));\r\n            tval.set(Long.toString(r.nextLong(), 36));\r\n            writer.append(tkey, tval);\r\n        }\r\n    } finally {\r\n        writer.close();\r\n    }\r\n    InputFormat<BytesWritable, BytesWritable> bformat = new SequenceFileAsBinaryInputFormat();\r\n    int count = 0;\r\n    r.setSeed(seed);\r\n    BytesWritable bkey = new BytesWritable();\r\n    BytesWritable bval = new BytesWritable();\r\n    Text cmpkey = new Text();\r\n    Text cmpval = new Text();\r\n    DataInputBuffer buf = new DataInputBuffer();\r\n    final int NUM_SPLITS = 3;\r\n    FileInputFormat.setInputPaths(job, file);\r\n    for (InputSplit split : bformat.getSplits(job, NUM_SPLITS)) {\r\n        RecordReader<BytesWritable, BytesWritable> reader = bformat.getRecordReader(split, job, Reporter.NULL);\r\n        try {\r\n            while (reader.next(bkey, bval)) {\r\n                tkey.set(Integer.toString(r.nextInt(), 36));\r\n                tval.set(Long.toString(r.nextLong(), 36));\r\n                buf.reset(bkey.getBytes(), bkey.getLength());\r\n                cmpkey.readFields(buf);\r\n                buf.reset(bval.getBytes(), bval.getLength());\r\n                cmpval.readFields(buf);\r\n                assertTrue(\"Keys don't match: \" + \"*\" + cmpkey.toString() + \":\" + tkey.toString() + \"*\", cmpkey.toString().equals(tkey.toString()));\r\n                assertTrue(\"Vals don't match: \" + \"*\" + cmpval.toString() + \":\" + tval.toString() + \"*\", cmpval.toString().equals(tval.toString()));\r\n                ++count;\r\n            }\r\n        } finally {\r\n            reader.close();\r\n        }\r\n    }\r\n    assertEquals(\"Some records not found\", RECORDS, count);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 2,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\ipc",
  "methodName" : "testSocketFactory",
  "errType" : null,
  "containingMethodsNum" : 28,
  "sourceCodeText" : "void testSocketFactory() throws IOException\n{\r\n    Configuration sconf = new Configuration();\r\n    MiniDFSCluster cluster = new MiniDFSCluster.Builder(sconf).numDataNodes(1).build();\r\n    final int nameNodePort = cluster.getNameNodePort();\r\n    FileSystem fs = cluster.getFileSystem();\r\n    Assert.assertTrue(fs instanceof DistributedFileSystem);\r\n    DistributedFileSystem directDfs = (DistributedFileSystem) fs;\r\n    Configuration cconf = getCustomSocketConfigs(nameNodePort);\r\n    fs = FileSystem.get(cconf);\r\n    Assert.assertTrue(fs instanceof DistributedFileSystem);\r\n    DistributedFileSystem dfs = (DistributedFileSystem) fs;\r\n    JobClient client = null;\r\n    MiniMRYarnCluster miniMRYarnCluster = null;\r\n    try {\r\n        Path filePath = new Path(\"/dir\");\r\n        Assert.assertFalse(directDfs.exists(filePath));\r\n        Assert.assertFalse(dfs.exists(filePath));\r\n        directDfs.mkdirs(filePath);\r\n        Assert.assertTrue(directDfs.exists(filePath));\r\n        Assert.assertTrue(dfs.exists(filePath));\r\n        fs = FileSystem.get(sconf);\r\n        JobConf jobConf = new JobConf();\r\n        FileSystem.setDefaultUri(jobConf, fs.getUri().toString());\r\n        miniMRYarnCluster = initAndStartMiniMRYarnCluster(jobConf);\r\n        JobConf jconf = new JobConf(miniMRYarnCluster.getConfig());\r\n        jconf.set(\"hadoop.rpc.socket.factory.class.default\", \"org.apache.hadoop.ipc.DummySocketFactory\");\r\n        jconf.set(MRConfig.FRAMEWORK_NAME, MRConfig.YARN_FRAMEWORK_NAME);\r\n        String rmAddress = jconf.get(YarnConfiguration.RM_ADDRESS);\r\n        String[] split = rmAddress.split(\":\");\r\n        jconf.set(YarnConfiguration.RM_ADDRESS, split[0] + ':' + (Integer.parseInt(split[1]) + 10));\r\n        client = new JobClient(jconf);\r\n        JobStatus[] jobs = client.jobsToComplete();\r\n        Assert.assertTrue(jobs.length == 0);\r\n    } finally {\r\n        closeClient(client);\r\n        closeDfs(dfs);\r\n        closeDfs(directDfs);\r\n        stopMiniMRYarnCluster(miniMRYarnCluster);\r\n        shutdownDFSCluster(cluster);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\ipc",
  "methodName" : "initAndStartMiniMRYarnCluster",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "MiniMRYarnCluster initAndStartMiniMRYarnCluster(JobConf jobConf)\n{\r\n    MiniMRYarnCluster miniMRYarnCluster;\r\n    miniMRYarnCluster = new MiniMRYarnCluster(this.getClass().getName(), 1);\r\n    miniMRYarnCluster.init(jobConf);\r\n    miniMRYarnCluster.start();\r\n    return miniMRYarnCluster;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\ipc",
  "methodName" : "getCustomSocketConfigs",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "Configuration getCustomSocketConfigs(final int nameNodePort)\n{\r\n    Configuration cconf = new Configuration();\r\n    FileSystem.setDefaultUri(cconf, String.format(\"hdfs://localhost:%s/\", nameNodePort + 10));\r\n    cconf.set(\"hadoop.rpc.socket.factory.class.default\", \"org.apache.hadoop.ipc.DummySocketFactory\");\r\n    cconf.set(\"hadoop.rpc.socket.factory.class.ClientProtocol\", \"org.apache.hadoop.ipc.DummySocketFactory\");\r\n    cconf.set(\"hadoop.rpc.socket.factory.class.JobSubmissionProtocol\", \"org.apache.hadoop.ipc.DummySocketFactory\");\r\n    return cconf;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\ipc",
  "methodName" : "shutdownDFSCluster",
  "errType" : [ "Exception" ],
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void shutdownDFSCluster(MiniDFSCluster cluster)\n{\r\n    try {\r\n        if (cluster != null)\r\n            cluster.shutdown();\r\n    } catch (Exception ignored) {\r\n        ignored.printStackTrace();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\ipc",
  "methodName" : "stopMiniMRYarnCluster",
  "errType" : [ "Exception" ],
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void stopMiniMRYarnCluster(MiniMRYarnCluster miniMRYarnCluster)\n{\r\n    try {\r\n        if (miniMRYarnCluster != null)\r\n            miniMRYarnCluster.stop();\r\n    } catch (Exception ignored) {\r\n        ignored.printStackTrace();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\ipc",
  "methodName" : "closeDfs",
  "errType" : [ "Exception" ],
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void closeDfs(DistributedFileSystem dfs)\n{\r\n    try {\r\n        if (dfs != null)\r\n            dfs.close();\r\n    } catch (Exception ignored) {\r\n        ignored.printStackTrace();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\ipc",
  "methodName" : "closeClient",
  "errType" : [ "Exception" ],
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void closeClient(JobClient client)\n{\r\n    try {\r\n        if (client != null)\r\n            client.close();\r\n    } catch (Exception ignored) {\r\n        ignored.printStackTrace();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\ipc",
  "methodName" : "createSocket",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "Socket createSocket() throws IOException\n{\r\n    return new Socket() {\r\n\r\n        @Override\r\n        public void connect(SocketAddress addr, int timeout) throws IOException {\r\n            assert (addr instanceof InetSocketAddress);\r\n            InetSocketAddress iaddr = (InetSocketAddress) addr;\r\n            SocketAddress newAddr = null;\r\n            if (iaddr.isUnresolved())\r\n                newAddr = new InetSocketAddress(iaddr.getHostName(), iaddr.getPort() - 10);\r\n            else\r\n                newAddr = new InetSocketAddress(iaddr.getAddress(), iaddr.getPort() - 10);\r\n            System.out.printf(\"Test socket: rerouting %s to %s\\n\", iaddr, newAddr);\r\n            super.connect(newAddr, timeout);\r\n        }\r\n    };\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\ipc",
  "methodName" : "equals",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean equals(Object obj)\n{\r\n    if (this == obj)\r\n        return true;\r\n    if (obj == null)\r\n        return false;\r\n    if (!(obj instanceof DummySocketFactory))\r\n        return false;\r\n    return true;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2",
  "methodName" : "setup",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 12,
  "sourceCodeText" : "void setup() throws IOException\n{\r\n    try {\r\n        dfsCluster = new MiniDFSCluster.Builder(conf).numDataNodes(2).format(true).racks(null).build();\r\n        remoteFs = dfsCluster.getFileSystem();\r\n    } catch (IOException io) {\r\n        throw new RuntimeException(\"problem starting mini dfs cluster\", io);\r\n    }\r\n    if (!(new File(MiniMRYarnCluster.APPJAR)).exists()) {\r\n        LOG.info(\"MRAppJar \" + MiniMRYarnCluster.APPJAR + \" not found. Not running test.\");\r\n        return;\r\n    }\r\n    if (mrCluster == null) {\r\n        mrCluster = new MiniMRYarnCluster(TestMRJobs.class.getName(), NUM_NODE_MGRS);\r\n        Configuration conf = new Configuration();\r\n        conf.set(\"fs.defaultFS\", remoteFs.getUri().toString());\r\n        conf.set(MRJobConfig.MR_AM_STAGING_DIR, \"/apps_staging_dir\");\r\n        conf.setInt(YarnConfiguration.MAX_CLUSTER_LEVEL_APPLICATION_PRIORITY, 10);\r\n        mrCluster.init(conf);\r\n        mrCluster.start();\r\n    }\r\n    localFs.copyFromLocalFile(new Path(MiniMRYarnCluster.APPJAR), APP_JAR);\r\n    localFs.setPermission(APP_JAR, new FsPermission(\"700\"));\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2",
  "methodName" : "tearDown",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void tearDown() throws IOException\n{\r\n    if (mrCluster != null) {\r\n        mrCluster.stop();\r\n        mrCluster = null;\r\n    }\r\n    if (dfsCluster != null) {\r\n        dfsCluster.shutdown();\r\n        dfsCluster = null;\r\n    }\r\n    if (localFs.exists(TEST_RESOURCES_DIR)) {\r\n        localFs.delete(TEST_RESOURCES_DIR, true);\r\n    }\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2",
  "methodName" : "resetInit",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void resetInit()\n{\r\n    numSleepReducers = DEFAULT_REDUCES;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2",
  "methodName" : "setupJobResourceDirs",
  "errType" : null,
  "containingMethodsNum" : 12,
  "sourceCodeText" : "void setupJobResourceDirs() throws IOException\n{\r\n    if (localFs.exists(TEST_RESOURCES_DIR)) {\r\n        localFs.delete(TEST_RESOURCES_DIR, true);\r\n    }\r\n    localFs.mkdirs(TEST_RESOURCES_DIR);\r\n    FSDataOutputStream outF1 = null;\r\n    try {\r\n        outF1 = localFs.create(new Path(TEST_RESOURCES_DIR, \"file1.txt\"));\r\n        outF1.write(new byte[10 * 1024]);\r\n    } finally {\r\n        if (outF1 != null) {\r\n            outF1.close();\r\n        }\r\n    }\r\n    localFs.createNewFile(new Path(TEST_RESOURCES_DIR, \"file2.txt\"));\r\n    Path subDir = new Path(TEST_RESOURCES_DIR, \"subDir\");\r\n    localFs.mkdirs(subDir);\r\n    FSDataOutputStream outF3 = null;\r\n    try {\r\n        outF3 = localFs.create(new Path(subDir, \"file3.txt\"));\r\n        outF3.write(new byte[(1 * 1024 * 1024) + 10]);\r\n    } finally {\r\n        if (outF3 != null) {\r\n            outF3.close();\r\n        }\r\n    }\r\n    localFs.createNewFile(new Path(subDir, \"file4.txt\"));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 2,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2",
  "methodName" : "testSleepJob",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testSleepJob() throws Exception\n{\r\n    testSleepJobInternal(false);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2",
  "methodName" : "testSleepJobWithRemoteJar",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testSleepJobWithRemoteJar() throws Exception\n{\r\n    testSleepJobInternal(true);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2",
  "methodName" : "testSleepJobWithLocalResourceUnderLimit",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void testSleepJobWithLocalResourceUnderLimit() throws Exception\n{\r\n    Configuration sleepConf = new Configuration(mrCluster.getConfig());\r\n    sleepConf.setInt(MRJobConfig.MAX_RESOURCES, 6);\r\n    sleepConf.setLong(MRJobConfig.MAX_RESOURCES_MB, 6);\r\n    setupJobResourceDirs();\r\n    sleepConf.set(\"tmpfiles\", TEST_RESOURCES_DIR.toString());\r\n    testSleepJobInternal(sleepConf, false, true, null);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2",
  "methodName" : "testSleepJobWithLocalResourceSizeOverLimit",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void testSleepJobWithLocalResourceSizeOverLimit() throws Exception\n{\r\n    Configuration sleepConf = new Configuration(mrCluster.getConfig());\r\n    sleepConf.setLong(MRJobConfig.MAX_RESOURCES_MB, 1);\r\n    setupJobResourceDirs();\r\n    sleepConf.set(\"tmpfiles\", TEST_RESOURCES_DIR.toString());\r\n    testSleepJobInternal(sleepConf, false, false, ResourceViolation.TOTAL_RESOURCE_SIZE);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2",
  "methodName" : "testSleepJobWithLocalResourceNumberOverLimit",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void testSleepJobWithLocalResourceNumberOverLimit() throws Exception\n{\r\n    Configuration sleepConf = new Configuration(mrCluster.getConfig());\r\n    sleepConf.setInt(MRJobConfig.MAX_RESOURCES, 1);\r\n    setupJobResourceDirs();\r\n    sleepConf.set(\"tmpfiles\", TEST_RESOURCES_DIR.toString());\r\n    testSleepJobInternal(sleepConf, false, false, ResourceViolation.NUMBER_OF_RESOURCES);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2",
  "methodName" : "testSleepJobWithLocalResourceCheckAndRemoteJar",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void testSleepJobWithLocalResourceCheckAndRemoteJar() throws Exception\n{\r\n    Configuration sleepConf = new Configuration(mrCluster.getConfig());\r\n    sleepConf.setInt(MRJobConfig.MAX_RESOURCES, 6);\r\n    sleepConf.setLong(MRJobConfig.MAX_RESOURCES_MB, 6);\r\n    setupJobResourceDirs();\r\n    sleepConf.set(\"tmpfiles\", TEST_RESOURCES_DIR.toString());\r\n    testSleepJobInternal(sleepConf, true, true, null);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2",
  "methodName" : "testSleepJobWithLocalIndividualResourceOverLimit",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void testSleepJobWithLocalIndividualResourceOverLimit() throws Exception\n{\r\n    Configuration sleepConf = new Configuration(mrCluster.getConfig());\r\n    sleepConf.setInt(MRJobConfig.MAX_SINGLE_RESOURCE_MB, 1);\r\n    setupJobResourceDirs();\r\n    sleepConf.set(\"tmpfiles\", TEST_RESOURCES_DIR.toString());\r\n    testSleepJobInternal(sleepConf, false, false, ResourceViolation.SINGLE_RESOURCE_SIZE);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2",
  "methodName" : "testSleepJobWithLocalIndividualResourceUnderLimit",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void testSleepJobWithLocalIndividualResourceUnderLimit() throws Exception\n{\r\n    Configuration sleepConf = new Configuration(mrCluster.getConfig());\r\n    sleepConf.setInt(MRJobConfig.MAX_SINGLE_RESOURCE_MB, 2);\r\n    setupJobResourceDirs();\r\n    sleepConf.set(\"tmpfiles\", TEST_RESOURCES_DIR.toString());\r\n    testSleepJobInternal(sleepConf, false, true, null);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2",
  "methodName" : "testSleepJobInternal",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testSleepJobInternal(boolean useRemoteJar) throws Exception\n{\r\n    testSleepJobInternal(new Configuration(mrCluster.getConfig()), useRemoteJar, true, null);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2",
  "methodName" : "testSleepJobInternal",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 31,
  "sourceCodeText" : "void testSleepJobInternal(Configuration sleepConf, boolean useRemoteJar, boolean jobSubmissionShouldSucceed, ResourceViolation violation) throws Exception\n{\r\n    LOG.info(\"\\n\\n\\nStarting testSleepJob: useRemoteJar=\" + useRemoteJar);\r\n    if (!jobSubmissionShouldSucceed && violation == null) {\r\n        Assert.fail(\"Test is misconfigured. jobSubmissionShouldSucceed is set\" + \" to false and a ResourceViolation is not specified.\");\r\n    }\r\n    if (!(new File(MiniMRYarnCluster.APPJAR)).exists()) {\r\n        LOG.info(\"MRAppJar \" + MiniMRYarnCluster.APPJAR + \" not found. Not running test.\");\r\n        return;\r\n    }\r\n    sleepConf.set(MRConfig.MASTER_ADDRESS, \"local\");\r\n    SleepJob sleepJob = new SleepJob();\r\n    sleepJob.setConf(sleepConf);\r\n    Job job = sleepJob.createJob(3, numSleepReducers, 10000, 1, 5000, 1);\r\n    job.addFileToClassPath(APP_JAR);\r\n    if (useRemoteJar) {\r\n        final Path localJar = new Path(ClassUtil.findContainingJar(SleepJob.class));\r\n        ConfigUtil.addLink(job.getConfiguration(), \"/jobjars\", localFs.makeQualified(localJar.getParent()).toUri());\r\n        job.setJar(\"viewfs:///jobjars/\" + localJar.getName());\r\n    } else {\r\n        job.setJarByClass(SleepJob.class);\r\n    }\r\n    job.setMaxMapAttempts(1);\r\n    try {\r\n        job.submit();\r\n        Assert.assertTrue(\"JobSubmission succeeded when it should have failed.\", jobSubmissionShouldSucceed);\r\n    } catch (IOException e) {\r\n        if (jobSubmissionShouldSucceed) {\r\n            Assert.fail(\"Job submission failed when it should have succeeded: \" + e);\r\n        }\r\n        switch(violation) {\r\n            case NUMBER_OF_RESOURCES:\r\n                if (!e.getMessage().contains(\"This job has exceeded the maximum number of\" + \" submitted resources\")) {\r\n                    Assert.fail(\"Test failed unexpectedly: \" + e);\r\n                }\r\n                break;\r\n            case TOTAL_RESOURCE_SIZE:\r\n                if (!e.getMessage().contains(\"This job has exceeded the maximum size of submitted resources\")) {\r\n                    Assert.fail(\"Test failed unexpectedly: \" + e);\r\n                }\r\n                break;\r\n            case SINGLE_RESOURCE_SIZE:\r\n                if (!e.getMessage().contains(\"This job has exceeded the maximum size of a single submitted\")) {\r\n                    Assert.fail(\"Test failed unexpectedly: \" + e);\r\n                }\r\n                break;\r\n            default:\r\n                Assert.fail(\"Test failed unexpectedly: \" + e);\r\n                break;\r\n        }\r\n        return;\r\n    }\r\n    String trackingUrl = job.getTrackingURL();\r\n    String jobId = job.getJobID().toString();\r\n    boolean succeeded = job.waitForCompletion(true);\r\n    Assert.assertTrue(succeeded);\r\n    Assert.assertEquals(JobStatus.State.SUCCEEDED, job.getJobState());\r\n    Assert.assertTrue(\"Tracking URL was \" + trackingUrl + \" but didn't Match Job ID \" + jobId, trackingUrl.endsWith(jobId.substring(jobId.lastIndexOf(\"_\")) + \"/\"));\r\n    verifySleepJobCounters(job);\r\n    verifyTaskProgress(job);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2",
  "methodName" : "testJobWithChangePriority",
  "errType" : null,
  "containingMethodsNum" : 24,
  "sourceCodeText" : "void testJobWithChangePriority() throws Exception\n{\r\n    Configuration sleepConf = new Configuration(mrCluster.getConfig());\r\n    Assume.assumeFalse(sleepConf.get(YarnConfiguration.RM_SCHEDULER).equals(FairScheduler.class.getCanonicalName()));\r\n    if (!(new File(MiniMRYarnCluster.APPJAR)).exists()) {\r\n        LOG.info(\"MRAppJar \" + MiniMRYarnCluster.APPJAR + \" not found. Not running test.\");\r\n        return;\r\n    }\r\n    sleepConf.set(MRConfig.MASTER_ADDRESS, \"local\");\r\n    sleepConf.setInt(MRJobConfig.MR_AM_TO_RM_HEARTBEAT_INTERVAL_MS, 5);\r\n    SleepJob sleepJob = new SleepJob();\r\n    sleepJob.setConf(sleepConf);\r\n    Job job = sleepJob.createJob(1, 1, 1000, 20, 50, 1);\r\n    job.addFileToClassPath(APP_JAR);\r\n    job.setJarByClass(SleepJob.class);\r\n    job.setMaxMapAttempts(1);\r\n    job.submit();\r\n    job.setPriority(JobPriority.HIGH);\r\n    waitForPriorityToUpdate(job, JobPriority.HIGH);\r\n    assertThat(job.getPriority()).isEqualTo(JobPriority.HIGH);\r\n    job.setPriorityAsInteger(3);\r\n    waitForPriorityToUpdate(job, JobPriority.NORMAL);\r\n    assertThat(job.getPriority()).isEqualTo(JobPriority.NORMAL);\r\n    job.setPriorityAsInteger(89);\r\n    waitForPriorityToUpdate(job, JobPriority.UNDEFINED_PRIORITY);\r\n    assertThat(job.getPriority()).isEqualTo(JobPriority.UNDEFINED_PRIORITY);\r\n    boolean succeeded = job.waitForCompletion(true);\r\n    Assert.assertTrue(succeeded);\r\n    Assert.assertEquals(JobStatus.State.SUCCEEDED, job.getJobState());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2",
  "methodName" : "testJobWithWorkflowPriority",
  "errType" : null,
  "containingMethodsNum" : 23,
  "sourceCodeText" : "void testJobWithWorkflowPriority() throws Exception\n{\r\n    Configuration sleepConf = new Configuration(mrCluster.getConfig());\r\n    if (!(new File(MiniMRYarnCluster.APPJAR)).exists()) {\r\n        LOG.info(\"MRAppJar \" + MiniMRYarnCluster.APPJAR + \" not found. Not running test.\");\r\n        return;\r\n    }\r\n    CapacityScheduler scheduler = (CapacityScheduler) mrCluster.getResourceManager().getResourceScheduler();\r\n    CapacitySchedulerConfiguration csConf = scheduler.getConfiguration();\r\n    csConf.set(CapacitySchedulerConfiguration.WORKFLOW_PRIORITY_MAPPINGS, WorkflowPriorityMappingsManager.getWorkflowPriorityMappingStr(Arrays.asList(new WorkflowPriorityMapping(\"wf1\", \"root.default\", Priority.newInstance(1)))));\r\n    csConf.setBoolean(CapacitySchedulerConfiguration.ENABLE_WORKFLOW_PRIORITY_MAPPINGS_OVERRIDE, true);\r\n    scheduler.reinitialize(csConf, scheduler.getRMContext());\r\n    sleepConf.set(MRConfig.MASTER_ADDRESS, \"local\");\r\n    sleepConf.setInt(\"yarn.app.mapreduce.am.scheduler.heartbeat.interval-ms\", 5);\r\n    sleepConf.set(MRJobConfig.JOB_TAGS, YarnConfiguration.DEFAULT_YARN_WORKFLOW_ID_TAG_PREFIX + \"wf1\");\r\n    SleepJob sleepJob = new SleepJob();\r\n    sleepJob.setConf(sleepConf);\r\n    Job job = sleepJob.createJob(1, 1, 1000, 20, 50, 1);\r\n    job.addFileToClassPath(APP_JAR);\r\n    job.setJarByClass(SleepJob.class);\r\n    job.setMaxMapAttempts(1);\r\n    job.setPriority(JobPriority.VERY_HIGH);\r\n    job.submit();\r\n    waitForPriorityToUpdate(job, JobPriority.VERY_LOW);\r\n    Assert.assertEquals(JobPriority.VERY_LOW, job.getPriority());\r\n    boolean succeeded = job.waitForCompletion(true);\r\n    Assert.assertTrue(succeeded);\r\n    Assert.assertEquals(JobStatus.State.SUCCEEDED, job.getJobState());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2",
  "methodName" : "waitForPriorityToUpdate",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void waitForPriorityToUpdate(Job job, JobPriority expectedStatus) throws IOException, InterruptedException\n{\r\n    int waitCnt = 200;\r\n    while (waitCnt-- > 0) {\r\n        if (job.getPriority().equals(expectedStatus)) {\r\n            break;\r\n        } else {\r\n            Thread.sleep(100);\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2",
  "methodName" : "testJobClassloader",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testJobClassloader() throws IOException, InterruptedException, ClassNotFoundException\n{\r\n    testJobClassloader(false);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2",
  "methodName" : "testJobClassloaderWithCustomClasses",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testJobClassloaderWithCustomClasses() throws IOException, InterruptedException, ClassNotFoundException\n{\r\n    testJobClassloader(true);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2",
  "methodName" : "testJobClassloader",
  "errType" : null,
  "containingMethodsNum" : 27,
  "sourceCodeText" : "void testJobClassloader(boolean useCustomClasses) throws IOException, InterruptedException, ClassNotFoundException\n{\r\n    LOG.info(\"\\n\\n\\nStarting testJobClassloader()\" + \" useCustomClasses=\" + useCustomClasses);\r\n    if (!(new File(MiniMRYarnCluster.APPJAR)).exists()) {\r\n        LOG.info(\"MRAppJar \" + MiniMRYarnCluster.APPJAR + \" not found. Not running test.\");\r\n        return;\r\n    }\r\n    final Configuration sleepConf = new Configuration(mrCluster.getConfig());\r\n    sleepConf.set(MRConfig.MASTER_ADDRESS, \"local\");\r\n    sleepConf.setBoolean(MRJobConfig.MAPREDUCE_JOB_CLASSLOADER, true);\r\n    if (useCustomClasses) {\r\n        String systemClasses = ApplicationClassLoader.SYSTEM_CLASSES_DEFAULT;\r\n        systemClasses = \"-\" + CustomOutputFormat.class.getName() + \",-\" + CustomSpeculator.class.getName() + \",\" + systemClasses;\r\n        sleepConf.set(MRJobConfig.MAPREDUCE_JOB_CLASSLOADER_SYSTEM_CLASSES, systemClasses);\r\n    }\r\n    sleepConf.set(MRJobConfig.IO_SORT_MB, TEST_IO_SORT_MB);\r\n    sleepConf.set(MRJobConfig.MR_AM_LOG_LEVEL, Level.ALL.toString());\r\n    sleepConf.set(MRJobConfig.MAP_LOG_LEVEL, Level.ALL.toString());\r\n    sleepConf.set(MRJobConfig.REDUCE_LOG_LEVEL, Level.ALL.toString());\r\n    sleepConf.set(MRJobConfig.MAP_JAVA_OPTS, \"-verbose:class\");\r\n    final SleepJob sleepJob = new SleepJob();\r\n    sleepJob.setConf(sleepConf);\r\n    final Job job = sleepJob.createJob(1, 1, 10, 1, 10, 1);\r\n    job.setMapperClass(ConfVerificationMapper.class);\r\n    job.addFileToClassPath(APP_JAR);\r\n    job.setJarByClass(SleepJob.class);\r\n    job.setMaxMapAttempts(1);\r\n    if (useCustomClasses) {\r\n        job.setOutputFormatClass(CustomOutputFormat.class);\r\n        final Configuration jobConf = job.getConfiguration();\r\n        jobConf.setClass(MRJobConfig.MR_AM_JOB_SPECULATOR, CustomSpeculator.class, Speculator.class);\r\n        jobConf.setBoolean(MRJobConfig.MAP_SPECULATIVE, true);\r\n    }\r\n    job.submit();\r\n    boolean succeeded = job.waitForCompletion(true);\r\n    Assert.assertTrue(\"Job status: \" + job.getStatus().getFailureInfo(), succeeded);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2",
  "methodName" : "verifySleepJobCounters",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void verifySleepJobCounters(Job job) throws InterruptedException, IOException\n{\r\n    Counters counters = job.getCounters();\r\n    Assert.assertEquals(3, counters.findCounter(JobCounter.OTHER_LOCAL_MAPS).getValue());\r\n    Assert.assertEquals(3, counters.findCounter(JobCounter.TOTAL_LAUNCHED_MAPS).getValue());\r\n    Assert.assertEquals(numSleepReducers, counters.findCounter(JobCounter.TOTAL_LAUNCHED_REDUCES).getValue());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2",
  "methodName" : "verifyTaskProgress",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void verifyTaskProgress(Job job) throws InterruptedException, IOException\n{\r\n    for (TaskReport taskReport : job.getTaskReports(TaskType.MAP)) {\r\n        Assert.assertTrue(0.9999f < taskReport.getProgress() && 1.0001f > taskReport.getProgress());\r\n    }\r\n    for (TaskReport taskReport : job.getTaskReports(TaskType.REDUCE)) {\r\n        Assert.assertTrue(0.9999f < taskReport.getProgress() && 1.0001f > taskReport.getProgress());\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2",
  "methodName" : "testRandomWriter",
  "errType" : null,
  "containingMethodsNum" : 24,
  "sourceCodeText" : "void testRandomWriter() throws IOException, InterruptedException, ClassNotFoundException\n{\r\n    LOG.info(\"\\n\\n\\nStarting testRandomWriter().\");\r\n    if (!(new File(MiniMRYarnCluster.APPJAR)).exists()) {\r\n        LOG.info(\"MRAppJar \" + MiniMRYarnCluster.APPJAR + \" not found. Not running test.\");\r\n        return;\r\n    }\r\n    RandomTextWriterJob randomWriterJob = new RandomTextWriterJob();\r\n    mrCluster.getConfig().set(RandomTextWriterJob.TOTAL_BYTES, \"3072\");\r\n    mrCluster.getConfig().set(RandomTextWriterJob.BYTES_PER_MAP, \"1024\");\r\n    Job job = randomWriterJob.createJob(mrCluster.getConfig());\r\n    Path outputDir = new Path(OUTPUT_ROOT_DIR, \"random-output\");\r\n    FileOutputFormat.setOutputPath(job, outputDir);\r\n    job.setSpeculativeExecution(false);\r\n    job.addFileToClassPath(APP_JAR);\r\n    job.setJarByClass(RandomTextWriterJob.class);\r\n    job.setMaxMapAttempts(1);\r\n    job.submit();\r\n    String trackingUrl = job.getTrackingURL();\r\n    String jobId = job.getJobID().toString();\r\n    boolean succeeded = job.waitForCompletion(true);\r\n    Assert.assertTrue(succeeded);\r\n    Assert.assertEquals(JobStatus.State.SUCCEEDED, job.getJobState());\r\n    Assert.assertTrue(\"Tracking URL was \" + trackingUrl + \" but didn't Match Job ID \" + jobId, trackingUrl.endsWith(jobId.substring(jobId.lastIndexOf(\"_\")) + \"/\"));\r\n    RemoteIterator<FileStatus> iterator = FileContext.getFileContext(mrCluster.getConfig()).listStatus(outputDir);\r\n    int count = 0;\r\n    while (iterator.hasNext()) {\r\n        FileStatus file = iterator.next();\r\n        if (!file.getPath().getName().equals(FileOutputCommitter.SUCCEEDED_FILE_NAME)) {\r\n            count++;\r\n        }\r\n    }\r\n    Assert.assertEquals(\"Number of part files is wrong!\", 3, count);\r\n    verifyRandomWriterCounters(job);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2",
  "methodName" : "verifyRandomWriterCounters",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void verifyRandomWriterCounters(Job job) throws InterruptedException, IOException\n{\r\n    Counters counters = job.getCounters();\r\n    Assert.assertEquals(3, counters.findCounter(JobCounter.OTHER_LOCAL_MAPS).getValue());\r\n    Assert.assertEquals(3, counters.findCounter(JobCounter.TOTAL_LAUNCHED_MAPS).getValue());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2",
  "methodName" : "testFailingMapper",
  "errType" : null,
  "containingMethodsNum" : 16,
  "sourceCodeText" : "void testFailingMapper() throws IOException, InterruptedException, ClassNotFoundException\n{\r\n    LOG.info(\"\\n\\n\\nStarting testFailingMapper().\");\r\n    if (!(new File(MiniMRYarnCluster.APPJAR)).exists()) {\r\n        LOG.info(\"MRAppJar \" + MiniMRYarnCluster.APPJAR + \" not found. Not running test.\");\r\n        return;\r\n    }\r\n    Job job = runFailingMapperJob();\r\n    TaskID taskID = new TaskID(job.getJobID(), TaskType.MAP, 0);\r\n    TaskAttemptID aId = new TaskAttemptID(taskID, 0);\r\n    System.out.println(\"Diagnostics for \" + aId + \" :\");\r\n    for (String diag : job.getTaskDiagnostics(aId)) {\r\n        System.out.println(diag);\r\n    }\r\n    aId = new TaskAttemptID(taskID, 1);\r\n    System.out.println(\"Diagnostics for \" + aId + \" :\");\r\n    for (String diag : job.getTaskDiagnostics(aId)) {\r\n        System.out.println(diag);\r\n    }\r\n    TaskCompletionEvent[] events = job.getTaskCompletionEvents(0, 2);\r\n    Assert.assertEquals(TaskCompletionEvent.Status.FAILED, events[0].getStatus());\r\n    Assert.assertEquals(TaskCompletionEvent.Status.TIPFAILED, events[1].getStatus());\r\n    Assert.assertEquals(JobStatus.State.FAILED, job.getJobState());\r\n    verifyFailingMapperCounters(job);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2",
  "methodName" : "verifyFailingMapperCounters",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void verifyFailingMapperCounters(Job job) throws InterruptedException, IOException\n{\r\n    Counters counters = job.getCounters();\r\n    Assert.assertEquals(2, counters.findCounter(JobCounter.OTHER_LOCAL_MAPS).getValue());\r\n    Assert.assertEquals(2, counters.findCounter(JobCounter.TOTAL_LAUNCHED_MAPS).getValue());\r\n    Assert.assertEquals(2, counters.findCounter(JobCounter.NUM_FAILED_MAPS).getValue());\r\n    Assert.assertTrue(counters.findCounter(JobCounter.SLOTS_MILLIS_MAPS) != null && counters.findCounter(JobCounter.SLOTS_MILLIS_MAPS).getValue() != 0);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2",
  "methodName" : "runFailingMapperJob",
  "errType" : null,
  "containingMethodsNum" : 20,
  "sourceCodeText" : "Job runFailingMapperJob() throws IOException, InterruptedException, ClassNotFoundException\n{\r\n    Configuration myConf = new Configuration(mrCluster.getConfig());\r\n    myConf.setInt(MRJobConfig.NUM_MAPS, 1);\r\n    myConf.setInt(MRJobConfig.MAP_MAX_ATTEMPTS, 2);\r\n    Job job = Job.getInstance(myConf);\r\n    job.setJarByClass(FailingMapper.class);\r\n    job.setJobName(\"failmapper\");\r\n    job.setOutputKeyClass(Text.class);\r\n    job.setOutputValueClass(Text.class);\r\n    job.setInputFormatClass(RandomInputFormat.class);\r\n    job.setOutputFormatClass(TextOutputFormat.class);\r\n    job.setMapperClass(FailingMapper.class);\r\n    job.setNumReduceTasks(0);\r\n    FileOutputFormat.setOutputPath(job, new Path(OUTPUT_ROOT_DIR, \"failmapper-output\"));\r\n    job.addFileToClassPath(APP_JAR);\r\n    job.submit();\r\n    String trackingUrl = job.getTrackingURL();\r\n    String jobId = job.getJobID().toString();\r\n    boolean succeeded = job.waitForCompletion(true);\r\n    Assert.assertFalse(succeeded);\r\n    Assert.assertTrue(\"Tracking URL was \" + trackingUrl + \" but didn't Match Job ID \" + jobId, trackingUrl.endsWith(jobId.substring(jobId.lastIndexOf(\"_\")) + \"/\"));\r\n    return job;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2",
  "methodName" : "testSleepJobWithSecurityOn",
  "errType" : null,
  "containingMethodsNum" : 13,
  "sourceCodeText" : "void testSleepJobWithSecurityOn() throws IOException, InterruptedException, ClassNotFoundException\n{\r\n    LOG.info(\"\\n\\n\\nStarting testSleepJobWithSecurityOn().\");\r\n    if (!(new File(MiniMRYarnCluster.APPJAR)).exists()) {\r\n        return;\r\n    }\r\n    mrCluster.getConfig().set(CommonConfigurationKeysPublic.HADOOP_SECURITY_AUTHENTICATION, \"kerberos\");\r\n    mrCluster.getConfig().set(YarnConfiguration.RM_KEYTAB, \"/etc/krb5.keytab\");\r\n    mrCluster.getConfig().set(YarnConfiguration.NM_KEYTAB, \"/etc/krb5.keytab\");\r\n    mrCluster.getConfig().set(YarnConfiguration.RM_PRINCIPAL, \"rm/sightbusy-lx@LOCALHOST\");\r\n    mrCluster.getConfig().set(YarnConfiguration.NM_PRINCIPAL, \"nm/sightbusy-lx@LOCALHOST\");\r\n    UserGroupInformation.setConfiguration(mrCluster.getConfig());\r\n    UserGroupInformation user = UserGroupInformation.getCurrentUser();\r\n    LOG.info(\"User name is \" + user.getUserName());\r\n    for (Token<? extends TokenIdentifier> str : user.getTokens()) {\r\n        LOG.info(\"Token is \" + str.encodeToUrlString());\r\n    }\r\n    user.doAs(new PrivilegedExceptionAction<Void>() {\r\n\r\n        @Override\r\n        public Void run() throws Exception {\r\n            SleepJob sleepJob = new SleepJob();\r\n            sleepJob.setConf(mrCluster.getConfig());\r\n            Job job = sleepJob.createJob(3, 0, 10000, 1, 0, 0);\r\n            job.addFileToClassPath(APP_JAR);\r\n            job.submit();\r\n            String trackingUrl = job.getTrackingURL();\r\n            String jobId = job.getJobID().toString();\r\n            job.waitForCompletion(true);\r\n            Assert.assertEquals(JobStatus.State.SUCCEEDED, job.getJobState());\r\n            Assert.assertTrue(\"Tracking URL was \" + trackingUrl + \" but didn't Match Job ID \" + jobId, trackingUrl.endsWith(jobId.substring(jobId.lastIndexOf(\"_\")) + \"/\"));\r\n            return null;\r\n        }\r\n    });\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2",
  "methodName" : "testContainerRollingLog",
  "errType" : null,
  "containingMethodsNum" : 40,
  "sourceCodeText" : "void testContainerRollingLog() throws IOException, InterruptedException, ClassNotFoundException\n{\r\n    if (!(new File(MiniMRYarnCluster.APPJAR)).exists()) {\r\n        LOG.info(\"MRAppJar \" + MiniMRYarnCluster.APPJAR + \" not found. Not running test.\");\r\n        return;\r\n    }\r\n    final SleepJob sleepJob = new SleepJob();\r\n    final JobConf sleepConf = new JobConf(mrCluster.getConfig());\r\n    sleepConf.set(MRJobConfig.MAP_LOG_LEVEL, Level.ALL.toString());\r\n    final long userLogKb = 4;\r\n    sleepConf.setLong(MRJobConfig.TASK_USERLOG_LIMIT, userLogKb);\r\n    sleepConf.setInt(MRJobConfig.TASK_LOG_BACKUPS, 3);\r\n    sleepConf.set(MRJobConfig.MR_AM_LOG_LEVEL, Level.ALL.toString());\r\n    final long amLogKb = 7;\r\n    sleepConf.setLong(MRJobConfig.MR_AM_LOG_KB, amLogKb);\r\n    sleepConf.setInt(MRJobConfig.MR_AM_LOG_BACKUPS, 7);\r\n    sleepJob.setConf(sleepConf);\r\n    final Job job = sleepJob.createJob(1, 0, 1L, 100, 0L, 0);\r\n    job.setJarByClass(SleepJob.class);\r\n    job.addFileToClassPath(APP_JAR);\r\n    job.waitForCompletion(true);\r\n    final JobId jobId = TypeConverter.toYarn(job.getJobID());\r\n    final ApplicationId appID = jobId.getAppId();\r\n    int pollElapsed = 0;\r\n    while (true) {\r\n        Thread.sleep(1000);\r\n        pollElapsed += 1000;\r\n        if (TERMINAL_RM_APP_STATES.contains(mrCluster.getResourceManager().getRMContext().getRMApps().get(appID).getState())) {\r\n            break;\r\n        }\r\n        if (pollElapsed >= 60000) {\r\n            LOG.warn(\"application did not reach terminal state within 60 seconds\");\r\n            break;\r\n        }\r\n    }\r\n    Assert.assertEquals(RMAppState.FINISHED, mrCluster.getResourceManager().getRMContext().getRMApps().get(appID).getState());\r\n    final String appIdStr = appID.toString();\r\n    final String appIdSuffix = appIdStr.substring(\"application_\".length(), appIdStr.length());\r\n    final String containerGlob = \"container_\" + appIdSuffix + \"_*_*\";\r\n    final String syslogGlob = appIdStr + Path.SEPARATOR + containerGlob + Path.SEPARATOR + TaskLog.LogName.SYSLOG;\r\n    int numAppMasters = 0;\r\n    int numMapTasks = 0;\r\n    for (int i = 0; i < NUM_NODE_MGRS; i++) {\r\n        final Configuration nmConf = mrCluster.getNodeManager(i).getConfig();\r\n        for (String logDir : nmConf.getTrimmedStrings(YarnConfiguration.NM_LOG_DIRS)) {\r\n            final Path absSyslogGlob = new Path(logDir + Path.SEPARATOR + syslogGlob);\r\n            LOG.info(\"Checking for glob: \" + absSyslogGlob);\r\n            final FileStatus[] syslogs = localFs.globStatus(absSyslogGlob);\r\n            for (FileStatus slog : syslogs) {\r\n                boolean foundAppMaster = job.isUber();\r\n                final Path containerPathComponent = slog.getPath().getParent();\r\n                if (!foundAppMaster) {\r\n                    final ContainerId cid = ContainerId.fromString(containerPathComponent.getName());\r\n                    foundAppMaster = ((cid.getContainerId() & ContainerId.CONTAINER_ID_BITMASK) == 1);\r\n                }\r\n                final FileStatus[] sysSiblings = localFs.globStatus(new Path(containerPathComponent, TaskLog.LogName.SYSLOG + \"*\"));\r\n                Arrays.sort(sysSiblings);\r\n                if (foundAppMaster) {\r\n                    numAppMasters++;\r\n                } else {\r\n                    numMapTasks++;\r\n                }\r\n                if (foundAppMaster) {\r\n                    Assert.assertSame(\"Unexpected number of AM sylog* files\", sleepConf.getInt(MRJobConfig.MR_AM_LOG_BACKUPS, 0) + 1, sysSiblings.length);\r\n                    Assert.assertTrue(\"AM syslog.1 length kb should be >= \" + amLogKb, sysSiblings[1].getLen() >= amLogKb * 1024);\r\n                } else {\r\n                    Assert.assertSame(\"Unexpected number of MR task sylog* files\", sleepConf.getInt(MRJobConfig.TASK_LOG_BACKUPS, 0) + 1, sysSiblings.length);\r\n                    Assert.assertTrue(\"MR syslog.1 length kb should be >= \" + userLogKb, sysSiblings[1].getLen() >= userLogKb * 1024);\r\n                }\r\n            }\r\n        }\r\n    }\r\n    Assert.assertEquals(\"No AppMaster log found!\", 1, numAppMasters);\r\n    if (sleepConf.getBoolean(MRJobConfig.JOB_UBERTASK_ENABLE, false)) {\r\n        Assert.assertEquals(\"MapTask log with uber found!\", 0, numMapTasks);\r\n    } else {\r\n        Assert.assertEquals(\"No MapTask log found!\", 1, numMapTasks);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2",
  "methodName" : "testDistributedCache",
  "errType" : null,
  "containingMethodsNum" : 28,
  "sourceCodeText" : "void testDistributedCache(String jobJarPath, boolean withWildcard) throws Exception\n{\r\n    if (!(new File(MiniMRYarnCluster.APPJAR)).exists()) {\r\n        LOG.info(\"MRAppJar \" + MiniMRYarnCluster.APPJAR + \" not found. Not running test.\");\r\n        return;\r\n    }\r\n    Path first = createTempFile(\"distributed.first\", \"x\");\r\n    Path second = makeJar(new Path(TEST_ROOT_DIR, \"distributed.second.jar\"), 2);\r\n    Path third = makeJar(new Path(TEST_ROOT_DIR, \"distributed.third.jar\"), 3);\r\n    Path fourth = makeJar(new Path(TEST_ROOT_DIR, \"distributed.fourth.jar\"), 4);\r\n    Job job = Job.getInstance(mrCluster.getConfig());\r\n    job.setJar(jobJarPath);\r\n    if (withWildcard) {\r\n        Path libs = new Path(\"testLibs\");\r\n        Path wildcard = remoteFs.makeQualified(new Path(libs, \"*\"));\r\n        remoteFs.mkdirs(libs);\r\n        remoteFs.copyFromLocalFile(third, libs);\r\n        job.addCacheFile(wildcard.toUri());\r\n    } else {\r\n        Path distributedCacheCheckerJar = new Path(JarFinder.getJar(DistributedCacheChecker.class));\r\n        job.addFileToClassPath(localFs.makeQualified(distributedCacheCheckerJar));\r\n    }\r\n    job.setMapperClass(DistributedCacheChecker.class);\r\n    job.setOutputFormatClass(NullOutputFormat.class);\r\n    FileInputFormat.setInputPaths(job, first);\r\n    job.addCacheFile(new URI(first.toUri().toString() + \"#distributed.first.symlink\"));\r\n    job.addFileToClassPath(second);\r\n    job.addFileToClassPath(APP_JAR.makeQualified(localFs.getUri(), APP_JAR.getParent()));\r\n    job.addArchiveToClassPath(third);\r\n    job.addCacheArchive(fourth.toUri());\r\n    job.setMaxMapAttempts(1);\r\n    job.submit();\r\n    String trackingUrl = job.getTrackingURL();\r\n    String jobId = job.getJobID().toString();\r\n    Assert.assertTrue(job.waitForCompletion(false));\r\n    Assert.assertTrue(\"Tracking URL was \" + trackingUrl + \" but didn't Match Job ID \" + jobId, trackingUrl.endsWith(jobId.substring(jobId.lastIndexOf(\"_\")) + \"/\"));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2",
  "methodName" : "testDistributedCache",
  "errType" : null,
  "containingMethodsNum" : 9,
  "sourceCodeText" : "void testDistributedCache(boolean withWildcard) throws Exception\n{\r\n    Path localJobJarPath = makeJobJarWithLib(TEST_ROOT_DIR.toUri().toString());\r\n    testDistributedCache(localJobJarPath.toUri().toString(), withWildcard);\r\n    Path remoteJobJarPath = new Path(remoteFs.getUri().toString() + \"/\", localJobJarPath.getName());\r\n    remoteFs.moveFromLocalFile(localJobJarPath, remoteJobJarPath);\r\n    File localJobJarFile = new File(localJobJarPath.toUri().toString());\r\n    if (localJobJarFile.exists()) {\r\n        localJobJarFile.delete();\r\n    }\r\n    testDistributedCache(remoteJobJarPath.toUri().toString(), withWildcard);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2",
  "methodName" : "testDistributedCache",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testDistributedCache() throws Exception\n{\r\n    testDistributedCache(false);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2",
  "methodName" : "testDistributedCacheWithWildcards",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testDistributedCacheWithWildcards() throws Exception\n{\r\n    testDistributedCache(true);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2",
  "methodName" : "testThreadDumpOnTaskTimeout",
  "errType" : null,
  "containingMethodsNum" : 39,
  "sourceCodeText" : "void testThreadDumpOnTaskTimeout() throws IOException, InterruptedException, ClassNotFoundException\n{\r\n    if (!(new File(MiniMRYarnCluster.APPJAR)).exists()) {\r\n        LOG.info(\"MRAppJar \" + MiniMRYarnCluster.APPJAR + \" not found. Not running test.\");\r\n        return;\r\n    }\r\n    final SleepJob sleepJob = new SleepJob();\r\n    final JobConf sleepConf = new JobConf(mrCluster.getConfig());\r\n    sleepConf.setLong(MRJobConfig.TASK_TIMEOUT, 3 * 1000L);\r\n    sleepConf.setInt(MRJobConfig.MAP_MAX_ATTEMPTS, 1);\r\n    sleepJob.setConf(sleepConf);\r\n    if (this instanceof TestUberAM) {\r\n        sleepConf.setInt(MRJobConfig.MR_AM_TO_RM_HEARTBEAT_INTERVAL_MS, 30 * 1000);\r\n    }\r\n    final Job job = sleepJob.createJob(1, 0, 10 * 60 * 1000L, 1, 0L, 0);\r\n    job.setJarByClass(SleepJob.class);\r\n    job.addFileToClassPath(APP_JAR);\r\n    job.waitForCompletion(true);\r\n    final JobId jobId = TypeConverter.toYarn(job.getJobID());\r\n    final ApplicationId appID = jobId.getAppId();\r\n    int pollElapsed = 0;\r\n    while (true) {\r\n        Thread.sleep(1000);\r\n        pollElapsed += 1000;\r\n        if (TERMINAL_RM_APP_STATES.contains(mrCluster.getResourceManager().getRMContext().getRMApps().get(appID).getState())) {\r\n            break;\r\n        }\r\n        if (pollElapsed >= 60000) {\r\n            LOG.warn(\"application did not reach terminal state within 60 seconds\");\r\n            break;\r\n        }\r\n    }\r\n    final String appIdStr = appID.toString();\r\n    final String appIdSuffix = appIdStr.substring(\"application_\".length(), appIdStr.length());\r\n    final String containerGlob = \"container_\" + appIdSuffix + \"_*_*\";\r\n    final String syslogGlob = appIdStr + Path.SEPARATOR + containerGlob + Path.SEPARATOR + TaskLog.LogName.SYSLOG;\r\n    int numAppMasters = 0;\r\n    int numMapTasks = 0;\r\n    for (int i = 0; i < NUM_NODE_MGRS; i++) {\r\n        final Configuration nmConf = mrCluster.getNodeManager(i).getConfig();\r\n        for (String logDir : nmConf.getTrimmedStrings(YarnConfiguration.NM_LOG_DIRS)) {\r\n            final Path absSyslogGlob = new Path(logDir + Path.SEPARATOR + syslogGlob);\r\n            LOG.info(\"Checking for glob: \" + absSyslogGlob);\r\n            for (FileStatus syslog : localFs.globStatus(absSyslogGlob)) {\r\n                boolean foundAppMaster = false;\r\n                boolean foundThreadDump = false;\r\n                final BufferedReader syslogReader = new BufferedReader(new InputStreamReader(localFs.open(syslog.getPath())));\r\n                try {\r\n                    for (String line; (line = syslogReader.readLine()) != null; ) {\r\n                        if (line.contains(MRAppMaster.class.getName())) {\r\n                            foundAppMaster = true;\r\n                            break;\r\n                        }\r\n                    }\r\n                } finally {\r\n                    syslogReader.close();\r\n                }\r\n                final Path stdoutPath = new Path(syslog.getPath().getParent(), TaskLog.LogName.STDOUT.toString());\r\n                final BufferedReader stdoutReader = new BufferedReader(new InputStreamReader(localFs.open(stdoutPath)));\r\n                try {\r\n                    for (String line; (line = stdoutReader.readLine()) != null; ) {\r\n                        if (line.contains(\"Full thread dump\")) {\r\n                            foundThreadDump = true;\r\n                            break;\r\n                        }\r\n                    }\r\n                } finally {\r\n                    stdoutReader.close();\r\n                }\r\n                if (foundAppMaster) {\r\n                    numAppMasters++;\r\n                    if (this instanceof TestUberAM) {\r\n                        Assert.assertTrue(\"No thread dump\", foundThreadDump);\r\n                    } else {\r\n                        Assert.assertFalse(\"Unexpected thread dump\", foundThreadDump);\r\n                    }\r\n                } else {\r\n                    numMapTasks++;\r\n                    Assert.assertTrue(\"No thread dump\", foundThreadDump);\r\n                }\r\n            }\r\n        }\r\n    }\r\n    Assert.assertEquals(\"No AppMaster log found!\", 1, numAppMasters);\r\n    if (sleepConf.getBoolean(MRJobConfig.JOB_UBERTASK_ENABLE, false)) {\r\n        Assert.assertSame(\"MapTask log with uber found!\", 0, numMapTasks);\r\n    } else {\r\n        Assert.assertSame(\"No MapTask log found!\", 1, numMapTasks);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 2,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2",
  "methodName" : "createTempFile",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "Path createTempFile(String filename, String contents) throws IOException\n{\r\n    Path path = new Path(TEST_ROOT_DIR, filename);\r\n    FSDataOutputStream os = localFs.create(path);\r\n    os.writeBytes(contents);\r\n    os.close();\r\n    localFs.setPermission(path, new FsPermission(\"700\"));\r\n    return path;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2",
  "methodName" : "makeJar",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "Path makeJar(Path p, int index) throws FileNotFoundException, IOException\n{\r\n    FileOutputStream fos = new FileOutputStream(new File(p.toUri().getPath()));\r\n    JarOutputStream jos = new JarOutputStream(fos);\r\n    ZipEntry ze = new ZipEntry(\"distributed.jar.inside\" + index);\r\n    jos.putNextEntry(ze);\r\n    jos.write((\"inside the jar!\" + index).getBytes());\r\n    jos.closeEntry();\r\n    jos.close();\r\n    localFs.setPermission(p, new FsPermission(\"700\"));\r\n    return p;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2",
  "methodName" : "makeJobJarWithLib",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "Path makeJobJarWithLib(String testDir) throws FileNotFoundException, IOException\n{\r\n    Path jobJarPath = new Path(testDir, \"thejob.jar\");\r\n    FileOutputStream fos = new FileOutputStream(new File(jobJarPath.toUri().getPath()));\r\n    JarOutputStream jos = new JarOutputStream(fos);\r\n    createAndAddJarToJar(jos, new File(new Path(testDir, \"lib1.jar\").toUri().getPath()));\r\n    createAndAddJarToJar(jos, new File(new Path(testDir, \"lib2.jar\").toUri().getPath()));\r\n    jos.close();\r\n    localFs.setPermission(jobJarPath, new FsPermission(\"700\"));\r\n    return jobJarPath;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2",
  "methodName" : "createAndAddJarToJar",
  "errType" : null,
  "containingMethodsNum" : 10,
  "sourceCodeText" : "void createAndAddJarToJar(JarOutputStream jos, File jarFile) throws FileNotFoundException, IOException\n{\r\n    FileOutputStream fos2 = new FileOutputStream(jarFile);\r\n    JarOutputStream jos2 = new JarOutputStream(fos2);\r\n    ZipEntry ze = new ZipEntry(\"lib1.inside\");\r\n    jos2.putNextEntry(ze);\r\n    jos2.closeEntry();\r\n    jos2.close();\r\n    ze = new ZipEntry(\"lib/\" + jarFile.getName());\r\n    jos.putNextEntry(ze);\r\n    FileInputStream in = new FileInputStream(jarFile);\r\n    byte[] buf = new byte[1024];\r\n    int numRead;\r\n    do {\r\n        numRead = in.read(buf);\r\n        if (numRead >= 0) {\r\n            jos.write(buf, 0, numRead);\r\n        }\r\n    } while (numRead != -1);\r\n    in.close();\r\n    jos.closeEntry();\r\n    jarFile.delete();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2",
  "methodName" : "testSharedCache",
  "errType" : null,
  "containingMethodsNum" : 22,
  "sourceCodeText" : "void testSharedCache() throws Exception\n{\r\n    Path localJobJarPath = makeJobJarWithLib(TEST_ROOT_DIR.toUri().toString());\r\n    if (!(new File(MiniMRYarnCluster.APPJAR)).exists()) {\r\n        LOG.info(\"MRAppJar \" + MiniMRYarnCluster.APPJAR + \" not found. Not running test.\");\r\n        return;\r\n    }\r\n    Job job = Job.getInstance(mrCluster.getConfig());\r\n    Configuration jobConf = job.getConfiguration();\r\n    jobConf.set(MRJobConfig.SHARED_CACHE_MODE, \"enabled\");\r\n    Path inputFile = createTempFile(\"input-file\", \"x\");\r\n    Path second = makeJar(new Path(TEST_ROOT_DIR, \"distributed.second.jar\"), 2);\r\n    Path third = makeJar(new Path(TEST_ROOT_DIR, \"distributed.third.jar\"), 3);\r\n    Path fourth = makeJar(new Path(TEST_ROOT_DIR, \"distributed.fourth.jar\"), 4);\r\n    jobConf.set(\"tmpjars\", second.toString() + \",\" + third.toString() + \",\" + fourth.toString());\r\n    Path distributedCacheCheckerJar = new Path(JarFinder.getJar(SharedCacheChecker.class));\r\n    job.addFileToClassPath(distributedCacheCheckerJar.makeQualified(localFs.getUri(), distributedCacheCheckerJar.getParent()));\r\n    job.setMapperClass(SharedCacheChecker.class);\r\n    job.setOutputFormatClass(NullOutputFormat.class);\r\n    FileInputFormat.setInputPaths(job, inputFile);\r\n    job.setMaxMapAttempts(1);\r\n    job.submit();\r\n    String trackingUrl = job.getTrackingURL();\r\n    String jobId = job.getJobID().toString();\r\n    Assert.assertTrue(job.waitForCompletion(true));\r\n    Assert.assertTrue(\"Tracking URL was \" + trackingUrl + \" but didn't Match Job ID \" + jobId, trackingUrl.endsWith(jobId.substring(jobId.lastIndexOf(\"_\")) + \"/\"));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2",
  "methodName" : "testSleepJobName",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void testSleepJobName() throws IOException\n{\r\n    SleepJob sleepJob = new SleepJob();\r\n    sleepJob.setConf(conf);\r\n    Job job1 = sleepJob.createJob(1, 1, 1, 1, 1, 1);\r\n    assertThat(job1.getJobName()).withFailMessage(\"Wrong default name of sleep job.\").isEqualTo(SleepJob.SLEEP_JOB_NAME);\r\n    String expectedJob2Name = SleepJob.SLEEP_JOB_NAME + \" - test\";\r\n    Job job2 = sleepJob.createJob(1, 1, 1, 1, 1, 1, \"test\");\r\n    assertThat(job2.getJobName()).withFailMessage(\"Wrong name of sleep job.\").isEqualTo(expectedJob2Name);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\fs",
  "methodName" : "testFs",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testFs() throws Exception\n{\r\n    testFs(10 * MEGA, 100, 0);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\fs",
  "methodName" : "testFs",
  "errType" : null,
  "containingMethodsNum" : 11,
  "sourceCodeText" : "void testFs(long megaBytes, int numFiles, long seed) throws Exception\n{\r\n    FileSystem fs = FileSystem.get(conf);\r\n    if (seed == 0)\r\n        seed = new Random().nextLong();\r\n    LOG.info(\"seed = \" + seed);\r\n    createControlFile(fs, megaBytes, numFiles, seed);\r\n    writeTest(fs, false);\r\n    readTest(fs, false);\r\n    seekTest(fs, false);\r\n    fs.delete(CONTROL_DIR, true);\r\n    fs.delete(DATA_DIR, true);\r\n    fs.delete(WRITE_DIR, true);\r\n    fs.delete(READ_DIR, true);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\fs",
  "methodName" : "testCommandFormat",
  "errType" : [ "Exception" ],
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void testCommandFormat() throws Exception\n{\r\n    CommandFormat cf;\r\n    cf = new CommandFormat(\"copyToLocal\", 2, 2, \"crc\", \"ignoreCrc\");\r\n    assertThat(cf.parse(new String[] { \"-get\", \"file\", \"-\" }, 1).get(1)).isEqualTo(\"-\");\r\n    try {\r\n        cf.parse(new String[] { \"-get\", \"file\", \"-ignoreCrc\", \"/foo\" }, 1);\r\n        fail(\"Expected parsing to fail as it should stop at first non-option\");\r\n    } catch (Exception e) {\r\n    }\r\n    cf = new CommandFormat(\"tail\", 1, 1, \"f\");\r\n    assertThat(cf.parse(new String[] { \"-tail\", \"fileName\" }, 1).get(0)).isEqualTo(\"fileName\");\r\n    assertThat(cf.parse(new String[] { \"-tail\", \"-f\", \"fileName\" }, 1).get(0)).isEqualTo(\"fileName\");\r\n    cf = new CommandFormat(\"setrep\", 2, 2, \"R\", \"w\");\r\n    assertThat(cf.parse(new String[] { \"-setrep\", \"-R\", \"2\", \"/foo/bar\" }, 1).get(1)).isEqualTo(\"/foo/bar\");\r\n    cf = new CommandFormat(\"put\", 2, 10000);\r\n    assertThat(cf.parse(new String[] { \"-put\", \"-\", \"dest\" }, 1).get(1)).isEqualTo(\"dest\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\fs",
  "methodName" : "createControlFile",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void createControlFile(FileSystem fs, long megaBytes, int numFiles, long seed) throws Exception\n{\r\n    LOG.info(\"creating control file: \" + megaBytes + \" bytes, \" + numFiles + \" files\");\r\n    Path controlFile = new Path(CONTROL_DIR, \"files\");\r\n    fs.delete(controlFile, true);\r\n    Random random = new Random(seed);\r\n    SequenceFile.Writer writer = SequenceFile.createWriter(fs, conf, controlFile, Text.class, LongWritable.class, CompressionType.NONE);\r\n    long totalSize = 0;\r\n    long maxSize = ((megaBytes / numFiles) * 2) + 1;\r\n    try {\r\n        while (totalSize < megaBytes) {\r\n            Text name = new Text(Long.toString(random.nextLong()));\r\n            long size = random.nextLong();\r\n            if (size < 0)\r\n                size = -size;\r\n            size = size % maxSize;\r\n            writer.append(name, new LongWritable(size));\r\n            totalSize += size;\r\n        }\r\n    } finally {\r\n        writer.close();\r\n    }\r\n    LOG.info(\"created control file for: \" + totalSize + \" bytes\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\fs",
  "methodName" : "writeTest",
  "errType" : null,
  "containingMethodsNum" : 12,
  "sourceCodeText" : "void writeTest(FileSystem fs, boolean fastCheck) throws Exception\n{\r\n    fs.delete(DATA_DIR, true);\r\n    fs.delete(WRITE_DIR, true);\r\n    JobConf job = new JobConf(conf, TestFileSystem.class);\r\n    job.setBoolean(\"fs.test.fastCheck\", fastCheck);\r\n    FileInputFormat.setInputPaths(job, CONTROL_DIR);\r\n    job.setInputFormat(SequenceFileInputFormat.class);\r\n    job.setMapperClass(WriteMapper.class);\r\n    job.setReducerClass(LongSumReducer.class);\r\n    FileOutputFormat.setOutputPath(job, WRITE_DIR);\r\n    job.setOutputKeyClass(Text.class);\r\n    job.setOutputValueClass(LongWritable.class);\r\n    job.setNumReduceTasks(1);\r\n    JobClient.runJob(job);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\fs",
  "methodName" : "readTest",
  "errType" : null,
  "containingMethodsNum" : 11,
  "sourceCodeText" : "void readTest(FileSystem fs, boolean fastCheck) throws Exception\n{\r\n    fs.delete(READ_DIR, true);\r\n    JobConf job = new JobConf(conf, TestFileSystem.class);\r\n    job.setBoolean(\"fs.test.fastCheck\", fastCheck);\r\n    FileInputFormat.setInputPaths(job, CONTROL_DIR);\r\n    job.setInputFormat(SequenceFileInputFormat.class);\r\n    job.setMapperClass(ReadMapper.class);\r\n    job.setReducerClass(LongSumReducer.class);\r\n    FileOutputFormat.setOutputPath(job, READ_DIR);\r\n    job.setOutputKeyClass(Text.class);\r\n    job.setOutputValueClass(LongWritable.class);\r\n    job.setNumReduceTasks(1);\r\n    JobClient.runJob(job);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\fs",
  "methodName" : "seekTest",
  "errType" : null,
  "containingMethodsNum" : 11,
  "sourceCodeText" : "void seekTest(FileSystem fs, boolean fastCheck) throws Exception\n{\r\n    fs.delete(READ_DIR, true);\r\n    JobConf job = new JobConf(conf, TestFileSystem.class);\r\n    job.setBoolean(\"fs.test.fastCheck\", fastCheck);\r\n    FileInputFormat.setInputPaths(job, CONTROL_DIR);\r\n    job.setInputFormat(SequenceFileInputFormat.class);\r\n    job.setMapperClass(SeekMapper.class);\r\n    job.setReducerClass(LongSumReducer.class);\r\n    FileOutputFormat.setOutputPath(job, READ_DIR);\r\n    job.setOutputKeyClass(Text.class);\r\n    job.setOutputValueClass(LongWritable.class);\r\n    job.setNumReduceTasks(1);\r\n    JobClient.runJob(job);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\fs",
  "methodName" : "main",
  "errType" : null,
  "containingMethodsNum" : 19,
  "sourceCodeText" : "void main(String[] args) throws Exception\n{\r\n    int megaBytes = 10;\r\n    int files = 100;\r\n    boolean noRead = false;\r\n    boolean noWrite = false;\r\n    boolean noSeek = false;\r\n    boolean fastCheck = false;\r\n    long seed = new Random().nextLong();\r\n    String usage = \"Usage: TestFileSystem -files N -megaBytes M [-noread] [-nowrite] [-noseek] [-fastcheck]\";\r\n    if (args.length == 0) {\r\n        System.err.println(usage);\r\n        System.exit(-1);\r\n    }\r\n    for (int i = 0; i < args.length; i++) {\r\n        if (args[i].equals(\"-files\")) {\r\n            files = Integer.parseInt(args[++i]);\r\n        } else if (args[i].equals(\"-megaBytes\")) {\r\n            megaBytes = Integer.parseInt(args[++i]);\r\n        } else if (args[i].equals(\"-noread\")) {\r\n            noRead = true;\r\n        } else if (args[i].equals(\"-nowrite\")) {\r\n            noWrite = true;\r\n        } else if (args[i].equals(\"-noseek\")) {\r\n            noSeek = true;\r\n        } else if (args[i].equals(\"-fastcheck\")) {\r\n            fastCheck = true;\r\n        }\r\n    }\r\n    LOG.info(\"seed = \" + seed);\r\n    LOG.info(\"files = \" + files);\r\n    LOG.info(\"megaBytes = \" + megaBytes);\r\n    FileSystem fs = FileSystem.get(conf);\r\n    if (!noWrite) {\r\n        createControlFile(fs, megaBytes * MEGA, files, seed);\r\n        writeTest(fs, fastCheck);\r\n    }\r\n    if (!noRead) {\r\n        readTest(fs, fastCheck);\r\n    }\r\n    if (!noSeek) {\r\n        seekTest(fs, fastCheck);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\fs",
  "methodName" : "testFsCache",
  "errType" : [ "java.net.BindException" ],
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void testFsCache() throws Exception\n{\r\n    {\r\n        long now = System.currentTimeMillis();\r\n        String[] users = new String[] { \"foo\", \"bar\" };\r\n        final Configuration conf = new Configuration();\r\n        FileSystem[] fs = new FileSystem[users.length];\r\n        for (int i = 0; i < users.length; i++) {\r\n            UserGroupInformation ugi = UserGroupInformation.createRemoteUser(users[i]);\r\n            fs[i] = ugi.doAs(new PrivilegedExceptionAction<FileSystem>() {\r\n\r\n                public FileSystem run() throws IOException {\r\n                    return FileSystem.get(conf);\r\n                }\r\n            });\r\n            for (int j = 0; j < i; j++) {\r\n                assertFalse(fs[j] == fs[i]);\r\n            }\r\n        }\r\n        FileSystem.closeAll();\r\n    }\r\n    {\r\n        try {\r\n            runTestCache(HdfsClientConfigKeys.DFS_NAMENODE_RPC_PORT_DEFAULT);\r\n        } catch (java.net.BindException be) {\r\n            LOG.warn(\"Cannot test HdfsClientConfigKeys.DFS_NAMENODE_RPC_PORT_DEFAULT (=\" + HdfsClientConfigKeys.DFS_NAMENODE_RPC_PORT_DEFAULT + \")\", be);\r\n        }\r\n        runTestCache(0);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\fs",
  "methodName" : "runTestCache",
  "errType" : null,
  "containingMethodsNum" : 17,
  "sourceCodeText" : "void runTestCache(int port) throws Exception\n{\r\n    Configuration conf = new Configuration();\r\n    MiniDFSCluster cluster = null;\r\n    try {\r\n        cluster = new MiniDFSCluster.Builder(conf).nameNodePort(port).numDataNodes(2).build();\r\n        URI uri = cluster.getFileSystem().getUri();\r\n        LOG.info(\"uri=\" + uri);\r\n        {\r\n            FileSystem fs = FileSystem.get(uri, new Configuration());\r\n            checkPath(cluster, fs);\r\n            for (int i = 0; i < 100; i++) {\r\n                assertTrue(fs == FileSystem.get(uri, new Configuration()));\r\n            }\r\n        }\r\n        if (port == HdfsClientConfigKeys.DFS_NAMENODE_RPC_PORT_DEFAULT) {\r\n            URI uri2 = new URI(uri.getScheme(), uri.getUserInfo(), uri.getHost(), HdfsClientConfigKeys.DFS_NAMENODE_RPC_PORT_DEFAULT, uri.getPath(), uri.getQuery(), uri.getFragment());\r\n            LOG.info(\"uri2=\" + uri2);\r\n            FileSystem fs = FileSystem.get(uri2, conf);\r\n            checkPath(cluster, fs);\r\n            for (int i = 0; i < 100; i++) {\r\n                assertTrue(fs == FileSystem.get(uri2, new Configuration()));\r\n            }\r\n        }\r\n    } finally {\r\n        if (cluster != null)\r\n            cluster.shutdown();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\fs",
  "methodName" : "checkPath",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void checkPath(MiniDFSCluster cluster, FileSystem fileSys) throws IOException\n{\r\n    InetSocketAddress add = cluster.getNameNode().getNameNodeAddress();\r\n    fileSys.checkPath(new Path(\"hdfs://\" + StringUtils.toUpperCase(add.getHostName()) + \":\" + add.getPort()));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\fs",
  "methodName" : "testFsClose",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void testFsClose() throws Exception\n{\r\n    {\r\n        Configuration conf = new Configuration();\r\n        new Path(\"file:///\").getFileSystem(conf);\r\n        FileSystem.closeAll();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\fs",
  "methodName" : "testFsShutdownHook",
  "errType" : null,
  "containingMethodsNum" : 16,
  "sourceCodeText" : "void testFsShutdownHook() throws Exception\n{\r\n    final Set<FileSystem> closed = Collections.synchronizedSet(new HashSet<FileSystem>());\r\n    Configuration conf = new Configuration();\r\n    Configuration confNoAuto = new Configuration();\r\n    conf.setClass(\"fs.test.impl\", TestShutdownFileSystem.class, FileSystem.class);\r\n    confNoAuto.setClass(\"fs.test.impl\", TestShutdownFileSystem.class, FileSystem.class);\r\n    confNoAuto.setBoolean(\"fs.automatic.close\", false);\r\n    TestShutdownFileSystem fsWithAuto = (TestShutdownFileSystem) (new Path(\"test://a/\").getFileSystem(conf));\r\n    TestShutdownFileSystem fsWithoutAuto = (TestShutdownFileSystem) (new Path(\"test://b/\").getFileSystem(confNoAuto));\r\n    fsWithAuto.setClosedSet(closed);\r\n    fsWithoutAuto.setClosedSet(closed);\r\n    assertNotSame(fsWithAuto, fsWithoutAuto);\r\n    FileSystem.CACHE.closeAll(true);\r\n    assertEquals(1, closed.size());\r\n    assertTrue(closed.contains(fsWithAuto));\r\n    closed.clear();\r\n    FileSystem.closeAll();\r\n    assertEquals(1, closed.size());\r\n    assertTrue(closed.contains(fsWithoutAuto));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\fs",
  "methodName" : "testCacheKeysAreCaseInsensitive",
  "errType" : null,
  "containingMethodsNum" : 11,
  "sourceCodeText" : "void testCacheKeysAreCaseInsensitive() throws Exception\n{\r\n    Configuration conf = new Configuration();\r\n    FileSystem.Cache.Key lowercaseCachekey1 = new FileSystem.Cache.Key(new URI(\"hdfs://localhost:12345/\"), conf);\r\n    FileSystem.Cache.Key lowercaseCachekey2 = new FileSystem.Cache.Key(new URI(\"hdfs://localhost:12345/\"), conf);\r\n    assertEquals(lowercaseCachekey1, lowercaseCachekey2);\r\n    FileSystem.Cache.Key uppercaseCachekey = new FileSystem.Cache.Key(new URI(\"HDFS://Localhost:12345/\"), conf);\r\n    assertEquals(lowercaseCachekey2, uppercaseCachekey);\r\n    List<FileSystem.Cache.Key> list = new ArrayList<FileSystem.Cache.Key>();\r\n    list.add(uppercaseCachekey);\r\n    assertTrue(list.contains(uppercaseCachekey));\r\n    assertTrue(list.contains(lowercaseCachekey2));\r\n    Set<FileSystem.Cache.Key> set = new HashSet<FileSystem.Cache.Key>();\r\n    set.add(uppercaseCachekey);\r\n    assertTrue(set.contains(uppercaseCachekey));\r\n    assertTrue(set.contains(lowercaseCachekey2));\r\n    Map<FileSystem.Cache.Key, String> map = new HashMap<FileSystem.Cache.Key, String>();\r\n    map.put(uppercaseCachekey, \"\");\r\n    assertTrue(map.containsKey(uppercaseCachekey));\r\n    assertTrue(map.containsKey(lowercaseCachekey2));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\fs",
  "methodName" : "testFsUniqueness",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void testFsUniqueness(long megaBytes, int numFiles, long seed) throws Exception\n{\r\n    FileSystem fs1 = FileSystem.get(conf);\r\n    FileSystem fs2 = FileSystem.get(conf);\r\n    assertTrue(fs1 == fs2);\r\n    fs1 = FileSystem.newInstance(conf);\r\n    fs2 = FileSystem.newInstance(conf);\r\n    assertTrue(fs1 != fs2 && !fs1.equals(fs2));\r\n    fs1.close();\r\n    fs2.close();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "generateInputData",
  "errType" : null,
  "containingMethodsNum" : 23,
  "sourceCodeText" : "void generateInputData(int dataSizePerMap, int numSpillsPerMap, int numMapsPerHost, JobConf masterConf) throws Exception\n{\r\n    JobConf job = new JobConf(masterConf, ThreadedMapBenchmark.class);\r\n    job.setJobName(\"threaded-map-benchmark-random-writer\");\r\n    job.setJarByClass(ThreadedMapBenchmark.class);\r\n    job.setInputFormat(UtilsForTests.RandomInputFormat.class);\r\n    job.setOutputFormat(SequenceFileOutputFormat.class);\r\n    job.setMapperClass(Map.class);\r\n    job.setReducerClass(IdentityReducer.class);\r\n    job.setOutputKeyClass(BytesWritable.class);\r\n    job.setOutputValueClass(BytesWritable.class);\r\n    JobClient client = new JobClient(job);\r\n    ClusterStatus cluster = client.getClusterStatus();\r\n    long totalDataSize = dataSizePerMap * numMapsPerHost * cluster.getTaskTrackers();\r\n    job.set(\"test.tmb.bytes_per_map\", String.valueOf(dataSizePerMap * 1024 * 1024));\r\n    job.setNumReduceTasks(0);\r\n    job.setNumMapTasks(numMapsPerHost * cluster.getTaskTrackers());\r\n    FileOutputFormat.setOutputPath(job, INPUT_DIR);\r\n    FileSystem fs = FileSystem.get(job);\r\n    fs.delete(BASE_DIR, true);\r\n    LOG.info(\"Generating random input for the benchmark\");\r\n    LOG.info(\"Total data : \" + totalDataSize + \" mb\");\r\n    LOG.info(\"Data per map: \" + dataSizePerMap + \" mb\");\r\n    LOG.info(\"Number of spills : \" + numSpillsPerMap);\r\n    LOG.info(\"Number of maps per host : \" + numMapsPerHost);\r\n    LOG.info(\"Number of hosts : \" + cluster.getTaskTrackers());\r\n    JobClient.runJob(job);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "run",
  "errType" : null,
  "containingMethodsNum" : 46,
  "sourceCodeText" : "int run(String[] args) throws Exception\n{\r\n    LOG.info(\"Starting the benchmark for threaded spills\");\r\n    String version = \"ThreadedMapBenchmark.0.0.1\";\r\n    System.out.println(version);\r\n    String usage = \"Usage: threadedmapbenchmark \" + \"[-dataSizePerMap <data size (in mb) per map, default is 128 mb>] \" + \"[-numSpillsPerMap <number of spills per map, default is 2>] \" + \"[-numMapsPerHost <number of maps per host, default is 1>]\";\r\n    int dataSizePerMap = 128;\r\n    int numSpillsPerMap = 2;\r\n    int numMapsPerHost = 1;\r\n    JobConf masterConf = new JobConf(getConf());\r\n    for (int i = 0; i < args.length; i++) {\r\n        if (args[i].equals(\"-dataSizePerMap\")) {\r\n            dataSizePerMap = Integer.parseInt(args[++i]);\r\n        } else if (args[i].equals(\"-numSpillsPerMap\")) {\r\n            numSpillsPerMap = Integer.parseInt(args[++i]);\r\n        } else if (args[i].equals(\"-numMapsPerHost\")) {\r\n            numMapsPerHost = Integer.parseInt(args[++i]);\r\n        } else {\r\n            System.err.println(usage);\r\n            System.exit(-1);\r\n        }\r\n    }\r\n    if (dataSizePerMap < 1 || numSpillsPerMap < 1 || numMapsPerHost < 1) {\r\n        System.err.println(usage);\r\n        System.exit(-1);\r\n    }\r\n    FileSystem fs = null;\r\n    try {\r\n        generateInputData(dataSizePerMap, numSpillsPerMap, numMapsPerHost, masterConf);\r\n        JobConf job = new JobConf(masterConf, ThreadedMapBenchmark.class);\r\n        job.setJobName(\"threaded-map-benchmark-unspilled\");\r\n        job.setJarByClass(ThreadedMapBenchmark.class);\r\n        job.setInputFormat(NonSplitableSequenceFileInputFormat.class);\r\n        job.setOutputFormat(SequenceFileOutputFormat.class);\r\n        job.setOutputKeyClass(BytesWritable.class);\r\n        job.setOutputValueClass(BytesWritable.class);\r\n        job.setMapperClass(IdentityMapper.class);\r\n        job.setReducerClass(IdentityReducer.class);\r\n        FileInputFormat.addInputPath(job, INPUT_DIR);\r\n        FileOutputFormat.setOutputPath(job, OUTPUT_DIR);\r\n        JobClient client = new JobClient(job);\r\n        ClusterStatus cluster = client.getClusterStatus();\r\n        job.setNumMapTasks(numMapsPerHost * cluster.getTaskTrackers());\r\n        job.setNumReduceTasks(1);\r\n        int ioSortMb = (int) Math.ceil(FACTOR * dataSizePerMap);\r\n        job.set(JobContext.IO_SORT_MB, String.valueOf(ioSortMb));\r\n        fs = FileSystem.get(job);\r\n        LOG.info(\"Running sort with 1 spill per map\");\r\n        long startTime = System.currentTimeMillis();\r\n        JobClient.runJob(job);\r\n        long endTime = System.currentTimeMillis();\r\n        LOG.info(\"Total time taken : \" + String.valueOf(endTime - startTime) + \" millisec\");\r\n        fs.delete(OUTPUT_DIR, true);\r\n        JobConf spilledJob = new JobConf(job, ThreadedMapBenchmark.class);\r\n        ioSortMb = (int) Math.ceil(FACTOR * Math.ceil((double) dataSizePerMap / numSpillsPerMap));\r\n        spilledJob.set(JobContext.IO_SORT_MB, String.valueOf(ioSortMb));\r\n        spilledJob.setJobName(\"threaded-map-benchmark-spilled\");\r\n        spilledJob.setJarByClass(ThreadedMapBenchmark.class);\r\n        LOG.info(\"Running sort with \" + numSpillsPerMap + \" spills per map\");\r\n        startTime = System.currentTimeMillis();\r\n        JobClient.runJob(spilledJob);\r\n        endTime = System.currentTimeMillis();\r\n        LOG.info(\"Total time taken : \" + String.valueOf(endTime - startTime) + \" millisec\");\r\n    } finally {\r\n        if (fs != null) {\r\n            fs.delete(BASE_DIR, true);\r\n        }\r\n    }\r\n    return 0;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "main",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void main(String[] args) throws Exception\n{\r\n    int res = ToolRunner.run(new ThreadedMapBenchmark(), args);\r\n    System.exit(res);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\join",
  "methodName" : "setUp",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void setUp() throws Exception\n{\r\n    Configuration conf = new Configuration();\r\n    cluster = new MiniDFSCluster.Builder(conf).numDataNodes(2).build();\r\n    base = cluster.getFileSystem().makeQualified(new Path(\"/nested\"));\r\n    src = generateSources(conf);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\join",
  "methodName" : "tearDown",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void tearDown() throws Exception\n{\r\n    if (cluster != null) {\r\n        cluster.shutdown();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\join",
  "methodName" : "createWriters",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "SequenceFile.Writer[] createWriters(Path testdir, Configuration conf, int srcs, Path[] src) throws IOException\n{\r\n    for (int i = 0; i < srcs; ++i) {\r\n        src[i] = new Path(testdir, Integer.toString(i + 10, 36));\r\n    }\r\n    SequenceFile.Writer[] out = new SequenceFile.Writer[srcs];\r\n    for (int i = 0; i < srcs - 1; ++i) {\r\n        out[i] = new SequenceFile.Writer(testdir.getFileSystem(conf), conf, src[i], IntWritable.class, IntWritable.class);\r\n    }\r\n    out[srcs - 1] = new SequenceFile.Writer(testdir.getFileSystem(conf), conf, src[srcs - 1], IntWritable.class, LongWritable.class);\r\n    return out;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\join",
  "methodName" : "stringify",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "String stringify(IntWritable key, Writable val)\n{\r\n    StringBuilder sb = new StringBuilder();\r\n    sb.append(\"(\" + key);\r\n    sb.append(\",\" + val + \")\");\r\n    return sb.toString();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\join",
  "methodName" : "generateSources",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "Path[] generateSources(Configuration conf) throws IOException\n{\r\n    for (int i = 0; i < SOURCES; ++i) {\r\n        source[i] = new int[ITEMS];\r\n        for (int j = 0; j < ITEMS; ++j) {\r\n            source[i][j] = (i + 2) * (j + 1);\r\n        }\r\n    }\r\n    Path[] src = new Path[SOURCES];\r\n    SequenceFile.Writer[] out = createWriters(base, conf, SOURCES, src);\r\n    IntWritable k = new IntWritable();\r\n    for (int i = 0; i < SOURCES; ++i) {\r\n        Writable v;\r\n        if (i != SOURCES - 1) {\r\n            v = new IntWritable();\r\n            ((IntWritable) v).set(i);\r\n        } else {\r\n            v = new LongWritable();\r\n            ((LongWritable) v).set(i);\r\n        }\r\n        for (int j = 0; j < ITEMS; ++j) {\r\n            k.set(source[i][j]);\r\n            out[i].append(k, v);\r\n        }\r\n        out[i].close();\r\n    }\r\n    return src;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\join",
  "methodName" : "A",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String A()\n{\r\n    return CompositeInputFormat.compose(SequenceFileInputFormat.class, src[0].toString());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\join",
  "methodName" : "B",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String B()\n{\r\n    return CompositeInputFormat.compose(SequenceFileInputFormat.class, src[1].toString());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\join",
  "methodName" : "C",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String C()\n{\r\n    return CompositeInputFormat.compose(SequenceFileInputFormat.class, src[2].toString());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\join",
  "methodName" : "constructExpr1",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "String constructExpr1(String op)\n{\r\n    StringBuilder sb = new StringBuilder();\r\n    sb.append(op + \"(\" + op + \"(\");\r\n    sb.append(A());\r\n    sb.append(\",\");\r\n    sb.append(B());\r\n    sb.append(\"),\");\r\n    sb.append(C());\r\n    sb.append(\")\");\r\n    return sb.toString();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\join",
  "methodName" : "constructExpr2",
  "errType" : null,
  "containingMethodsNum" : 9,
  "sourceCodeText" : "String constructExpr2(String op)\n{\r\n    StringBuilder sb = new StringBuilder();\r\n    sb.append(op + \"(\");\r\n    sb.append(A());\r\n    sb.append(\",\");\r\n    sb.append(op + \"(\");\r\n    sb.append(B());\r\n    sb.append(\",\");\r\n    sb.append(C());\r\n    sb.append(\"))\");\r\n    return sb.toString();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\join",
  "methodName" : "constructExpr3",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "String constructExpr3(String op)\n{\r\n    StringBuilder sb = new StringBuilder();\r\n    sb.append(op + \"(\");\r\n    sb.append(A());\r\n    sb.append(\",\");\r\n    sb.append(B());\r\n    sb.append(\",\");\r\n    sb.append(C());\r\n    sb.append(\")\");\r\n    return sb.toString();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\join",
  "methodName" : "constructExpr4",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "String constructExpr4()\n{\r\n    StringBuilder sb = new StringBuilder();\r\n    sb.append(\"override(inner(\");\r\n    sb.append(A());\r\n    sb.append(\",\");\r\n    sb.append(B());\r\n    sb.append(\"),\");\r\n    sb.append(A());\r\n    sb.append(\")\");\r\n    return sb.toString();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\join",
  "methodName" : "validateKeyValue",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void validateKeyValue(WritableComparable<?> k, Writable v, int tupleSize, boolean firstTuple, boolean secondTuple, TestType ttype) throws IOException\n{\r\n    System.out.println(\"out k:\" + k + \" v:\" + v);\r\n    if (ttype.equals(TestType.OUTER_ASSOCIATIVITY)) {\r\n        validateOuterKeyValue((IntWritable) k, (TupleWritable) v, tupleSize, firstTuple, secondTuple);\r\n    } else if (ttype.equals(TestType.INNER_ASSOCIATIVITY)) {\r\n        validateInnerKeyValue((IntWritable) k, (TupleWritable) v, tupleSize, firstTuple, secondTuple);\r\n    }\r\n    if (ttype.equals(TestType.INNER_IDENTITY)) {\r\n        validateKeyValue_INNER_IDENTITY((IntWritable) k, (IntWritable) v);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\join",
  "methodName" : "testExpr1",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void testExpr1(Configuration conf, String op, TestType ttype, int expectedCount) throws Exception\n{\r\n    String joinExpr = constructExpr1(op);\r\n    conf.set(CompositeInputFormat.JOIN_EXPR, joinExpr);\r\n    int count = testFormat(conf, 2, true, false, ttype);\r\n    assertTrue(\"not all keys present\", count == expectedCount);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\join",
  "methodName" : "testExpr2",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void testExpr2(Configuration conf, String op, TestType ttype, int expectedCount) throws Exception\n{\r\n    String joinExpr = constructExpr2(op);\r\n    conf.set(CompositeInputFormat.JOIN_EXPR, joinExpr);\r\n    int count = testFormat(conf, 2, false, true, ttype);\r\n    assertTrue(\"not all keys present\", count == expectedCount);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\join",
  "methodName" : "testExpr3",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void testExpr3(Configuration conf, String op, TestType ttype, int expectedCount) throws Exception\n{\r\n    String joinExpr = constructExpr3(op);\r\n    conf.set(CompositeInputFormat.JOIN_EXPR, joinExpr);\r\n    int count = testFormat(conf, 3, false, false, ttype);\r\n    assertTrue(\"not all keys present\", count == expectedCount);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\join",
  "methodName" : "testExpr4",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void testExpr4(Configuration conf) throws Exception\n{\r\n    String joinExpr = constructExpr4();\r\n    conf.set(CompositeInputFormat.JOIN_EXPR, joinExpr);\r\n    int count = testFormat(conf, 0, false, false, TestType.INNER_IDENTITY);\r\n    assertTrue(\"not all keys present\", count == ITEMS);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\join",
  "methodName" : "testOuterAssociativity",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void testOuterAssociativity() throws Exception\n{\r\n    Configuration conf = new Configuration();\r\n    testExpr1(conf, \"outer\", TestType.OUTER_ASSOCIATIVITY, 33);\r\n    testExpr2(conf, \"outer\", TestType.OUTER_ASSOCIATIVITY, 33);\r\n    testExpr3(conf, \"outer\", TestType.OUTER_ASSOCIATIVITY, 33);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\join",
  "methodName" : "testInnerAssociativity",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void testInnerAssociativity() throws Exception\n{\r\n    Configuration conf = new Configuration();\r\n    testExpr1(conf, \"inner\", TestType.INNER_ASSOCIATIVITY, 2);\r\n    testExpr2(conf, \"inner\", TestType.INNER_ASSOCIATIVITY, 2);\r\n    testExpr3(conf, \"inner\", TestType.INNER_ASSOCIATIVITY, 2);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\join",
  "methodName" : "testIdentity",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testIdentity() throws Exception\n{\r\n    Configuration conf = new Configuration();\r\n    testExpr4(conf);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\join",
  "methodName" : "validateOuterKeyValue",
  "errType" : null,
  "containingMethodsNum" : 26,
  "sourceCodeText" : "void validateOuterKeyValue(IntWritable k, TupleWritable v, int tupleSize, boolean firstTuple, boolean secondTuple)\n{\r\n    final String kvstr = \"Unexpected tuple: \" + stringify(k, v);\r\n    assertTrue(kvstr, v.size() == tupleSize);\r\n    int key = k.get();\r\n    IntWritable val0 = null;\r\n    IntWritable val1 = null;\r\n    LongWritable val2 = null;\r\n    if (firstTuple) {\r\n        TupleWritable v0 = ((TupleWritable) v.get(0));\r\n        if (key % 2 == 0 && key / 2 <= ITEMS) {\r\n            val0 = (IntWritable) v0.get(0);\r\n        } else {\r\n            assertFalse(kvstr, v0.has(0));\r\n        }\r\n        if (key % 3 == 0 && key / 3 <= ITEMS) {\r\n            val1 = (IntWritable) v0.get(1);\r\n        } else {\r\n            assertFalse(kvstr, v0.has(1));\r\n        }\r\n        if (key % 4 == 0 && key / 4 <= ITEMS) {\r\n            val2 = (LongWritable) v.get(1);\r\n        } else {\r\n            assertFalse(kvstr, v.has(2));\r\n        }\r\n    } else if (secondTuple) {\r\n        if (key % 2 == 0 && key / 2 <= ITEMS) {\r\n            val0 = (IntWritable) v.get(0);\r\n        } else {\r\n            assertFalse(kvstr, v.has(0));\r\n        }\r\n        TupleWritable v1 = ((TupleWritable) v.get(1));\r\n        if (key % 3 == 0 && key / 3 <= ITEMS) {\r\n            val1 = (IntWritable) v1.get(0);\r\n        } else {\r\n            assertFalse(kvstr, v1.has(0));\r\n        }\r\n        if (key % 4 == 0 && key / 4 <= ITEMS) {\r\n            val2 = (LongWritable) v1.get(1);\r\n        } else {\r\n            assertFalse(kvstr, v1.has(1));\r\n        }\r\n    } else {\r\n        if (key % 2 == 0 && key / 2 <= ITEMS) {\r\n            val0 = (IntWritable) v.get(0);\r\n        } else {\r\n            assertFalse(kvstr, v.has(0));\r\n        }\r\n        if (key % 3 == 0 && key / 3 <= ITEMS) {\r\n            val1 = (IntWritable) v.get(1);\r\n        } else {\r\n            assertFalse(kvstr, v.has(1));\r\n        }\r\n        if (key % 4 == 0 && key / 4 <= ITEMS) {\r\n            val2 = (LongWritable) v.get(2);\r\n        } else {\r\n            assertFalse(kvstr, v.has(2));\r\n        }\r\n    }\r\n    if (val0 != null) {\r\n        assertTrue(kvstr, val0.get() == 0);\r\n    }\r\n    if (val1 != null) {\r\n        assertTrue(kvstr, val1.get() == 1);\r\n    }\r\n    if (val2 != null) {\r\n        assertTrue(kvstr, val2.get() == 2);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\join",
  "methodName" : "validateInnerKeyValue",
  "errType" : null,
  "containingMethodsNum" : 20,
  "sourceCodeText" : "void validateInnerKeyValue(IntWritable k, TupleWritable v, int tupleSize, boolean firstTuple, boolean secondTuple)\n{\r\n    final String kvstr = \"Unexpected tuple: \" + stringify(k, v);\r\n    assertTrue(kvstr, v.size() == tupleSize);\r\n    int key = k.get();\r\n    IntWritable val0 = null;\r\n    IntWritable val1 = null;\r\n    LongWritable val2 = null;\r\n    assertTrue(kvstr, key % 2 == 0 && key / 2 <= ITEMS);\r\n    assertTrue(kvstr, key % 3 == 0 && key / 3 <= ITEMS);\r\n    assertTrue(kvstr, key % 4 == 0 && key / 4 <= ITEMS);\r\n    if (firstTuple) {\r\n        TupleWritable v0 = ((TupleWritable) v.get(0));\r\n        val0 = (IntWritable) v0.get(0);\r\n        val1 = (IntWritable) v0.get(1);\r\n        val2 = (LongWritable) v.get(1);\r\n    } else if (secondTuple) {\r\n        val0 = (IntWritable) v.get(0);\r\n        TupleWritable v1 = ((TupleWritable) v.get(1));\r\n        val1 = (IntWritable) v1.get(0);\r\n        val2 = (LongWritable) v1.get(1);\r\n    } else {\r\n        val0 = (IntWritable) v.get(0);\r\n        val1 = (IntWritable) v.get(1);\r\n        val2 = (LongWritable) v.get(2);\r\n    }\r\n    assertTrue(kvstr, val0.get() == 0);\r\n    assertTrue(kvstr, val1.get() == 1);\r\n    assertTrue(kvstr, val2.get() == 2);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\join",
  "methodName" : "validateKeyValue_INNER_IDENTITY",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void validateKeyValue_INNER_IDENTITY(IntWritable k, IntWritable v)\n{\r\n    final String kvstr = \"Unexpected tuple: \" + stringify(k, v);\r\n    int key = k.get();\r\n    assertTrue(kvstr, (key % 2 == 0 && key / 2 <= ITEMS));\r\n    assertTrue(kvstr, v.get() == 0);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\join",
  "methodName" : "testFormat",
  "errType" : null,
  "containingMethodsNum" : 11,
  "sourceCodeText" : "int testFormat(Configuration conf, int tupleSize, boolean firstTuple, boolean secondTuple, TestType ttype) throws Exception\n{\r\n    Job job = Job.getInstance(conf);\r\n    CompositeInputFormat format = new CompositeInputFormat();\r\n    int count = 0;\r\n    for (InputSplit split : (List<InputSplit>) format.getSplits(job)) {\r\n        TaskAttemptContext context = MapReduceTestUtil.createDummyMapTaskAttemptContext(conf);\r\n        RecordReader reader = format.createRecordReader(split, context);\r\n        MapContext mcontext = new MapContextImpl(conf, context.getTaskAttemptID(), reader, null, null, MapReduceTestUtil.createDummyReporter(), split);\r\n        reader.initialize(split, mcontext);\r\n        WritableComparable key = null;\r\n        Writable value = null;\r\n        while (reader.nextKeyValue()) {\r\n            key = (WritableComparable) reader.getCurrentKey();\r\n            value = (Writable) reader.getCurrentValue();\r\n            validateKeyValue(key, value, tupleSize, firstTuple, secondTuple, ttype);\r\n            count++;\r\n        }\r\n    }\r\n    return count;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred\\lib",
  "methodName" : "testFormat",
  "errType" : null,
  "containingMethodsNum" : 10,
  "sourceCodeText" : "void testFormat() throws Exception\n{\r\n    JobConf job = new JobConf();\r\n    Path file = new Path(workDir, \"test.txt\");\r\n    int seed = new Random().nextInt();\r\n    Random random = new Random(seed);\r\n    localFs.delete(workDir, true);\r\n    FileInputFormat.setInputPaths(job, workDir);\r\n    int numLinesPerMap = 5;\r\n    job.setInt(\"mapreduce.input.lineinputformat.linespermap\", numLinesPerMap);\r\n    for (int length = 0; length < MAX_LENGTH; length += random.nextInt(MAX_LENGTH / 10) + 1) {\r\n        Writer writer = new OutputStreamWriter(localFs.create(file));\r\n        try {\r\n            for (int i = 0; i < length; i++) {\r\n                writer.write(Integer.toString(i));\r\n                writer.write(\"\\n\");\r\n            }\r\n        } finally {\r\n            writer.close();\r\n        }\r\n        checkFormat(job, numLinesPerMap);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred\\lib",
  "methodName" : "checkFormat",
  "errType" : null,
  "containingMethodsNum" : 15,
  "sourceCodeText" : "void checkFormat(JobConf job, int expectedN) throws IOException\n{\r\n    NLineInputFormat format = new NLineInputFormat();\r\n    format.configure(job);\r\n    int ignoredNumSplits = 1;\r\n    InputSplit[] splits = format.getSplits(job, ignoredNumSplits);\r\n    int count = 0;\r\n    for (int j = 0; j < splits.length - 1; j++) {\r\n        assertEquals(\"There are no split locations\", 0, splits[j].getLocations().length);\r\n        RecordReader<LongWritable, Text> reader = format.getRecordReader(splits[j], job, voidReporter);\r\n        Class readerClass = reader.getClass();\r\n        assertEquals(\"reader class is LineRecordReader.\", LineRecordReader.class, readerClass);\r\n        LongWritable key = reader.createKey();\r\n        Class keyClass = key.getClass();\r\n        assertEquals(\"Key class is LongWritable.\", LongWritable.class, keyClass);\r\n        Text value = reader.createValue();\r\n        Class valueClass = value.getClass();\r\n        assertEquals(\"Value class is Text.\", Text.class, valueClass);\r\n        try {\r\n            count = 0;\r\n            while (reader.next(key, value)) {\r\n                count++;\r\n            }\r\n        } finally {\r\n            reader.close();\r\n        }\r\n        assertEquals(\"number of lines in split is \" + expectedN, expectedN, count);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred\\lib",
  "methodName" : "main",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void main(String[] args) throws Exception\n{\r\n    new TestLineInputFormat().testFormat();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "setUp",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void setUp() throws Exception\n{\r\n    Configuration conf = new Configuration();\r\n    dfsCluster = new MiniDFSCluster.Builder(conf).numDataNodes(2).build();\r\n    mrCluster = new MiniMRCluster(2, dfsCluster.getFileSystem().getUri().toString(), 1);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "tearDown",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void tearDown() throws Exception\n{\r\n    if (dfsCluster != null) {\r\n        dfsCluster.shutdown();\r\n    }\r\n    if (mrCluster != null) {\r\n        mrCluster.shutdown();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getValLen",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int getValLen(int id, int nMaps)\n{\r\n    return 4096 / nMaps * (id + 1);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "testReduceFromPartialMem",
  "errType" : null,
  "containingMethodsNum" : 14,
  "sourceCodeText" : "void testReduceFromPartialMem() throws Exception\n{\r\n    final int MAP_TASKS = 7;\r\n    JobConf job = mrCluster.createJobConf();\r\n    job.setNumMapTasks(MAP_TASKS);\r\n    job.setInt(JobContext.REDUCE_MERGE_INMEM_THRESHOLD, 0);\r\n    job.set(JobContext.REDUCE_INPUT_BUFFER_PERCENT, \"1.0\");\r\n    job.setInt(JobContext.SHUFFLE_PARALLEL_COPIES, 1);\r\n    job.setInt(JobContext.IO_SORT_MB, 10);\r\n    job.set(JobConf.MAPRED_REDUCE_TASK_JAVA_OPTS, \"-Xmx128m\");\r\n    job.setLong(JobContext.REDUCE_MEMORY_TOTAL_BYTES, 128 << 20);\r\n    job.set(JobContext.SHUFFLE_INPUT_BUFFER_PERCENT, \"0.14\");\r\n    job.set(JobContext.SHUFFLE_MERGE_PERCENT, \"1.0\");\r\n    Counters c = runJob(job);\r\n    final long out = c.findCounter(TaskCounter.MAP_OUTPUT_RECORDS).getCounter();\r\n    final long spill = c.findCounter(TaskCounter.SPILLED_RECORDS).getCounter();\r\n    assertTrue(\"Expected some records not spilled during reduce\" + spill + \")\", spill < 2 * out);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "runJob",
  "errType" : null,
  "containingMethodsNum" : 17,
  "sourceCodeText" : "Counters runJob(JobConf conf) throws Exception\n{\r\n    conf.setMapperClass(MapMB.class);\r\n    conf.setReducerClass(MBValidate.class);\r\n    conf.setOutputKeyClass(Text.class);\r\n    conf.setOutputValueClass(Text.class);\r\n    conf.setNumReduceTasks(1);\r\n    conf.setInputFormat(FakeIF.class);\r\n    conf.setNumTasksToExecutePerJvm(1);\r\n    conf.setInt(JobContext.MAP_MAX_ATTEMPTS, 0);\r\n    conf.setInt(JobContext.REDUCE_MAX_ATTEMPTS, 0);\r\n    FileInputFormat.setInputPaths(conf, new Path(\"/in\"));\r\n    final Path outp = new Path(\"/out\");\r\n    FileOutputFormat.setOutputPath(conf, outp);\r\n    RunningJob job = null;\r\n    try {\r\n        job = JobClient.runJob(conf);\r\n        assertTrue(job.isSuccessful());\r\n    } finally {\r\n        FileSystem fs = dfsCluster.getFileSystem();\r\n        if (fs.exists(outp)) {\r\n            fs.delete(outp, true);\r\n        }\r\n    }\r\n    return job.getCounters();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "testFormat",
  "errType" : null,
  "containingMethodsNum" : 22,
  "sourceCodeText" : "void testFormat() throws Exception\n{\r\n    JobConf job = new JobConf(conf);\r\n    FileSystem fs = FileSystem.getLocal(conf);\r\n    Path dir = new Path(System.getProperty(\"test.build.data\", \".\") + \"/mapred\");\r\n    Path file = new Path(dir, \"test.seq\");\r\n    Reporter reporter = Reporter.NULL;\r\n    int seed = new Random().nextInt();\r\n    Random random = new Random(seed);\r\n    fs.delete(dir, true);\r\n    FileInputFormat.setInputPaths(job, dir);\r\n    for (int length = 0; length < MAX_LENGTH; length += random.nextInt(MAX_LENGTH / 10) + 1) {\r\n        SequenceFile.Writer writer = SequenceFile.createWriter(fs, conf, file, IntWritable.class, LongWritable.class);\r\n        try {\r\n            for (int i = 0; i < length; i++) {\r\n                IntWritable key = new IntWritable(i);\r\n                LongWritable value = new LongWritable(10 * i);\r\n                writer.append(key, value);\r\n            }\r\n        } finally {\r\n            writer.close();\r\n        }\r\n        InputFormat<Text, Text> format = new SequenceFileAsTextInputFormat();\r\n        for (int i = 0; i < 3; i++) {\r\n            int numSplits = random.nextInt(MAX_LENGTH / (SequenceFile.SYNC_INTERVAL / 20)) + 1;\r\n            InputSplit[] splits = format.getSplits(job, numSplits);\r\n            BitSet bits = new BitSet(length);\r\n            for (int j = 0; j < splits.length; j++) {\r\n                RecordReader<Text, Text> reader = format.getRecordReader(splits[j], job, reporter);\r\n                Class readerClass = reader.getClass();\r\n                assertEquals(\"reader class is SequenceFileAsTextRecordReader.\", SequenceFileAsTextRecordReader.class, readerClass);\r\n                Text value = reader.createValue();\r\n                Text key = reader.createKey();\r\n                try {\r\n                    int count = 0;\r\n                    while (reader.next(key, value)) {\r\n                        int keyInt = Integer.parseInt(key.toString());\r\n                        assertFalse(\"Key in multiple partitions.\", bits.get(keyInt));\r\n                        bits.set(keyInt);\r\n                        count++;\r\n                    }\r\n                } finally {\r\n                    reader.close();\r\n                }\r\n            }\r\n            assertEquals(\"Some keys in no partition.\", length, bits.cardinality());\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 2,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "runTest",
  "errType" : null,
  "containingMethodsNum" : 9,
  "sourceCodeText" : "void runTest(String name, int keylen, int vallen, int records, int ioSortMB, float spillPer) throws Exception\n{\r\n    Configuration conf = new Configuration();\r\n    conf.setInt(Job.COMPLETION_POLL_INTERVAL_KEY, 100);\r\n    Job job = Job.getInstance(conf);\r\n    conf = job.getConfiguration();\r\n    conf.setInt(MRJobConfig.IO_SORT_MB, ioSortMB);\r\n    conf.set(MRJobConfig.MAP_SORT_SPILL_PERCENT, Float.toString(spillPer));\r\n    conf.setClass(\"test.mapcollection.class\", FixedRecordFactory.class, RecordFactory.class);\r\n    FixedRecordFactory.setLengths(conf, keylen, vallen);\r\n    conf.setInt(\"test.spillmap.records\", records);\r\n    runTest(name, job);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "runTest",
  "errType" : null,
  "containingMethodsNum" : 14,
  "sourceCodeText" : "void runTest(String name, Job job) throws Exception\n{\r\n    job.setNumReduceTasks(1);\r\n    job.getConfiguration().set(MRConfig.FRAMEWORK_NAME, MRConfig.LOCAL_FRAMEWORK_NAME);\r\n    job.getConfiguration().setInt(MRJobConfig.IO_SORT_FACTOR, 1000);\r\n    job.getConfiguration().set(\"fs.defaultFS\", \"file:///\");\r\n    job.getConfiguration().setInt(\"test.mapcollection.num.maps\", 1);\r\n    job.setInputFormatClass(FakeIF.class);\r\n    job.setOutputFormatClass(NullOutputFormat.class);\r\n    job.setMapperClass(Mapper.class);\r\n    job.setReducerClass(SpillReducer.class);\r\n    job.setMapOutputKeyClass(KeyWritable.class);\r\n    job.setMapOutputValueClass(ValWritable.class);\r\n    job.setSortComparatorClass(VariableComparator.class);\r\n    LOG.info(\"Running \" + name);\r\n    assertTrue(\"Job failed!\", job.waitForCompletion(false));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "testValLastByte",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void testValLastByte() throws Exception\n{\r\n    runTest(\"vallastbyte\", 128, 896, 1344, 1, 0.5f);\r\n    runTest(\"keylastbyte\", 512, 1024, 896, 1, 0.5f);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "testLargeRecords",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void testLargeRecords() throws Exception\n{\r\n    runTest(\"largerec\", 100, 1024 * 1024, 5, 1, .8f);\r\n    runTest(\"largekeyzeroval\", 1024 * 1024, 0, 5, 1, .8f);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "testSpillPer2B",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void testSpillPer2B() throws Exception\n{\r\n    runTest(\"fullspill2B\", 1, 1, 10000, 1, 1.0f);\r\n    runTest(\"fullspill200B\", 100, 100, 10000, 1, 1.0f);\r\n    runTest(\"fullspillbuf\", 10 * 1024, 20 * 1024, 256, 1, 1.0f);\r\n    runTest(\"lt50perspill\", 100, 100, 10000, 1, 0.3f);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "testZeroVal",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void testZeroVal() throws Exception\n{\r\n    runTest(\"zeroval\", 1, 0, 10000, 1, .8f);\r\n    runTest(\"zerokey\", 0, 1, 10000, 1, .8f);\r\n    runTest(\"zerokeyval\", 0, 0, 10000, 1, .8f);\r\n    runTest(\"zerokeyvalfull\", 0, 0, 10000, 1, 1.0f);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "testSingleRecord",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void testSingleRecord() throws Exception\n{\r\n    runTest(\"singlerecord\", 100, 100, 1, 1, 1.0f);\r\n    runTest(\"zerokeyvalsingle\", 0, 0, 1, 1, 1.0f);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "testLowSpill",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testLowSpill() throws Exception\n{\r\n    runTest(\"lowspill\", 4000, 96, 20, 1, 0.00390625f);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "testSplitMetaSpill",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testSplitMetaSpill() throws Exception\n{\r\n    runTest(\"splitmetaspill\", 7, 1, 131072, 1, 0.8f);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "testPostSpillMeta",
  "errType" : null,
  "containingMethodsNum" : 11,
  "sourceCodeText" : "void testPostSpillMeta() throws Exception\n{\r\n    Configuration conf = new Configuration();\r\n    conf.setInt(Job.COMPLETION_POLL_INTERVAL_KEY, 100);\r\n    Job job = Job.getInstance(conf);\r\n    conf = job.getConfiguration();\r\n    conf.setInt(MRJobConfig.IO_SORT_MB, 1);\r\n    conf.set(MRJobConfig.MAP_SORT_SPILL_PERCENT, Float.toString(.986328125f));\r\n    conf.setClass(\"test.mapcollection.class\", StepFactory.class, RecordFactory.class);\r\n    StepFactory.setLengths(conf, 4000, 0, 96, 0, 252);\r\n    conf.setInt(\"test.spillmap.records\", 1000);\r\n    conf.setBoolean(\"test.disable.key.read\", true);\r\n    conf.setBoolean(\"test.disable.val.read\", true);\r\n    runTest(\"postspillmeta\", job);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "testLargeRecConcurrent",
  "errType" : null,
  "containingMethodsNum" : 11,
  "sourceCodeText" : "void testLargeRecConcurrent() throws Exception\n{\r\n    Configuration conf = new Configuration();\r\n    conf.setInt(Job.COMPLETION_POLL_INTERVAL_KEY, 100);\r\n    Job job = Job.getInstance(conf);\r\n    conf = job.getConfiguration();\r\n    conf.setInt(MRJobConfig.IO_SORT_MB, 1);\r\n    conf.set(MRJobConfig.MAP_SORT_SPILL_PERCENT, Float.toString(.986328125f));\r\n    conf.setClass(\"test.mapcollection.class\", StepFactory.class, RecordFactory.class);\r\n    StepFactory.setLengths(conf, 4000, 261120, 96, 1024, 251);\r\n    conf.setInt(\"test.spillmap.records\", 255);\r\n    conf.setBoolean(\"test.disable.key.read\", false);\r\n    conf.setBoolean(\"test.disable.val.read\", false);\r\n    runTest(\"largeconcurrent\", job);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "testRandom",
  "errType" : null,
  "containingMethodsNum" : 13,
  "sourceCodeText" : "void testRandom() throws Exception\n{\r\n    Configuration conf = new Configuration();\r\n    conf.setInt(Job.COMPLETION_POLL_INTERVAL_KEY, 100);\r\n    Job job = Job.getInstance(conf);\r\n    conf = job.getConfiguration();\r\n    conf.setInt(MRJobConfig.IO_SORT_MB, 1);\r\n    conf.setClass(\"test.mapcollection.class\", RandomFactory.class, RecordFactory.class);\r\n    final Random r = new Random();\r\n    final long seed = r.nextLong();\r\n    LOG.info(\"SEED: \" + seed);\r\n    r.setSeed(seed);\r\n    conf.set(MRJobConfig.MAP_SORT_SPILL_PERCENT, Float.toString(Math.max(0.1f, r.nextFloat())));\r\n    RandomFactory.setLengths(conf, r, 1 << 14);\r\n    conf.setInt(\"test.spillmap.records\", r.nextInt(500));\r\n    conf.setLong(\"test.randomfactory.seed\", r.nextLong());\r\n    runTest(\"random\", job);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "testRandomCompress",
  "errType" : null,
  "containingMethodsNum" : 14,
  "sourceCodeText" : "void testRandomCompress() throws Exception\n{\r\n    Configuration conf = new Configuration();\r\n    conf.setInt(Job.COMPLETION_POLL_INTERVAL_KEY, 100);\r\n    Job job = Job.getInstance(conf);\r\n    conf = job.getConfiguration();\r\n    conf.setInt(MRJobConfig.IO_SORT_MB, 1);\r\n    conf.setBoolean(MRJobConfig.MAP_OUTPUT_COMPRESS, true);\r\n    conf.setClass(\"test.mapcollection.class\", RandomFactory.class, RecordFactory.class);\r\n    final Random r = new Random();\r\n    final long seed = r.nextLong();\r\n    LOG.info(\"SEED: \" + seed);\r\n    r.setSeed(seed);\r\n    conf.set(MRJobConfig.MAP_SORT_SPILL_PERCENT, Float.toString(Math.max(0.1f, r.nextFloat())));\r\n    RandomFactory.setLengths(conf, r, 1 << 14);\r\n    conf.setInt(\"test.spillmap.records\", r.nextInt(500));\r\n    conf.setLong(\"test.randomfactory.seed\", r.nextLong());\r\n    runTest(\"randomCompress\", job);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\fs\\slive",
  "methodName" : "toString",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String toString()\n{\r\n    return op.toString();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\fs\\slive",
  "methodName" : "run",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "List<OperationOutput> run(FileSystem fs)\n{\r\n    List<OperationOutput> result = null;\r\n    try {\r\n        if (observer != null) {\r\n            observer.notifyStarting(op);\r\n        }\r\n        result = op.run(fs);\r\n    } finally {\r\n        if (observer != null) {\r\n            observer.notifyFinished(op);\r\n        }\r\n    }\r\n    return result;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred\\pipes",
  "methodName" : "cleanup",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void cleanup(FileSystem fs, Path p) throws IOException\n{\r\n    fs.delete(p, true);\r\n    assertFalse(\"output not cleaned up\", fs.exists(p));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred\\pipes",
  "methodName" : "testPipes",
  "errType" : null,
  "containingMethodsNum" : 14,
  "sourceCodeText" : "void testPipes() throws IOException\n{\r\n    if (System.getProperty(\"compile.c++\") == null) {\r\n        LOG.info(\"compile.c++ is not defined, so skipping TestPipes\");\r\n        return;\r\n    }\r\n    MiniDFSCluster dfs = null;\r\n    MiniMRCluster mr = null;\r\n    Path inputPath = new Path(\"testing/in\");\r\n    Path outputPath = new Path(\"testing/out\");\r\n    try {\r\n        final int numWorkers = 2;\r\n        Configuration conf = new Configuration();\r\n        dfs = new MiniDFSCluster.Builder(conf).numDataNodes(numWorkers).build();\r\n        mr = new MiniMRCluster(numWorkers, dfs.getFileSystem().getUri().toString(), 1);\r\n        writeInputFile(dfs.getFileSystem(), inputPath);\r\n        runProgram(mr, dfs, wordCountSimple, inputPath, outputPath, 3, 2, twoSplitOutput, null);\r\n        cleanup(dfs.getFileSystem(), outputPath);\r\n        runProgram(mr, dfs, wordCountSimple, inputPath, outputPath, 3, 0, noSortOutput, null);\r\n        cleanup(dfs.getFileSystem(), outputPath);\r\n        runProgram(mr, dfs, wordCountPart, inputPath, outputPath, 3, 2, fixedPartitionOutput, null);\r\n        runNonPipedProgram(mr, dfs, wordCountNoPipes, null);\r\n        mr.waitUntilIdle();\r\n    } finally {\r\n        mr.shutdown();\r\n        dfs.shutdown();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred\\pipes",
  "methodName" : "writeInputFile",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void writeInputFile(FileSystem fs, Path dir) throws IOException\n{\r\n    DataOutputStream out = fs.create(new Path(dir, \"part0\"));\r\n    out.writeBytes(\"Alice was beginning to get very tired of sitting by her\\n\");\r\n    out.writeBytes(\"sister on the bank, and of having nothing to do: once\\n\");\r\n    out.writeBytes(\"or twice she had peeped into the book her sister was\\n\");\r\n    out.writeBytes(\"reading, but it had no pictures or conversations in\\n\");\r\n    out.writeBytes(\"it, `and what is the use of a book,' thought Alice\\n\");\r\n    out.writeBytes(\"`without pictures or conversation?'\\n\");\r\n    out.close();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred\\pipes",
  "methodName" : "runProgram",
  "errType" : [ "InterruptedException" ],
  "containingMethodsNum" : 25,
  "sourceCodeText" : "void runProgram(MiniMRCluster mr, MiniDFSCluster dfs, Path program, Path inputPath, Path outputPath, int numMaps, int numReduces, String[] expectedResults, JobConf conf) throws IOException\n{\r\n    Path wordExec = new Path(\"testing/bin/application\");\r\n    JobConf job = null;\r\n    if (conf == null) {\r\n        job = mr.createJobConf();\r\n    } else {\r\n        job = new JobConf(conf);\r\n    }\r\n    job.setNumMapTasks(numMaps);\r\n    job.setNumReduceTasks(numReduces);\r\n    {\r\n        FileSystem fs = dfs.getFileSystem();\r\n        fs.delete(wordExec.getParent(), true);\r\n        fs.copyFromLocalFile(program, wordExec);\r\n        Submitter.setExecutable(job, fs.makeQualified(wordExec).toString());\r\n        Submitter.setIsJavaRecordReader(job, true);\r\n        Submitter.setIsJavaRecordWriter(job, true);\r\n        FileInputFormat.setInputPaths(job, inputPath);\r\n        FileOutputFormat.setOutputPath(job, outputPath);\r\n        RunningJob rJob = null;\r\n        if (numReduces == 0) {\r\n            rJob = Submitter.jobSubmit(job);\r\n            while (!rJob.isComplete()) {\r\n                try {\r\n                    Thread.sleep(1000);\r\n                } catch (InterruptedException ie) {\r\n                    throw new RuntimeException(ie);\r\n                }\r\n            }\r\n        } else {\r\n            rJob = Submitter.runJob(job);\r\n        }\r\n        assertTrue(\"pipes job failed\", rJob.isSuccessful());\r\n        Counters counters = rJob.getCounters();\r\n        Counters.Group wordCountCounters = counters.getGroup(\"WORDCOUNT\");\r\n        int numCounters = 0;\r\n        for (Counter c : wordCountCounters) {\r\n            System.out.println(c);\r\n            ++numCounters;\r\n        }\r\n        assertTrue(\"No counters found!\", (numCounters > 0));\r\n    }\r\n    List<String> results = new ArrayList<String>();\r\n    for (Path p : FileUtil.stat2Paths(dfs.getFileSystem().listStatus(outputPath, new Utils.OutputFileUtils.OutputFilesFilter()))) {\r\n        results.add(MapReduceTestUtil.readOutput(p, job));\r\n    }\r\n    assertEquals(\"number of reduces is wrong\", expectedResults.length, results.size());\r\n    for (int i = 0; i < results.size(); i++) {\r\n        assertEquals(\"pipes program \" + program + \" output \" + i + \" wrong\", expectedResults[i], results.get(i));\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred\\pipes",
  "methodName" : "runNonPipedProgram",
  "errType" : [ "Exception" ],
  "containingMethodsNum" : 30,
  "sourceCodeText" : "void runNonPipedProgram(MiniMRCluster mr, MiniDFSCluster dfs, Path program, JobConf conf) throws IOException\n{\r\n    JobConf job;\r\n    if (conf == null) {\r\n        job = mr.createJobConf();\r\n    } else {\r\n        job = new JobConf(conf);\r\n    }\r\n    job.setInputFormat(WordCountInputFormat.class);\r\n    FileSystem local = FileSystem.getLocal(job);\r\n    Path testDir = new Path(\"file:\" + System.getProperty(\"test.build.data\"), \"pipes\");\r\n    Path inDir = new Path(testDir, \"input\");\r\n    nonPipedOutDir = new Path(testDir, \"output\");\r\n    Path wordExec = new Path(\"testing/bin/application\");\r\n    Path jobXml = new Path(testDir, \"job.xml\");\r\n    {\r\n        FileSystem fs = dfs.getFileSystem();\r\n        fs.delete(wordExec.getParent(), true);\r\n        fs.copyFromLocalFile(program, wordExec);\r\n    }\r\n    DataOutputStream out = local.create(new Path(inDir, \"part0\"));\r\n    out.writeBytes(\"i am a silly test\\n\");\r\n    out.writeBytes(\"you are silly\\n\");\r\n    out.writeBytes(\"i am a cat test\\n\");\r\n    out.writeBytes(\"you is silly\\n\");\r\n    out.writeBytes(\"i am a billy test\\n\");\r\n    out.writeBytes(\"hello are silly\\n\");\r\n    out.close();\r\n    out = local.create(new Path(inDir, \"part1\"));\r\n    out.writeBytes(\"mall world things drink java\\n\");\r\n    out.writeBytes(\"hall silly cats drink java\\n\");\r\n    out.writeBytes(\"all dogs bow wow\\n\");\r\n    out.writeBytes(\"hello drink java\\n\");\r\n    out.close();\r\n    local.delete(nonPipedOutDir, true);\r\n    local.mkdirs(nonPipedOutDir, new FsPermission(FsAction.ALL, FsAction.ALL, FsAction.ALL));\r\n    out = local.create(jobXml);\r\n    job.writeXml(out);\r\n    out.close();\r\n    System.err.println(\"About to run: Submitter -conf \" + jobXml + \" -input \" + inDir + \" -output \" + nonPipedOutDir + \" -program \" + dfs.getFileSystem().makeQualified(wordExec));\r\n    try {\r\n        int ret = ToolRunner.run(new Submitter(), new String[] { \"-conf\", jobXml.toString(), \"-input\", inDir.toString(), \"-output\", nonPipedOutDir.toString(), \"-program\", dfs.getFileSystem().makeQualified(wordExec).toString(), \"-reduces\", \"2\" });\r\n        assertEquals(0, ret);\r\n    } catch (Exception e) {\r\n        assertTrue(\"got exception: \" + StringUtils.stringifyException(e), false);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "testIFileStream",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void testIFileStream() throws Exception\n{\r\n    final int DLEN = 100;\r\n    DataOutputBuffer dob = new DataOutputBuffer(DLEN + 4);\r\n    IFileOutputStream ifos = new IFileOutputStream(dob);\r\n    for (int i = 0; i < DLEN; ++i) {\r\n        ifos.write(i);\r\n    }\r\n    ifos.close();\r\n    DataInputBuffer dib = new DataInputBuffer();\r\n    dib.reset(dob.getData(), DLEN + 4);\r\n    IFileInputStream ifis = new IFileInputStream(dib, 104, new Configuration());\r\n    for (int i = 0; i < DLEN; ++i) {\r\n        assertEquals(i, ifis.read());\r\n    }\r\n    ifis.close();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "testBadIFileStream",
  "errType" : [ "ChecksumException" ],
  "containingMethodsNum" : 9,
  "sourceCodeText" : "void testBadIFileStream() throws Exception\n{\r\n    final int DLEN = 100;\r\n    DataOutputBuffer dob = new DataOutputBuffer(DLEN + 4);\r\n    IFileOutputStream ifos = new IFileOutputStream(dob);\r\n    for (int i = 0; i < DLEN; ++i) {\r\n        ifos.write(i);\r\n    }\r\n    ifos.close();\r\n    DataInputBuffer dib = new DataInputBuffer();\r\n    final byte[] b = dob.getData();\r\n    ++b[17];\r\n    dib.reset(b, DLEN + 4);\r\n    IFileInputStream ifis = new IFileInputStream(dib, 104, new Configuration());\r\n    int i = 0;\r\n    try {\r\n        while (i < DLEN) {\r\n            if (17 == i) {\r\n                assertEquals(18, ifis.read());\r\n            } else {\r\n                assertEquals(i, ifis.read());\r\n            }\r\n            ++i;\r\n        }\r\n        ifis.close();\r\n    } catch (ChecksumException e) {\r\n        assertEquals(\"Unexpected bad checksum\", DLEN - 1, i);\r\n        return;\r\n    }\r\n    fail(\"Did not detect bad data in checksum\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "testBadLength",
  "errType" : [ "ChecksumException" ],
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void testBadLength() throws Exception\n{\r\n    final int DLEN = 100;\r\n    DataOutputBuffer dob = new DataOutputBuffer(DLEN + 4);\r\n    IFileOutputStream ifos = new IFileOutputStream(dob);\r\n    for (int i = 0; i < DLEN; ++i) {\r\n        ifos.write(i);\r\n    }\r\n    ifos.close();\r\n    DataInputBuffer dib = new DataInputBuffer();\r\n    dib.reset(dob.getData(), DLEN + 4);\r\n    IFileInputStream ifis = new IFileInputStream(dib, 100, new Configuration());\r\n    int i = 0;\r\n    try {\r\n        while (i < DLEN - 8) {\r\n            assertEquals(i++, ifis.read());\r\n        }\r\n        ifis.close();\r\n    } catch (ChecksumException e) {\r\n        assertEquals(\"Checksum before close\", i, DLEN - 8);\r\n        return;\r\n    }\r\n    fail(\"Did not detect bad data in checksum\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "testCloseStreamOnException",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void testCloseStreamOnException() throws Exception\n{\r\n    OutputStream outputStream = Mockito.mock(OutputStream.class);\r\n    IFileOutputStream ifos = new IFileOutputStream(outputStream);\r\n    Mockito.doThrow(new IOException(\"Dummy Exception\")).when(outputStream).flush();\r\n    try {\r\n        ifos.close();\r\n        fail(\"IOException is not thrown\");\r\n    } catch (IOException ioe) {\r\n        assertEquals(\"Dummy Exception\", ioe.getMessage());\r\n    }\r\n    Mockito.verify(outputStream).close();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop",
  "methodName" : "map",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void map(WritableComparable key, Writable value, OutputCollector<WritableComparable, Writable> out, Reporter reporter) throws IOException\n{\r\n    System.err.println(\"failing map\");\r\n    throw new RuntimeException(\"failing map\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "setupCache",
  "errType" : null,
  "containingMethodsNum" : 10,
  "sourceCodeText" : "void setupCache(String cacheDir, FileSystem fs) throws IOException\n{\r\n    Path localPath = new Path(System.getProperty(\"test.cache.data\", \"build/test/cache\"));\r\n    Path txtPath = new Path(localPath, new Path(\"test.txt\"));\r\n    Path jarPath = new Path(localPath, new Path(\"test.jar\"));\r\n    Path zipPath = new Path(localPath, new Path(\"test.zip\"));\r\n    Path tarPath = new Path(localPath, new Path(\"test.tgz\"));\r\n    Path tarPath1 = new Path(localPath, new Path(\"test.tar.gz\"));\r\n    Path tarPath2 = new Path(localPath, new Path(\"test.tar\"));\r\n    Path cachePath = new Path(cacheDir);\r\n    fs.delete(cachePath, true);\r\n    if (!fs.mkdirs(cachePath)) {\r\n        throw new IOException(\"Mkdirs failed to create \" + cachePath.toString());\r\n    }\r\n    fs.copyFromLocalFile(txtPath, cachePath);\r\n    fs.copyFromLocalFile(jarPath, cachePath);\r\n    fs.copyFromLocalFile(zipPath, cachePath);\r\n    fs.copyFromLocalFile(tarPath, cachePath);\r\n    fs.copyFromLocalFile(tarPath1, cachePath);\r\n    fs.copyFromLocalFile(tarPath2, cachePath);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "launchMRCache",
  "errType" : null,
  "containingMethodsNum" : 39,
  "sourceCodeText" : "TestResult launchMRCache(String indir, String outdir, String cacheDir, JobConf conf, String input) throws IOException\n{\r\n    String TEST_ROOT_DIR = new Path(System.getProperty(\"test.build.data\", \"/tmp\")).toString().replace(' ', '+');\r\n    conf.set(\"test.build.data\", TEST_ROOT_DIR);\r\n    final Path inDir = new Path(indir);\r\n    final Path outDir = new Path(outdir);\r\n    FileSystem fs = FileSystem.get(conf);\r\n    fs.delete(outDir, true);\r\n    if (!fs.mkdirs(inDir)) {\r\n        throw new IOException(\"Mkdirs failed to create \" + inDir.toString());\r\n    }\r\n    {\r\n        System.out.println(\"HERE:\" + inDir);\r\n        DataOutputStream file = fs.create(new Path(inDir, \"part-0\"));\r\n        file.writeBytes(input);\r\n        file.close();\r\n    }\r\n    conf.setJobName(\"cachetest\");\r\n    conf.setOutputKeyClass(Text.class);\r\n    conf.setOutputValueClass(IntWritable.class);\r\n    conf.setCombinerClass(MRCaching.ReduceClass.class);\r\n    conf.setReducerClass(MRCaching.ReduceClass.class);\r\n    FileInputFormat.setInputPaths(conf, inDir);\r\n    FileOutputFormat.setOutputPath(conf, outDir);\r\n    conf.setNumMapTasks(1);\r\n    conf.setNumReduceTasks(1);\r\n    conf.setSpeculativeExecution(false);\r\n    URI[] uris = new URI[6];\r\n    conf.setMapperClass(MRCaching.MapClass2.class);\r\n    uris[0] = fs.getUri().resolve(cacheDir + \"/test.txt\");\r\n    uris[1] = fs.getUri().resolve(cacheDir + \"/test.jar\");\r\n    uris[2] = fs.getUri().resolve(cacheDir + \"/test.zip\");\r\n    uris[3] = fs.getUri().resolve(cacheDir + \"/test.tgz\");\r\n    uris[4] = fs.getUri().resolve(cacheDir + \"/test.tar.gz\");\r\n    uris[5] = fs.getUri().resolve(cacheDir + \"/test.tar\");\r\n    Job.addCacheFile(uris[0], conf);\r\n    long[] fileSizes = new long[1];\r\n    fileSizes[0] = fs.getFileStatus(new Path(uris[0].getPath())).getLen();\r\n    long[] archiveSizes = new long[5];\r\n    for (int i = 1; i < 6; i++) {\r\n        Job.addCacheArchive(uris[i], conf);\r\n        archiveSizes[i - 1] = fs.getFileStatus(new Path(uris[i].getPath())).getLen();\r\n    }\r\n    RunningJob job = JobClient.runJob(conf);\r\n    int count = 0;\r\n    Path result = new Path(TEST_ROOT_DIR + \"/test.txt\");\r\n    {\r\n        BufferedReader file = new BufferedReader(new InputStreamReader(FileSystem.getLocal(conf).open(result)));\r\n        String line = file.readLine();\r\n        while (line != null) {\r\n            if (!testStr.equals(line))\r\n                return new TestResult(job, false);\r\n            count++;\r\n            line = file.readLine();\r\n        }\r\n        file.close();\r\n    }\r\n    if (count != 6)\r\n        return new TestResult(job, false);\r\n    validateCacheFileSizes(job.getConfiguration(), fileSizes, MRJobConfig.CACHE_FILES_SIZES);\r\n    validateCacheFileSizes(job.getConfiguration(), archiveSizes, MRJobConfig.CACHE_ARCHIVES_SIZES);\r\n    return new TestResult(job, true);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "validateCacheFileSizes",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void validateCacheFileSizes(Configuration job, long[] expectedSizes, String configKey) throws IOException\n{\r\n    String configValues = job.get(configKey, \"\");\r\n    System.out.println(configKey + \" -> \" + configValues);\r\n    String[] realSizes = StringUtils.getStrings(configValues);\r\n    Assert.assertEquals(\"Number of files for \" + configKey, expectedSizes.length, realSizes.length);\r\n    for (int i = 0; i < expectedSizes.length; ++i) {\r\n        long actual = Long.valueOf(realSizes[i]);\r\n        long expected = expectedSizes[i];\r\n        Assert.assertEquals(\"File \" + i + \" for \" + configKey, expected, actual);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "runTestLazyOutput",
  "errType" : null,
  "containingMethodsNum" : 14,
  "sourceCodeText" : "void runTestLazyOutput(JobConf job, Path output, int numReducers, boolean createLazily) throws Exception\n{\r\n    job.setJobName(\"test-lazy-output\");\r\n    FileInputFormat.setInputPaths(job, INPUTPATH);\r\n    FileOutputFormat.setOutputPath(job, output);\r\n    job.setInputFormat(TextInputFormat.class);\r\n    job.setMapOutputKeyClass(LongWritable.class);\r\n    job.setMapOutputValueClass(Text.class);\r\n    job.setOutputKeyClass(LongWritable.class);\r\n    job.setOutputValueClass(Text.class);\r\n    job.setMapperClass(TestMapper.class);\r\n    job.setReducerClass(TestReducer.class);\r\n    JobClient client = new JobClient(job);\r\n    job.setNumReduceTasks(numReducers);\r\n    if (createLazily) {\r\n        LazyOutputFormat.setOutputFormatClass(job, TextOutputFormat.class);\r\n    } else {\r\n        job.setOutputFormat(TextOutputFormat.class);\r\n    }\r\n    JobClient.runJob(job);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "createInput",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void createInput(FileSystem fs, int numMappers) throws Exception\n{\r\n    for (int i = 0; i < numMappers; i++) {\r\n        OutputStream os = fs.create(new Path(INPUTPATH, \"text\" + i + \".txt\"));\r\n        Writer wr = new OutputStreamWriter(os);\r\n        for (String inp : INPUTLIST) {\r\n            wr.write(inp + \"\\n\");\r\n        }\r\n        wr.close();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "testLazyOutput",
  "errType" : null,
  "containingMethodsNum" : 18,
  "sourceCodeText" : "void testLazyOutput() throws Exception\n{\r\n    MiniDFSCluster dfs = null;\r\n    MiniMRCluster mr = null;\r\n    FileSystem fileSys = null;\r\n    try {\r\n        Configuration conf = new Configuration();\r\n        dfs = new MiniDFSCluster.Builder(conf).numDataNodes(NUM_HADOOP_WORKERS).build();\r\n        fileSys = dfs.getFileSystem();\r\n        mr = new MiniMRCluster(NUM_HADOOP_WORKERS, fileSys.getUri().toString(), 1);\r\n        int numReducers = 2;\r\n        int numMappers = NUM_HADOOP_WORKERS * NUM_MAPS_PER_NODE;\r\n        createInput(fileSys, numMappers);\r\n        Path output1 = new Path(\"/testlazy/output1\");\r\n        runTestLazyOutput(mr.createJobConf(), output1, numReducers, true);\r\n        Path[] fileList = FileUtil.stat2Paths(fileSys.listStatus(output1, new Utils.OutputFileUtils.OutputFilesFilter()));\r\n        for (int i = 0; i < fileList.length; ++i) {\r\n            System.out.println(\"Test1 File list[\" + i + \"]\" + \": \" + fileList[i]);\r\n        }\r\n        assertTrue(fileList.length == (numReducers - 1));\r\n        Path output2 = new Path(\"/testlazy/output2\");\r\n        runTestLazyOutput(mr.createJobConf(), output2, 0, true);\r\n        fileList = FileUtil.stat2Paths(fileSys.listStatus(output2, new Utils.OutputFileUtils.OutputFilesFilter()));\r\n        for (int i = 0; i < fileList.length; ++i) {\r\n            System.out.println(\"Test2 File list[\" + i + \"]\" + \": \" + fileList[i]);\r\n        }\r\n        assertTrue(fileList.length == numMappers - 1);\r\n        Path output3 = new Path(\"/testlazy/output3\");\r\n        runTestLazyOutput(mr.createJobConf(), output3, 0, false);\r\n        fileList = FileUtil.stat2Paths(fileSys.listStatus(output3, new Utils.OutputFileUtils.OutputFilesFilter()));\r\n        for (int i = 0; i < fileList.length; ++i) {\r\n            System.out.println(\"Test3 File list[\" + i + \"]\" + \": \" + fileList[i]);\r\n        }\r\n        assertTrue(fileList.length == numMappers);\r\n    } finally {\r\n        if (dfs != null) {\r\n            dfs.shutdown();\r\n        }\r\n        if (mr != null) {\r\n            mr.shutdown();\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "testFormat",
  "errType" : null,
  "containingMethodsNum" : 24,
  "sourceCodeText" : "void testFormat() throws Exception\n{\r\n    JobConf job = new JobConf(defaultConf);\r\n    Random random = new Random();\r\n    long seed = random.nextLong();\r\n    LOG.info(\"seed = \" + seed);\r\n    random.setSeed(seed);\r\n    localFs.delete(workDir, true);\r\n    FileInputFormat.setInputPaths(job, workDir);\r\n    final int length = 10000;\r\n    final int numFiles = 10;\r\n    createFiles(length, numFiles, random);\r\n    CombineTextInputFormat format = new CombineTextInputFormat();\r\n    LongWritable key = new LongWritable();\r\n    Text value = new Text();\r\n    for (int i = 0; i < 3; i++) {\r\n        int numSplits = random.nextInt(length / 20) + 1;\r\n        LOG.info(\"splitting: requesting = \" + numSplits);\r\n        InputSplit[] splits = format.getSplits(job, numSplits);\r\n        LOG.info(\"splitting: got =        \" + splits.length);\r\n        assertEquals(\"We got more than one splits!\", 1, splits.length);\r\n        InputSplit split = splits[0];\r\n        assertEquals(\"It should be CombineFileSplit\", CombineFileSplit.class, split.getClass());\r\n        BitSet bits = new BitSet(length);\r\n        LOG.debug(\"split= \" + split);\r\n        RecordReader<LongWritable, Text> reader = format.getRecordReader(split, job, voidReporter);\r\n        try {\r\n            int count = 0;\r\n            while (reader.next(key, value)) {\r\n                int v = Integer.parseInt(value.toString());\r\n                LOG.debug(\"read \" + v);\r\n                if (bits.get(v)) {\r\n                    LOG.warn(\"conflict with \" + v + \" at position \" + reader.getPos());\r\n                }\r\n                assertFalse(\"Key in multiple partitions.\", bits.get(v));\r\n                bits.set(v);\r\n                count++;\r\n            }\r\n            LOG.info(\"splits=\" + split + \" count=\" + count);\r\n        } finally {\r\n            reader.close();\r\n        }\r\n        assertEquals(\"Some keys in no partition.\", length, bits.cardinality());\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "createRanges",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Range[] createRanges(int length, int numFiles, Random random)\n{\r\n    Range[] ranges = new Range[numFiles];\r\n    for (int i = 0; i < numFiles; i++) {\r\n        int start = i == 0 ? 0 : ranges[i - 1].end;\r\n        int end = i == numFiles - 1 ? length : (length / numFiles) * (2 * i + 1) / 2 + random.nextInt(length / numFiles) + 1;\r\n        ranges[i] = new Range(start, end);\r\n    }\r\n    return ranges;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "createFiles",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void createFiles(int length, int numFiles, Random random) throws IOException\n{\r\n    Range[] ranges = createRanges(length, numFiles, random);\r\n    for (int i = 0; i < numFiles; i++) {\r\n        Path file = new Path(workDir, \"test_\" + i + \".txt\");\r\n        Writer writer = new OutputStreamWriter(localFs.create(file));\r\n        Range range = ranges[i];\r\n        try {\r\n            for (int j = range.start; j < range.end; j++) {\r\n                writer.write(Integer.toString(j));\r\n                writer.write(\"\\n\");\r\n            }\r\n        } finally {\r\n            writer.close();\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "writeFile",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void writeFile(FileSystem fs, Path name, CompressionCodec codec, String contents) throws IOException\n{\r\n    OutputStream stm;\r\n    if (codec == null) {\r\n        stm = fs.create(name);\r\n    } else {\r\n        stm = codec.createOutputStream(fs.create(name));\r\n    }\r\n    stm.write(contents.getBytes());\r\n    stm.close();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "readSplit",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "List<Text> readSplit(InputFormat<LongWritable, Text> format, InputSplit split, JobConf job) throws IOException\n{\r\n    List<Text> result = new ArrayList<Text>();\r\n    RecordReader<LongWritable, Text> reader = format.getRecordReader(split, job, voidReporter);\r\n    LongWritable key = reader.createKey();\r\n    Text value = reader.createValue();\r\n    while (reader.next(key, value)) {\r\n        result.add(value);\r\n        value = reader.createValue();\r\n    }\r\n    reader.close();\r\n    return result;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "testGzip",
  "errType" : null,
  "containingMethodsNum" : 15,
  "sourceCodeText" : "void testGzip() throws IOException\n{\r\n    JobConf job = new JobConf(defaultConf);\r\n    CompressionCodec gzip = new GzipCodec();\r\n    ReflectionUtils.setConf(gzip, job);\r\n    localFs.delete(workDir, true);\r\n    writeFile(localFs, new Path(workDir, \"part1.txt.gz\"), gzip, \"the quick\\nbrown\\nfox jumped\\nover\\n the lazy\\n dog\\n\");\r\n    writeFile(localFs, new Path(workDir, \"part2.txt.gz\"), gzip, \"this is a test\\nof gzip\\n\");\r\n    FileInputFormat.setInputPaths(job, workDir);\r\n    CombineTextInputFormat format = new CombineTextInputFormat();\r\n    InputSplit[] splits = format.getSplits(job, 100);\r\n    assertEquals(\"compressed splits == 1\", 1, splits.length);\r\n    List<Text> results = readSplit(format, splits[0], job);\r\n    assertEquals(\"splits[0] length\", 8, results.size());\r\n    final String[] firstList = { \"the quick\", \"brown\", \"fox jumped\", \"over\", \" the lazy\", \" dog\" };\r\n    final String[] secondList = { \"this is a test\", \"of gzip\" };\r\n    String first = results.get(0).toString();\r\n    if (first.equals(firstList[0])) {\r\n        testResults(results, firstList, secondList);\r\n    } else if (first.equals(secondList[0])) {\r\n        testResults(results, secondList, firstList);\r\n    } else {\r\n        fail(\"unexpected first token!\");\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "testResults",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void testResults(List<Text> results, String[] first, String[] second)\n{\r\n    for (int i = 0; i < first.length; i++) {\r\n        assertEquals(\"splits[0][\" + i + \"]\", first[i], results.get(i).toString());\r\n    }\r\n    for (int i = 0; i < second.length; i++) {\r\n        int j = i + first.length;\r\n        assertEquals(\"splits[0][\" + j + \"]\", second[i], results.get(j).toString());\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "writeOutput",
  "errType" : null,
  "containingMethodsNum" : 10,
  "sourceCodeText" : "void writeOutput(RecordWriter theRecordWriter, Reporter reporter) throws IOException\n{\r\n    NullWritable nullWritable = NullWritable.get();\r\n    try {\r\n        theRecordWriter.write(key1, val1);\r\n        theRecordWriter.write(null, nullWritable);\r\n        theRecordWriter.write(null, val1);\r\n        theRecordWriter.write(nullWritable, val2);\r\n        theRecordWriter.write(key2, nullWritable);\r\n        theRecordWriter.write(key1, null);\r\n        theRecordWriter.write(null, null);\r\n        theRecordWriter.write(key2, val2);\r\n    } finally {\r\n        theRecordWriter.close(reporter);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "setConfForFileOutputCommitter",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void setConfForFileOutputCommitter(JobConf job)\n{\r\n    job.set(JobContext.TASK_ATTEMPT_ID, attempt);\r\n    job.setOutputCommitter(FileOutputCommitter.class);\r\n    FileOutputFormat.setOutputPath(job, outDir);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "testCommitter",
  "errType" : null,
  "containingMethodsNum" : 19,
  "sourceCodeText" : "void testCommitter() throws Exception\n{\r\n    JobConf job = new JobConf();\r\n    setConfForFileOutputCommitter(job);\r\n    JobContext jContext = new JobContextImpl(job, taskID.getJobID());\r\n    TaskAttemptContext tContext = new TaskAttemptContextImpl(job, taskID);\r\n    FileOutputCommitter committer = new FileOutputCommitter();\r\n    FileOutputFormat.setWorkOutputPath(job, committer.getTaskAttemptPath(tContext));\r\n    committer.setupJob(jContext);\r\n    committer.setupTask(tContext);\r\n    String file = \"test.txt\";\r\n    Reporter reporter = Reporter.NULL;\r\n    FileSystem localFs = FileSystem.getLocal(job);\r\n    TextOutputFormat theOutputFormat = new TextOutputFormat();\r\n    RecordWriter theRecordWriter = theOutputFormat.getRecordWriter(localFs, job, file, reporter);\r\n    writeOutput(theRecordWriter, reporter);\r\n    committer.commitTask(tContext);\r\n    committer.commitJob(jContext);\r\n    File expectedFile = new File(new Path(outDir, file).toString());\r\n    StringBuffer expectedOutput = new StringBuffer();\r\n    expectedOutput.append(key1).append('\\t').append(val1).append(\"\\n\");\r\n    expectedOutput.append(val1).append(\"\\n\");\r\n    expectedOutput.append(val2).append(\"\\n\");\r\n    expectedOutput.append(key2).append(\"\\n\");\r\n    expectedOutput.append(key1).append(\"\\n\");\r\n    expectedOutput.append(key2).append('\\t').append(val2).append(\"\\n\");\r\n    String output = UtilsForTests.slurp(expectedFile);\r\n    assertThat(output).isEqualTo(expectedOutput.toString());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "testAbort",
  "errType" : null,
  "containingMethodsNum" : 16,
  "sourceCodeText" : "void testAbort() throws IOException\n{\r\n    FileUtil.fullyDelete(new File(outDir.toString()));\r\n    JobConf job = new JobConf();\r\n    setConfForFileOutputCommitter(job);\r\n    JobContext jContext = new JobContextImpl(job, taskID.getJobID());\r\n    TaskAttemptContext tContext = new TaskAttemptContextImpl(job, taskID);\r\n    FileOutputCommitter committer = new FileOutputCommitter();\r\n    FileOutputFormat.setWorkOutputPath(job, committer.getTaskAttemptPath(tContext));\r\n    committer.setupJob(jContext);\r\n    committer.setupTask(tContext);\r\n    String file = \"test.txt\";\r\n    Reporter reporter = Reporter.NULL;\r\n    FileSystem localFs = FileSystem.getLocal(job);\r\n    TextOutputFormat theOutputFormat = new TextOutputFormat();\r\n    RecordWriter theRecordWriter = theOutputFormat.getRecordWriter(localFs, job, file, reporter);\r\n    writeOutput(theRecordWriter, reporter);\r\n    committer.abortTask(tContext);\r\n    File expectedFile = new File(new Path(committer.getTaskAttemptPath(tContext), file).toString());\r\n    assertFalse(\"task temp dir still exists\", expectedFile.exists());\r\n    committer.abortJob(jContext, JobStatus.State.FAILED);\r\n    expectedFile = new File(new Path(outDir, FileOutputCommitter.TEMP_DIR_NAME).toString());\r\n    assertFalse(\"job temp dir \" + expectedFile + \" still exists\", expectedFile.exists());\r\n    assertEquals(\"Output directory not empty\", 0, new File(outDir.toString()).listFiles().length);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "testFailAbort",
  "errType" : [ "IOException", "IOException" ],
  "containingMethodsNum" : 21,
  "sourceCodeText" : "void testFailAbort() throws IOException\n{\r\n    JobConf job = new JobConf();\r\n    job.set(FileSystem.FS_DEFAULT_NAME_KEY, \"faildel:///\");\r\n    job.setClass(\"fs.faildel.impl\", FakeFileSystem.class, FileSystem.class);\r\n    setConfForFileOutputCommitter(job);\r\n    JobContext jContext = new JobContextImpl(job, taskID.getJobID());\r\n    TaskAttemptContext tContext = new TaskAttemptContextImpl(job, taskID);\r\n    FileOutputCommitter committer = new FileOutputCommitter();\r\n    FileOutputFormat.setWorkOutputPath(job, committer.getTaskAttemptPath(tContext));\r\n    committer.setupJob(jContext);\r\n    committer.setupTask(tContext);\r\n    String file = \"test.txt\";\r\n    File jobTmpDir = new File(committer.getJobAttemptPath(jContext).toUri().getPath());\r\n    File taskTmpDir = new File(committer.getTaskAttemptPath(tContext).toUri().getPath());\r\n    File expectedFile = new File(taskTmpDir, file);\r\n    Reporter reporter = Reporter.NULL;\r\n    FileSystem localFs = new FakeFileSystem();\r\n    TextOutputFormat theOutputFormat = new TextOutputFormat();\r\n    RecordWriter theRecordWriter = theOutputFormat.getRecordWriter(localFs, job, expectedFile.getAbsolutePath(), reporter);\r\n    writeOutput(theRecordWriter, reporter);\r\n    Throwable th = null;\r\n    try {\r\n        committer.abortTask(tContext);\r\n    } catch (IOException ie) {\r\n        th = ie;\r\n    }\r\n    assertNotNull(th);\r\n    assertTrue(th instanceof IOException);\r\n    assertTrue(th.getMessage().contains(\"fake delete failed\"));\r\n    assertTrue(expectedFile + \" does not exists\", expectedFile.exists());\r\n    th = null;\r\n    try {\r\n        committer.abortJob(jContext, JobStatus.State.FAILED);\r\n    } catch (IOException ie) {\r\n        th = ie;\r\n    }\r\n    assertNotNull(th);\r\n    assertTrue(th instanceof IOException);\r\n    assertTrue(th.getMessage().contains(\"fake delete failed\"));\r\n    assertTrue(\"job temp dir does not exists\", jobTmpDir.exists());\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 2,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "teardown",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void teardown()\n{\r\n    FileUtil.fullyDelete(new File(outDir.toString()));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\security",
  "methodName" : "setUp",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void setUp() throws Exception\n{\r\n    System.setProperty(\"hadoop.log.dir\", \"logs\");\r\n    Configuration conf = new Configuration();\r\n    dfsCluster = new MiniDFSCluster.Builder(conf).numDataNodes(numWorkers).build();\r\n    jConf = new JobConf(conf);\r\n    FileSystem.setDefaultUri(conf, dfsCluster.getFileSystem().getUri().toString());\r\n    mrCluster = MiniMRClientClusterFactory.create(TestMRCredentials.class, 1, jConf);\r\n    createKeysAsJson(\"keys.json\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\security",
  "methodName" : "tearDown",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void tearDown() throws Exception\n{\r\n    if (mrCluster != null)\r\n        mrCluster.stop();\r\n    mrCluster = null;\r\n    if (dfsCluster != null)\r\n        dfsCluster.shutdown();\r\n    dfsCluster = null;\r\n    new File(\"keys.json\").delete();\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\security",
  "methodName" : "createKeysAsJson",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void createKeysAsJson(String fileName) throws FileNotFoundException, IOException\n{\r\n    StringBuilder jsonString = new StringBuilder();\r\n    jsonString.append(\"{\");\r\n    for (int i = 0; i < NUM_OF_KEYS; i++) {\r\n        String keyName = \"alias\" + i;\r\n        String password = \"password\" + i;\r\n        jsonString.append(\"\\\"\" + keyName + \"\\\":\" + \"\\\"\" + password + \"\\\"\");\r\n        if (i < (NUM_OF_KEYS - 1)) {\r\n            jsonString.append(\",\");\r\n        }\r\n    }\r\n    jsonString.append(\"}\");\r\n    FileOutputStream fos = new FileOutputStream(fileName);\r\n    fos.write(jsonString.toString().getBytes());\r\n    fos.close();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\security",
  "methodName" : "test",
  "errType" : [ "Exception" ],
  "containingMethodsNum" : 10,
  "sourceCodeText" : "void test() throws IOException\n{\r\n    Configuration jobConf = new JobConf(mrCluster.getConfig());\r\n    NameNode nn = dfsCluster.getNameNode();\r\n    URI nnUri = DFSUtilClient.getNNUri(nn.getNameNodeAddress());\r\n    jobConf.set(JobContext.JOB_NAMENODES, nnUri + \",\" + nnUri.toString());\r\n    jobConf.set(\"mapreduce.job.credentials.json\", \"keys.json\");\r\n    String[] args = { \"-m\", \"1\", \"-r\", \"1\", \"-mt\", \"1\", \"-rt\", \"1\" };\r\n    int res = -1;\r\n    try {\r\n        res = ToolRunner.run(jobConf, new CredentialsTestJob(), args);\r\n    } catch (Exception e) {\r\n        System.out.println(\"Job failed with\" + e.getLocalizedMessage());\r\n        e.printStackTrace(System.out);\r\n        fail(\"Job failed\");\r\n    }\r\n    assertThat(res).withFailMessage(\"dist job res is not 0\").isEqualTo(0);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "main",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void main(String[] args) throws Exception\n{\r\n    int res = ToolRunner.run(new Configuration(), new FailJob(), args);\r\n    System.exit(res);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "createJob",
  "errType" : null,
  "containingMethodsNum" : 14,
  "sourceCodeText" : "Job createJob(boolean failMappers, boolean failReducers, Path inputFile) throws IOException\n{\r\n    Configuration conf = getConf();\r\n    conf.setBoolean(FAIL_MAP, failMappers);\r\n    conf.setBoolean(FAIL_REDUCE, failReducers);\r\n    Job job = Job.getInstance(conf, \"fail\");\r\n    job.setJarByClass(FailJob.class);\r\n    job.setMapperClass(FailMapper.class);\r\n    job.setMapOutputKeyClass(LongWritable.class);\r\n    job.setMapOutputValueClass(NullWritable.class);\r\n    job.setReducerClass(FailReducer.class);\r\n    job.setOutputFormatClass(NullOutputFormat.class);\r\n    job.setInputFormatClass(TextInputFormat.class);\r\n    job.setSpeculativeExecution(false);\r\n    job.setJobName(\"Fail job\");\r\n    FileInputFormat.addInputPath(job, inputFile);\r\n    return job;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "run",
  "errType" : null,
  "containingMethodsNum" : 14,
  "sourceCodeText" : "int run(String[] args) throws Exception\n{\r\n    if (args.length < 1) {\r\n        System.err.println(\"FailJob \" + \" (-failMappers|-failReducers)\");\r\n        ToolRunner.printGenericCommandUsage(System.err);\r\n        return 2;\r\n    }\r\n    boolean failMappers = false, failReducers = false;\r\n    for (int i = 0; i < args.length; i++) {\r\n        if (args[i].equals(\"-failMappers\")) {\r\n            failMappers = true;\r\n        } else if (args[i].equals(\"-failReducers\")) {\r\n            failReducers = true;\r\n        }\r\n    }\r\n    if (!(failMappers ^ failReducers)) {\r\n        System.err.println(\"Exactly one of -failMappers or -failReducers must be specified.\");\r\n        return 3;\r\n    }\r\n    final FileSystem fs = FileSystem.get(getConf());\r\n    Path inputDir = new Path(FailJob.class.getSimpleName() + \"_in\");\r\n    fs.mkdirs(inputDir);\r\n    for (int i = 0; i < getConf().getInt(\"mapred.map.tasks\", 1); ++i) {\r\n        BufferedWriter w = new BufferedWriter(new OutputStreamWriter(fs.create(new Path(inputDir, Integer.toString(i)))));\r\n        w.write(Integer.toString(i) + \"\\n\");\r\n        w.close();\r\n    }\r\n    Job job = createJob(failMappers, failReducers, inputDir);\r\n    return job.waitForCompletion(true) ? 0 : 1;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred\\jobcontrol",
  "methodName" : "testLocalJobControlDataCopy",
  "errType" : [ "Exception" ],
  "containingMethodsNum" : 35,
  "sourceCodeText" : "void testLocalJobControlDataCopy() throws Exception\n{\r\n    FileSystem fs = FileSystem.get(createJobConf());\r\n    Path rootDataDir = new Path(System.getProperty(\"test.build.data\", \".\"), \"TestLocalJobControlData\");\r\n    Path indir = new Path(rootDataDir, \"indir\");\r\n    Path outdir_1 = new Path(rootDataDir, \"outdir_1\");\r\n    Path outdir_2 = new Path(rootDataDir, \"outdir_2\");\r\n    Path outdir_3 = new Path(rootDataDir, \"outdir_3\");\r\n    Path outdir_4 = new Path(rootDataDir, \"outdir_4\");\r\n    JobControlTestUtils.cleanData(fs, indir);\r\n    JobControlTestUtils.generateData(fs, indir);\r\n    JobControlTestUtils.cleanData(fs, outdir_1);\r\n    JobControlTestUtils.cleanData(fs, outdir_2);\r\n    JobControlTestUtils.cleanData(fs, outdir_3);\r\n    JobControlTestUtils.cleanData(fs, outdir_4);\r\n    ArrayList<Job> dependingJobs = null;\r\n    ArrayList<Path> inPaths_1 = new ArrayList<Path>();\r\n    inPaths_1.add(indir);\r\n    JobConf jobConf_1 = JobControlTestUtils.createCopyJob(inPaths_1, outdir_1);\r\n    Job job_1 = new Job(jobConf_1, dependingJobs);\r\n    ArrayList<Path> inPaths_2 = new ArrayList<Path>();\r\n    inPaths_2.add(indir);\r\n    JobConf jobConf_2 = JobControlTestUtils.createCopyJob(inPaths_2, outdir_2);\r\n    Job job_2 = new Job(jobConf_2, dependingJobs);\r\n    ArrayList<Path> inPaths_3 = new ArrayList<Path>();\r\n    inPaths_3.add(outdir_1);\r\n    inPaths_3.add(outdir_2);\r\n    JobConf jobConf_3 = JobControlTestUtils.createCopyJob(inPaths_3, outdir_3);\r\n    dependingJobs = new ArrayList<Job>();\r\n    dependingJobs.add(job_1);\r\n    dependingJobs.add(job_2);\r\n    Job job_3 = new Job(jobConf_3, dependingJobs);\r\n    ArrayList<Path> inPaths_4 = new ArrayList<Path>();\r\n    inPaths_4.add(outdir_3);\r\n    JobConf jobConf_4 = JobControlTestUtils.createCopyJob(inPaths_4, outdir_4);\r\n    dependingJobs = new ArrayList<Job>();\r\n    dependingJobs.add(job_3);\r\n    Job job_4 = new Job(jobConf_4, dependingJobs);\r\n    JobControl theControl = new JobControl(\"Test\");\r\n    theControl.addJob(job_1);\r\n    theControl.addJob(job_2);\r\n    theControl.addJob(job_3);\r\n    theControl.addJob(job_4);\r\n    Thread theController = new Thread(theControl);\r\n    theController.start();\r\n    while (!theControl.allFinished()) {\r\n        LOG.debug(\"Jobs in waiting state: \" + theControl.getWaitingJobs().size());\r\n        LOG.debug(\"Jobs in ready state: \" + theControl.getReadyJobs().size());\r\n        LOG.debug(\"Jobs in running state: \" + theControl.getRunningJobs().size());\r\n        LOG.debug(\"Jobs in success state: \" + theControl.getSuccessfulJobs().size());\r\n        LOG.debug(\"Jobs in failed state: \" + theControl.getFailedJobs().size());\r\n        LOG.debug(\"\\n\");\r\n        try {\r\n            Thread.sleep(5000);\r\n        } catch (Exception e) {\r\n        }\r\n    }\r\n    assertEquals(\"Some jobs failed\", 0, theControl.getFailedJobs().size());\r\n    theControl.stop();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "testFormat",
  "errType" : null,
  "containingMethodsNum" : 26,
  "sourceCodeText" : "void testFormat() throws Exception\n{\r\n    JobConf job = new JobConf();\r\n    job.set(JobContext.TASK_ATTEMPT_ID, attempt);\r\n    FileOutputFormat.setOutputPath(job, workDir.getParent().getParent());\r\n    FileOutputFormat.setWorkOutputPath(job, workDir);\r\n    FileSystem fs = workDir.getFileSystem(job);\r\n    if (!fs.mkdirs(workDir)) {\r\n        fail(\"Failed to create output directory\");\r\n    }\r\n    String file = \"test_format.txt\";\r\n    Reporter reporter = Reporter.NULL;\r\n    TextOutputFormat<Object, Object> theOutputFormat = new TextOutputFormat<Object, Object>();\r\n    RecordWriter<Object, Object> theRecordWriter = theOutputFormat.getRecordWriter(localFs, job, file, reporter);\r\n    Text key1 = new Text(\"key1\");\r\n    Text key2 = new Text(\"key2\");\r\n    Text val1 = new Text(\"val1\");\r\n    Text val2 = new Text(\"val2\");\r\n    NullWritable nullWritable = NullWritable.get();\r\n    try {\r\n        theRecordWriter.write(key1, val1);\r\n        theRecordWriter.write(null, nullWritable);\r\n        theRecordWriter.write(null, val1);\r\n        theRecordWriter.write(nullWritable, val2);\r\n        theRecordWriter.write(key2, nullWritable);\r\n        theRecordWriter.write(key1, null);\r\n        theRecordWriter.write(null, null);\r\n        theRecordWriter.write(key2, val2);\r\n    } finally {\r\n        theRecordWriter.close(reporter);\r\n    }\r\n    File expectedFile = new File(new Path(workDir, file).toString());\r\n    StringBuffer expectedOutput = new StringBuffer();\r\n    expectedOutput.append(key1).append('\\t').append(val1).append(\"\\n\");\r\n    expectedOutput.append(val1).append(\"\\n\");\r\n    expectedOutput.append(val2).append(\"\\n\");\r\n    expectedOutput.append(key2).append(\"\\n\");\r\n    expectedOutput.append(key1).append(\"\\n\");\r\n    expectedOutput.append(key2).append('\\t').append(val2).append(\"\\n\");\r\n    String output = UtilsForTests.slurp(expectedFile);\r\n    assertEquals(expectedOutput.toString(), output);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "testFormatWithCustomSeparator",
  "errType" : null,
  "containingMethodsNum" : 27,
  "sourceCodeText" : "void testFormatWithCustomSeparator() throws Exception\n{\r\n    JobConf job = new JobConf();\r\n    String separator = \"\\u0001\";\r\n    job.set(\"mapreduce.output.textoutputformat.separator\", separator);\r\n    job.set(JobContext.TASK_ATTEMPT_ID, attempt);\r\n    FileOutputFormat.setOutputPath(job, workDir.getParent().getParent());\r\n    FileOutputFormat.setWorkOutputPath(job, workDir);\r\n    FileSystem fs = workDir.getFileSystem(job);\r\n    if (!fs.mkdirs(workDir)) {\r\n        fail(\"Failed to create output directory\");\r\n    }\r\n    String file = \"test_custom.txt\";\r\n    Reporter reporter = Reporter.NULL;\r\n    TextOutputFormat<Object, Object> theOutputFormat = new TextOutputFormat<Object, Object>();\r\n    RecordWriter<Object, Object> theRecordWriter = theOutputFormat.getRecordWriter(localFs, job, file, reporter);\r\n    Text key1 = new Text(\"key1\");\r\n    Text key2 = new Text(\"key2\");\r\n    Text val1 = new Text(\"val1\");\r\n    Text val2 = new Text(\"val2\");\r\n    NullWritable nullWritable = NullWritable.get();\r\n    try {\r\n        theRecordWriter.write(key1, val1);\r\n        theRecordWriter.write(null, nullWritable);\r\n        theRecordWriter.write(null, val1);\r\n        theRecordWriter.write(nullWritable, val2);\r\n        theRecordWriter.write(key2, nullWritable);\r\n        theRecordWriter.write(key1, null);\r\n        theRecordWriter.write(null, null);\r\n        theRecordWriter.write(key2, val2);\r\n    } finally {\r\n        theRecordWriter.close(reporter);\r\n    }\r\n    File expectedFile = new File(new Path(workDir, file).toString());\r\n    StringBuffer expectedOutput = new StringBuffer();\r\n    expectedOutput.append(key1).append(separator).append(val1).append(\"\\n\");\r\n    expectedOutput.append(val1).append(\"\\n\");\r\n    expectedOutput.append(val2).append(\"\\n\");\r\n    expectedOutput.append(key2).append(\"\\n\");\r\n    expectedOutput.append(key1).append(\"\\n\");\r\n    expectedOutput.append(key2).append(separator).append(val2).append(\"\\n\");\r\n    String output = UtilsForTests.slurp(expectedFile);\r\n    assertEquals(expectedOutput.toString(), output);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "testCompress",
  "errType" : null,
  "containingMethodsNum" : 31,
  "sourceCodeText" : "void testCompress() throws IOException\n{\r\n    JobConf job = new JobConf();\r\n    job.set(JobContext.TASK_ATTEMPT_ID, attempt);\r\n    job.set(org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.COMPRESS, \"true\");\r\n    FileOutputFormat.setOutputPath(job, workDir.getParent().getParent());\r\n    FileOutputFormat.setWorkOutputPath(job, workDir);\r\n    FileSystem fs = workDir.getFileSystem(job);\r\n    if (!fs.mkdirs(workDir)) {\r\n        fail(\"Failed to create output directory\");\r\n    }\r\n    String file = \"test_compress.txt\";\r\n    Reporter reporter = Reporter.NULL;\r\n    TextOutputFormat<Object, Object> theOutputFormat = new TextOutputFormat<Object, Object>();\r\n    RecordWriter<Object, Object> theRecordWriter = theOutputFormat.getRecordWriter(localFs, job, file, reporter);\r\n    Text key1 = new Text(\"key1\");\r\n    Text key2 = new Text(\"key2\");\r\n    Text val1 = new Text(\"val1\");\r\n    Text val2 = new Text(\"val2\");\r\n    NullWritable nullWritable = NullWritable.get();\r\n    try {\r\n        theRecordWriter.write(key1, val1);\r\n        theRecordWriter.write(null, nullWritable);\r\n        theRecordWriter.write(null, val1);\r\n        theRecordWriter.write(nullWritable, val2);\r\n        theRecordWriter.write(key2, nullWritable);\r\n        theRecordWriter.write(key1, null);\r\n        theRecordWriter.write(null, null);\r\n        theRecordWriter.write(key2, val2);\r\n    } finally {\r\n        theRecordWriter.close(reporter);\r\n    }\r\n    StringBuffer expectedOutput = new StringBuffer();\r\n    expectedOutput.append(key1).append(\"\\t\").append(val1).append(\"\\n\");\r\n    expectedOutput.append(val1).append(\"\\n\");\r\n    expectedOutput.append(val2).append(\"\\n\");\r\n    expectedOutput.append(key2).append(\"\\n\");\r\n    expectedOutput.append(key1).append(\"\\n\");\r\n    expectedOutput.append(key2).append(\"\\t\").append(val2).append(\"\\n\");\r\n    DefaultCodec codec = new DefaultCodec();\r\n    codec.setConf(job);\r\n    Path expectedFile = new Path(workDir, file + codec.getDefaultExtension());\r\n    final FileInputStream istream = new FileInputStream(expectedFile.toString());\r\n    CompressionInputStream cistream = codec.createInputStream(istream);\r\n    LineReader reader = new LineReader(cistream);\r\n    String output = \"\";\r\n    Text out = new Text();\r\n    while (reader.readLine(out) > 0) {\r\n        output += out;\r\n        output += \"\\n\";\r\n    }\r\n    reader.close();\r\n    assertEquals(expectedOutput.toString(), output);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "main",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void main(String[] args) throws Exception\n{\r\n    new TestTextOutputFormat().testFormat();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "tearDown",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void tearDown() throws Exception\n{\r\n    super.tearDown();\r\n    FileUtil.fullyDelete(new File(rootDir.toString()));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "testCommitFail",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void testCommitFail() throws IOException\n{\r\n    final Path inDir = new Path(rootDir, \"./input\");\r\n    final Path outDir = new Path(rootDir, \"./output\");\r\n    JobConf jobConf = createJobConf();\r\n    jobConf.setMaxMapAttempts(1);\r\n    jobConf.setOutputCommitter(CommitterWithCommitFail.class);\r\n    RunningJob rJob = UtilsForTests.runJob(jobConf, inDir, outDir, 1, 0);\r\n    rJob.waitForCompletion();\r\n    assertEquals(JobStatus.FAILED, rJob.getJobState());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "testTaskCleanupDoesNotCommit",
  "errType" : null,
  "containingMethodsNum" : 16,
  "sourceCodeText" : "void testTaskCleanupDoesNotCommit() throws Exception\n{\r\n    JobConf job = new JobConf();\r\n    job.setOutputCommitter(CommitterWithoutCleanup.class);\r\n    Path outDir = new Path(rootDir, \"output\");\r\n    FileOutputFormat.setOutputPath(job, outDir);\r\n    String dummyAttemptID = \"attempt_200707121733_0001_m_000000_0\";\r\n    TaskAttemptID attemptID = TaskAttemptID.forName(dummyAttemptID);\r\n    OutputCommitter committer = new CommitterWithoutCleanup();\r\n    JobContext jContext = new JobContextImpl(job, attemptID.getJobID());\r\n    committer.setupJob(jContext);\r\n    dummyAttemptID = \"attempt_200707121733_0001_m_000001_0\";\r\n    attemptID = TaskAttemptID.forName(dummyAttemptID);\r\n    Task task = new MapTask(null, attemptID, 0, null, 1);\r\n    task.setConf(job);\r\n    task.localizeConfiguration(job);\r\n    task.initialize(job, attemptID.getJobID(), Reporter.NULL, false);\r\n    String file = \"test.txt\";\r\n    FileSystem localFs = FileSystem.getLocal(job);\r\n    TextOutputFormat<Text, Text> theOutputFormat = new TextOutputFormat<Text, Text>();\r\n    RecordWriter<Text, Text> theRecordWriter = theOutputFormat.getRecordWriter(localFs, job, file, Reporter.NULL);\r\n    theRecordWriter.write(new Text(\"key\"), new Text(\"value\"));\r\n    theRecordWriter.close(Reporter.NULL);\r\n    task.setTaskCleanupTask();\r\n    MyUmbilical umbilical = new MyUmbilical();\r\n    task.run(job, umbilical);\r\n    assertTrue(\"Task did not succeed\", umbilical.taskDone);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "testCommitRequiredForMapTask",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void testCommitRequiredForMapTask() throws Exception\n{\r\n    Task testTask = createDummyTask(TaskType.MAP);\r\n    assertTrue(\"MapTask should need commit\", testTask.isCommitRequired());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "testCommitRequiredForReduceTask",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void testCommitRequiredForReduceTask() throws Exception\n{\r\n    Task testTask = createDummyTask(TaskType.REDUCE);\r\n    assertTrue(\"ReduceTask should need commit\", testTask.isCommitRequired());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "testCommitNotRequiredForJobSetup",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void testCommitNotRequiredForJobSetup() throws Exception\n{\r\n    Task testTask = createDummyTask(TaskType.MAP);\r\n    testTask.setJobSetupTask();\r\n    assertFalse(\"Job setup task should not need commit\", testTask.isCommitRequired());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "testCommitNotRequiredForJobCleanup",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void testCommitNotRequiredForJobCleanup() throws Exception\n{\r\n    Task testTask = createDummyTask(TaskType.MAP);\r\n    testTask.setJobCleanupTask();\r\n    assertFalse(\"Job cleanup task should not need commit\", testTask.isCommitRequired());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "testCommitNotRequiredForTaskCleanup",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void testCommitNotRequiredForTaskCleanup() throws Exception\n{\r\n    Task testTask = createDummyTask(TaskType.REDUCE);\r\n    testTask.setTaskCleanupTask();\r\n    assertFalse(\"Task cleanup task should not need commit\", testTask.isCommitRequired());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "createDummyTask",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "Task createDummyTask(TaskType type) throws IOException, ClassNotFoundException, InterruptedException\n{\r\n    JobConf conf = new JobConf();\r\n    conf.setOutputCommitter(CommitterThatAlwaysRequiresCommit.class);\r\n    Path outDir = new Path(rootDir, \"output\");\r\n    FileOutputFormat.setOutputPath(conf, outDir);\r\n    JobID jobId = JobID.forName(\"job_201002121132_0001\");\r\n    Task testTask;\r\n    if (type == TaskType.MAP) {\r\n        testTask = new MapTask();\r\n    } else {\r\n        testTask = new ReduceTask();\r\n    }\r\n    testTask.setConf(conf);\r\n    testTask.initialize(conf, jobId, Reporter.NULL, false);\r\n    return testTask;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "main",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void main(String[] argv) throws Exception\n{\r\n    TestTaskCommit td = new TestTaskCommit();\r\n    td.testCommitFail();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2",
  "methodName" : "getTestParameters",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Collection<Object[]> getTestParameters()\n{\r\n    return Arrays.asList(new Object[][] { { SimpleExponentialTaskRuntimeEstimator.class }, { LegacyTaskRuntimeEstimator.class } });\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2",
  "methodName" : "setup",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setup()\n{\r\n    this.controlledClk.setTime(System.currentTimeMillis());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2",
  "methodName" : "testSpeculateSuccessfulWithoutUpdateEvents",
  "errType" : null,
  "containingMethodsNum" : 27,
  "sourceCodeText" : "void testSpeculateSuccessfulWithoutUpdateEvents() throws Exception\n{\r\n    MRApp app = new MRApp(NUM_MAPPERS, NUM_REDUCERS, false, \"test\", true, controlledClk);\r\n    Job job = app.submit(createConfiguration(), true, true);\r\n    app.waitForState(job, JobState.RUNNING);\r\n    Map<TaskId, Task> tasks = job.getTasks();\r\n    Assert.assertEquals(\"Num tasks is not correct\", NUM_MAPPERS + NUM_REDUCERS, tasks.size());\r\n    Iterator<Task> taskIter = tasks.values().iterator();\r\n    while (taskIter.hasNext()) {\r\n        app.waitForState(taskIter.next(), TaskState.RUNNING);\r\n    }\r\n    controlledClk.tickMsec(1000L);\r\n    EventHandler appEventHandler = app.getContext().getEventHandler();\r\n    for (Map.Entry<TaskId, Task> mapTask : tasks.entrySet()) {\r\n        for (Map.Entry<TaskAttemptId, TaskAttempt> taskAttempt : mapTask.getValue().getAttempts().entrySet()) {\r\n            updateTaskProgress(appEventHandler, taskAttempt.getValue(), 0.8f);\r\n        }\r\n    }\r\n    Random generator = new Random();\r\n    Object[] taskValues = tasks.values().toArray();\r\n    final Task taskToBeSpeculated = (Task) taskValues[generator.nextInt(taskValues.length)];\r\n    for (Map.Entry<TaskId, Task> mapTask : tasks.entrySet()) {\r\n        if (mapTask.getKey() != taskToBeSpeculated.getID()) {\r\n            for (Map.Entry<TaskAttemptId, TaskAttempt> taskAttempt : mapTask.getValue().getAttempts().entrySet()) {\r\n                TaskAttemptId taId = taskAttempt.getKey();\r\n                if (taId.getId() > 0) {\r\n                    continue;\r\n                }\r\n                markTACompleted(appEventHandler, taskAttempt.getValue());\r\n                waitForTAState(taskAttempt.getValue(), TaskAttemptState.SUCCEEDED, controlledClk);\r\n            }\r\n        }\r\n    }\r\n    controlledClk.tickMsec(2000L);\r\n    waitForSpeculation(taskToBeSpeculated, controlledClk);\r\n    TaskAttempt[] ta = makeFirstAttemptWin(appEventHandler, taskToBeSpeculated);\r\n    waitForTAState(ta[0], TaskAttemptState.SUCCEEDED, controlledClk);\r\n    waitForAppStop(app, controlledClk);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2",
  "methodName" : "testSpeculateSuccessfulWithUpdateEvents",
  "errType" : null,
  "containingMethodsNum" : 31,
  "sourceCodeText" : "void testSpeculateSuccessfulWithUpdateEvents() throws Exception\n{\r\n    MRApp app = new MRApp(NUM_MAPPERS, NUM_REDUCERS, false, \"test\", true, controlledClk);\r\n    Job job = app.submit(createConfiguration(), true, true);\r\n    app.waitForState(job, JobState.RUNNING);\r\n    Map<TaskId, Task> tasks = job.getTasks();\r\n    Assert.assertEquals(\"Num tasks is not correct\", NUM_MAPPERS + NUM_REDUCERS, tasks.size());\r\n    Iterator<Task> taskIter = tasks.values().iterator();\r\n    while (taskIter.hasNext()) {\r\n        app.waitForState(taskIter.next(), TaskState.RUNNING);\r\n    }\r\n    controlledClk.tickMsec(2000L);\r\n    EventHandler appEventHandler = app.getContext().getEventHandler();\r\n    for (Map.Entry<TaskId, Task> mapTask : tasks.entrySet()) {\r\n        for (Map.Entry<TaskAttemptId, TaskAttempt> taskAttempt : mapTask.getValue().getAttempts().entrySet()) {\r\n            updateTaskProgress(appEventHandler, taskAttempt.getValue(), 0.5f);\r\n        }\r\n    }\r\n    Task speculatedTask = null;\r\n    int numTasksToFinish = NUM_MAPPERS + NUM_REDUCERS - 1;\r\n    controlledClk.tickMsec(1000L);\r\n    for (Map.Entry<TaskId, Task> task : tasks.entrySet()) {\r\n        for (Map.Entry<TaskAttemptId, TaskAttempt> taskAttempt : task.getValue().getAttempts().entrySet()) {\r\n            TaskAttemptId taId = taskAttempt.getKey();\r\n            if (numTasksToFinish > 0 && taId.getId() == 0) {\r\n                markTACompleted(appEventHandler, taskAttempt.getValue());\r\n                numTasksToFinish--;\r\n                waitForTAState(taskAttempt.getValue(), TaskAttemptState.SUCCEEDED, controlledClk);\r\n            } else {\r\n                speculatedTask = task.getValue();\r\n                updateTaskProgress(appEventHandler, taskAttempt.getValue(), 0.75f);\r\n            }\r\n        }\r\n    }\r\n    controlledClk.tickMsec(15000L);\r\n    for (Map.Entry<TaskId, Task> task : tasks.entrySet()) {\r\n        for (Map.Entry<TaskAttemptId, TaskAttempt> taskAttempt : task.getValue().getAttempts().entrySet()) {\r\n            if (!(taskAttempt.getValue().getState() == TaskAttemptState.SUCCEEDED || taskAttempt.getValue().getState() == TaskAttemptState.KILLED)) {\r\n                updateTaskProgress(appEventHandler, taskAttempt.getValue(), 0.75f);\r\n            }\r\n        }\r\n    }\r\n    final Task speculatedTaskConst = speculatedTask;\r\n    waitForSpeculation(speculatedTaskConst, controlledClk);\r\n    TaskAttempt[] ta = makeFirstAttemptWin(appEventHandler, speculatedTask);\r\n    waitForTAState(ta[0], TaskAttemptState.SUCCEEDED, controlledClk);\r\n    waitForAppStop(app, controlledClk);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2",
  "methodName" : "makeFirstAttemptWin",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "TaskAttempt[] makeFirstAttemptWin(EventHandler appEventHandler, Task speculatedTask)\n{\r\n    Collection<TaskAttempt> attempts = speculatedTask.getAttempts().values();\r\n    TaskAttempt[] ta = new TaskAttempt[attempts.size()];\r\n    attempts.toArray(ta);\r\n    markTACompleted(appEventHandler, ta[0]);\r\n    return ta;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2",
  "methodName" : "markTACompleted",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void markTACompleted(EventHandler appEventHandler, TaskAttempt attempt)\n{\r\n    appEventHandler.handle(new TaskAttemptEvent(attempt.getID(), TaskAttemptEventType.TA_DONE));\r\n    appEventHandler.handle(new TaskAttemptEvent(attempt.getID(), TaskAttemptEventType.TA_CONTAINER_COMPLETED));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2",
  "methodName" : "createTaskAttemptStatus",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "TaskAttemptStatus createTaskAttemptStatus(TaskAttemptId id, float progress, TaskAttemptState state)\n{\r\n    TaskAttemptStatus status = new TaskAttemptStatus();\r\n    status.id = id;\r\n    status.progress = progress;\r\n    status.taskState = state;\r\n    return status;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2",
  "methodName" : "createConfiguration",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "Configuration createConfiguration()\n{\r\n    Configuration conf = new Configuration();\r\n    conf.setClass(MRJobConfig.MR_AM_TASK_ESTIMATOR, estimatorClass, TaskRuntimeEstimator.class);\r\n    if (SimpleExponentialTaskRuntimeEstimator.class.equals(estimatorClass)) {\r\n        conf.setInt(MRJobConfig.MR_AM_TASK_ESTIMATOR_SIMPLE_SMOOTH_SKIP_INITIALS, 1);\r\n        conf.setLong(MRJobConfig.MR_AM_TASK_ESTIMATOR_SIMPLE_SMOOTH_LAMBDA_MS, 1000L * 10);\r\n    }\r\n    conf.setLong(MRJobConfig.SPECULATIVE_RETRY_AFTER_NO_SPECULATE, 3000L);\r\n    return conf;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2",
  "methodName" : "waitForAppStop",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void waitForAppStop(final MRApp app, final ControlledClock cClock) throws TimeoutException, InterruptedException\n{\r\n    GenericTestUtils.waitFor(() -> {\r\n        if (app.getServiceState() != Service.STATE.STOPPED) {\r\n            cClock.tickMsec(250L);\r\n            return false;\r\n        }\r\n        return true;\r\n    }, 250, 60000);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2",
  "methodName" : "waitForSpeculation",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void waitForSpeculation(final Task speculatedTask, final ControlledClock cClock) throws TimeoutException, InterruptedException\n{\r\n    GenericTestUtils.waitFor(() -> {\r\n        if (speculatedTask.getAttempts().size() != 2) {\r\n            cClock.tickMsec(250L);\r\n            return false;\r\n        }\r\n        return true;\r\n    }, 250, 60000);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2",
  "methodName" : "waitForTAState",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void waitForTAState(TaskAttempt attempt, TaskAttemptState finalState, final ControlledClock cClock) throws Exception\n{\r\n    GenericTestUtils.waitFor(() -> {\r\n        if (attempt.getReport().getTaskAttemptState() != finalState) {\r\n            cClock.tickMsec(250L);\r\n            return false;\r\n        }\r\n        return true;\r\n    }, 250, 10000);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2",
  "methodName" : "updateTaskProgress",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void updateTaskProgress(EventHandler appEventHandler, TaskAttempt attempt, float newProgress)\n{\r\n    TaskAttemptStatus status = createTaskAttemptStatus(attempt.getID(), newProgress, TaskAttemptState.RUNNING);\r\n    TaskAttemptStatusUpdateEvent event = new TaskAttemptStatusUpdateEvent(attempt.getID(), new AtomicReference<>(status));\r\n    appEventHandler.handle(event);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\security\\ssl",
  "methodName" : "setUp",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setUp() throws Exception\n{\r\n    testRootDir = GenericTestUtils.setupTestRootDir(TestEncryptedShuffle.class);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\security\\ssl",
  "methodName" : "createCustomYarnClasspath",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void createCustomYarnClasspath() throws Exception\n{\r\n    classpathDir = KeyStoreTestUtil.getClasspathDir(TestEncryptedShuffle.class);\r\n    new File(classpathDir, \"core-site.xml\").delete();\r\n    dfsFolder = new File(testRootDir, String.format(\"dfs-%d\", Time.monotonicNow()));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\security\\ssl",
  "methodName" : "cleanUpMiniClusterSpecialConfig",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void cleanUpMiniClusterSpecialConfig() throws Exception\n{\r\n    new File(classpathDir, \"core-site.xml\").delete();\r\n    String keystoresDir = testRootDir.getAbsolutePath();\r\n    KeyStoreTestUtil.cleanupSSLConfig(keystoresDir, classpathDir);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\security\\ssl",
  "methodName" : "startCluster",
  "errType" : null,
  "containingMethodsNum" : 19,
  "sourceCodeText" : "void startCluster(Configuration conf) throws Exception\n{\r\n    if (System.getProperty(\"hadoop.log.dir\") == null) {\r\n        System.setProperty(\"hadoop.log.dir\", testRootDir.getAbsolutePath());\r\n    }\r\n    conf.set(\"dfs.block.access.token.enable\", \"false\");\r\n    conf.set(\"dfs.permissions\", \"true\");\r\n    conf.set(\"hadoop.security.authentication\", \"simple\");\r\n    String cp = conf.get(YarnConfiguration.YARN_APPLICATION_CLASSPATH, StringUtils.join(\",\", YarnConfiguration.DEFAULT_YARN_CROSS_PLATFORM_APPLICATION_CLASSPATH)) + File.pathSeparator + classpathDir;\r\n    conf.set(YarnConfiguration.YARN_APPLICATION_CLASSPATH, cp);\r\n    dfsCluster = new MiniDFSCluster.Builder(conf, dfsFolder).build();\r\n    FileSystem fileSystem = dfsCluster.getFileSystem();\r\n    fileSystem.mkdirs(new Path(\"/tmp\"));\r\n    fileSystem.mkdirs(new Path(\"/user\"));\r\n    fileSystem.mkdirs(new Path(\"/hadoop/mapred/system\"));\r\n    fileSystem.setPermission(new Path(\"/tmp\"), FsPermission.valueOf(\"-rwxrwxrwx\"));\r\n    fileSystem.setPermission(new Path(\"/user\"), FsPermission.valueOf(\"-rwxrwxrwx\"));\r\n    fileSystem.setPermission(new Path(\"/hadoop/mapred/system\"), FsPermission.valueOf(\"-rwx------\"));\r\n    FileSystem.setDefaultUri(conf, fileSystem.getUri());\r\n    mrCluster = MiniMRClientClusterFactory.create(this.getClass(), 1, conf);\r\n    Writer writer = new FileWriter(classpathDir + \"/core-site.xml\");\r\n    mrCluster.getConfig().writeXml(writer);\r\n    writer.close();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\security\\ssl",
  "methodName" : "stopCluster",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void stopCluster() throws Exception\n{\r\n    if (mrCluster != null) {\r\n        mrCluster.stop();\r\n    }\r\n    if (dfsCluster != null) {\r\n        dfsCluster.shutdown();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\security\\ssl",
  "methodName" : "getJobConf",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "JobConf getJobConf() throws IOException\n{\r\n    return new JobConf(mrCluster.getConfig());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\security\\ssl",
  "methodName" : "encryptedShuffleWithCerts",
  "errType" : null,
  "containingMethodsNum" : 21,
  "sourceCodeText" : "void encryptedShuffleWithCerts(boolean useClientCerts) throws Exception\n{\r\n    try {\r\n        Configuration conf = new Configuration();\r\n        String keystoresDir = testRootDir.getAbsolutePath();\r\n        String sslConfsDir = KeyStoreTestUtil.getClasspathDir(TestEncryptedShuffle.class);\r\n        KeyStoreTestUtil.setupSSLConfig(keystoresDir, sslConfsDir, conf, useClientCerts);\r\n        conf.setBoolean(MRConfig.SHUFFLE_SSL_ENABLED_KEY, true);\r\n        startCluster(conf);\r\n        FileSystem fs = FileSystem.get(getJobConf());\r\n        Path inputDir = new Path(\"input\");\r\n        fs.mkdirs(inputDir);\r\n        Writer writer = new OutputStreamWriter(fs.create(new Path(inputDir, \"data.txt\")));\r\n        writer.write(\"hello\");\r\n        writer.close();\r\n        Path outputDir = new Path(\"output\", \"output\");\r\n        JobConf jobConf = new JobConf(getJobConf());\r\n        jobConf.setInt(\"mapred.map.tasks\", 1);\r\n        jobConf.setInt(\"mapred.map.max.attempts\", 1);\r\n        jobConf.setInt(\"mapred.reduce.max.attempts\", 1);\r\n        jobConf.set(\"mapred.input.dir\", inputDir.toString());\r\n        jobConf.set(\"mapred.output.dir\", outputDir.toString());\r\n        JobClient jobClient = new JobClient(jobConf);\r\n        RunningJob runJob = jobClient.submitJob(jobConf);\r\n        runJob.waitForCompletion();\r\n        Assert.assertTrue(runJob.isComplete());\r\n        Assert.assertTrue(runJob.isSuccessful());\r\n    } finally {\r\n        stopCluster();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\security\\ssl",
  "methodName" : "encryptedShuffleWithClientCerts",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void encryptedShuffleWithClientCerts() throws Exception\n{\r\n    encryptedShuffleWithCerts(true);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\security\\ssl",
  "methodName" : "encryptedShuffleWithoutClientCerts",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void encryptedShuffleWithoutClientCerts() throws Exception\n{\r\n    encryptedShuffleWithCerts(false);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "testCounterValue",
  "errType" : null,
  "containingMethodsNum" : 10,
  "sourceCodeText" : "void testCounterValue()\n{\r\n    final int NUMBER_TESTS = 100;\r\n    final int NUMBER_INC = 10;\r\n    final Random rand = new Random();\r\n    for (int i = 0; i < NUMBER_TESTS; i++) {\r\n        long initValue = rand.nextInt();\r\n        long expectedValue = initValue;\r\n        Counter counter = new Counters().findCounter(\"test\", \"foo\");\r\n        counter.setValue(initValue);\r\n        assertEquals(\"Counter value is not initialized correctly\", expectedValue, counter.getValue());\r\n        for (int j = 0; j < NUMBER_INC; j++) {\r\n            int incValue = rand.nextInt();\r\n            counter.increment(incValue);\r\n            expectedValue += incValue;\r\n            assertEquals(\"Counter value is not incremented correctly\", expectedValue, counter.getValue());\r\n        }\r\n        expectedValue = rand.nextInt();\r\n        counter.setValue(expectedValue);\r\n        assertEquals(\"Counter value is not set correctly\", expectedValue, counter.getValue());\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "testLimits",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void testLimits()\n{\r\n    for (int i = 0; i < 3; ++i) {\r\n        testMaxCounters(new Counters());\r\n        testMaxGroups(new Counters());\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "testCountersIncrement",
  "errType" : null,
  "containingMethodsNum" : 11,
  "sourceCodeText" : "void testCountersIncrement()\n{\r\n    Counters fCounters = new Counters();\r\n    Counter fCounter = fCounters.findCounter(FRAMEWORK_COUNTER);\r\n    fCounter.setValue(100);\r\n    Counter gCounter = fCounters.findCounter(\"test\", \"foo\");\r\n    gCounter.setValue(200);\r\n    Counters counters = new Counters();\r\n    counters.incrAllCounters(fCounters);\r\n    Counter counter;\r\n    for (CounterGroup cg : fCounters) {\r\n        CounterGroup group = counters.getGroup(cg.getName());\r\n        if (group.getName().equals(\"test\")) {\r\n            counter = counters.findCounter(\"test\", \"foo\");\r\n            assertEquals(200, counter.getValue());\r\n        } else {\r\n            counter = counters.findCounter(FRAMEWORK_COUNTER);\r\n            assertEquals(100, counter.getValue());\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "testMaxCounters",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void testMaxCounters(final Counters counters)\n{\r\n    LOG.info(\"counters max=\" + Limits.getCountersMax());\r\n    for (int i = 0; i < Limits.getCountersMax(); ++i) {\r\n        counters.findCounter(\"test\", \"test\" + i);\r\n    }\r\n    setExpected(counters);\r\n    shouldThrow(LimitExceededException.class, new Runnable() {\r\n\r\n        public void run() {\r\n            counters.findCounter(\"test\", \"bad\");\r\n        }\r\n    });\r\n    checkExpected(counters);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "testMaxGroups",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void testMaxGroups(final Counters counters)\n{\r\n    LOG.info(\"counter groups max=\" + Limits.getGroupsMax());\r\n    for (int i = 0; i < Limits.getGroupsMax(); ++i) {\r\n        counters.findCounter(\"test\" + i, \"test\");\r\n    }\r\n    setExpected(counters);\r\n    shouldThrow(LimitExceededException.class, new Runnable() {\r\n\r\n        public void run() {\r\n            counters.findCounter(\"bad\", \"test\");\r\n        }\r\n    });\r\n    checkExpected(counters);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "setExpected",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void setExpected(Counters counters)\n{\r\n    counters.findCounter(FRAMEWORK_COUNTER).setValue(FRAMEWORK_COUNTER_VALUE);\r\n    counters.findCounter(FS_SCHEME, FS_COUNTER).setValue(FS_COUNTER_VALUE);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "checkExpected",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void checkExpected(Counters counters)\n{\r\n    assertEquals(FRAMEWORK_COUNTER_VALUE, counters.findCounter(FRAMEWORK_COUNTER).getValue());\r\n    assertEquals(FS_COUNTER_VALUE, counters.findCounter(FS_SCHEME, FS_COUNTER).getValue());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "shouldThrow",
  "errType" : [ "Exception" ],
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void shouldThrow(Class<? extends Exception> ecls, Runnable runnable)\n{\r\n    try {\r\n        runnable.run();\r\n    } catch (Exception e) {\r\n        assertSame(ecls, e.getClass());\r\n        LOG.info(\"got expected: \" + e);\r\n        return;\r\n    }\r\n    assertTrue(\"Should've thrown \" + ecls.getSimpleName(), false);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\fs\\slive",
  "methodName" : "getOperation",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "Operation getOperation(OperationType type)\n{\r\n    Operation op = typedOperations.get(type);\r\n    if (op != null) {\r\n        return op;\r\n    }\r\n    switch(type) {\r\n        case READ:\r\n            op = new ReadOp(this.config, rnd);\r\n            break;\r\n        case LS:\r\n            op = new ListOp(this.config, rnd);\r\n            break;\r\n        case MKDIR:\r\n            op = new MkdirOp(this.config, rnd);\r\n            break;\r\n        case APPEND:\r\n            op = new AppendOp(this.config, rnd);\r\n            break;\r\n        case RENAME:\r\n            op = new RenameOp(this.config, rnd);\r\n            break;\r\n        case DELETE:\r\n            op = new DeleteOp(this.config, rnd);\r\n            break;\r\n        case CREATE:\r\n            op = new CreateOp(this.config, rnd);\r\n            break;\r\n        case TRUNCATE:\r\n            op = new TruncateOp(this.config, rnd);\r\n            break;\r\n    }\r\n    typedOperations.put(type, op);\r\n    return op;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "isLocalMR",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean isLocalMR()\n{\r\n    return localMR;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "isLocalFS",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean isLocalFS()\n{\r\n    return localFS;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "setUp",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void setUp() throws Exception\n{\r\n    if (localFS) {\r\n        fileSystem = FileSystem.getLocal(new JobConf());\r\n    } else {\r\n        dfsCluster = new MiniDFSCluster.Builder(new JobConf()).numDataNodes(dataNodes).build();\r\n        fileSystem = dfsCluster.getFileSystem();\r\n    }\r\n    if (localMR) {\r\n    } else {\r\n        mrCluster = new MiniMRCluster(taskTrackers, fileSystem.getUri().toString(), 1);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "tearDown",
  "errType" : [ "Exception", "Exception" ],
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void tearDown() throws Exception\n{\r\n    try {\r\n        if (mrCluster != null) {\r\n            mrCluster.shutdown();\r\n        }\r\n    } catch (Exception ex) {\r\n        System.out.println(ex);\r\n    }\r\n    try {\r\n        if (dfsCluster != null) {\r\n            dfsCluster.shutdown();\r\n        }\r\n    } catch (Exception ex) {\r\n        System.out.println(ex);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 2,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getFileSystem",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "FileSystem getFileSystem()\n{\r\n    return fileSystem;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "createJobConf",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "JobConf createJobConf()\n{\r\n    if (localMR) {\r\n        JobConf conf = new JobConf();\r\n        conf.set(MRConfig.FRAMEWORK_NAME, MRConfig.LOCAL_FRAMEWORK_NAME);\r\n        return conf;\r\n    } else {\r\n        return mrCluster.createJobConf();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "testFormat",
  "errType" : null,
  "containingMethodsNum" : 37,
  "sourceCodeText" : "void testFormat() throws Exception\n{\r\n    JobConf job = new JobConf();\r\n    Path file = new Path(workDir, \"test.txt\");\r\n    Reporter reporter = Reporter.NULL;\r\n    int seed = new Random().nextInt();\r\n    LOG.info(\"seed = \" + seed);\r\n    Random random = new Random(seed);\r\n    localFs.delete(workDir, true);\r\n    FileInputFormat.setInputPaths(job, workDir);\r\n    for (int length = 0; length < MAX_LENGTH; length += random.nextInt(MAX_LENGTH / 10) + 1) {\r\n        LOG.debug(\"creating; entries = \" + length);\r\n        Writer writer = new OutputStreamWriter(localFs.create(file));\r\n        try {\r\n            for (int i = 0; i < length; i++) {\r\n                writer.write(Integer.toString(i * 2));\r\n                writer.write(\"\\t\");\r\n                writer.write(Integer.toString(i));\r\n                writer.write(\"\\n\");\r\n            }\r\n        } finally {\r\n            writer.close();\r\n        }\r\n        KeyValueTextInputFormat format = new KeyValueTextInputFormat();\r\n        format.configure(job);\r\n        for (int i = 0; i < 3; i++) {\r\n            int numSplits = random.nextInt(MAX_LENGTH / 20) + 1;\r\n            LOG.debug(\"splitting: requesting = \" + numSplits);\r\n            InputSplit[] splits = format.getSplits(job, numSplits);\r\n            LOG.debug(\"splitting: got =        \" + splits.length);\r\n            BitSet bits = new BitSet(length);\r\n            for (int j = 0; j < splits.length; j++) {\r\n                LOG.debug(\"split[\" + j + \"]= \" + splits[j]);\r\n                RecordReader<Text, Text> reader = format.getRecordReader(splits[j], job, reporter);\r\n                Class readerClass = reader.getClass();\r\n                assertEquals(\"reader class is KeyValueLineRecordReader.\", KeyValueLineRecordReader.class, readerClass);\r\n                Text key = reader.createKey();\r\n                Class keyClass = key.getClass();\r\n                Text value = reader.createValue();\r\n                Class valueClass = value.getClass();\r\n                assertEquals(\"Key class is Text.\", Text.class, keyClass);\r\n                assertEquals(\"Value class is Text.\", Text.class, valueClass);\r\n                try {\r\n                    int count = 0;\r\n                    while (reader.next(key, value)) {\r\n                        int v = Integer.parseInt(value.toString());\r\n                        LOG.debug(\"read \" + v);\r\n                        if (bits.get(v)) {\r\n                            LOG.warn(\"conflict with \" + v + \" in split \" + j + \" at position \" + reader.getPos());\r\n                        }\r\n                        assertFalse(\"Key in multiple partitions.\", bits.get(v));\r\n                        bits.set(v);\r\n                        count++;\r\n                    }\r\n                    LOG.debug(\"splits[\" + j + \"]=\" + splits[j] + \" count=\" + count);\r\n                } finally {\r\n                    reader.close();\r\n                }\r\n            }\r\n            assertEquals(\"Some keys in no partition.\", length, bits.cardinality());\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 2,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "makeStream",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "LineReader makeStream(String str) throws IOException\n{\r\n    return new LineReader(new ByteArrayInputStream(str.getBytes(\"UTF-8\")), defaultConf);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "testUTF8",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void testUTF8() throws Exception\n{\r\n    LineReader in = null;\r\n    try {\r\n        in = makeStream(\"abcd\\u20acbdcd\\u20ac\");\r\n        Text line = new Text();\r\n        in.readLine(line);\r\n        assertEquals(\"readLine changed utf8 characters\", \"abcd\\u20acbdcd\\u20ac\", line.toString());\r\n        in = makeStream(\"abc\\u200axyz\");\r\n        in.readLine(line);\r\n        assertEquals(\"split on fake newline\", \"abc\\u200axyz\", line.toString());\r\n    } finally {\r\n        if (in != null) {\r\n            in.close();\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "testNewLines",
  "errType" : null,
  "containingMethodsNum" : 15,
  "sourceCodeText" : "void testNewLines() throws Exception\n{\r\n    LineReader in = null;\r\n    try {\r\n        in = makeStream(\"a\\nbb\\n\\nccc\\rdddd\\r\\neeeee\");\r\n        Text out = new Text();\r\n        in.readLine(out);\r\n        assertEquals(\"line1 length\", 1, out.getLength());\r\n        in.readLine(out);\r\n        assertEquals(\"line2 length\", 2, out.getLength());\r\n        in.readLine(out);\r\n        assertEquals(\"line3 length\", 0, out.getLength());\r\n        in.readLine(out);\r\n        assertEquals(\"line4 length\", 3, out.getLength());\r\n        in.readLine(out);\r\n        assertEquals(\"line5 length\", 4, out.getLength());\r\n        in.readLine(out);\r\n        assertEquals(\"line5 length\", 5, out.getLength());\r\n        assertEquals(\"end of file\", 0, in.readLine(out));\r\n    } finally {\r\n        if (in != null) {\r\n            in.close();\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "writeFile",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void writeFile(FileSystem fs, Path name, CompressionCodec codec, String contents) throws IOException\n{\r\n    OutputStream stm;\r\n    if (codec == null) {\r\n        stm = fs.create(name);\r\n    } else {\r\n        stm = codec.createOutputStream(fs.create(name));\r\n    }\r\n    stm.write(contents.getBytes());\r\n    stm.close();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "readSplit",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "List<Text> readSplit(KeyValueTextInputFormat format, InputSplit split, JobConf job) throws IOException\n{\r\n    List<Text> result = new ArrayList<Text>();\r\n    RecordReader<Text, Text> reader = null;\r\n    try {\r\n        reader = format.getRecordReader(split, job, voidReporter);\r\n        Text key = reader.createKey();\r\n        Text value = reader.createValue();\r\n        while (reader.next(key, value)) {\r\n            result.add(value);\r\n            value = (Text) reader.createValue();\r\n        }\r\n    } finally {\r\n        if (reader != null) {\r\n            reader.close();\r\n        }\r\n    }\r\n    return result;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "testGzip",
  "errType" : null,
  "containingMethodsNum" : 16,
  "sourceCodeText" : "void testGzip() throws IOException\n{\r\n    JobConf job = new JobConf();\r\n    CompressionCodec gzip = new GzipCodec();\r\n    ReflectionUtils.setConf(gzip, job);\r\n    localFs.delete(workDir, true);\r\n    writeFile(localFs, new Path(workDir, \"part1.txt.gz\"), gzip, \"line-1\\tthe quick\\nline-2\\tbrown\\nline-3\\tfox jumped\\nline-4\\tover\\nline-5\\t the lazy\\nline-6\\t dog\\n\");\r\n    writeFile(localFs, new Path(workDir, \"part2.txt.gz\"), gzip, \"line-1\\tthis is a test\\nline-1\\tof gzip\\n\");\r\n    FileInputFormat.setInputPaths(job, workDir);\r\n    KeyValueTextInputFormat format = new KeyValueTextInputFormat();\r\n    format.configure(job);\r\n    InputSplit[] splits = format.getSplits(job, 100);\r\n    assertEquals(\"compressed splits == 2\", 2, splits.length);\r\n    FileSplit tmp = (FileSplit) splits[0];\r\n    if (tmp.getPath().getName().equals(\"part2.txt.gz\")) {\r\n        splits[0] = splits[1];\r\n        splits[1] = tmp;\r\n    }\r\n    List<Text> results = readSplit(format, splits[0], job);\r\n    assertEquals(\"splits[0] length\", 6, results.size());\r\n    assertEquals(\"splits[0][5]\", \" dog\", results.get(5).toString());\r\n    results = readSplit(format, splits[1], job);\r\n    assertEquals(\"splits[1] length\", 2, results.size());\r\n    assertEquals(\"splits[1][0]\", \"this is a test\", results.get(0).toString());\r\n    assertEquals(\"splits[1][1]\", \"of gzip\", results.get(1).toString());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "main",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void main(String[] args) throws Exception\n{\r\n    new TestKeyValueTextInputFormat().testFormat();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "testGetRootQueues",
  "errType" : [ "YarnException", "YarnException" ],
  "containingMethodsNum" : 12,
  "sourceCodeText" : "void testGetRootQueues() throws IOException, InterruptedException\n{\r\n    final ApplicationClientProtocol applicationsManager = Mockito.mock(ApplicationClientProtocol.class);\r\n    GetQueueInfoResponse response = Mockito.mock(GetQueueInfoResponse.class);\r\n    org.apache.hadoop.yarn.api.records.QueueInfo queueInfo = Mockito.mock(org.apache.hadoop.yarn.api.records.QueueInfo.class);\r\n    Mockito.when(response.getQueueInfo()).thenReturn(queueInfo);\r\n    try {\r\n        Mockito.when(applicationsManager.getQueueInfo(Mockito.any(GetQueueInfoRequest.class))).thenReturn(response);\r\n    } catch (YarnException e) {\r\n        throw new IOException(e);\r\n    }\r\n    ResourceMgrDelegate delegate = new ResourceMgrDelegate(new YarnConfiguration()) {\r\n\r\n        @Override\r\n        protected void serviceStart() throws Exception {\r\n            Assert.assertTrue(this.client instanceof YarnClientImpl);\r\n            ((YarnClientImpl) this.client).setRMClient(applicationsManager);\r\n        }\r\n    };\r\n    delegate.getRootQueues();\r\n    ArgumentCaptor<GetQueueInfoRequest> argument = ArgumentCaptor.forClass(GetQueueInfoRequest.class);\r\n    try {\r\n        Mockito.verify(applicationsManager).getQueueInfo(argument.capture());\r\n    } catch (YarnException e) {\r\n        throw new IOException(e);\r\n    }\r\n    Assert.assertTrue(\"Children of root queue not requested\", argument.getValue().getIncludeChildQueues());\r\n    Assert.assertTrue(\"Request wasn't to recurse through children\", argument.getValue().getRecursive());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 2,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "tesAllJobs",
  "errType" : null,
  "containingMethodsNum" : 15,
  "sourceCodeText" : "void tesAllJobs() throws Exception\n{\r\n    final ApplicationClientProtocol applicationsManager = Mockito.mock(ApplicationClientProtocol.class);\r\n    GetApplicationsResponse allApplicationsResponse = Records.newRecord(GetApplicationsResponse.class);\r\n    List<ApplicationReport> applications = new ArrayList<ApplicationReport>();\r\n    applications.add(getApplicationReport(YarnApplicationState.FINISHED, FinalApplicationStatus.FAILED));\r\n    applications.add(getApplicationReport(YarnApplicationState.FINISHED, FinalApplicationStatus.SUCCEEDED));\r\n    applications.add(getApplicationReport(YarnApplicationState.FINISHED, FinalApplicationStatus.KILLED));\r\n    applications.add(getApplicationReport(YarnApplicationState.FAILED, FinalApplicationStatus.FAILED));\r\n    allApplicationsResponse.setApplicationList(applications);\r\n    Mockito.when(applicationsManager.getApplications(Mockito.any(GetApplicationsRequest.class))).thenReturn(allApplicationsResponse);\r\n    ResourceMgrDelegate resourceMgrDelegate = new ResourceMgrDelegate(new YarnConfiguration()) {\r\n\r\n        @Override\r\n        protected void serviceStart() throws Exception {\r\n            Assert.assertTrue(this.client instanceof YarnClientImpl);\r\n            ((YarnClientImpl) this.client).setRMClient(applicationsManager);\r\n        }\r\n    };\r\n    JobStatus[] allJobs = resourceMgrDelegate.getAllJobs();\r\n    Assert.assertEquals(State.FAILED, allJobs[0].getState());\r\n    Assert.assertEquals(State.SUCCEEDED, allJobs[1].getState());\r\n    Assert.assertEquals(State.KILLED, allJobs[2].getState());\r\n    Assert.assertEquals(State.FAILED, allJobs[3].getState());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getApplicationReport",
  "errType" : null,
  "containingMethodsNum" : 9,
  "sourceCodeText" : "ApplicationReport getApplicationReport(YarnApplicationState yarnApplicationState, FinalApplicationStatus finalApplicationStatus)\n{\r\n    ApplicationReport appReport = Mockito.mock(ApplicationReport.class);\r\n    ApplicationResourceUsageReport appResources = Mockito.mock(ApplicationResourceUsageReport.class);\r\n    Mockito.when(appReport.getApplicationId()).thenReturn(ApplicationId.newInstance(0, 0));\r\n    Mockito.when(appResources.getNeededResources()).thenReturn(Records.newRecord(Resource.class));\r\n    Mockito.when(appResources.getReservedResources()).thenReturn(Records.newRecord(Resource.class));\r\n    Mockito.when(appResources.getUsedResources()).thenReturn(Records.newRecord(Resource.class));\r\n    Mockito.when(appReport.getApplicationResourceUsageReport()).thenReturn(appResources);\r\n    Mockito.when(appReport.getYarnApplicationState()).thenReturn(yarnApplicationState);\r\n    Mockito.when(appReport.getFinalApplicationStatus()).thenReturn(finalApplicationStatus);\r\n    return appReport;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred\\lib",
  "methodName" : "testSetReducerWithReducerByValueAsTrue",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void testSetReducerWithReducerByValueAsTrue() throws Exception\n{\r\n    JobConf jobConf = new JobConf();\r\n    JobConf reducerConf = new JobConf();\r\n    Chain.setReducer(jobConf, MyReducer.class, Object.class, Object.class, Object.class, Object.class, true, reducerConf);\r\n    boolean reduceByValue = reducerConf.getBoolean(\"chain.reducer.byValue\", false);\r\n    assertThat(reduceByValue).withFailMessage(\"It should set chain.reducer.byValue as true in \" + \"reducerConf when we give value as true\").isTrue();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred\\lib",
  "methodName" : "testSetReducerWithReducerByValueAsFalse",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void testSetReducerWithReducerByValueAsFalse() throws Exception\n{\r\n    JobConf jobConf = new JobConf();\r\n    JobConf reducerConf = new JobConf();\r\n    Chain.setReducer(jobConf, MyReducer.class, Object.class, Object.class, Object.class, Object.class, false, reducerConf);\r\n    boolean reduceByValue = reducerConf.getBoolean(\"chain.reducer.byValue\", true);\r\n    assertThat(reduceByValue).withFailMessage(\"It should set chain.reducer.byValue as false \" + \"in reducerConf when we give value as false\").isFalse();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "testCombiner",
  "errType" : null,
  "containingMethodsNum" : 42,
  "sourceCodeText" : "void testCombiner() throws Exception\n{\r\n    if (!new File(TEST_ROOT_DIR).mkdirs()) {\r\n        throw new RuntimeException(\"Could not create test dir: \" + TEST_ROOT_DIR);\r\n    }\r\n    File in = new File(TEST_ROOT_DIR, \"input\");\r\n    if (!in.mkdirs()) {\r\n        throw new RuntimeException(\"Could not create test dir: \" + in);\r\n    }\r\n    File out = new File(TEST_ROOT_DIR, \"output\");\r\n    PrintWriter pw = new PrintWriter(new FileWriter(new File(in, \"data.txt\")));\r\n    pw.println(\"A|a,1\");\r\n    pw.println(\"A|b,2\");\r\n    pw.println(\"B|a,3\");\r\n    pw.println(\"B|b,4\");\r\n    pw.println(\"B|c,5\");\r\n    pw.close();\r\n    JobConf job = new JobConf();\r\n    job.set(\"mapreduce.framework.name\", \"local\");\r\n    TextInputFormat.setInputPaths(job, new Path(in.getPath()));\r\n    TextOutputFormat.setOutputPath(job, new Path(out.getPath()));\r\n    job.setMapperClass(Map.class);\r\n    job.setReducerClass(Reduce.class);\r\n    job.setInputFormat(TextInputFormat.class);\r\n    job.setMapOutputKeyClass(Text.class);\r\n    job.setMapOutputValueClass(LongWritable.class);\r\n    job.setOutputFormat(TextOutputFormat.class);\r\n    job.setOutputValueGroupingComparator(GroupComparator.class);\r\n    job.setCombinerClass(Combiner.class);\r\n    job.setCombinerKeyGroupingComparator(GroupComparator.class);\r\n    job.setInt(\"min.num.spills.for.combine\", 0);\r\n    JobClient client = new JobClient(job);\r\n    RunningJob runningJob = client.submitJob(job);\r\n    runningJob.waitForCompletion();\r\n    if (runningJob.isSuccessful()) {\r\n        Counters counters = runningJob.getCounters();\r\n        long combinerInputRecords = counters.getGroup(\"org.apache.hadoop.mapreduce.TaskCounter\").getCounter(\"COMBINE_INPUT_RECORDS\");\r\n        long combinerOutputRecords = counters.getGroup(\"org.apache.hadoop.mapreduce.TaskCounter\").getCounter(\"COMBINE_OUTPUT_RECORDS\");\r\n        Assert.assertTrue(combinerInputRecords > 0);\r\n        Assert.assertTrue(combinerInputRecords > combinerOutputRecords);\r\n        BufferedReader br = new BufferedReader(new FileReader(new File(out, \"part-00000\")));\r\n        Set<String> output = new HashSet<String>();\r\n        String line = br.readLine();\r\n        Assert.assertNotNull(line);\r\n        output.add(line.substring(0, 1) + line.substring(4, 5));\r\n        line = br.readLine();\r\n        Assert.assertNotNull(line);\r\n        output.add(line.substring(0, 1) + line.substring(4, 5));\r\n        line = br.readLine();\r\n        Assert.assertNull(line);\r\n        br.close();\r\n        Set<String> expected = new HashSet<String>();\r\n        expected.add(\"A2\");\r\n        expected.add(\"B5\");\r\n        Assert.assertEquals(expected, output);\r\n    } else {\r\n        Assert.fail(\"Job failed\");\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred\\lib",
  "methodName" : "testEmptyKey",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void testEmptyKey() throws Exception\n{\r\n    KeyFieldBasedPartitioner<Text, Text> kfbp = new KeyFieldBasedPartitioner<Text, Text>();\r\n    JobConf conf = new JobConf();\r\n    conf.setInt(\"num.key.fields.for.partition\", 10);\r\n    kfbp.configure(conf);\r\n    assertEquals(\"Empty key should map to 0th partition\", 0, kfbp.getPartition(new Text(), new Text(), 10));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred\\lib",
  "methodName" : "testMultiConfigure",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void testMultiConfigure()\n{\r\n    KeyFieldBasedPartitioner<Text, Text> kfbp = new KeyFieldBasedPartitioner<Text, Text>();\r\n    JobConf conf = new JobConf();\r\n    conf.set(KeyFieldBasedPartitioner.PARTITIONER_OPTIONS, \"-k1,1\");\r\n    kfbp.setConf(conf);\r\n    Text key = new Text(\"foo\\tbar\");\r\n    Text val = new Text(\"val\");\r\n    int partNum = kfbp.getPartition(key, val, 4096);\r\n    kfbp.configure(conf);\r\n    assertEquals(partNum, kfbp.getPartition(key, val, 4096));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred\\lib",
  "methodName" : "testWithoutCounters",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void testWithoutCounters() throws Exception\n{\r\n    _testMultipleOutputs(false);\r\n    _testMOWithJavaSerialization(false);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred\\lib",
  "methodName" : "testWithCounters",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void testWithCounters() throws Exception\n{\r\n    _testMultipleOutputs(true);\r\n    _testMOWithJavaSerialization(true);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred\\lib",
  "methodName" : "getDir",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "Path getDir(Path dir)\n{\r\n    if (isLocalFS()) {\r\n        String localPathRoot = System.getProperty(\"test.build.data\", \"/tmp\").replace(' ', '+');\r\n        dir = new Path(localPathRoot, dir);\r\n    }\r\n    return dir;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred\\lib",
  "methodName" : "setUp",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void setUp() throws Exception\n{\r\n    super.setUp();\r\n    Path rootDir = getDir(ROOT_DIR);\r\n    Path inDir = getDir(IN_DIR);\r\n    JobConf conf = createJobConf();\r\n    FileSystem fs = FileSystem.get(conf);\r\n    fs.delete(rootDir, true);\r\n    if (!fs.mkdirs(inDir)) {\r\n        throw new IOException(\"Mkdirs failed to create \" + inDir.toString());\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred\\lib",
  "methodName" : "tearDown",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void tearDown() throws Exception\n{\r\n    Path rootDir = getDir(ROOT_DIR);\r\n    JobConf conf = createJobConf();\r\n    FileSystem fs = FileSystem.get(conf);\r\n    fs.delete(rootDir, true);\r\n    super.tearDown();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred\\lib",
  "methodName" : "_testMOWithJavaSerialization",
  "errType" : null,
  "containingMethodsNum" : 44,
  "sourceCodeText" : "void _testMOWithJavaSerialization(boolean withCounters) throws Exception\n{\r\n    Path inDir = getDir(IN_DIR);\r\n    Path outDir = getDir(OUT_DIR);\r\n    JobConf conf = createJobConf();\r\n    FileSystem fs = FileSystem.get(conf);\r\n    DataOutputStream file = fs.create(new Path(inDir, \"part-0\"));\r\n    file.writeBytes(\"a\\nb\\n\\nc\\nd\\ne\");\r\n    file.close();\r\n    fs.delete(inDir, true);\r\n    fs.delete(outDir, true);\r\n    file = fs.create(new Path(inDir, \"part-1\"));\r\n    file.writeBytes(\"a\\nb\\n\\nc\\nd\\ne\");\r\n    file.close();\r\n    conf.setJobName(\"mo\");\r\n    conf.set(\"io.serializations\", \"org.apache.hadoop.io.serializer.JavaSerialization,\" + \"org.apache.hadoop.io.serializer.WritableSerialization\");\r\n    conf.setInputFormat(TextInputFormat.class);\r\n    conf.setMapOutputKeyClass(Long.class);\r\n    conf.setMapOutputValueClass(String.class);\r\n    conf.setOutputKeyComparatorClass(JavaSerializationComparator.class);\r\n    conf.setOutputKeyClass(Long.class);\r\n    conf.setOutputValueClass(String.class);\r\n    conf.setOutputFormat(TextOutputFormat.class);\r\n    MultipleOutputs.addNamedOutput(conf, \"text\", TextOutputFormat.class, Long.class, String.class);\r\n    MultipleOutputs.setCountersEnabled(conf, withCounters);\r\n    conf.setMapperClass(MOJavaSerDeMap.class);\r\n    conf.setReducerClass(MOJavaSerDeReduce.class);\r\n    FileInputFormat.setInputPaths(conf, inDir);\r\n    FileOutputFormat.setOutputPath(conf, outDir);\r\n    JobClient jc = new JobClient(conf);\r\n    RunningJob job = jc.submitJob(conf);\r\n    while (!job.isComplete()) {\r\n        Thread.sleep(100);\r\n    }\r\n    int namedOutputCount = 0;\r\n    FileStatus[] statuses = fs.listStatus(outDir);\r\n    for (FileStatus status : statuses) {\r\n        if (status.getPath().getName().equals(\"text-m-00000\") || status.getPath().getName().equals(\"text-r-00000\")) {\r\n            namedOutputCount++;\r\n        }\r\n    }\r\n    assertEquals(2, namedOutputCount);\r\n    BufferedReader reader = new BufferedReader(new InputStreamReader(fs.open(new Path(FileOutputFormat.getOutputPath(conf), \"text-r-00000\"))));\r\n    int count = 0;\r\n    String line = reader.readLine();\r\n    while (line != null) {\r\n        assertTrue(line.endsWith(\"text\"));\r\n        line = reader.readLine();\r\n        count++;\r\n    }\r\n    reader.close();\r\n    assertFalse(count == 0);\r\n    Counters.Group counters = job.getCounters().getGroup(MultipleOutputs.class.getName());\r\n    if (!withCounters) {\r\n        assertEquals(0, counters.size());\r\n    } else {\r\n        assertEquals(1, counters.size());\r\n        assertEquals(2, counters.getCounter(\"text\"));\r\n    }\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred\\lib",
  "methodName" : "_testMultipleOutputs",
  "errType" : null,
  "containingMethodsNum" : 58,
  "sourceCodeText" : "void _testMultipleOutputs(boolean withCounters) throws Exception\n{\r\n    Path inDir = getDir(IN_DIR);\r\n    Path outDir = getDir(OUT_DIR);\r\n    JobConf conf = createJobConf();\r\n    FileSystem fs = FileSystem.get(conf);\r\n    DataOutputStream file = fs.create(new Path(inDir, \"part-0\"));\r\n    file.writeBytes(\"a\\nb\\n\\nc\\nd\\ne\");\r\n    file.close();\r\n    file = fs.create(new Path(inDir, \"part-1\"));\r\n    file.writeBytes(\"a\\nb\\n\\nc\\nd\\ne\");\r\n    file.close();\r\n    conf.setJobName(\"mo\");\r\n    conf.setInputFormat(TextInputFormat.class);\r\n    conf.setOutputKeyClass(LongWritable.class);\r\n    conf.setOutputValueClass(Text.class);\r\n    conf.setMapOutputKeyClass(LongWritable.class);\r\n    conf.setMapOutputValueClass(Text.class);\r\n    conf.setOutputFormat(TextOutputFormat.class);\r\n    MultipleOutputs.addNamedOutput(conf, \"text\", TextOutputFormat.class, LongWritable.class, Text.class);\r\n    MultipleOutputs.addMultiNamedOutput(conf, \"sequence\", SequenceFileOutputFormat.class, LongWritable.class, Text.class);\r\n    MultipleOutputs.setCountersEnabled(conf, withCounters);\r\n    conf.setMapperClass(MOMap.class);\r\n    conf.setReducerClass(MOReduce.class);\r\n    FileInputFormat.setInputPaths(conf, inDir);\r\n    FileOutputFormat.setOutputPath(conf, outDir);\r\n    JobClient jc = new JobClient(conf);\r\n    RunningJob job = jc.submitJob(conf);\r\n    while (!job.isComplete()) {\r\n        Thread.sleep(100);\r\n    }\r\n    int namedOutputCount = 0;\r\n    FileStatus[] statuses = fs.listStatus(outDir);\r\n    for (FileStatus status : statuses) {\r\n        if (status.getPath().getName().equals(\"text-m-00000\") || status.getPath().getName().equals(\"text-m-00001\") || status.getPath().getName().equals(\"text-r-00000\") || status.getPath().getName().equals(\"sequence_A-m-00000\") || status.getPath().getName().equals(\"sequence_A-m-00001\") || status.getPath().getName().equals(\"sequence_B-m-00000\") || status.getPath().getName().equals(\"sequence_B-m-00001\") || status.getPath().getName().equals(\"sequence_B-r-00000\") || status.getPath().getName().equals(\"sequence_C-r-00000\")) {\r\n            namedOutputCount++;\r\n        }\r\n    }\r\n    assertEquals(9, namedOutputCount);\r\n    BufferedReader reader = new BufferedReader(new InputStreamReader(fs.open(new Path(FileOutputFormat.getOutputPath(conf), \"text-r-00000\"))));\r\n    int count = 0;\r\n    String line = reader.readLine();\r\n    while (line != null) {\r\n        assertTrue(line.endsWith(\"text\"));\r\n        line = reader.readLine();\r\n        count++;\r\n    }\r\n    reader.close();\r\n    assertFalse(count == 0);\r\n    SequenceFile.Reader seqReader = new SequenceFile.Reader(fs, new Path(FileOutputFormat.getOutputPath(conf), \"sequence_B-r-00000\"), conf);\r\n    assertEquals(LongWritable.class, seqReader.getKeyClass());\r\n    assertEquals(Text.class, seqReader.getValueClass());\r\n    count = 0;\r\n    LongWritable key = new LongWritable();\r\n    Text value = new Text();\r\n    while (seqReader.next(key, value)) {\r\n        assertEquals(\"sequence\", value.toString());\r\n        count++;\r\n    }\r\n    seqReader.close();\r\n    assertFalse(count == 0);\r\n    Counters.Group counters = job.getCounters().getGroup(MultipleOutputs.class.getName());\r\n    if (!withCounters) {\r\n        assertEquals(0, counters.size());\r\n    } else {\r\n        assertEquals(4, counters.size());\r\n        assertEquals(4, counters.getCounter(\"text\"));\r\n        assertEquals(2, counters.getCounter(\"sequence_A\"));\r\n        assertEquals(4, counters.getCounter(\"sequence_B\"));\r\n        assertEquals(2, counters.getCounter(\"sequence_C\"));\r\n    }\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\fs\\slive",
  "methodName" : "getMerged",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Configuration getMerged(ParsedOutput opts, Configuration base) throws ConfigException\n{\r\n    return handleOptions(opts, base);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\fs\\slive",
  "methodName" : "getBaseOperations",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "Map<OperationType, OperationData> getBaseOperations()\n{\r\n    Map<OperationType, OperationData> base = new HashMap<OperationType, OperationData>();\r\n    OperationType[] types = OperationType.values();\r\n    for (OperationType type : types) {\r\n        base.put(type, new OperationData(Distribution.UNIFORM, null));\r\n    }\r\n    return base;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\fs\\slive",
  "methodName" : "handleOperations",
  "errType" : null,
  "containingMethodsNum" : 37,
  "sourceCodeText" : "Configuration handleOperations(ParsedOutput opts, Configuration base, ConfigExtractor extractor) throws ConfigException\n{\r\n    Map<OperationType, OperationData> operations = getBaseOperations();\r\n    Map<OperationType, OperationData> cfgOperations = extractor.getOperations();\r\n    for (OperationType opType : cfgOperations.keySet()) {\r\n        operations.put(opType, cfgOperations.get(opType));\r\n    }\r\n    for (OperationType opType : OperationType.values()) {\r\n        String opName = opType.lowerName();\r\n        String opVal = opts.getValue(opName);\r\n        if (opVal != null) {\r\n            operations.put(opType, new OperationData(opVal));\r\n        }\r\n    }\r\n    {\r\n        Map<OperationType, OperationData> cleanedOps = new HashMap<OperationType, OperationData>();\r\n        for (OperationType opType : operations.keySet()) {\r\n            OperationData data = operations.get(opType);\r\n            if (data.getPercent() == null || data.getPercent() > 0.0d) {\r\n                cleanedOps.put(opType, data);\r\n            }\r\n        }\r\n        operations = cleanedOps;\r\n    }\r\n    if (operations.isEmpty()) {\r\n        throw new ConfigException(\"No operations provided!\");\r\n    }\r\n    double currPct = 0;\r\n    int needFill = 0;\r\n    for (OperationType type : operations.keySet()) {\r\n        OperationData op = operations.get(type);\r\n        if (op.getPercent() != null) {\r\n            currPct += op.getPercent();\r\n        } else {\r\n            needFill++;\r\n        }\r\n    }\r\n    if (currPct > 1) {\r\n        throw new ConfigException(\"Unable to have accumlative percent greater than 100%\");\r\n    }\r\n    if (needFill > 0 && currPct < 1) {\r\n        double leftOver = 1.0 - currPct;\r\n        Map<OperationType, OperationData> mpcp = new HashMap<OperationType, OperationData>();\r\n        for (OperationType type : operations.keySet()) {\r\n            OperationData op = operations.get(type);\r\n            if (op.getPercent() == null) {\r\n                op = new OperationData(op.getDistribution(), (leftOver / needFill));\r\n            }\r\n            mpcp.put(type, op);\r\n        }\r\n        operations = mpcp;\r\n    } else if (needFill == 0 && currPct < 1) {\r\n        double leftOver = 1.0 - currPct;\r\n        Map<OperationType, OperationData> mpcp = new HashMap<OperationType, OperationData>();\r\n        double each = leftOver / operations.keySet().size();\r\n        for (OperationType t : operations.keySet()) {\r\n            OperationData op = operations.get(t);\r\n            op = new OperationData(op.getDistribution(), (op.getPercent() + each));\r\n            mpcp.put(t, op);\r\n        }\r\n        operations = mpcp;\r\n    } else if (needFill > 0 && currPct >= 1) {\r\n        throw new ConfigException(needFill + \" unfilled operations but no percentage left to fill with\");\r\n    }\r\n    for (OperationType opType : operations.keySet()) {\r\n        String opName = opType.lowerName();\r\n        OperationData opData = operations.get(opType);\r\n        String distr = opData.getDistribution().lowerName();\r\n        String ratio = new Double(opData.getPercent() * 100.0d).toString();\r\n        base.set(String.format(Constants.OP, opName), opData.toString());\r\n        base.set(String.format(Constants.OP_DISTR, opName), distr);\r\n        base.set(String.format(Constants.OP_PERCENT, opName), ratio);\r\n    }\r\n    return base;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\fs\\slive",
  "methodName" : "handleOptions",
  "errType" : [ "Exception", "Exception", "Exception", "Exception", "Exception", "Exception", "Exception", "Exception", "Exception", "Exception", "Exception", "Exception", "Exception", "Exception", "Exception", "Exception", "Exception", "Exception", "Exception", "Exception", "Exception", "Exception" ],
  "containingMethodsNum" : 71,
  "sourceCodeText" : "Configuration handleOptions(ParsedOutput opts, Configuration base) throws ConfigException\n{\r\n    ConfigExtractor extractor = new ConfigExtractor(base);\r\n    {\r\n        Integer mapAmount = null;\r\n        try {\r\n            mapAmount = extractor.getMapAmount(opts.getValue(ConfigOption.MAPS.getOpt()));\r\n        } catch (Exception e) {\r\n            throw new ConfigException(\"Error extracting & merging map amount\", e);\r\n        }\r\n        if (mapAmount != null) {\r\n            if (mapAmount <= 0) {\r\n                throw new ConfigException(\"Map amount can not be less than or equal to zero\");\r\n            }\r\n            base.set(ConfigOption.MAPS.getCfgOption(), mapAmount.toString());\r\n        }\r\n    }\r\n    {\r\n        Integer reduceAmount = null;\r\n        try {\r\n            reduceAmount = extractor.getMapAmount(opts.getValue(ConfigOption.REDUCES.getOpt()));\r\n        } catch (Exception e) {\r\n            throw new ConfigException(\"Error extracting & merging reducer amount\", e);\r\n        }\r\n        if (reduceAmount != null) {\r\n            if (reduceAmount <= 0) {\r\n                throw new ConfigException(\"Reducer amount can not be less than or equal to zero\");\r\n            }\r\n            base.set(ConfigOption.REDUCES.getCfgOption(), reduceAmount.toString());\r\n        }\r\n    }\r\n    {\r\n        Integer duration = null;\r\n        try {\r\n            duration = extractor.getDuration(opts.getValue(ConfigOption.DURATION.getOpt()));\r\n        } catch (Exception e) {\r\n            throw new ConfigException(\"Error extracting & merging duration\", e);\r\n        }\r\n        if (duration != null) {\r\n            if (duration <= 0) {\r\n                throw new ConfigException(\"Duration can not be less than or equal to zero\");\r\n            }\r\n            base.set(ConfigOption.DURATION.getCfgOption(), duration.toString());\r\n        }\r\n    }\r\n    {\r\n        Integer operationAmount = null;\r\n        try {\r\n            operationAmount = extractor.getOpCount(opts.getValue(ConfigOption.OPS.getOpt()));\r\n        } catch (Exception e) {\r\n            throw new ConfigException(\"Error extracting & merging operation amount\", e);\r\n        }\r\n        if (operationAmount != null) {\r\n            if (operationAmount <= 0) {\r\n                throw new ConfigException(\"Operation amount can not be less than or equal to zero\");\r\n            }\r\n            base.set(ConfigOption.OPS.getCfgOption(), operationAmount.toString());\r\n        }\r\n    }\r\n    {\r\n        try {\r\n            boolean exitOnError = extractor.shouldExitOnFirstError(opts.getValue(ConfigOption.EXIT_ON_ERROR.getOpt()));\r\n            base.setBoolean(ConfigOption.EXIT_ON_ERROR.getCfgOption(), exitOnError);\r\n        } catch (Exception e) {\r\n            throw new ConfigException(\"Error extracting & merging exit on error value\", e);\r\n        }\r\n    }\r\n    {\r\n        try {\r\n            boolean waitOnTruncate = extractor.shouldWaitOnTruncate(opts.getValue(ConfigOption.TRUNCATE_WAIT.getOpt()));\r\n            base.setBoolean(ConfigOption.TRUNCATE_WAIT.getCfgOption(), waitOnTruncate);\r\n        } catch (Exception e) {\r\n            throw new ConfigException(\"Error extracting & merging wait on truncate value\", e);\r\n        }\r\n    }\r\n    {\r\n        Integer fileAm = null;\r\n        try {\r\n            fileAm = extractor.getTotalFiles(opts.getValue(ConfigOption.FILES.getOpt()));\r\n        } catch (Exception e) {\r\n            throw new ConfigException(\"Error extracting & merging total file limit amount\", e);\r\n        }\r\n        if (fileAm != null) {\r\n            if (fileAm <= 0) {\r\n                throw new ConfigException(\"File amount can not be less than or equal to zero\");\r\n            }\r\n            base.set(ConfigOption.FILES.getCfgOption(), fileAm.toString());\r\n        }\r\n    }\r\n    {\r\n        try {\r\n            String qname = extractor.getQueueName(opts.getValue(ConfigOption.QUEUE_NAME.getOpt()));\r\n            if (qname != null) {\r\n                base.set(ConfigOption.QUEUE_NAME.getCfgOption(), qname);\r\n            }\r\n        } catch (Exception e) {\r\n            throw new ConfigException(\"Error extracting & merging queue name\", e);\r\n        }\r\n    }\r\n    {\r\n        Integer directoryLimit = null;\r\n        try {\r\n            directoryLimit = extractor.getDirSize(opts.getValue(ConfigOption.DIR_SIZE.getOpt()));\r\n        } catch (Exception e) {\r\n            throw new ConfigException(\"Error extracting & merging directory file limit\", e);\r\n        }\r\n        if (directoryLimit != null) {\r\n            if (directoryLimit <= 0) {\r\n                throw new ConfigException(\"Directory file limit can not be less than or equal to zero\");\r\n            }\r\n            base.set(ConfigOption.DIR_SIZE.getCfgOption(), directoryLimit.toString());\r\n        }\r\n    }\r\n    {\r\n        Path basedir = null;\r\n        try {\r\n            basedir = extractor.getBaseDirectory(opts.getValue(ConfigOption.BASE_DIR.getOpt()));\r\n        } catch (Exception e) {\r\n            throw new ConfigException(\"Error extracting & merging base directory\", e);\r\n        }\r\n        if (basedir != null) {\r\n            basedir = new Path(basedir, Constants.BASE_DIR);\r\n            base.set(ConfigOption.BASE_DIR.getCfgOption(), basedir.toString());\r\n        }\r\n    }\r\n    {\r\n        String fn = null;\r\n        try {\r\n            fn = extractor.getResultFile(opts.getValue(ConfigOption.RESULT_FILE.getOpt()));\r\n        } catch (Exception e) {\r\n            throw new ConfigException(\"Error extracting & merging result file\", e);\r\n        }\r\n        if (fn != null) {\r\n            base.set(ConfigOption.RESULT_FILE.getCfgOption(), fn);\r\n        }\r\n    }\r\n    {\r\n        String fn = null;\r\n        try {\r\n            fn = extractor.getResultFile(opts.getValue(ConfigOption.RESULT_FILE.getOpt()));\r\n        } catch (Exception e) {\r\n            throw new ConfigException(\"Error extracting & merging result file\", e);\r\n        }\r\n        if (fn != null) {\r\n            base.set(ConfigOption.RESULT_FILE.getCfgOption(), fn);\r\n        }\r\n    }\r\n    {\r\n        try {\r\n            base = handleOperations(opts, base, extractor);\r\n        } catch (Exception e) {\r\n            throw new ConfigException(\"Error extracting & merging operations\", e);\r\n        }\r\n    }\r\n    {\r\n        Range<Short> replicationAm = null;\r\n        try {\r\n            replicationAm = extractor.getReplication(opts.getValue(ConfigOption.REPLICATION_AM.getOpt()));\r\n        } catch (Exception e) {\r\n            throw new ConfigException(\"Error extracting & merging replication amount range\", e);\r\n        }\r\n        if (replicationAm != null) {\r\n            int minRepl = base.getInt(Constants.MIN_REPLICATION, 1);\r\n            if (replicationAm.getLower() < minRepl) {\r\n                throw new ConfigException(\"Replication amount minimum is less than property configured minimum \" + minRepl);\r\n            }\r\n            if (replicationAm.getLower() > replicationAm.getUpper()) {\r\n                throw new ConfigException(\"Replication amount minimum is greater than its maximum\");\r\n            }\r\n            if (replicationAm.getLower() <= 0) {\r\n                throw new ConfigException(\"Replication amount minimum must be greater than zero\");\r\n            }\r\n            base.set(ConfigOption.REPLICATION_AM.getCfgOption(), replicationAm.toString());\r\n        }\r\n    }\r\n    {\r\n        Range<Long> sleepRange = null;\r\n        try {\r\n            sleepRange = extractor.getSleepRange(opts.getValue(ConfigOption.SLEEP_TIME.getOpt()));\r\n        } catch (Exception e) {\r\n            throw new ConfigException(\"Error extracting & merging sleep size range\", e);\r\n        }\r\n        if (sleepRange != null) {\r\n            if (sleepRange.getLower() > sleepRange.getUpper()) {\r\n                throw new ConfigException(\"Sleep range minimum is greater than its maximum\");\r\n            }\r\n            if (sleepRange.getLower() <= 0) {\r\n                throw new ConfigException(\"Sleep range minimum must be greater than zero\");\r\n            }\r\n            base.set(ConfigOption.SLEEP_TIME.getCfgOption(), sleepRange.toString());\r\n        }\r\n    }\r\n    {\r\n        String pSize = opts.getValue(ConfigOption.PACKET_SIZE.getOpt());\r\n        if (pSize == null) {\r\n            pSize = ConfigOption.PACKET_SIZE.getDefault();\r\n        }\r\n        if (pSize != null) {\r\n            try {\r\n                Long packetSize = StringUtils.TraditionalBinaryPrefix.string2long(pSize);\r\n                base.set(ConfigOption.PACKET_SIZE.getCfgOption(), packetSize.toString());\r\n            } catch (Exception e) {\r\n                throw new ConfigException(\"Error extracting & merging write packet size\", e);\r\n            }\r\n        }\r\n    }\r\n    {\r\n        Range<Long> blockSize = null;\r\n        try {\r\n            blockSize = extractor.getBlockSize(opts.getValue(ConfigOption.BLOCK_SIZE.getOpt()));\r\n        } catch (Exception e) {\r\n            throw new ConfigException(\"Error extracting & merging block size range\", e);\r\n        }\r\n        if (blockSize != null) {\r\n            if (blockSize.getLower() > blockSize.getUpper()) {\r\n                throw new ConfigException(\"Block size minimum is greater than its maximum\");\r\n            }\r\n            if (blockSize.getLower() <= 0) {\r\n                throw new ConfigException(\"Block size minimum must be greater than zero\");\r\n            }\r\n            Long bytesPerChecksum = extractor.getByteCheckSum();\r\n            if (bytesPerChecksum != null) {\r\n                if ((blockSize.getLower() % bytesPerChecksum) != 0) {\r\n                    throw new ConfigException(\"Blocksize lower bound must be a multiple of \" + bytesPerChecksum);\r\n                }\r\n                if ((blockSize.getUpper() % bytesPerChecksum) != 0) {\r\n                    throw new ConfigException(\"Blocksize upper bound must be a multiple of \" + bytesPerChecksum);\r\n                }\r\n            }\r\n            base.set(ConfigOption.BLOCK_SIZE.getCfgOption(), blockSize.toString());\r\n        }\r\n    }\r\n    {\r\n        Range<Long> readSize = null;\r\n        try {\r\n            readSize = extractor.getReadSize(opts.getValue(ConfigOption.READ_SIZE.getOpt()));\r\n        } catch (Exception e) {\r\n            throw new ConfigException(\"Error extracting & merging read size range\", e);\r\n        }\r\n        if (readSize != null) {\r\n            if (readSize.getLower() > readSize.getUpper()) {\r\n                throw new ConfigException(\"Read size minimum is greater than its maximum\");\r\n            }\r\n            if (readSize.getLower() < 0) {\r\n                throw new ConfigException(\"Read size minimum must be greater than or equal to zero\");\r\n            }\r\n            base.set(ConfigOption.READ_SIZE.getCfgOption(), readSize.toString());\r\n        }\r\n    }\r\n    {\r\n        Range<Long> writeSize = null;\r\n        try {\r\n            writeSize = extractor.getWriteSize(opts.getValue(ConfigOption.WRITE_SIZE.getOpt()));\r\n        } catch (Exception e) {\r\n            throw new ConfigException(\"Error extracting & merging write size range\", e);\r\n        }\r\n        if (writeSize != null) {\r\n            if (writeSize.getLower() > writeSize.getUpper()) {\r\n                throw new ConfigException(\"Write size minimum is greater than its maximum\");\r\n            }\r\n            if (writeSize.getLower() < 0) {\r\n                throw new ConfigException(\"Write size minimum must be greater than or equal to zero\");\r\n            }\r\n            base.set(ConfigOption.WRITE_SIZE.getCfgOption(), writeSize.toString());\r\n        }\r\n    }\r\n    {\r\n        Range<Long> appendSize = null;\r\n        try {\r\n            appendSize = extractor.getAppendSize(opts.getValue(ConfigOption.APPEND_SIZE.getOpt()));\r\n        } catch (Exception e) {\r\n            throw new ConfigException(\"Error extracting & merging append size range\", e);\r\n        }\r\n        if (appendSize != null) {\r\n            if (appendSize.getLower() > appendSize.getUpper()) {\r\n                throw new ConfigException(\"Append size minimum is greater than its maximum\");\r\n            }\r\n            if (appendSize.getLower() < 0) {\r\n                throw new ConfigException(\"Append size minimum must be greater than or equal to zero\");\r\n            }\r\n            base.set(ConfigOption.APPEND_SIZE.getCfgOption(), appendSize.toString());\r\n        }\r\n    }\r\n    {\r\n        Range<Long> truncateSize = null;\r\n        try {\r\n            truncateSize = extractor.getTruncateSize(opts.getValue(ConfigOption.TRUNCATE_SIZE.getOpt()));\r\n        } catch (Exception e) {\r\n            throw new ConfigException(\"Error extracting & merging truncate size range\", e);\r\n        }\r\n        if (truncateSize != null) {\r\n            if (truncateSize.getLower() > truncateSize.getUpper()) {\r\n                throw new ConfigException(\"Truncate size minimum is greater than its maximum\");\r\n            }\r\n            if (truncateSize.getLower() < 0) {\r\n                throw new ConfigException(\"Truncate size minimum must be greater than or equal to zero\");\r\n            }\r\n            base.set(ConfigOption.TRUNCATE_SIZE.getCfgOption(), truncateSize.toString());\r\n        }\r\n    }\r\n    {\r\n        Long seed = null;\r\n        try {\r\n            seed = extractor.getRandomSeed(opts.getValue(ConfigOption.RANDOM_SEED.getOpt()));\r\n        } catch (Exception e) {\r\n            throw new ConfigException(\"Error extracting & merging random number seed\", e);\r\n        }\r\n        if (seed != null) {\r\n            base.set(ConfigOption.RANDOM_SEED.getCfgOption(), seed.toString());\r\n        }\r\n    }\r\n    return base;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 22,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "testCustomFile",
  "errType" : null,
  "containingMethodsNum" : 36,
  "sourceCodeText" : "void testCustomFile() throws Exception\n{\r\n    Path inDir = new Path(\"testing/fileoutputformat/input\");\r\n    Path outDir = new Path(\"testing/fileoutputformat/output\");\r\n    if (isLocalFS()) {\r\n        String localPathRoot = System.getProperty(\"test.build.data\", \"/tmp\").replace(' ', '+');\r\n        inDir = new Path(localPathRoot, inDir);\r\n        outDir = new Path(localPathRoot, outDir);\r\n    }\r\n    JobConf conf = createJobConf();\r\n    FileSystem fs = FileSystem.get(conf);\r\n    fs.delete(outDir, true);\r\n    if (!fs.mkdirs(inDir)) {\r\n        throw new IOException(\"Mkdirs failed to create \" + inDir.toString());\r\n    }\r\n    DataOutputStream file = fs.create(new Path(inDir, \"part-0\"));\r\n    file.writeBytes(\"a\\nb\\n\\nc\\nd\\ne\");\r\n    file.close();\r\n    file = fs.create(new Path(inDir, \"part-1\"));\r\n    file.writeBytes(\"a\\nb\\n\\nc\\nd\\ne\");\r\n    file.close();\r\n    conf.setJobName(\"fof\");\r\n    conf.setInputFormat(TextInputFormat.class);\r\n    conf.setMapOutputKeyClass(LongWritable.class);\r\n    conf.setMapOutputValueClass(Text.class);\r\n    conf.setOutputFormat(TextOutputFormat.class);\r\n    conf.setOutputKeyClass(LongWritable.class);\r\n    conf.setOutputValueClass(Text.class);\r\n    conf.setMapperClass(TestMap.class);\r\n    conf.setReducerClass(TestReduce.class);\r\n    conf.set(MRConfig.FRAMEWORK_NAME, MRConfig.LOCAL_FRAMEWORK_NAME);\r\n    FileInputFormat.setInputPaths(conf, inDir);\r\n    FileOutputFormat.setOutputPath(conf, outDir);\r\n    JobClient jc = new JobClient(conf);\r\n    RunningJob job = jc.submitJob(conf);\r\n    while (!job.isComplete()) {\r\n        Thread.sleep(100);\r\n    }\r\n    assertTrue(job.isSuccessful());\r\n    boolean map0 = false;\r\n    boolean map1 = false;\r\n    boolean reduce = false;\r\n    FileStatus[] statuses = fs.listStatus(outDir);\r\n    for (FileStatus status : statuses) {\r\n        map0 = map0 || status.getPath().getName().equals(\"test-m-00000\");\r\n        map1 = map1 || status.getPath().getName().equals(\"test-m-00001\");\r\n        reduce = reduce || status.getPath().getName().equals(\"test-r-00000\");\r\n    }\r\n    assertTrue(map0);\r\n    assertTrue(map1);\r\n    assertTrue(reduce);\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\input",
  "methodName" : "createSequenceFile",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void createSequenceFile(int numRecords) throws Exception\n{\r\n    SequenceFile.Writer writer = SequenceFile.createWriter(fs, conf, inFile, Text.class, BytesWritable.class);\r\n    try {\r\n        for (int i = 1; i <= numRecords; i++) {\r\n            Text key = new Text(Integer.toString(i));\r\n            byte[] data = new byte[random.nextInt(10)];\r\n            random.nextBytes(data);\r\n            BytesWritable value = new BytesWritable(data);\r\n            writer.append(key, value);\r\n        }\r\n    } finally {\r\n        writer.close();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\input",
  "methodName" : "countRecords",
  "errType" : null,
  "containingMethodsNum" : 12,
  "sourceCodeText" : "int countRecords(int numSplits) throws IOException, InterruptedException\n{\r\n    InputFormat<Text, BytesWritable> format = new SequenceFileInputFilter<Text, BytesWritable>();\r\n    if (numSplits == 0) {\r\n        numSplits = random.nextInt(MAX_LENGTH / (SequenceFile.SYNC_INTERVAL / 20)) + 1;\r\n    }\r\n    FileInputFormat.setMaxInputSplitSize(job, fs.getFileStatus(inFile).getLen() / numSplits);\r\n    TaskAttemptContext context = MapReduceTestUtil.createDummyMapTaskAttemptContext(job.getConfiguration());\r\n    int count = 0;\r\n    for (InputSplit split : format.getSplits(job)) {\r\n        RecordReader<Text, BytesWritable> reader = format.createRecordReader(split, context);\r\n        MapContext<Text, BytesWritable, Text, BytesWritable> mcontext = new MapContextImpl<Text, BytesWritable, Text, BytesWritable>(job.getConfiguration(), context.getTaskAttemptID(), reader, null, null, MapReduceTestUtil.createDummyReporter(), split);\r\n        reader.initialize(split, mcontext);\r\n        try {\r\n            while (reader.nextKeyValue()) {\r\n                LOG.info(\"Accept record \" + reader.getCurrentKey().toString());\r\n                count++;\r\n            }\r\n        } finally {\r\n            reader.close();\r\n        }\r\n    }\r\n    return count;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\input",
  "methodName" : "testRegexFilter",
  "errType" : null,
  "containingMethodsNum" : 10,
  "sourceCodeText" : "void testRegexFilter() throws Exception\n{\r\n    LOG.info(\"Testing Regex Filter with patter: \\\\A10*\");\r\n    SequenceFileInputFilter.setFilterClass(job, SequenceFileInputFilter.RegexFilter.class);\r\n    SequenceFileInputFilter.RegexFilter.setPattern(job.getConfiguration(), \"\\\\A10*\");\r\n    fs.delete(inDir, true);\r\n    for (int length = 1; length < MAX_LENGTH; length += random.nextInt(MAX_LENGTH / 10) + 1) {\r\n        LOG.info(\"******Number of records: \" + length);\r\n        createSequenceFile(length);\r\n        int count = countRecords(0);\r\n        assertEquals(count, length == 0 ? 0 : (int) Math.log10(length) + 1);\r\n    }\r\n    fs.delete(inDir, true);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\input",
  "methodName" : "testPercentFilter",
  "errType" : null,
  "containingMethodsNum" : 11,
  "sourceCodeText" : "void testPercentFilter() throws Exception\n{\r\n    LOG.info(\"Testing Percent Filter with frequency: 1000\");\r\n    SequenceFileInputFilter.setFilterClass(job, SequenceFileInputFilter.PercentFilter.class);\r\n    SequenceFileInputFilter.PercentFilter.setFrequency(job.getConfiguration(), 1000);\r\n    fs.delete(inDir, true);\r\n    for (int length = 0; length < MAX_LENGTH; length += random.nextInt(MAX_LENGTH / 10) + 1) {\r\n        LOG.info(\"******Number of records: \" + length);\r\n        createSequenceFile(length);\r\n        int count = countRecords(1);\r\n        LOG.info(\"Accepted \" + count + \" records\");\r\n        int expectedCount = length / 1000;\r\n        if (expectedCount * 1000 != length)\r\n            expectedCount++;\r\n        assertThat(count).isEqualTo(expectedCount);\r\n    }\r\n    fs.delete(inDir, true);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\input",
  "methodName" : "testMD5Filter",
  "errType" : null,
  "containingMethodsNum" : 9,
  "sourceCodeText" : "void testMD5Filter() throws Exception\n{\r\n    LOG.info(\"Testing MD5 Filter with frequency: 1000\");\r\n    SequenceFileInputFilter.setFilterClass(job, SequenceFileInputFilter.MD5Filter.class);\r\n    SequenceFileInputFilter.MD5Filter.setFrequency(job.getConfiguration(), 1000);\r\n    fs.delete(inDir, true);\r\n    for (int length = 0; length < MAX_LENGTH; length += random.nextInt(MAX_LENGTH / 10) + 1) {\r\n        LOG.info(\"******Number of records: \" + length);\r\n        createSequenceFile(length);\r\n        LOG.info(\"Accepted \" + countRecords(0) + \" records\");\r\n    }\r\n    fs.delete(inDir, true);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output",
  "methodName" : "testWithoutCounters",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void testWithoutCounters() throws Exception\n{\r\n    _testMultipleOutputs(false);\r\n    _testMOWithJavaSerialization(false);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output",
  "methodName" : "testWithCounters",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void testWithCounters() throws Exception\n{\r\n    _testMultipleOutputs(true);\r\n    _testMOWithJavaSerialization(true);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output",
  "methodName" : "setUp",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void setUp() throws Exception\n{\r\n    super.setUp();\r\n    Configuration conf = createJobConf();\r\n    FileSystem fs = FileSystem.get(conf);\r\n    fs.delete(ROOT_DIR, true);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output",
  "methodName" : "tearDown",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void tearDown() throws Exception\n{\r\n    Configuration conf = createJobConf();\r\n    FileSystem fs = FileSystem.get(conf);\r\n    fs.delete(ROOT_DIR, true);\r\n    super.tearDown();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output",
  "methodName" : "_testMOWithJavaSerialization",
  "errType" : null,
  "containingMethodsNum" : 41,
  "sourceCodeText" : "void _testMOWithJavaSerialization(boolean withCounters) throws Exception\n{\r\n    String input = \"a\\nb\\nc\\nd\\ne\\nc\\nd\\ne\";\r\n    Configuration conf = createJobConf();\r\n    conf.set(\"io.serializations\", \"org.apache.hadoop.io.serializer.JavaSerialization,\" + \"org.apache.hadoop.io.serializer.WritableSerialization\");\r\n    Job job = MapReduceTestUtil.createJob(conf, IN_DIR, OUT_DIR, 2, 1, input);\r\n    job.setJobName(\"mo\");\r\n    MultipleOutputs.addNamedOutput(job, TEXT, TextOutputFormat.class, Long.class, String.class);\r\n    MultipleOutputs.setCountersEnabled(job, withCounters);\r\n    job.setSortComparatorClass(JavaSerializationComparator.class);\r\n    job.setMapOutputKeyClass(Long.class);\r\n    job.setMapOutputValueClass(String.class);\r\n    job.setOutputKeyClass(Long.class);\r\n    job.setOutputValueClass(String.class);\r\n    job.setMapperClass(MOJavaSerDeMap.class);\r\n    job.setReducerClass(MOJavaSerDeReduce.class);\r\n    job.waitForCompletion(true);\r\n    int namedOutputCount = 0;\r\n    int valueBasedOutputCount = 0;\r\n    FileSystem fs = OUT_DIR.getFileSystem(conf);\r\n    FileStatus[] statuses = fs.listStatus(OUT_DIR);\r\n    for (FileStatus status : statuses) {\r\n        String fileName = status.getPath().getName();\r\n        if (fileName.equals(\"text-m-00000\") || fileName.equals(\"text-m-00001\") || fileName.equals(\"text-r-00000\")) {\r\n            namedOutputCount++;\r\n        } else if (fileName.equals(\"a-r-00000\") || fileName.equals(\"b-r-00000\") || fileName.equals(\"c-r-00000\") || fileName.equals(\"d-r-00000\") || fileName.equals(\"e-r-00000\")) {\r\n            valueBasedOutputCount++;\r\n        }\r\n    }\r\n    assertEquals(3, namedOutputCount);\r\n    assertEquals(5, valueBasedOutputCount);\r\n    BufferedReader reader = new BufferedReader(new InputStreamReader(fs.open(new Path(FileOutputFormat.getOutputPath(job), \"text-r-00000\"))));\r\n    int count = 0;\r\n    String line = reader.readLine();\r\n    while (line != null) {\r\n        assertTrue(line.endsWith(TEXT));\r\n        line = reader.readLine();\r\n        count++;\r\n    }\r\n    reader.close();\r\n    assertFalse(count == 0);\r\n    if (withCounters) {\r\n        CounterGroup counters = job.getCounters().getGroup(MultipleOutputs.class.getName());\r\n        assertEquals(6, counters.size());\r\n        assertEquals(4, counters.findCounter(TEXT).getValue());\r\n        assertEquals(2, counters.findCounter(\"a\").getValue());\r\n        assertEquals(2, counters.findCounter(\"b\").getValue());\r\n        assertEquals(4, counters.findCounter(\"c\").getValue());\r\n        assertEquals(4, counters.findCounter(\"d\").getValue());\r\n        assertEquals(4, counters.findCounter(\"e\").getValue());\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output",
  "methodName" : "_testMultipleOutputs",
  "errType" : null,
  "containingMethodsNum" : 52,
  "sourceCodeText" : "void _testMultipleOutputs(boolean withCounters) throws Exception\n{\r\n    String input = \"a\\nb\\nc\\nd\\ne\\nc\\nd\\ne\";\r\n    Configuration conf = createJobConf();\r\n    Job job = MapReduceTestUtil.createJob(conf, IN_DIR, OUT_DIR, 2, 1, input);\r\n    job.setJobName(\"mo\");\r\n    MultipleOutputs.addNamedOutput(job, TEXT, TextOutputFormat.class, LongWritable.class, Text.class);\r\n    MultipleOutputs.addNamedOutput(job, SEQUENCE, SequenceFileOutputFormat.class, IntWritable.class, Text.class);\r\n    MultipleOutputs.setCountersEnabled(job, withCounters);\r\n    job.setMapperClass(MOMap.class);\r\n    job.setReducerClass(MOReduce.class);\r\n    job.waitForCompletion(true);\r\n    int namedOutputCount = 0;\r\n    int valueBasedOutputCount = 0;\r\n    FileSystem fs = OUT_DIR.getFileSystem(conf);\r\n    FileStatus[] statuses = fs.listStatus(OUT_DIR);\r\n    for (FileStatus status : statuses) {\r\n        String fileName = status.getPath().getName();\r\n        if (fileName.equals(\"text-m-00000\") || fileName.equals(\"text-m-00001\") || fileName.equals(\"text-r-00000\") || fileName.equals(\"sequence_A-m-00000\") || fileName.equals(\"sequence_A-m-00001\") || fileName.equals(\"sequence_B-m-00000\") || fileName.equals(\"sequence_B-m-00001\") || fileName.equals(\"sequence_B-r-00000\") || fileName.equals(\"sequence_C-r-00000\")) {\r\n            namedOutputCount++;\r\n        } else if (fileName.equals(\"a-r-00000\") || fileName.equals(\"b-r-00000\") || fileName.equals(\"c-r-00000\") || fileName.equals(\"d-r-00000\") || fileName.equals(\"e-r-00000\")) {\r\n            valueBasedOutputCount++;\r\n        }\r\n    }\r\n    assertEquals(9, namedOutputCount);\r\n    assertEquals(5, valueBasedOutputCount);\r\n    BufferedReader reader = new BufferedReader(new InputStreamReader(fs.open(new Path(FileOutputFormat.getOutputPath(job), \"text-r-00000\"))));\r\n    int count = 0;\r\n    String line = reader.readLine();\r\n    while (line != null) {\r\n        assertTrue(line.endsWith(TEXT));\r\n        line = reader.readLine();\r\n        count++;\r\n    }\r\n    reader.close();\r\n    assertFalse(count == 0);\r\n    SequenceFile.Reader seqReader = new SequenceFile.Reader(fs, new Path(FileOutputFormat.getOutputPath(job), \"sequence_B-r-00000\"), conf);\r\n    assertEquals(IntWritable.class, seqReader.getKeyClass());\r\n    assertEquals(Text.class, seqReader.getValueClass());\r\n    count = 0;\r\n    IntWritable key = new IntWritable();\r\n    Text value = new Text();\r\n    while (seqReader.next(key, value)) {\r\n        assertEquals(SEQUENCE, value.toString());\r\n        count++;\r\n    }\r\n    seqReader.close();\r\n    assertFalse(count == 0);\r\n    if (withCounters) {\r\n        CounterGroup counters = job.getCounters().getGroup(MultipleOutputs.class.getName());\r\n        assertEquals(9, counters.size());\r\n        assertEquals(4, counters.findCounter(TEXT).getValue());\r\n        assertEquals(2, counters.findCounter(SEQUENCE + \"_A\").getValue());\r\n        assertEquals(4, counters.findCounter(SEQUENCE + \"_B\").getValue());\r\n        assertEquals(2, counters.findCounter(SEQUENCE + \"_C\").getValue());\r\n        assertEquals(2, counters.findCounter(\"a\").getValue());\r\n        assertEquals(2, counters.findCounter(\"b\").getValue());\r\n        assertEquals(4, counters.findCounter(\"c\").getValue());\r\n        assertEquals(4, counters.findCounter(\"d\").getValue());\r\n        assertEquals(4, counters.findCounter(\"e\").getValue());\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "createUGI",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "UserGroupInformation createUGI(String name, boolean issuper)\n{\r\n    String group = issuper ? \"supergroup\" : name;\r\n    return UserGroupInformation.createUserForTesting(name, new String[] { group });\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "mkdir",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void mkdir(FileSystem fs, String dir, String user, String group, short mode) throws IOException\n{\r\n    Path p = new Path(dir);\r\n    fs.mkdirs(p);\r\n    fs.setPermission(p, new FsPermission(mode));\r\n    fs.setOwner(p, user, group);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "runJobAsUser",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void runJobAsUser(final JobConf job, UserGroupInformation ugi) throws Exception\n{\r\n    RunningJob rj = ugi.doAs(new PrivilegedExceptionAction<RunningJob>() {\r\n\r\n        public RunningJob run() throws IOException {\r\n            return JobClient.runJob(job);\r\n        }\r\n    });\r\n    rj.waitForCompletion();\r\n    Assert.assertEquals(\"SUCCEEDED\", JobStatus.getJobRunState(rj.getJobState()));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "setUp",
  "errType" : null,
  "containingMethodsNum" : 9,
  "sourceCodeText" : "void setUp() throws Exception\n{\r\n    dfs = new MiniDFSCluster.Builder(conf).numDataNodes(4).build();\r\n    fs = DFS_UGI.doAs(new PrivilegedExceptionAction<FileSystem>() {\r\n\r\n        public FileSystem run() throws IOException {\r\n            return dfs.getFileSystem();\r\n        }\r\n    });\r\n    mkdir(fs, \"/user\", \"nobody\", \"nogroup\", (short) 01777);\r\n    mkdir(fs, \"/user/alice\", \"alice\", \"nogroup\", (short) 0755);\r\n    mkdir(fs, \"/user/bob\", \"bob\", \"nogroup\", (short) 0755);\r\n    UserGroupInformation MR_UGI = UserGroupInformation.getLoginUser();\r\n    mkdir(fs, \"/staging\", MR_UGI.getShortUserName(), \"nogroup\", (short) 01777);\r\n    JobConf mrConf = new JobConf();\r\n    mrConf.set(JTConfig.JT_STAGING_AREA_ROOT, \"/staging\");\r\n    mr = new MiniMRCluster(0, 0, 4, dfs.getFileSystem().getUri().toString(), 1, null, null, MR_UGI, mrConf);\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "tearDown",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void tearDown() throws Exception\n{\r\n    if (mr != null) {\r\n        mr.shutdown();\r\n    }\r\n    if (dfs != null) {\r\n        dfs.shutdown();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "testDistinctUsers",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void testDistinctUsers() throws Exception\n{\r\n    JobConf job1 = mr.createJobConf();\r\n    String input = \"The quick brown fox\\nhas many silly\\n\" + \"red fox sox\\n\";\r\n    Path inDir = new Path(\"/testing/distinct/input\");\r\n    Path outDir = new Path(\"/user/alice/output\");\r\n    TestMiniMRClasspath.configureWordCount(fs, job1, input, 2, 1, inDir, outDir);\r\n    runJobAsUser(job1, ALICE_UGI);\r\n    JobConf job2 = mr.createJobConf();\r\n    Path inDir2 = new Path(\"/testing/distinct/input2\");\r\n    Path outDir2 = new Path(\"/user/bob/output2\");\r\n    TestMiniMRClasspath.configureWordCount(fs, job2, input, 2, 1, inDir2, outDir2);\r\n    runJobAsUser(job2, BOB_UGI);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "testMultipleSpills",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void testMultipleSpills() throws Exception\n{\r\n    JobConf job1 = mr.createJobConf();\r\n    job1.setFloat(MRJobConfig.MAP_SORT_SPILL_PERCENT, 0.0001f);\r\n    job1.setInt(MRJobConfig.IO_SORT_MB, 1);\r\n    job1.setInt(MRJobConfig.INDEX_CACHE_MEMORY_LIMIT, 0);\r\n    String input = \"The quick brown fox\\nhas many silly\\n\" + \"red fox sox\\n\";\r\n    Path inDir = new Path(\"/testing/distinct/input\");\r\n    Path outDir = new Path(\"/user/alice/output\");\r\n    TestMiniMRClasspath.configureWordCount(fs, job1, input, 2, 1, inDir, outDir);\r\n    runJobAsUser(job1, ALICE_UGI);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\partition",
  "methodName" : "testComparator",
  "errType" : null,
  "containingMethodsNum" : 22,
  "sourceCodeText" : "void testComparator(String keySpec, int expect) throws Exception\n{\r\n    String root = System.getProperty(\"test.build.data\", \"/tmp\");\r\n    Path inDir = new Path(root, \"test_cmp/in\");\r\n    Path outDir = new Path(root, \"test_cmp/out\");\r\n    conf.set(\"mapreduce.partition.keycomparator.options\", keySpec);\r\n    conf.set(\"mapreduce.partition.keypartitioner.options\", \"-k1.1,1.1\");\r\n    conf.set(MRJobConfig.MAP_OUTPUT_KEY_FIELD_SEPARATOR, \" \");\r\n    Job job = MapReduceTestUtil.createJob(conf, inDir, outDir, 1, 1, line1 + \"\\n\" + line2 + \"\\n\");\r\n    job.setMapperClass(InverseMapper.class);\r\n    job.setReducerClass(Reducer.class);\r\n    job.setOutputKeyClass(Text.class);\r\n    job.setOutputValueClass(LongWritable.class);\r\n    job.setSortComparatorClass(KeyFieldBasedComparator.class);\r\n    job.setPartitionerClass(KeyFieldBasedPartitioner.class);\r\n    job.waitForCompletion(true);\r\n    assertTrue(job.isSuccessful());\r\n    Path[] outputFiles = FileUtil.stat2Paths(getFileSystem().listStatus(outDir, new Utils.OutputFileUtils.OutputFilesFilter()));\r\n    if (outputFiles.length > 0) {\r\n        InputStream is = getFileSystem().open(outputFiles[0]);\r\n        BufferedReader reader = new BufferedReader(new InputStreamReader(is));\r\n        String line = reader.readLine();\r\n        if (expect == 1) {\r\n            assertTrue(line.startsWith(line1));\r\n        } else if (expect == 2) {\r\n            assertTrue(line.startsWith(line2));\r\n        }\r\n        line = reader.readLine();\r\n        if (expect == 1) {\r\n            assertTrue(line.startsWith(line2));\r\n        } else if (expect == 2) {\r\n            assertTrue(line.startsWith(line1));\r\n        }\r\n        reader.close();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\partition",
  "methodName" : "testBasicUnixComparator",
  "errType" : null,
  "containingMethodsNum" : 15,
  "sourceCodeText" : "void testBasicUnixComparator() throws Exception\n{\r\n    testComparator(\"-k1,1n\", 1);\r\n    testComparator(\"-k2,2n\", 1);\r\n    testComparator(\"-k2.2,2n\", 2);\r\n    testComparator(\"-k3.4,3n\", 2);\r\n    testComparator(\"-k3.2,3.3n -k4,4n\", 2);\r\n    testComparator(\"-k3.2,3.3n -k4,4nr\", 1);\r\n    testComparator(\"-k2.4,2.4n\", 2);\r\n    testComparator(\"-k7,7\", 1);\r\n    testComparator(\"-k7,7n\", 2);\r\n    testComparator(\"-k8,8n\", 1);\r\n    testComparator(\"-k9,9\", 2);\r\n    testComparator(\"-k11,11\", 2);\r\n    testComparator(\"-k10,10\", 2);\r\n    testWithoutMRJob(\"-k9,9\", 1);\r\n    testWithoutMRJob(\"-k9n\", 1);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\partition",
  "methodName" : "testWithoutMRJob",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void testWithoutMRJob(String keySpec, int expect) throws Exception\n{\r\n    KeyFieldBasedComparator<Void, Void> keyFieldCmp = new KeyFieldBasedComparator<Void, Void>();\r\n    conf.set(\"mapreduce.partition.keycomparator.options\", keySpec);\r\n    keyFieldCmp.setConf(conf);\r\n    int result = keyFieldCmp.compare(line1_bytes, 0, line1_bytes.length, line2_bytes, 0, line2_bytes.length);\r\n    if ((expect >= 0 && result < 0) || (expect < 0 && result >= 0))\r\n        fail();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "configureWordCount",
  "errType" : null,
  "containingMethodsNum" : 20,
  "sourceCodeText" : "void configureWordCount(FileSystem fs, JobConf conf, String input, int numMaps, int numReduces, Path inDir, Path outDir) throws IOException\n{\r\n    fs.delete(outDir, true);\r\n    if (!fs.mkdirs(inDir)) {\r\n        throw new IOException(\"Mkdirs failed to create \" + inDir.toString());\r\n    }\r\n    DataOutputStream file = fs.create(new Path(inDir, \"part-0\"));\r\n    file.writeBytes(input);\r\n    file.close();\r\n    FileSystem.setDefaultUri(conf, fs.getUri());\r\n    conf.set(JTConfig.FRAMEWORK_NAME, JTConfig.YARN_FRAMEWORK_NAME);\r\n    conf.setJobName(\"wordcount\");\r\n    conf.setInputFormat(TextInputFormat.class);\r\n    conf.setOutputKeyClass(Text.class);\r\n    conf.setOutputValueClass(IntWritable.class);\r\n    conf.set(\"mapred.mapper.class\", \"testjar.ClassWordCount$MapClass\");\r\n    conf.set(\"mapred.combine.class\", \"testjar.ClassWordCount$Reduce\");\r\n    conf.set(\"mapred.reducer.class\", \"testjar.ClassWordCount$Reduce\");\r\n    FileInputFormat.setInputPaths(conf, inDir);\r\n    FileOutputFormat.setOutputPath(conf, outDir);\r\n    conf.setNumMapTasks(numMaps);\r\n    conf.setNumReduceTasks(numReduces);\r\n    conf.setJarByClass(TestMiniMRClasspath.class);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "launchWordCount",
  "errType" : null,
  "containingMethodsNum" : 12,
  "sourceCodeText" : "String launchWordCount(URI fileSys, JobConf conf, String input, int numMaps, int numReduces) throws IOException\n{\r\n    final Path inDir = new Path(\"/testing/wc/input\");\r\n    final Path outDir = new Path(\"/testing/wc/output\");\r\n    FileSystem fs = FileSystem.get(fileSys, conf);\r\n    configureWordCount(fs, conf, input, numMaps, numReduces, inDir, outDir);\r\n    JobClient.runJob(conf);\r\n    StringBuffer result = new StringBuffer();\r\n    {\r\n        Path[] parents = FileUtil.stat2Paths(fs.listStatus(outDir.getParent()));\r\n        Path[] fileList = FileUtil.stat2Paths(fs.listStatus(outDir, new Utils.OutputFileUtils.OutputFilesFilter()));\r\n        for (int i = 0; i < fileList.length; ++i) {\r\n            BufferedReader file = new BufferedReader(new InputStreamReader(fs.open(fileList[i])));\r\n            String line = file.readLine();\r\n            while (line != null) {\r\n                result.append(line);\r\n                result.append(\"\\n\");\r\n                line = file.readLine();\r\n            }\r\n            file.close();\r\n        }\r\n    }\r\n    return result.toString();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "launchExternal",
  "errType" : null,
  "containingMethodsNum" : 29,
  "sourceCodeText" : "String launchExternal(URI uri, JobConf conf, String input, int numMaps, int numReduces) throws IOException\n{\r\n    final Path inDir = new Path(\"/testing/ext/input\");\r\n    final Path outDir = new Path(\"/testing/ext/output\");\r\n    FileSystem fs = FileSystem.get(uri, conf);\r\n    fs.delete(outDir, true);\r\n    if (!fs.mkdirs(inDir)) {\r\n        throw new IOException(\"Mkdirs failed to create \" + inDir.toString());\r\n    }\r\n    {\r\n        DataOutputStream file = fs.create(new Path(inDir, \"part-0\"));\r\n        file.writeBytes(input);\r\n        file.close();\r\n    }\r\n    FileSystem.setDefaultUri(conf, uri);\r\n    conf.set(JTConfig.FRAMEWORK_NAME, JTConfig.YARN_FRAMEWORK_NAME);\r\n    conf.setJobName(\"wordcount\");\r\n    conf.setInputFormat(TextInputFormat.class);\r\n    conf.setOutputValueClass(IntWritable.class);\r\n    conf.set(JobContext.OUTPUT_KEY_CLASS, \"testjar.ExternalWritable\");\r\n    FileInputFormat.setInputPaths(conf, inDir);\r\n    FileOutputFormat.setOutputPath(conf, outDir);\r\n    conf.setNumMapTasks(numMaps);\r\n    conf.setNumReduceTasks(numReduces);\r\n    conf.set(\"mapred.mapper.class\", \"testjar.ExternalMapperReducer\");\r\n    conf.set(\"mapred.reducer.class\", \"testjar.ExternalMapperReducer\");\r\n    conf.setJarByClass(TestMiniMRClasspath.class);\r\n    JobClient.runJob(conf);\r\n    StringBuffer result = new StringBuffer();\r\n    Path[] fileList = FileUtil.stat2Paths(fs.listStatus(outDir, new Utils.OutputFileUtils.OutputFilesFilter()));\r\n    for (int i = 0; i < fileList.length; ++i) {\r\n        BufferedReader file = new BufferedReader(new InputStreamReader(fs.open(fileList[i])));\r\n        String line = file.readLine();\r\n        while (line != null) {\r\n            result.append(line);\r\n            line = file.readLine();\r\n            result.append(\"\\n\");\r\n        }\r\n        file.close();\r\n    }\r\n    return result.toString();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "testClassPath",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void testClassPath() throws IOException\n{\r\n    String namenode = null;\r\n    MiniDFSCluster dfs = null;\r\n    MiniMRCluster mr = null;\r\n    FileSystem fileSys = null;\r\n    try {\r\n        final int taskTrackers = 4;\r\n        final int jobTrackerPort = 60050;\r\n        Configuration conf = new Configuration();\r\n        dfs = new MiniDFSCluster.Builder(conf).build();\r\n        fileSys = dfs.getFileSystem();\r\n        namenode = fileSys.getUri().toString();\r\n        mr = new MiniMRCluster(taskTrackers, namenode, 3);\r\n        JobConf jobConf = mr.createJobConf();\r\n        String result;\r\n        result = launchWordCount(fileSys.getUri(), jobConf, \"The quick brown fox\\nhas many silly\\n\" + \"red fox sox\\n\", 3, 1);\r\n        Assert.assertEquals(\"The\\t1\\nbrown\\t1\\nfox\\t2\\nhas\\t1\\nmany\\t1\\n\" + \"quick\\t1\\nred\\t1\\nsilly\\t1\\nsox\\t1\\n\", result);\r\n    } finally {\r\n        if (dfs != null) {\r\n            dfs.shutdown();\r\n        }\r\n        if (mr != null) {\r\n            mr.shutdown();\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "testExternalWritable",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void testExternalWritable() throws IOException\n{\r\n    String namenode = null;\r\n    MiniDFSCluster dfs = null;\r\n    MiniMRCluster mr = null;\r\n    FileSystem fileSys = null;\r\n    try {\r\n        final int taskTrackers = 4;\r\n        Configuration conf = new Configuration();\r\n        dfs = new MiniDFSCluster.Builder(conf).build();\r\n        fileSys = dfs.getFileSystem();\r\n        namenode = fileSys.getUri().toString();\r\n        mr = new MiniMRCluster(taskTrackers, namenode, 3);\r\n        JobConf jobConf = mr.createJobConf();\r\n        String result;\r\n        result = launchExternal(fileSys.getUri(), jobConf, \"Dennis was here!\\nDennis again!\", 3, 1);\r\n        Assert.assertEquals(\"Dennis again!\\t1\\nDennis was here!\\t1\\n\", result);\r\n    } finally {\r\n        if (dfs != null) {\r\n            dfs.shutdown();\r\n        }\r\n        if (mr != null) {\r\n            mr.shutdown();\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\fs\\slive",
  "methodName" : "getSplits",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "InputSplit[] getSplits(JobConf job, int numSplits) throws IOException\n{\r\n    InputSplit[] splits = new InputSplit[numSplits];\r\n    for (int i = 0; i < splits.length; ++i) {\r\n        splits[i] = new EmptySplit();\r\n    }\r\n    return splits;\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\fs\\slive",
  "methodName" : "getRecordReader",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "RecordReader<Object, Object> getRecordReader(InputSplit split, JobConf job, Reporter reporter) throws IOException\n{\r\n    return new RecordReader<Object, Object>() {\r\n\r\n        boolean once = false;\r\n\r\n        public boolean next(Object key, Object value) throws IOException {\r\n            if (!once) {\r\n                once = true;\r\n                return true;\r\n            }\r\n            return false;\r\n        }\r\n\r\n        public Object createKey() {\r\n            return new Object();\r\n        }\r\n\r\n        public Object createValue() {\r\n            return new Object();\r\n        }\r\n\r\n        public long getPos() throws IOException {\r\n            return 0L;\r\n        }\r\n\r\n        public void close() throws IOException {\r\n        }\r\n\r\n        public float getProgress() throws IOException {\r\n            return 0.0f;\r\n        }\r\n    };\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output",
  "methodName" : "writeOutput",
  "errType" : null,
  "containingMethodsNum" : 10,
  "sourceCodeText" : "void writeOutput(RecordWriter theRecordWriter, TaskAttemptContext context) throws IOException, InterruptedException\n{\r\n    NullWritable nullWritable = NullWritable.get();\r\n    try {\r\n        theRecordWriter.write(key1, val1);\r\n        theRecordWriter.write(null, nullWritable);\r\n        theRecordWriter.write(null, val1);\r\n        theRecordWriter.write(nullWritable, val2);\r\n        theRecordWriter.write(key2, nullWritable);\r\n        theRecordWriter.write(key1, null);\r\n        theRecordWriter.write(null, null);\r\n        theRecordWriter.write(key2, val2);\r\n    } finally {\r\n        theRecordWriter.close(context);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output",
  "methodName" : "cleanup",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void cleanup() throws IOException\n{\r\n    Configuration conf = new Configuration();\r\n    FileSystem fs = outDir.getFileSystem(conf);\r\n    fs.delete(outDir, true);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output",
  "methodName" : "setUp",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setUp() throws IOException\n{\r\n    cleanup();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output",
  "methodName" : "tearDown",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void tearDown() throws IOException\n{\r\n    cleanup();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output",
  "methodName" : "testCommitter",
  "errType" : null,
  "containingMethodsNum" : 21,
  "sourceCodeText" : "void testCommitter() throws Exception\n{\r\n    Job job = Job.getInstance();\r\n    FileOutputFormat.setOutputPath(job, outDir);\r\n    Configuration conf = job.getConfiguration();\r\n    conf.set(MRJobConfig.TASK_ATTEMPT_ID, attempt);\r\n    JobContext jContext = new JobContextImpl(conf, taskID.getJobID());\r\n    TaskAttemptContext tContext = new TaskAttemptContextImpl(conf, taskID);\r\n    FileOutputCommitter committer = new FileOutputCommitter(outDir, tContext);\r\n    committer.setupJob(jContext);\r\n    committer.setupTask(tContext);\r\n    TextOutputFormat theOutputFormat = new TextOutputFormat();\r\n    RecordWriter theRecordWriter = theOutputFormat.getRecordWriter(tContext);\r\n    writeOutput(theRecordWriter, tContext);\r\n    committer.commitTask(tContext);\r\n    committer.commitJob(jContext);\r\n    File expectedFile = new File(new Path(outDir, partFile).toString());\r\n    StringBuffer expectedOutput = new StringBuffer();\r\n    expectedOutput.append(key1).append('\\t').append(val1).append(\"\\n\");\r\n    expectedOutput.append(val1).append(\"\\n\");\r\n    expectedOutput.append(val2).append(\"\\n\");\r\n    expectedOutput.append(key2).append(\"\\n\");\r\n    expectedOutput.append(key1).append(\"\\n\");\r\n    expectedOutput.append(key2).append('\\t').append(val2).append(\"\\n\");\r\n    String output = UtilsForTests.slurp(expectedFile);\r\n    assertThat(output).isEqualTo(expectedOutput.toString());\r\n    FileUtil.fullyDelete(new File(outDir.toString()));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output",
  "methodName" : "testEmptyOutput",
  "errType" : null,
  "containingMethodsNum" : 10,
  "sourceCodeText" : "void testEmptyOutput() throws Exception\n{\r\n    Job job = Job.getInstance();\r\n    FileOutputFormat.setOutputPath(job, outDir);\r\n    Configuration conf = job.getConfiguration();\r\n    conf.set(MRJobConfig.TASK_ATTEMPT_ID, attempt);\r\n    JobContext jContext = new JobContextImpl(conf, taskID.getJobID());\r\n    TaskAttemptContext tContext = new TaskAttemptContextImpl(conf, taskID);\r\n    FileOutputCommitter committer = new FileOutputCommitter(outDir, tContext);\r\n    committer.setupJob(jContext);\r\n    committer.setupTask(tContext);\r\n    committer.commitTask(tContext);\r\n    committer.commitJob(jContext);\r\n    FileUtil.fullyDelete(new File(outDir.toString()));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output",
  "methodName" : "testAbort",
  "errType" : null,
  "containingMethodsNum" : 17,
  "sourceCodeText" : "void testAbort() throws IOException, InterruptedException\n{\r\n    Job job = Job.getInstance();\r\n    FileOutputFormat.setOutputPath(job, outDir);\r\n    Configuration conf = job.getConfiguration();\r\n    conf.set(MRJobConfig.TASK_ATTEMPT_ID, attempt);\r\n    JobContext jContext = new JobContextImpl(conf, taskID.getJobID());\r\n    TaskAttemptContext tContext = new TaskAttemptContextImpl(conf, taskID);\r\n    FileOutputCommitter committer = new FileOutputCommitter(outDir, tContext);\r\n    committer.setupJob(jContext);\r\n    committer.setupTask(tContext);\r\n    TextOutputFormat theOutputFormat = new TextOutputFormat();\r\n    RecordWriter theRecordWriter = theOutputFormat.getRecordWriter(tContext);\r\n    writeOutput(theRecordWriter, tContext);\r\n    committer.abortTask(tContext);\r\n    File expectedFile = new File(new Path(committer.getWorkPath(), partFile).toString());\r\n    assertFalse(\"task temp dir still exists\", expectedFile.exists());\r\n    committer.abortJob(jContext, JobStatus.State.FAILED);\r\n    expectedFile = new File(new Path(outDir, FileOutputCommitter.PENDING_DIR_NAME).toString());\r\n    assertFalse(\"job temp dir still exists\", expectedFile.exists());\r\n    assertThat(new File(outDir.toString()).listFiles()).withFailMessage(\"Output directory not empty\").isEmpty();\r\n    FileUtil.fullyDelete(new File(outDir.toString()));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\output",
  "methodName" : "testFailAbort",
  "errType" : [ "IOException", "IOException" ],
  "containingMethodsNum" : 24,
  "sourceCodeText" : "void testFailAbort() throws IOException, InterruptedException\n{\r\n    Job job = Job.getInstance();\r\n    Configuration conf = job.getConfiguration();\r\n    conf.set(FileSystem.FS_DEFAULT_NAME_KEY, \"faildel:///\");\r\n    conf.setClass(\"fs.faildel.impl\", FakeFileSystem.class, FileSystem.class);\r\n    conf.set(MRJobConfig.TASK_ATTEMPT_ID, attempt);\r\n    FileOutputFormat.setOutputPath(job, outDir);\r\n    JobContext jContext = new JobContextImpl(conf, taskID.getJobID());\r\n    TaskAttemptContext tContext = new TaskAttemptContextImpl(conf, taskID);\r\n    FileOutputCommitter committer = new FileOutputCommitter(outDir, tContext);\r\n    committer.setupJob(jContext);\r\n    committer.setupTask(tContext);\r\n    TextOutputFormat<?, ?> theOutputFormat = new TextOutputFormat();\r\n    RecordWriter<?, ?> theRecordWriter = theOutputFormat.getRecordWriter(tContext);\r\n    writeOutput(theRecordWriter, tContext);\r\n    Throwable th = null;\r\n    try {\r\n        committer.abortTask(tContext);\r\n    } catch (IOException ie) {\r\n        th = ie;\r\n    }\r\n    assertNotNull(th);\r\n    assertTrue(th instanceof IOException);\r\n    assertTrue(th.getMessage().contains(\"fake delete failed\"));\r\n    File jobTmpDir = new File(committer.getJobAttemptPath(jContext).toUri().getPath());\r\n    File taskTmpDir = new File(committer.getTaskAttemptPath(tContext).toUri().getPath());\r\n    File expectedFile = new File(taskTmpDir, partFile);\r\n    assertTrue(expectedFile + \" does not exists\", expectedFile.exists());\r\n    th = null;\r\n    try {\r\n        committer.abortJob(jContext, JobStatus.State.FAILED);\r\n    } catch (IOException ie) {\r\n        th = ie;\r\n    }\r\n    assertNotNull(th);\r\n    assertTrue(th instanceof IOException);\r\n    assertTrue(th.getMessage().contains(\"fake delete failed\"));\r\n    assertTrue(\"job temp dir does not exists\", jobTmpDir.exists());\r\n    FileUtil.fullyDelete(new File(outDir.toString()));\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 2,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\input",
  "methodName" : "testFormat",
  "errType" : null,
  "containingMethodsNum" : 33,
  "sourceCodeText" : "void testFormat() throws Exception\n{\r\n    Job job = Job.getInstance(new Configuration(defaultConf));\r\n    Random random = new Random();\r\n    long seed = random.nextLong();\r\n    LOG.info(\"seed = \" + seed);\r\n    random.setSeed(seed);\r\n    localFs.delete(workDir, true);\r\n    FileInputFormat.setInputPaths(job, workDir);\r\n    final int length = 10000;\r\n    final int numFiles = 10;\r\n    createFiles(length, numFiles, random);\r\n    CombineTextInputFormat format = new CombineTextInputFormat();\r\n    for (int i = 0; i < 3; i++) {\r\n        int numSplits = random.nextInt(length / 20) + 1;\r\n        LOG.info(\"splitting: requesting = \" + numSplits);\r\n        List<InputSplit> splits = format.getSplits(job);\r\n        LOG.info(\"splitting: got =        \" + splits.size());\r\n        assertEquals(\"We got more than one splits!\", 1, splits.size());\r\n        InputSplit split = splits.get(0);\r\n        assertEquals(\"It should be CombineFileSplit\", CombineFileSplit.class, split.getClass());\r\n        BitSet bits = new BitSet(length);\r\n        LOG.debug(\"split= \" + split);\r\n        TaskAttemptContext context = MapReduceTestUtil.createDummyMapTaskAttemptContext(job.getConfiguration());\r\n        RecordReader<LongWritable, Text> reader = format.createRecordReader(split, context);\r\n        assertEquals(\"reader class is CombineFileRecordReader.\", CombineFileRecordReader.class, reader.getClass());\r\n        MapContext<LongWritable, Text, LongWritable, Text> mcontext = new MapContextImpl<LongWritable, Text, LongWritable, Text>(job.getConfiguration(), context.getTaskAttemptID(), reader, null, null, MapReduceTestUtil.createDummyReporter(), split);\r\n        reader.initialize(split, mcontext);\r\n        try {\r\n            int count = 0;\r\n            while (reader.nextKeyValue()) {\r\n                LongWritable key = reader.getCurrentKey();\r\n                assertNotNull(\"Key should not be null.\", key);\r\n                Text value = reader.getCurrentValue();\r\n                final int v = Integer.parseInt(value.toString());\r\n                LOG.debug(\"read \" + v);\r\n                assertFalse(\"Key in multiple partitions.\", bits.get(v));\r\n                bits.set(v);\r\n                count++;\r\n            }\r\n            LOG.debug(\"split=\" + split + \" count=\" + count);\r\n        } finally {\r\n            reader.close();\r\n        }\r\n        assertEquals(\"Some keys in no partition.\", length, bits.cardinality());\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\input",
  "methodName" : "createRanges",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Range[] createRanges(int length, int numFiles, Random random)\n{\r\n    Range[] ranges = new Range[numFiles];\r\n    for (int i = 0; i < numFiles; i++) {\r\n        int start = i == 0 ? 0 : ranges[i - 1].end;\r\n        int end = i == numFiles - 1 ? length : (length / numFiles) * (2 * i + 1) / 2 + random.nextInt(length / numFiles) + 1;\r\n        ranges[i] = new Range(start, end);\r\n    }\r\n    return ranges;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\input",
  "methodName" : "createFiles",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void createFiles(int length, int numFiles, Random random) throws IOException\n{\r\n    Range[] ranges = createRanges(length, numFiles, random);\r\n    for (int i = 0; i < numFiles; i++) {\r\n        Path file = new Path(workDir, \"test_\" + i + \".txt\");\r\n        Writer writer = new OutputStreamWriter(localFs.create(file));\r\n        Range range = ranges[i];\r\n        try {\r\n            for (int j = range.start; j < range.end; j++) {\r\n                writer.write(Integer.toString(j));\r\n                writer.write(\"\\n\");\r\n            }\r\n        } finally {\r\n            writer.close();\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\input",
  "methodName" : "writeFile",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void writeFile(FileSystem fs, Path name, CompressionCodec codec, String contents) throws IOException\n{\r\n    OutputStream stm;\r\n    if (codec == null) {\r\n        stm = fs.create(name);\r\n    } else {\r\n        stm = codec.createOutputStream(fs.create(name));\r\n    }\r\n    stm.write(contents.getBytes());\r\n    stm.close();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\input",
  "methodName" : "readSplit",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "List<Text> readSplit(InputFormat<LongWritable, Text> format, InputSplit split, Job job) throws IOException, InterruptedException\n{\r\n    List<Text> result = new ArrayList<Text>();\r\n    Configuration conf = job.getConfiguration();\r\n    TaskAttemptContext context = MapReduceTestUtil.createDummyMapTaskAttemptContext(conf);\r\n    RecordReader<LongWritable, Text> reader = format.createRecordReader(split, MapReduceTestUtil.createDummyMapTaskAttemptContext(conf));\r\n    MapContext<LongWritable, Text, LongWritable, Text> mcontext = new MapContextImpl<LongWritable, Text, LongWritable, Text>(conf, context.getTaskAttemptID(), reader, null, null, MapReduceTestUtil.createDummyReporter(), split);\r\n    reader.initialize(split, mcontext);\r\n    while (reader.nextKeyValue()) {\r\n        result.add(new Text(reader.getCurrentValue()));\r\n    }\r\n    return result;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\input",
  "methodName" : "testGzip",
  "errType" : null,
  "containingMethodsNum" : 16,
  "sourceCodeText" : "void testGzip() throws IOException, InterruptedException\n{\r\n    Configuration conf = new Configuration(defaultConf);\r\n    CompressionCodec gzip = new GzipCodec();\r\n    ReflectionUtils.setConf(gzip, conf);\r\n    localFs.delete(workDir, true);\r\n    writeFile(localFs, new Path(workDir, \"part1.txt.gz\"), gzip, \"the quick\\nbrown\\nfox jumped\\nover\\n the lazy\\n dog\\n\");\r\n    writeFile(localFs, new Path(workDir, \"part2.txt.gz\"), gzip, \"this is a test\\nof gzip\\n\");\r\n    Job job = Job.getInstance(conf);\r\n    FileInputFormat.setInputPaths(job, workDir);\r\n    CombineTextInputFormat format = new CombineTextInputFormat();\r\n    List<InputSplit> splits = format.getSplits(job);\r\n    assertEquals(\"compressed splits == 1\", 1, splits.size());\r\n    List<Text> results = readSplit(format, splits.get(0), job);\r\n    assertEquals(\"splits[0] length\", 8, results.size());\r\n    final String[] firstList = { \"the quick\", \"brown\", \"fox jumped\", \"over\", \" the lazy\", \" dog\" };\r\n    final String[] secondList = { \"this is a test\", \"of gzip\" };\r\n    String first = results.get(0).toString();\r\n    if (first.equals(firstList[0])) {\r\n        testResults(results, firstList, secondList);\r\n    } else if (first.equals(secondList[0])) {\r\n        testResults(results, secondList, firstList);\r\n    } else {\r\n        fail(\"unexpected first token!\");\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\input",
  "methodName" : "testResults",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void testResults(List<Text> results, String[] first, String[] second)\n{\r\n    for (int i = 0; i < first.length; i++) {\r\n        assertEquals(\"splits[0][\" + i + \"]\", first[i], results.get(i).toString());\r\n    }\r\n    for (int i = 0; i < second.length; i++) {\r\n        int j = i + first.length;\r\n        assertEquals(\"splits[0][\" + j + \"]\", second[i], results.get(j).toString());\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\io",
  "methodName" : "printUsage",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "int printUsage()\n{\r\n    ToolRunner.printGenericCommandUsage(System.out);\r\n    System.out.println(\"Usage: Task list:           -[no]r -[no]w\\n\" + \"       Format:              -[no]seq -[no]txt\\n\" + \"       CompressionCodec:    -[no]zip -[no]pln\\n\" + \"       CompressionType:     -[no]blk -[no]rec\\n\" + \"       Required:            -dir <working dir>\\n\" + \"All valid combinations are implicitly enabled, unless an option is enabled\\n\" + \"explicitly. For example, specifying \\\"-zip\\\", excludes -pln,\\n\" + \"unless they are also explicitly included, as in \\\"-pln -zip\\\"\\n\" + \"Note that CompressionType params only apply to SequenceFiles\\n\\n\" + \"Useful options to set:\\n\" + \"-D fs.defaultFS=\\\"file:///\\\" \\\\\\n\" + \"-D fs.file.impl=org.apache.hadoop.fs.RawLocalFileSystem \\\\\\n\" + \"-D filebench.file.bytes=$((10*1024*1024*1024)) \\\\\\n\" + \"-D filebench.key.words=5 \\\\\\n\" + \"-D filebench.val.words=20\\n\");\r\n    return -1;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\io",
  "methodName" : "generateSentence",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "String generateSentence(Random r, int noWords)\n{\r\n    sentence.setLength(0);\r\n    for (int i = 0; i < noWords; ++i) {\r\n        sentence.append(words[r.nextInt(words.length)]);\r\n        sentence.append(\" \");\r\n    }\r\n    return sentence.toString();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\io",
  "methodName" : "fillBlocks",
  "errType" : null,
  "containingMethodsNum" : 13,
  "sourceCodeText" : "void fillBlocks(JobConf conf)\n{\r\n    Random r = new Random();\r\n    long seed = conf.getLong(\"filebench.seed\", -1);\r\n    if (seed > 0) {\r\n        r.setSeed(seed);\r\n    }\r\n    int keylen = conf.getInt(\"filebench.key.words\", 5);\r\n    int vallen = conf.getInt(\"filebench.val.words\", 20);\r\n    int acc = (3 * conf.getInt(\"io.seqfile.compress.blocksize\", 1000000)) >> 1;\r\n    ArrayList<String> k = new ArrayList<String>();\r\n    ArrayList<String> v = new ArrayList<String>();\r\n    for (int i = 0; acc > 0; ++i) {\r\n        String s = generateSentence(r, keylen);\r\n        acc -= s.length();\r\n        k.add(s);\r\n        s = generateSentence(r, vallen);\r\n        acc -= s.length();\r\n        v.add(s);\r\n    }\r\n    keys = k.toArray(new String[0]);\r\n    values = v.toArray(new String[0]);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\io",
  "methodName" : "writeBench",
  "errType" : null,
  "containingMethodsNum" : 14,
  "sourceCodeText" : "long writeBench(JobConf conf) throws IOException\n{\r\n    long filelen = conf.getLong(\"filebench.file.bytes\", 5 * 1024 * 1024 * 1024);\r\n    Text key = new Text();\r\n    Text val = new Text();\r\n    final String fn = conf.get(\"test.filebench.name\", \"\");\r\n    final Path outd = FileOutputFormat.getOutputPath(conf);\r\n    conf.set(\"mapred.work.output.dir\", outd.toString());\r\n    OutputFormat outf = conf.getOutputFormat();\r\n    RecordWriter<Text, Text> rw = outf.getRecordWriter(outd.getFileSystem(conf), conf, fn, Reporter.NULL);\r\n    try {\r\n        long acc = 0L;\r\n        Date start = new Date();\r\n        for (int i = 0; acc < filelen; ++i) {\r\n            i %= keys.length;\r\n            key.set(keys[i]);\r\n            val.set(values[i]);\r\n            rw.write(key, val);\r\n            acc += keys[i].length();\r\n            acc += values[i].length();\r\n        }\r\n        Date end = new Date();\r\n        return end.getTime() - start.getTime();\r\n    } finally {\r\n        rw.close(Reporter.NULL);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\io",
  "methodName" : "readBench",
  "errType" : null,
  "containingMethodsNum" : 11,
  "sourceCodeText" : "long readBench(JobConf conf) throws IOException\n{\r\n    InputFormat inf = conf.getInputFormat();\r\n    final String fn = conf.get(\"test.filebench.name\", \"\");\r\n    Path pin = new Path(FileInputFormat.getInputPaths(conf)[0], fn);\r\n    FileStatus in = pin.getFileSystem(conf).getFileStatus(pin);\r\n    RecordReader rr = inf.getRecordReader(new FileSplit(pin, 0, in.getLen(), (String[]) null), conf, Reporter.NULL);\r\n    try {\r\n        Object key = rr.createKey();\r\n        Object val = rr.createValue();\r\n        Date start = new Date();\r\n        while (rr.next(key, val)) ;\r\n        Date end = new Date();\r\n        return end.getTime() - start.getTime();\r\n    } finally {\r\n        rr.close();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\io",
  "methodName" : "main",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void main(String[] args) throws Exception\n{\r\n    int res = ToolRunner.run(new Configuration(), new FileBench(), args);\r\n    System.exit(res);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\io",
  "methodName" : "run",
  "errType" : [ "Exception" ],
  "containingMethodsNum" : 44,
  "sourceCodeText" : "int run(String[] argv) throws IOException\n{\r\n    JobConf job = new JobConf(getConf());\r\n    EnumSet<CCodec> cc = null;\r\n    EnumSet<CType> ct = null;\r\n    EnumSet<Format> f = null;\r\n    EnumSet<RW> rw = null;\r\n    Path root = null;\r\n    FileSystem fs = FileSystem.get(job);\r\n    for (int i = 0; i < argv.length; ++i) {\r\n        try {\r\n            if (\"-dir\".equals(argv[i])) {\r\n                root = fs.makeQualified(new Path(argv[++i]));\r\n                System.out.println(\"DIR: \" + root.toString());\r\n            } else if (\"-seed\".equals(argv[i])) {\r\n                job.setLong(\"filebench.seed\", Long.valueOf(argv[++i]));\r\n            } else if (argv[i].startsWith(\"-no\")) {\r\n                String arg = argv[i].substring(3);\r\n                cc = rem(CCodec.class, cc, arg);\r\n                ct = rem(CType.class, ct, arg);\r\n                f = rem(Format.class, f, arg);\r\n                rw = rem(RW.class, rw, arg);\r\n            } else {\r\n                String arg = argv[i].substring(1);\r\n                cc = add(CCodec.class, cc, arg);\r\n                ct = add(CType.class, ct, arg);\r\n                f = add(Format.class, f, arg);\r\n                rw = add(RW.class, rw, arg);\r\n            }\r\n        } catch (Exception e) {\r\n            throw (IOException) new IOException().initCause(e);\r\n        }\r\n    }\r\n    if (null == root) {\r\n        System.out.println(\"Missing -dir param\");\r\n        printUsage();\r\n        return -1;\r\n    }\r\n    fillBlocks(job);\r\n    job.setOutputKeyClass(Text.class);\r\n    job.setOutputValueClass(Text.class);\r\n    FileInputFormat.setInputPaths(job, root);\r\n    FileOutputFormat.setOutputPath(job, root);\r\n    if (null == cc)\r\n        cc = EnumSet.allOf(CCodec.class);\r\n    if (null == ct)\r\n        ct = EnumSet.allOf(CType.class);\r\n    if (null == f)\r\n        f = EnumSet.allOf(Format.class);\r\n    if (null == rw)\r\n        rw = EnumSet.allOf(RW.class);\r\n    for (RW rwop : rw) {\r\n        for (Format fmt : f) {\r\n            fmt.configure(job);\r\n            for (CCodec cod : cc) {\r\n                cod.configure(job);\r\n                if (!(fmt == Format.txt || cod == CCodec.pln)) {\r\n                    for (CType typ : ct) {\r\n                        String fn = StringUtils.toUpperCase(fmt.name()) + \"_\" + StringUtils.toUpperCase(cod.name()) + \"_\" + StringUtils.toUpperCase(typ.name());\r\n                        typ.configure(job);\r\n                        System.out.print(StringUtils.toUpperCase(rwop.name()) + \" \" + fn + \": \");\r\n                        System.out.println(rwop.exec(fn, job) / 1000 + \" seconds\");\r\n                    }\r\n                } else {\r\n                    String fn = StringUtils.toUpperCase(fmt.name()) + \"_\" + StringUtils.toUpperCase(cod.name());\r\n                    Path p = new Path(root, fn);\r\n                    if (rwop == RW.r && !fs.exists(p)) {\r\n                        fn += cod.getExt();\r\n                    }\r\n                    System.out.print(StringUtils.toUpperCase(rwop.name()) + \" \" + fn + \": \");\r\n                    System.out.println(rwop.exec(fn, job) / 1000 + \" seconds\");\r\n                }\r\n            }\r\n        }\r\n    }\r\n    return 0;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\io",
  "methodName" : "rem",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "EnumSet<T> rem(Class<T> c, EnumSet<T> set, String s)\n{\r\n    if (null != fullmap.get(c) && fullmap.get(c).get(s) != null) {\r\n        if (null == set) {\r\n            set = EnumSet.allOf(c);\r\n        }\r\n        set.remove(fullmap.get(c).get(s));\r\n    }\r\n    return set;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\io",
  "methodName" : "add",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "EnumSet<T> add(Class<T> c, EnumSet<T> set, String s)\n{\r\n    if (null != fullmap.get(c) && fullmap.get(c).get(s) != null) {\r\n        if (null == set) {\r\n            set = EnumSet.noneOf(c);\r\n        }\r\n        set.add((T) fullmap.get(c).get(s));\r\n    }\r\n    return set;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\io",
  "methodName" : "testMergeProgressWithNoCompression",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testMergeProgressWithNoCompression() throws IOException\n{\r\n    runTest(SequenceFile.CompressionType.NONE);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\io",
  "methodName" : "testMergeProgressWithRecordCompression",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testMergeProgressWithRecordCompression() throws IOException\n{\r\n    runTest(SequenceFile.CompressionType.RECORD);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\io",
  "methodName" : "testMergeProgressWithBlockCompression",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testMergeProgressWithBlockCompression() throws IOException\n{\r\n    runTest(SequenceFile.CompressionType.BLOCK);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\io",
  "methodName" : "runTest",
  "errType" : null,
  "containingMethodsNum" : 19,
  "sourceCodeText" : "void runTest(CompressionType compressionType) throws IOException\n{\r\n    JobConf job = new JobConf();\r\n    FileSystem fs = FileSystem.getLocal(job);\r\n    Path dir = new Path(System.getProperty(\"test.build.data\", \".\") + \"/mapred\");\r\n    Path file = new Path(dir, \"test.seq\");\r\n    Path tempDir = new Path(dir, \"tmp\");\r\n    fs.delete(dir, true);\r\n    FileInputFormat.setInputPaths(job, dir);\r\n    fs.mkdirs(tempDir);\r\n    LongWritable tkey = new LongWritable();\r\n    Text tval = new Text();\r\n    SequenceFile.Writer writer = SequenceFile.createWriter(fs, job, file, LongWritable.class, Text.class, compressionType, new DefaultCodec());\r\n    try {\r\n        for (int i = 0; i < RECORDS; ++i) {\r\n            tkey.set(1234);\r\n            tval.set(\"valuevaluevaluevaluevaluevaluevaluevaluevaluevaluevalue\");\r\n            writer.append(tkey, tval);\r\n        }\r\n    } finally {\r\n        writer.close();\r\n    }\r\n    long fileLength = fs.getFileStatus(file).getLen();\r\n    LOG.info(\"With compression = \" + compressionType + \": \" + \"compressed length = \" + fileLength);\r\n    SequenceFile.Sorter sorter = new SequenceFile.Sorter(fs, job.getOutputKeyComparator(), job.getMapOutputKeyClass(), job.getMapOutputValueClass(), job);\r\n    Path[] paths = new Path[] { file };\r\n    RawKeyValueIterator rIter = sorter.merge(paths, tempDir, false);\r\n    int count = 0;\r\n    while (rIter.next()) {\r\n        count++;\r\n    }\r\n    assertEquals(RECORDS, count);\r\n    assertEquals(1.0f, rIter.getProgress().get(), 0.0000);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "testBringUp",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testBringUp() throws IOException\n{\r\n    MiniMRCluster mr = null;\r\n    try {\r\n        mr = new MiniMRCluster(1, \"local\", 1);\r\n    } finally {\r\n        if (mr != null) {\r\n            mr.shutdown();\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "testMiniMRYarnClusterWithoutJHS",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void testMiniMRYarnClusterWithoutJHS() throws IOException\n{\r\n    MiniMRYarnCluster mr = null;\r\n    try {\r\n        final Configuration conf = new Configuration();\r\n        conf.setBoolean(MiniMRYarnCluster.MR_HISTORY_MINICLUSTER_ENABLED, false);\r\n        mr = new MiniMRYarnCluster(\"testMiniMRYarnClusterWithoutJHS\");\r\n        mr.init(conf);\r\n        mr.start();\r\n        Assert.assertEquals(null, mr.getHistoryServer());\r\n    } finally {\r\n        if (mr != null) {\r\n            mr.stop();\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\filecache",
  "methodName" : "testURIs",
  "errType" : null,
  "containingMethodsNum" : 21,
  "sourceCodeText" : "void testURIs() throws URISyntaxException\n{\r\n    assertTrue(DistributedCache.checkURIs(null, null));\r\n    assertFalse(DistributedCache.checkURIs(new URI[] { new URI(\"file://foo/bar/myCacheFile.txt\") }, null));\r\n    assertFalse(DistributedCache.checkURIs(null, new URI[] { new URI(\"file://foo/bar/myCacheArchive.txt\") }));\r\n    assertFalse(DistributedCache.checkURIs(new URI[] { new URI(\"file://foo/bar/myCacheFile1.txt#file\"), new URI(\"file://foo/bar/myCacheFile2.txt\") }, null));\r\n    assertFalse(DistributedCache.checkURIs(null, new URI[] { new URI(\"file://foo/bar/myCacheArchive1.txt\"), new URI(\"file://foo/bar/myCacheArchive2.txt#archive\") }));\r\n    assertFalse(DistributedCache.checkURIs(new URI[] { new URI(\"file://foo/bar/myCacheFile.txt\") }, new URI[] { new URI(\"file://foo/bar/myCacheArchive.txt\") }));\r\n    assertFalse(DistributedCache.checkURIs(new URI[] { new URI(\"file://foo/bar/myCacheFile1.txt#file\"), new URI(\"file://foo/bar/myCacheFile2.txt#file\") }, null));\r\n    assertFalse(DistributedCache.checkURIs(null, new URI[] { new URI(\"file://foo/bar/myCacheArchive1.txt#archive\"), new URI(\"file://foo/bar/myCacheArchive2.txt#archive\") }));\r\n    assertFalse(DistributedCache.checkURIs(new URI[] { new URI(\"file://foo/bar/myCacheFile.txt#cache\") }, new URI[] { new URI(\"file://foo/bar/myCacheArchive.txt#cache\") }));\r\n    assertFalse(DistributedCache.checkURIs(new URI[] { new URI(\"file://foo/bar/myCacheFile1.txt#file1\"), new URI(\"file://foo/bar/myCacheFile2.txt#file2\") }, new URI[] { new URI(\"file://foo/bar/myCacheArchive1.txt#archive\"), new URI(\"file://foo/bar/myCacheArchive2.txt#archive\") }));\r\n    assertFalse(DistributedCache.checkURIs(new URI[] { new URI(\"file://foo/bar/myCacheFile1.txt#file\"), new URI(\"file://foo/bar/myCacheFile2.txt#file\") }, new URI[] { new URI(\"file://foo/bar/myCacheArchive1.txt#archive1\"), new URI(\"file://foo/bar/myCacheArchive2.txt#archive2\") }));\r\n    assertFalse(DistributedCache.checkURIs(new URI[] { new URI(\"file://foo/bar/myCacheFile1.txt#file1\"), new URI(\"file://foo/bar/myCacheFile2.txt#cache\") }, new URI[] { new URI(\"file://foo/bar/myCacheArchive1.txt#cache\"), new URI(\"file://foo/bar/myCacheArchive2.txt#archive2\") }));\r\n    assertFalse(DistributedCache.checkURIs(new URI[] { new URI(\"file://foo/bar/myCacheFile1.txt#file\"), new URI(\"file://foo/bar/myCacheFile2.txt#FILE\") }, null));\r\n    assertFalse(DistributedCache.checkURIs(null, new URI[] { new URI(\"file://foo/bar/myCacheArchive1.txt#archive\"), new URI(\"file://foo/bar/myCacheArchive2.txt#ARCHIVE\") }));\r\n    assertFalse(DistributedCache.checkURIs(new URI[] { new URI(\"file://foo/bar/myCacheFile.txt#cache\") }, new URI[] { new URI(\"file://foo/bar/myCacheArchive.txt#CACHE\") }));\r\n    assertFalse(DistributedCache.checkURIs(new URI[] { new URI(\"file://foo/bar/myCacheFile1.txt#file1\"), new URI(\"file://foo/bar/myCacheFile2.txt#file2\") }, new URI[] { new URI(\"file://foo/bar/myCacheArchive1.txt#ARCHIVE\"), new URI(\"file://foo/bar/myCacheArchive2.txt#archive\") }));\r\n    assertFalse(DistributedCache.checkURIs(new URI[] { new URI(\"file://foo/bar/myCacheFile1.txt#FILE\"), new URI(\"file://foo/bar/myCacheFile2.txt#file\") }, new URI[] { new URI(\"file://foo/bar/myCacheArchive1.txt#archive1\"), new URI(\"file://foo/bar/myCacheArchive2.txt#archive2\") }));\r\n    assertFalse(DistributedCache.checkURIs(new URI[] { new URI(\"file://foo/bar/myCacheFile1.txt#file1\"), new URI(\"file://foo/bar/myCacheFile2.txt#CACHE\") }, new URI[] { new URI(\"file://foo/bar/myCacheArchive1.txt#cache\"), new URI(\"file://foo/bar/myCacheArchive2.txt#archive2\") }));\r\n    assertTrue(DistributedCache.checkURIs(new URI[] { new URI(\"file://foo/bar/myCacheFile1.txt#file1\"), new URI(\"file://foo/bar/myCacheFile2.txt#file2\") }, null));\r\n    assertTrue(DistributedCache.checkURIs(null, new URI[] { new URI(\"file://foo/bar/myCacheArchive1.txt#archive1\"), new URI(\"file://foo/bar/myCacheArchive2.txt#archive2\") }));\r\n    assertTrue(DistributedCache.checkURIs(new URI[] { new URI(\"file://foo/bar/myCacheFile1.txt#file1\"), new URI(\"file://foo/bar/myCacheFile2.txt#file2\") }, new URI[] { new URI(\"file://foo/bar/myCacheArchive1.txt#archive1\"), new URI(\"file://foo/bar/myCacheArchive2.txt#archive2\") }));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\map",
  "methodName" : "testOKRun",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testOKRun() throws Exception\n{\r\n    run(false, false);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\map",
  "methodName" : "testIOExRun",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testIOExRun() throws Exception\n{\r\n    run(true, false);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\map",
  "methodName" : "testRuntimeExRun",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testRuntimeExRun() throws Exception\n{\r\n    run(false, true);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\map",
  "methodName" : "run",
  "errType" : null,
  "containingMethodsNum" : 14,
  "sourceCodeText" : "void run(boolean ioEx, boolean rtEx) throws Exception\n{\r\n    String localPathRoot = System.getProperty(\"test.build.data\", \"/tmp\");\r\n    Path inDir = new Path(localPathRoot, \"testing/mt/input\");\r\n    Path outDir = new Path(localPathRoot, \"testing/mt/output\");\r\n    Configuration conf = createJobConf();\r\n    if (ioEx) {\r\n        conf.setBoolean(\"multithreaded.ioException\", true);\r\n    }\r\n    if (rtEx) {\r\n        conf.setBoolean(\"multithreaded.runtimeException\", true);\r\n    }\r\n    Job job = MapReduceTestUtil.createJob(conf, inDir, outDir, 1, 1);\r\n    job.setJobName(\"mt\");\r\n    job.setMapperClass(MultithreadedMapper.class);\r\n    MultithreadedMapper.setMapperClass(job, IDMap.class);\r\n    MultithreadedMapper.setNumberOfThreads(job, 2);\r\n    job.setReducerClass(Reducer.class);\r\n    job.waitForCompletion(true);\r\n    if (job.isSuccessful()) {\r\n        assertFalse(ioEx || rtEx);\r\n    } else {\r\n        assertTrue(ioEx || rtEx);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "testJobShell",
  "errType" : null,
  "containingMethodsNum" : 37,
  "sourceCodeText" : "void testJobShell() throws Exception\n{\r\n    MiniDFSCluster dfs = null;\r\n    MiniMRCluster mr = null;\r\n    FileSystem fs = null;\r\n    Path testFile = new Path(input, \"testfile\");\r\n    try {\r\n        Configuration conf = new Configuration();\r\n        dfs = new MiniDFSCluster.Builder(conf).numDataNodes(2).build();\r\n        fs = dfs.getFileSystem();\r\n        FSDataOutputStream stream = fs.create(testFile);\r\n        stream.write(\"teststring\".getBytes());\r\n        stream.close();\r\n        mr = new MiniMRCluster(2, fs.getUri().toString(), 1);\r\n        File thisbuildDir = new File(buildDir, \"jobCommand\");\r\n        assertTrue(\"create build dir\", thisbuildDir.mkdirs());\r\n        File f = new File(thisbuildDir, \"files_tmp\");\r\n        FileOutputStream fstream = new FileOutputStream(f);\r\n        fstream.write(\"somestrings\".getBytes());\r\n        fstream.close();\r\n        File f1 = new File(thisbuildDir, \"files_tmp1\");\r\n        fstream = new FileOutputStream(f1);\r\n        fstream.write(\"somestrings\".getBytes());\r\n        fstream.close();\r\n        Path cachePath = new Path(\"/cacheDir\");\r\n        if (!fs.mkdirs(cachePath)) {\r\n            throw new IOException(\"Mkdirs failed to create \" + cachePath.toString());\r\n        }\r\n        Path localCachePath = new Path(System.getProperty(\"test.cache.data\"));\r\n        Path txtPath = new Path(localCachePath, new Path(\"test.txt\"));\r\n        Path jarPath = new Path(localCachePath, new Path(\"test.jar\"));\r\n        Path zipPath = new Path(localCachePath, new Path(\"test.zip\"));\r\n        Path tarPath = new Path(localCachePath, new Path(\"test.tar\"));\r\n        Path tgzPath = new Path(localCachePath, new Path(\"test.tgz\"));\r\n        fs.copyFromLocalFile(txtPath, cachePath);\r\n        fs.copyFromLocalFile(jarPath, cachePath);\r\n        fs.copyFromLocalFile(zipPath, cachePath);\r\n        String[] files = new String[3];\r\n        files[0] = f.toString();\r\n        files[1] = f1.toString() + \"#localfilelink\";\r\n        files[2] = fs.getUri().resolve(cachePath + \"/test.txt#dfsfilelink\").toString();\r\n        String[] libjars = new String[2];\r\n        libjars[0] = \"build/test/mapred/testjar/testjob.jar\";\r\n        libjars[1] = fs.getUri().resolve(cachePath + \"/test.jar\").toString();\r\n        String[] archives = new String[3];\r\n        archives[0] = tgzPath.toString();\r\n        archives[1] = tarPath + \"#tarlink\";\r\n        archives[2] = fs.getUri().resolve(cachePath + \"/test.zip#ziplink\").toString();\r\n        String[] args = new String[10];\r\n        args[0] = \"-files\";\r\n        args[1] = StringUtils.arrayToString(files);\r\n        args[2] = \"-libjars\";\r\n        args[3] = StringUtils.arrayToString(libjars);\r\n        args[4] = \"-archives\";\r\n        args[5] = StringUtils.arrayToString(archives);\r\n        args[6] = \"-D\";\r\n        args[7] = \"mapred.output.committer.class=testjar.CustomOutputCommitter\";\r\n        args[8] = input.toString();\r\n        args[9] = output.toString();\r\n        JobConf jobConf = mr.createJobConf();\r\n        assertTrue(\"libjar not in client classpath\", loadLibJar(jobConf) == null);\r\n        int ret = ToolRunner.run(jobConf, new testshell.ExternalMapReduce(), args);\r\n        assertTrue(\"libjar added to client classpath\", loadLibJar(jobConf) != null);\r\n        assertTrue(\"not failed \", ret != -1);\r\n        f.delete();\r\n        thisbuildDir.delete();\r\n    } finally {\r\n        if (dfs != null) {\r\n            dfs.shutdown();\r\n        }\r\n        ;\r\n        if (mr != null) {\r\n            mr.shutdown();\r\n        }\r\n        ;\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "loadLibJar",
  "errType" : [ "ClassNotFoundException" ],
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Class loadLibJar(JobConf jobConf)\n{\r\n    try {\r\n        return jobConf.getClassByName(\"testjar.ClassWordCount\");\r\n    } catch (ClassNotFoundException e) {\r\n        return null;\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "testFieldSelection",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testFieldSelection() throws Exception\n{\r\n    launch();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "launch",
  "errType" : null,
  "containingMethodsNum" : 28,
  "sourceCodeText" : "void launch() throws Exception\n{\r\n    JobConf conf = new JobConf(TestFieldSelection.class);\r\n    FileSystem fs = FileSystem.get(conf);\r\n    int numOfInputLines = 10;\r\n    String baseDir = System.getProperty(\"test.build.data\", \"build/test/data\");\r\n    Path OUTPUT_DIR = new Path(baseDir + \"/output_for_field_selection_test\");\r\n    Path INPUT_DIR = new Path(baseDir + \"/input_for_field_selection_test\");\r\n    String inputFile = \"input.txt\";\r\n    fs.delete(INPUT_DIR, true);\r\n    fs.mkdirs(INPUT_DIR);\r\n    fs.delete(OUTPUT_DIR, true);\r\n    StringBuffer inputData = new StringBuffer();\r\n    StringBuffer expectedOutput = new StringBuffer();\r\n    TestMRFieldSelection.constructInputOutputData(inputData, expectedOutput, numOfInputLines);\r\n    FSDataOutputStream fileOut = fs.create(new Path(INPUT_DIR, inputFile));\r\n    fileOut.write(inputData.toString().getBytes(\"utf-8\"));\r\n    fileOut.close();\r\n    System.out.println(\"inputData:\");\r\n    System.out.println(inputData.toString());\r\n    JobConf job = new JobConf(conf, TestFieldSelection.class);\r\n    FileInputFormat.setInputPaths(job, INPUT_DIR);\r\n    job.setInputFormat(TextInputFormat.class);\r\n    job.setMapperClass(FieldSelectionMapReduce.class);\r\n    job.setReducerClass(FieldSelectionMapReduce.class);\r\n    FileOutputFormat.setOutputPath(job, OUTPUT_DIR);\r\n    job.setOutputKeyClass(Text.class);\r\n    job.setOutputValueClass(Text.class);\r\n    job.setOutputFormat(TextOutputFormat.class);\r\n    job.setNumReduceTasks(1);\r\n    job.set(FieldSelectionHelper.DATA_FIELD_SEPARATOR, \"-\");\r\n    job.set(FieldSelectionHelper.MAP_OUTPUT_KEY_VALUE_SPEC, \"6,5,1-3:0-\");\r\n    job.set(FieldSelectionHelper.REDUCE_OUTPUT_KEY_VALUE_SPEC, \":4,3,2,1,0,0-\");\r\n    JobClient.runJob(job);\r\n    boolean success = true;\r\n    Path outPath = new Path(OUTPUT_DIR, \"part-00000\");\r\n    String outdata = MapReduceTestUtil.readOutput(outPath, job);\r\n    assertEquals(expectedOutput.toString(), outdata);\r\n    fs.delete(OUTPUT_DIR, true);\r\n    fs.delete(INPUT_DIR, true);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "main",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void main(String[] argv) throws Exception\n{\r\n    launch();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "createInputFile",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void createInputFile(Path rootDir) throws IOException\n{\r\n    if (fs.exists(rootDir)) {\r\n        fs.delete(rootDir, true);\r\n    }\r\n    String str = \"The quick brown fox\\n\" + \"The brown quick fox\\n\" + \"The fox brown quick\\n\";\r\n    DataOutputStream inpFile = fs.create(new Path(rootDir, \"part-0\"));\r\n    inpFile.writeBytes(str);\r\n    inpFile.close();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "testMapProgress",
  "errType" : null,
  "containingMethodsNum" : 18,
  "sourceCodeText" : "void testMapProgress() throws Exception\n{\r\n    JobConf job = new JobConf();\r\n    fs = FileSystem.getLocal(job);\r\n    Path rootDir = new Path(TEST_ROOT_DIR);\r\n    createInputFile(rootDir);\r\n    job.setNumReduceTasks(0);\r\n    TaskAttemptID taskId = TaskAttemptID.forName(\"attempt_200907082313_0424_m_000000_0\");\r\n    job.setClass(\"mapreduce.job.outputformat.class\", NullOutputFormat.class, OutputFormat.class);\r\n    job.set(org.apache.hadoop.mapreduce.lib.input.FileInputFormat.INPUT_DIR, TEST_ROOT_DIR);\r\n    jobId = taskId.getJobID();\r\n    JobContext jContext = new JobContextImpl(job, jobId);\r\n    InputFormat<?, ?> input = ReflectionUtils.newInstance(jContext.getInputFormatClass(), job);\r\n    List<InputSplit> splits = input.getSplits(jContext);\r\n    JobSplitWriter.createSplitFiles(new Path(TEST_ROOT_DIR), job, new Path(TEST_ROOT_DIR).getFileSystem(job), splits);\r\n    TaskSplitMetaInfo[] splitMetaInfo = SplitMetaInfoReader.readSplitMetaInfo(jobId, fs, job, new Path(TEST_ROOT_DIR));\r\n    job.setUseNewMapper(true);\r\n    for (int i = 0; i < splitMetaInfo.length; i++) {\r\n        map = new TestMapTask(job.get(JTConfig.JT_SYSTEM_DIR, \"/tmp/hadoop/mapred/system\") + jobId + \"job.xml\", taskId, i, splitMetaInfo[i].getSplitIndex(), 1);\r\n        JobConf localConf = new JobConf(job);\r\n        map.localizeConfiguration(localConf);\r\n        map.setConf(localConf);\r\n        map.run(localConf, fakeUmbilical);\r\n    }\r\n    fs.delete(rootDir, true);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\fs\\slive",
  "methodName" : "getConfig",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Configuration getConfig()\n{\r\n    return this.config;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\fs\\slive",
  "methodName" : "getDataPath",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Path getDataPath()\n{\r\n    Path base = getBaseDirectory();\r\n    if (base == null) {\r\n        return null;\r\n    }\r\n    return new Path(base, Constants.DATA_DIR);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\fs\\slive",
  "methodName" : "getOutputPath",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Path getOutputPath()\n{\r\n    Path base = getBaseDirectory();\r\n    if (base == null) {\r\n        return null;\r\n    }\r\n    return new Path(base, Constants.OUTPUT_DIR);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\fs\\slive",
  "methodName" : "getBaseDirectory",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "Path getBaseDirectory(String primary)\n{\r\n    String path = primary;\r\n    if (path == null) {\r\n        path = config.get(ConfigOption.BASE_DIR.getCfgOption());\r\n    }\r\n    if (path == null) {\r\n        path = ConfigOption.BASE_DIR.getDefault();\r\n    }\r\n    if (path == null) {\r\n        return null;\r\n    }\r\n    return new Path(path);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\fs\\slive",
  "methodName" : "getBaseDirectory",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Path getBaseDirectory()\n{\r\n    return getBaseDirectory(null);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\fs\\slive",
  "methodName" : "shouldExitOnFirstError",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "boolean shouldExitOnFirstError()\n{\r\n    return shouldExitOnFirstError(null);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\fs\\slive",
  "methodName" : "shouldExitOnFirstError",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "boolean shouldExitOnFirstError(String primary)\n{\r\n    String val = primary;\r\n    if (val == null) {\r\n        val = config.get(ConfigOption.EXIT_ON_ERROR.getCfgOption());\r\n    }\r\n    if (val == null) {\r\n        return ConfigOption.EXIT_ON_ERROR.getDefault();\r\n    }\r\n    return Boolean.parseBoolean(val);\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\fs\\slive",
  "methodName" : "shouldWaitOnTruncate",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "boolean shouldWaitOnTruncate()\n{\r\n    return shouldWaitOnTruncate(null);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\fs\\slive",
  "methodName" : "shouldWaitOnTruncate",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "boolean shouldWaitOnTruncate(String primary)\n{\r\n    String val = primary;\r\n    if (val == null) {\r\n        val = config.get(ConfigOption.EXIT_ON_ERROR.getCfgOption());\r\n    }\r\n    if (val == null) {\r\n        return ConfigOption.EXIT_ON_ERROR.getDefault();\r\n    }\r\n    return Boolean.parseBoolean(val);\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\fs\\slive",
  "methodName" : "getReducerAmount",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Integer getReducerAmount()\n{\r\n    return getInteger(null, ConfigOption.REDUCES);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\fs\\slive",
  "methodName" : "getMapAmount",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Integer getMapAmount()\n{\r\n    return getMapAmount(null);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\fs\\slive",
  "methodName" : "getMapAmount",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Integer getMapAmount(String primary)\n{\r\n    return getInteger(primary, ConfigOption.MAPS);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\fs\\slive",
  "methodName" : "getDuration",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Integer getDuration()\n{\r\n    return getDuration(null);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\fs\\slive",
  "methodName" : "getDurationMilliseconds",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Integer getDurationMilliseconds()\n{\r\n    Integer seconds = getDuration();\r\n    if (seconds == null || seconds == Integer.MAX_VALUE) {\r\n        return Integer.MAX_VALUE;\r\n    }\r\n    int milliseconds = (seconds * 1000);\r\n    if (milliseconds < 0) {\r\n        milliseconds = 0;\r\n    }\r\n    return milliseconds;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\fs\\slive",
  "methodName" : "getDuration",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Integer getDuration(String primary)\n{\r\n    return getInteger(primary, ConfigOption.DURATION);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\fs\\slive",
  "methodName" : "getOpCount",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Integer getOpCount()\n{\r\n    return getOpCount(null);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\fs\\slive",
  "methodName" : "getOpCount",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Integer getOpCount(String primary)\n{\r\n    return getInteger(primary, ConfigOption.OPS);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\fs\\slive",
  "methodName" : "getDirSize",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Integer getDirSize()\n{\r\n    return getDirSize(null);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\fs\\slive",
  "methodName" : "getDirSize",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Integer getDirSize(String primary)\n{\r\n    return getInteger(primary, ConfigOption.DIR_SIZE);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\fs\\slive",
  "methodName" : "getInteger",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "Integer getInteger(String primary, ConfigOption<Integer> opt)\n{\r\n    String value = primary;\r\n    if (value == null) {\r\n        value = config.get(opt.getCfgOption());\r\n    }\r\n    if (value == null) {\r\n        return opt.getDefault();\r\n    }\r\n    return Integer.parseInt(value);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\fs\\slive",
  "methodName" : "getTotalFiles",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Integer getTotalFiles()\n{\r\n    return getTotalFiles(null);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\fs\\slive",
  "methodName" : "getTotalFiles",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Integer getTotalFiles(String primary)\n{\r\n    return getInteger(primary, ConfigOption.FILES);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\fs\\slive",
  "methodName" : "getRandomSeed",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "Long getRandomSeed(String primary)\n{\r\n    String seed = primary;\r\n    if (seed == null) {\r\n        seed = config.get(ConfigOption.RANDOM_SEED.getCfgOption());\r\n    }\r\n    if (seed == null) {\r\n        return null;\r\n    }\r\n    return Long.parseLong(seed);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\fs\\slive",
  "methodName" : "getRandomSeed",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Long getRandomSeed()\n{\r\n    return getRandomSeed(null);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\fs\\slive",
  "methodName" : "getResultFile",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String getResultFile()\n{\r\n    return getResultFile(null);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\fs\\slive",
  "methodName" : "getQueueName",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String getQueueName()\n{\r\n    return getQueueName(null);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\fs\\slive",
  "methodName" : "getQueueName",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "String getQueueName(String primary)\n{\r\n    String q = primary;\r\n    if (q == null) {\r\n        q = config.get(ConfigOption.QUEUE_NAME.getCfgOption());\r\n    }\r\n    if (q == null) {\r\n        q = ConfigOption.QUEUE_NAME.getDefault();\r\n    }\r\n    return q;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\fs\\slive",
  "methodName" : "getResultFile",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "String getResultFile(String primary)\n{\r\n    String fn = primary;\r\n    if (fn == null) {\r\n        fn = config.get(ConfigOption.RESULT_FILE.getCfgOption());\r\n    }\r\n    if (fn == null) {\r\n        fn = ConfigOption.RESULT_FILE.getDefault();\r\n    }\r\n    return fn;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\fs\\slive",
  "methodName" : "getBlockSize",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Range<Long> getBlockSize(String primary)\n{\r\n    return getMinMaxBytes(ConfigOption.BLOCK_SIZE, primary);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\fs\\slive",
  "methodName" : "getBlockSize",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Range<Long> getBlockSize()\n{\r\n    return getBlockSize(null);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\fs\\slive",
  "methodName" : "getMinMaxShort",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "Range<Short> getMinMaxShort(ConfigOption<Short> cfgopt, String primary)\n{\r\n    String sval = primary;\r\n    if (sval == null) {\r\n        sval = config.get(cfgopt.getCfgOption());\r\n    }\r\n    Range<Short> range = null;\r\n    if (sval != null) {\r\n        String[] pieces = Helper.getTrimmedStrings(sval);\r\n        if (pieces.length == 2) {\r\n            String min = pieces[0];\r\n            String max = pieces[1];\r\n            short minVal = Short.parseShort(min);\r\n            short maxVal = Short.parseShort(max);\r\n            if (minVal > maxVal) {\r\n                short tmp = minVal;\r\n                minVal = maxVal;\r\n                maxVal = tmp;\r\n            }\r\n            range = new Range<Short>(minVal, maxVal);\r\n        }\r\n    }\r\n    if (range == null) {\r\n        Short def = cfgopt.getDefault();\r\n        if (def != null) {\r\n            range = new Range<Short>(def, def);\r\n        }\r\n    }\r\n    return range;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\fs\\slive",
  "methodName" : "getMinMaxLong",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "Range<Long> getMinMaxLong(ConfigOption<Long> cfgopt, String primary)\n{\r\n    String sval = primary;\r\n    if (sval == null) {\r\n        sval = config.get(cfgopt.getCfgOption());\r\n    }\r\n    Range<Long> range = null;\r\n    if (sval != null) {\r\n        String[] pieces = Helper.getTrimmedStrings(sval);\r\n        if (pieces.length == 2) {\r\n            String min = pieces[0];\r\n            String max = pieces[1];\r\n            long minVal = Long.parseLong(min);\r\n            long maxVal = Long.parseLong(max);\r\n            if (minVal > maxVal) {\r\n                long tmp = minVal;\r\n                minVal = maxVal;\r\n                maxVal = tmp;\r\n            }\r\n            range = new Range<Long>(minVal, maxVal);\r\n        }\r\n    }\r\n    if (range == null) {\r\n        Long def = cfgopt.getDefault();\r\n        if (def != null) {\r\n            range = new Range<Long>(def, def);\r\n        }\r\n    }\r\n    return range;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\fs\\slive",
  "methodName" : "getMinMaxBytes",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "Range<Long> getMinMaxBytes(ConfigOption<Long> cfgopt, String primary)\n{\r\n    String sval = primary;\r\n    if (sval == null) {\r\n        sval = config.get(cfgopt.getCfgOption());\r\n    }\r\n    Range<Long> range = null;\r\n    if (sval != null) {\r\n        String[] pieces = Helper.getTrimmedStrings(sval);\r\n        if (pieces.length == 2) {\r\n            String min = pieces[0];\r\n            String max = pieces[1];\r\n            long tMin = StringUtils.TraditionalBinaryPrefix.string2long(min);\r\n            long tMax = StringUtils.TraditionalBinaryPrefix.string2long(max);\r\n            if (tMin > tMax) {\r\n                long tmp = tMin;\r\n                tMin = tMax;\r\n                tMax = tmp;\r\n            }\r\n            range = new Range<Long>(tMin, tMax);\r\n        }\r\n    }\r\n    if (range == null) {\r\n        Long def = cfgopt.getDefault();\r\n        if (def != null) {\r\n            range = new Range<Long>(def, def);\r\n        }\r\n    }\r\n    return range;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\fs\\slive",
  "methodName" : "getReplication",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Range<Short> getReplication(String primary)\n{\r\n    return getMinMaxShort(ConfigOption.REPLICATION_AM, primary);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\fs\\slive",
  "methodName" : "getReplication",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Range<Short> getReplication()\n{\r\n    return getReplication(null);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\fs\\slive",
  "methodName" : "getOperations",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "Map<OperationType, OperationData> getOperations()\n{\r\n    Map<OperationType, OperationData> operations = new HashMap<OperationType, OperationData>();\r\n    for (OperationType type : OperationType.values()) {\r\n        String opname = type.lowerName();\r\n        String keyname = String.format(Constants.OP, opname);\r\n        String kval = config.get(keyname);\r\n        if (kval == null) {\r\n            continue;\r\n        }\r\n        operations.put(type, new OperationData(kval));\r\n    }\r\n    return operations;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\fs\\slive",
  "methodName" : "getAppendSize",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Range<Long> getAppendSize(String primary)\n{\r\n    return getMinMaxBytes(ConfigOption.APPEND_SIZE, primary);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\fs\\slive",
  "methodName" : "getAppendSize",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Range<Long> getAppendSize()\n{\r\n    return getAppendSize(null);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\fs\\slive",
  "methodName" : "getTruncateSize",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Range<Long> getTruncateSize(String primary)\n{\r\n    return getMinMaxBytes(ConfigOption.TRUNCATE_SIZE, primary);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\fs\\slive",
  "methodName" : "getTruncateSize",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Range<Long> getTruncateSize()\n{\r\n    return getTruncateSize(null);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\fs\\slive",
  "methodName" : "getSleepRange",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Range<Long> getSleepRange(String primary)\n{\r\n    return getMinMaxLong(ConfigOption.SLEEP_TIME, primary);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\fs\\slive",
  "methodName" : "getSleepRange",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Range<Long> getSleepRange()\n{\r\n    return getSleepRange(null);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\fs\\slive",
  "methodName" : "getWriteSize",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Range<Long> getWriteSize(String primary)\n{\r\n    return getMinMaxBytes(ConfigOption.WRITE_SIZE, primary);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\fs\\slive",
  "methodName" : "getWriteSize",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Range<Long> getWriteSize()\n{\r\n    return getWriteSize(null);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\fs\\slive",
  "methodName" : "shouldWriteUseBlockSize",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "boolean shouldWriteUseBlockSize()\n{\r\n    Range<Long> writeRange = getWriteSize();\r\n    if (writeRange == null || (writeRange.getLower() == writeRange.getUpper() && (writeRange.getUpper() == Long.MAX_VALUE))) {\r\n        return true;\r\n    }\r\n    return false;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\fs\\slive",
  "methodName" : "shouldAppendUseBlockSize",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "boolean shouldAppendUseBlockSize()\n{\r\n    Range<Long> appendRange = getAppendSize();\r\n    if (appendRange == null || (appendRange.getLower() == appendRange.getUpper() && (appendRange.getUpper() == Long.MAX_VALUE))) {\r\n        return true;\r\n    }\r\n    return false;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\fs\\slive",
  "methodName" : "shouldTruncateUseBlockSize",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "boolean shouldTruncateUseBlockSize()\n{\r\n    Range<Long> truncateRange = getTruncateSize();\r\n    if (truncateRange == null || (truncateRange.getLower() == truncateRange.getUpper() && (truncateRange.getUpper() == Long.MAX_VALUE))) {\r\n        return true;\r\n    }\r\n    return false;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\fs\\slive",
  "methodName" : "shouldReadFullFile",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "boolean shouldReadFullFile()\n{\r\n    Range<Long> readRange = getReadSize();\r\n    if (readRange == null || (readRange.getLower() == readRange.getUpper() && (readRange.getUpper() == Long.MAX_VALUE))) {\r\n        return true;\r\n    }\r\n    return false;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\fs\\slive",
  "methodName" : "getReadSize",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Range<Long> getReadSize(String primary)\n{\r\n    return getMinMaxBytes(ConfigOption.READ_SIZE, primary);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\fs\\slive",
  "methodName" : "getByteCheckSum",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "Long getByteCheckSum()\n{\r\n    String val = config.get(Constants.BYTES_PER_CHECKSUM);\r\n    if (val == null) {\r\n        return null;\r\n    }\r\n    return Long.parseLong(val);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\fs\\slive",
  "methodName" : "getReadSize",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Range<Long> getReadSize()\n{\r\n    return getReadSize(null);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\fs\\slive",
  "methodName" : "dumpOptions",
  "errType" : null,
  "containingMethodsNum" : 41,
  "sourceCodeText" : "void dumpOptions(ConfigExtractor cfg)\n{\r\n    if (cfg == null) {\r\n        return;\r\n    }\r\n    LOG.info(\"Base directory = \" + cfg.getBaseDirectory());\r\n    LOG.info(\"Data directory = \" + cfg.getDataPath());\r\n    LOG.info(\"Output directory = \" + cfg.getOutputPath());\r\n    LOG.info(\"Result file = \" + cfg.getResultFile());\r\n    LOG.info(\"Grid queue = \" + cfg.getQueueName());\r\n    LOG.info(\"Should exit on first error = \" + cfg.shouldExitOnFirstError());\r\n    {\r\n        String duration = \"Duration = \";\r\n        if (cfg.getDurationMilliseconds() == Integer.MAX_VALUE) {\r\n            duration += \"unlimited\";\r\n        } else {\r\n            duration += cfg.getDurationMilliseconds() + \" milliseconds\";\r\n        }\r\n        LOG.info(duration);\r\n    }\r\n    LOG.info(\"Map amount = \" + cfg.getMapAmount());\r\n    LOG.info(\"Reducer amount = \" + cfg.getReducerAmount());\r\n    LOG.info(\"Operation amount = \" + cfg.getOpCount());\r\n    LOG.info(\"Total file limit = \" + cfg.getTotalFiles());\r\n    LOG.info(\"Total dir file limit = \" + cfg.getDirSize());\r\n    {\r\n        String read = \"Read size = \";\r\n        if (cfg.shouldReadFullFile()) {\r\n            read += \"entire file\";\r\n        } else {\r\n            read += cfg.getReadSize() + \" bytes\";\r\n        }\r\n        LOG.info(read);\r\n    }\r\n    {\r\n        String write = \"Write size = \";\r\n        if (cfg.shouldWriteUseBlockSize()) {\r\n            write += \"blocksize\";\r\n        } else {\r\n            write += cfg.getWriteSize() + \" bytes\";\r\n        }\r\n        LOG.info(write);\r\n    }\r\n    {\r\n        String append = \"Append size = \";\r\n        if (cfg.shouldAppendUseBlockSize()) {\r\n            append += \"blocksize\";\r\n        } else {\r\n            append += cfg.getAppendSize() + \" bytes\";\r\n        }\r\n        LOG.info(append);\r\n    }\r\n    {\r\n        String bsize = \"Block size = \";\r\n        bsize += cfg.getBlockSize() + \" bytes\";\r\n        LOG.info(bsize);\r\n    }\r\n    if (cfg.getRandomSeed() != null) {\r\n        LOG.info(\"Random seed = \" + cfg.getRandomSeed());\r\n    }\r\n    if (cfg.getSleepRange() != null) {\r\n        LOG.info(\"Sleep range = \" + cfg.getSleepRange() + \" milliseconds\");\r\n    }\r\n    LOG.info(\"Replication amount = \" + cfg.getReplication());\r\n    LOG.info(\"Operations are:\");\r\n    NumberFormat percFormatter = Formatter.getPercentFormatter();\r\n    Map<OperationType, OperationData> operations = cfg.getOperations();\r\n    for (OperationType type : operations.keySet()) {\r\n        String name = type.name();\r\n        LOG.info(name);\r\n        OperationData opInfo = operations.get(type);\r\n        LOG.info(\" \" + opInfo.getDistribution().name());\r\n        if (opInfo.getPercent() != null) {\r\n            LOG.info(\" \" + percFormatter.format(opInfo.getPercent()));\r\n        } else {\r\n            LOG.info(\" ???\");\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "onlyOnce",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void onlyOnce()\n{\r\n    try {\r\n        defaultConf = new Configuration();\r\n        defaultConf.set(\"fs.defaultFS\", \"file:///\");\r\n        localFs = FileSystem.getLocal(defaultConf);\r\n        voidReporter = Reporter.NULL;\r\n        chars = (\"abcdefghijklmnopqrstuvABCDEFGHIJKLMN OPQRSTUVWXYZ1234567890)\" + \"(*&^%$#@!-=><?:\\\"{}][';/.,']\").toCharArray();\r\n        workDir = new Path(new Path(System.getProperty(\"test.build.data\", \".\"), \"data\"), \"TestKeyValueFixedLengthInputFormat\");\r\n        charRand = new Random();\r\n    } catch (IOException e) {\r\n        throw new RuntimeException(\"init failure\", e);\r\n    }\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "testFormat",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testFormat() throws IOException\n{\r\n    runRandomTests(null);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "testFormatCompressedIn",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testFormatCompressedIn() throws IOException\n{\r\n    runRandomTests(new GzipCodec());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "testNoRecordLength",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void testNoRecordLength() throws IOException\n{\r\n    localFs.delete(workDir, true);\r\n    Path file = new Path(workDir, new String(\"testFormat.txt\"));\r\n    createFile(file, null, 10, 10);\r\n    JobConf job = new JobConf(defaultConf);\r\n    FileInputFormat.setInputPaths(job, workDir);\r\n    FixedLengthInputFormat format = new FixedLengthInputFormat();\r\n    format.configure(job);\r\n    InputSplit[] splits = format.getSplits(job, 1);\r\n    boolean exceptionThrown = false;\r\n    for (InputSplit split : splits) {\r\n        try {\r\n            RecordReader<LongWritable, BytesWritable> reader = format.getRecordReader(split, job, voidReporter);\r\n        } catch (IOException ioe) {\r\n            exceptionThrown = true;\r\n            LOG.info(\"Exception message:\" + ioe.getMessage());\r\n        }\r\n    }\r\n    assertTrue(\"Exception for not setting record length:\", exceptionThrown);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "testZeroRecordLength",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 9,
  "sourceCodeText" : "void testZeroRecordLength() throws IOException\n{\r\n    localFs.delete(workDir, true);\r\n    Path file = new Path(workDir, new String(\"testFormat.txt\"));\r\n    createFile(file, null, 10, 10);\r\n    JobConf job = new JobConf(defaultConf);\r\n    FileInputFormat.setInputPaths(job, workDir);\r\n    FixedLengthInputFormat format = new FixedLengthInputFormat();\r\n    format.setRecordLength(job, 0);\r\n    format.configure(job);\r\n    InputSplit[] splits = format.getSplits(job, 1);\r\n    boolean exceptionThrown = false;\r\n    for (InputSplit split : splits) {\r\n        try {\r\n            RecordReader<LongWritable, BytesWritable> reader = format.getRecordReader(split, job, voidReporter);\r\n        } catch (IOException ioe) {\r\n            exceptionThrown = true;\r\n            LOG.info(\"Exception message:\" + ioe.getMessage());\r\n        }\r\n    }\r\n    assertTrue(\"Exception for zero record length:\", exceptionThrown);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "testNegativeRecordLength",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 9,
  "sourceCodeText" : "void testNegativeRecordLength() throws IOException\n{\r\n    localFs.delete(workDir, true);\r\n    Path file = new Path(workDir, new String(\"testFormat.txt\"));\r\n    createFile(file, null, 10, 10);\r\n    JobConf job = new JobConf(defaultConf);\r\n    FileInputFormat.setInputPaths(job, workDir);\r\n    FixedLengthInputFormat format = new FixedLengthInputFormat();\r\n    format.setRecordLength(job, -10);\r\n    format.configure(job);\r\n    InputSplit[] splits = format.getSplits(job, 1);\r\n    boolean exceptionThrown = false;\r\n    for (InputSplit split : splits) {\r\n        try {\r\n            RecordReader<LongWritable, BytesWritable> reader = format.getRecordReader(split, job, voidReporter);\r\n        } catch (IOException ioe) {\r\n            exceptionThrown = true;\r\n            LOG.info(\"Exception message:\" + ioe.getMessage());\r\n        }\r\n    }\r\n    assertTrue(\"Exception for negative record length:\", exceptionThrown);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "testPartialRecordCompressedIn",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testPartialRecordCompressedIn() throws IOException\n{\r\n    CompressionCodec gzip = new GzipCodec();\r\n    runPartialRecordTest(gzip);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "testPartialRecordUncompressedIn",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testPartialRecordUncompressedIn() throws IOException\n{\r\n    runPartialRecordTest(null);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "testGzipWithTwoInputs",
  "errType" : null,
  "containingMethodsNum" : 17,
  "sourceCodeText" : "void testGzipWithTwoInputs() throws IOException\n{\r\n    CompressionCodec gzip = new GzipCodec();\r\n    localFs.delete(workDir, true);\r\n    FixedLengthInputFormat format = new FixedLengthInputFormat();\r\n    JobConf job = new JobConf(defaultConf);\r\n    format.setRecordLength(job, 5);\r\n    FileInputFormat.setInputPaths(job, workDir);\r\n    ReflectionUtils.setConf(gzip, job);\r\n    format.configure(job);\r\n    writeFile(localFs, new Path(workDir, \"part1.txt.gz\"), gzip, \"one  two  threefour five six  seveneightnine ten  \");\r\n    writeFile(localFs, new Path(workDir, \"part2.txt.gz\"), gzip, \"ten  nine eightsevensix  five four threetwo  one  \");\r\n    InputSplit[] splits = format.getSplits(job, 100);\r\n    assertEquals(\"compressed splits == 2\", 2, splits.length);\r\n    FileSplit tmp = (FileSplit) splits[0];\r\n    if (tmp.getPath().getName().equals(\"part2.txt.gz\")) {\r\n        splits[0] = splits[1];\r\n        splits[1] = tmp;\r\n    }\r\n    List<String> results = readSplit(format, splits[0], job);\r\n    assertEquals(\"splits[0] length\", 10, results.size());\r\n    assertEquals(\"splits[0][5]\", \"six  \", results.get(5));\r\n    results = readSplit(format, splits[1], job);\r\n    assertEquals(\"splits[1] length\", 10, results.size());\r\n    assertEquals(\"splits[1][0]\", \"ten  \", results.get(0));\r\n    assertEquals(\"splits[1][1]\", \"nine \", results.get(1));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "createFile",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "ArrayList<String> createFile(Path targetFile, CompressionCodec codec, int recordLen, int numRecords) throws IOException\n{\r\n    ArrayList<String> recordList = new ArrayList<String>(numRecords);\r\n    OutputStream ostream = localFs.create(targetFile);\r\n    if (codec != null) {\r\n        ostream = codec.createOutputStream(ostream);\r\n    }\r\n    Writer writer = new OutputStreamWriter(ostream);\r\n    try {\r\n        StringBuffer sb = new StringBuffer();\r\n        for (int i = 0; i < numRecords; i++) {\r\n            for (int j = 0; j < recordLen; j++) {\r\n                sb.append(chars[charRand.nextInt(chars.length)]);\r\n            }\r\n            String recordData = sb.toString();\r\n            recordList.add(recordData);\r\n            writer.write(recordData);\r\n            sb.setLength(0);\r\n        }\r\n    } finally {\r\n        writer.close();\r\n    }\r\n    return recordList;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "runRandomTests",
  "errType" : null,
  "containingMethodsNum" : 34,
  "sourceCodeText" : "void runRandomTests(CompressionCodec codec) throws IOException\n{\r\n    StringBuilder fileName = new StringBuilder(\"testFormat.txt\");\r\n    if (codec != null) {\r\n        fileName.append(\".gz\");\r\n    }\r\n    localFs.delete(workDir, true);\r\n    Path file = new Path(workDir, fileName.toString());\r\n    int seed = new Random().nextInt();\r\n    LOG.info(\"Seed = \" + seed);\r\n    Random random = new Random(seed);\r\n    int MAX_TESTS = 20;\r\n    LongWritable key = new LongWritable();\r\n    BytesWritable value = new BytesWritable();\r\n    for (int i = 0; i < MAX_TESTS; i++) {\r\n        LOG.info(\"----------------------------------------------------------\");\r\n        int totalRecords = random.nextInt(999) + 1;\r\n        if (i == 8) {\r\n            totalRecords = 0;\r\n        }\r\n        int recordLength = random.nextInt(1024 * 100) + 1;\r\n        if (i == 10) {\r\n            recordLength = 1;\r\n        }\r\n        int fileSize = (totalRecords * recordLength);\r\n        LOG.info(\"totalRecords=\" + totalRecords + \" recordLength=\" + recordLength);\r\n        JobConf job = new JobConf(defaultConf);\r\n        if (codec != null) {\r\n            ReflectionUtils.setConf(codec, job);\r\n        }\r\n        ArrayList<String> recordList = createFile(file, codec, recordLength, totalRecords);\r\n        assertTrue(localFs.exists(file));\r\n        FixedLengthInputFormat.setRecordLength(job, recordLength);\r\n        int numSplits = 1;\r\n        if (i > 0) {\r\n            if (i == (MAX_TESTS - 1)) {\r\n                numSplits = (int) (fileSize / Math.max(1, Math.floor(recordLength / 2)));\r\n            } else {\r\n                if (MAX_TESTS % i == 0) {\r\n                    numSplits = fileSize / (fileSize - random.nextInt(fileSize));\r\n                } else {\r\n                    numSplits = Math.max(1, fileSize / random.nextInt(Integer.MAX_VALUE));\r\n                }\r\n            }\r\n            LOG.info(\"Number of splits set to: \" + numSplits);\r\n        }\r\n        FileInputFormat.setInputPaths(job, workDir);\r\n        FixedLengthInputFormat format = new FixedLengthInputFormat();\r\n        format.configure(job);\r\n        InputSplit[] splits = format.getSplits(job, numSplits);\r\n        LOG.info(\"Actual number of splits = \" + splits.length);\r\n        long recordOffset = 0;\r\n        int recordNumber = 0;\r\n        for (InputSplit split : splits) {\r\n            RecordReader<LongWritable, BytesWritable> reader = format.getRecordReader(split, job, voidReporter);\r\n            Class<?> clazz = reader.getClass();\r\n            assertEquals(\"RecordReader class should be FixedLengthRecordReader:\", FixedLengthRecordReader.class, clazz);\r\n            while (reader.next(key, value)) {\r\n                assertEquals(\"Checking key\", (long) (recordNumber * recordLength), key.get());\r\n                String valueString = new String(value.getBytes(), 0, value.getLength());\r\n                assertEquals(\"Checking record length:\", recordLength, value.getLength());\r\n                assertTrue(\"Checking for more records than expected:\", recordNumber < totalRecords);\r\n                String origRecord = recordList.get(recordNumber);\r\n                assertEquals(\"Checking record content:\", origRecord, valueString);\r\n                recordNumber++;\r\n            }\r\n            reader.close();\r\n        }\r\n        assertEquals(\"Total original records should be total read records:\", recordList.size(), recordNumber);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "writeFile",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void writeFile(FileSystem fs, Path name, CompressionCodec codec, String contents) throws IOException\n{\r\n    OutputStream stm;\r\n    if (codec == null) {\r\n        stm = fs.create(name);\r\n    } else {\r\n        stm = codec.createOutputStream(fs.create(name));\r\n    }\r\n    stm.write(contents.getBytes());\r\n    stm.close();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "readSplit",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "List<String> readSplit(FixedLengthInputFormat format, InputSplit split, JobConf job) throws IOException\n{\r\n    List<String> result = new ArrayList<String>();\r\n    RecordReader<LongWritable, BytesWritable> reader = format.getRecordReader(split, job, voidReporter);\r\n    LongWritable key = reader.createKey();\r\n    BytesWritable value = reader.createValue();\r\n    try {\r\n        while (reader.next(key, value)) {\r\n            result.add(new String(value.getBytes(), 0, value.getLength()));\r\n        }\r\n    } finally {\r\n        reader.close();\r\n    }\r\n    return result;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "runPartialRecordTest",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 12,
  "sourceCodeText" : "void runPartialRecordTest(CompressionCodec codec) throws IOException\n{\r\n    localFs.delete(workDir, true);\r\n    StringBuilder fileName = new StringBuilder(\"testFormat.txt\");\r\n    if (codec != null) {\r\n        fileName.append(\".gz\");\r\n    }\r\n    FixedLengthInputFormat format = new FixedLengthInputFormat();\r\n    JobConf job = new JobConf(defaultConf);\r\n    format.setRecordLength(job, 5);\r\n    FileInputFormat.setInputPaths(job, workDir);\r\n    if (codec != null) {\r\n        ReflectionUtils.setConf(codec, job);\r\n    }\r\n    format.configure(job);\r\n    writeFile(localFs, new Path(workDir, fileName.toString()), codec, \"one  two  threefour five six  seveneightnine ten\");\r\n    InputSplit[] splits = format.getSplits(job, 100);\r\n    if (codec != null) {\r\n        assertEquals(\"compressed splits == 1\", 1, splits.length);\r\n    }\r\n    boolean exceptionThrown = false;\r\n    for (InputSplit split : splits) {\r\n        try {\r\n            List<String> results = readSplit(format, split, job);\r\n        } catch (IOException ioe) {\r\n            exceptionThrown = true;\r\n            LOG.info(\"Exception message:\" + ioe.getMessage());\r\n        }\r\n    }\r\n    assertTrue(\"Exception for partial record:\", exceptionThrown);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "testCustomCollect",
  "errType" : null,
  "containingMethodsNum" : 9,
  "sourceCodeText" : "void testCustomCollect() throws Throwable\n{\r\n    TaskReporter mockTaskReporter = mock(TaskReporter.class);\r\n    @SuppressWarnings(\"unchecked\")\r\n    Writer<String, Integer> mockWriter = mock(Writer.class);\r\n    Configuration conf = new Configuration();\r\n    conf.set(MRJobConfig.COMBINE_RECORDS_BEFORE_PROGRESS, \"2\");\r\n    coc = new CombineOutputCollector<String, Integer>(outCounter, mockTaskReporter, conf);\r\n    coc.setWriter(mockWriter);\r\n    verify(mockTaskReporter, never()).progress();\r\n    coc.collect(\"dummy\", 1);\r\n    verify(mockTaskReporter, never()).progress();\r\n    coc.collect(\"dummy\", 2);\r\n    verify(mockTaskReporter, times(1)).progress();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "testDefaultCollect",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void testDefaultCollect() throws Throwable\n{\r\n    TaskReporter mockTaskReporter = mock(TaskReporter.class);\r\n    @SuppressWarnings(\"unchecked\")\r\n    Writer<String, Integer> mockWriter = mock(Writer.class);\r\n    Configuration conf = new Configuration();\r\n    coc = new CombineOutputCollector<String, Integer>(outCounter, mockTaskReporter, conf);\r\n    coc.setWriter(mockWriter);\r\n    verify(mockTaskReporter, never()).progress();\r\n    for (int i = 0; i < Task.DEFAULT_COMBINE_RECORDS_BEFORE_PROGRESS; i++) {\r\n        coc.collect(\"dummy\", i);\r\n    }\r\n    verify(mockTaskReporter, times(1)).progress();\r\n    for (int i = 0; i < Task.DEFAULT_COMBINE_RECORDS_BEFORE_PROGRESS; i++) {\r\n        coc.collect(\"dummy\", i);\r\n    }\r\n    verify(mockTaskReporter, times(2)).progress();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\util",
  "methodName" : "setUp",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setUp()\n{\r\n    ReflectionUtils.clearCache();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\util",
  "methodName" : "testSetConf",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void testSetConf()\n{\r\n    JobConfigurableOb ob = new JobConfigurableOb();\r\n    ReflectionUtils.setConf(ob, new Configuration());\r\n    assertFalse(ob.configured);\r\n    ReflectionUtils.setConf(ob, new JobConf());\r\n    assertTrue(ob.configured);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "testClusterWithLocalClientProvider",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void testClusterWithLocalClientProvider() throws Exception\n{\r\n    Configuration conf = new Configuration();\r\n    conf.set(MRConfig.FRAMEWORK_NAME, \"local\");\r\n    Cluster cluster = new Cluster(conf);\r\n    assertTrue(cluster.getClient() instanceof LocalJobRunner);\r\n    cluster.close();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "testClusterWithJTClientProvider",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void testClusterWithJTClientProvider() throws Exception\n{\r\n    Configuration conf = new Configuration();\r\n    try {\r\n        conf.set(MRConfig.FRAMEWORK_NAME, \"classic\");\r\n        conf.set(JTConfig.JT_IPC_ADDRESS, \"local\");\r\n        new Cluster(conf);\r\n        fail(\"Cluster with classic Framework name should not use \" + \"local JT address\");\r\n    } catch (IOException e) {\r\n        assertTrue(e.getMessage().contains(\"Cannot initialize Cluster. Please check\"));\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "testClusterWithYarnClientProvider",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void testClusterWithYarnClientProvider() throws Exception\n{\r\n    Configuration conf = new Configuration();\r\n    conf.set(MRConfig.FRAMEWORK_NAME, \"yarn\");\r\n    Cluster cluster = new Cluster(conf);\r\n    assertTrue(cluster.getClient() instanceof YARNRunner);\r\n    cluster.close();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "testClusterException",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void testClusterException()\n{\r\n    Configuration conf = new Configuration();\r\n    try {\r\n        conf.set(MRConfig.FRAMEWORK_NAME, \"incorrect\");\r\n        new Cluster(conf);\r\n        fail(\"Cluster should not be initialized with incorrect framework name\");\r\n    } catch (IOException e) {\r\n        assertTrue(e.getMessage().contains(\"Cannot initialize Cluster. Please check\"));\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "testClusterExceptionRootCause",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void testClusterExceptionRootCause() throws Exception\n{\r\n    final Configuration conf = new Configuration();\r\n    conf.set(MRConfig.FRAMEWORK_NAME, MRConfig.YARN_FRAMEWORK_NAME);\r\n    conf.set(FileSystem.FS_DEFAULT_NAME_KEY, \"nosuchfs:///\");\r\n    conf.set(JTConfig.JT_IPC_ADDRESS, \"local\");\r\n    try {\r\n        new Cluster(conf);\r\n        fail(\"Cluster init should fail because of non-existing FileSystem\");\r\n    } catch (IOException ioEx) {\r\n        final String stackTrace = StringUtils.stringifyException(ioEx);\r\n        assertTrue(\"No root cause detected\", stackTrace.contains(UnsupportedFileSystemException.class.getName()) && stackTrace.contains(\"nosuchfs\"));\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "testGetNullCounters",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void testGetNullCounters() throws Exception\n{\r\n    Job mockJob = mock(Job.class);\r\n    RunningJob underTest = new JobClient.NetworkedJob(mockJob);\r\n    when(mockJob.getCounters()).thenReturn(null);\r\n    assertNull(underTest.getCounters());\r\n    verify(mockJob).getCounters();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "testGetJobStatus",
  "errType" : null,
  "containingMethodsNum" : 21,
  "sourceCodeText" : "void testGetJobStatus() throws IOException, InterruptedException, ClassNotFoundException\n{\r\n    MiniMRClientCluster mr = null;\r\n    FileSystem fileSys = null;\r\n    try {\r\n        mr = createMiniClusterWithCapacityScheduler();\r\n        JobConf job = new JobConf(mr.getConfig());\r\n        fileSys = FileSystem.get(job);\r\n        fileSys.delete(testDir, true);\r\n        FSDataOutputStream out = fileSys.create(inFile, true);\r\n        out.writeBytes(\"This is a test file\");\r\n        out.close();\r\n        FileInputFormat.setInputPaths(job, inFile);\r\n        FileOutputFormat.setOutputPath(job, outDir);\r\n        job.setInputFormat(TextInputFormat.class);\r\n        job.setOutputFormat(TextOutputFormat.class);\r\n        job.setMapperClass(IdentityMapper.class);\r\n        job.setReducerClass(IdentityReducer.class);\r\n        job.setNumReduceTasks(0);\r\n        JobClient client = new JobClient(mr.getConfig());\r\n        RunningJob rj = client.submitJob(job);\r\n        JobID jobId = rj.getID();\r\n        assertEquals(\"Expected matching JobIDs\", jobId, client.getJob(jobId).getJobStatus().getJobID());\r\n        assertEquals(\"Expected matching startTimes\", rj.getJobStatus().getStartTime(), client.getJob(jobId).getJobStatus().getStartTime());\r\n    } finally {\r\n        if (fileSys != null) {\r\n            fileSys.delete(testDir, true);\r\n        }\r\n        if (mr != null) {\r\n            mr.stop();\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "testNetworkedJob",
  "errType" : [ "YarnRuntimeException", "YarnRuntimeException" ],
  "containingMethodsNum" : 86,
  "sourceCodeText" : "void testNetworkedJob() throws Exception\n{\r\n    MiniMRClientCluster mr = null;\r\n    FileSystem fileSys = null;\r\n    try {\r\n        mr = createMiniClusterWithCapacityScheduler();\r\n        JobConf job = new JobConf(mr.getConfig());\r\n        fileSys = FileSystem.get(job);\r\n        fileSys.delete(testDir, true);\r\n        FSDataOutputStream out = fileSys.create(inFile, true);\r\n        out.writeBytes(\"This is a test file\");\r\n        out.close();\r\n        FileInputFormat.setInputPaths(job, inFile);\r\n        FileOutputFormat.setOutputPath(job, outDir);\r\n        job.setInputFormat(TextInputFormat.class);\r\n        job.setOutputFormat(TextOutputFormat.class);\r\n        job.setMapperClass(IdentityMapper.class);\r\n        job.setReducerClass(IdentityReducer.class);\r\n        job.setNumReduceTasks(0);\r\n        JobClient client = new JobClient(mr.getConfig());\r\n        RunningJob rj = client.submitJob(job);\r\n        JobID jobId = rj.getID();\r\n        NetworkedJob runningJob = (NetworkedJob) client.getJob(jobId);\r\n        runningJob.setJobPriority(JobPriority.HIGH.name());\r\n        assertTrue(runningJob.getConfiguration().toString().endsWith(\"0001/job.xml\"));\r\n        assertEquals(jobId, runningJob.getID());\r\n        assertEquals(jobId.toString(), runningJob.getJobID());\r\n        assertEquals(\"N/A\", runningJob.getJobName());\r\n        assertTrue(runningJob.getJobFile().endsWith(\".staging/\" + runningJob.getJobID() + \"/job.xml\"));\r\n        assertTrue(runningJob.getTrackingURL().length() > 0);\r\n        assertThat(runningJob.mapProgress()).isEqualTo(0.0f);\r\n        assertThat(runningJob.reduceProgress()).isEqualTo(0.0f);\r\n        assertThat(runningJob.cleanupProgress()).isEqualTo(0.0f);\r\n        assertThat(runningJob.setupProgress()).isEqualTo(0.0f);\r\n        TaskCompletionEvent[] tce = runningJob.getTaskCompletionEvents(0);\r\n        assertEquals(tce.length, 0);\r\n        assertEquals(\"\", runningJob.getHistoryUrl());\r\n        assertFalse(runningJob.isRetired());\r\n        assertEquals(\"\", runningJob.getFailureInfo());\r\n        assertEquals(\"N/A\", runningJob.getJobStatus().getJobName());\r\n        assertEquals(0, client.getMapTaskReports(jobId).length);\r\n        try {\r\n            client.getSetupTaskReports(jobId);\r\n        } catch (YarnRuntimeException e) {\r\n            assertEquals(\"Unrecognized task type: JOB_SETUP\", e.getMessage());\r\n        }\r\n        try {\r\n            client.getCleanupTaskReports(jobId);\r\n        } catch (YarnRuntimeException e) {\r\n            assertEquals(\"Unrecognized task type: JOB_CLEANUP\", e.getMessage());\r\n        }\r\n        assertEquals(0, client.getReduceTaskReports(jobId).length);\r\n        ClusterStatus status = client.getClusterStatus(true);\r\n        assertEquals(2, status.getActiveTrackerNames().size());\r\n        assertEquals(0, status.getBlacklistedTrackers());\r\n        assertEquals(0, status.getBlacklistedTrackerNames().size());\r\n        assertEquals(0, status.getBlackListedTrackersInfo().size());\r\n        assertEquals(JobTrackerStatus.RUNNING, status.getJobTrackerStatus());\r\n        assertEquals(1, status.getMapTasks());\r\n        assertEquals(20, status.getMaxMapTasks());\r\n        assertEquals(4, status.getMaxReduceTasks());\r\n        assertEquals(0, status.getNumExcludedNodes());\r\n        assertEquals(1, status.getReduceTasks());\r\n        assertEquals(2, status.getTaskTrackers());\r\n        assertEquals(0, status.getTTExpiryInterval());\r\n        assertEquals(JobTrackerStatus.RUNNING, status.getJobTrackerStatus());\r\n        assertEquals(0, status.getGraylistedTrackers());\r\n        ByteArrayOutputStream dataOut = new ByteArrayOutputStream();\r\n        status.write(new DataOutputStream(dataOut));\r\n        ClusterStatus status2 = new ClusterStatus();\r\n        status2.readFields(new DataInputStream(new ByteArrayInputStream(dataOut.toByteArray())));\r\n        assertEquals(status.getActiveTrackerNames(), status2.getActiveTrackerNames());\r\n        assertEquals(status.getBlackListedTrackersInfo(), status2.getBlackListedTrackersInfo());\r\n        assertEquals(status.getMapTasks(), status2.getMapTasks());\r\n        JobClient.setTaskOutputFilter(job, TaskStatusFilter.ALL);\r\n        assertEquals(TaskStatusFilter.ALL, JobClient.getTaskOutputFilter(job));\r\n        assertEquals(20, client.getDefaultMaps());\r\n        assertEquals(4, client.getDefaultReduces());\r\n        assertEquals(\"jobSubmitDir\", client.getSystemDir().getName());\r\n        JobQueueInfo[] rootQueueInfo = client.getRootQueues();\r\n        assertEquals(1, rootQueueInfo.length);\r\n        assertEquals(\"default\", rootQueueInfo[0].getQueueName());\r\n        JobQueueInfo[] qinfo = client.getQueues();\r\n        assertEquals(1, qinfo.length);\r\n        assertEquals(\"default\", qinfo[0].getQueueName());\r\n        assertEquals(0, client.getChildQueues(\"default\").length);\r\n        assertEquals(1, client.getJobsFromQueue(\"default\").length);\r\n        assertTrue(client.getJobsFromQueue(\"default\")[0].getJobFile().endsWith(\"/job.xml\"));\r\n        JobQueueInfo qi = client.getQueueInfo(\"default\");\r\n        assertEquals(\"default\", qi.getQueueName());\r\n        assertEquals(\"running\", qi.getQueueState());\r\n        QueueAclsInfo[] aai = client.getQueueAclsForCurrentUser();\r\n        assertEquals(2, aai.length);\r\n        assertEquals(\"root\", aai[0].getQueueName());\r\n        assertEquals(\"root.default\", aai[1].getQueueName());\r\n        assertEquals(\"Expected matching JobIDs\", jobId, client.getJob(jobId).getJobStatus().getJobID());\r\n        assertEquals(\"Expected matching startTimes\", rj.getJobStatus().getStartTime(), client.getJob(jobId).getJobStatus().getStartTime());\r\n    } finally {\r\n        if (fileSys != null) {\r\n            fileSys.delete(testDir, true);\r\n        }\r\n        if (mr != null) {\r\n            mr.stop();\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 3,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "testBlackListInfo",
  "errType" : null,
  "containingMethodsNum" : 10,
  "sourceCodeText" : "void testBlackListInfo() throws IOException\n{\r\n    BlackListInfo info = new BlackListInfo();\r\n    info.setBlackListReport(\"blackListInfo\");\r\n    info.setReasonForBlackListing(\"reasonForBlackListing\");\r\n    info.setTrackerName(\"trackerName\");\r\n    ByteArrayOutputStream byteOut = new ByteArrayOutputStream();\r\n    DataOutput out = new DataOutputStream(byteOut);\r\n    info.write(out);\r\n    BlackListInfo info2 = new BlackListInfo();\r\n    info2.readFields(new DataInputStream(new ByteArrayInputStream(byteOut.toByteArray())));\r\n    assertEquals(info, info2);\r\n    assertEquals(info.toString(), info2.toString());\r\n    assertEquals(\"trackerName\", info2.getTrackerName());\r\n    assertEquals(\"reasonForBlackListing\", info2.getReasonForBlackListing());\r\n    assertEquals(\"blackListInfo\", info2.getBlackListReport());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "testJobQueueClient",
  "errType" : null,
  "containingMethodsNum" : 40,
  "sourceCodeText" : "void testJobQueueClient() throws Exception\n{\r\n    MiniMRClientCluster mr = null;\r\n    FileSystem fileSys = null;\r\n    PrintStream oldOut = System.out;\r\n    try {\r\n        mr = createMiniClusterWithCapacityScheduler();\r\n        JobConf job = new JobConf(mr.getConfig());\r\n        fileSys = FileSystem.get(job);\r\n        fileSys.delete(testDir, true);\r\n        FSDataOutputStream out = fileSys.create(inFile, true);\r\n        out.writeBytes(\"This is a test file\");\r\n        out.close();\r\n        FileInputFormat.setInputPaths(job, inFile);\r\n        FileOutputFormat.setOutputPath(job, outDir);\r\n        job.setInputFormat(TextInputFormat.class);\r\n        job.setOutputFormat(TextOutputFormat.class);\r\n        job.setMapperClass(IdentityMapper.class);\r\n        job.setReducerClass(IdentityReducer.class);\r\n        job.setNumReduceTasks(0);\r\n        JobClient client = new JobClient(mr.getConfig());\r\n        client.submitJob(job);\r\n        JobQueueClient jobClient = new JobQueueClient(job);\r\n        ByteArrayOutputStream bytes = new ByteArrayOutputStream();\r\n        System.setOut(new PrintStream(bytes));\r\n        String[] arg = { \"-list\" };\r\n        jobClient.run(arg);\r\n        assertTrue(bytes.toString().contains(\"Queue Name : default\"));\r\n        assertTrue(bytes.toString().contains(\"Queue State : running\"));\r\n        bytes = new ByteArrayOutputStream();\r\n        System.setOut(new PrintStream(bytes));\r\n        String[] arg1 = { \"-showacls\" };\r\n        jobClient.run(arg1);\r\n        assertTrue(bytes.toString().contains(\"Queue acls for user :\"));\r\n        assertTrue(bytes.toString().contains(\"root  ADMINISTER_QUEUE,SUBMIT_APPLICATIONS\"));\r\n        assertTrue(bytes.toString().contains(\"default  ADMINISTER_QUEUE,SUBMIT_APPLICATIONS\"));\r\n        bytes = new ByteArrayOutputStream();\r\n        System.setOut(new PrintStream(bytes));\r\n        String[] arg2 = { \"-info\", \"default\" };\r\n        jobClient.run(arg2);\r\n        assertTrue(bytes.toString().contains(\"Queue Name : default\"));\r\n        assertTrue(bytes.toString().contains(\"Queue State : running\"));\r\n        assertTrue(bytes.toString().contains(\"Scheduling Info\"));\r\n        bytes = new ByteArrayOutputStream();\r\n        System.setOut(new PrintStream(bytes));\r\n        String[] arg3 = { \"-info\", \"default\", \"-showJobs\" };\r\n        jobClient.run(arg3);\r\n        assertTrue(bytes.toString().contains(\"Queue Name : default\"));\r\n        assertTrue(bytes.toString().contains(\"Queue State : running\"));\r\n        assertTrue(bytes.toString().contains(\"Scheduling Info\"));\r\n        assertTrue(bytes.toString().contains(\"job_1\"));\r\n        String[] arg4 = {};\r\n        jobClient.run(arg4);\r\n    } finally {\r\n        System.setOut(oldOut);\r\n        if (fileSys != null) {\r\n            fileSys.delete(testDir, true);\r\n        }\r\n        if (mr != null) {\r\n            mr.stop();\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "createMiniClusterWithCapacityScheduler",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "MiniMRClientCluster createMiniClusterWithCapacityScheduler() throws IOException\n{\r\n    Configuration conf = new Configuration();\r\n    conf.setClass(YarnConfiguration.RM_SCHEDULER, CapacityScheduler.class, CapacityScheduler.class);\r\n    conf.set(YarnConfiguration.NM_MAX_PER_DISK_UTILIZATION_PERCENTAGE, \"99\");\r\n    return MiniMRClientClusterFactory.create(this.getClass(), 2, conf);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "setupClass",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setupClass() throws Exception\n{\r\n    setupClassBase(TestBadRecords.class);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "runMapReduce",
  "errType" : null,
  "containingMethodsNum" : 20,
  "sourceCodeText" : "void runMapReduce(JobConf conf, List<String> mapperBadRecords, List<String> redBadRecords) throws Exception\n{\r\n    createInput();\r\n    conf.setJobName(\"mr\");\r\n    conf.setNumMapTasks(1);\r\n    conf.setNumReduceTasks(1);\r\n    conf.setInt(JobContext.TASK_TIMEOUT, 30 * 1000);\r\n    SkipBadRecords.setMapperMaxSkipRecords(conf, Long.MAX_VALUE);\r\n    SkipBadRecords.setReducerMaxSkipGroups(conf, Long.MAX_VALUE);\r\n    SkipBadRecords.setAttemptsToStartSkipping(conf, 0);\r\n    conf.setMaxMapAttempts(SkipBadRecords.getAttemptsToStartSkipping(conf) + 1 + mapperBadRecords.size());\r\n    conf.setMaxReduceAttempts(SkipBadRecords.getAttemptsToStartSkipping(conf) + 1 + redBadRecords.size());\r\n    FileInputFormat.setInputPaths(conf, getInputDir());\r\n    FileOutputFormat.setOutputPath(conf, getOutputDir());\r\n    conf.setInputFormat(TextInputFormat.class);\r\n    conf.setMapOutputKeyClass(LongWritable.class);\r\n    conf.setMapOutputValueClass(Text.class);\r\n    conf.setOutputFormat(TextOutputFormat.class);\r\n    conf.setOutputKeyClass(LongWritable.class);\r\n    conf.setOutputValueClass(Text.class);\r\n    RunningJob runningJob = JobClient.runJob(conf);\r\n    validateOutput(conf, runningJob, mapperBadRecords, redBadRecords);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "createInput",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void createInput() throws Exception\n{\r\n    OutputStream os = getFileSystem().create(new Path(getInputDir(), \"text.txt\"));\r\n    Writer wr = new OutputStreamWriter(os);\r\n    for (String inp : input) {\r\n        wr.write(inp + \"\\n\");\r\n    }\r\n    wr.close();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "validateOutput",
  "errType" : null,
  "containingMethodsNum" : 46,
  "sourceCodeText" : "void validateOutput(JobConf conf, RunningJob runningJob, List<String> mapperBadRecords, List<String> redBadRecords) throws Exception\n{\r\n    LOG.info(runningJob.getCounters().toString());\r\n    assertTrue(runningJob.isSuccessful());\r\n    Counters counters = runningJob.getCounters();\r\n    assertEquals(counters.findCounter(TaskCounter.MAP_SKIPPED_RECORDS).getCounter(), mapperBadRecords.size());\r\n    int mapRecs = input.size() - mapperBadRecords.size();\r\n    assertEquals(counters.findCounter(TaskCounter.MAP_INPUT_RECORDS).getCounter(), mapRecs);\r\n    assertEquals(counters.findCounter(TaskCounter.MAP_OUTPUT_RECORDS).getCounter(), mapRecs);\r\n    int redRecs = mapRecs - redBadRecords.size();\r\n    assertEquals(counters.findCounter(TaskCounter.REDUCE_SKIPPED_RECORDS).getCounter(), redBadRecords.size());\r\n    assertEquals(counters.findCounter(TaskCounter.REDUCE_SKIPPED_GROUPS).getCounter(), redBadRecords.size());\r\n    assertEquals(counters.findCounter(TaskCounter.REDUCE_INPUT_GROUPS).getCounter(), redRecs);\r\n    assertEquals(counters.findCounter(TaskCounter.REDUCE_INPUT_RECORDS).getCounter(), redRecs);\r\n    assertEquals(counters.findCounter(TaskCounter.REDUCE_OUTPUT_RECORDS).getCounter(), redRecs);\r\n    Path skipDir = SkipBadRecords.getSkipOutputPath(conf);\r\n    assertNotNull(skipDir);\r\n    Path[] skips = FileUtil.stat2Paths(getFileSystem().listStatus(skipDir));\r\n    List<String> mapSkipped = new ArrayList<String>();\r\n    List<String> redSkipped = new ArrayList<String>();\r\n    for (Path skipPath : skips) {\r\n        LOG.info(\"skipPath: \" + skipPath);\r\n        SequenceFile.Reader reader = new SequenceFile.Reader(getFileSystem(), skipPath, conf);\r\n        Object key = ReflectionUtils.newInstance(reader.getKeyClass(), conf);\r\n        Object value = ReflectionUtils.newInstance(reader.getValueClass(), conf);\r\n        key = reader.next(key);\r\n        while (key != null) {\r\n            value = reader.getCurrentValue(value);\r\n            LOG.debug(\"key:\" + key + \" value:\" + value.toString());\r\n            if (skipPath.getName().contains(\"_r_\")) {\r\n                redSkipped.add(value.toString());\r\n            } else {\r\n                mapSkipped.add(value.toString());\r\n            }\r\n            key = reader.next(key);\r\n        }\r\n        reader.close();\r\n    }\r\n    assertTrue(mapSkipped.containsAll(mapperBadRecords));\r\n    assertTrue(redSkipped.containsAll(redBadRecords));\r\n    Path[] outputFiles = FileUtil.stat2Paths(getFileSystem().listStatus(getOutputDir(), new Utils.OutputFileUtils.OutputFilesFilter()));\r\n    List<String> mapperOutput = getProcessed(input, mapperBadRecords);\r\n    LOG.debug(\"mapperOutput \" + mapperOutput.size());\r\n    List<String> reducerOutput = getProcessed(mapperOutput, redBadRecords);\r\n    LOG.debug(\"reducerOutput \" + reducerOutput.size());\r\n    if (outputFiles.length > 0) {\r\n        InputStream is = getFileSystem().open(outputFiles[0]);\r\n        BufferedReader reader = new BufferedReader(new InputStreamReader(is));\r\n        String line = reader.readLine();\r\n        int counter = 0;\r\n        while (line != null) {\r\n            counter++;\r\n            StringTokenizer tokeniz = new StringTokenizer(line, \"\\t\");\r\n            String key = tokeniz.nextToken();\r\n            String value = tokeniz.nextToken();\r\n            LOG.debug(\"Output: key:\" + key + \"  value:\" + value);\r\n            assertTrue(value.contains(\"hello\"));\r\n            assertTrue(reducerOutput.contains(value));\r\n            line = reader.readLine();\r\n        }\r\n        reader.close();\r\n        assertEquals(reducerOutput.size(), counter);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getProcessed",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "List<String> getProcessed(List<String> inputs, List<String> badRecs)\n{\r\n    List<String> processed = new ArrayList<String>();\r\n    for (String input : inputs) {\r\n        if (!badRecs.contains(input)) {\r\n            processed.add(input);\r\n        }\r\n    }\r\n    return processed;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "testBadMapRed",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void testBadMapRed() throws Exception\n{\r\n    JobConf conf = createJobConf();\r\n    conf.setMapperClass(BadMapper.class);\r\n    conf.setReducerClass(BadReducer.class);\r\n    runMapReduce(conf, MAPPER_BAD_RECORDS, REDUCER_BAD_RECORDS);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\input",
  "methodName" : "getDir",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "Path getDir(Path dir)\n{\r\n    if (isLocalFS()) {\r\n        String localPathRoot = System.getProperty(\"test.build.data\", \"/tmp\").replace(' ', '+');\r\n        dir = new Path(localPathRoot, dir);\r\n    }\r\n    return dir;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\input",
  "methodName" : "setUp",
  "errType" : null,
  "containingMethodsNum" : 11,
  "sourceCodeText" : "void setUp() throws Exception\n{\r\n    super.setUp();\r\n    Path rootDir = getDir(ROOT_DIR);\r\n    Path in1Dir = getDir(IN1_DIR);\r\n    Path in2Dir = getDir(IN2_DIR);\r\n    Configuration conf = createJobConf();\r\n    FileSystem fs = FileSystem.get(conf);\r\n    fs.delete(rootDir, true);\r\n    if (!fs.mkdirs(in1Dir)) {\r\n        throw new IOException(\"Mkdirs failed to create \" + in1Dir.toString());\r\n    }\r\n    if (!fs.mkdirs(in2Dir)) {\r\n        throw new IOException(\"Mkdirs failed to create \" + in2Dir.toString());\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\input",
  "methodName" : "testDoMultipleInputs",
  "errType" : [ "InterruptedException", "ClassNotFoundException" ],
  "containingMethodsNum" : 29,
  "sourceCodeText" : "void testDoMultipleInputs() throws IOException\n{\r\n    Path in1Dir = getDir(IN1_DIR);\r\n    Path in2Dir = getDir(IN2_DIR);\r\n    Path outDir = getDir(OUT_DIR);\r\n    Configuration conf = createJobConf();\r\n    FileSystem fs = FileSystem.get(conf);\r\n    fs.delete(outDir, true);\r\n    DataOutputStream file1 = fs.create(new Path(in1Dir, \"part-0\"));\r\n    file1.writeBytes(\"a\\nb\\nc\\nd\\ne\");\r\n    file1.close();\r\n    DataOutputStream file2 = fs.create(new Path(in2Dir, \"part-0\"));\r\n    file2.writeBytes(\"a\\tblah\\nb\\tblah\\nc\\tblah\\nd\\tblah\\ne\\tblah\");\r\n    file2.close();\r\n    Job job = Job.getInstance(conf);\r\n    job.setJobName(\"mi\");\r\n    MultipleInputs.addInputPath(job, in1Dir, TextInputFormat.class, MapClass.class);\r\n    MultipleInputs.addInputPath(job, in2Dir, KeyValueTextInputFormat.class, KeyValueMapClass.class);\r\n    job.setMapOutputKeyClass(Text.class);\r\n    job.setMapOutputValueClass(Text.class);\r\n    job.setOutputKeyClass(NullWritable.class);\r\n    job.setOutputValueClass(Text.class);\r\n    job.setReducerClass(ReducerClass.class);\r\n    FileOutputFormat.setOutputPath(job, outDir);\r\n    boolean success = false;\r\n    try {\r\n        success = job.waitForCompletion(true);\r\n    } catch (InterruptedException ie) {\r\n        throw new RuntimeException(ie);\r\n    } catch (ClassNotFoundException instante) {\r\n        throw new RuntimeException(instante);\r\n    }\r\n    if (!success)\r\n        throw new RuntimeException(\"Job failed!\");\r\n    BufferedReader output = new BufferedReader(new InputStreamReader(fs.open(new Path(outDir, \"part-r-00000\"))));\r\n    assertEquals(\"a 2\", output.readLine());\r\n    assertEquals(\"b 2\", output.readLine());\r\n    assertEquals(\"c 2\", output.readLine());\r\n    assertEquals(\"d 2\", output.readLine());\r\n    assertEquals(\"e 2\", output.readLine());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\input",
  "methodName" : "testAddInputPathWithFormat",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void testAddInputPathWithFormat() throws IOException\n{\r\n    final Job conf = Job.getInstance();\r\n    MultipleInputs.addInputPath(conf, new Path(\"/foo\"), TextInputFormat.class);\r\n    MultipleInputs.addInputPath(conf, new Path(\"/bar\"), KeyValueTextInputFormat.class);\r\n    final Map<Path, InputFormat> inputs = MultipleInputs.getInputFormatMap(conf);\r\n    assertEquals(TextInputFormat.class, inputs.get(new Path(\"/foo\")).getClass());\r\n    assertEquals(KeyValueTextInputFormat.class, inputs.get(new Path(\"/bar\")).getClass());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\input",
  "methodName" : "testAddInputPathWithMapper",
  "errType" : null,
  "containingMethodsNum" : 9,
  "sourceCodeText" : "void testAddInputPathWithMapper() throws IOException\n{\r\n    final Job conf = Job.getInstance();\r\n    MultipleInputs.addInputPath(conf, new Path(\"/foo\"), TextInputFormat.class, MapClass.class);\r\n    MultipleInputs.addInputPath(conf, new Path(\"/bar\"), KeyValueTextInputFormat.class, KeyValueMapClass.class);\r\n    final Map<Path, InputFormat> inputs = MultipleInputs.getInputFormatMap(conf);\r\n    final Map<Path, Class<? extends Mapper>> maps = MultipleInputs.getMapperTypeMap(conf);\r\n    assertEquals(TextInputFormat.class, inputs.get(new Path(\"/foo\")).getClass());\r\n    assertEquals(KeyValueTextInputFormat.class, inputs.get(new Path(\"/bar\")).getClass());\r\n    assertEquals(MapClass.class, maps.get(new Path(\"/foo\")));\r\n    assertEquals(KeyValueMapClass.class, maps.get(new Path(\"/bar\")));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "setupClass",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setupClass() throws Exception\n{\r\n    setupClassBase(TestMRJobClient.class);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "runJob",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "Job runJob(Configuration conf) throws Exception\n{\r\n    String input = \"hello1\\nhello2\\nhello3\\n\";\r\n    Job job = MapReduceTestUtil.createJob(conf, getInputDir(), getOutputDir(), 1, 1, input);\r\n    job.setJobName(\"mr\");\r\n    job.setPriority(JobPriority.NORMAL);\r\n    job.waitForCompletion(true);\r\n    return job;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "runJobInBackGround",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "Job runJobInBackGround(Configuration conf) throws Exception\n{\r\n    String input = \"hello1\\nhello2\\nhello3\\n\";\r\n    Job job = MapReduceTestUtil.createJob(conf, getInputDir(), getOutputDir(), 1, 1, input);\r\n    job.setJobName(\"mr\");\r\n    job.setPriority(JobPriority.NORMAL);\r\n    job.submit();\r\n    int i = 0;\r\n    while (i++ < 200 && job.getJobID() == null) {\r\n        LOG.info(\"waiting for jobId...\");\r\n        Thread.sleep(100);\r\n    }\r\n    return job;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "runTool",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "int runTool(Configuration conf, Tool tool, String[] args, OutputStream out) throws Exception\n{\r\n    LOG.info(\"args = \" + Arrays.toString(args));\r\n    PrintStream oldOut = System.out;\r\n    PrintStream newOut = new PrintStream(out, true);\r\n    try {\r\n        System.setOut(newOut);\r\n        return ToolRunner.run(conf, tool, args);\r\n    } finally {\r\n        System.setOut(oldOut);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "testJobSubmissionSpecsAndFiles",
  "errType" : [ "Exception" ],
  "containingMethodsNum" : 9,
  "sourceCodeText" : "void testJobSubmissionSpecsAndFiles() throws Exception\n{\r\n    Configuration conf = createJobConf();\r\n    Job job = MapReduceTestUtil.createJob(conf, getInputDir(), getOutputDir(), 1, 1);\r\n    job.setOutputFormatClass(BadOutputFormat.class);\r\n    try {\r\n        job.submit();\r\n        fail(\"Should've thrown an exception while checking output specs.\");\r\n    } catch (Exception e) {\r\n        assertTrue(e instanceof IOException);\r\n    }\r\n    Cluster cluster = new Cluster(conf);\r\n    Path jobStagingArea = JobSubmissionFiles.getStagingDir(cluster, job.getConfiguration());\r\n    Path submitJobDir = new Path(jobStagingArea, \"JobId\");\r\n    Path submitJobFile = JobSubmissionFiles.getJobConfPath(submitJobDir);\r\n    assertFalse(\"Shouldn't have created a job file if job specs failed.\", FileSystem.get(conf).exists(submitJobFile));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "testJobClient",
  "errType" : null,
  "containingMethodsNum" : 19,
  "sourceCodeText" : "void testJobClient() throws Exception\n{\r\n    Configuration conf = createJobConf();\r\n    Job job = runJob(conf);\r\n    String jobId = job.getJobID().toString();\r\n    testAllJobList(jobId, conf);\r\n    testSubmittedJobList(conf);\r\n    testGetCounter(jobId, conf);\r\n    testJobStatus(jobId, conf);\r\n    testJobEvents(jobId, conf);\r\n    testJobHistory(jobId, conf);\r\n    testListTrackers(conf);\r\n    testListAttemptIds(jobId, conf);\r\n    testListBlackList(conf);\r\n    startStop();\r\n    testChangingJobPriority(jobId, conf);\r\n    testSubmit(conf);\r\n    testKillTask(conf);\r\n    testfailTask(conf);\r\n    testKillJob(conf);\r\n    testConfig(jobId, conf);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "testfailTask",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void testfailTask(Configuration conf) throws Exception\n{\r\n    Job job = runJobInBackGround(conf);\r\n    CLI jc = createJobClient();\r\n    TaskID tid = new TaskID(job.getJobID(), TaskType.MAP, 0);\r\n    TaskAttemptID taid = new TaskAttemptID(tid, 1);\r\n    ByteArrayOutputStream out = new ByteArrayOutputStream();\r\n    int exitCode = runTool(conf, jc, new String[] { \"-fail-task\" }, out);\r\n    assertEquals(\"Exit code\", -1, exitCode);\r\n    runTool(conf, jc, new String[] { \"-fail-task\", taid.toString() }, out);\r\n    String answer = new String(out.toByteArray(), \"UTF-8\");\r\n    assertTrue(answer.contains(\"Killed task \" + taid + \" by failing it\"));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "testKillTask",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void testKillTask(Configuration conf) throws Exception\n{\r\n    Job job = runJobInBackGround(conf);\r\n    CLI jc = createJobClient();\r\n    TaskID tid = new TaskID(job.getJobID(), TaskType.MAP, 0);\r\n    TaskAttemptID taid = new TaskAttemptID(tid, 1);\r\n    ByteArrayOutputStream out = new ByteArrayOutputStream();\r\n    int exitCode = runTool(conf, jc, new String[] { \"-kill-task\" }, out);\r\n    assertEquals(\"Exit code\", -1, exitCode);\r\n    runTool(conf, jc, new String[] { \"-kill-task\", taid.toString() }, out);\r\n    String answer = new String(out.toByteArray(), \"UTF-8\");\r\n    assertTrue(answer.contains(\"Killed task \" + taid));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "testKillJob",
  "errType" : null,
  "containingMethodsNum" : 9,
  "sourceCodeText" : "void testKillJob(Configuration conf) throws Exception\n{\r\n    Job job = runJobInBackGround(conf);\r\n    String jobId = job.getJobID().toString();\r\n    CLI jc = createJobClient();\r\n    ByteArrayOutputStream out = new ByteArrayOutputStream();\r\n    int exitCode = runTool(conf, jc, new String[] { \"-kill\" }, out);\r\n    assertEquals(\"Exit code\", -1, exitCode);\r\n    exitCode = runTool(conf, jc, new String[] { \"-kill\", jobId }, out);\r\n    assertEquals(\"Exit code\", 0, exitCode);\r\n    String answer = new String(out.toByteArray(), \"UTF-8\");\r\n    assertTrue(answer.contains(\"Killed job \" + jobId));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "testSubmit",
  "errType" : null,
  "containingMethodsNum" : 14,
  "sourceCodeText" : "void testSubmit(Configuration conf) throws Exception\n{\r\n    CLI jc = createJobClient();\r\n    Job job = MapReduceTestUtil.createJob(conf, getInputDir(), getOutputDir(), 1, 1, \"ping\");\r\n    job.setJobName(\"mr\");\r\n    job.setPriority(JobPriority.NORMAL);\r\n    File fcon = File.createTempFile(\"config\", \".xml\");\r\n    FileSystem localFs = FileSystem.getLocal(conf);\r\n    String fconUri = new Path(fcon.getAbsolutePath()).makeQualified(localFs.getUri(), localFs.getWorkingDirectory()).toUri().toString();\r\n    job.getConfiguration().writeXml(new FileOutputStream(fcon));\r\n    ByteArrayOutputStream out = new ByteArrayOutputStream();\r\n    int exitCode = runTool(conf, jc, new String[] { \"-submit\" }, out);\r\n    assertEquals(\"Exit code\", -1, exitCode);\r\n    exitCode = runTool(conf, jc, new String[] { \"-submit\", fconUri }, out);\r\n    assertEquals(\"Exit code\", 0, exitCode);\r\n    String answer = new String(out.toByteArray());\r\n    assertTrue(answer.contains(\"Created job \"));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "startStop",
  "errType" : [ "ExitUtil.ExitException", "Exception" ],
  "containingMethodsNum" : 21,
  "sourceCodeText" : "void startStop()\n{\r\n    ByteArrayOutputStream data = new ByteArrayOutputStream();\r\n    PrintStream error = System.err;\r\n    System.setErr(new PrintStream(data));\r\n    ExitUtil.disableSystemExit();\r\n    try {\r\n        CLI.main(new String[0]);\r\n        fail(\" CLI.main should call System.exit\");\r\n    } catch (ExitUtil.ExitException e) {\r\n        ExitUtil.resetFirstExitException();\r\n        assertEquals(-1, e.status);\r\n    } catch (Exception e) {\r\n    } finally {\r\n        System.setErr(error);\r\n    }\r\n    String s = new String(data.toByteArray());\r\n    assertTrue(s.contains(\"-submit\"));\r\n    assertTrue(s.contains(\"-status\"));\r\n    assertTrue(s.contains(\"-kill\"));\r\n    assertTrue(s.contains(\"-set-priority\"));\r\n    assertTrue(s.contains(\"-events\"));\r\n    assertTrue(s.contains(\"-history\"));\r\n    assertTrue(s.contains(\"-list\"));\r\n    assertTrue(s.contains(\"-list-active-trackers\"));\r\n    assertTrue(s.contains(\"-list-blacklisted-trackers\"));\r\n    assertTrue(s.contains(\"-list-attempt-ids\"));\r\n    assertTrue(s.contains(\"-kill-task\"));\r\n    assertTrue(s.contains(\"-fail-task\"));\r\n    assertTrue(s.contains(\"-logs\"));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "testListBlackList",
  "errType" : null,
  "containingMethodsNum" : 9,
  "sourceCodeText" : "void testListBlackList(Configuration conf) throws Exception\n{\r\n    CLI jc = createJobClient();\r\n    ByteArrayOutputStream out = new ByteArrayOutputStream();\r\n    int exitCode = runTool(conf, jc, new String[] { \"-list-blacklisted-trackers\", \"second in\" }, out);\r\n    assertEquals(\"Exit code\", -1, exitCode);\r\n    exitCode = runTool(conf, jc, new String[] { \"-list-blacklisted-trackers\" }, out);\r\n    assertEquals(\"Exit code\", 0, exitCode);\r\n    String line;\r\n    BufferedReader br = new BufferedReader(new InputStreamReader(new ByteArrayInputStream(out.toByteArray())));\r\n    int counter = 0;\r\n    while ((line = br.readLine()) != null) {\r\n        LOG.info(\"line = \" + line);\r\n        counter++;\r\n    }\r\n    assertEquals(0, counter);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "testListAttemptIds",
  "errType" : null,
  "containingMethodsNum" : 9,
  "sourceCodeText" : "void testListAttemptIds(String jobId, Configuration conf) throws Exception\n{\r\n    CLI jc = createJobClient();\r\n    ByteArrayOutputStream out = new ByteArrayOutputStream();\r\n    int exitCode = runTool(conf, jc, new String[] { \"-list-attempt-ids\" }, out);\r\n    assertEquals(\"Exit code\", -1, exitCode);\r\n    exitCode = runTool(conf, jc, new String[] { \"-list-attempt-ids\", jobId, \"MAP\", \"completed\" }, out);\r\n    assertEquals(\"Exit code\", 0, exitCode);\r\n    String line;\r\n    BufferedReader br = new BufferedReader(new InputStreamReader(new ByteArrayInputStream(out.toByteArray())));\r\n    int counter = 0;\r\n    while ((line = br.readLine()) != null) {\r\n        LOG.info(\"line = \" + line);\r\n        counter++;\r\n    }\r\n    assertEquals(1, counter);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "testListTrackers",
  "errType" : null,
  "containingMethodsNum" : 9,
  "sourceCodeText" : "void testListTrackers(Configuration conf) throws Exception\n{\r\n    CLI jc = createJobClient();\r\n    ByteArrayOutputStream out = new ByteArrayOutputStream();\r\n    int exitCode = runTool(conf, jc, new String[] { \"-list-active-trackers\", \"second parameter\" }, out);\r\n    assertEquals(\"Exit code\", -1, exitCode);\r\n    exitCode = runTool(conf, jc, new String[] { \"-list-active-trackers\" }, out);\r\n    assertEquals(\"Exit code\", 0, exitCode);\r\n    String line;\r\n    BufferedReader br = new BufferedReader(new InputStreamReader(new ByteArrayInputStream(out.toByteArray())));\r\n    int counter = 0;\r\n    while ((line = br.readLine()) != null) {\r\n        LOG.info(\"line = \" + line);\r\n        counter++;\r\n    }\r\n    assertEquals(2, counter);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "testJobHistory",
  "errType" : [ "IllegalArgumentException", "IllegalArgumentException" ],
  "containingMethodsNum" : 41,
  "sourceCodeText" : "void testJobHistory(String jobId, Configuration conf) throws Exception\n{\r\n    CLI jc = createJobClient();\r\n    ByteArrayOutputStream out = new ByteArrayOutputStream();\r\n    String historyFileUri = null;\r\n    RemoteIterator<LocatedFileStatus> it = getFileSystem().listFiles(new Path(\"/\"), true);\r\n    while (it.hasNext() && historyFileUri == null) {\r\n        LocatedFileStatus file = it.next();\r\n        if (file.getPath().getName().endsWith(\".jhist\")) {\r\n            historyFileUri = file.getPath().toUri().toString();\r\n        }\r\n    }\r\n    assertNotNull(\"Could not find jhist file\", historyFileUri);\r\n    for (String historyFileOrJobId : new String[] { historyFileUri, jobId }) {\r\n        int exitCode = runTool(conf, jc, new String[] { \"-history\", \"all\", historyFileOrJobId }, out);\r\n        assertEquals(\"Exit code\", 0, exitCode);\r\n        checkHistoryHumanOutput(jobId, out);\r\n        File outFile = File.createTempFile(\"myout\", \".txt\");\r\n        exitCode = runTool(conf, jc, new String[] { \"-history\", \"all\", historyFileOrJobId, \"-outfile\", outFile.getAbsolutePath() }, out);\r\n        assertEquals(\"Exit code\", 0, exitCode);\r\n        checkHistoryHumanFileOutput(jobId, out, outFile);\r\n        outFile = File.createTempFile(\"myout\", \".txt\");\r\n        exitCode = runTool(conf, jc, new String[] { \"-history\", \"all\", historyFileOrJobId, \"-outfile\", outFile.getAbsolutePath(), \"-format\", \"human\" }, out);\r\n        assertEquals(\"Exit code\", 0, exitCode);\r\n        checkHistoryHumanFileOutput(jobId, out, outFile);\r\n        exitCode = runTool(conf, jc, new String[] { \"-history\", historyFileOrJobId, \"-format\", \"human\" }, out);\r\n        assertEquals(\"Exit code\", 0, exitCode);\r\n        checkHistoryHumanOutput(jobId, out);\r\n        exitCode = runTool(conf, jc, new String[] { \"-history\", \"all\", historyFileOrJobId, \"-format\", \"json\" }, out);\r\n        assertEquals(\"Exit code\", 0, exitCode);\r\n        checkHistoryJSONOutput(jobId, out);\r\n        outFile = File.createTempFile(\"myout\", \".txt\");\r\n        exitCode = runTool(conf, jc, new String[] { \"-history\", \"all\", historyFileOrJobId, \"-outfile\", outFile.getAbsolutePath(), \"-format\", \"json\" }, out);\r\n        assertEquals(\"Exit code\", 0, exitCode);\r\n        checkHistoryJSONFileOutput(jobId, out, outFile);\r\n        exitCode = runTool(conf, jc, new String[] { \"-history\", historyFileOrJobId, \"-format\", \"json\" }, out);\r\n        assertEquals(\"Exit code\", 0, exitCode);\r\n        checkHistoryJSONOutput(jobId, out);\r\n        exitCode = runTool(conf, jc, new String[] { \"-history\", historyFileOrJobId, \"foo\" }, out);\r\n        assertEquals(\"Exit code\", -1, exitCode);\r\n        exitCode = runTool(conf, jc, new String[] { \"-history\", historyFileOrJobId, \"-format\" }, out);\r\n        assertEquals(\"Exit code\", -1, exitCode);\r\n        exitCode = runTool(conf, jc, new String[] { \"-history\", historyFileOrJobId, \"-outfile\" }, out);\r\n        assertEquals(\"Exit code\", -1, exitCode);\r\n        try {\r\n            runTool(conf, jc, new String[] { \"-history\", historyFileOrJobId, \"-format\", \"foo\" }, out);\r\n            fail();\r\n        } catch (IllegalArgumentException e) {\r\n        }\r\n    }\r\n    try {\r\n        runTool(conf, jc, new String[] { \"-history\", \"not_a_valid_history_file_or_job_id\" }, out);\r\n        fail();\r\n    } catch (IllegalArgumentException e) {\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 2,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "checkHistoryHumanOutput",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void checkHistoryHumanOutput(String jobId, ByteArrayOutputStream out) throws IOException, JSONException\n{\r\n    BufferedReader br = new BufferedReader(new InputStreamReader(new ByteArrayInputStream(out.toByteArray())));\r\n    br.readLine();\r\n    String line = br.readLine();\r\n    br.close();\r\n    assertEquals(\"Hadoop job: \" + jobId, line);\r\n    out.reset();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "checkHistoryJSONOutput",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void checkHistoryJSONOutput(String jobId, ByteArrayOutputStream out) throws IOException, JSONException\n{\r\n    BufferedReader br = new BufferedReader(new InputStreamReader(new ByteArrayInputStream(out.toByteArray())));\r\n    String line = org.apache.commons.io.IOUtils.toString(br);\r\n    br.close();\r\n    JSONObject json = new JSONObject(line);\r\n    assertEquals(jobId, json.getString(\"hadoopJob\"));\r\n    out.reset();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "checkHistoryHumanFileOutput",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void checkHistoryHumanFileOutput(String jobId, ByteArrayOutputStream out, File outFile) throws IOException, JSONException\n{\r\n    BufferedReader br = new BufferedReader(new FileReader(outFile));\r\n    br.readLine();\r\n    String line = br.readLine();\r\n    br.close();\r\n    assertEquals(\"Hadoop job: \" + jobId, line);\r\n    assertEquals(0, out.size());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "checkHistoryJSONFileOutput",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void checkHistoryJSONFileOutput(String jobId, ByteArrayOutputStream out, File outFile) throws IOException, JSONException\n{\r\n    BufferedReader br = new BufferedReader(new FileReader(outFile));\r\n    String line = org.apache.commons.io.IOUtils.toString(br);\r\n    br.close();\r\n    JSONObject json = new JSONObject(line);\r\n    assertEquals(jobId, json.getString(\"hadoopJob\"));\r\n    assertEquals(0, out.size());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "testConfig",
  "errType" : null,
  "containingMethodsNum" : 11,
  "sourceCodeText" : "void testConfig(String jobId, Configuration conf) throws Exception\n{\r\n    CLI jc = createJobClient();\r\n    ByteArrayOutputStream out = new ByteArrayOutputStream();\r\n    int exitCode = runTool(conf, jc, new String[] { \"-config\" }, out);\r\n    assertEquals(\"Exit code\", -1, exitCode);\r\n    exitCode = runTool(conf, jc, new String[] { \"-config job_invalid foo.xml\" }, out);\r\n    assertEquals(\"Exit code\", -1, exitCode);\r\n    File outFile = File.createTempFile(\"config\", \".xml\");\r\n    exitCode = runTool(conf, jc, new String[] { \"-config\", jobId, outFile.toString() }, out);\r\n    assertEquals(\"Exit code\", 0, exitCode);\r\n    BufferedReader br = new BufferedReader(new FileReader(outFile));\r\n    String line = br.readLine();\r\n    br.close();\r\n    assertEquals(\"<?xml version=\\\"1.0\\\" encoding=\\\"UTF-8\\\" \" + \"standalone=\\\"no\\\"?><configuration>\", line);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "testJobEvents",
  "errType" : null,
  "containingMethodsNum" : 11,
  "sourceCodeText" : "void testJobEvents(String jobId, Configuration conf) throws Exception\n{\r\n    CLI jc = createJobClient();\r\n    ByteArrayOutputStream out = new ByteArrayOutputStream();\r\n    int exitCode = runTool(conf, jc, new String[] { \"-events\" }, out);\r\n    assertEquals(\"Exit code\", -1, exitCode);\r\n    exitCode = runTool(conf, jc, new String[] { \"-events\", jobId, \"0\", \"100\" }, out);\r\n    assertEquals(\"Exit code\", 0, exitCode);\r\n    String line;\r\n    BufferedReader br = new BufferedReader(new InputStreamReader(new ByteArrayInputStream(out.toByteArray())));\r\n    int counter = 0;\r\n    String attemptId = (\"attempt\" + jobId.substring(3));\r\n    while ((line = br.readLine()) != null) {\r\n        LOG.info(\"line = \" + line);\r\n        if (line.contains(attemptId)) {\r\n            counter++;\r\n        }\r\n    }\r\n    assertEquals(2, counter);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "testJobStatus",
  "errType" : null,
  "containingMethodsNum" : 11,
  "sourceCodeText" : "void testJobStatus(String jobId, Configuration conf) throws Exception\n{\r\n    CLI jc = createJobClient();\r\n    ByteArrayOutputStream out = new ByteArrayOutputStream();\r\n    int exitCode = runTool(conf, jc, new String[] { \"-status\" }, out);\r\n    assertEquals(\"Exit code\", -1, exitCode);\r\n    exitCode = runTool(conf, jc, new String[] { \"-status\", jobId }, out);\r\n    assertEquals(\"Exit code\", 0, exitCode);\r\n    String line;\r\n    BufferedReader br = new BufferedReader(new InputStreamReader(new ByteArrayInputStream(out.toByteArray())));\r\n    while ((line = br.readLine()) != null) {\r\n        LOG.info(\"line = \" + line);\r\n        if (!line.contains(\"Job state:\")) {\r\n            continue;\r\n        }\r\n        break;\r\n    }\r\n    assertNotNull(line);\r\n    assertTrue(line.contains(\"SUCCEEDED\"));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "testGetCounter",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void testGetCounter(String jobId, Configuration conf) throws Exception\n{\r\n    ByteArrayOutputStream out = new ByteArrayOutputStream();\r\n    int exitCode = runTool(conf, createJobClient(), new String[] { \"-counter\" }, out);\r\n    assertEquals(\"Exit code\", -1, exitCode);\r\n    exitCode = runTool(conf, createJobClient(), new String[] { \"-counter\", jobId, \"org.apache.hadoop.mapreduce.TaskCounter\", \"MAP_INPUT_RECORDS\" }, out);\r\n    assertEquals(\"Exit code\", 0, exitCode);\r\n    assertEquals(\"Counter\", \"3\", out.toString().trim());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "testAllJobList",
  "errType" : null,
  "containingMethodsNum" : 10,
  "sourceCodeText" : "void testAllJobList(String jobId, Configuration conf) throws Exception\n{\r\n    ByteArrayOutputStream out = new ByteArrayOutputStream();\r\n    int exitCode = runTool(conf, createJobClient(), new String[] { \"-list\", \"alldata\" }, out);\r\n    assertEquals(\"Exit code\", -1, exitCode);\r\n    exitCode = runTool(conf, createJobClient(), new String[] { \"-list\", \"all\" }, out);\r\n    assertEquals(\"Exit code\", 0, exitCode);\r\n    BufferedReader br = new BufferedReader(new InputStreamReader(new ByteArrayInputStream(out.toByteArray())));\r\n    String line;\r\n    int counter = 0;\r\n    while ((line = br.readLine()) != null) {\r\n        LOG.info(\"line = \" + line);\r\n        if (line.contains(jobId)) {\r\n            counter++;\r\n        }\r\n    }\r\n    assertEquals(1, counter);\r\n    out.reset();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "testSubmittedJobList",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void testSubmittedJobList(Configuration conf) throws Exception\n{\r\n    Job job = runJobInBackGround(conf);\r\n    ByteArrayOutputStream out = new ByteArrayOutputStream();\r\n    String line;\r\n    int counter = 0;\r\n    int exitCode = runTool(conf, createJobClient(), new String[] { \"-list\" }, out);\r\n    assertEquals(\"Exit code\", 0, exitCode);\r\n    BufferedReader br = new BufferedReader(new InputStreamReader(new ByteArrayInputStream(out.toByteArray())));\r\n    counter = 0;\r\n    while ((line = br.readLine()) != null) {\r\n        LOG.info(\"line = \" + line);\r\n        if (line.contains(job.getJobID().toString())) {\r\n            counter++;\r\n        }\r\n    }\r\n    assertEquals(1, counter);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "verifyJobPriority",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void verifyJobPriority(String jobId, String priority, Configuration conf, CLI jc) throws Exception\n{\r\n    PipedInputStream pis = new PipedInputStream();\r\n    PipedOutputStream pos = new PipedOutputStream(pis);\r\n    int exitCode = runTool(conf, jc, new String[] { \"-list\", \"all\" }, pos);\r\n    assertEquals(\"Exit code\", 0, exitCode);\r\n    BufferedReader br = new BufferedReader(new InputStreamReader(pis));\r\n    String line;\r\n    while ((line = br.readLine()) != null) {\r\n        LOG.info(\"line = \" + line);\r\n        if (!line.contains(jobId)) {\r\n            continue;\r\n        }\r\n        assertTrue(line.contains(priority));\r\n        break;\r\n    }\r\n    pis.close();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "testChangingJobPriority",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void testChangingJobPriority(String jobId, Configuration conf) throws Exception\n{\r\n    int exitCode = runTool(conf, createJobClient(), new String[] { \"-set-priority\" }, new ByteArrayOutputStream());\r\n    assertEquals(\"Exit code\", -1, exitCode);\r\n    exitCode = runTool(conf, createJobClient(), new String[] { \"-set-priority\", jobId, \"VERY_LOW\" }, new ByteArrayOutputStream());\r\n    assertEquals(\"Exit code\", 0, exitCode);\r\n    verifyJobPriority(jobId, \"DEFAULT\", conf, createJobClient());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "testJobName",
  "errType" : null,
  "containingMethodsNum" : 14,
  "sourceCodeText" : "void testJobName() throws Exception\n{\r\n    Configuration conf = createJobConf();\r\n    CLI jc = createJobClient();\r\n    Job job = MapReduceTestUtil.createJob(conf, getInputDir(), getOutputDir(), 1, 1, \"short_name\");\r\n    job.setJobName(\"mapreduce\");\r\n    job.setPriority(JobPriority.NORMAL);\r\n    job.waitForCompletion(true);\r\n    String jobId = job.getJobID().toString();\r\n    verifyJobName(jobId, \"mapreduce\", conf, jc);\r\n    Job job2 = MapReduceTestUtil.createJob(conf, getInputDir(), getOutputDir(), 1, 1, \"long_name\");\r\n    job2.setJobName(\"mapreduce_job_with_long_name\");\r\n    job2.setPriority(JobPriority.NORMAL);\r\n    job2.waitForCompletion(true);\r\n    jobId = job2.getJobID().toString();\r\n    verifyJobName(jobId, \"mapreduce_job_with_l\", conf, jc);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "verifyJobName",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void verifyJobName(String jobId, String name, Configuration conf, CLI jc) throws Exception\n{\r\n    PipedInputStream pis = new PipedInputStream();\r\n    PipedOutputStream pos = new PipedOutputStream(pis);\r\n    int exitCode = runTool(conf, jc, new String[] { \"-list\", \"all\" }, pos);\r\n    assertEquals(\"Exit code\", 0, exitCode);\r\n    BufferedReader br = new BufferedReader(new InputStreamReader(pis));\r\n    String line = null;\r\n    while ((line = br.readLine()) != null) {\r\n        LOG.info(\"line = \" + line);\r\n        if (!line.contains(jobId)) {\r\n            continue;\r\n        }\r\n        assertTrue(line.contains(name));\r\n        break;\r\n    }\r\n    pis.close();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "createJobClient",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "CLI createJobClient() throws IOException\n{\r\n    return new CLI();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred\\lib",
  "methodName" : "testAddInputPathWithFormat",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void testAddInputPathWithFormat()\n{\r\n    final JobConf conf = new JobConf();\r\n    MultipleInputs.addInputPath(conf, new Path(\"/foo\"), TextInputFormat.class);\r\n    MultipleInputs.addInputPath(conf, new Path(\"/bar\"), KeyValueTextInputFormat.class);\r\n    final Map<Path, InputFormat> inputs = MultipleInputs.getInputFormatMap(conf);\r\n    assertEquals(TextInputFormat.class, inputs.get(new Path(\"/foo\")).getClass());\r\n    assertEquals(KeyValueTextInputFormat.class, inputs.get(new Path(\"/bar\")).getClass());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred\\lib",
  "methodName" : "testAddInputPathWithMapper",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void testAddInputPathWithMapper()\n{\r\n    final JobConf conf = new JobConf();\r\n    MultipleInputs.addInputPath(conf, new Path(\"/foo\"), TextInputFormat.class, MapClass.class);\r\n    MultipleInputs.addInputPath(conf, new Path(\"/bar\"), KeyValueTextInputFormat.class, MapClass2.class);\r\n    final Map<Path, InputFormat> inputs = MultipleInputs.getInputFormatMap(conf);\r\n    final Map<Path, Class<? extends Mapper>> maps = MultipleInputs.getMapperTypeMap(conf);\r\n    assertEquals(TextInputFormat.class, inputs.get(new Path(\"/foo\")).getClass());\r\n    assertEquals(KeyValueTextInputFormat.class, inputs.get(new Path(\"/bar\")).getClass());\r\n    assertEquals(MapClass.class, maps.get(new Path(\"/foo\")));\r\n    assertEquals(MapClass2.class, maps.get(new Path(\"/bar\")));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "setup",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void setup() throws Exception\n{\r\n    fs = FileSystem.getLocal(new Configuration());\r\n    fs.delete(testRootTempDir, true);\r\n    fs.mkdirs(testRootTempDir);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "cleanup",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void cleanup() throws Exception\n{\r\n    fs.delete(testRootTempDir, true);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "testContextStatus",
  "errType" : null,
  "containingMethodsNum" : 18,
  "sourceCodeText" : "void testContextStatus() throws IOException, InterruptedException, ClassNotFoundException\n{\r\n    Path test = new Path(testRootTempDir, \"testContextStatus\");\r\n    int numMaps = 1;\r\n    Job job = MapReduceTestUtil.createJob(createJobConf(), new Path(test, \"in\"), new Path(test, \"out\"), numMaps, 0);\r\n    job.setMapperClass(MyMapper.class);\r\n    job.waitForCompletion(true);\r\n    assertTrue(\"Job failed\", job.isSuccessful());\r\n    TaskReport[] reports = job.getTaskReports(TaskType.MAP);\r\n    assertEquals(numMaps, reports.length);\r\n    assertEquals(myStatus, reports[0].getState());\r\n    int numReduces = 1;\r\n    job = MapReduceTestUtil.createJob(createJobConf(), new Path(test, \"in\"), new Path(test, \"out\"), numMaps, numReduces);\r\n    job.setMapperClass(DataCopyMapper.class);\r\n    job.setReducerClass(DataCopyReducer.class);\r\n    job.setMapOutputKeyClass(Text.class);\r\n    job.setMapOutputValueClass(Text.class);\r\n    job.setOutputKeyClass(Text.class);\r\n    job.setOutputValueClass(Text.class);\r\n    job.setMaxMapAttempts(1);\r\n    job.setMaxReduceAttempts(0);\r\n    job.waitForCompletion(true);\r\n    assertTrue(\"Job failed\", job.isSuccessful());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "testMapContextProgress",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void testMapContextProgress() throws IOException, InterruptedException, ClassNotFoundException\n{\r\n    int numMaps = 1;\r\n    Path test = new Path(testRootTempDir, \"testMapContextProgress\");\r\n    Job job = MapReduceTestUtil.createJob(createJobConf(), new Path(test, \"in\"), new Path(test, \"out\"), numMaps, 0, INPUT);\r\n    job.setMapperClass(ProgressCheckerMapper.class);\r\n    job.setMapOutputKeyClass(Text.class);\r\n    job.setMaxMapAttempts(1);\r\n    job.waitForCompletion(true);\r\n    assertTrue(\"Job failed\", job.isSuccessful());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "testReduceContextProgress",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void testReduceContextProgress() throws IOException, InterruptedException, ClassNotFoundException\n{\r\n    int numTasks = 1;\r\n    Path test = new Path(testRootTempDir, \"testReduceContextProgress\");\r\n    Job job = MapReduceTestUtil.createJob(createJobConf(), new Path(test, \"in\"), new Path(test, \"out\"), numTasks, numTasks, INPUT);\r\n    job.setMapperClass(ProgressCheckerMapper.class);\r\n    job.setReducerClass(ProgressCheckerReducer.class);\r\n    job.setMapOutputKeyClass(Text.class);\r\n    job.setMaxMapAttempts(1);\r\n    job.setMaxReduceAttempts(1);\r\n    job.waitForCompletion(true);\r\n    assertTrue(\"Job failed\", job.isSuccessful());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\fs\\slive",
  "methodName" : "getReadFile",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Path getReadFile()\n{\r\n    Path fn = getFinder().getFile();\r\n    return fn;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\fs\\slive",
  "methodName" : "run",
  "errType" : [ "FileNotFoundException", "BadFileException", "IOException", "IOException" ],
  "containingMethodsNum" : 32,
  "sourceCodeText" : "List<OperationOutput> run(FileSystem fs)\n{\r\n    List<OperationOutput> out = super.run(fs);\r\n    DataInputStream is = null;\r\n    try {\r\n        Path fn = getReadFile();\r\n        Range<Long> readSizeRange = getConfig().getReadSize();\r\n        long readSize = 0;\r\n        String readStrAm = \"\";\r\n        if (getConfig().shouldReadFullFile()) {\r\n            readSize = Long.MAX_VALUE;\r\n            readStrAm = \"full file\";\r\n        } else {\r\n            readSize = Range.betweenPositive(getRandom(), readSizeRange);\r\n            readStrAm = Helper.toByteInfo(readSize);\r\n        }\r\n        long timeTaken = 0;\r\n        long chunkSame = 0;\r\n        long chunkDiff = 0;\r\n        long bytesRead = 0;\r\n        long startTime = 0;\r\n        DataVerifier vf = new DataVerifier();\r\n        LOG.info(\"Attempting to read file at \" + fn + \" of size (\" + readStrAm + \")\");\r\n        {\r\n            startTime = Timer.now();\r\n            is = fs.open(fn);\r\n            timeTaken += Timer.elapsed(startTime);\r\n            VerifyOutput vo = vf.verifyFile(readSize, is);\r\n            timeTaken += vo.getReadTime();\r\n            chunkSame += vo.getChunksSame();\r\n            chunkDiff += vo.getChunksDifferent();\r\n            bytesRead += vo.getBytesRead();\r\n            startTime = Timer.now();\r\n            is.close();\r\n            is = null;\r\n            timeTaken += Timer.elapsed(startTime);\r\n        }\r\n        out.add(new OperationOutput(OutputType.LONG, getType(), ReportWriter.OK_TIME_TAKEN, timeTaken));\r\n        out.add(new OperationOutput(OutputType.LONG, getType(), ReportWriter.BYTES_READ, bytesRead));\r\n        out.add(new OperationOutput(OutputType.LONG, getType(), ReportWriter.SUCCESSES, 1L));\r\n        out.add(new OperationOutput(OutputType.LONG, getType(), ReportWriter.CHUNKS_VERIFIED, chunkSame));\r\n        out.add(new OperationOutput(OutputType.LONG, getType(), ReportWriter.CHUNKS_UNVERIFIED, chunkDiff));\r\n        LOG.info(\"Read \" + Helper.toByteInfo(bytesRead) + \" of \" + fn + \" with \" + chunkSame + \" chunks being same as expected and \" + chunkDiff + \" chunks being different than expected in \" + timeTaken + \" milliseconds\");\r\n    } catch (FileNotFoundException e) {\r\n        out.add(new OperationOutput(OutputType.LONG, getType(), ReportWriter.NOT_FOUND, 1L));\r\n        LOG.warn(\"Error with reading\", e);\r\n    } catch (BadFileException e) {\r\n        out.add(new OperationOutput(OutputType.LONG, getType(), ReportWriter.BAD_FILES, 1L));\r\n        LOG.warn(\"Error reading bad file\", e);\r\n    } catch (IOException e) {\r\n        out.add(new OperationOutput(OutputType.LONG, getType(), ReportWriter.FAILURES, 1L));\r\n        LOG.warn(\"Error reading\", e);\r\n    } finally {\r\n        if (is != null) {\r\n            try {\r\n                is.close();\r\n            } catch (IOException e) {\r\n                LOG.warn(\"Error closing read stream\", e);\r\n            }\r\n        }\r\n    }\r\n    return out;\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 2,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "randomizeBytes",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void randomizeBytes(byte[] data, int offset, int length)\n{\r\n    for (int i = offset + length - 1; i >= offset; --i) {\r\n        data[i] = (byte) random.nextInt(256);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "createBigMapInputFile",
  "errType" : null,
  "containingMethodsNum" : 19,
  "sourceCodeText" : "void createBigMapInputFile(Configuration conf, FileSystem fs, Path dir, long fileSizeInMB) throws IOException\n{\r\n    if (fs.exists(dir)) {\r\n        FileStatus[] list = fs.listStatus(dir);\r\n        if (list.length > 0) {\r\n            throw new IOException(\"Input path: \" + dir + \" already exists... \");\r\n        }\r\n    }\r\n    Path file = new Path(dir, \"part-0\");\r\n    SequenceFile.Writer writer = SequenceFile.createWriter(fs, conf, file, BytesWritable.class, BytesWritable.class, CompressionType.NONE);\r\n    long numBytesToWrite = fileSizeInMB * 1024 * 1024;\r\n    int minKeySize = conf.getInt(MIN_KEY, 10);\r\n    ;\r\n    int keySizeRange = conf.getInt(MAX_KEY, 1000) - minKeySize;\r\n    int minValueSize = conf.getInt(MIN_VALUE, 0);\r\n    int valueSizeRange = conf.getInt(MAX_VALUE, 20000) - minValueSize;\r\n    BytesWritable randomKey = new BytesWritable();\r\n    BytesWritable randomValue = new BytesWritable();\r\n    LOG.info(\"Writing \" + numBytesToWrite + \" bytes to \" + file + \" with \" + \"minKeySize: \" + minKeySize + \" keySizeRange: \" + keySizeRange + \" minValueSize: \" + minValueSize + \" valueSizeRange: \" + valueSizeRange);\r\n    long start = System.currentTimeMillis();\r\n    while (numBytesToWrite > 0) {\r\n        int keyLength = minKeySize + (keySizeRange != 0 ? random.nextInt(keySizeRange) : 0);\r\n        randomKey.setSize(keyLength);\r\n        randomizeBytes(randomKey.getBytes(), 0, randomKey.getLength());\r\n        int valueLength = minValueSize + (valueSizeRange != 0 ? random.nextInt(valueSizeRange) : 0);\r\n        randomValue.setSize(valueLength);\r\n        randomizeBytes(randomValue.getBytes(), 0, randomValue.getLength());\r\n        writer.append(randomKey, randomValue);\r\n        numBytesToWrite -= keyLength + valueLength;\r\n    }\r\n    writer.close();\r\n    long end = System.currentTimeMillis();\r\n    LOG.info(\"Created \" + file + \" of size: \" + fileSizeInMB + \"MB in \" + (end - start) / 1000 + \"secs\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "usage",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void usage()\n{\r\n    System.err.println(\"BigMapOutput -input <input-dir> -output <output-dir> \" + \"[-create <filesize in MB>]\");\r\n    ToolRunner.printGenericCommandUsage(System.err);\r\n    System.exit(1);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "run",
  "errType" : null,
  "containingMethodsNum" : 22,
  "sourceCodeText" : "int run(String[] args) throws Exception\n{\r\n    if (args.length < 4) {\r\n        usage();\r\n    }\r\n    Path bigMapInput = null;\r\n    Path outputPath = null;\r\n    boolean createInput = false;\r\n    long fileSizeInMB = 3 * 1024;\r\n    for (int i = 0; i < args.length; ++i) {\r\n        if (\"-input\".equals(args[i])) {\r\n            bigMapInput = new Path(args[++i]);\r\n        } else if (\"-output\".equals(args[i])) {\r\n            outputPath = new Path(args[++i]);\r\n        } else if (\"-create\".equals(args[i])) {\r\n            createInput = true;\r\n            fileSizeInMB = Long.parseLong(args[++i]);\r\n        } else {\r\n            usage();\r\n        }\r\n    }\r\n    if (bigMapInput == null || outputPath == null) {\r\n        usage();\r\n        return -1;\r\n    }\r\n    JobConf jobConf = new JobConf(getConf(), BigMapOutput.class);\r\n    jobConf.setJobName(\"BigMapOutput\");\r\n    jobConf.setInputFormat(NonSplitableSequenceFileInputFormat.class);\r\n    jobConf.setOutputFormat(SequenceFileOutputFormat.class);\r\n    FileInputFormat.setInputPaths(jobConf, bigMapInput);\r\n    outputPath.getFileSystem(jobConf).delete(outputPath, true);\r\n    FileOutputFormat.setOutputPath(jobConf, outputPath);\r\n    jobConf.setMapperClass(IdentityMapper.class);\r\n    jobConf.setReducerClass(IdentityReducer.class);\r\n    jobConf.setOutputKeyClass(BytesWritable.class);\r\n    jobConf.setOutputValueClass(BytesWritable.class);\r\n    if (createInput) {\r\n        createBigMapInputFile(jobConf, bigMapInput.getFileSystem(jobConf), bigMapInput, fileSizeInMB);\r\n    }\r\n    Date startTime = new Date();\r\n    System.out.println(\"Job started: \" + startTime);\r\n    JobClient.runJob(jobConf);\r\n    Date end_time = new Date();\r\n    System.out.println(\"Job ended: \" + end_time);\r\n    return 0;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "main",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void main(String[] argv) throws Exception\n{\r\n    int res = ToolRunner.run(new Configuration(), new BigMapOutput(), argv);\r\n    System.exit(res);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\jobcontrol",
  "methodName" : "cleanupData",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void cleanupData(Configuration conf) throws Exception\n{\r\n    FileSystem fs = FileSystem.get(conf);\r\n    MapReduceTestUtil.cleanData(fs, indir);\r\n    MapReduceTestUtil.generateData(fs, indir);\r\n    MapReduceTestUtil.cleanData(fs, outdir_1);\r\n    MapReduceTestUtil.cleanData(fs, outdir_2);\r\n    MapReduceTestUtil.cleanData(fs, outdir_3);\r\n    MapReduceTestUtil.cleanData(fs, outdir_4);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\jobcontrol",
  "methodName" : "createDependencies",
  "errType" : null,
  "containingMethodsNum" : 11,
  "sourceCodeText" : "JobControl createDependencies(Configuration conf, Job job1) throws Exception\n{\r\n    List<ControlledJob> dependingJobs = null;\r\n    cjob1 = new ControlledJob(job1, dependingJobs);\r\n    Job job2 = MapReduceTestUtil.createCopyJob(conf, outdir_2, indir);\r\n    cjob2 = new ControlledJob(job2, dependingJobs);\r\n    Job job3 = MapReduceTestUtil.createCopyJob(conf, outdir_3, outdir_1, outdir_2);\r\n    dependingJobs = new ArrayList<ControlledJob>();\r\n    dependingJobs.add(cjob1);\r\n    dependingJobs.add(cjob2);\r\n    cjob3 = new ControlledJob(job3, dependingJobs);\r\n    Job job4 = MapReduceTestUtil.createCopyJob(conf, outdir_4, outdir_3);\r\n    dependingJobs = new ArrayList<ControlledJob>();\r\n    dependingJobs.add(cjob3);\r\n    cjob4 = new ControlledJob(job4, dependingJobs);\r\n    JobControl theControl = new JobControl(\"Test\");\r\n    theControl.addJob(cjob1);\r\n    theControl.addJob(cjob2);\r\n    theControl.addJob(cjob3);\r\n    theControl.addJob(cjob4);\r\n    Thread theController = new Thread(theControl);\r\n    theController.start();\r\n    return theControl;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\jobcontrol",
  "methodName" : "waitTillAllFinished",
  "errType" : [ "Exception" ],
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void waitTillAllFinished(JobControl theControl)\n{\r\n    while (!theControl.allFinished()) {\r\n        try {\r\n            Thread.sleep(100);\r\n        } catch (Exception e) {\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\jobcontrol",
  "methodName" : "testJobControlWithFailJob",
  "errType" : null,
  "containingMethodsNum" : 11,
  "sourceCodeText" : "void testJobControlWithFailJob() throws Exception\n{\r\n    LOG.info(\"Starting testJobControlWithFailJob\");\r\n    Configuration conf = createJobConf();\r\n    cleanupData(conf);\r\n    Job job1 = MapReduceTestUtil.createFailJob(conf, outdir_1, indir);\r\n    JobControl theControl = createDependencies(conf, job1);\r\n    waitTillAllFinished(theControl);\r\n    assertTrue(cjob1.getJobState() == ControlledJob.State.FAILED);\r\n    assertTrue(cjob2.getJobState() == ControlledJob.State.SUCCESS);\r\n    assertTrue(cjob3.getJobState() == ControlledJob.State.DEPENDENT_FAILED);\r\n    assertTrue(cjob4.getJobState() == ControlledJob.State.DEPENDENT_FAILED);\r\n    theControl.stop();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\jobcontrol",
  "methodName" : "testJobControlWithKillJob",
  "errType" : [ "InterruptedException" ],
  "containingMethodsNum" : 18,
  "sourceCodeText" : "void testJobControlWithKillJob() throws Exception\n{\r\n    LOG.info(\"Starting testJobControlWithKillJob\");\r\n    Configuration conf = createJobConf();\r\n    cleanupData(conf);\r\n    Job job1 = MapReduceTestUtil.createKillJob(conf, outdir_1, indir);\r\n    JobControl theControl = createDependencies(conf, job1);\r\n    while (cjob1.getJobState() != ControlledJob.State.RUNNING) {\r\n        try {\r\n            Thread.sleep(100);\r\n        } catch (InterruptedException e) {\r\n            break;\r\n        }\r\n    }\r\n    assertFalse(cjob1.addDependingJob(cjob2));\r\n    theControl.suspend();\r\n    assertTrue(theControl.getThreadState() == JobControl.ThreadState.SUSPENDED);\r\n    theControl.resume();\r\n    cjob1.killJob();\r\n    waitTillAllFinished(theControl);\r\n    assertTrue(cjob1.getJobState() == ControlledJob.State.FAILED);\r\n    assertTrue(cjob2.getJobState() == ControlledJob.State.SUCCESS);\r\n    assertTrue(cjob3.getJobState() == ControlledJob.State.DEPENDENT_FAILED);\r\n    assertTrue(cjob4.getJobState() == ControlledJob.State.DEPENDENT_FAILED);\r\n    theControl.stop();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\jobcontrol",
  "methodName" : "testJobControl",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void testJobControl() throws Exception\n{\r\n    LOG.info(\"Starting testJobControl\");\r\n    Configuration conf = createJobConf();\r\n    cleanupData(conf);\r\n    Job job1 = MapReduceTestUtil.createCopyJob(conf, outdir_1, indir);\r\n    JobControl theControl = createDependencies(conf, job1);\r\n    waitTillAllFinished(theControl);\r\n    assertEquals(\"Some jobs failed\", 0, theControl.getFailedJobList().size());\r\n    theControl.stop();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\jobcontrol",
  "methodName" : "testControlledJob",
  "errType" : [ "InterruptedException" ],
  "containingMethodsNum" : 11,
  "sourceCodeText" : "void testControlledJob() throws Exception\n{\r\n    LOG.info(\"Starting testControlledJob\");\r\n    Configuration conf = createJobConf();\r\n    cleanupData(conf);\r\n    Job job1 = MapReduceTestUtil.createCopyJob(conf, outdir_1, indir);\r\n    JobControl theControl = createDependencies(conf, job1);\r\n    while (cjob1.getJobState() != ControlledJob.State.RUNNING) {\r\n        try {\r\n            Thread.sleep(100);\r\n        } catch (InterruptedException e) {\r\n            break;\r\n        }\r\n    }\r\n    Assert.assertNotNull(cjob1.getMapredJobId());\r\n    waitTillAllFinished(theControl);\r\n    assertEquals(\"Some jobs failed\", 0, theControl.getFailedJobList().size());\r\n    theControl.stop();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "testGetSplitHosts",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void testGetSplitHosts() throws Exception\n{\r\n    int numBlocks = 3;\r\n    int block1Size = 100, block2Size = 150, block3Size = 75;\r\n    int fileSize = block1Size + block2Size + block3Size;\r\n    int replicationFactor = 3;\r\n    NetworkTopology clusterMap = new NetworkTopology();\r\n    BlockLocation[] bs = new BlockLocation[numBlocks];\r\n    String[] block1Hosts = { \"host1\", \"host2\", \"host3\" };\r\n    String[] block1Names = { \"host1:100\", \"host2:100\", \"host3:100\" };\r\n    String[] block1Racks = { \"/rack1/\", \"/rack1/\", \"/rack2/\" };\r\n    String[] block1Paths = new String[replicationFactor];\r\n    for (int i = 0; i < replicationFactor; i++) {\r\n        block1Paths[i] = block1Racks[i] + block1Names[i];\r\n    }\r\n    bs[0] = new BlockLocation(block1Names, block1Hosts, block1Paths, 0, block1Size);\r\n    String[] block2Hosts = { \"host4\", \"host5\", \"host6\" };\r\n    String[] block2Names = { \"host4:100\", \"host5:100\", \"host6:100\" };\r\n    String[] block2Racks = { \"/rack2/\", \"/rack3/\", \"/rack3/\" };\r\n    String[] block2Paths = new String[replicationFactor];\r\n    for (int i = 0; i < replicationFactor; i++) {\r\n        block2Paths[i] = block2Racks[i] + block2Names[i];\r\n    }\r\n    bs[1] = new BlockLocation(block2Names, block2Hosts, block2Paths, block1Size, block2Size);\r\n    String[] block3Hosts = { \"host1\", \"host7\", \"host8\" };\r\n    String[] block3Names = { \"host1:100\", \"host7:100\", \"host8:100\" };\r\n    String[] block3Racks = { \"/rack1/\", \"/rack4/\", \"/rack4/\" };\r\n    String[] block3Paths = new String[replicationFactor];\r\n    for (int i = 0; i < replicationFactor; i++) {\r\n        block3Paths[i] = block3Racks[i] + block3Names[i];\r\n    }\r\n    bs[2] = new BlockLocation(block3Names, block3Hosts, block3Paths, block1Size + block2Size, block3Size);\r\n    SequenceFileInputFormat<String, String> sif = new SequenceFileInputFormat<String, String>();\r\n    String[] hosts = sif.getSplitHosts(bs, 0, fileSize, clusterMap);\r\n    assertTrue(hosts.length == replicationFactor);\r\n    assertTrue(hosts[0].equalsIgnoreCase(\"host4\"));\r\n    assertTrue(hosts[1].equalsIgnoreCase(\"host3\"));\r\n    assertTrue(hosts[2].equalsIgnoreCase(\"host1\"));\r\n    bs[0] = new BlockLocation(block1Names, block1Hosts, 0, block1Size);\r\n    bs[1] = new BlockLocation(block2Names, block2Hosts, block1Size, block2Size);\r\n    bs[2] = new BlockLocation(block3Names, block3Hosts, block1Size + block2Size, block3Size);\r\n    hosts = sif.getSplitHosts(bs, 0, fileSize, clusterMap);\r\n    assertTrue(hosts.length == replicationFactor);\r\n    assertTrue(hosts[0].equalsIgnoreCase(\"host1\"));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\fs",
  "methodName" : "reduce",
  "errType" : null,
  "containingMethodsNum" : 17,
  "sourceCodeText" : "void reduce(Text key, Iterator<Text> values, OutputCollector<Text, Text> output, Reporter reporter) throws IOException\n{\r\n    String field = key.toString();\r\n    reporter.setStatus(\"starting \" + field + \" ::host = \" + hostName);\r\n    if (field.startsWith(VALUE_TYPE_STRING)) {\r\n        StringBuffer sSum = new StringBuffer();\r\n        while (values.hasNext()) sSum.append(values.next().toString()).append(\";\");\r\n        output.collect(key, new Text(sSum.toString()));\r\n        reporter.setStatus(\"finished \" + field + \" ::host = \" + hostName);\r\n        return;\r\n    }\r\n    if (field.startsWith(VALUE_TYPE_FLOAT)) {\r\n        float fSum = 0;\r\n        while (values.hasNext()) fSum += Float.parseFloat(values.next().toString());\r\n        output.collect(key, new Text(String.valueOf(fSum)));\r\n        reporter.setStatus(\"finished \" + field + \" ::host = \" + hostName);\r\n        return;\r\n    }\r\n    if (field.startsWith(VALUE_TYPE_LONG)) {\r\n        long lSum = 0;\r\n        while (values.hasNext()) {\r\n            lSum += Long.parseLong(values.next().toString());\r\n        }\r\n        output.collect(key, new Text(String.valueOf(lSum)));\r\n    }\r\n    reporter.setStatus(\"finished \" + field + \" ::host = \" + hostName);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2",
  "methodName" : "setup",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void setup() throws IOException\n{\r\n    if (!(new File(MiniMRYarnCluster.APPJAR)).exists()) {\r\n        LOG.info(\"MRAppJar \" + MiniMRYarnCluster.APPJAR + \" not found. Not running test.\");\r\n        return;\r\n    }\r\n    if (mrCluster == null) {\r\n        mrCluster = new MiniMRYarnCluster(TestRMNMInfo.class.getName(), NUMNODEMANAGERS);\r\n        Configuration conf = new Configuration();\r\n        mrCluster.init(conf);\r\n        mrCluster.start();\r\n    }\r\n    localFs.copyFromLocalFile(new Path(MiniMRYarnCluster.APPJAR), APP_JAR);\r\n    localFs.setPermission(APP_JAR, new FsPermission(\"700\"));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2",
  "methodName" : "tearDown",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void tearDown()\n{\r\n    if (mrCluster != null) {\r\n        mrCluster.stop();\r\n        mrCluster = null;\r\n    }\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2",
  "methodName" : "testRMNMInfo",
  "errType" : null,
  "containingMethodsNum" : 21,
  "sourceCodeText" : "void testRMNMInfo() throws Exception\n{\r\n    if (!(new File(MiniMRYarnCluster.APPJAR)).exists()) {\r\n        LOG.info(\"MRAppJar \" + MiniMRYarnCluster.APPJAR + \" not found. Not running test.\");\r\n        return;\r\n    }\r\n    RMContext rmc = mrCluster.getResourceManager().getRMContext();\r\n    ResourceScheduler rms = mrCluster.getResourceManager().getResourceScheduler();\r\n    RMNMInfo rmInfo = new RMNMInfo(rmc, rms);\r\n    String liveNMs = rmInfo.getLiveNodeManagers();\r\n    ObjectMapper mapper = new ObjectMapper();\r\n    JsonNode jn = mapper.readTree(liveNMs);\r\n    Assert.assertEquals(\"Unexpected number of live nodes:\", NUMNODEMANAGERS, jn.size());\r\n    Iterator<JsonNode> it = jn.iterator();\r\n    while (it.hasNext()) {\r\n        JsonNode n = it.next();\r\n        Assert.assertNotNull(n.get(\"HostName\"));\r\n        Assert.assertNotNull(n.get(\"Rack\"));\r\n        Assert.assertTrue(\"Node \" + n.get(\"NodeId\") + \" should be RUNNING\", n.get(\"State\").asText().contains(\"RUNNING\"));\r\n        Assert.assertNotNull(n.get(\"NodeHTTPAddress\"));\r\n        Assert.assertNotNull(n.get(\"LastHealthUpdate\"));\r\n        Assert.assertNotNull(n.get(\"HealthReport\"));\r\n        Assert.assertNotNull(n.get(\"NodeManagerVersion\"));\r\n        Assert.assertNotNull(n.get(\"NumContainers\"));\r\n        Assert.assertEquals(n.get(\"NodeId\") + \": Unexpected number of used containers\", 0, n.get(\"NumContainers\").asInt());\r\n        Assert.assertEquals(n.get(\"NodeId\") + \": Unexpected amount of used memory\", 0, n.get(\"UsedMemoryMB\").asInt());\r\n        Assert.assertNotNull(n.get(\"AvailableMemoryMB\"));\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2",
  "methodName" : "testRMNMInfoMissmatch",
  "errType" : null,
  "containingMethodsNum" : 21,
  "sourceCodeText" : "void testRMNMInfoMissmatch() throws Exception\n{\r\n    RMContext rmc = mock(RMContext.class);\r\n    ResourceScheduler rms = mock(ResourceScheduler.class);\r\n    ConcurrentMap<NodeId, RMNode> map = new ConcurrentHashMap<NodeId, RMNode>();\r\n    RMNode node = MockNodes.newNodeInfo(1, MockNodes.newResource(4 * 1024));\r\n    map.put(node.getNodeID(), node);\r\n    when(rmc.getRMNodes()).thenReturn(map);\r\n    RMNMInfo rmInfo = new RMNMInfo(rmc, rms);\r\n    String liveNMs = rmInfo.getLiveNodeManagers();\r\n    ObjectMapper mapper = new ObjectMapper();\r\n    JsonNode jn = mapper.readTree(liveNMs);\r\n    Assert.assertEquals(\"Unexpected number of live nodes:\", 1, jn.size());\r\n    Iterator<JsonNode> it = jn.iterator();\r\n    while (it.hasNext()) {\r\n        JsonNode n = it.next();\r\n        Assert.assertNotNull(n.get(\"HostName\"));\r\n        Assert.assertNotNull(n.get(\"Rack\"));\r\n        Assert.assertTrue(\"Node \" + n.get(\"NodeId\") + \" should be RUNNING\", n.get(\"State\").asText().contains(\"RUNNING\"));\r\n        Assert.assertNotNull(n.get(\"NodeHTTPAddress\"));\r\n        Assert.assertNotNull(n.get(\"LastHealthUpdate\"));\r\n        Assert.assertNotNull(n.get(\"HealthReport\"));\r\n        Assert.assertNotNull(n.get(\"NodeManagerVersion\"));\r\n        Assert.assertNull(n.get(\"NumContainers\"));\r\n        Assert.assertNull(n.get(\"UsedMemoryMB\"));\r\n        Assert.assertNull(n.get(\"AvailableMemoryMB\"));\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred\\lib",
  "methodName" : "testOKRun",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testOKRun() throws Exception\n{\r\n    run(false, false);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred\\lib",
  "methodName" : "testIOExRun",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testIOExRun() throws Exception\n{\r\n    run(true, false);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred\\lib",
  "methodName" : "testRuntimeExRun",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testRuntimeExRun() throws Exception\n{\r\n    run(false, true);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred\\lib",
  "methodName" : "run",
  "errType" : null,
  "containingMethodsNum" : 33,
  "sourceCodeText" : "void run(boolean ioEx, boolean rtEx) throws Exception\n{\r\n    Path inDir = new Path(\"testing/mt/input\");\r\n    Path outDir = new Path(\"testing/mt/output\");\r\n    if (isLocalFS()) {\r\n        String localPathRoot = System.getProperty(\"test.build.data\", \"/tmp\").replace(' ', '+');\r\n        inDir = new Path(localPathRoot, inDir);\r\n        outDir = new Path(localPathRoot, outDir);\r\n    }\r\n    JobConf conf = createJobConf();\r\n    FileSystem fs = FileSystem.get(conf);\r\n    fs.delete(outDir, true);\r\n    if (!fs.mkdirs(inDir)) {\r\n        throw new IOException(\"Mkdirs failed to create \" + inDir.toString());\r\n    }\r\n    {\r\n        DataOutputStream file = fs.create(new Path(inDir, \"part-0\"));\r\n        file.writeBytes(\"a\\nb\\n\\nc\\nd\\ne\");\r\n        file.close();\r\n    }\r\n    conf.setJobName(\"mt\");\r\n    conf.setInputFormat(TextInputFormat.class);\r\n    conf.setOutputKeyClass(LongWritable.class);\r\n    conf.setOutputValueClass(Text.class);\r\n    conf.setMapOutputKeyClass(LongWritable.class);\r\n    conf.setMapOutputValueClass(Text.class);\r\n    conf.setOutputFormat(TextOutputFormat.class);\r\n    conf.setOutputKeyClass(LongWritable.class);\r\n    conf.setOutputValueClass(Text.class);\r\n    conf.setMapperClass(IDMap.class);\r\n    conf.setReducerClass(IDReduce.class);\r\n    FileInputFormat.setInputPaths(conf, inDir);\r\n    FileOutputFormat.setOutputPath(conf, outDir);\r\n    conf.setMapRunnerClass(MultithreadedMapRunner.class);\r\n    conf.setInt(MultithreadedMapper.NUM_THREADS, 2);\r\n    if (ioEx) {\r\n        conf.setBoolean(\"multithreaded.ioException\", true);\r\n    }\r\n    if (rtEx) {\r\n        conf.setBoolean(\"multithreaded.runtimeException\", true);\r\n    }\r\n    JobClient jc = new JobClient(conf);\r\n    RunningJob job = jc.submitJob(conf);\r\n    while (!job.isComplete()) {\r\n        Thread.sleep(100);\r\n    }\r\n    if (job.isSuccessful()) {\r\n        assertFalse(ioEx || rtEx);\r\n    } else {\r\n        assertTrue(ioEx || rtEx);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getTaskTrackerLocalDir",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getTaskTrackerLocalDir(int taskTracker)\n{\r\n    throw new UnsupportedOperationException();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getTaskTrackerLocalDirs",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String[] getTaskTrackerLocalDirs(int taskTracker)\n{\r\n    throw new UnsupportedOperationException();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getJobTrackerRunner",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "JobTrackerRunner getJobTrackerRunner()\n{\r\n    throw new UnsupportedOperationException();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getTaskTrackerRunner",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "TaskTrackerRunner getTaskTrackerRunner(int id)\n{\r\n    throw new UnsupportedOperationException();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getNumTaskTrackers",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int getNumTaskTrackers()\n{\r\n    throw new UnsupportedOperationException();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "setInlineCleanupThreads",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setInlineCleanupThreads()\n{\r\n    throw new UnsupportedOperationException();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "waitUntilIdle",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void waitUntilIdle()\n{\r\n    throw new UnsupportedOperationException();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "waitTaskTrackers",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void waitTaskTrackers()\n{\r\n    throw new UnsupportedOperationException();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getJobTrackerPort",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int getJobTrackerPort()\n{\r\n    throw new UnsupportedOperationException();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "createJobConf",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 2,
  "sourceCodeText" : "JobConf createJobConf()\n{\r\n    JobConf jobConf = null;\r\n    try {\r\n        jobConf = new JobConf(mrClientCluster.getConfig());\r\n    } catch (IOException e) {\r\n        LOG.error(e.getMessage());\r\n    }\r\n    return jobConf;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "createJobConf",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 2,
  "sourceCodeText" : "JobConf createJobConf(JobConf conf)\n{\r\n    JobConf jobConf = null;\r\n    try {\r\n        jobConf = new JobConf(mrClientCluster.getConfig());\r\n    } catch (IOException e) {\r\n        LOG.error(e.getMessage());\r\n    }\r\n    return jobConf;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "configureJobConf",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "JobConf configureJobConf(JobConf conf, String namenode, int jobTrackerPort, int jobTrackerInfoPort, UserGroupInformation ugi)\n{\r\n    throw new UnsupportedOperationException();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getUgi",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "UserGroupInformation getUgi()\n{\r\n    throw new UnsupportedOperationException();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getTaskCompletionEvents",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "TaskCompletionEvent[] getTaskCompletionEvents(JobID id, int from, int max) throws IOException\n{\r\n    throw new UnsupportedOperationException();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "setJobPriority",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setJobPriority(JobID jobId, JobPriority priority) throws AccessControlException, IOException\n{\r\n    throw new UnsupportedOperationException();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getJobPriority",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "JobPriority getJobPriority(JobID jobId)\n{\r\n    throw new UnsupportedOperationException();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getJobFinishTime",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getJobFinishTime(JobID jobId)\n{\r\n    throw new UnsupportedOperationException();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "initializeJob",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void initializeJob(JobID jobId) throws IOException\n{\r\n    throw new UnsupportedOperationException();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getMapTaskCompletionEventsUpdates",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "MapTaskCompletionEventsUpdate getMapTaskCompletionEventsUpdates(int index, JobID jobId, int max) throws IOException\n{\r\n    throw new UnsupportedOperationException();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getJobTrackerConf",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 2,
  "sourceCodeText" : "JobConf getJobTrackerConf()\n{\r\n    JobConf jobConf = null;\r\n    try {\r\n        jobConf = new JobConf(mrClientCluster.getConfig());\r\n    } catch (IOException e) {\r\n        LOG.error(e.getMessage());\r\n    }\r\n    return jobConf;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getFaultCount",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int getFaultCount(String hostName)\n{\r\n    throw new UnsupportedOperationException();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "startJobTracker",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void startJobTracker()\n{\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "startJobTracker",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void startJobTracker(boolean wait)\n{\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "stopJobTracker",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void stopJobTracker()\n{\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "stopTaskTracker",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void stopTaskTracker(int id)\n{\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "startTaskTracker",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void startTaskTracker(String host, String rack, int idx, int numDir) throws IOException\n{\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "addTaskTracker",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void addTaskTracker(TaskTrackerRunner taskTracker)\n{\r\n    throw new UnsupportedOperationException();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getTaskTrackerID",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int getTaskTrackerID(String trackerName)\n{\r\n    throw new UnsupportedOperationException();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "shutdown",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void shutdown()\n{\r\n    try {\r\n        mrClientCluster.stop();\r\n    } catch (IOException e) {\r\n        LOG.error(e.getMessage());\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getMrClientCluster",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "MiniMRClientCluster getMrClientCluster()\n{\r\n    return mrClientCluster;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "testFindContainingJar",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testFindContainingJar() throws Exception\n{\r\n    testJarAtPath(JAR_RELATIVE_PATH);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "testFindContainingJarWithPlus",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void testFindContainingJarWithPlus() throws Exception\n{\r\n    new File(TEST_DIR_WITH_SPECIAL_CHARS).mkdirs();\r\n    Configuration conf = new Configuration();\r\n    FileSystem localfs = FileSystem.getLocal(conf);\r\n    FileUtil.copy(localfs, new Path(JAR_RELATIVE_PATH), localfs, new Path(TEST_DIR_WITH_SPECIAL_CHARS, \"test.jar\"), false, true, conf);\r\n    testJarAtPath(TEST_DIR_WITH_SPECIAL_CHARS + File.separator + \"test.jar\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "testJarAtPath",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void testJarAtPath(String path) throws Exception\n{\r\n    File jar = new File(path).getAbsoluteFile();\r\n    assertTrue(jar.exists());\r\n    URL[] urls = new URL[] { jar.toURI().toURL() };\r\n    ClassLoader cl = new URLClassLoader(urls);\r\n    Class clazz = Class.forName(CLASSNAME, true, cl);\r\n    assertNotNull(clazz);\r\n    String containingJar = ClassUtil.findContainingJar(clazz);\r\n    assertEquals(jar.getAbsolutePath(), containingJar);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\fs\\slive",
  "methodName" : "getRenames",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "SrcTarget getRenames()\n{\r\n    Path src = getFinder().getFile();\r\n    Path target = getFinder().getFile();\r\n    return new SrcTarget(src, target);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\fs\\slive",
  "methodName" : "run",
  "errType" : [ "FileNotFoundException", "IOException" ],
  "containingMethodsNum" : 16,
  "sourceCodeText" : "List<OperationOutput> run(FileSystem fs)\n{\r\n    List<OperationOutput> out = super.run(fs);\r\n    try {\r\n        SrcTarget targets = getRenames();\r\n        Path src = targets.getSrc();\r\n        Path target = targets.getTarget();\r\n        boolean renamedOk = false;\r\n        long timeTaken = 0;\r\n        {\r\n            long startTime = Timer.now();\r\n            renamedOk = fs.rename(src, target);\r\n            timeTaken = Timer.elapsed(startTime);\r\n        }\r\n        if (renamedOk) {\r\n            out.add(new OperationOutput(OutputType.LONG, getType(), ReportWriter.OK_TIME_TAKEN, timeTaken));\r\n            out.add(new OperationOutput(OutputType.LONG, getType(), ReportWriter.SUCCESSES, 1L));\r\n            LOG.info(\"Renamed \" + src + \" to \" + target);\r\n        } else {\r\n            out.add(new OperationOutput(OutputType.LONG, getType(), ReportWriter.FAILURES, 1L));\r\n            LOG.warn(\"Could not rename \" + src + \" to \" + target);\r\n        }\r\n    } catch (FileNotFoundException e) {\r\n        out.add(new OperationOutput(OutputType.LONG, getType(), ReportWriter.NOT_FOUND, 1L));\r\n        LOG.warn(\"Error with renaming\", e);\r\n    } catch (IOException e) {\r\n        out.add(new OperationOutput(OutputType.LONG, getType(), ReportWriter.FAILURES, 1L));\r\n        LOG.warn(\"Error with renaming\", e);\r\n    }\r\n    return out;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred\\join",
  "methodName" : "makeTuple",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "TupleWritable makeTuple(Writable[] writs)\n{\r\n    Writable[] sub1 = { writs[1], writs[2] };\r\n    Writable[] sub3 = { writs[4], writs[5] };\r\n    Writable[] sub2 = { writs[3], new TupleWritable(sub3), writs[6] };\r\n    Writable[] vals = { writs[0], new TupleWritable(sub1), new TupleWritable(sub2), writs[7], writs[8], writs[9] };\r\n    TupleWritable ret = new TupleWritable(vals);\r\n    for (int i = 0; i < 6; ++i) {\r\n        ret.setWritten(i);\r\n    }\r\n    ((TupleWritable) sub2[1]).setWritten(0);\r\n    ((TupleWritable) sub2[1]).setWritten(1);\r\n    ((TupleWritable) vals[1]).setWritten(0);\r\n    ((TupleWritable) vals[1]).setWritten(1);\r\n    for (int i = 0; i < 3; ++i) {\r\n        ((TupleWritable) vals[2]).setWritten(i);\r\n    }\r\n    return ret;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred\\join",
  "methodName" : "makeRandomWritables",
  "errType" : null,
  "containingMethodsNum" : 10,
  "sourceCodeText" : "Writable[] makeRandomWritables()\n{\r\n    Random r = new Random();\r\n    Writable[] writs = { new BooleanWritable(r.nextBoolean()), new FloatWritable(r.nextFloat()), new FloatWritable(r.nextFloat()), new IntWritable(r.nextInt()), new LongWritable(r.nextLong()), new BytesWritable(\"dingo\".getBytes()), new LongWritable(r.nextLong()), new IntWritable(r.nextInt()), new BytesWritable(\"yak\".getBytes()), new IntWritable(r.nextInt()) };\r\n    return writs;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred\\join",
  "methodName" : "makeRandomWritables",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Writable[] makeRandomWritables(int numWrits)\n{\r\n    Writable[] writs = makeRandomWritables();\r\n    Writable[] manyWrits = new Writable[numWrits];\r\n    for (int i = 0; i < manyWrits.length; i++) {\r\n        manyWrits[i] = writs[i % writs.length];\r\n    }\r\n    return manyWrits;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred\\join",
  "methodName" : "verifIter",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "int verifIter(Writable[] writs, TupleWritable t, int i)\n{\r\n    for (Writable w : t) {\r\n        if (w instanceof TupleWritable) {\r\n            i = verifIter(writs, ((TupleWritable) w), i);\r\n            continue;\r\n        }\r\n        assertTrue(\"Bad value\", w.equals(writs[i++]));\r\n    }\r\n    return i;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred\\join",
  "methodName" : "testIterable",
  "errType" : null,
  "containingMethodsNum" : 12,
  "sourceCodeText" : "void testIterable() throws Exception\n{\r\n    Random r = new Random();\r\n    Writable[] writs = { new BooleanWritable(r.nextBoolean()), new FloatWritable(r.nextFloat()), new FloatWritable(r.nextFloat()), new IntWritable(r.nextInt()), new LongWritable(r.nextLong()), new BytesWritable(\"dingo\".getBytes()), new LongWritable(r.nextLong()), new IntWritable(r.nextInt()), new BytesWritable(\"yak\".getBytes()), new IntWritable(r.nextInt()) };\r\n    TupleWritable t = new TupleWritable(writs);\r\n    for (int i = 0; i < 6; ++i) {\r\n        t.setWritten(i);\r\n    }\r\n    verifIter(writs, t, 0);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred\\join",
  "methodName" : "testNestedIterable",
  "errType" : null,
  "containingMethodsNum" : 12,
  "sourceCodeText" : "void testNestedIterable() throws Exception\n{\r\n    Random r = new Random();\r\n    Writable[] writs = { new BooleanWritable(r.nextBoolean()), new FloatWritable(r.nextFloat()), new FloatWritable(r.nextFloat()), new IntWritable(r.nextInt()), new LongWritable(r.nextLong()), new BytesWritable(\"dingo\".getBytes()), new LongWritable(r.nextLong()), new IntWritable(r.nextInt()), new BytesWritable(\"yak\".getBytes()), new IntWritable(r.nextInt()) };\r\n    TupleWritable sTuple = makeTuple(writs);\r\n    assertTrue(\"Bad count\", writs.length == verifIter(writs, sTuple, 0));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred\\join",
  "methodName" : "testWritable",
  "errType" : null,
  "containingMethodsNum" : 15,
  "sourceCodeText" : "void testWritable() throws Exception\n{\r\n    Random r = new Random();\r\n    Writable[] writs = { new BooleanWritable(r.nextBoolean()), new FloatWritable(r.nextFloat()), new FloatWritable(r.nextFloat()), new IntWritable(r.nextInt()), new LongWritable(r.nextLong()), new BytesWritable(\"dingo\".getBytes()), new LongWritable(r.nextLong()), new IntWritable(r.nextInt()), new BytesWritable(\"yak\".getBytes()), new IntWritable(r.nextInt()) };\r\n    TupleWritable sTuple = makeTuple(writs);\r\n    ByteArrayOutputStream out = new ByteArrayOutputStream();\r\n    sTuple.write(new DataOutputStream(out));\r\n    ByteArrayInputStream in = new ByteArrayInputStream(out.toByteArray());\r\n    TupleWritable dTuple = new TupleWritable();\r\n    dTuple.readFields(new DataInputStream(in));\r\n    assertTrue(\"Failed to write/read tuple\", sTuple.equals(dTuple));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred\\join",
  "methodName" : "testWideWritable",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void testWideWritable() throws Exception\n{\r\n    Writable[] manyWrits = makeRandomWritables(131);\r\n    TupleWritable sTuple = new TupleWritable(manyWrits);\r\n    for (int i = 0; i < manyWrits.length; i++) {\r\n        if (i % 3 == 0) {\r\n            sTuple.setWritten(i);\r\n        }\r\n    }\r\n    ByteArrayOutputStream out = new ByteArrayOutputStream();\r\n    sTuple.write(new DataOutputStream(out));\r\n    ByteArrayInputStream in = new ByteArrayInputStream(out.toByteArray());\r\n    TupleWritable dTuple = new TupleWritable();\r\n    dTuple.readFields(new DataInputStream(in));\r\n    assertTrue(\"Failed to write/read tuple\", sTuple.equals(dTuple));\r\n    assertEquals(\"All tuple data has not been read from the stream\", -1, in.read());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred\\join",
  "methodName" : "testWideWritable2",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void testWideWritable2() throws Exception\n{\r\n    Writable[] manyWrits = makeRandomWritables(71);\r\n    TupleWritable sTuple = new TupleWritable(manyWrits);\r\n    for (int i = 0; i < manyWrits.length; i++) {\r\n        sTuple.setWritten(i);\r\n    }\r\n    ByteArrayOutputStream out = new ByteArrayOutputStream();\r\n    sTuple.write(new DataOutputStream(out));\r\n    ByteArrayInputStream in = new ByteArrayInputStream(out.toByteArray());\r\n    TupleWritable dTuple = new TupleWritable();\r\n    dTuple.readFields(new DataInputStream(in));\r\n    assertTrue(\"Failed to write/read tuple\", sTuple.equals(dTuple));\r\n    assertEquals(\"All tuple data has not been read from the stream\", -1, in.read());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred\\join",
  "methodName" : "testSparseWideWritable",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void testSparseWideWritable() throws Exception\n{\r\n    Writable[] manyWrits = makeRandomWritables(131);\r\n    TupleWritable sTuple = new TupleWritable(manyWrits);\r\n    for (int i = 0; i < manyWrits.length; i++) {\r\n        if (i % 65 == 0) {\r\n            sTuple.setWritten(i);\r\n        }\r\n    }\r\n    ByteArrayOutputStream out = new ByteArrayOutputStream();\r\n    sTuple.write(new DataOutputStream(out));\r\n    ByteArrayInputStream in = new ByteArrayInputStream(out.toByteArray());\r\n    TupleWritable dTuple = new TupleWritable();\r\n    dTuple.readFields(new DataInputStream(in));\r\n    assertTrue(\"Failed to write/read tuple\", sTuple.equals(dTuple));\r\n    assertEquals(\"All tuple data has not been read from the stream\", -1, in.read());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred\\join",
  "methodName" : "testWideTuple",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void testWideTuple() throws Exception\n{\r\n    Text emptyText = new Text(\"Should be empty\");\r\n    Writable[] values = new Writable[64];\r\n    Arrays.fill(values, emptyText);\r\n    values[42] = new Text(\"Number 42\");\r\n    TupleWritable tuple = new TupleWritable(values);\r\n    tuple.setWritten(42);\r\n    for (int pos = 0; pos < tuple.size(); pos++) {\r\n        boolean has = tuple.has(pos);\r\n        if (pos == 42) {\r\n            assertTrue(has);\r\n        } else {\r\n            assertFalse(\"Tuple position is incorrectly labelled as set: \" + pos, has);\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred\\join",
  "methodName" : "testWideTuple2",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void testWideTuple2() throws Exception\n{\r\n    Text emptyText = new Text(\"Should be empty\");\r\n    Writable[] values = new Writable[64];\r\n    Arrays.fill(values, emptyText);\r\n    values[9] = new Text(\"Number 9\");\r\n    TupleWritable tuple = new TupleWritable(values);\r\n    tuple.setWritten(9);\r\n    for (int pos = 0; pos < tuple.size(); pos++) {\r\n        boolean has = tuple.has(pos);\r\n        if (pos == 9) {\r\n            assertTrue(has);\r\n        } else {\r\n            assertFalse(\"Tuple position is incorrectly labelled as set: \" + pos, has);\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred\\join",
  "methodName" : "testWideTupleBoundary",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void testWideTupleBoundary() throws Exception\n{\r\n    Text emptyText = new Text(\"Should not be set written\");\r\n    Writable[] values = new Writable[65];\r\n    Arrays.fill(values, emptyText);\r\n    values[64] = new Text(\"Should be the only value set written\");\r\n    TupleWritable tuple = new TupleWritable(values);\r\n    tuple.setWritten(64);\r\n    for (int pos = 0; pos < tuple.size(); pos++) {\r\n        boolean has = tuple.has(pos);\r\n        if (pos == 64) {\r\n            assertTrue(has);\r\n        } else {\r\n            assertFalse(\"Tuple position is incorrectly labelled as set: \" + pos, has);\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred\\join",
  "methodName" : "testPreVersion21Compatibility",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void testPreVersion21Compatibility() throws Exception\n{\r\n    Writable[] manyWrits = makeRandomWritables(64);\r\n    PreVersion21TupleWritable oldTuple = new PreVersion21TupleWritable(manyWrits);\r\n    for (int i = 0; i < manyWrits.length; i++) {\r\n        if (i % 3 == 0) {\r\n            oldTuple.setWritten(i);\r\n        }\r\n    }\r\n    ByteArrayOutputStream out = new ByteArrayOutputStream();\r\n    oldTuple.write(new DataOutputStream(out));\r\n    ByteArrayInputStream in = new ByteArrayInputStream(out.toByteArray());\r\n    TupleWritable dTuple = new TupleWritable();\r\n    dTuple.readFields(new DataInputStream(in));\r\n    assertTrue(\"Tuple writable is unable to read pre-0.21 versions of TupleWritable\", oldTuple.isCompatible(dTuple));\r\n    assertEquals(\"All tuple data has not been read from the stream\", -1, in.read());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred\\join",
  "methodName" : "testPreVersion21CompatibilityEmptyTuple",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void testPreVersion21CompatibilityEmptyTuple() throws Exception\n{\r\n    Writable[] manyWrits = new Writable[0];\r\n    PreVersion21TupleWritable oldTuple = new PreVersion21TupleWritable(manyWrits);\r\n    ByteArrayOutputStream out = new ByteArrayOutputStream();\r\n    oldTuple.write(new DataOutputStream(out));\r\n    ByteArrayInputStream in = new ByteArrayInputStream(out.toByteArray());\r\n    TupleWritable dTuple = new TupleWritable();\r\n    dTuple.readFields(new DataInputStream(in));\r\n    assertTrue(\"Tuple writable is unable to read pre-0.21 versions of TupleWritable\", oldTuple.isCompatible(dTuple));\r\n    assertEquals(\"All tuple data has not been read from the stream\", -1, in.read());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\jobcontrol",
  "methodName" : "testSuccessfulJobs",
  "errType" : null,
  "containingMethodsNum" : 12,
  "sourceCodeText" : "void testSuccessfulJobs() throws Exception\n{\r\n    JobControl jobControl = new JobControl(\"Test\");\r\n    ControlledJob job1 = createSuccessfulControlledJob(jobControl);\r\n    ControlledJob job2 = createSuccessfulControlledJob(jobControl);\r\n    ControlledJob job3 = createSuccessfulControlledJob(jobControl, job1, job2);\r\n    ControlledJob job4 = createSuccessfulControlledJob(jobControl, job3);\r\n    runJobControl(jobControl);\r\n    assertEquals(\"Success list\", 4, jobControl.getSuccessfulJobList().size());\r\n    assertEquals(\"Failed list\", 0, jobControl.getFailedJobList().size());\r\n    assertEquals(ControlledJob.State.SUCCESS, job1.getJobState());\r\n    assertEquals(ControlledJob.State.SUCCESS, job2.getJobState());\r\n    assertEquals(ControlledJob.State.SUCCESS, job3.getJobState());\r\n    assertEquals(ControlledJob.State.SUCCESS, job4.getJobState());\r\n    jobControl.stop();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\jobcontrol",
  "methodName" : "testFailedJob",
  "errType" : null,
  "containingMethodsNum" : 12,
  "sourceCodeText" : "void testFailedJob() throws Exception\n{\r\n    JobControl jobControl = new JobControl(\"Test\");\r\n    ControlledJob job1 = createFailedControlledJob(jobControl);\r\n    ControlledJob job2 = createSuccessfulControlledJob(jobControl);\r\n    ControlledJob job3 = createSuccessfulControlledJob(jobControl, job1, job2);\r\n    ControlledJob job4 = createSuccessfulControlledJob(jobControl, job3);\r\n    runJobControl(jobControl);\r\n    assertEquals(\"Success list\", 1, jobControl.getSuccessfulJobList().size());\r\n    assertEquals(\"Failed list\", 3, jobControl.getFailedJobList().size());\r\n    assertEquals(ControlledJob.State.FAILED, job1.getJobState());\r\n    assertEquals(ControlledJob.State.SUCCESS, job2.getJobState());\r\n    assertEquals(ControlledJob.State.DEPENDENT_FAILED, job3.getJobState());\r\n    assertEquals(ControlledJob.State.DEPENDENT_FAILED, job4.getJobState());\r\n    jobControl.stop();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\jobcontrol",
  "methodName" : "testErrorWhileSubmitting",
  "errType" : null,
  "containingMethodsNum" : 9,
  "sourceCodeText" : "void testErrorWhileSubmitting() throws Exception\n{\r\n    JobControl jobControl = new JobControl(\"Test\");\r\n    Job mockJob = mock(Job.class);\r\n    ControlledJob job1 = new ControlledJob(mockJob, null);\r\n    when(mockJob.getConfiguration()).thenReturn(new Configuration());\r\n    doThrow(new IncompatibleClassChangeError(\"This is a test\")).when(mockJob).submit();\r\n    jobControl.addJob(job1);\r\n    runJobControl(jobControl);\r\n    try {\r\n        assertEquals(\"Success list\", 0, jobControl.getSuccessfulJobList().size());\r\n        assertEquals(\"Failed list\", 1, jobControl.getFailedJobList().size());\r\n        assertEquals(ControlledJob.State.FAILED, job1.getJobState());\r\n    } finally {\r\n        jobControl.stop();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\jobcontrol",
  "methodName" : "testKillJob",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void testKillJob() throws Exception\n{\r\n    JobControl jobControl = new JobControl(\"Test\");\r\n    ControlledJob job = createFailedControlledJob(jobControl);\r\n    job.killJob();\r\n    verify(job.getJob()).killJob();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\jobcontrol",
  "methodName" : "createJob",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "Job createJob(boolean complete, boolean successful) throws IOException, InterruptedException\n{\r\n    Job mockJob = mock(Job.class);\r\n    when(mockJob.getConfiguration()).thenReturn(new Configuration());\r\n    when(mockJob.isComplete()).thenReturn(complete);\r\n    when(mockJob.isSuccessful()).thenReturn(successful);\r\n    return mockJob;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\jobcontrol",
  "methodName" : "createControlledJob",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "ControlledJob createControlledJob(JobControl jobControl, boolean successful, ControlledJob... dependingJobs) throws IOException, InterruptedException\n{\r\n    List<ControlledJob> dependingJobsList = dependingJobs == null ? null : Arrays.asList(dependingJobs);\r\n    ControlledJob job = new ControlledJob(createJob(true, successful), dependingJobsList);\r\n    jobControl.addJob(job);\r\n    return job;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\jobcontrol",
  "methodName" : "createSuccessfulControlledJob",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "ControlledJob createSuccessfulControlledJob(JobControl jobControl, ControlledJob... dependingJobs) throws IOException, InterruptedException\n{\r\n    return createControlledJob(jobControl, true, dependingJobs);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\jobcontrol",
  "methodName" : "createFailedControlledJob",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "ControlledJob createFailedControlledJob(JobControl jobControl, ControlledJob... dependingJobs) throws IOException, InterruptedException\n{\r\n    return createControlledJob(jobControl, false, dependingJobs);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\jobcontrol",
  "methodName" : "runJobControl",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void runJobControl(JobControl jobControl)\n{\r\n    Thread controller = new Thread(jobControl);\r\n    controller.start();\r\n    waitTillAllFinished(jobControl);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\jobcontrol",
  "methodName" : "waitTillAllFinished",
  "errType" : [ "InterruptedException" ],
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void waitTillAllFinished(JobControl jobControl)\n{\r\n    while (!jobControl.allFinished()) {\r\n        try {\r\n            Thread.sleep(100);\r\n        } catch (InterruptedException e) {\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\fs\\slive",
  "methodName" : "generatePartialSegment",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "GenerateResult generatePartialSegment(int byteAm, long offset, DataHasher hasher)\n{\r\n    if (byteAm > BYTES_PER_LONG) {\r\n        throw new IllegalArgumentException(\"Partial bytes must be less or equal to \" + BYTES_PER_LONG);\r\n    }\r\n    if (byteAm <= 0) {\r\n        throw new IllegalArgumentException(\"Partial bytes must be greater than zero and not \" + byteAm);\r\n    }\r\n    ByteBuffer buf = ByteBuffer.wrap(new byte[BYTES_PER_LONG]);\r\n    buf.putLong(hasher.generate(offset));\r\n    ByteBuffer allBytes = ByteBuffer.wrap(new byte[byteAm]);\r\n    buf.rewind();\r\n    for (int i = 0; i < byteAm; ++i) {\r\n        allBytes.put(buf.get());\r\n    }\r\n    allBytes.rewind();\r\n    return new GenerateResult(offset, allBytes);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\fs\\slive",
  "methodName" : "generateFullSegment",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "GenerateResult generateFullSegment(int byteAm, long startOffset, DataHasher hasher)\n{\r\n    if (byteAm <= 0) {\r\n        throw new IllegalArgumentException(\"Byte amount must be greater than zero and not \" + byteAm);\r\n    }\r\n    if ((byteAm % BYTES_PER_LONG) != 0) {\r\n        throw new IllegalArgumentException(\"Byte amount \" + byteAm + \" must be a multiple of \" + BYTES_PER_LONG);\r\n    }\r\n    ByteBuffer allBytes = ByteBuffer.wrap(new byte[byteAm]);\r\n    long offset = startOffset;\r\n    ByteBuffer buf = ByteBuffer.wrap(new byte[BYTES_PER_LONG]);\r\n    for (long i = 0; i < byteAm; i += BYTES_PER_LONG) {\r\n        buf.rewind();\r\n        buf.putLong(hasher.generate(offset));\r\n        buf.rewind();\r\n        allBytes.put(buf);\r\n        offset += BYTES_PER_LONG;\r\n    }\r\n    allBytes.rewind();\r\n    return new GenerateResult(offset, allBytes);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\fs\\slive",
  "methodName" : "writePieces",
  "errType" : null,
  "containingMethodsNum" : 21,
  "sourceCodeText" : "GenerateOutput writePieces(long byteAm, long startPos, DataHasher hasher, OutputStream out) throws IOException\n{\r\n    if (byteAm <= 0) {\r\n        return new GenerateOutput(0, 0);\r\n    }\r\n    if (startPos < 0) {\r\n        startPos = 0;\r\n    }\r\n    int leftOver = (int) (byteAm % bufferSize);\r\n    long fullPieces = byteAm / bufferSize;\r\n    long offset = startPos;\r\n    long bytesWritten = 0;\r\n    long timeTaken = 0;\r\n    for (long i = 0; i < fullPieces; ++i) {\r\n        GenerateResult genData = generateFullSegment(bufferSize, offset, hasher);\r\n        offset = genData.getOffset();\r\n        ByteBuffer gBuf = genData.getBuffer();\r\n        {\r\n            byte[] buf = gBuf.array();\r\n            long startTime = Timer.now();\r\n            out.write(buf);\r\n            if (Constants.FLUSH_WRITES) {\r\n                out.flush();\r\n            }\r\n            timeTaken += Timer.elapsed(startTime);\r\n            bytesWritten += buf.length;\r\n        }\r\n    }\r\n    if (leftOver > 0) {\r\n        ByteBuffer leftOverBuf = ByteBuffer.wrap(new byte[leftOver]);\r\n        int bytesLeft = leftOver % BYTES_PER_LONG;\r\n        leftOver = leftOver - bytesLeft;\r\n        if (leftOver > 0) {\r\n            GenerateResult genData = generateFullSegment(leftOver, offset, hasher);\r\n            offset = genData.getOffset();\r\n            leftOverBuf.put(genData.getBuffer());\r\n        }\r\n        if (bytesLeft > 0) {\r\n            GenerateResult genData = generatePartialSegment(bytesLeft, offset, hasher);\r\n            offset = genData.getOffset();\r\n            leftOverBuf.put(genData.getBuffer());\r\n        }\r\n        leftOverBuf.rewind();\r\n        {\r\n            byte[] buf = leftOverBuf.array();\r\n            long startTime = Timer.now();\r\n            out.write(buf);\r\n            if (Constants.FLUSH_WRITES) {\r\n                out.flush();\r\n            }\r\n            timeTaken += Timer.elapsed(startTime);\r\n            bytesWritten += buf.length;\r\n        }\r\n    }\r\n    return new GenerateOutput(bytesWritten, timeTaken);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\fs\\slive",
  "methodName" : "writeSegment",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "GenerateOutput writeSegment(long byteAm, OutputStream out) throws IOException\n{\r\n    long headerLen = getHeaderLength();\r\n    if (byteAm < headerLen) {\r\n        return new GenerateOutput(0, 0);\r\n    }\r\n    byteAm -= headerLen;\r\n    if (byteAm < 0) {\r\n        byteAm = 0;\r\n    }\r\n    WriteInfo header = writeHeader(out, byteAm);\r\n    DataHasher hasher = new DataHasher(header.getHashValue());\r\n    GenerateOutput pRes = writePieces(byteAm, 0, hasher, out);\r\n    long bytesWritten = pRes.getBytesWritten() + header.getBytesWritten();\r\n    long timeTaken = header.getTimeTaken() + pRes.getTimeTaken();\r\n    return new GenerateOutput(bytesWritten, timeTaken);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\fs\\slive",
  "methodName" : "getHeaderLength",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int getHeaderLength()\n{\r\n    return HEADER_LENGTH;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\fs\\slive",
  "methodName" : "writeHeader",
  "errType" : null,
  "containingMethodsNum" : 11,
  "sourceCodeText" : "WriteInfo writeHeader(OutputStream os, long fileSize) throws IOException\n{\r\n    int headerLen = getHeaderLength();\r\n    ByteBuffer buf = ByteBuffer.wrap(new byte[headerLen]);\r\n    long hash = rnd.nextLong();\r\n    buf.putLong(hash);\r\n    buf.putLong(fileSize);\r\n    buf.rewind();\r\n    byte[] headerData = buf.array();\r\n    long elapsed = 0;\r\n    {\r\n        long startTime = Timer.now();\r\n        os.write(headerData);\r\n        if (Constants.FLUSH_WRITES) {\r\n            os.flush();\r\n        }\r\n        elapsed += Timer.elapsed(startTime);\r\n    }\r\n    return new WriteInfo(hash, headerLen, elapsed);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\fs\\slive",
  "methodName" : "select",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "Operation select(List<OperationWeight> ops)\n{\r\n    if (ops.isEmpty()) {\r\n        return null;\r\n    }\r\n    double totalWeight = 0;\r\n    for (OperationWeight w : ops) {\r\n        if (w.getWeight() < 0) {\r\n            throw new IllegalArgumentException(\"Negative weights not allowed\");\r\n        }\r\n        totalWeight += w.getWeight();\r\n    }\r\n    double sAm = picker.nextDouble() * totalWeight;\r\n    int index = 0;\r\n    for (int i = 0; i < ops.size(); ++i) {\r\n        sAm -= ops.get(i).getWeight();\r\n        if (sAm <= 0) {\r\n            index = i;\r\n            break;\r\n        }\r\n    }\r\n    return ops.get(index).getOperation();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "createInputFile",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void createInputFile(Path dirPath, int id, int numRecords) throws IOException\n{\r\n    final String MESSAGE = \"This is a line in a file: \";\r\n    Path filePath = new Path(dirPath, \"\" + id);\r\n    Configuration conf = new Configuration();\r\n    FileSystem fs = FileSystem.getLocal(conf);\r\n    OutputStream os = fs.create(filePath);\r\n    BufferedWriter w = new BufferedWriter(new OutputStreamWriter(os));\r\n    for (int i = 0; i < numRecords; i++) {\r\n        w.write(MESSAGE + id + \" \" + i + \"\\n\");\r\n    }\r\n    w.close();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "getInputPath",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Path getInputPath()\n{\r\n    String dataDir = System.getProperty(\"test.build.data\");\r\n    if (null == dataDir) {\r\n        return new Path(INPUT_DIR);\r\n    } else {\r\n        return new Path(new Path(dataDir), INPUT_DIR);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "getOutputPath",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Path getOutputPath()\n{\r\n    String dataDir = System.getProperty(\"test.build.data\");\r\n    if (null == dataDir) {\r\n        return new Path(OUTPUT_DIR);\r\n    } else {\r\n        return new Path(new Path(dataDir), OUTPUT_DIR);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "createMultiMapsInput",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "Path createMultiMapsInput() throws IOException\n{\r\n    Configuration conf = new Configuration();\r\n    FileSystem fs = FileSystem.getLocal(conf);\r\n    Path inputPath = getInputPath();\r\n    if (fs.exists(inputPath)) {\r\n        fs.delete(inputPath, true);\r\n    }\r\n    for (int i = 0; i < 6; i++) {\r\n        createInputFile(inputPath, i, INPUT_SIZES[i]);\r\n    }\r\n    return inputPath;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "verifyOutput",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void verifyOutput(Path outputPath) throws IOException\n{\r\n    Configuration conf = new Configuration();\r\n    FileSystem fs = FileSystem.getLocal(conf);\r\n    Path outputFile = new Path(outputPath, \"part-r-00000\");\r\n    InputStream is = fs.open(outputFile);\r\n    BufferedReader r = new BufferedReader(new InputStreamReader(is));\r\n    String line = r.readLine().trim();\r\n    assertTrue(\"Line does not have correct key\", line.startsWith(\"0\\t\"));\r\n    int count = Integer.valueOf(line.substring(2));\r\n    assertEquals(\"Incorrect count generated!\", TOTAL_RECORDS, count);\r\n    r.close();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "testGcCounter",
  "errType" : null,
  "containingMethodsNum" : 19,
  "sourceCodeText" : "void testGcCounter() throws Exception\n{\r\n    Path inputPath = getInputPath();\r\n    Path outputPath = getOutputPath();\r\n    Configuration conf = new Configuration();\r\n    FileSystem fs = FileSystem.getLocal(conf);\r\n    if (fs.exists(outputPath)) {\r\n        fs.delete(outputPath, true);\r\n    }\r\n    if (fs.exists(inputPath)) {\r\n        fs.delete(inputPath, true);\r\n    }\r\n    createInputFile(inputPath, 0, 20);\r\n    Job job = Job.getInstance();\r\n    job.setMapperClass(GCMapper.class);\r\n    job.setNumReduceTasks(0);\r\n    job.getConfiguration().set(MRJobConfig.IO_SORT_MB, \"25\");\r\n    FileInputFormat.addInputPath(job, inputPath);\r\n    FileOutputFormat.setOutputPath(job, outputPath);\r\n    boolean ret = job.waitForCompletion(true);\r\n    assertTrue(\"job failed\", ret);\r\n    Counter gcCounter = job.getCounters().findCounter(TaskCounter.GC_TIME_MILLIS);\r\n    assertNotNull(gcCounter);\r\n    assertTrue(\"No time spent in gc\", gcCounter.getValue() > 0);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "testMultiMaps",
  "errType" : [ "InterruptedException", "InterruptedException", "InterruptedException" ],
  "containingMethodsNum" : 31,
  "sourceCodeText" : "void testMultiMaps() throws Exception\n{\r\n    Job job = Job.getInstance();\r\n    Path inputPath = createMultiMapsInput();\r\n    Path outputPath = getOutputPath();\r\n    Configuration conf = new Configuration();\r\n    FileSystem fs = FileSystem.getLocal(conf);\r\n    if (fs.exists(outputPath)) {\r\n        fs.delete(outputPath, true);\r\n    }\r\n    job.setMapperClass(StressMapper.class);\r\n    job.setReducerClass(CountingReducer.class);\r\n    job.setNumReduceTasks(1);\r\n    LocalJobRunner.setLocalMaxRunningMaps(job, 6);\r\n    job.getConfiguration().set(MRJobConfig.IO_SORT_MB, \"25\");\r\n    FileInputFormat.addInputPath(job, inputPath);\r\n    FileOutputFormat.setOutputPath(job, outputPath);\r\n    final Thread toInterrupt = Thread.currentThread();\r\n    Thread interrupter = new Thread() {\r\n\r\n        public void run() {\r\n            try {\r\n                Thread.sleep(120 * 1000);\r\n                toInterrupt.interrupt();\r\n            } catch (InterruptedException ie) {\r\n            }\r\n        }\r\n    };\r\n    LOG.info(\"Submitting job...\");\r\n    job.submit();\r\n    LOG.info(\"Starting thread to interrupt main thread in 2 minutes\");\r\n    interrupter.start();\r\n    LOG.info(\"Waiting for job to complete...\");\r\n    try {\r\n        job.waitForCompletion(true);\r\n    } catch (InterruptedException ie) {\r\n        LOG.error(\"Interrupted while waiting for job completion\", ie);\r\n        for (int i = 0; i < 10; i++) {\r\n            LOG.error(\"Dumping stacks\");\r\n            ReflectionUtils.logThreadInfo(LOG, \"multimap threads\", 0);\r\n            Thread.sleep(1000);\r\n        }\r\n        throw ie;\r\n    }\r\n    LOG.info(\"Job completed, stopping interrupter\");\r\n    interrupter.interrupt();\r\n    try {\r\n        interrupter.join();\r\n    } catch (InterruptedException ie) {\r\n    }\r\n    LOG.info(\"Verifying output\");\r\n    verifyOutput(outputPath);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 3,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "testInvalidMultiMapParallelism",
  "errType" : null,
  "containingMethodsNum" : 14,
  "sourceCodeText" : "void testInvalidMultiMapParallelism() throws Exception\n{\r\n    Job job = Job.getInstance();\r\n    Path inputPath = createMultiMapsInput();\r\n    Path outputPath = getOutputPath();\r\n    Configuration conf = new Configuration();\r\n    FileSystem fs = FileSystem.getLocal(conf);\r\n    if (fs.exists(outputPath)) {\r\n        fs.delete(outputPath, true);\r\n    }\r\n    job.setMapperClass(StressMapper.class);\r\n    job.setReducerClass(CountingReducer.class);\r\n    job.setNumReduceTasks(1);\r\n    LocalJobRunner.setLocalMaxRunningMaps(job, -6);\r\n    FileInputFormat.addInputPath(job, inputPath);\r\n    FileOutputFormat.setOutputPath(job, outputPath);\r\n    boolean success = job.waitForCompletion(true);\r\n    assertFalse(\"Job succeeded somehow\", success);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "testEmptyMaps",
  "errType" : null,
  "containingMethodsNum" : 10,
  "sourceCodeText" : "void testEmptyMaps() throws Exception\n{\r\n    Job job = Job.getInstance();\r\n    Path outputPath = getOutputPath();\r\n    Configuration conf = new Configuration();\r\n    FileSystem fs = FileSystem.getLocal(conf);\r\n    if (fs.exists(outputPath)) {\r\n        fs.delete(outputPath, true);\r\n    }\r\n    job.setInputFormatClass(EmptyInputFormat.class);\r\n    job.setNumReduceTasks(1);\r\n    FileOutputFormat.setOutputPath(job, outputPath);\r\n    boolean success = job.waitForCompletion(true);\r\n    assertTrue(\"Empty job should work\", success);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "getNumberDirPath",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Path getNumberDirPath()\n{\r\n    return new Path(getInputPath(), \"numberfiles\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "makeNumberFile",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "Path makeNumberFile(int fileNum, int value) throws IOException\n{\r\n    Path workDir = getNumberDirPath();\r\n    Path filePath = new Path(workDir, \"file\" + fileNum);\r\n    Configuration conf = new Configuration();\r\n    FileSystem fs = FileSystem.getLocal(conf);\r\n    OutputStream os = fs.create(filePath);\r\n    BufferedWriter w = new BufferedWriter(new OutputStreamWriter(os));\r\n    w.write(\"\" + value);\r\n    w.close();\r\n    return filePath;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "verifyNumberJob",
  "errType" : null,
  "containingMethodsNum" : 9,
  "sourceCodeText" : "void verifyNumberJob(int numMaps) throws Exception\n{\r\n    Path outputDir = getOutputPath();\r\n    Configuration conf = new Configuration();\r\n    FileSystem fs = FileSystem.getLocal(conf);\r\n    FileStatus[] stats = fs.listStatus(outputDir);\r\n    int valueSum = 0;\r\n    for (FileStatus f : stats) {\r\n        FSDataInputStream istream = fs.open(f.getPath());\r\n        BufferedReader r = new BufferedReader(new InputStreamReader(istream));\r\n        String line = null;\r\n        while ((line = r.readLine()) != null) {\r\n            valueSum += Integer.valueOf(line.trim());\r\n        }\r\n        r.close();\r\n    }\r\n    int maxVal = NUMBER_FILE_VAL - 1;\r\n    int expectedPerMapper = maxVal * (maxVal + 1) / 2;\r\n    int expectedSum = expectedPerMapper * numMaps;\r\n    LOG.info(\"expected sum: \" + expectedSum + \", got \" + valueSum);\r\n    assertEquals(\"Didn't get all our results back\", expectedSum, valueSum);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "doMultiReducerTest",
  "errType" : null,
  "containingMethodsNum" : 20,
  "sourceCodeText" : "void doMultiReducerTest(int numMaps, int numReduces, int parallelMaps, int parallelReduces) throws Exception\n{\r\n    Path in = getNumberDirPath();\r\n    Path out = getOutputPath();\r\n    Configuration conf = new Configuration();\r\n    FileSystem fs = FileSystem.getLocal(conf);\r\n    if (fs.exists(out)) {\r\n        fs.delete(out, true);\r\n    }\r\n    if (fs.exists(in)) {\r\n        fs.delete(in, true);\r\n    }\r\n    for (int i = 0; i < numMaps; i++) {\r\n        makeNumberFile(i, 100);\r\n    }\r\n    Job job = Job.getInstance();\r\n    job.setNumReduceTasks(numReduces);\r\n    job.setMapperClass(SequenceMapper.class);\r\n    job.setOutputKeyClass(Text.class);\r\n    job.setOutputValueClass(NullWritable.class);\r\n    FileInputFormat.addInputPath(job, in);\r\n    FileOutputFormat.setOutputPath(job, out);\r\n    LocalJobRunner.setLocalMaxRunningMaps(job, parallelMaps);\r\n    LocalJobRunner.setLocalMaxRunningReduces(job, parallelReduces);\r\n    boolean result = job.waitForCompletion(true);\r\n    assertTrue(\"Job failed!!\", result);\r\n    verifyNumberJob(numMaps);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "testOneMapMultiReduce",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testOneMapMultiReduce() throws Exception\n{\r\n    doMultiReducerTest(1, 2, 1, 1);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "testOneMapMultiParallelReduce",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testOneMapMultiParallelReduce() throws Exception\n{\r\n    doMultiReducerTest(1, 2, 1, 2);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "testMultiMapOneReduce",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testMultiMapOneReduce() throws Exception\n{\r\n    doMultiReducerTest(4, 1, 2, 1);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "testMultiMapMultiReduce",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testMultiMapMultiReduce() throws Exception\n{\r\n    doMultiReducerTest(4, 4, 2, 2);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\fs",
  "methodName" : "setUp",
  "errType" : null,
  "containingMethodsNum" : 142,
  "sourceCodeText" : "void setUp() throws Exception\n{\r\n    File logFile = new File(historyLog);\r\n    if (!logFile.getParentFile().exists())\r\n        if (!logFile.getParentFile().mkdirs())\r\n            LOG.error(\"Cannot create dirs for history log file: \" + historyLog);\r\n    if (!logFile.createNewFile())\r\n        LOG.error(\"Cannot create history log file: \" + historyLog);\r\n    BufferedWriter writer = new BufferedWriter(new OutputStreamWriter(new FileOutputStream(historyLog)));\r\n    writer.write(\"$!!FILE=file1.log!!\");\r\n    writer.newLine();\r\n    writer.write(\"Meta VERSION=\\\"1\\\" .\");\r\n    writer.newLine();\r\n    writer.write(\"Job JOBID=\\\"job_200903250600_0004\\\" JOBNAME=\\\"streamjob21364.jar\\\" USER=\\\"hadoop\\\" SUBMIT_TIME=\\\"1237962008012\\\" JOBCONF=\\\"hdfs:///job_200903250600_0004/job.xml\\\" .\");\r\n    writer.newLine();\r\n    writer.write(\"Job JOBID=\\\"job_200903250600_0004\\\" JOB_PRIORITY=\\\"NORMAL\\\" .\");\r\n    writer.newLine();\r\n    writer.write(\"Job JOBID=\\\"job_200903250600_0004\\\" LAUNCH_TIME=\\\"1237962008712\\\" TOTAL_MAPS=\\\"2\\\" TOTAL_REDUCES=\\\"0\\\" JOB_STATUS=\\\"PREP\\\" .\");\r\n    writer.newLine();\r\n    writer.write(\"Task TASKID=\\\"task_200903250600_0004_m_000003\\\" TASK_TYPE=\\\"SETUP\\\" START_TIME=\\\"1237962008736\\\" SPLITS=\\\"\\\" .\");\r\n    writer.newLine();\r\n    writer.write(\"MapAttempt TASK_TYPE=\\\"SETUP\\\" TASKID=\\\"task_200903250600_0004_m_000003\\\" TASK_ATTEMPT_ID=\\\"attempt_200903250600_0004_m_000003_0\\\" START_TIME=\\\"1237962010929\\\" TRACKER_NAME=\\\"tracker_50445\\\" HTTP_PORT=\\\"50060\\\" .\");\r\n    writer.newLine();\r\n    writer.write(\"MapAttempt TASK_TYPE=\\\"SETUP\\\" TASKID=\\\"task_200903250600_0004_m_000003\\\" TASK_ATTEMPT_ID=\\\"attempt_200903250600_0004_m_000003_0\\\" TASK_STATUS=\\\"SUCCESS\\\" FINISH_TIME=\\\"1237962012459\\\" HOSTNAME=\\\"host.com\\\" STATE_STRING=\\\"setup\\\" COUNTERS=\\\"{(org.apache.hadoop.mapred.Task$Counter)(Map-Reduce Framework)[(SPILLED_RECORDS)(Spilled Records)(0)]}\\\" .\");\r\n    writer.newLine();\r\n    writer.write(\"Task TASKID=\\\"task_200903250600_0004_m_000003\\\" TASK_TYPE=\\\"SETUP\\\" TASK_STATUS=\\\"SUCCESS\\\" FINISH_TIME=\\\"1237962023824\\\" COUNTERS=\\\"{(org.apache.hadoop.mapred.Task$Counter)(Map-Reduce Framework)[(SPILLED_RECORDS)(Spilled Records)(0)]}\\\" .\");\r\n    writer.newLine();\r\n    writer.write(\"Job JOBID=\\\"job_200903250600_0004\\\" JOB_STATUS=\\\"RUNNING\\\" .\");\r\n    writer.newLine();\r\n    writer.write(\"Task TASKID=\\\"task_200903250600_0004_m_000000\\\" TASK_TYPE=\\\"MAP\\\" START_TIME=\\\"1237962024049\\\" SPLITS=\\\"host1.com,host2.com,host3.com\\\" .\");\r\n    writer.newLine();\r\n    writer.write(\"Task TASKID=\\\"task_200903250600_0004_m_000001\\\" TASK_TYPE=\\\"MAP\\\" START_TIME=\\\"1237962024065\\\" SPLITS=\\\"host1.com,host2.com,host3.com\\\" .\");\r\n    writer.newLine();\r\n    writer.write(\"MapAttempt TASK_TYPE=\\\"MAP\\\" TASKID=\\\"task_200903250600_0004_m_000000\\\" TASK_ATTEMPT_ID=\\\"attempt_200903250600_0004_m_000000_0\\\" START_TIME=\\\"1237962026157\\\" TRACKER_NAME=\\\"tracker_50524\\\" HTTP_PORT=\\\"50060\\\" .\");\r\n    writer.newLine();\r\n    writer.write(\"MapAttempt TASK_TYPE=\\\"MAP\\\" TASKID=\\\"task_200903250600_0004_m_000000\\\" TASK_ATTEMPT_ID=\\\"attempt_200903250600_0004_m_000000_0\\\" TASK_STATUS=\\\"SUCCESS\\\" FINISH_TIME=\\\"1237962041307\\\" HOSTNAME=\\\"host.com\\\" STATE_STRING=\\\"Records R/W=2681/1\\\" COUNTERS=\\\"{(FileSystemCounters)(FileSystemCounters)[(HDFS_BYTES_READ)(HDFS_BYTES_READ)(56630)][(HDFS_BYTES_WRITTEN)(HDFS_BYTES_WRITTEN)(28327)]}{(org.apache.hadoop.mapred.Task$Counter)(Map-Reduce Framework)[(MAP_INPUT_RECORDS)(Map input records)(2681)][(SPILLED_RECORDS)(Spilled Records)(0)][(MAP_INPUT_BYTES)(Map input bytes)(28327)][(MAP_OUTPUT_RECORDS)(Map output records)(2681)]}\\\" .\");\r\n    writer.newLine();\r\n    writer.write(\"Task TASKID=\\\"task_200903250600_0004_m_000000\\\" TASK_TYPE=\\\"MAP\\\" TASK_STATUS=\\\"SUCCESS\\\" FINISH_TIME=\\\"1237962054138\\\" COUNTERS=\\\"{(FileSystemCounters)(FileSystemCounters)[(HDFS_BYTES_READ)(HDFS_BYTES_READ)(56630)][(HDFS_BYTES_WRITTEN)(HDFS_BYTES_WRITTEN)(28327)]}{(org.apache.hadoop.mapred.Task$Counter)(Map-Reduce Framework)[(MAP_INPUT_RECORDS)(Map input records)(2681)][(SPILLED_RECORDS)(Spilled Records)(0)][(MAP_INPUT_BYTES)(Map input bytes)(28327)][(MAP_OUTPUT_RECORDS)(Map output records)(2681)]}\\\" .\");\r\n    writer.newLine();\r\n    writer.write(\"MapAttempt TASK_TYPE=\\\"MAP\\\" TASKID=\\\"task_200903250600_0004_m_000001\\\" TASK_ATTEMPT_ID=\\\"attempt_200903250600_0004_m_000001_0\\\" START_TIME=\\\"1237962026077\\\" TRACKER_NAME=\\\"tracker_50162\\\" HTTP_PORT=\\\"50060\\\" .\");\r\n    writer.newLine();\r\n    writer.write(\"MapAttempt TASK_TYPE=\\\"MAP\\\" TASKID=\\\"task_200903250600_0004_m_000001\\\" TASK_ATTEMPT_ID=\\\"attempt_200903250600_0004_m_000001_0\\\" TASK_STATUS=\\\"SUCCESS\\\" FINISH_TIME=\\\"1237962041030\\\" HOSTNAME=\\\"host.com\\\" STATE_STRING=\\\"Records R/W=2634/1\\\" COUNTERS=\\\"{(FileSystemCounters)(FileSystemCounters)[(HDFS_BYTES_READ)(HDFS_BYTES_READ)(28316)][(HDFS_BYTES_WRITTEN)(HDFS_BYTES_WRITTEN)(28303)]}{(org.apache.hadoop.mapred.Task$Counter)(Map-Reduce Framework)[(MAP_INPUT_RECORDS)(Map input records)(2634)][(SPILLED_RECORDS)(Spilled Records)(0)][(MAP_INPUT_BYTES)(Map input bytes)(28303)][(MAP_OUTPUT_RECORDS)(Map output records)(2634)]}\\\" .\");\r\n    writer.newLine();\r\n    writer.write(\"Task TASKID=\\\"task_200903250600_0004_m_000001\\\" TASK_TYPE=\\\"MAP\\\" TASK_STATUS=\\\"SUCCESS\\\" FINISH_TIME=\\\"1237962054187\\\" COUNTERS=\\\"{(FileSystemCounters)(FileSystemCounters)[(HDFS_BYTES_READ)(HDFS_BYTES_READ)(28316)][(HDFS_BYTES_WRITTEN)(HDFS_BYTES_WRITTEN)(28303)]}{(org.apache.hadoop.mapred.Task$Counter)(Map-Reduce Framework)[(MAP_INPUT_RECORDS)(Map input records)(2634)][(SPILLED_RECORDS)(Spilled Records)(0)][(MAP_INPUT_BYTES)(Map input bytes)(28303)][(MAP_OUTPUT_RECORDS)(Map output records)(2634)]}\\\" .\");\r\n    writer.newLine();\r\n    writer.write(\"Task TASKID=\\\"task_200903250600_0004_m_000002\\\" TASK_TYPE=\\\"CLEANUP\\\" START_TIME=\\\"1237962054187\\\" SPLITS=\\\"\\\" .\");\r\n    writer.newLine();\r\n    writer.write(\"MapAttempt TASK_TYPE=\\\"CLEANUP\\\" TASKID=\\\"task_200903250600_0004_m_000002\\\" TASK_ATTEMPT_ID=\\\"attempt_200903250600_0004_m_000002_0\\\" START_TIME=\\\"1237962055578\\\" TRACKER_NAME=\\\"tracker_50162\\\" HTTP_PORT=\\\"50060\\\" .\");\r\n    writer.newLine();\r\n    writer.write(\"MapAttempt TASK_TYPE=\\\"CLEANUP\\\" TASKID=\\\"task_200903250600_0004_m_000002\\\" TASK_ATTEMPT_ID=\\\"attempt_200903250600_0004_m_000002_0\\\" TASK_STATUS=\\\"SUCCESS\\\" FINISH_TIME=\\\"1237962056782\\\" HOSTNAME=\\\"host.com\\\" STATE_STRING=\\\"cleanup\\\" COUNTERS=\\\"{(org.apache.hadoop.mapred.Task$Counter)(Map-Reduce Framework)[(SPILLED_RECORDS)(Spilled Records)(0)]}\\\" .\");\r\n    writer.newLine();\r\n    writer.write(\"Task TASKID=\\\"task_200903250600_0004_m_000002\\\" TASK_TYPE=\\\"CLEANUP\\\" TASK_STATUS=\\\"SUCCESS\\\" FINISH_TIME=\\\"1237962069193\\\" COUNTERS=\\\"{(org.apache.hadoop.mapred.Task$Counter)(Map-Reduce Framework)[(SPILLED_RECORDS)(Spilled Records)(0)]}\\\" .\");\r\n    writer.newLine();\r\n    writer.write(\"Job JOBID=\\\"job_200903250600_0004\\\" FINISH_TIME=\\\"1237962069193\\\" JOB_STATUS=\\\"SUCCESS\\\" FINISHED_MAPS=\\\"2\\\" FINISHED_REDUCES=\\\"0\\\" FAILED_MAPS=\\\"0\\\" FAILED_REDUCES=\\\"0\\\" COUNTERS=\\\"{(org.apache.hadoop.mapred.JobInProgress$Counter)(Job Counters )[(TOTAL_LAUNCHED_MAPS)(Launched map tasks)(2)]}{(FileSystemCounters)(FileSystemCounters)[(HDFS_BYTES_READ)(HDFS_BYTES_READ)(84946)][(HDFS_BYTES_WRITTEN)(HDFS_BYTES_WRITTEN)(56630)]}{(org.apache.hadoop.mapred.Task$Counter)(Map-Reduce Framework)[(MAP_INPUT_RECORDS)(Map input records)(5315)][(SPILLED_RECORDS)(Spilled Records)(0)][(MAP_INPUT_BYTES)(Map input bytes)(56630)][(MAP_OUTPUT_RECORDS)(Map output records)(5315)]}\\\" .\");\r\n    writer.newLine();\r\n    writer.write(\"$!!FILE=file2.log!!\");\r\n    writer.newLine();\r\n    writer.write(\"Meta VERSION=\\\"1\\\" .\");\r\n    writer.newLine();\r\n    writer.write(\"Job JOBID=\\\"job_200903250600_0023\\\" JOBNAME=\\\"TestJob\\\" USER=\\\"hadoop2\\\" SUBMIT_TIME=\\\"1237964779799\\\" JOBCONF=\\\"hdfs:///job_200903250600_0023/job.xml\\\" .\");\r\n    writer.newLine();\r\n    writer.write(\"Job JOBID=\\\"job_200903250600_0023\\\" JOB_PRIORITY=\\\"NORMAL\\\" .\");\r\n    writer.newLine();\r\n    writer.write(\"Job JOBID=\\\"job_200903250600_0023\\\" LAUNCH_TIME=\\\"1237964780928\\\" TOTAL_MAPS=\\\"2\\\" TOTAL_REDUCES=\\\"0\\\" JOB_STATUS=\\\"PREP\\\" .\");\r\n    writer.newLine();\r\n    writer.write(\"Task TASKID=\\\"task_200903250600_0023_r_000001\\\" TASK_TYPE=\\\"SETUP\\\" START_TIME=\\\"1237964780940\\\" SPLITS=\\\"\\\" .\");\r\n    writer.newLine();\r\n    writer.write(\"ReduceAttempt TASK_TYPE=\\\"SETUP\\\" TASKID=\\\"task_200903250600_0023_r_000001\\\" TASK_ATTEMPT_ID=\\\"attempt_200903250600_0023_r_000001_0\\\" START_TIME=\\\"1237964720322\\\" TRACKER_NAME=\\\"tracker_3065\\\" HTTP_PORT=\\\"50060\\\" .\");\r\n    writer.newLine();\r\n    writer.write(\"ReduceAttempt TASK_TYPE=\\\"SETUP\\\" TASKID=\\\"task_200903250600_0023_r_000001\\\" TASK_ATTEMPT_ID=\\\"attempt_200903250600_0023_r_000001_0\\\" TASK_STATUS=\\\"SUCCESS\\\" SHUFFLE_FINISHED=\\\"1237964722118\\\" SORT_FINISHED=\\\"1237964722118\\\" FINISH_TIME=\\\"1237964722118\\\" HOSTNAME=\\\"host.com\\\" STATE_STRING=\\\"setup\\\" COUNTERS=\\\"{(org.apache.hadoop.mapred.Task$Counter)(Map-Reduce Framework)[(REDUCE_INPUT_GROUPS)(Reduce input groups)(0)][(COMBINE_OUTPUT_RECORDS)(Combine output records)(0)][(REDUCE_SHUFFLE_BYTES)(Reduce shuffle bytes)(0)][(REDUCE_OUTPUT_RECORDS)(Reduce output records)(0)][(SPILLED_RECORDS)(Spilled Records)(0)][(REDUCE_INPUT_RECORDS)(Reduce input records)(0)]}\\\" .\");\r\n    writer.newLine();\r\n    writer.write(\"Task TASKID=\\\"task_200903250600_0023_r_000001\\\" TASK_TYPE=\\\"SETUP\\\" TASK_STATUS=\\\"SUCCESS\\\" FINISH_TIME=\\\"1237964796054\\\" COUNTERS=\\\"{(org.apache.hadoop.mapred.Task$Counter)(Map-Reduce Framework)[(REDUCE_INPUT_GROUPS)(Reduce input groups)(0)][(COMBINE_OUTPUT_RECORDS)(Combine output records)(0)][(REDUCE_SHUFFLE_BYTES)(Reduce shuffle bytes)(0)][(REDUCE_OUTPUT_RECORDS)(Reduce output records)(0)][(SPILLED_RECORDS)(Spilled Records)(0)][(REDUCE_INPUT_RECORDS)(Reduce input records)(0)]}\\\" .\");\r\n    writer.newLine();\r\n    writer.write(\"Job JOBID=\\\"job_200903250600_0023\\\" JOB_STATUS=\\\"RUNNING\\\" .\");\r\n    writer.newLine();\r\n    writer.write(\"Task TASKID=\\\"task_200903250600_0023_m_000000\\\" TASK_TYPE=\\\"MAP\\\" START_TIME=\\\"1237964796176\\\" SPLITS=\\\"\\\" .\");\r\n    writer.newLine();\r\n    writer.write(\"Task TASKID=\\\"task_200903250600_0023_m_000001\\\" TASK_TYPE=\\\"MAP\\\" START_TIME=\\\"1237964796176\\\" SPLITS=\\\"\\\" .\");\r\n    writer.newLine();\r\n    writer.write(\"MapAttempt TASK_TYPE=\\\"MAP\\\" TASKID=\\\"task_200903250600_0023_m_000000\\\" TASK_ATTEMPT_ID=\\\"attempt_200903250600_0023_m_000000_0\\\" START_TIME=\\\"1237964809765\\\" TRACKER_NAME=\\\"tracker_50459\\\" HTTP_PORT=\\\"50060\\\" .\");\r\n    writer.newLine();\r\n    writer.write(\"MapAttempt TASK_TYPE=\\\"MAP\\\" TASKID=\\\"task_200903250600_0023_m_000000\\\" TASK_ATTEMPT_ID=\\\"attempt_200903250600_0023_m_000000_0\\\" TASK_STATUS=\\\"SUCCESS\\\" FINISH_TIME=\\\"1237964911772\\\" HOSTNAME=\\\"host.com\\\" STATE_STRING=\\\"\\\" COUNTERS=\\\"{(FileSystemCounters)(FileSystemCounters)[(HDFS_BYTES_WRITTEN)(HDFS_BYTES_WRITTEN)(500000000)]}{(org.apache.hadoop.mapred.Task$Counter)(Map-Reduce Framework)[(MAP_INPUT_RECORDS)(Map input records)(5000000)][(SPILLED_RECORDS)(Spilled Records)(0)][(MAP_INPUT_BYTES)(Map input bytes)(5000000)][(MAP_OUTPUT_RECORDS)(Map output records)(5000000)]}\\\" .\");\r\n    writer.newLine();\r\n    writer.write(\"Task TASKID=\\\"task_200903250600_0023_m_000000\\\" TASK_TYPE=\\\"MAP\\\" TASK_STATUS=\\\"SUCCESS\\\" FINISH_TIME=\\\"1237964916534\\\" COUNTERS=\\\"{(FileSystemCounters)(FileSystemCounters)[(HDFS_BYTES_WRITTEN)(HDFS_BYTES_WRITTEN)(500000000)]}{(org.apache.hadoop.mapred.Task$Counter)(Map-Reduce Framework)[(MAP_INPUT_RECORDS)(Map input records)(5000000)][(SPILLED_RECORDS)(Spilled Records)(0)][(MAP_INPUT_BYTES)(Map input bytes)(5000000)][(MAP_OUTPUT_RECORDS)(Map output records)(5000000)]}\\\" .\");\r\n    writer.newLine();\r\n    writer.write(\"MapAttempt TASK_TYPE=\\\"MAP\\\" TASKID=\\\"task_200903250600_0023_m_000001\\\" TASK_ATTEMPT_ID=\\\"attempt_200903250600_0023_m_000001_0\\\" START_TIME=\\\"1237964798169\\\" TRACKER_NAME=\\\"tracker_1524\\\" HTTP_PORT=\\\"50060\\\" .\");\r\n    writer.newLine();\r\n    writer.write(\"MapAttempt TASK_TYPE=\\\"MAP\\\" TASKID=\\\"task_200903250600_0023_m_000001\\\" TASK_ATTEMPT_ID=\\\"attempt_200903250600_0023_m_000001_0\\\" TASK_STATUS=\\\"SUCCESS\\\" FINISH_TIME=\\\"1237964962960\\\" HOSTNAME=\\\"host.com\\\" STATE_STRING=\\\"\\\" COUNTERS=\\\"{(FileSystemCounters)(FileSystemCounters)[(HDFS_BYTES_WRITTEN)(HDFS_BYTES_WRITTEN)(500000000)]}{(org.apache.hadoop.mapred.Task$Counter)(Map-Reduce Framework)[(MAP_INPUT_RECORDS)(Map input records)(5000000)][(SPILLED_RECORDS)(Spilled Records)(0)][(MAP_INPUT_BYTES)(Map input bytes)(5000000)][(MAP_OUTPUT_RECORDS)(Map output records)(5000000)]}\\\" .\");\r\n    writer.newLine();\r\n    writer.write(\"Task TASKID=\\\"task_200903250600_0023_m_000001\\\" TASK_TYPE=\\\"MAP\\\" TASK_STATUS=\\\"SUCCESS\\\" FINISH_TIME=\\\"1237964976870\\\" COUNTERS=\\\"{(FileSystemCounters)(FileSystemCounters)[(HDFS_BYTES_WRITTEN)(HDFS_BYTES_WRITTEN)(500000000)]}{(org.apache.hadoop.mapred.Task$Counter)(Map-Reduce Framework)[(MAP_INPUT_RECORDS)(Map input records)(5000000)][(SPILLED_RECORDS)(Spilled Records)(0)][(MAP_INPUT_BYTES)(Map input bytes)(5000000)][(MAP_OUTPUT_RECORDS)(Map output records)(5000000)]}\\\" .\");\r\n    writer.newLine();\r\n    writer.write(\"Task TASKID=\\\"task_200903250600_0023_r_000000\\\" TASK_TYPE=\\\"CLEANUP\\\" START_TIME=\\\"1237964976871\\\" SPLITS=\\\"\\\" .\");\r\n    writer.newLine();\r\n    writer.write(\"ReduceAttempt TASK_TYPE=\\\"CLEANUP\\\" TASKID=\\\"task_200903250600_0023_r_000000\\\" TASK_ATTEMPT_ID=\\\"attempt_200903250600_0023_r_000000_0\\\" START_TIME=\\\"1237964977208\\\" TRACKER_NAME=\\\"tracker_1524\\\" HTTP_PORT=\\\"50060\\\" .\");\r\n    writer.newLine();\r\n    writer.write(\"ReduceAttempt TASK_TYPE=\\\"CLEANUP\\\" TASKID=\\\"task_200903250600_0023_r_000000\\\" TASK_ATTEMPT_ID=\\\"attempt_200903250600_0023_r_000000_0\\\" TASK_STATUS=\\\"SUCCESS\\\" SHUFFLE_FINISHED=\\\"1237964979031\\\" SORT_FINISHED=\\\"1237964979031\\\" FINISH_TIME=\\\"1237964979032\\\" HOSTNAME=\\\"host.com\\\" STATE_STRING=\\\"cleanup\\\" COUNTERS=\\\"{(org.apache.hadoop.mapred.Task$Counter)(Map-Reduce Framework)[(REDUCE_INPUT_GROUPS)(Reduce input groups)(0)][(COMBINE_OUTPUT_RECORDS)(Combine output records)(0)][(REDUCE_SHUFFLE_BYTES)(Reduce shuffle bytes)(0)][(REDUCE_OUTPUT_RECORDS)(Reduce output records)(0)][(SPILLED_RECORDS)(Spilled Records)(0)][(REDUCE_INPUT_RECORDS)(Reduce input records)(0)]}\\\" .\");\r\n    writer.newLine();\r\n    writer.write(\"Task TASKID=\\\"task_200903250600_0023_r_000000\\\" TASK_TYPE=\\\"CLEANUP\\\" TASK_STATUS=\\\"SUCCESS\\\" FINISH_TIME=\\\"1237964991879\\\" COUNTERS=\\\"{(org.apache.hadoop.mapred.Task$Counter)(Map-Reduce Framework)[(REDUCE_INPUT_GROUPS)(Reduce input groups)(0)][(COMBINE_OUTPUT_RECORDS)(Combine output records)(0)][(REDUCE_SHUFFLE_BYTES)(Reduce shuffle bytes)(0)][(REDUCE_OUTPUT_RECORDS)(Reduce output records)(0)][(SPILLED_RECORDS)(Spilled Records)(0)][(REDUCE_INPUT_RECORDS)(Reduce input records)(0)]}\\\" .\");\r\n    writer.newLine();\r\n    writer.write(\"Job JOBID=\\\"job_200903250600_0023\\\" FINISH_TIME=\\\"1237964991879\\\" JOB_STATUS=\\\"SUCCESS\\\" FINISHED_MAPS=\\\"2\\\" FINISHED_REDUCES=\\\"0\\\" FAILED_MAPS=\\\"0\\\" FAILED_REDUCES=\\\"0\\\" COUNTERS=\\\"{(org.apache.hadoop.mapred.JobInProgress$Counter)(Job Counters )[(TOTAL_LAUNCHED_MAPS)(Launched map tasks)(2)]}{(FileSystemCounters)(FileSystemCounters)[(HDFS_BYTES_WRITTEN)(HDFS_BYTES_WRITTEN)(1000000000)]}{(org.apache.hadoop.mapred.Task$Counter)(Map-Reduce Framework)[(MAP_INPUT_RECORDS)(Map input records)(10000000)][(SPILLED_RECORDS)(Spilled Records)(0)][(MAP_INPUT_BYTES)(Map input bytes)(10000000)][(MAP_OUTPUT_RECORDS)(Map output records)(10000000)]}\\\" .\");\r\n    writer.newLine();\r\n    writer.write(\"$!!FILE=file3.log!!\");\r\n    writer.newLine();\r\n    writer.write(\"Meta VERSION=\\\"1\\\" .\");\r\n    writer.newLine();\r\n    writer.write(\"Job JOBID=\\\"job_200903250600_0034\\\" JOBNAME=\\\"TestJob\\\" USER=\\\"hadoop3\\\" SUBMIT_TIME=\\\"1237966370007\\\" JOBCONF=\\\"hdfs:///job_200903250600_0034/job.xml\\\" .\");\r\n    writer.newLine();\r\n    writer.write(\"Job JOBID=\\\"job_200903250600_0034\\\" JOB_PRIORITY=\\\"NORMAL\\\" .\");\r\n    writer.newLine();\r\n    writer.write(\"Job JOBID=\\\"job_200903250600_0034\\\" LAUNCH_TIME=\\\"1237966371076\\\" TOTAL_MAPS=\\\"2\\\" TOTAL_REDUCES=\\\"0\\\" JOB_STATUS=\\\"PREP\\\" .\");\r\n    writer.newLine();\r\n    writer.write(\"Task TASKID=\\\"task_200903250600_0034_m_000003\\\" TASK_TYPE=\\\"SETUP\\\" START_TIME=\\\"1237966371093\\\" SPLITS=\\\"\\\" .\");\r\n    writer.newLine();\r\n    writer.write(\"MapAttempt TASK_TYPE=\\\"SETUP\\\" TASKID=\\\"task_200903250600_0034_m_000003\\\" TASK_ATTEMPT_ID=\\\"attempt_200903250600_0034_m_000003_0\\\" START_TIME=\\\"1237966371524\\\" TRACKER_NAME=\\\"tracker_50118\\\" HTTP_PORT=\\\"50060\\\" .\");\r\n    writer.newLine();\r\n    writer.write(\"MapAttempt TASK_TYPE=\\\"SETUP\\\" TASKID=\\\"task_200903250600_0034_m_000003\\\" TASK_ATTEMPT_ID=\\\"attempt_200903250600_0034_m_000003_0\\\" TASK_STATUS=\\\"SUCCESS\\\" FINISH_TIME=\\\"1237966373174\\\" HOSTNAME=\\\"host.com\\\" STATE_STRING=\\\"setup\\\" COUNTERS=\\\"{(org.apache.hadoop.mapred.Task$Counter)(Map-Reduce Framework)[(SPILLED_RECORDS)(Spilled Records)(0)]}\\\" .\");\r\n    writer.newLine();\r\n    writer.write(\"Task TASKID=\\\"task_200903250600_0034_m_000003\\\" TASK_TYPE=\\\"SETUP\\\" TASK_STATUS=\\\"SUCCESS\\\" FINISH_TIME=\\\"1237966386098\\\" COUNTERS=\\\"{(org.apache.hadoop.mapred.Task$Counter)(Map-Reduce Framework)[(SPILLED_RECORDS)(Spilled Records)(0)]}\\\" .\");\r\n    writer.newLine();\r\n    writer.write(\"Job JOBID=\\\"job_200903250600_0034\\\" JOB_STATUS=\\\"RUNNING\\\" .\");\r\n    writer.newLine();\r\n    writer.write(\"Task TASKID=\\\"task_200903250600_0034_m_000000\\\" TASK_TYPE=\\\"MAP\\\" START_TIME=\\\"1237966386111\\\" SPLITS=\\\"\\\" .\");\r\n    writer.newLine();\r\n    writer.write(\"Task TASKID=\\\"task_200903250600_0034_m_000001\\\" TASK_TYPE=\\\"MAP\\\" START_TIME=\\\"1237966386124\\\" SPLITS=\\\"\\\" .\");\r\n    writer.newLine();\r\n    writer.write(\"MapAttempt TASK_TYPE=\\\"MAP\\\" TASKID=\\\"task_200903250600_0034_m_000001\\\" TASK_ATTEMPT_ID=\\\"attempt_200903250600_0034_m_000001_0\\\" TASK_STATUS=\\\"FAILED\\\" FINISH_TIME=\\\"1237967174546\\\" HOSTNAME=\\\"host.com\\\" ERROR=\\\"java.io.IOException: Task process exit with nonzero status of 15.\");\r\n    writer.newLine();\r\n    writer.write(\"  at org.apache.hadoop.mapred.TaskRunner.run(TaskRunner.java:424)\");\r\n    writer.newLine();\r\n    writer.write(\",java.io.IOException: Task process exit with nonzero status of 15.\");\r\n    writer.newLine();\r\n    writer.write(\"  at org.apache.hadoop.mapred.TaskRunner.run(TaskRunner.java:424)\");\r\n    writer.newLine();\r\n    writer.write(\"\\\" .\");\r\n    writer.newLine();\r\n    writer.write(\"Task TASKID=\\\"task_200903250600_0034_m_000002\\\" TASK_TYPE=\\\"CLEANUP\\\" START_TIME=\\\"1237967170815\\\" SPLITS=\\\"\\\" .\");\r\n    writer.newLine();\r\n    writer.write(\"MapAttempt TASK_TYPE=\\\"CLEANUP\\\" TASKID=\\\"task_200903250600_0034_m_000002\\\" TASK_ATTEMPT_ID=\\\"attempt_200903250600_0034_m_000002_0\\\" START_TIME=\\\"1237967168653\\\" TRACKER_NAME=\\\"tracker_3105\\\" HTTP_PORT=\\\"50060\\\" .\");\r\n    writer.newLine();\r\n    writer.write(\"MapAttempt TASK_TYPE=\\\"CLEANUP\\\" TASKID=\\\"task_200903250600_0034_m_000002\\\" TASK_ATTEMPT_ID=\\\"attempt_200903250600_0034_m_000002_0\\\" TASK_STATUS=\\\"SUCCESS\\\" FINISH_TIME=\\\"1237967171301\\\" HOSTNAME=\\\"host.com\\\" STATE_STRING=\\\"cleanup\\\" COUNTERS=\\\"{(org.apache.hadoop.mapred.Task$Counter)(Map-Reduce Framework)[(SPILLED_RECORDS)(Spilled Records)(0)]}\\\" .\");\r\n    writer.newLine();\r\n    writer.write(\"Task TASKID=\\\"task_200903250600_0034_m_000002\\\" TASK_TYPE=\\\"CLEANUP\\\" TASK_STATUS=\\\"SUCCESS\\\" FINISH_TIME=\\\"1237967185818\\\" COUNTERS=\\\"{(org.apache.hadoop.mapred.Task$Counter)(Map-Reduce Framework)[(SPILLED_RECORDS)(Spilled Records)(0)]}\\\" .\");\r\n    writer.newLine();\r\n    writer.write(\"Job JOBID=\\\"job_200903250600_0034\\\" FINISH_TIME=\\\"1237967185818\\\" JOB_STATUS=\\\"KILLED\\\" FINISHED_MAPS=\\\"0\\\" FINISHED_REDUCES=\\\"0\\\" .\");\r\n    writer.newLine();\r\n    writer.close();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\fs",
  "methodName" : "tearDown",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void tearDown() throws Exception\n{\r\n    File logFile = new File(historyLog);\r\n    if (!logFile.delete())\r\n        LOG.error(\"Cannot delete history log file: \" + historyLog);\r\n    if (!logFile.getParentFile().delete())\r\n        LOG.error(\"Cannot delete history log dir: \" + historyLog);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\fs",
  "methodName" : "testJHLA",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void testJHLA()\n{\r\n    String[] args = { \"-test\", historyLog, \"-jobDelimiter\", \".!!FILE=.*!!\" };\r\n    JHLogAnalyzer.main(args);\r\n    args = new String[] { \"-test\", historyLog, \"-jobDelimiter\", \".!!FILE=.*!!\", \"-usersIncluded\", \"hadoop,hadoop2\" };\r\n    JHLogAnalyzer.main(args);\r\n    args = new String[] { \"-test\", historyLog, \"-jobDelimiter\", \".!!FILE=.*!!\", \"-usersExcluded\", \"hadoop,hadoop3\" };\r\n    JHLogAnalyzer.main(args);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\partition",
  "methodName" : "writePartitionFile",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "Path writePartitionFile(String testname, Configuration conf, T[] splits) throws IOException\n{\r\n    final FileSystem fs = FileSystem.getLocal(conf);\r\n    final Path testdir = new Path(System.getProperty(\"test.build.data\", \"/tmp\")).makeQualified(fs.getUri(), fs.getWorkingDirectory());\r\n    Path p = new Path(testdir, testname + \"/_partition.lst\");\r\n    TotalOrderPartitioner.setPartitionFile(conf, p);\r\n    conf.setInt(MRJobConfig.NUM_REDUCES, splits.length + 1);\r\n    SequenceFile.Writer w = null;\r\n    try {\r\n        w = SequenceFile.createWriter(conf, SequenceFile.Writer.file(p), SequenceFile.Writer.keyClass(splits[0].getClass()), SequenceFile.Writer.valueClass(NullWritable.class), SequenceFile.Writer.compression(CompressionType.NONE));\r\n        for (int i = 0; i < splits.length; ++i) {\r\n            w.append(splits[i], NullWritable.get());\r\n        }\r\n    } finally {\r\n        if (null != w)\r\n            w.close();\r\n    }\r\n    return p;\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\partition",
  "methodName" : "testTotalOrderWithCustomSerialization",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void testTotalOrderWithCustomSerialization() throws Exception\n{\r\n    TotalOrderPartitioner<String, NullWritable> partitioner = new TotalOrderPartitioner<String, NullWritable>();\r\n    Configuration conf = new Configuration();\r\n    conf.setStrings(CommonConfigurationKeys.IO_SERIALIZATIONS_KEY, JavaSerialization.class.getName(), WritableSerialization.class.getName());\r\n    conf.setClass(MRJobConfig.KEY_COMPARATOR, JavaSerializationComparator.class, Comparator.class);\r\n    Path p = TestTotalOrderPartitioner.<String>writePartitionFile(\"totalordercustomserialization\", conf, splitJavaStrings);\r\n    conf.setClass(MRJobConfig.MAP_OUTPUT_KEY_CLASS, String.class, Object.class);\r\n    try {\r\n        partitioner.setConf(conf);\r\n        NullWritable nw = NullWritable.get();\r\n        for (Check<String> chk : testJavaStrings) {\r\n            assertEquals(chk.data.toString(), chk.part, partitioner.getPartition(chk.data, nw, splitJavaStrings.length + 1));\r\n        }\r\n    } finally {\r\n        p.getFileSystem(conf).delete(p, true);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\partition",
  "methodName" : "testTotalOrderMemCmp",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void testTotalOrderMemCmp() throws Exception\n{\r\n    TotalOrderPartitioner<Text, NullWritable> partitioner = new TotalOrderPartitioner<Text, NullWritable>();\r\n    Configuration conf = new Configuration();\r\n    Path p = TestTotalOrderPartitioner.<Text>writePartitionFile(\"totalordermemcmp\", conf, splitStrings);\r\n    conf.setClass(MRJobConfig.MAP_OUTPUT_KEY_CLASS, Text.class, Object.class);\r\n    try {\r\n        partitioner.setConf(conf);\r\n        NullWritable nw = NullWritable.get();\r\n        for (Check<Text> chk : testStrings) {\r\n            assertEquals(chk.data.toString(), chk.part, partitioner.getPartition(chk.data, nw, splitStrings.length + 1));\r\n        }\r\n    } finally {\r\n        p.getFileSystem(conf).delete(p, true);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\partition",
  "methodName" : "testTotalOrderBinarySearch",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void testTotalOrderBinarySearch() throws Exception\n{\r\n    TotalOrderPartitioner<Text, NullWritable> partitioner = new TotalOrderPartitioner<Text, NullWritable>();\r\n    Configuration conf = new Configuration();\r\n    Path p = TestTotalOrderPartitioner.<Text>writePartitionFile(\"totalorderbinarysearch\", conf, splitStrings);\r\n    conf.setBoolean(TotalOrderPartitioner.NATURAL_ORDER, false);\r\n    conf.setClass(MRJobConfig.MAP_OUTPUT_KEY_CLASS, Text.class, Object.class);\r\n    try {\r\n        partitioner.setConf(conf);\r\n        NullWritable nw = NullWritable.get();\r\n        for (Check<Text> chk : testStrings) {\r\n            assertEquals(chk.data.toString(), chk.part, partitioner.getPartition(chk.data, nw, splitStrings.length + 1));\r\n        }\r\n    } finally {\r\n        p.getFileSystem(conf).delete(p, true);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\partition",
  "methodName" : "testTotalOrderCustomComparator",
  "errType" : null,
  "containingMethodsNum" : 21,
  "sourceCodeText" : "void testTotalOrderCustomComparator() throws Exception\n{\r\n    TotalOrderPartitioner<Text, NullWritable> partitioner = new TotalOrderPartitioner<Text, NullWritable>();\r\n    Configuration conf = new Configuration();\r\n    Text[] revSplitStrings = Arrays.copyOf(splitStrings, splitStrings.length);\r\n    Arrays.sort(revSplitStrings, new ReverseStringComparator());\r\n    Path p = TestTotalOrderPartitioner.<Text>writePartitionFile(\"totalordercustomcomparator\", conf, revSplitStrings);\r\n    conf.setBoolean(TotalOrderPartitioner.NATURAL_ORDER, false);\r\n    conf.setClass(MRJobConfig.MAP_OUTPUT_KEY_CLASS, Text.class, Object.class);\r\n    conf.setClass(MRJobConfig.KEY_COMPARATOR, ReverseStringComparator.class, RawComparator.class);\r\n    ArrayList<Check<Text>> revCheck = new ArrayList<Check<Text>>();\r\n    revCheck.add(new Check<Text>(new Text(\"aaaaa\"), 9));\r\n    revCheck.add(new Check<Text>(new Text(\"aaabb\"), 9));\r\n    revCheck.add(new Check<Text>(new Text(\"aabbb\"), 9));\r\n    revCheck.add(new Check<Text>(new Text(\"aaaaa\"), 9));\r\n    revCheck.add(new Check<Text>(new Text(\"babbb\"), 8));\r\n    revCheck.add(new Check<Text>(new Text(\"baabb\"), 8));\r\n    revCheck.add(new Check<Text>(new Text(\"yai\"), 1));\r\n    revCheck.add(new Check<Text>(new Text(\"yak\"), 1));\r\n    revCheck.add(new Check<Text>(new Text(\"z\"), 0));\r\n    revCheck.add(new Check<Text>(new Text(\"ddngo\"), 4));\r\n    revCheck.add(new Check<Text>(new Text(\"hi\"), 3));\r\n    try {\r\n        partitioner.setConf(conf);\r\n        NullWritable nw = NullWritable.get();\r\n        for (Check<Text> chk : revCheck) {\r\n            assertEquals(chk.data.toString(), chk.part, partitioner.getPartition(chk.data, nw, splitStrings.length + 1));\r\n        }\r\n    } finally {\r\n        p.getFileSystem(conf).delete(p, true);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "getReplayMode",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int getReplayMode()\n{\r\n    return replayMode;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "getJobFiles",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Collection<JobFiles> getJobFiles()\n{\r\n    return jobFiles;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "getParser",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "JobHistoryFileParser getParser()\n{\r\n    return parser;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "selectJobFiles",
  "errType" : null,
  "containingMethodsNum" : 14,
  "sourceCodeText" : "Collection<JobFiles> selectJobFiles(FileSystem fs, Path processingRoot, int i, int size) throws IOException\n{\r\n    Map<String, JobFiles> jobs = new HashMap<>();\r\n    RemoteIterator<LocatedFileStatus> it = fs.listFiles(processingRoot, true);\r\n    while (it.hasNext()) {\r\n        LocatedFileStatus status = it.next();\r\n        Path path = status.getPath();\r\n        String fileName = path.getName();\r\n        Matcher m = JOB_ID_PARSER.matcher(fileName);\r\n        if (!m.matches()) {\r\n            continue;\r\n        }\r\n        String jobId = m.group(1);\r\n        int lastId = Integer.parseInt(m.group(2));\r\n        int mod = lastId % size;\r\n        if (mod != i) {\r\n            continue;\r\n        }\r\n        LOG.info(\"this mapper will process file \" + fileName);\r\n        JobFiles jobFiles = jobs.get(jobId);\r\n        if (jobFiles == null) {\r\n            jobFiles = new JobFiles(jobId);\r\n            jobs.put(jobId, jobFiles);\r\n        }\r\n        setFilePath(fileName, path, jobFiles);\r\n    }\r\n    return jobs.values();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "setFilePath",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void setFilePath(String fileName, Path path, JobFiles jobFiles)\n{\r\n    FileType type = getFileType(fileName);\r\n    switch(type) {\r\n        case JOB_HISTORY_FILE:\r\n            if (jobFiles.getJobHistoryFilePath() == null) {\r\n                jobFiles.setJobHistoryFilePath(path);\r\n            } else {\r\n                LOG.warn(\"we already have the job history file \" + jobFiles.getJobHistoryFilePath() + \": skipping \" + path);\r\n            }\r\n            break;\r\n        case JOB_CONF_FILE:\r\n            if (jobFiles.getJobConfFilePath() == null) {\r\n                jobFiles.setJobConfFilePath(path);\r\n            } else {\r\n                LOG.warn(\"we already have the job conf file \" + jobFiles.getJobConfFilePath() + \": skipping \" + path);\r\n            }\r\n            break;\r\n        case UNKNOWN:\r\n            LOG.warn(\"unknown type: \" + path);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "getFileType",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "FileType getFileType(String fileName)\n{\r\n    if (fileName.endsWith(\".jhist\")) {\r\n        return FileType.JOB_HISTORY_FILE;\r\n    }\r\n    if (fileName.endsWith(\"_conf.xml\")) {\r\n        return FileType.JOB_CONF_FILE;\r\n    }\r\n    return FileType.UNKNOWN;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "setup",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void setup() throws IOException\n{\r\n    Configuration conf = new YarnConfiguration();\r\n    cluster = MiniMRClientClusterFactory.create(this.getClass(), 2, conf);\r\n    cluster.start();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "cleanup",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void cleanup() throws IOException\n{\r\n    if (cluster != null) {\r\n        cluster.stop();\r\n        cluster = null;\r\n    }\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "testLargeSort",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void testLargeSort() throws Exception\n{\r\n    String[] args = new String[0];\r\n    int[] ioSortMbs = { 128, 256, 1536 };\r\n    for (int ioSortMb : ioSortMbs) {\r\n        Configuration conf = new Configuration(cluster.getConfig());\r\n        conf.setInt(MRJobConfig.MAP_MEMORY_MB, 2048);\r\n        conf.setInt(MRJobConfig.IO_SORT_MB, ioSortMb);\r\n        conf.setInt(LargeSorter.NUM_MAP_TASKS, 1);\r\n        conf.setInt(LargeSorter.MBS_PER_MAP, ioSortMb);\r\n        assertEquals(\"Large sort failed for \" + ioSortMb, 0, ToolRunner.run(conf, new LargeSorter(), args));\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "testIFileWriterWithCodec",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void testIFileWriterWithCodec() throws Exception\n{\r\n    Configuration conf = new Configuration();\r\n    FileSystem localFs = FileSystem.getLocal(conf);\r\n    FileSystem rfs = ((LocalFileSystem) localFs).getRaw();\r\n    Path path = new Path(new Path(\"build/test.ifile\"), \"data\");\r\n    DefaultCodec codec = new GzipCodec();\r\n    codec.setConf(conf);\r\n    IFile.Writer<Text, Text> writer = new IFile.Writer<Text, Text>(conf, rfs.create(path), Text.class, Text.class, codec, null);\r\n    writer.close();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "testIFileReaderWithCodec",
  "errType" : null,
  "containingMethodsNum" : 10,
  "sourceCodeText" : "void testIFileReaderWithCodec() throws Exception\n{\r\n    Configuration conf = new Configuration();\r\n    FileSystem localFs = FileSystem.getLocal(conf);\r\n    FileSystem rfs = ((LocalFileSystem) localFs).getRaw();\r\n    Path path = new Path(new Path(\"build/test.ifile\"), \"data\");\r\n    DefaultCodec codec = new GzipCodec();\r\n    codec.setConf(conf);\r\n    FSDataOutputStream out = rfs.create(path);\r\n    IFile.Writer<Text, Text> writer = new IFile.Writer<Text, Text>(conf, out, Text.class, Text.class, codec, null);\r\n    writer.close();\r\n    FSDataInputStream in = rfs.open(path);\r\n    IFile.Reader<Text, Text> reader = new IFile.Reader<Text, Text>(conf, in, rfs.getFileStatus(path).getLen(), codec, null);\r\n    reader.close();\r\n    byte[] ab = new byte[100];\r\n    int readed = reader.checksumIn.readWithChecksum(ab, 0, ab.length);\r\n    assertEquals(readed, reader.checksumIn.getChecksum().length);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "parseHistoryFile",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "JobInfo parseHistoryFile(Path path) throws IOException\n{\r\n    LOG.info(\"parsing job history file \" + path);\r\n    JobHistoryParser parser = new JobHistoryParser(fs, path);\r\n    return parser.parse();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "parseConfiguration",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "Configuration parseConfiguration(Path path) throws IOException\n{\r\n    LOG.info(\"parsing job configuration file \" + path);\r\n    Configuration conf = new Configuration(false);\r\n    conf.addResource(fs.open(path));\r\n    return conf;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\fs\\slive",
  "methodName" : "generate",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "long generate(long offSet)\n{\r\n    return ((offSet * 47) ^ (rnd.nextLong() * 97)) * 37;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred\\pipes",
  "methodName" : "main",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void main(String[] args)\n{\r\n    PipeApplicationStub client = new PipeApplicationStub();\r\n    client.binaryProtocolStub();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred\\pipes",
  "methodName" : "binaryProtocolStub",
  "errType" : [ "Exception" ],
  "containingMethodsNum" : 33,
  "sourceCodeText" : "void binaryProtocolStub()\n{\r\n    try {\r\n        initSoket();\r\n        WritableUtils.writeVInt(dataOut, 50);\r\n        IntWritable wt = new IntWritable();\r\n        wt.set(123);\r\n        writeObject(wt, dataOut);\r\n        writeObject(new Text(\"value\"), dataOut);\r\n        WritableUtils.writeVInt(dataOut, 51);\r\n        WritableUtils.writeVInt(dataOut, 0);\r\n        writeObject(wt, dataOut);\r\n        writeObject(new Text(\"value\"), dataOut);\r\n        WritableUtils.writeVInt(dataOut, 52);\r\n        Text.writeString(dataOut, \"PROGRESS\");\r\n        dataOut.flush();\r\n        WritableUtils.writeVInt(dataOut, 53);\r\n        dataOut.writeFloat(0.55f);\r\n        WritableUtils.writeVInt(dataOut, 55);\r\n        WritableUtils.writeVInt(dataOut, 0);\r\n        Text.writeString(dataOut, \"group\");\r\n        Text.writeString(dataOut, \"name\");\r\n        WritableUtils.writeVInt(dataOut, 56);\r\n        WritableUtils.writeVInt(dataOut, 0);\r\n        WritableUtils.writeVLong(dataOut, 2);\r\n        int intValue = WritableUtils.readVInt(dataInput);\r\n        System.out.println(\"intValue:\" + intValue);\r\n        IntWritable iw = new IntWritable();\r\n        readObject(iw, dataInput);\r\n        System.out.println(\"key:\" + iw.get());\r\n        Text txt = new Text();\r\n        readObject(txt, dataInput);\r\n        System.out.println(\"value:\" + txt.toString());\r\n        WritableUtils.writeVInt(dataOut, 54);\r\n        System.out.println(\"finish\");\r\n        dataOut.flush();\r\n        dataOut.close();\r\n    } catch (Exception x) {\r\n        x.printStackTrace();\r\n    } finally {\r\n        closeSoket();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "cleanAndCreateInput",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void cleanAndCreateInput(FileSystem fs) throws IOException\n{\r\n    fs.delete(INPUT_DIR, true);\r\n    fs.delete(OUTPUT_DIR, true);\r\n    OutputStream os = fs.create(INPUT_FILE);\r\n    Writer wr = new OutputStreamWriter(os);\r\n    wr.write(\"b a\\n\");\r\n    wr.close();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "testMapReduceJob",
  "errType" : null,
  "containingMethodsNum" : 24,
  "sourceCodeText" : "void testMapReduceJob() throws Exception\n{\r\n    JobConf conf = new JobConf(TestJavaSerialization.class);\r\n    conf.setJobName(\"JavaSerialization\");\r\n    FileSystem fs = FileSystem.get(conf);\r\n    cleanAndCreateInput(fs);\r\n    conf.set(\"io.serializations\", \"org.apache.hadoop.io.serializer.JavaSerialization,\" + \"org.apache.hadoop.io.serializer.WritableSerialization\");\r\n    conf.setInputFormat(TextInputFormat.class);\r\n    conf.setOutputKeyClass(String.class);\r\n    conf.setOutputValueClass(Long.class);\r\n    conf.setOutputKeyComparatorClass(JavaSerializationComparator.class);\r\n    conf.setMapperClass(WordCountMapper.class);\r\n    conf.setReducerClass(SumReducer.class);\r\n    conf.set(MRConfig.FRAMEWORK_NAME, MRConfig.LOCAL_FRAMEWORK_NAME);\r\n    FileInputFormat.setInputPaths(conf, INPUT_DIR);\r\n    FileOutputFormat.setOutputPath(conf, OUTPUT_DIR);\r\n    String inputFileContents = FileUtils.readFileToString(new File(INPUT_FILE.toUri().getPath()));\r\n    assertTrue(\"Input file contents not as expected; contents are '\" + inputFileContents + \"', expected \\\"b a\\n\\\" \", inputFileContents.equals(\"b a\\n\"));\r\n    JobClient.runJob(conf);\r\n    Path[] outputFiles = FileUtil.stat2Paths(fs.listStatus(OUTPUT_DIR, new Utils.OutputFileUtils.OutputFilesFilter()));\r\n    assertEquals(1, outputFiles.length);\r\n    try (InputStream is = fs.open(outputFiles[0])) {\r\n        String reduceOutput = org.apache.commons.io.IOUtils.toString(is, StandardCharsets.UTF_8);\r\n        String[] lines = reduceOutput.split(\"\\n\");\r\n        assertEquals(\"Unexpected output; received output '\" + reduceOutput + \"'\", \"a\\t1\", lines[0]);\r\n        assertEquals(\"Unexpected output; received output '\" + reduceOutput + \"'\", \"b\\t1\", lines[1]);\r\n        assertEquals(\"Reduce output has extra lines; output is '\" + reduceOutput + \"'\", 2, lines.length);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "testWriteToSequencefile",
  "errType" : null,
  "containingMethodsNum" : 17,
  "sourceCodeText" : "void testWriteToSequencefile() throws Exception\n{\r\n    JobConf conf = new JobConf(TestJavaSerialization.class);\r\n    conf.setJobName(\"JavaSerialization\");\r\n    FileSystem fs = FileSystem.get(conf);\r\n    cleanAndCreateInput(fs);\r\n    conf.set(\"io.serializations\", \"org.apache.hadoop.io.serializer.JavaSerialization,\" + \"org.apache.hadoop.io.serializer.WritableSerialization\");\r\n    conf.setInputFormat(TextInputFormat.class);\r\n    conf.setOutputFormat(SequenceFileOutputFormat.class);\r\n    conf.setOutputKeyClass(String.class);\r\n    conf.setOutputValueClass(Long.class);\r\n    conf.setOutputKeyComparatorClass(JavaSerializationComparator.class);\r\n    conf.setMapperClass(WordCountMapper.class);\r\n    conf.setReducerClass(SumReducer.class);\r\n    conf.set(MRConfig.FRAMEWORK_NAME, MRConfig.LOCAL_FRAMEWORK_NAME);\r\n    FileInputFormat.setInputPaths(conf, INPUT_DIR);\r\n    FileOutputFormat.setOutputPath(conf, OUTPUT_DIR);\r\n    JobClient.runJob(conf);\r\n    Path[] outputFiles = FileUtil.stat2Paths(fs.listStatus(OUTPUT_DIR, new Utils.OutputFileUtils.OutputFilesFilter()));\r\n    assertEquals(1, outputFiles.length);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "createSequenceFile",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void createSequenceFile(int numRecords) throws Exception\n{\r\n    SequenceFile.Writer writer = SequenceFile.createWriter(fs, conf, inFile, Text.class, BytesWritable.class);\r\n    try {\r\n        for (int i = 1; i <= numRecords; i++) {\r\n            Text key = new Text(Integer.toString(i));\r\n            byte[] data = new byte[random.nextInt(10)];\r\n            random.nextBytes(data);\r\n            BytesWritable value = new BytesWritable(data);\r\n            writer.append(key, value);\r\n        }\r\n    } finally {\r\n        writer.close();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "countRecords",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "int countRecords(int numSplits) throws IOException\n{\r\n    InputFormat<Text, BytesWritable> format = new SequenceFileInputFilter<Text, BytesWritable>();\r\n    Text key = new Text();\r\n    BytesWritable value = new BytesWritable();\r\n    if (numSplits == 0) {\r\n        numSplits = random.nextInt(MAX_LENGTH / (SequenceFile.SYNC_INTERVAL / 20)) + 1;\r\n    }\r\n    InputSplit[] splits = format.getSplits(job, numSplits);\r\n    int count = 0;\r\n    LOG.info(\"Generated \" + splits.length + \" splits.\");\r\n    for (int j = 0; j < splits.length; j++) {\r\n        RecordReader<Text, BytesWritable> reader = format.getRecordReader(splits[j], job, reporter);\r\n        try {\r\n            while (reader.next(key, value)) {\r\n                LOG.info(\"Accept record \" + key.toString());\r\n                count++;\r\n            }\r\n        } finally {\r\n            reader.close();\r\n        }\r\n    }\r\n    return count;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "testRegexFilter",
  "errType" : null,
  "containingMethodsNum" : 10,
  "sourceCodeText" : "void testRegexFilter() throws Exception\n{\r\n    LOG.info(\"Testing Regex Filter with patter: \\\\A10*\");\r\n    SequenceFileInputFilter.setFilterClass(job, SequenceFileInputFilter.RegexFilter.class);\r\n    SequenceFileInputFilter.RegexFilter.setPattern(job, \"\\\\A10*\");\r\n    fs.delete(inDir, true);\r\n    for (int length = 1; length < MAX_LENGTH; length += random.nextInt(MAX_LENGTH / 10) + 1) {\r\n        LOG.info(\"******Number of records: \" + length);\r\n        createSequenceFile(length);\r\n        int count = countRecords(0);\r\n        assertEquals(count, length == 0 ? 0 : (int) Math.log10(length) + 1);\r\n    }\r\n    fs.delete(inDir, true);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "testPercentFilter",
  "errType" : null,
  "containingMethodsNum" : 11,
  "sourceCodeText" : "void testPercentFilter() throws Exception\n{\r\n    LOG.info(\"Testing Percent Filter with frequency: 1000\");\r\n    SequenceFileInputFilter.setFilterClass(job, SequenceFileInputFilter.PercentFilter.class);\r\n    SequenceFileInputFilter.PercentFilter.setFrequency(job, 1000);\r\n    fs.delete(inDir, true);\r\n    for (int length = 0; length < MAX_LENGTH; length += random.nextInt(MAX_LENGTH / 10) + 1) {\r\n        LOG.info(\"******Number of records: \" + length);\r\n        createSequenceFile(length);\r\n        int count = countRecords(1);\r\n        LOG.info(\"Accepted \" + count + \" records\");\r\n        int expectedCount = length / 1000;\r\n        if (expectedCount * 1000 != length)\r\n            expectedCount++;\r\n        assertThat(count).isEqualTo(expectedCount);\r\n    }\r\n    fs.delete(inDir, true);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "testMD5Filter",
  "errType" : null,
  "containingMethodsNum" : 9,
  "sourceCodeText" : "void testMD5Filter() throws Exception\n{\r\n    LOG.info(\"Testing MD5 Filter with frequency: 1000\");\r\n    SequenceFileInputFilter.setFilterClass(job, SequenceFileInputFilter.MD5Filter.class);\r\n    SequenceFileInputFilter.MD5Filter.setFrequency(job, 1000);\r\n    fs.delete(inDir, true);\r\n    for (int length = 0; length < MAX_LENGTH; length += random.nextInt(MAX_LENGTH / 10) + 1) {\r\n        LOG.info(\"******Number of records: \" + length);\r\n        createSequenceFile(length);\r\n        LOG.info(\"Accepted \" + countRecords(0) + \" records\");\r\n    }\r\n    fs.delete(inDir, true);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\aggregate",
  "methodName" : "generateKeyValPairs",
  "errType" : null,
  "containingMethodsNum" : 16,
  "sourceCodeText" : "ArrayList<Entry<Text, Text>> generateKeyValPairs(Object key, Object val)\n{\r\n    ArrayList<Entry<Text, Text>> retv = new ArrayList<Entry<Text, Text>>();\r\n    String[] words = val.toString().split(\" \");\r\n    String countType;\r\n    String id;\r\n    Entry<Text, Text> e;\r\n    for (String word : words) {\r\n        long numVal = Long.parseLong(word);\r\n        countType = LONG_VALUE_SUM;\r\n        id = \"count_\" + word;\r\n        e = generateEntry(countType, id, ONE);\r\n        if (e != null) {\r\n            retv.add(e);\r\n        }\r\n        countType = LONG_VALUE_MAX;\r\n        id = \"max\";\r\n        e = generateEntry(countType, id, new Text(word));\r\n        if (e != null) {\r\n            retv.add(e);\r\n        }\r\n        countType = LONG_VALUE_MIN;\r\n        id = \"min\";\r\n        e = generateEntry(countType, id, new Text(word));\r\n        if (e != null) {\r\n            retv.add(e);\r\n        }\r\n        countType = STRING_VALUE_MAX;\r\n        id = \"value_as_string_max\";\r\n        e = generateEntry(countType, id, new Text(\"\" + numVal));\r\n        if (e != null) {\r\n            retv.add(e);\r\n        }\r\n        countType = STRING_VALUE_MIN;\r\n        id = \"value_as_string_min\";\r\n        e = generateEntry(countType, id, new Text(\"\" + numVal));\r\n        if (e != null) {\r\n            retv.add(e);\r\n        }\r\n        countType = UNIQ_VALUE_COUNT;\r\n        id = \"uniq_count\";\r\n        e = generateEntry(countType, id, new Text(word));\r\n        if (e != null) {\r\n            retv.add(e);\r\n        }\r\n        countType = VALUE_HISTOGRAM;\r\n        id = \"histogram\";\r\n        e = generateEntry(countType, id, new Text(word));\r\n        if (e != null) {\r\n            retv.add(e);\r\n        }\r\n    }\r\n    return retv;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\util",
  "methodName" : "execute",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void execute(String root, Runnable task)\n{\r\n    asyncDiskService.execute(root, task);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\util",
  "methodName" : "shutdown",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void shutdown()\n{\r\n    asyncDiskService.shutdown();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\util",
  "methodName" : "shutdownNow",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "List<Runnable> shutdownNow()\n{\r\n    return asyncDiskService.shutdownNow();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\util",
  "methodName" : "awaitTermination",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "boolean awaitTermination(long milliseconds) throws InterruptedException\n{\r\n    return asyncDiskService.awaitTermination(milliseconds);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\util",
  "methodName" : "moveAndDeleteRelativePath",
  "errType" : [ "FileNotFoundException" ],
  "containingMethodsNum" : 8,
  "sourceCodeText" : "boolean moveAndDeleteRelativePath(String volume, String pathName) throws IOException\n{\r\n    volume = normalizePath(volume);\r\n    String newPathName = format.format(new Date()) + \"_\" + uniqueId.getAndIncrement();\r\n    newPathName = TOBEDELETED + Path.SEPARATOR_CHAR + newPathName;\r\n    Path source = new Path(volume, pathName);\r\n    Path target = new Path(volume, newPathName);\r\n    try {\r\n        if (!localFileSystem.rename(source, target)) {\r\n            if (!localFileSystem.exists(source)) {\r\n                return false;\r\n            }\r\n            if (!localFileSystem.mkdirs(new Path(volume, TOBEDELETED))) {\r\n                throw new IOException(\"Cannot create \" + TOBEDELETED + \" under \" + volume);\r\n            }\r\n            if (!localFileSystem.rename(source, target)) {\r\n                throw new IOException(\"Cannot rename \" + source + \" to \" + target);\r\n            }\r\n        }\r\n    } catch (FileNotFoundException e) {\r\n        return false;\r\n    }\r\n    DeleteTask task = new DeleteTask(volume, pathName, newPathName);\r\n    execute(volume, task);\r\n    return true;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\util",
  "methodName" : "moveAndDeleteFromEachVolume",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "boolean moveAndDeleteFromEachVolume(String pathName) throws IOException\n{\r\n    boolean result = true;\r\n    for (int i = 0; i < volumes.length; i++) {\r\n        result = result && moveAndDeleteRelativePath(volumes[i], pathName);\r\n    }\r\n    return result;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\util",
  "methodName" : "cleanupAllVolumes",
  "errType" : [ "Exception" ],
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void cleanupAllVolumes() throws IOException\n{\r\n    for (int v = 0; v < volumes.length; v++) {\r\n        FileStatus[] files = null;\r\n        try {\r\n            files = localFileSystem.listStatus(new Path(volumes[v]));\r\n        } catch (Exception e) {\r\n        }\r\n        if (files != null) {\r\n            for (int f = 0; f < files.length; f++) {\r\n                String entryName = files[f].getPath().getName();\r\n                if (!TOBEDELETED.equals(entryName)) {\r\n                    moveAndDeleteRelativePath(volumes[v], entryName);\r\n                }\r\n            }\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\util",
  "methodName" : "normalizePath",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String normalizePath(String path)\n{\r\n    return this.localFileSystem.makeQualified(new Path(path)).toUri().getPath();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\util",
  "methodName" : "getRelativePathName",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "String getRelativePathName(String absolutePathName, String volume)\n{\r\n    absolutePathName = normalizePath(absolutePathName);\r\n    if (!absolutePathName.startsWith(volume)) {\r\n        return null;\r\n    }\r\n    String fileName = absolutePathName.substring(volume.length());\r\n    if (fileName.charAt(0) == Path.SEPARATOR_CHAR) {\r\n        fileName = fileName.substring(1);\r\n    }\r\n    return fileName;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\util",
  "methodName" : "moveAndDeleteAbsolutePath",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "boolean moveAndDeleteAbsolutePath(String absolutePathName) throws IOException\n{\r\n    for (int v = 0; v < volumes.length; v++) {\r\n        String relative = getRelativePathName(absolutePathName, volumes[v]);\r\n        if (relative != null) {\r\n            return moveAndDeleteRelativePath(volumes[v], relative);\r\n        }\r\n    }\r\n    throw new IOException(\"Cannot delete \" + absolutePathName + \" because it's outside of all volumes.\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\chain",
  "methodName" : "cleanFlags",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void cleanFlags(Configuration conf) throws IOException\n{\r\n    FileSystem fs = FileSystem.get(conf);\r\n    fs.delete(flagDir, true);\r\n    fs.mkdirs(flagDir);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\chain",
  "methodName" : "writeFlag",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void writeFlag(Configuration conf, String flag) throws IOException\n{\r\n    FileSystem fs = FileSystem.get(conf);\r\n    if (getFlag(conf, flag)) {\r\n        fail(\"Flag \" + flag + \" already exists\");\r\n    }\r\n    DataOutputStream file = fs.create(new Path(flagDir, flag));\r\n    file.close();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\chain",
  "methodName" : "getFlag",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "boolean getFlag(Configuration conf, String flag) throws IOException\n{\r\n    FileSystem fs = FileSystem.get(conf);\r\n    return fs.exists(new Path(flagDir, flag));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\chain",
  "methodName" : "testChain",
  "errType" : null,
  "containingMethodsNum" : 46,
  "sourceCodeText" : "void testChain() throws Exception\n{\r\n    Path inDir = new Path(localPathRoot, \"testing/chain/input\");\r\n    Path outDir = new Path(localPathRoot, \"testing/chain/output\");\r\n    String input = \"1\\n2\\n\";\r\n    String expectedOutput = \"0\\t1ABCRDEF\\n2\\t2ABCRDEF\\n\";\r\n    Configuration conf = createJobConf();\r\n    cleanFlags(conf);\r\n    conf.set(\"a\", \"X\");\r\n    Job job = MapReduceTestUtil.createJob(conf, inDir, outDir, 1, 1, input);\r\n    job.setJobName(\"chain\");\r\n    Configuration mapAConf = new Configuration(false);\r\n    mapAConf.set(\"a\", \"A\");\r\n    ChainMapper.addMapper(job, AMap.class, LongWritable.class, Text.class, LongWritable.class, Text.class, mapAConf);\r\n    ChainMapper.addMapper(job, BMap.class, LongWritable.class, Text.class, LongWritable.class, Text.class, null);\r\n    ChainMapper.addMapper(job, CMap.class, LongWritable.class, Text.class, LongWritable.class, Text.class, null);\r\n    Configuration reduceConf = new Configuration(false);\r\n    reduceConf.set(\"a\", \"C\");\r\n    ChainReducer.setReducer(job, RReduce.class, LongWritable.class, Text.class, LongWritable.class, Text.class, reduceConf);\r\n    ChainReducer.addMapper(job, DMap.class, LongWritable.class, Text.class, LongWritable.class, Text.class, null);\r\n    Configuration mapEConf = new Configuration(false);\r\n    mapEConf.set(\"a\", \"E\");\r\n    ChainReducer.addMapper(job, EMap.class, LongWritable.class, Text.class, LongWritable.class, Text.class, mapEConf);\r\n    ChainReducer.addMapper(job, FMap.class, LongWritable.class, Text.class, LongWritable.class, Text.class, null);\r\n    job.waitForCompletion(true);\r\n    assertTrue(\"Job failed\", job.isSuccessful());\r\n    String str = \"flag not set\";\r\n    assertTrue(str, getFlag(conf, \"map.setup.A\"));\r\n    assertTrue(str, getFlag(conf, \"map.setup.B\"));\r\n    assertTrue(str, getFlag(conf, \"map.setup.C\"));\r\n    assertTrue(str, getFlag(conf, \"reduce.setup.R\"));\r\n    assertTrue(str, getFlag(conf, \"map.setup.D\"));\r\n    assertTrue(str, getFlag(conf, \"map.setup.E\"));\r\n    assertTrue(str, getFlag(conf, \"map.setup.F\"));\r\n    assertTrue(str, getFlag(conf, \"map.A.value.1\"));\r\n    assertTrue(str, getFlag(conf, \"map.A.value.2\"));\r\n    assertTrue(str, getFlag(conf, \"map.B.value.1A\"));\r\n    assertTrue(str, getFlag(conf, \"map.B.value.2A\"));\r\n    assertTrue(str, getFlag(conf, \"map.C.value.1AB\"));\r\n    assertTrue(str, getFlag(conf, \"map.C.value.2AB\"));\r\n    assertTrue(str, getFlag(conf, \"reduce.R.value.1ABC\"));\r\n    assertTrue(str, getFlag(conf, \"reduce.R.value.2ABC\"));\r\n    assertTrue(str, getFlag(conf, \"map.D.value.1ABCR\"));\r\n    assertTrue(str, getFlag(conf, \"map.D.value.2ABCR\"));\r\n    assertTrue(str, getFlag(conf, \"map.E.value.1ABCRD\"));\r\n    assertTrue(str, getFlag(conf, \"map.E.value.2ABCRD\"));\r\n    assertTrue(str, getFlag(conf, \"map.F.value.1ABCRDE\"));\r\n    assertTrue(str, getFlag(conf, \"map.F.value.2ABCRDE\"));\r\n    assertTrue(getFlag(conf, \"map.cleanup.A\"));\r\n    assertTrue(getFlag(conf, \"map.cleanup.B\"));\r\n    assertTrue(getFlag(conf, \"map.cleanup.C\"));\r\n    assertTrue(getFlag(conf, \"reduce.cleanup.R\"));\r\n    assertTrue(getFlag(conf, \"map.cleanup.D\"));\r\n    assertTrue(getFlag(conf, \"map.cleanup.E\"));\r\n    assertTrue(getFlag(conf, \"map.cleanup.F\"));\r\n    assertEquals(\"Outputs doesn't match\", expectedOutput, MapReduceTestUtil.readOutput(outDir, conf));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\fs",
  "methodName" : "testFSBlocks",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testFSBlocks() throws Exception\n{\r\n    testFSBlocks(\"/\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\fs",
  "methodName" : "testFSBlocks",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void testFSBlocks(String rootName) throws Exception\n{\r\n    createInputFile(rootName);\r\n    runDistributedFSCheck();\r\n    cleanup();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\fs",
  "methodName" : "createInputFile",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void createInputFile(String rootName) throws IOException\n{\r\n    cleanup();\r\n    Path inputFile = new Path(MAP_INPUT_DIR, \"in_file\");\r\n    SequenceFile.Writer writer = SequenceFile.createWriter(fs, fsConfig, inputFile, Text.class, LongWritable.class, CompressionType.NONE);\r\n    try {\r\n        nrFiles = 0;\r\n        listSubtree(new Path(rootName), writer);\r\n    } finally {\r\n        writer.close();\r\n    }\r\n    LOG.info(\"Created map input files.\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\fs",
  "methodName" : "listSubtree",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void listSubtree(Path rootFile, SequenceFile.Writer writer) throws IOException\n{\r\n    FileStatus rootStatus = fs.getFileStatus(rootFile);\r\n    listSubtree(rootStatus, writer);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\fs",
  "methodName" : "listSubtree",
  "errType" : [ "FileNotFoundException" ],
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void listSubtree(FileStatus rootStatus, SequenceFile.Writer writer) throws IOException\n{\r\n    Path rootFile = rootStatus.getPath();\r\n    if (rootStatus.isFile()) {\r\n        nrFiles++;\r\n        long blockSize = fs.getDefaultBlockSize(rootFile);\r\n        long fileLength = rootStatus.getLen();\r\n        for (long offset = 0; offset < fileLength; offset += blockSize) writer.append(new Text(rootFile.toString()), new LongWritable(offset));\r\n        return;\r\n    }\r\n    FileStatus[] children = null;\r\n    try {\r\n        children = fs.listStatus(rootFile);\r\n    } catch (FileNotFoundException fnfe) {\r\n        throw new IOException(\"Could not get listing for \" + rootFile);\r\n    }\r\n    for (int i = 0; i < children.length; i++) listSubtree(children[i], writer);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\fs",
  "methodName" : "runDistributedFSCheck",
  "errType" : null,
  "containingMethodsNum" : 10,
  "sourceCodeText" : "void runDistributedFSCheck() throws Exception\n{\r\n    JobConf job = new JobConf(fs.getConf(), DistributedFSCheck.class);\r\n    FileInputFormat.setInputPaths(job, MAP_INPUT_DIR);\r\n    job.setInputFormat(SequenceFileInputFormat.class);\r\n    job.setMapperClass(DistributedFSCheckMapper.class);\r\n    job.setReducerClass(AccumulatingReducer.class);\r\n    FileOutputFormat.setOutputPath(job, READ_DIR);\r\n    job.setOutputKeyClass(Text.class);\r\n    job.setOutputValueClass(Text.class);\r\n    job.setNumReduceTasks(1);\r\n    JobClient.runJob(job);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\fs",
  "methodName" : "main",
  "errType" : null,
  "containingMethodsNum" : 18,
  "sourceCodeText" : "void main(String[] args) throws Exception\n{\r\n    int testType = TEST_TYPE_READ;\r\n    int bufferSize = DEFAULT_BUFFER_SIZE;\r\n    String resFileName = DEFAULT_RES_FILE_NAME;\r\n    String rootName = \"/\";\r\n    boolean viewStats = false;\r\n    String usage = \"Usage: DistributedFSCheck [-root name] [-clean] [-resFile resultFileName] [-bufferSize Bytes] [-stats] \";\r\n    if (args.length == 1 && args[0].startsWith(\"-h\")) {\r\n        System.err.println(usage);\r\n        System.exit(-1);\r\n    }\r\n    for (int i = 0; i < args.length; i++) {\r\n        if (args[i].equals(\"-root\")) {\r\n            rootName = args[++i];\r\n        } else if (args[i].startsWith(\"-clean\")) {\r\n            testType = TEST_TYPE_CLEANUP;\r\n        } else if (args[i].equals(\"-bufferSize\")) {\r\n            bufferSize = Integer.parseInt(args[++i]);\r\n        } else if (args[i].equals(\"-resFile\")) {\r\n            resFileName = args[++i];\r\n        } else if (args[i].startsWith(\"-stat\")) {\r\n            viewStats = true;\r\n        }\r\n    }\r\n    LOG.info(\"root = \" + rootName);\r\n    LOG.info(\"bufferSize = \" + bufferSize);\r\n    Configuration conf = new Configuration();\r\n    conf.setInt(\"test.io.file.buffer.size\", bufferSize);\r\n    DistributedFSCheck test = new DistributedFSCheck(conf);\r\n    if (testType == TEST_TYPE_CLEANUP) {\r\n        test.cleanup();\r\n        return;\r\n    }\r\n    test.createInputFile(rootName);\r\n    long tStart = System.currentTimeMillis();\r\n    test.runDistributedFSCheck();\r\n    long execTime = System.currentTimeMillis() - tStart;\r\n    test.analyzeResult(execTime, resFileName, viewStats);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\fs",
  "methodName" : "analyzeResult",
  "errType" : null,
  "containingMethodsNum" : 38,
  "sourceCodeText" : "void analyzeResult(long execTime, String resFileName, boolean viewStats) throws IOException\n{\r\n    Path reduceFile = new Path(READ_DIR, \"part-00000\");\r\n    DataInputStream in;\r\n    in = new DataInputStream(fs.open(reduceFile));\r\n    BufferedReader lines;\r\n    lines = new BufferedReader(new InputStreamReader(in));\r\n    long blocks = 0;\r\n    long size = 0;\r\n    long time = 0;\r\n    float rate = 0;\r\n    StringTokenizer badBlocks = null;\r\n    long nrBadBlocks = 0;\r\n    String line;\r\n    while ((line = lines.readLine()) != null) {\r\n        StringTokenizer tokens = new StringTokenizer(line, \" \\t\\n\\r\\f%\");\r\n        String attr = tokens.nextToken();\r\n        if (attr.endsWith(\"blocks\"))\r\n            blocks = Long.parseLong(tokens.nextToken());\r\n        else if (attr.endsWith(\"size\"))\r\n            size = Long.parseLong(tokens.nextToken());\r\n        else if (attr.endsWith(\"time\"))\r\n            time = Long.parseLong(tokens.nextToken());\r\n        else if (attr.endsWith(\"rate\"))\r\n            rate = Float.parseFloat(tokens.nextToken());\r\n        else if (attr.endsWith(\"badBlocks\")) {\r\n            badBlocks = new StringTokenizer(tokens.nextToken(), \";\");\r\n            nrBadBlocks = badBlocks.countTokens();\r\n        }\r\n    }\r\n    Vector<String> resultLines = new Vector<String>();\r\n    resultLines.add(\"----- DistributedFSCheck ----- : \");\r\n    resultLines.add(\"               Date & time: \" + new Date(System.currentTimeMillis()));\r\n    resultLines.add(\"    Total number of blocks: \" + blocks);\r\n    resultLines.add(\"    Total number of  files: \" + nrFiles);\r\n    resultLines.add(\"Number of corrupted blocks: \" + nrBadBlocks);\r\n    int nrBadFilesPos = resultLines.size();\r\n    TreeSet<String> badFiles = new TreeSet<String>();\r\n    long nrBadFiles = 0;\r\n    if (nrBadBlocks > 0) {\r\n        resultLines.add(\"\");\r\n        resultLines.add(\"----- Corrupted Blocks (file@offset) ----- : \");\r\n        while (badBlocks.hasMoreTokens()) {\r\n            String curBlock = badBlocks.nextToken();\r\n            resultLines.add(curBlock);\r\n            badFiles.add(curBlock.substring(0, curBlock.indexOf('@')));\r\n        }\r\n        nrBadFiles = badFiles.size();\r\n    }\r\n    resultLines.insertElementAt(\" Number of corrupted files: \" + nrBadFiles, nrBadFilesPos);\r\n    if (viewStats) {\r\n        resultLines.add(\"\");\r\n        resultLines.add(\"-----   Performance  ----- : \");\r\n        resultLines.add(\"         Total MBytes read: \" + size / MEGA);\r\n        resultLines.add(\"         Throughput mb/sec: \" + (float) size * 1000.0 / (time * MEGA));\r\n        resultLines.add(\"    Average IO rate mb/sec: \" + rate / 1000 / blocks);\r\n        resultLines.add(\"        Test exec time sec: \" + (float) execTime / 1000);\r\n    }\r\n    PrintStream res = new PrintStream(new FileOutputStream(new File(resFileName), true));\r\n    for (int i = 0; i < resultLines.size(); i++) {\r\n        String cur = resultLines.get(i);\r\n        LOG.info(cur);\r\n        res.println(cur);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\fs",
  "methodName" : "cleanup",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void cleanup() throws IOException\n{\r\n    LOG.info(\"Cleaning up test files\");\r\n    fs.delete(TEST_ROOT_DIR, true);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "runTestLazyOutput",
  "errType" : null,
  "containingMethodsNum" : 13,
  "sourceCodeText" : "void runTestLazyOutput(Configuration conf, Path output, int numReducers, boolean createLazily) throws Exception\n{\r\n    Job job = Job.getInstance(conf, \"Test-Lazy-Output\");\r\n    FileInputFormat.setInputPaths(job, INPUTPATH);\r\n    FileOutputFormat.setOutputPath(job, output);\r\n    job.setJarByClass(TestMapReduceLazyOutput.class);\r\n    job.setInputFormatClass(TextInputFormat.class);\r\n    job.setOutputKeyClass(LongWritable.class);\r\n    job.setOutputValueClass(Text.class);\r\n    job.setNumReduceTasks(numReducers);\r\n    job.setMapperClass(TestMapper.class);\r\n    job.setReducerClass(TestReducer.class);\r\n    if (createLazily) {\r\n        LazyOutputFormat.setOutputFormatClass(job, TextOutputFormat.class);\r\n    } else {\r\n        job.setOutputFormatClass(TextOutputFormat.class);\r\n    }\r\n    assertTrue(job.waitForCompletion(true));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "createInput",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void createInput(FileSystem fs, int numMappers) throws Exception\n{\r\n    for (int i = 0; i < numMappers; i++) {\r\n        OutputStream os = fs.create(new Path(INPUTPATH, \"text\" + i + \".txt\"));\r\n        Writer wr = new OutputStreamWriter(os);\r\n        for (String inp : INPUTLIST) {\r\n            wr.write(inp + \"\\n\");\r\n        }\r\n        wr.close();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "testLazyOutput",
  "errType" : null,
  "containingMethodsNum" : 18,
  "sourceCodeText" : "void testLazyOutput() throws Exception\n{\r\n    MiniDFSCluster dfs = null;\r\n    MiniMRCluster mr = null;\r\n    FileSystem fileSys = null;\r\n    try {\r\n        Configuration conf = new Configuration();\r\n        dfs = new MiniDFSCluster.Builder(conf).numDataNodes(NUM_HADOOP_WORKERS).build();\r\n        fileSys = dfs.getFileSystem();\r\n        mr = new MiniMRCluster(NUM_HADOOP_WORKERS, fileSys.getUri().toString(), 1);\r\n        int numReducers = 2;\r\n        int numMappers = NUM_HADOOP_WORKERS * NUM_MAPS_PER_NODE;\r\n        createInput(fileSys, numMappers);\r\n        Path output1 = new Path(\"/testlazy/output1\");\r\n        runTestLazyOutput(mr.createJobConf(), output1, numReducers, true);\r\n        Path[] fileList = FileUtil.stat2Paths(fileSys.listStatus(output1, new Utils.OutputFileUtils.OutputFilesFilter()));\r\n        for (int i = 0; i < fileList.length; ++i) {\r\n            System.out.println(\"Test1 File list[\" + i + \"]\" + \": \" + fileList[i]);\r\n        }\r\n        assertTrue(fileList.length == (numReducers - 1));\r\n        Path output2 = new Path(\"/testlazy/output2\");\r\n        runTestLazyOutput(mr.createJobConf(), output2, 0, true);\r\n        fileList = FileUtil.stat2Paths(fileSys.listStatus(output2, new Utils.OutputFileUtils.OutputFilesFilter()));\r\n        for (int i = 0; i < fileList.length; ++i) {\r\n            System.out.println(\"Test2 File list[\" + i + \"]\" + \": \" + fileList[i]);\r\n        }\r\n        assertTrue(fileList.length == numMappers - 1);\r\n        Path output3 = new Path(\"/testlazy/output3\");\r\n        runTestLazyOutput(mr.createJobConf(), output3, 0, false);\r\n        fileList = FileUtil.stat2Paths(fileSys.listStatus(output3, new Utils.OutputFileUtils.OutputFilesFilter()));\r\n        for (int i = 0; i < fileList.length; ++i) {\r\n            System.out.println(\"Test3 File list[\" + i + \"]\" + \": \" + fileList[i]);\r\n        }\r\n        assertTrue(fileList.length == numMappers);\r\n    } finally {\r\n        if (dfs != null) {\r\n            dfs.shutdown();\r\n        }\r\n        if (mr != null) {\r\n            mr.shutdown();\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "verifyNotZero",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void verifyNotZero(Configuration conf, String config)\n{\r\n    if (conf.getInt(config, 1) <= 0) {\r\n        throw new IllegalArgumentException(config + \"should be > 0\");\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "run",
  "errType" : null,
  "containingMethodsNum" : 26,
  "sourceCodeText" : "int run(String[] args) throws Exception\n{\r\n    Path outDir = new Path(LargeSorter.class.getName() + System.currentTimeMillis());\r\n    Configuration conf = getConf();\r\n    verifyNotZero(conf, MBS_PER_MAP);\r\n    verifyNotZero(conf, NUM_MAP_TASKS);\r\n    conf.setInt(MRJobConfig.NUM_MAPS, conf.getInt(NUM_MAP_TASKS, 2));\r\n    int ioSortMb = conf.getInt(MRJobConfig.IO_SORT_MB, 512);\r\n    int mapMb = Math.max(2 * ioSortMb, conf.getInt(MRJobConfig.MAP_MEMORY_MB, MRJobConfig.DEFAULT_MAP_MEMORY_MB));\r\n    conf.setInt(MRJobConfig.MAP_MEMORY_MB, mapMb);\r\n    conf.set(MRJobConfig.MAP_JAVA_OPTS, \"-Xmx\" + (mapMb - 200) + \"m\");\r\n    Job job = Job.getInstance(conf);\r\n    job.setJarByClass(LargeSorter.class);\r\n    job.setJobName(\"large-sorter\");\r\n    FileOutputFormat.setOutputPath(job, outDir);\r\n    job.setOutputKeyClass(BytesWritable.class);\r\n    job.setOutputValueClass(BytesWritable.class);\r\n    job.setInputFormatClass(RandomInputFormat.class);\r\n    job.setMapperClass(RandomMapper.class);\r\n    job.setReducerClass(Discarder.class);\r\n    job.setOutputFormatClass(SequenceFileOutputFormat.class);\r\n    job.setNumReduceTasks(conf.getInt(NUM_REDUCE_TASKS, 1));\r\n    Date startTime = new Date();\r\n    System.out.println(\"Job started: \" + startTime);\r\n    int ret = 1;\r\n    try {\r\n        ret = job.waitForCompletion(true) ? 0 : 1;\r\n    } finally {\r\n        FileSystem.get(conf).delete(outDir, true);\r\n    }\r\n    Date endTime = new Date();\r\n    System.out.println(\"Job ended: \" + endTime);\r\n    System.out.println(\"The job took \" + (endTime.getTime() - startTime.getTime()) / 1000 + \" seconds.\");\r\n    return ret;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "main",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void main(String[] args) throws Exception\n{\r\n    int res = ToolRunner.run(new Configuration(), new LargeSorter(), args);\r\n    System.exit(res);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\fs\\slive",
  "methodName" : "getAppendFile",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Path getAppendFile()\n{\r\n    Path fn = getFinder().getFile();\r\n    return fn;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\fs\\slive",
  "methodName" : "run",
  "errType" : [ "FileNotFoundException", "IOException|UnsupportedOperationException", "IOException" ],
  "containingMethodsNum" : 27,
  "sourceCodeText" : "List<OperationOutput> run(FileSystem fs)\n{\r\n    List<OperationOutput> out = super.run(fs);\r\n    OutputStream os = null;\r\n    try {\r\n        Path fn = getAppendFile();\r\n        Range<Long> appendSizeRange = getConfig().getAppendSize();\r\n        if (getConfig().shouldAppendUseBlockSize()) {\r\n            appendSizeRange = getConfig().getBlockSize();\r\n        }\r\n        long appendSize = Range.betweenPositive(getRandom(), appendSizeRange);\r\n        long timeTaken = 0, bytesAppended = 0;\r\n        DataWriter writer = new DataWriter(getRandom());\r\n        LOG.info(\"Attempting to append to file at \" + fn + \" of size \" + Helper.toByteInfo(appendSize));\r\n        {\r\n            long startTime = Timer.now();\r\n            os = fs.append(fn);\r\n            timeTaken += Timer.elapsed(startTime);\r\n            GenerateOutput stats = writer.writeSegment(appendSize, os);\r\n            timeTaken += stats.getTimeTaken();\r\n            bytesAppended += stats.getBytesWritten();\r\n            startTime = Timer.now();\r\n            os.close();\r\n            os = null;\r\n            timeTaken += Timer.elapsed(startTime);\r\n        }\r\n        out.add(new OperationOutput(OutputType.LONG, getType(), ReportWriter.BYTES_WRITTEN, bytesAppended));\r\n        out.add(new OperationOutput(OutputType.LONG, getType(), ReportWriter.OK_TIME_TAKEN, timeTaken));\r\n        out.add(new OperationOutput(OutputType.LONG, getType(), ReportWriter.SUCCESSES, 1L));\r\n        LOG.info(\"Appended \" + Helper.toByteInfo(bytesAppended) + \" to file \" + fn + \" in \" + timeTaken + \" milliseconds\");\r\n    } catch (FileNotFoundException e) {\r\n        out.add(new OperationOutput(OutputType.LONG, getType(), ReportWriter.NOT_FOUND, 1L));\r\n        LOG.warn(\"Error with appending\", e);\r\n    } catch (IOException | UnsupportedOperationException e) {\r\n        out.add(new OperationOutput(OutputType.LONG, getType(), ReportWriter.FAILURES, 1L));\r\n        LOG.warn(\"Error with appending\", e);\r\n    } finally {\r\n        if (os != null) {\r\n            try {\r\n                os.close();\r\n            } catch (IOException e) {\r\n                LOG.warn(\"Error with closing append stream\", e);\r\n            }\r\n        }\r\n    }\r\n    return out;\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 2,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "map",
  "errType" : [ "Exception" ],
  "containingMethodsNum" : 24,
  "sourceCodeText" : "void map(IntWritable key, IntWritable val, Context context) throws IOException\n{\r\n    TimelineClient tlc = TimelineClient.createTimelineClient();\r\n    Configuration conf = context.getConfiguration();\r\n    final int kbs = conf.getInt(KBS_SENT, KBS_SENT_DEFAULT);\r\n    long totalTime = 0;\r\n    final int testtimes = conf.getInt(TEST_TIMES, TEST_TIMES_DEFAULT);\r\n    final Random rand = new Random();\r\n    final TaskAttemptID taskAttemptId = context.getTaskAttemptID();\r\n    final char[] payLoad = new char[kbs * 1024];\r\n    for (int i = 0; i < testtimes; i++) {\r\n        for (int xx = 0; xx < kbs * 1024; xx++) {\r\n            int alphaNumIdx = rand.nextInt(ALPHA_NUMS.length);\r\n            payLoad[xx] = ALPHA_NUMS[alphaNumIdx];\r\n        }\r\n        String entId = taskAttemptId + \"_\" + Integer.toString(i);\r\n        final TimelineEntity entity = new TimelineEntity();\r\n        entity.setEntityId(entId);\r\n        entity.setEntityType(\"FOO_ATTEMPT\");\r\n        entity.addOtherInfo(\"PERF_TEST\", payLoad);\r\n        TimelineEvent event = new TimelineEvent();\r\n        event.setTimestamp(System.currentTimeMillis());\r\n        event.setEventType(\"foo_event\");\r\n        entity.addEvent(event);\r\n        UserGroupInformation ugi = UserGroupInformation.getCurrentUser();\r\n        long startWrite = System.nanoTime();\r\n        try {\r\n            tlc.putEntities(entity);\r\n        } catch (Exception e) {\r\n            context.getCounter(PerfCounters.TIMELINE_SERVICE_WRITE_FAILURES).increment(1);\r\n            LOG.error(\"writing to the timeline service failed\", e);\r\n        }\r\n        long endWrite = System.nanoTime();\r\n        totalTime += TimeUnit.NANOSECONDS.toMillis(endWrite - startWrite);\r\n    }\r\n    LOG.info(\"wrote \" + testtimes + \" entities (\" + kbs * testtimes + \" kB) in \" + totalTime + \" ms\");\r\n    context.getCounter(PerfCounters.TIMELINE_SERVICE_WRITE_TIME).increment(totalTime);\r\n    context.getCounter(PerfCounters.TIMELINE_SERVICE_WRITE_COUNTER).increment(testtimes);\r\n    context.getCounter(PerfCounters.TIMELINE_SERVICE_WRITE_KBS).increment(kbs * testtimes);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "setupClass",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setupClass() throws Exception\n{\r\n    setupClassBase(TestClusterMapReduceTestCase.class);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "_testMapReduce",
  "errType" : null,
  "containingMethodsNum" : 28,
  "sourceCodeText" : "void _testMapReduce(boolean restart) throws Exception\n{\r\n    OutputStream os = getFileSystem().create(new Path(getInputDir(), \"text.txt\"));\r\n    Writer wr = new OutputStreamWriter(os);\r\n    wr.write(\"hello1\\n\");\r\n    wr.write(\"hello2\\n\");\r\n    wr.write(\"hello3\\n\");\r\n    wr.write(\"hello4\\n\");\r\n    wr.close();\r\n    if (restart) {\r\n        stopCluster();\r\n        startCluster(false, null);\r\n    }\r\n    JobConf conf = createJobConf();\r\n    conf.setJobName(\"mr\");\r\n    conf.setInputFormat(TextInputFormat.class);\r\n    conf.setMapOutputKeyClass(LongWritable.class);\r\n    conf.setMapOutputValueClass(Text.class);\r\n    conf.setOutputFormat(TextOutputFormat.class);\r\n    conf.setOutputKeyClass(LongWritable.class);\r\n    conf.setOutputValueClass(Text.class);\r\n    conf.setMapperClass(org.apache.hadoop.mapred.lib.IdentityMapper.class);\r\n    conf.setReducerClass(org.apache.hadoop.mapred.lib.IdentityReducer.class);\r\n    FileInputFormat.setInputPaths(conf, getInputDir());\r\n    FileOutputFormat.setOutputPath(conf, getOutputDir());\r\n    JobClient.runJob(conf);\r\n    Path[] outputFiles = FileUtil.stat2Paths(getFileSystem().listStatus(getOutputDir(), new Utils.OutputFileUtils.OutputFilesFilter()));\r\n    if (outputFiles.length > 0) {\r\n        InputStream is = getFileSystem().open(outputFiles[0]);\r\n        BufferedReader reader = new BufferedReader(new InputStreamReader(is));\r\n        String line = reader.readLine();\r\n        int counter = 0;\r\n        while (line != null) {\r\n            counter++;\r\n            assertTrue(line.contains(\"hello\"));\r\n            line = reader.readLine();\r\n        }\r\n        reader.close();\r\n        assertEquals(4, counter);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "testMapReduce",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testMapReduce() throws Exception\n{\r\n    _testMapReduce(false);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "testMapReduceRestarting",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testMapReduceRestarting() throws Exception\n{\r\n    _testMapReduce(true);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "testDFSRestart",
  "errType" : null,
  "containingMethodsNum" : 9,
  "sourceCodeText" : "void testDFSRestart() throws Exception\n{\r\n    Path file = new Path(getInputDir(), \"text.txt\");\r\n    OutputStream os = getFileSystem().create(file);\r\n    Writer wr = new OutputStreamWriter(os);\r\n    wr.close();\r\n    stopCluster();\r\n    startCluster(false, null);\r\n    assertTrue(getFileSystem().exists(file));\r\n    stopCluster();\r\n    startCluster(true, null);\r\n    assertFalse(getFileSystem().exists(file));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "testMRConfig",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void testMRConfig() throws Exception\n{\r\n    JobConf conf = createJobConf();\r\n    assertNull(conf.get(\"xyz\"));\r\n    Properties config = new Properties();\r\n    config.setProperty(\"xyz\", \"XYZ\");\r\n    stopCluster();\r\n    startCluster(false, config);\r\n    conf = createJobConf();\r\n    assertEquals(\"XYZ\", conf.get(\"xyz\"));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred\\join",
  "methodName" : "write",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void write(DataOutput out)\n{\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred\\join",
  "methodName" : "readFields",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void readFields(DataInput in)\n{\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred\\join",
  "methodName" : "compareTo",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int compareTo(Object o)\n{\r\n    throw new RuntimeException(\"Should never see this.\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred\\lib",
  "methodName" : "getFlagDir",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Path getFlagDir(boolean local)\n{\r\n    Path flagDir = new Path(\"testing/chain/flags\");\r\n    if (local) {\r\n        String localPathRoot = System.getProperty(\"test.build.data\", \"/tmp\").replace(' ', '+');\r\n        flagDir = new Path(localPathRoot, flagDir);\r\n    }\r\n    return flagDir;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred\\lib",
  "methodName" : "cleanFlags",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void cleanFlags(JobConf conf) throws IOException\n{\r\n    FileSystem fs = FileSystem.get(conf);\r\n    fs.delete(getFlagDir(conf.getBoolean(\"localFS\", true)), true);\r\n    fs.mkdirs(getFlagDir(conf.getBoolean(\"localFS\", true)));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred\\lib",
  "methodName" : "writeFlag",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void writeFlag(JobConf conf, String flag) throws IOException\n{\r\n    FileSystem fs = FileSystem.get(conf);\r\n    if (getFlag(conf, flag)) {\r\n        fail(\"Flag \" + flag + \" already exists\");\r\n    }\r\n    DataOutputStream file = fs.create(new Path(getFlagDir(conf.getBoolean(\"localFS\", true)), flag));\r\n    file.close();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred\\lib",
  "methodName" : "getFlag",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "boolean getFlag(JobConf conf, String flag) throws IOException\n{\r\n    FileSystem fs = FileSystem.get(conf);\r\n    return fs.exists(new Path(getFlagDir(conf.getBoolean(\"localFS\", true)), flag));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred\\lib",
  "methodName" : "testChain",
  "errType" : null,
  "containingMethodsNum" : 50,
  "sourceCodeText" : "void testChain() throws Exception\n{\r\n    Path inDir = new Path(\"testing/chain/input\");\r\n    Path outDir = new Path(\"testing/chain/output\");\r\n    if (isLocalFS()) {\r\n        String localPathRoot = System.getProperty(\"test.build.data\", \"/tmp\").replace(' ', '+');\r\n        inDir = new Path(localPathRoot, inDir);\r\n        outDir = new Path(localPathRoot, outDir);\r\n    }\r\n    JobConf conf = createJobConf();\r\n    conf.setBoolean(\"localFS\", isLocalFS());\r\n    conf.setInt(\"mapreduce.job.maps\", 1);\r\n    cleanFlags(conf);\r\n    FileSystem fs = FileSystem.get(conf);\r\n    fs.delete(outDir, true);\r\n    if (!fs.mkdirs(inDir)) {\r\n        throw new IOException(\"Mkdirs failed to create \" + inDir.toString());\r\n    }\r\n    DataOutputStream file = fs.create(new Path(inDir, \"part-0\"));\r\n    file.writeBytes(\"1\\n2\\n\");\r\n    file.close();\r\n    conf.setJobName(\"chain\");\r\n    conf.setInputFormat(TextInputFormat.class);\r\n    conf.setOutputFormat(TextOutputFormat.class);\r\n    conf.set(\"a\", \"X\");\r\n    JobConf mapAConf = new JobConf(false);\r\n    mapAConf.set(\"a\", \"A\");\r\n    ChainMapper.addMapper(conf, AMap.class, LongWritable.class, Text.class, LongWritable.class, Text.class, true, mapAConf);\r\n    ChainMapper.addMapper(conf, BMap.class, LongWritable.class, Text.class, LongWritable.class, Text.class, false, null);\r\n    JobConf reduceConf = new JobConf(false);\r\n    reduceConf.set(\"a\", \"C\");\r\n    ChainReducer.setReducer(conf, CReduce.class, LongWritable.class, Text.class, LongWritable.class, Text.class, true, reduceConf);\r\n    ChainReducer.addMapper(conf, DMap.class, LongWritable.class, Text.class, LongWritable.class, Text.class, false, null);\r\n    JobConf mapEConf = new JobConf(false);\r\n    mapEConf.set(\"a\", \"E\");\r\n    ChainReducer.addMapper(conf, EMap.class, LongWritable.class, Text.class, LongWritable.class, Text.class, true, mapEConf);\r\n    FileInputFormat.setInputPaths(conf, inDir);\r\n    FileOutputFormat.setOutputPath(conf, outDir);\r\n    JobClient jc = new JobClient(conf);\r\n    RunningJob job = jc.submitJob(conf);\r\n    while (!job.isComplete()) {\r\n        Thread.sleep(100);\r\n    }\r\n    assertTrue(getFlag(conf, \"configure.A\"));\r\n    assertTrue(getFlag(conf, \"configure.B\"));\r\n    assertTrue(getFlag(conf, \"configure.C\"));\r\n    assertTrue(getFlag(conf, \"configure.D\"));\r\n    assertTrue(getFlag(conf, \"configure.E\"));\r\n    assertTrue(getFlag(conf, \"map.A.value.1\"));\r\n    assertTrue(getFlag(conf, \"map.A.value.2\"));\r\n    assertTrue(getFlag(conf, \"map.B.value.1\"));\r\n    assertTrue(getFlag(conf, \"map.B.value.2\"));\r\n    assertTrue(getFlag(conf, \"reduce.C.value.2\"));\r\n    assertTrue(getFlag(conf, \"reduce.C.value.1\"));\r\n    assertTrue(getFlag(conf, \"map.D.value.1\"));\r\n    assertTrue(getFlag(conf, \"map.D.value.2\"));\r\n    assertTrue(getFlag(conf, \"map.E.value.1\"));\r\n    assertTrue(getFlag(conf, \"map.E.value.2\"));\r\n    assertTrue(getFlag(conf, \"close.A\"));\r\n    assertTrue(getFlag(conf, \"close.B\"));\r\n    assertTrue(getFlag(conf, \"close.C\"));\r\n    assertTrue(getFlag(conf, \"close.D\"));\r\n    assertTrue(getFlag(conf, \"close.E\"));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\fs\\slive",
  "methodName" : "getWeight",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "double getWeight()\n{\r\n    return weight;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\fs\\slive",
  "methodName" : "getOperation",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Operation getOperation()\n{\r\n    return operation;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred\\lib",
  "methodName" : "configure",
  "errType" : null,
  "containingMethodsNum" : 39,
  "sourceCodeText" : "void configure(String keySpec, int expect) throws Exception\n{\r\n    Path testdir = new Path(TEST_DIR.getAbsolutePath());\r\n    Path inDir = new Path(testdir, \"in\");\r\n    Path outDir = new Path(testdir, \"out\");\r\n    FileSystem fs = getFileSystem();\r\n    fs.delete(testdir, true);\r\n    conf.setInputFormat(TextInputFormat.class);\r\n    FileInputFormat.setInputPaths(conf, inDir);\r\n    FileOutputFormat.setOutputPath(conf, outDir);\r\n    conf.setOutputKeyClass(Text.class);\r\n    conf.setOutputValueClass(LongWritable.class);\r\n    conf.setNumMapTasks(1);\r\n    conf.setNumReduceTasks(1);\r\n    conf.setOutputFormat(TextOutputFormat.class);\r\n    conf.setOutputKeyComparatorClass(KeyFieldBasedComparator.class);\r\n    conf.setKeyFieldComparatorOptions(keySpec);\r\n    conf.setKeyFieldPartitionerOptions(\"-k1.1,1.1\");\r\n    conf.set(JobContext.MAP_OUTPUT_KEY_FIELD_SEPARATOR, \" \");\r\n    conf.setMapperClass(InverseMapper.class);\r\n    conf.setReducerClass(IdentityReducer.class);\r\n    if (!fs.mkdirs(testdir)) {\r\n        throw new IOException(\"Mkdirs failed to create \" + testdir.toString());\r\n    }\r\n    if (!fs.mkdirs(inDir)) {\r\n        throw new IOException(\"Mkdirs failed to create \" + inDir.toString());\r\n    }\r\n    Path inFile = new Path(inDir, \"part0\");\r\n    FileOutputStream fos = new FileOutputStream(inFile.toString());\r\n    fos.write((line1 + \"\\n\").getBytes());\r\n    fos.write((line2 + \"\\n\").getBytes());\r\n    fos.close();\r\n    JobClient jc = new JobClient(conf);\r\n    RunningJob r_job = jc.submitJob(conf);\r\n    while (!r_job.isComplete()) {\r\n        Thread.sleep(1000);\r\n    }\r\n    if (!r_job.isSuccessful()) {\r\n        fail(\"Oops! The job broke due to an unexpected error\");\r\n    }\r\n    Path[] outputFiles = FileUtil.stat2Paths(getFileSystem().listStatus(outDir, new Utils.OutputFileUtils.OutputFilesFilter()));\r\n    if (outputFiles.length > 0) {\r\n        InputStream is = getFileSystem().open(outputFiles[0]);\r\n        BufferedReader reader = new BufferedReader(new InputStreamReader(is));\r\n        String line = reader.readLine();\r\n        if (expect == 1) {\r\n            assertTrue(line.startsWith(line1));\r\n        } else if (expect == 2) {\r\n            assertTrue(line.startsWith(line2));\r\n        }\r\n        line = reader.readLine();\r\n        if (expect == 1) {\r\n            assertTrue(line.startsWith(line2));\r\n        } else if (expect == 2) {\r\n            assertTrue(line.startsWith(line1));\r\n        }\r\n        reader.close();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred\\lib",
  "methodName" : "cleanup",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void cleanup()\n{\r\n    FileUtil.fullyDelete(TEST_DIR);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred\\lib",
  "methodName" : "testBasicUnixComparator",
  "errType" : null,
  "containingMethodsNum" : 14,
  "sourceCodeText" : "void testBasicUnixComparator() throws Exception\n{\r\n    configure(\"-k1,1n\", 1);\r\n    configure(\"-k2,2n\", 1);\r\n    configure(\"-k2.2,2n\", 2);\r\n    configure(\"-k3.4,3n\", 2);\r\n    configure(\"-k3.2,3.3n -k4,4n\", 2);\r\n    configure(\"-k3.2,3.3n -k4,4nr\", 1);\r\n    configure(\"-k2.4,2.4n\", 2);\r\n    configure(\"-k7,7\", 1);\r\n    configure(\"-k7,7n\", 2);\r\n    configure(\"-k8,8n\", 1);\r\n    configure(\"-k9,9\", 2);\r\n    configure(\"-k11,11\", 2);\r\n    configure(\"-k10,10\", 2);\r\n    localTestWithoutMRJob(\"-k9,9\", 1);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred\\lib",
  "methodName" : "localTestWithoutMRJob",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void localTestWithoutMRJob(String keySpec, int expect) throws Exception\n{\r\n    KeyFieldBasedComparator<Void, Void> keyFieldCmp = new KeyFieldBasedComparator<Void, Void>();\r\n    localConf.setKeyFieldComparatorOptions(keySpec);\r\n    keyFieldCmp.configure(localConf);\r\n    int result = keyFieldCmp.compare(line1_bytes, 0, line1_bytes.length, line2_bytes, 0, line2_bytes.length);\r\n    if ((expect >= 0 && result < 0) || (expect < 0 && result >= 0))\r\n        fail();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "testFormat",
  "errType" : null,
  "containingMethodsNum" : 19,
  "sourceCodeText" : "void testFormat() throws Exception\n{\r\n    JobConf job = new JobConf(conf);\r\n    FileSystem fs = FileSystem.getLocal(conf);\r\n    Path dir = new Path(System.getProperty(\"test.build.data\", \".\") + \"/mapred\");\r\n    Path file = new Path(dir, \"test.seq\");\r\n    Reporter reporter = Reporter.NULL;\r\n    int seed = new Random().nextInt();\r\n    Random random = new Random(seed);\r\n    fs.delete(dir, true);\r\n    FileInputFormat.setInputPaths(job, dir);\r\n    for (int length = 0; length < MAX_LENGTH; length += random.nextInt(MAX_LENGTH / 10) + 1) {\r\n        SequenceFile.Writer writer = SequenceFile.createWriter(fs, conf, file, IntWritable.class, BytesWritable.class);\r\n        try {\r\n            for (int i = 0; i < length; i++) {\r\n                IntWritable key = new IntWritable(i);\r\n                byte[] data = new byte[random.nextInt(10)];\r\n                random.nextBytes(data);\r\n                BytesWritable value = new BytesWritable(data);\r\n                writer.append(key, value);\r\n            }\r\n        } finally {\r\n            writer.close();\r\n        }\r\n        InputFormat<IntWritable, BytesWritable> format = new SequenceFileInputFormat<IntWritable, BytesWritable>();\r\n        IntWritable key = new IntWritable();\r\n        BytesWritable value = new BytesWritable();\r\n        for (int i = 0; i < 3; i++) {\r\n            int numSplits = random.nextInt(MAX_LENGTH / (SequenceFile.SYNC_INTERVAL / 20)) + 1;\r\n            InputSplit[] splits = format.getSplits(job, numSplits);\r\n            BitSet bits = new BitSet(length);\r\n            for (int j = 0; j < splits.length; j++) {\r\n                RecordReader<IntWritable, BytesWritable> reader = format.getRecordReader(splits[j], job, reporter);\r\n                try {\r\n                    int count = 0;\r\n                    while (reader.next(key, value)) {\r\n                        assertFalse(\"Key in multiple partitions.\", bits.get(key.get()));\r\n                        bits.set(key.get());\r\n                        count++;\r\n                    }\r\n                } finally {\r\n                    reader.close();\r\n                }\r\n            }\r\n            assertEquals(\"Some keys in no partition.\", length, bits.cardinality());\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 2,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "main",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void main(String[] args) throws Exception\n{\r\n    new TestSequenceFileInputFormat().testFormat();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "reset",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void reset()\n{\r\n    mapCleanup = false;\r\n    reduceCleanup = false;\r\n    recordReaderCleanup = false;\r\n    recordWriterCleanup = false;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "createInputFile",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void createInputFile(Path dirPath, int id, int numRecords) throws IOException\n{\r\n    final String MESSAGE = \"This is a line in a file: \";\r\n    Path filePath = new Path(dirPath, \"\" + id);\r\n    Configuration conf = new Configuration();\r\n    FileSystem fs = FileSystem.getLocal(conf);\r\n    OutputStream os = fs.create(filePath);\r\n    BufferedWriter w = new BufferedWriter(new OutputStreamWriter(os));\r\n    for (int i = 0; i < numRecords; i++) {\r\n        w.write(MESSAGE + id + \" \" + i + \"\\n\");\r\n    }\r\n    w.close();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "getInputPath",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Path getInputPath()\n{\r\n    String dataDir = System.getProperty(\"test.build.data\");\r\n    if (null == dataDir) {\r\n        return new Path(INPUT_DIR);\r\n    } else {\r\n        return new Path(new Path(dataDir), INPUT_DIR);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "getOutputPath",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Path getOutputPath()\n{\r\n    String dataDir = System.getProperty(\"test.build.data\");\r\n    if (null == dataDir) {\r\n        return new Path(OUTPUT_DIR);\r\n    } else {\r\n        return new Path(new Path(dataDir), OUTPUT_DIR);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "createInput",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "Path createInput() throws IOException\n{\r\n    Configuration conf = new Configuration();\r\n    FileSystem fs = FileSystem.getLocal(conf);\r\n    Path inputPath = getInputPath();\r\n    if (fs.exists(inputPath)) {\r\n        fs.delete(inputPath, true);\r\n    }\r\n    createInputFile(inputPath, 0, 10);\r\n    return inputPath;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "testMapCleanup",
  "errType" : null,
  "containingMethodsNum" : 17,
  "sourceCodeText" : "void testMapCleanup() throws Exception\n{\r\n    reset();\r\n    Job job = Job.getInstance();\r\n    Path inputPath = createInput();\r\n    Path outputPath = getOutputPath();\r\n    Configuration conf = new Configuration();\r\n    FileSystem fs = FileSystem.getLocal(conf);\r\n    if (fs.exists(outputPath)) {\r\n        fs.delete(outputPath, true);\r\n    }\r\n    job.setMapperClass(FailingMapper.class);\r\n    job.setInputFormatClass(TrackingTextInputFormat.class);\r\n    job.setOutputFormatClass(TrackingTextOutputFormat.class);\r\n    job.setNumReduceTasks(0);\r\n    FileInputFormat.addInputPath(job, inputPath);\r\n    FileOutputFormat.setOutputPath(job, outputPath);\r\n    job.waitForCompletion(true);\r\n    Assert.assertTrue(mapCleanup);\r\n    Assert.assertTrue(recordReaderCleanup);\r\n    Assert.assertTrue(recordWriterCleanup);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "testReduceCleanup",
  "errType" : null,
  "containingMethodsNum" : 21,
  "sourceCodeText" : "void testReduceCleanup() throws Exception\n{\r\n    reset();\r\n    Job job = Job.getInstance();\r\n    Path inputPath = createInput();\r\n    Path outputPath = getOutputPath();\r\n    Configuration conf = new Configuration();\r\n    FileSystem fs = FileSystem.getLocal(conf);\r\n    if (fs.exists(outputPath)) {\r\n        fs.delete(outputPath, true);\r\n    }\r\n    job.setMapperClass(TrackingTokenizerMapper.class);\r\n    job.setReducerClass(FailingReducer.class);\r\n    job.setOutputKeyClass(Text.class);\r\n    job.setOutputValueClass(IntWritable.class);\r\n    job.setInputFormatClass(TrackingTextInputFormat.class);\r\n    job.setOutputFormatClass(TrackingTextOutputFormat.class);\r\n    job.setNumReduceTasks(1);\r\n    FileInputFormat.addInputPath(job, inputPath);\r\n    FileOutputFormat.setOutputPath(job, outputPath);\r\n    job.waitForCompletion(true);\r\n    Assert.assertTrue(mapCleanup);\r\n    Assert.assertTrue(reduceCleanup);\r\n    Assert.assertTrue(recordReaderCleanup);\r\n    Assert.assertTrue(recordWriterCleanup);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "testJobSuccessCleanup",
  "errType" : null,
  "containingMethodsNum" : 24,
  "sourceCodeText" : "void testJobSuccessCleanup() throws Exception\n{\r\n    reset();\r\n    Job job = Job.getInstance();\r\n    Path inputPath = createInput();\r\n    Path outputPath = getOutputPath();\r\n    Configuration conf = new Configuration();\r\n    FileSystem fs = FileSystem.getLocal(conf);\r\n    if (fs.exists(outputPath)) {\r\n        fs.delete(outputPath, true);\r\n    }\r\n    job.setMapperClass(TrackingTokenizerMapper.class);\r\n    job.setReducerClass(TrackingIntSumReducer.class);\r\n    job.setOutputKeyClass(Text.class);\r\n    job.setOutputValueClass(IntWritable.class);\r\n    job.setInputFormatClass(TrackingTextInputFormat.class);\r\n    job.setOutputFormatClass(TrackingTextOutputFormat.class);\r\n    job.setNumReduceTasks(1);\r\n    FileInputFormat.addInputPath(job, inputPath);\r\n    FileOutputFormat.setOutputPath(job, outputPath);\r\n    job.waitForCompletion(true);\r\n    Assert.assertTrue(mapCleanup);\r\n    Assert.assertTrue(reduceCleanup);\r\n    Assert.assertTrue(recordReaderCleanup);\r\n    Assert.assertTrue(recordWriterCleanup);\r\n    Assert.assertNotNull(job.getCluster());\r\n    job.close();\r\n    Assert.assertNull(job.getCluster());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "testAllOpportunisticMaps",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testAllOpportunisticMaps() throws Exception\n{\r\n    doTest(4, 1, 1, 4);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "testHalfOpportunisticMaps",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testHalfOpportunisticMaps() throws Exception\n{\r\n    doTest(4, 1, 1, 2);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "doTest",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void doTest(int numMappers, int numReducers, int numNodes, int percent) throws Exception\n{\r\n    doTest(numMappers, numReducers, numNodes, 1000, percent);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "doTest",
  "errType" : null,
  "containingMethodsNum" : 12,
  "sourceCodeText" : "void doTest(int numMappers, int numReducers, int numNodes, int numLines, int percent) throws Exception\n{\r\n    MiniDFSCluster dfsCluster = null;\r\n    MiniMRClientCluster mrCluster = null;\r\n    FileSystem fileSystem = null;\r\n    try {\r\n        Configuration conf = MRJobConfUtil.initEncryptedIntermediateConfigsForTesting(null);\r\n        conf.setBoolean(YarnConfiguration.AMRM_PROXY_ENABLED, true);\r\n        conf.setBoolean(YarnConfiguration.OPPORTUNISTIC_CONTAINER_ALLOCATION_ENABLED, true);\r\n        conf.setBoolean(YarnConfiguration.DIST_SCHEDULING_ENABLED, true);\r\n        conf.setInt(YarnConfiguration.NM_OPPORTUNISTIC_CONTAINERS_MAX_QUEUE_LENGTH, 10);\r\n        dfsCluster = new MiniDFSCluster.Builder(conf).numDataNodes(numNodes).build();\r\n        fileSystem = dfsCluster.getFileSystem();\r\n        mrCluster = MiniMRClientClusterFactory.create(this.getClass(), numNodes, conf);\r\n        createInput(fileSystem, numMappers, numLines);\r\n        runMergeTest(new JobConf(conf), fileSystem, numMappers, numReducers, numLines, percent);\r\n    } finally {\r\n        if (dfsCluster != null) {\r\n            dfsCluster.shutdown();\r\n        }\r\n        if (mrCluster != null) {\r\n            mrCluster.stop();\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "createInput",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void createInput(FileSystem fs, int numMappers, int numLines) throws Exception\n{\r\n    fs.delete(INPUT_DIR, true);\r\n    for (int i = 0; i < numMappers; i++) {\r\n        OutputStream os = fs.create(new Path(INPUT_DIR, \"input_\" + i + \".txt\"));\r\n        Writer writer = new OutputStreamWriter(os);\r\n        for (int j = 0; j < numLines; j++) {\r\n            int k = j + 1;\r\n            String formattedNumber = String.format(\"%09d\", k);\r\n            writer.write(formattedNumber + \" \" + formattedNumber + \"\\n\");\r\n        }\r\n        writer.close();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "runMergeTest",
  "errType" : [ "IOException", "InterruptedException" ],
  "containingMethodsNum" : 23,
  "sourceCodeText" : "void runMergeTest(JobConf job, FileSystem fileSystem, int numMappers, int numReducers, int numLines, int percent) throws Exception\n{\r\n    fileSystem.delete(OUTPUT, true);\r\n    job.setJobName(\"Test\");\r\n    JobClient client = new JobClient(job);\r\n    RunningJob submittedJob = null;\r\n    FileInputFormat.setInputPaths(job, INPUT_DIR);\r\n    FileOutputFormat.setOutputPath(job, OUTPUT);\r\n    job.set(\"mapreduce.output.textoutputformat.separator\", \" \");\r\n    job.setInputFormat(TextInputFormat.class);\r\n    job.setMapOutputKeyClass(Text.class);\r\n    job.setMapOutputValueClass(Text.class);\r\n    job.setOutputKeyClass(Text.class);\r\n    job.setOutputValueClass(Text.class);\r\n    job.setMapperClass(MyMapper.class);\r\n    job.setPartitionerClass(MyPartitioner.class);\r\n    job.setOutputFormat(TextOutputFormat.class);\r\n    job.setNumReduceTasks(numReducers);\r\n    job.setInt(MRJobConfig.MR_NUM_OPPORTUNISTIC_MAPS_PERCENT, percent);\r\n    job.setInt(\"mapreduce.map.maxattempts\", 1);\r\n    job.setInt(\"mapreduce.reduce.maxattempts\", 1);\r\n    job.setInt(\"mapred.test.num_lines\", numLines);\r\n    try {\r\n        submittedJob = client.submitJob(job);\r\n        try {\r\n            if (!client.monitorAndPrintJob(job, submittedJob)) {\r\n                throw new IOException(\"Job failed!\");\r\n            }\r\n        } catch (InterruptedException ie) {\r\n            Thread.currentThread().interrupt();\r\n        }\r\n    } catch (IOException ioe) {\r\n        System.err.println(\"Job failed with: \" + ioe);\r\n    } finally {\r\n        verifyOutput(fileSystem, numMappers, numLines);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 2,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "verifyOutput",
  "errType" : null,
  "containingMethodsNum" : 11,
  "sourceCodeText" : "void verifyOutput(FileSystem fileSystem, int numMappers, int numLines) throws Exception\n{\r\n    FSDataInputStream dis = null;\r\n    long numValidRecords = 0;\r\n    long numInvalidRecords = 0;\r\n    String prevKeyValue = \"000000000\";\r\n    Path[] fileList = FileUtil.stat2Paths(fileSystem.listStatus(OUTPUT, new Utils.OutputFileUtils.OutputFilesFilter()));\r\n    for (Path outFile : fileList) {\r\n        try {\r\n            dis = fileSystem.open(outFile);\r\n            String record;\r\n            while ((record = dis.readLine()) != null) {\r\n                int blankPos = record.indexOf(\" \");\r\n                String keyString = record.substring(0, blankPos);\r\n                String valueString = record.substring(blankPos + 1);\r\n                if (keyString.compareTo(prevKeyValue) >= 0 && keyString.equals(valueString)) {\r\n                    prevKeyValue = keyString;\r\n                    numValidRecords++;\r\n                } else {\r\n                    numInvalidRecords++;\r\n                }\r\n            }\r\n        } finally {\r\n            if (dis != null) {\r\n                dis.close();\r\n                dis = null;\r\n            }\r\n        }\r\n    }\r\n    assertEquals((long) (numMappers * numLines), numValidRecords);\r\n    assertEquals(0, numInvalidRecords);\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getSplits",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "InputSplit[] getSplits(JobConf job, int numSplits) throws IOException\n{\r\n    return new InputSplit[0];\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getRecordReader",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "RecordReader<K, V> getRecordReader(InputSplit split, JobConf job, Reporter reporter) throws IOException\n{\r\n    return new RecordReader<K, V>() {\r\n\r\n        public boolean next(K key, V value) throws IOException {\r\n            return false;\r\n        }\r\n\r\n        public K createKey() {\r\n            return null;\r\n        }\r\n\r\n        public V createValue() {\r\n            return null;\r\n        }\r\n\r\n        public long getPos() throws IOException {\r\n            return 0L;\r\n        }\r\n\r\n        public void close() throws IOException {\r\n        }\r\n\r\n        public float getProgress() throws IOException {\r\n            return 0.0f;\r\n        }\r\n    };\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "testRedirect",
  "errType" : null,
  "containingMethodsNum" : 40,
  "sourceCodeText" : "void testRedirect() throws Exception\n{\r\n    Configuration conf = new YarnConfiguration();\r\n    conf.set(MRConfig.FRAMEWORK_NAME, MRConfig.YARN_FRAMEWORK_NAME);\r\n    conf.set(YarnConfiguration.RM_ADDRESS, RMADDRESS);\r\n    conf.set(JHAdminConfig.MR_HISTORY_ADDRESS, HSHOSTADDRESS);\r\n    RMService rmService = new RMService(\"test\");\r\n    rmService.init(conf);\r\n    rmService.start();\r\n    AMService amService = new AMService();\r\n    amService.init(conf);\r\n    amService.start(conf);\r\n    HistoryService historyService = new HistoryService();\r\n    historyService.init(conf);\r\n    historyService.start(conf);\r\n    LOG.info(\"services started\");\r\n    Cluster cluster = new Cluster(conf);\r\n    org.apache.hadoop.mapreduce.JobID jobID = new org.apache.hadoop.mapred.JobID(\"201103121733\", 1);\r\n    org.apache.hadoop.mapreduce.Counters counters = cluster.getJob(jobID).getCounters();\r\n    validateCounters(counters);\r\n    Assert.assertTrue(amContact);\r\n    LOG.info(\"Sleeping for 5 seconds before stop for\" + \" the client socket to not get EOF immediately..\");\r\n    Thread.sleep(5000);\r\n    amService.stop();\r\n    LOG.info(\"Sleeping for 5 seconds after stop for\" + \" the server to exit cleanly..\");\r\n    Thread.sleep(5000);\r\n    amRestarting = true;\r\n    counters = cluster.getJob(jobID).getCounters();\r\n    Assert.assertEquals(0, counters.countCounters());\r\n    Job job = cluster.getJob(jobID);\r\n    org.apache.hadoop.mapreduce.TaskID taskId = new org.apache.hadoop.mapreduce.TaskID(jobID, TaskType.MAP, 0);\r\n    TaskAttemptID tId = new TaskAttemptID(taskId, 0);\r\n    job.killJob();\r\n    job.killTask(tId);\r\n    job.failTask(tId);\r\n    job.getTaskCompletionEvents(0, 100);\r\n    job.getStatus();\r\n    job.getTaskDiagnostics(tId);\r\n    job.getTaskReports(TaskType.MAP);\r\n    job.getTrackingURL();\r\n    amRestarting = false;\r\n    amService = new AMService();\r\n    amService.init(conf);\r\n    amService.start(conf);\r\n    amContact = false;\r\n    counters = cluster.getJob(jobID).getCounters();\r\n    validateCounters(counters);\r\n    Assert.assertTrue(amContact);\r\n    amService.stop();\r\n    counters = cluster.getJob(jobID).getCounters();\r\n    validateCounters(counters);\r\n    Assert.assertTrue(hsContact);\r\n    rmService.stop();\r\n    historyService.stop();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "validateCounters",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void validateCounters(org.apache.hadoop.mapreduce.Counters counters)\n{\r\n    Iterator<org.apache.hadoop.mapreduce.CounterGroup> it = counters.iterator();\r\n    while (it.hasNext()) {\r\n        org.apache.hadoop.mapreduce.CounterGroup group = it.next();\r\n        LOG.info(\"Group \" + group.getDisplayName());\r\n        Iterator<org.apache.hadoop.mapreduce.Counter> itc = group.iterator();\r\n        while (itc.hasNext()) {\r\n            LOG.info(\"Counter is \" + itc.next().getDisplayName());\r\n        }\r\n    }\r\n    Assert.assertEquals(1, counters.countCounters());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getMyCounters",
  "errType" : null,
  "containingMethodsNum" : 10,
  "sourceCodeText" : "Counters getMyCounters()\n{\r\n    Counter counter = recordFactory.newRecordInstance(Counter.class);\r\n    counter.setName(\"Mycounter\");\r\n    counter.setDisplayName(\"My counter display name\");\r\n    counter.setValue(12345);\r\n    CounterGroup group = recordFactory.newRecordInstance(CounterGroup.class);\r\n    group.setName(\"MyGroup\");\r\n    group.setDisplayName(\"My groupd display name\");\r\n    group.setCounter(\"myCounter\", counter);\r\n    Counters counters = recordFactory.newRecordInstance(Counters.class);\r\n    counters.setCounterGroup(\"myGroupd\", group);\r\n    return counters;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\security",
  "methodName" : "checkSecrets",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void checkSecrets(Credentials ts)\n{\r\n    if (ts == null) {\r\n        throw new RuntimeException(\"The credentials are not available\");\r\n    }\r\n    for (int i = 0; i < NUM_OF_KEYS; i++) {\r\n        String secretName = \"alias\" + i;\r\n        byte[] secretValue = ts.getSecretKey(new Text(secretName));\r\n        if (secretValue == null) {\r\n            throw new RuntimeException(\"The key \" + secretName + \" is not available. \");\r\n        }\r\n        String secretValueStr = new String(secretValue, StandardCharsets.UTF_8);\r\n        System.out.println(secretValueStr);\r\n        if (!(\"password\" + i).equals(secretValueStr)) {\r\n            throw new RuntimeException(\"The key \" + secretName + \" is not correct. Expected value is \" + (\"password\" + i) + \". Actual value is \" + secretValueStr);\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\security",
  "methodName" : "main",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void main(String[] args) throws Exception\n{\r\n    int res = ToolRunner.run(new Configuration(), new CredentialsTestJob(), args);\r\n    System.exit(res);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\security",
  "methodName" : "createJob",
  "errType" : null,
  "containingMethodsNum" : 16,
  "sourceCodeText" : "Job createJob() throws IOException\n{\r\n    Configuration conf = getConf();\r\n    conf.setInt(MRJobConfig.NUM_MAPS, 1);\r\n    Job job = Job.getInstance(conf, \"test\");\r\n    job.setNumReduceTasks(1);\r\n    job.setJarByClass(CredentialsTestJob.class);\r\n    job.setNumReduceTasks(1);\r\n    job.setMapperClass(CredentialsTestJob.CredentialsTestMapper.class);\r\n    job.setMapOutputKeyClass(IntWritable.class);\r\n    job.setMapOutputValueClass(NullWritable.class);\r\n    job.setReducerClass(CredentialsTestJob.CredentialsTestReducer.class);\r\n    job.setInputFormatClass(SleepJob.SleepInputFormat.class);\r\n    job.setPartitionerClass(SleepJob.SleepJobPartitioner.class);\r\n    job.setOutputFormatClass(NullOutputFormat.class);\r\n    job.setSpeculativeExecution(false);\r\n    job.setJobName(\"test job\");\r\n    FileInputFormat.addInputPath(job, new Path(\"ignored\"));\r\n    return job;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\security",
  "methodName" : "run",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "int run(String[] args) throws Exception\n{\r\n    Job job = createJob();\r\n    return job.waitForCompletion(true) ? 0 : 1;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "after",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void after()\n{\r\n    ZlibFactory.loadNativeZLib();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "makeStream",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "LineReader makeStream(String str) throws IOException\n{\r\n    return new LineReader(new ByteArrayInputStream(str.getBytes(\"UTF-8\")), defaultConf);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "writeFile",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void writeFile(FileSystem fs, Path name, CompressionCodec codec, String contents) throws IOException\n{\r\n    OutputStream stm;\r\n    if (codec == null) {\r\n        stm = fs.create(name);\r\n    } else {\r\n        stm = codec.createOutputStream(fs.create(name));\r\n    }\r\n    stm.write(contents.getBytes());\r\n    stm.close();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "readSplit",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "List<Text> readSplit(TextInputFormat format, InputSplit split, JobConf jobConf) throws IOException\n{\r\n    List<Text> result = new ArrayList<Text>();\r\n    RecordReader<LongWritable, Text> reader = format.getRecordReader(split, jobConf, voidReporter);\r\n    LongWritable key = reader.createKey();\r\n    Text value = reader.createValue();\r\n    while (reader.next(key, value)) {\r\n        result.add(value);\r\n        value = reader.createValue();\r\n    }\r\n    reader.close();\r\n    return result;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "testGzip",
  "errType" : null,
  "containingMethodsNum" : 21,
  "sourceCodeText" : "void testGzip() throws IOException\n{\r\n    JobConf jobConf = new JobConf(defaultConf);\r\n    CompressionCodec gzip = new GzipCodec();\r\n    ReflectionUtils.setConf(gzip, jobConf);\r\n    localFs.delete(workDir, true);\r\n    if (org.apache.hadoop.io.compress.zlib.BuiltInGzipDecompressor.class == gzip.getDecompressorType()) {\r\n        System.out.println(COLOR_BR_RED + \"testGzip() using native-zlib Decompressor (\" + gzip.getDecompressorType() + \")\" + COLOR_NORMAL);\r\n    } else {\r\n        LOG.warn(\"testGzip() skipped:  native (C/C++) libs not loaded\");\r\n        return;\r\n    }\r\n    final String fn = \"concat\" + gzip.getDefaultExtension();\r\n    Path fnLocal = new Path(System.getProperty(\"test.concat.data\", DEFAULT_WORK_DIR), fn);\r\n    Path fnHDFS = new Path(workDir, fn);\r\n    localFs.copyFromLocalFile(fnLocal, fnHDFS);\r\n    writeFile(localFs, new Path(workDir, \"part2.txt.gz\"), gzip, \"this is a test\\nof gzip\\n\");\r\n    FileInputFormat.setInputPaths(jobConf, workDir);\r\n    TextInputFormat format = new TextInputFormat();\r\n    format.configure(jobConf);\r\n    InputSplit[] splits = format.getSplits(jobConf, 100);\r\n    assertEquals(\"compressed splits == 2\", 2, splits.length);\r\n    FileSplit tmp = (FileSplit) splits[0];\r\n    if (tmp.getPath().getName().equals(\"part2.txt.gz\")) {\r\n        splits[0] = splits[1];\r\n        splits[1] = tmp;\r\n    }\r\n    List<Text> results = readSplit(format, splits[0], jobConf);\r\n    assertEquals(\"splits[0] num lines\", 6, results.size());\r\n    assertEquals(\"splits[0][5]\", \"member #3\", results.get(5).toString());\r\n    results = readSplit(format, splits[1], jobConf);\r\n    assertEquals(\"splits[1] num lines\", 2, results.size());\r\n    assertEquals(\"splits[1][0]\", \"this is a test\", results.get(0).toString());\r\n    assertEquals(\"splits[1][1]\", \"of gzip\", results.get(1).toString());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "testPrototypeInflaterGzip",
  "errType" : [ "java.util.zip.DataFormatException" ],
  "containingMethodsNum" : 28,
  "sourceCodeText" : "void testPrototypeInflaterGzip() throws IOException\n{\r\n    CompressionCodec gzip = new GzipCodec();\r\n    localFs.delete(workDir, true);\r\n    System.out.println(COLOR_BR_BLUE + \"testPrototypeInflaterGzip() using \" + \"non-native/Java Inflater and manual gzip header/trailer parsing\" + COLOR_NORMAL);\r\n    final String fn = \"concat\" + gzip.getDefaultExtension();\r\n    Path fnLocal = new Path(System.getProperty(\"test.concat.data\", DEFAULT_WORK_DIR), fn);\r\n    Path fnHDFS = new Path(workDir, fn);\r\n    localFs.copyFromLocalFile(fnLocal, fnHDFS);\r\n    final FileInputStream in = new FileInputStream(fnLocal.toString());\r\n    assertEquals(\"concat bytes available\", 148, in.available());\r\n    byte[] compressedBuf = new byte[256];\r\n    int numBytesRead = in.read(compressedBuf, 0, 10);\r\n    assertEquals(\"header bytes read\", 10, numBytesRead);\r\n    assertEquals(\"1st byte\", 0x1f, compressedBuf[0] & 0xff);\r\n    assertEquals(\"2nd byte\", 0x8b, compressedBuf[1] & 0xff);\r\n    assertEquals(\"3rd byte (compression method)\", 8, compressedBuf[2] & 0xff);\r\n    byte flags = (byte) (compressedBuf[3] & 0xff);\r\n    if ((flags & 0x04) != 0) {\r\n        numBytesRead = in.read(compressedBuf, 0, 2);\r\n        assertEquals(\"XLEN bytes read\", 2, numBytesRead);\r\n        int xlen = ((compressedBuf[1] << 8) | compressedBuf[0]) & 0xffff;\r\n        in.skip(xlen);\r\n    }\r\n    if ((flags & 0x08) != 0) {\r\n        while ((numBytesRead = in.read()) != 0) {\r\n            assertFalse(\"unexpected end-of-file while reading filename\", numBytesRead == -1);\r\n        }\r\n    }\r\n    if ((flags & 0x10) != 0) {\r\n        while ((numBytesRead = in.read()) != 0) {\r\n            assertFalse(\"unexpected end-of-file while reading comment\", numBytesRead == -1);\r\n        }\r\n    }\r\n    if ((flags & 0xe0) != 0) {\r\n        assertTrue(\"reserved bits are set??\", (flags & 0xe0) == 0);\r\n    }\r\n    if ((flags & 0x02) != 0) {\r\n        numBytesRead = in.read(compressedBuf, 0, 2);\r\n        assertEquals(\"CRC16 bytes read\", 2, numBytesRead);\r\n        int crc16 = ((compressedBuf[1] << 8) | compressedBuf[0]) & 0xffff;\r\n    }\r\n    numBytesRead = in.read(compressedBuf);\r\n    byte[] uncompressedBuf = new byte[256];\r\n    Inflater inflater = new Inflater(true);\r\n    inflater.setInput(compressedBuf, 0, numBytesRead);\r\n    try {\r\n        int numBytesUncompressed = inflater.inflate(uncompressedBuf);\r\n        String outString = new String(uncompressedBuf, 0, numBytesUncompressed, \"UTF-8\");\r\n        System.out.println(\"uncompressed data of first gzip member = [\" + outString + \"]\");\r\n    } catch (java.util.zip.DataFormatException ex) {\r\n        throw new IOException(ex.getMessage());\r\n    }\r\n    in.close();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "testBuiltInGzipDecompressor",
  "errType" : null,
  "containingMethodsNum" : 23,
  "sourceCodeText" : "void testBuiltInGzipDecompressor() throws IOException\n{\r\n    JobConf jobConf = new JobConf(defaultConf);\r\n    CompressionCodec gzip = new GzipCodec();\r\n    ReflectionUtils.setConf(gzip, jobConf);\r\n    localFs.delete(workDir, true);\r\n    ZlibFactory.setNativeZlibLoaded(false);\r\n    assertEquals(\"[non-native (Java) codec]\", org.apache.hadoop.io.compress.zlib.BuiltInGzipDecompressor.class, gzip.getDecompressorType());\r\n    System.out.println(COLOR_BR_YELLOW + \"testBuiltInGzipDecompressor() using\" + \" non-native (Java Inflater) Decompressor (\" + gzip.getDecompressorType() + \")\" + COLOR_NORMAL);\r\n    String fn1 = \"testConcatThenCompress.txt\" + gzip.getDefaultExtension();\r\n    Path fnLocal1 = new Path(System.getProperty(\"test.concat.data\", DEFAULT_WORK_DIR), fn1);\r\n    Path fnHDFS1 = new Path(workDir, fn1);\r\n    localFs.copyFromLocalFile(fnLocal1, fnHDFS1);\r\n    String fn2 = \"testCompressThenConcat.txt\" + gzip.getDefaultExtension();\r\n    Path fnLocal2 = new Path(System.getProperty(\"test.concat.data\", DEFAULT_WORK_DIR), fn2);\r\n    Path fnHDFS2 = new Path(workDir, fn2);\r\n    localFs.copyFromLocalFile(fnLocal2, fnHDFS2);\r\n    FileInputFormat.setInputPaths(jobConf, workDir);\r\n    final FileInputStream in1 = new FileInputStream(fnLocal1.toString());\r\n    final FileInputStream in2 = new FileInputStream(fnLocal2.toString());\r\n    assertEquals(\"concat bytes available\", 2734, in1.available());\r\n    assertEquals(\"concat bytes available\", 3413, in2.available());\r\n    CompressionInputStream cin2 = gzip.createInputStream(in2);\r\n    LineReader in = new LineReader(cin2);\r\n    Text out = new Text();\r\n    int numBytes, totalBytes = 0, lineNum = 0;\r\n    while ((numBytes = in.readLine(out)) > 0) {\r\n        ++lineNum;\r\n        totalBytes += numBytes;\r\n    }\r\n    in.close();\r\n    assertEquals(\"total uncompressed bytes in concatenated test file\", 5346, totalBytes);\r\n    assertEquals(\"total uncompressed lines in concatenated test file\", 84, lineNum);\r\n    ZlibFactory.loadNativeZLib();\r\n    doMultipleGzipBufferSizes(jobConf, true);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "doMultipleGzipBufferSizes",
  "errType" : null,
  "containingMethodsNum" : 23,
  "sourceCodeText" : "void doMultipleGzipBufferSizes(JobConf jConf, boolean useNative) throws IOException\n{\r\n    System.out.println(COLOR_YELLOW + \"doMultipleGzipBufferSizes() using \" + (useNative ? \"GzipZlibDecompressor\" : \"BuiltInGzipDecompressor\") + COLOR_NORMAL);\r\n    int bufferSize;\r\n    for (bufferSize = 1; bufferSize < 34; ++bufferSize) {\r\n        jConf.setInt(\"io.file.buffer.size\", bufferSize);\r\n        doSingleGzipBufferSize(jConf);\r\n    }\r\n    bufferSize = 512;\r\n    jConf.setInt(\"io.file.buffer.size\", bufferSize);\r\n    doSingleGzipBufferSize(jConf);\r\n    bufferSize = 1024;\r\n    jConf.setInt(\"io.file.buffer.size\", bufferSize);\r\n    doSingleGzipBufferSize(jConf);\r\n    bufferSize = 2 * 1024;\r\n    jConf.setInt(\"io.file.buffer.size\", bufferSize);\r\n    doSingleGzipBufferSize(jConf);\r\n    bufferSize = 4 * 1024;\r\n    jConf.setInt(\"io.file.buffer.size\", bufferSize);\r\n    doSingleGzipBufferSize(jConf);\r\n    bufferSize = 63 * 1024;\r\n    jConf.setInt(\"io.file.buffer.size\", bufferSize);\r\n    doSingleGzipBufferSize(jConf);\r\n    bufferSize = 64 * 1024;\r\n    jConf.setInt(\"io.file.buffer.size\", bufferSize);\r\n    doSingleGzipBufferSize(jConf);\r\n    bufferSize = 65 * 1024;\r\n    jConf.setInt(\"io.file.buffer.size\", bufferSize);\r\n    doSingleGzipBufferSize(jConf);\r\n    bufferSize = 127 * 1024;\r\n    jConf.setInt(\"io.file.buffer.size\", bufferSize);\r\n    doSingleGzipBufferSize(jConf);\r\n    bufferSize = 128 * 1024;\r\n    jConf.setInt(\"io.file.buffer.size\", bufferSize);\r\n    doSingleGzipBufferSize(jConf);\r\n    bufferSize = 129 * 1024;\r\n    jConf.setInt(\"io.file.buffer.size\", bufferSize);\r\n    doSingleGzipBufferSize(jConf);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "doSingleGzipBufferSize",
  "errType" : null,
  "containingMethodsNum" : 13,
  "sourceCodeText" : "void doSingleGzipBufferSize(JobConf jConf) throws IOException\n{\r\n    TextInputFormat format = new TextInputFormat();\r\n    format.configure(jConf);\r\n    InputSplit[] splits = format.getSplits(jConf, 100);\r\n    assertEquals(\"compressed splits == 2\", 2, splits.length);\r\n    FileSplit tmp = (FileSplit) splits[0];\r\n    if (tmp.getPath().getName().equals(\"testdata/testCompressThenConcat.txt.gz\")) {\r\n        System.out.println(\"  (swapping)\");\r\n        splits[0] = splits[1];\r\n        splits[1] = tmp;\r\n    }\r\n    List<Text> results = readSplit(format, splits[0], jConf);\r\n    assertEquals(\"splits[0] length (num lines)\", 84, results.size());\r\n    assertEquals(\"splits[0][0]\", \"Call me Ishmael. Some years ago--never mind how long precisely--having\", results.get(0).toString());\r\n    assertEquals(\"splits[0][42]\", \"Tell me, does the magnetic virtue of the needles of the compasses of\", results.get(42).toString());\r\n    results = readSplit(format, splits[1], jConf);\r\n    assertEquals(\"splits[1] length (num lines)\", 84, results.size());\r\n    assertEquals(\"splits[1][0]\", \"Call me Ishmael. Some years ago--never mind how long precisely--having\", results.get(0).toString());\r\n    assertEquals(\"splits[1][42]\", \"Tell me, does the magnetic virtue of the needles of the compasses of\", results.get(42).toString());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "testBzip2",
  "errType" : null,
  "containingMethodsNum" : 20,
  "sourceCodeText" : "void testBzip2() throws IOException\n{\r\n    JobConf jobConf = new JobConf(defaultConf);\r\n    CompressionCodec bzip2 = new BZip2Codec();\r\n    ReflectionUtils.setConf(bzip2, jobConf);\r\n    localFs.delete(workDir, true);\r\n    System.out.println(COLOR_BR_CYAN + \"testBzip2() using non-native CBZip2InputStream (presumably)\" + COLOR_NORMAL);\r\n    final String fn = \"concat\" + bzip2.getDefaultExtension();\r\n    Path fnLocal = new Path(System.getProperty(\"test.concat.data\", DEFAULT_WORK_DIR), fn);\r\n    Path fnHDFS = new Path(workDir, fn);\r\n    localFs.copyFromLocalFile(fnLocal, fnHDFS);\r\n    writeFile(localFs, new Path(workDir, \"part2.txt.bz2\"), bzip2, \"this is a test\\nof bzip2\\n\");\r\n    FileInputFormat.setInputPaths(jobConf, workDir);\r\n    TextInputFormat format = new TextInputFormat();\r\n    format.configure(jobConf);\r\n    format.setMinSplitSize(256);\r\n    InputSplit[] splits = format.getSplits(jobConf, 100);\r\n    assertEquals(\"compressed splits == 2\", 2, splits.length);\r\n    FileSplit tmp = (FileSplit) splits[0];\r\n    if (tmp.getPath().getName().equals(\"part2.txt.bz2\")) {\r\n        splits[0] = splits[1];\r\n        splits[1] = tmp;\r\n    }\r\n    List<Text> results = readSplit(format, splits[0], jobConf);\r\n    assertEquals(\"splits[0] num lines\", 6, results.size());\r\n    assertEquals(\"splits[0][5]\", \"member #3\", results.get(5).toString());\r\n    results = readSplit(format, splits[1], jobConf);\r\n    assertEquals(\"splits[1] num lines\", 2, results.size());\r\n    assertEquals(\"splits[1][0]\", \"this is a test\", results.get(0).toString());\r\n    assertEquals(\"splits[1][1]\", \"of bzip2\", results.get(1).toString());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "testMoreBzip2",
  "errType" : null,
  "containingMethodsNum" : 20,
  "sourceCodeText" : "void testMoreBzip2() throws IOException\n{\r\n    JobConf jobConf = new JobConf(defaultConf);\r\n    CompressionCodec bzip2 = new BZip2Codec();\r\n    ReflectionUtils.setConf(bzip2, jobConf);\r\n    localFs.delete(workDir, true);\r\n    System.out.println(COLOR_BR_MAGENTA + \"testMoreBzip2() using non-native CBZip2InputStream (presumably)\" + COLOR_NORMAL);\r\n    String fn1 = \"testConcatThenCompress.txt\" + bzip2.getDefaultExtension();\r\n    Path fnLocal1 = new Path(System.getProperty(\"test.concat.data\", DEFAULT_WORK_DIR), fn1);\r\n    Path fnHDFS1 = new Path(workDir, fn1);\r\n    localFs.copyFromLocalFile(fnLocal1, fnHDFS1);\r\n    String fn2 = \"testCompressThenConcat.txt\" + bzip2.getDefaultExtension();\r\n    Path fnLocal2 = new Path(System.getProperty(\"test.concat.data\", DEFAULT_WORK_DIR), fn2);\r\n    Path fnHDFS2 = new Path(workDir, fn2);\r\n    localFs.copyFromLocalFile(fnLocal2, fnHDFS2);\r\n    FileInputFormat.setInputPaths(jobConf, workDir);\r\n    final FileInputStream in1 = new FileInputStream(fnLocal1.toString());\r\n    final FileInputStream in2 = new FileInputStream(fnLocal2.toString());\r\n    assertEquals(\"concat bytes available\", 2567, in1.available());\r\n    assertEquals(\"concat bytes available\", 3056, in2.available());\r\n    CompressionInputStream cin2 = bzip2.createInputStream(in2);\r\n    LineReader in = new LineReader(cin2);\r\n    Text out = new Text();\r\n    int numBytes, totalBytes = 0, lineNum = 0;\r\n    while ((numBytes = in.readLine(out)) > 0) {\r\n        ++lineNum;\r\n        totalBytes += numBytes;\r\n    }\r\n    in.close();\r\n    assertEquals(\"total uncompressed bytes in concatenated test file\", 5346, totalBytes);\r\n    assertEquals(\"total uncompressed lines in concatenated test file\", 84, lineNum);\r\n    doMultipleBzip2BufferSizes(jobConf);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "doMultipleBzip2BufferSizes",
  "errType" : null,
  "containingMethodsNum" : 23,
  "sourceCodeText" : "void doMultipleBzip2BufferSizes(JobConf jConf) throws IOException\n{\r\n    System.out.println(COLOR_MAGENTA + \"doMultipleBzip2BufferSizes() using \" + \"default bzip2 decompressor\" + COLOR_NORMAL);\r\n    int bufferSize;\r\n    for (bufferSize = 1; bufferSize < 34; ++bufferSize) {\r\n        jConf.setInt(\"io.file.buffer.size\", bufferSize);\r\n        doSingleBzip2BufferSize(jConf);\r\n    }\r\n    bufferSize = 512;\r\n    jConf.setInt(\"io.file.buffer.size\", bufferSize);\r\n    doSingleBzip2BufferSize(jConf);\r\n    bufferSize = 1024;\r\n    jConf.setInt(\"io.file.buffer.size\", bufferSize);\r\n    doSingleBzip2BufferSize(jConf);\r\n    bufferSize = 2 * 1024;\r\n    jConf.setInt(\"io.file.buffer.size\", bufferSize);\r\n    doSingleBzip2BufferSize(jConf);\r\n    bufferSize = 4 * 1024;\r\n    jConf.setInt(\"io.file.buffer.size\", bufferSize);\r\n    doSingleBzip2BufferSize(jConf);\r\n    bufferSize = 63 * 1024;\r\n    jConf.setInt(\"io.file.buffer.size\", bufferSize);\r\n    doSingleBzip2BufferSize(jConf);\r\n    bufferSize = 64 * 1024;\r\n    jConf.setInt(\"io.file.buffer.size\", bufferSize);\r\n    doSingleBzip2BufferSize(jConf);\r\n    bufferSize = 65 * 1024;\r\n    jConf.setInt(\"io.file.buffer.size\", bufferSize);\r\n    doSingleBzip2BufferSize(jConf);\r\n    bufferSize = 127 * 1024;\r\n    jConf.setInt(\"io.file.buffer.size\", bufferSize);\r\n    doSingleBzip2BufferSize(jConf);\r\n    bufferSize = 128 * 1024;\r\n    jConf.setInt(\"io.file.buffer.size\", bufferSize);\r\n    doSingleBzip2BufferSize(jConf);\r\n    bufferSize = 129 * 1024;\r\n    jConf.setInt(\"io.file.buffer.size\", bufferSize);\r\n    doSingleBzip2BufferSize(jConf);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "doSingleBzip2BufferSize",
  "errType" : null,
  "containingMethodsNum" : 14,
  "sourceCodeText" : "void doSingleBzip2BufferSize(JobConf jConf) throws IOException\n{\r\n    TextInputFormat format = new TextInputFormat();\r\n    format.configure(jConf);\r\n    format.setMinSplitSize(5500);\r\n    InputSplit[] splits = format.getSplits(jConf, 100);\r\n    assertEquals(\"compressed splits == 2\", 2, splits.length);\r\n    FileSplit tmp = (FileSplit) splits[0];\r\n    if (tmp.getPath().getName().equals(\"testdata/testCompressThenConcat.txt.gz\")) {\r\n        System.out.println(\"  (swapping)\");\r\n        splits[0] = splits[1];\r\n        splits[1] = tmp;\r\n    }\r\n    List<Text> results = readSplit(format, splits[0], jConf);\r\n    assertEquals(\"splits[0] length (num lines)\", 84, results.size());\r\n    assertEquals(\"splits[0][0]\", \"Call me Ishmael. Some years ago--never mind how long precisely--having\", results.get(0).toString());\r\n    assertEquals(\"splits[0][42]\", \"Tell me, does the magnetic virtue of the needles of the compasses of\", results.get(42).toString());\r\n    results = readSplit(format, splits[1], jConf);\r\n    assertEquals(\"splits[1] length (num lines)\", 84, results.size());\r\n    assertEquals(\"splits[1][0]\", \"Call me Ishmael. Some years ago--never mind how long precisely--having\", results.get(0).toString());\r\n    assertEquals(\"splits[1][42]\", \"Tell me, does the magnetic virtue of the needles of the compasses of\", results.get(42).toString());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "unquote",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "String unquote(String in)\n{\r\n    StringBuffer result = new StringBuffer();\r\n    for (int i = 0; i < in.length(); ++i) {\r\n        char ch = in.charAt(i);\r\n        if (ch == '\\\\') {\r\n            ch = in.charAt(++i);\r\n            switch(ch) {\r\n                case 'n':\r\n                    result.append('\\n');\r\n                    break;\r\n                case 'r':\r\n                    result.append('\\r');\r\n                    break;\r\n                default:\r\n                    result.append(ch);\r\n                    break;\r\n            }\r\n        } else {\r\n            result.append(ch);\r\n        }\r\n    }\r\n    return result.toString();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "main",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void main(String[] args) throws Exception\n{\r\n    for (String arg : args) {\r\n        System.out.println(\"Working on \" + arg);\r\n        LineReader reader = makeStream(unquote(arg));\r\n        Text line = new Text();\r\n        int size = reader.readLine(line);\r\n        while (size > 0) {\r\n            System.out.println(\"Got: \" + line.toString());\r\n            size = reader.readLine(line);\r\n        }\r\n        reader.close();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "submitAndValidateJob",
  "errType" : null,
  "containingMethodsNum" : 13,
  "sourceCodeText" : "Job submitAndValidateJob(Configuration conf, int numMaps, int numReds) throws IOException, InterruptedException, ClassNotFoundException\n{\r\n    Job job = MapReduceTestUtil.createJob(conf, inDir, outDir, numMaps, numReds);\r\n    job.setJobSetupCleanupNeeded(false);\r\n    job.setOutputFormatClass(MyOutputFormat.class);\r\n    job.waitForCompletion(true);\r\n    assertTrue(job.isSuccessful());\r\n    assertTrue(job.getTaskReports(TaskType.JOB_SETUP).length == 0);\r\n    assertTrue(job.getTaskReports(TaskType.JOB_CLEANUP).length == 0);\r\n    assertTrue(job.getTaskReports(TaskType.MAP).length == numMaps);\r\n    assertTrue(job.getTaskReports(TaskType.REDUCE).length == numReds);\r\n    FileSystem fs = FileSystem.get(conf);\r\n    assertTrue(\"Job output directory doesn't exit!\", fs.exists(outDir));\r\n    String tempWorkingPathStr = outDir + Path.SEPARATOR + \"_temporary\" + Path.SEPARATOR + \"0\";\r\n    Path tempWorkingPath = new Path(tempWorkingPathStr);\r\n    FileStatus[] list = fs.listStatus(tempWorkingPath, new OutputFilter());\r\n    int numPartFiles = numReds == 0 ? numMaps : numReds;\r\n    assertTrue(\"Number of part-files is \" + list.length + \" and not \" + numPartFiles, list.length == numPartFiles);\r\n    return job;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "testNoJobSetupCleanup",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void testNoJobSetupCleanup() throws Exception\n{\r\n    try {\r\n        Configuration conf = createJobConf();\r\n        submitAndValidateJob(conf, 1, 1);\r\n        submitAndValidateJob(conf, 1, 0);\r\n        submitAndValidateJob(conf, 0, 0);\r\n        submitAndValidateJob(conf, 0, 1);\r\n    } finally {\r\n        tearDown();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\fieldsel",
  "methodName" : "testFieldSelection",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testFieldSelection() throws Exception\n{\r\n    launch();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\fieldsel",
  "methodName" : "launch",
  "errType" : null,
  "containingMethodsNum" : 16,
  "sourceCodeText" : "void launch() throws Exception\n{\r\n    Configuration conf = new Configuration();\r\n    FileSystem fs = FileSystem.get(conf);\r\n    int numOfInputLines = 10;\r\n    Path outDir = new Path(testDir, \"output_for_field_selection_test\");\r\n    Path inDir = new Path(testDir, \"input_for_field_selection_test\");\r\n    StringBuffer inputData = new StringBuffer();\r\n    StringBuffer expectedOutput = new StringBuffer();\r\n    constructInputOutputData(inputData, expectedOutput, numOfInputLines);\r\n    conf.set(FieldSelectionHelper.DATA_FIELD_SEPARATOR, \"-\");\r\n    conf.set(FieldSelectionHelper.MAP_OUTPUT_KEY_VALUE_SPEC, \"6,5,1-3:0-\");\r\n    conf.set(FieldSelectionHelper.REDUCE_OUTPUT_KEY_VALUE_SPEC, \":4,3,2,1,0,0-\");\r\n    Job job = MapReduceTestUtil.createJob(conf, inDir, outDir, 1, 1, inputData.toString());\r\n    job.setMapperClass(FieldSelectionMapper.class);\r\n    job.setReducerClass(FieldSelectionReducer.class);\r\n    job.setOutputKeyClass(Text.class);\r\n    job.setOutputValueClass(Text.class);\r\n    job.setNumReduceTasks(1);\r\n    job.waitForCompletion(true);\r\n    assertTrue(\"Job Failed!\", job.isSuccessful());\r\n    String outdata = MapReduceTestUtil.readOutput(outDir, conf);\r\n    assertEquals(\"Outputs doesnt match.\", expectedOutput.toString(), outdata);\r\n    fs.delete(outDir, true);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\fieldsel",
  "methodName" : "constructInputOutputData",
  "errType" : null,
  "containingMethodsNum" : 30,
  "sourceCodeText" : "void constructInputOutputData(StringBuffer inputData, StringBuffer expectedOutput, int numOfInputLines)\n{\r\n    for (int i = 0; i < numOfInputLines; i++) {\r\n        inputData.append(idFormat.format(i));\r\n        inputData.append(\"-\").append(idFormat.format(i + 1));\r\n        inputData.append(\"-\").append(idFormat.format(i + 2));\r\n        inputData.append(\"-\").append(idFormat.format(i + 3));\r\n        inputData.append(\"-\").append(idFormat.format(i + 4));\r\n        inputData.append(\"-\").append(idFormat.format(i + 5));\r\n        inputData.append(\"-\").append(idFormat.format(i + 6));\r\n        inputData.append(\"\\n\");\r\n        expectedOutput.append(idFormat.format(i + 3));\r\n        expectedOutput.append(\"-\").append(idFormat.format(i + 2));\r\n        expectedOutput.append(\"-\").append(idFormat.format(i + 1));\r\n        expectedOutput.append(\"-\").append(idFormat.format(i + 5));\r\n        expectedOutput.append(\"-\").append(idFormat.format(i + 6));\r\n        expectedOutput.append(\"-\").append(idFormat.format(i + 6));\r\n        expectedOutput.append(\"-\").append(idFormat.format(i + 5));\r\n        expectedOutput.append(\"-\").append(idFormat.format(i + 1));\r\n        expectedOutput.append(\"-\").append(idFormat.format(i + 2));\r\n        expectedOutput.append(\"-\").append(idFormat.format(i + 3));\r\n        expectedOutput.append(\"-\").append(idFormat.format(i + 0));\r\n        expectedOutput.append(\"-\").append(idFormat.format(i + 1));\r\n        expectedOutput.append(\"-\").append(idFormat.format(i + 2));\r\n        expectedOutput.append(\"-\").append(idFormat.format(i + 3));\r\n        expectedOutput.append(\"-\").append(idFormat.format(i + 4));\r\n        expectedOutput.append(\"-\").append(idFormat.format(i + 5));\r\n        expectedOutput.append(\"-\").append(idFormat.format(i + 6));\r\n        expectedOutput.append(\"\\n\");\r\n    }\r\n    System.out.println(\"inputData:\");\r\n    System.out.println(inputData.toString());\r\n    System.out.println(\"ExpectedData:\");\r\n    System.out.println(expectedOutput.toString());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "createTimelineEntities",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "List<TimelineEntity> createTimelineEntities(JobInfo jobInfo, Configuration conf)\n{\r\n    List<TimelineEntity> entities = new ArrayList<>();\r\n    TimelineEntity job = createJobEntity(jobInfo, conf);\r\n    entities.add(job);\r\n    List<TimelineEntity> tasksAndAttempts = createTaskAndTaskAttemptEntities(jobInfo);\r\n    entities.addAll(tasksAndAttempts);\r\n    return entities;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "createJobEntity",
  "errType" : null,
  "containingMethodsNum" : 19,
  "sourceCodeText" : "TimelineEntity createJobEntity(JobInfo jobInfo, Configuration conf)\n{\r\n    TimelineEntity job = new TimelineEntity();\r\n    job.setType(JOB);\r\n    job.setId(jobInfo.getJobId().toString());\r\n    job.setCreatedTime(jobInfo.getSubmitTime());\r\n    job.addInfo(\"JOBNAME\", jobInfo.getJobname());\r\n    job.addInfo(\"USERNAME\", jobInfo.getUsername());\r\n    job.addInfo(\"JOB_QUEUE_NAME\", jobInfo.getJobQueueName());\r\n    job.addInfo(\"SUBMIT_TIME\", jobInfo.getSubmitTime());\r\n    job.addInfo(\"LAUNCH_TIME\", jobInfo.getLaunchTime());\r\n    job.addInfo(\"FINISH_TIME\", jobInfo.getFinishTime());\r\n    job.addInfo(\"JOB_STATUS\", jobInfo.getJobStatus());\r\n    job.addInfo(\"PRIORITY\", jobInfo.getPriority());\r\n    job.addInfo(\"TOTAL_MAPS\", jobInfo.getTotalMaps());\r\n    job.addInfo(\"TOTAL_REDUCES\", jobInfo.getTotalReduces());\r\n    job.addInfo(\"UBERIZED\", jobInfo.getUberized());\r\n    job.addInfo(\"ERROR_INFO\", jobInfo.getErrorInfo());\r\n    Counters totalCounters = jobInfo.getTotalCounters();\r\n    if (totalCounters != null) {\r\n        addMetrics(job, totalCounters);\r\n    }\r\n    addConfiguration(job, conf);\r\n    LOG.info(\"converted job \" + jobInfo.getJobId() + \" to a timeline entity\");\r\n    return job;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "addConfiguration",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void addConfiguration(TimelineEntity job, Configuration conf)\n{\r\n    for (Map.Entry<String, String> e : conf) {\r\n        job.addConfig(e.getKey(), e.getValue());\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "addMetrics",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void addMetrics(TimelineEntity entity, Counters counters)\n{\r\n    for (CounterGroup g : counters) {\r\n        String groupName = g.getName();\r\n        for (Counter c : g) {\r\n            String name = groupName + \":\" + c.getName();\r\n            TimelineMetric metric = new TimelineMetric();\r\n            metric.setId(name);\r\n            metric.addValue(System.currentTimeMillis(), c.getValue());\r\n            entity.addMetric(metric);\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "createTaskAndTaskAttemptEntities",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "List<TimelineEntity> createTaskAndTaskAttemptEntities(JobInfo jobInfo)\n{\r\n    List<TimelineEntity> entities = new ArrayList<>();\r\n    Map<TaskID, TaskInfo> taskInfoMap = jobInfo.getAllTasks();\r\n    LOG.info(\"job \" + jobInfo.getJobId() + \" has \" + taskInfoMap.size() + \" tasks\");\r\n    for (TaskInfo taskInfo : taskInfoMap.values()) {\r\n        TimelineEntity task = createTaskEntity(taskInfo);\r\n        entities.add(task);\r\n        Set<TimelineEntity> taskAttempts = createTaskAttemptEntities(taskInfo);\r\n        entities.addAll(taskAttempts);\r\n    }\r\n    return entities;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "createTaskEntity",
  "errType" : null,
  "containingMethodsNum" : 11,
  "sourceCodeText" : "TimelineEntity createTaskEntity(TaskInfo taskInfo)\n{\r\n    TimelineEntity task = new TimelineEntity();\r\n    task.setType(TASK);\r\n    task.setId(taskInfo.getTaskId().toString());\r\n    task.setCreatedTime(taskInfo.getStartTime());\r\n    task.addInfo(\"START_TIME\", taskInfo.getStartTime());\r\n    task.addInfo(\"FINISH_TIME\", taskInfo.getFinishTime());\r\n    task.addInfo(\"TASK_TYPE\", taskInfo.getTaskType());\r\n    task.addInfo(\"TASK_STATUS\", taskInfo.getTaskStatus());\r\n    task.addInfo(\"ERROR_INFO\", taskInfo.getError());\r\n    Counters counters = taskInfo.getCounters();\r\n    if (counters != null) {\r\n        addMetrics(task, counters);\r\n    }\r\n    LOG.info(\"converted task \" + taskInfo.getTaskId() + \" to a timeline entity\");\r\n    return task;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "createTaskAttemptEntities",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "Set<TimelineEntity> createTaskAttemptEntities(TaskInfo taskInfo)\n{\r\n    Set<TimelineEntity> taskAttempts = new HashSet<TimelineEntity>();\r\n    Map<TaskAttemptID, TaskAttemptInfo> taskAttemptInfoMap = taskInfo.getAllTaskAttempts();\r\n    LOG.info(\"task \" + taskInfo.getTaskId() + \" has \" + taskAttemptInfoMap.size() + \" task attempts\");\r\n    for (TaskAttemptInfo taskAttemptInfo : taskAttemptInfoMap.values()) {\r\n        TimelineEntity taskAttempt = createTaskAttemptEntity(taskAttemptInfo);\r\n        taskAttempts.add(taskAttempt);\r\n    }\r\n    return taskAttempts;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "createTaskAttemptEntity",
  "errType" : null,
  "containingMethodsNum" : 15,
  "sourceCodeText" : "TimelineEntity createTaskAttemptEntity(TaskAttemptInfo taskAttemptInfo)\n{\r\n    TimelineEntity taskAttempt = new TimelineEntity();\r\n    taskAttempt.setType(TASK_ATTEMPT);\r\n    taskAttempt.setId(taskAttemptInfo.getAttemptId().toString());\r\n    taskAttempt.setCreatedTime(taskAttemptInfo.getStartTime());\r\n    taskAttempt.addInfo(\"START_TIME\", taskAttemptInfo.getStartTime());\r\n    taskAttempt.addInfo(\"FINISH_TIME\", taskAttemptInfo.getFinishTime());\r\n    taskAttempt.addInfo(\"MAP_FINISH_TIME\", taskAttemptInfo.getMapFinishTime());\r\n    taskAttempt.addInfo(\"SHUFFLE_FINISH_TIME\", taskAttemptInfo.getShuffleFinishTime());\r\n    taskAttempt.addInfo(\"SORT_FINISH_TIME\", taskAttemptInfo.getSortFinishTime());\r\n    taskAttempt.addInfo(\"TASK_STATUS\", taskAttemptInfo.getTaskStatus());\r\n    taskAttempt.addInfo(\"STATE\", taskAttemptInfo.getState());\r\n    taskAttempt.addInfo(\"ERROR\", taskAttemptInfo.getError());\r\n    taskAttempt.addInfo(\"CONTAINER_ID\", taskAttemptInfo.getContainerId().toString());\r\n    Counters counters = taskAttemptInfo.getCounters();\r\n    if (counters != null) {\r\n        addMetrics(taskAttempt, counters);\r\n    }\r\n    LOG.info(\"converted task attempt \" + taskAttemptInfo.getAttemptId() + \" to a timeline entity\");\r\n    return taskAttempt;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2",
  "methodName" : "setup",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void setup() throws IOException\n{\r\n    if (!(new File(MiniMRYarnCluster.APPJAR)).exists()) {\r\n        LOG.info(\"MRAppJar \" + MiniMRYarnCluster.APPJAR + \" not found. Not running test.\");\r\n        return;\r\n    }\r\n    if (mrCluster == null) {\r\n        mrCluster = new MiniMRYarnCluster(TestMROldApiJobs.class.getName());\r\n        mrCluster.init(new Configuration());\r\n        mrCluster.start();\r\n    }\r\n    mrCluster.getConfig().setBoolean(MRJobConfig.JOB_UBERTASK_ENABLE, false);\r\n    localFs.copyFromLocalFile(new Path(MiniMRYarnCluster.APPJAR), TestMRJobs.APP_JAR);\r\n    localFs.setPermission(TestMRJobs.APP_JAR, new FsPermission(\"700\"));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2",
  "methodName" : "tearDown",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void tearDown()\n{\r\n    if (mrCluster != null) {\r\n        mrCluster.stop();\r\n        mrCluster = null;\r\n    }\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2",
  "methodName" : "testJobSucceed",
  "errType" : null,
  "containingMethodsNum" : 14,
  "sourceCodeText" : "void testJobSucceed() throws IOException, InterruptedException, ClassNotFoundException\n{\r\n    LOG.info(\"\\n\\n\\nStarting testJobSucceed().\");\r\n    if (!(new File(MiniMRYarnCluster.APPJAR)).exists()) {\r\n        LOG.info(\"MRAppJar \" + MiniMRYarnCluster.APPJAR + \" not found. Not running test.\");\r\n        return;\r\n    }\r\n    JobConf conf = new JobConf(mrCluster.getConfig());\r\n    Path in = new Path(mrCluster.getTestWorkDir().getAbsolutePath(), \"in\");\r\n    Path out = new Path(mrCluster.getTestWorkDir().getAbsolutePath(), \"out\");\r\n    runJobSucceed(conf, in, out);\r\n    FileSystem fs = FileSystem.get(conf);\r\n    Assert.assertTrue(fs.exists(new Path(out, CustomOutputCommitter.JOB_SETUP_FILE_NAME)));\r\n    Assert.assertFalse(fs.exists(new Path(out, CustomOutputCommitter.JOB_ABORT_FILE_NAME)));\r\n    Assert.assertTrue(fs.exists(new Path(out, CustomOutputCommitter.JOB_COMMIT_FILE_NAME)));\r\n    Assert.assertTrue(fs.exists(new Path(out, CustomOutputCommitter.TASK_SETUP_FILE_NAME)));\r\n    Assert.assertFalse(fs.exists(new Path(out, CustomOutputCommitter.TASK_ABORT_FILE_NAME)));\r\n    Assert.assertTrue(fs.exists(new Path(out, CustomOutputCommitter.TASK_COMMIT_FILE_NAME)));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2",
  "methodName" : "testJobFail",
  "errType" : null,
  "containingMethodsNum" : 14,
  "sourceCodeText" : "void testJobFail() throws IOException, InterruptedException, ClassNotFoundException\n{\r\n    LOG.info(\"\\n\\n\\nStarting testJobFail().\");\r\n    if (!(new File(MiniMRYarnCluster.APPJAR)).exists()) {\r\n        LOG.info(\"MRAppJar \" + MiniMRYarnCluster.APPJAR + \" not found. Not running test.\");\r\n        return;\r\n    }\r\n    JobConf conf = new JobConf(mrCluster.getConfig());\r\n    Path in = new Path(mrCluster.getTestWorkDir().getAbsolutePath(), \"fail-in\");\r\n    Path out = new Path(mrCluster.getTestWorkDir().getAbsolutePath(), \"fail-out\");\r\n    runJobFail(conf, in, out);\r\n    FileSystem fs = FileSystem.get(conf);\r\n    Assert.assertTrue(fs.exists(new Path(out, CustomOutputCommitter.JOB_SETUP_FILE_NAME)));\r\n    Assert.assertTrue(fs.exists(new Path(out, CustomOutputCommitter.JOB_ABORT_FILE_NAME)));\r\n    Assert.assertFalse(fs.exists(new Path(out, CustomOutputCommitter.JOB_COMMIT_FILE_NAME)));\r\n    Assert.assertTrue(fs.exists(new Path(out, CustomOutputCommitter.TASK_SETUP_FILE_NAME)));\r\n    Assert.assertTrue(fs.exists(new Path(out, CustomOutputCommitter.TASK_ABORT_FILE_NAME)));\r\n    Assert.assertFalse(fs.exists(new Path(out, CustomOutputCommitter.TASK_COMMIT_FILE_NAME)));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2",
  "methodName" : "runJobFail",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void runJobFail(JobConf conf, Path inDir, Path outDir) throws IOException, InterruptedException\n{\r\n    conf.setJobName(\"test-job-fail\");\r\n    conf.setMapperClass(FailMapper.class);\r\n    conf.setJarByClass(FailMapper.class);\r\n    conf.setReducerClass(IdentityReducer.class);\r\n    conf.setMaxMapAttempts(1);\r\n    boolean success = runJob(conf, inDir, outDir, 1, 0);\r\n    Assert.assertFalse(\"Job expected to fail succeeded\", success);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2",
  "methodName" : "runJobSucceed",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void runJobSucceed(JobConf conf, Path inDir, Path outDir) throws IOException, InterruptedException\n{\r\n    conf.setJobName(\"test-job-succeed\");\r\n    conf.setMapperClass(IdentityMapper.class);\r\n    conf.setReducerClass(IdentityReducer.class);\r\n    boolean success = runJob(conf, inDir, outDir, 1, 1);\r\n    Assert.assertTrue(\"Job expected to succeed failed\", success);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2",
  "methodName" : "runJob",
  "errType" : null,
  "containingMethodsNum" : 19,
  "sourceCodeText" : "boolean runJob(JobConf conf, Path inDir, Path outDir, int numMaps, int numReds) throws IOException, InterruptedException\n{\r\n    FileSystem fs = FileSystem.get(conf);\r\n    if (fs.exists(outDir)) {\r\n        fs.delete(outDir, true);\r\n    }\r\n    if (!fs.exists(inDir)) {\r\n        fs.mkdirs(inDir);\r\n    }\r\n    String input = \"The quick brown fox\\n\" + \"has many silly\\n\" + \"red fox sox\\n\";\r\n    for (int i = 0; i < numMaps; ++i) {\r\n        DataOutputStream file = fs.create(new Path(inDir, \"part-\" + i));\r\n        file.writeBytes(input);\r\n        file.close();\r\n    }\r\n    DistributedCache.addFileToClassPath(TestMRJobs.APP_JAR, conf, fs);\r\n    conf.setOutputCommitter(CustomOutputCommitter.class);\r\n    conf.setInputFormat(TextInputFormat.class);\r\n    conf.setOutputKeyClass(LongWritable.class);\r\n    conf.setOutputValueClass(Text.class);\r\n    FileInputFormat.setInputPaths(conf, inDir);\r\n    FileOutputFormat.setOutputPath(conf, outDir);\r\n    conf.setNumMapTasks(numMaps);\r\n    conf.setNumReduceTasks(numReds);\r\n    JobClient jobClient = new JobClient(conf);\r\n    RunningJob job = jobClient.submitJob(conf);\r\n    return jobClient.monitorAndPrintJob(conf, job);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "dfmt",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String dfmt(double d)\n{\r\n    return dfm.format(d);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "ifmt",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String ifmt(double d)\n{\r\n    return ifm.format(d);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "formatBytes",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "String formatBytes(long numBytes)\n{\r\n    StringBuffer buf = new StringBuffer();\r\n    boolean bDetails = true;\r\n    double num = numBytes;\r\n    if (numBytes < KB) {\r\n        buf.append(numBytes + \" B\");\r\n        bDetails = false;\r\n    } else if (numBytes < MB) {\r\n        buf.append(dfmt(num / KB) + \" KB\");\r\n    } else if (numBytes < GB) {\r\n        buf.append(dfmt(num / MB) + \" MB\");\r\n    } else if (numBytes < TB) {\r\n        buf.append(dfmt(num / GB) + \" GB\");\r\n    } else if (numBytes < PB) {\r\n        buf.append(dfmt(num / TB) + \" TB\");\r\n    } else {\r\n        buf.append(dfmt(num / PB) + \" PB\");\r\n    }\r\n    if (bDetails) {\r\n        buf.append(\" (\" + ifmt(numBytes) + \" bytes)\");\r\n    }\r\n    return buf.toString();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "formatBytes2",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "String formatBytes2(long numBytes)\n{\r\n    StringBuffer buf = new StringBuffer();\r\n    long u = 0;\r\n    if (numBytes >= TB) {\r\n        u = numBytes / TB;\r\n        numBytes -= u * TB;\r\n        buf.append(u + \" TB \");\r\n    }\r\n    if (numBytes >= GB) {\r\n        u = numBytes / GB;\r\n        numBytes -= u * GB;\r\n        buf.append(u + \" GB \");\r\n    }\r\n    if (numBytes >= MB) {\r\n        u = numBytes / MB;\r\n        numBytes -= u * MB;\r\n        buf.append(u + \" MB \");\r\n    }\r\n    if (numBytes >= KB) {\r\n        u = numBytes / KB;\r\n        numBytes -= u * KB;\r\n        buf.append(u + \" KB \");\r\n    }\r\n    buf.append(u + \" B\");\r\n    return buf.toString();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "regexpEscape",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "String regexpEscape(String plain)\n{\r\n    StringBuffer buf = new StringBuffer();\r\n    char[] ch = plain.toCharArray();\r\n    int csup = ch.length;\r\n    for (int c = 0; c < csup; c++) {\r\n        if (regexpSpecials.indexOf(ch[c]) != -1) {\r\n            buf.append(\"\\\\\");\r\n        }\r\n        buf.append(ch[c]);\r\n    }\r\n    return buf.toString();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "createConfigValue",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "String createConfigValue(int msgSize)\n{\r\n    StringBuilder sb = new StringBuilder(msgSize);\r\n    for (int i = 0; i < msgSize; i++) {\r\n        sb.append('a');\r\n    }\r\n    return sb.toString();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "safeGetCanonicalPath",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 3,
  "sourceCodeText" : "String safeGetCanonicalPath(File f)\n{\r\n    try {\r\n        String s = f.getCanonicalPath();\r\n        return (s == null) ? f.toString() : s;\r\n    } catch (IOException io) {\r\n        return f.toString();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "slurp",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "String slurp(File f) throws IOException\n{\r\n    int len = (int) f.length();\r\n    byte[] buf = new byte[len];\r\n    FileInputStream in = new FileInputStream(f);\r\n    String contents = null;\r\n    try {\r\n        in.read(buf, 0, len);\r\n        contents = new String(buf, \"UTF-8\");\r\n    } finally {\r\n        in.close();\r\n    }\r\n    return contents;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "slurpHadoop",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "String slurpHadoop(Path p, FileSystem fs) throws IOException\n{\r\n    int len = (int) fs.getFileStatus(p).getLen();\r\n    byte[] buf = new byte[len];\r\n    InputStream in = fs.open(p);\r\n    String contents = null;\r\n    try {\r\n        in.read(buf, 0, len);\r\n        contents = new String(buf, \"UTF-8\");\r\n    } finally {\r\n        in.close();\r\n    }\r\n    return contents;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "rjustify",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "String rjustify(String s, int width)\n{\r\n    if (s == null)\r\n        s = \"null\";\r\n    if (width > s.length()) {\r\n        s = getSpace(width - s.length()) + s;\r\n    }\r\n    return s;\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "ljustify",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "String ljustify(String s, int width)\n{\r\n    if (s == null)\r\n        s = \"null\";\r\n    if (width > s.length()) {\r\n        s = s + getSpace(width - s.length());\r\n    }\r\n    return s;\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getSpace",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "String getSpace(int len)\n{\r\n    if (len > space.length) {\r\n        space = new char[Math.max(len, 2 * space.length)];\r\n        Arrays.fill(space, '\\u0020');\r\n    }\r\n    return new String(space, 0, len);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getJobStatus",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "JobStatus getJobStatus(JobClient jc, JobID id) throws IOException\n{\r\n    JobStatus[] statuses = jc.getAllJobs();\r\n    for (JobStatus jobStatus : statuses) {\r\n        if (jobStatus.getJobID().equals(id)) {\r\n            return jobStatus;\r\n        }\r\n    }\r\n    return null;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "waitFor",
  "errType" : [ "InterruptedException" ],
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void waitFor(long duration)\n{\r\n    try {\r\n        synchronized (waitLock) {\r\n            waitLock.wait(duration);\r\n        }\r\n    } catch (InterruptedException ie) {\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "waitForJobTracker",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void waitForJobTracker(JobClient jobClient)\n{\r\n    while (true) {\r\n        try {\r\n            ClusterStatus status = jobClient.getClusterStatus();\r\n            while (status.getJobTrackerStatus() != JobTrackerStatus.RUNNING) {\r\n                waitFor(100);\r\n                status = jobClient.getClusterStatus();\r\n            }\r\n            break;\r\n        } catch (IOException ioe) {\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "waitTillDone",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void waitTillDone(JobClient jobClient) throws IOException\n{\r\n    while (true) {\r\n        boolean shouldWait = false;\r\n        for (JobStatus jobStatuses : jobClient.getAllJobs()) {\r\n            if (jobStatuses.getRunState() != JobStatus.SUCCEEDED && jobStatuses.getRunState() != JobStatus.FAILED && jobStatuses.getRunState() != JobStatus.KILLED) {\r\n                shouldWait = true;\r\n                break;\r\n            }\r\n        }\r\n        if (shouldWait) {\r\n            waitFor(100);\r\n        } else {\r\n            break;\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "configureWaitingJobConf",
  "errType" : null,
  "containingMethodsNum" : 15,
  "sourceCodeText" : "void configureWaitingJobConf(JobConf jobConf, Path inDir, Path outputPath, int numMaps, int numRed, String jobName, String mapSignalFilename, String redSignalFilename) throws IOException\n{\r\n    jobConf.setJobName(jobName);\r\n    jobConf.setInputFormat(NonSplitableSequenceFileInputFormat.class);\r\n    jobConf.setOutputFormat(SequenceFileOutputFormat.class);\r\n    FileInputFormat.setInputPaths(jobConf, inDir);\r\n    FileOutputFormat.setOutputPath(jobConf, outputPath);\r\n    jobConf.setMapperClass(UtilsForTests.HalfWaitingMapper.class);\r\n    jobConf.setReducerClass(IdentityReducer.class);\r\n    jobConf.setOutputKeyClass(BytesWritable.class);\r\n    jobConf.setOutputValueClass(BytesWritable.class);\r\n    jobConf.setInputFormat(RandomInputFormat.class);\r\n    jobConf.setNumMapTasks(numMaps);\r\n    jobConf.setNumReduceTasks(numRed);\r\n    jobConf.setJar(\"build/test/mapred/testjar/testjob.jar\");\r\n    jobConf.set(getTaskSignalParameter(true), mapSignalFilename);\r\n    jobConf.set(getTaskSignalParameter(false), redSignalFilename);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getTaskSignalParameter",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getTaskSignalParameter(boolean isMap)\n{\r\n    return isMap ? \"test.mapred.map.waiting.target\" : \"test.mapred.reduce.waiting.target\";\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "signalTasks",
  "errType" : [ "InterruptedException" ],
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void signalTasks(MiniDFSCluster dfs, FileSystem fileSys, String mapSignalFile, String reduceSignalFile, int replication) throws IOException, TimeoutException\n{\r\n    try {\r\n        writeFile(dfs.getNameNode(), fileSys.getConf(), new Path(mapSignalFile), (short) replication);\r\n        writeFile(dfs.getNameNode(), fileSys.getConf(), new Path(reduceSignalFile), (short) replication);\r\n    } catch (InterruptedException ie) {\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "signalTasks",
  "errType" : [ "InterruptedException" ],
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void signalTasks(MiniDFSCluster dfs, FileSystem fileSys, boolean isMap, String mapSignalFile, String reduceSignalFile) throws IOException, TimeoutException\n{\r\n    try {\r\n        writeFile(dfs.getNameNode(), fileSys.getConf(), isMap ? new Path(mapSignalFile) : new Path(reduceSignalFile), (short) 1);\r\n    } catch (InterruptedException ie) {\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getSignalFile",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String getSignalFile(Path dir)\n{\r\n    return (new Path(dir, \"signal\")).toString();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getMapSignalFile",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String getMapSignalFile(Path dir)\n{\r\n    return (new Path(dir, \"map-signal\")).toString();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getReduceSignalFile",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String getReduceSignalFile(Path dir)\n{\r\n    return (new Path(dir, \"reduce-signal\")).toString();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "writeFile",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void writeFile(NameNode namenode, Configuration conf, Path name, short replication) throws IOException, TimeoutException, InterruptedException\n{\r\n    FileSystem fileSys = FileSystem.get(conf);\r\n    SequenceFile.Writer writer = SequenceFile.createWriter(fileSys, conf, name, BytesWritable.class, BytesWritable.class, CompressionType.NONE);\r\n    writer.append(new BytesWritable(), new BytesWritable());\r\n    writer.close();\r\n    fileSys.setReplication(name, replication);\r\n    DFSTestUtil.waitReplication(fileSys, name, replication);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "runJob",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "RunningJob runJob(JobConf conf, Path inDir, Path outDir) throws IOException\n{\r\n    return runJob(conf, inDir, outDir, conf.getNumMapTasks(), conf.getNumReduceTasks());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "runJob",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "RunningJob runJob(JobConf conf, Path inDir, Path outDir, int numMaps, int numReds) throws IOException\n{\r\n    String input = \"The quick brown fox\\n\" + \"has many silly\\n\" + \"red fox sox\\n\";\r\n    return runJob(conf, inDir, outDir, numMaps, numReds, input);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "runJob",
  "errType" : null,
  "containingMethodsNum" : 16,
  "sourceCodeText" : "RunningJob runJob(JobConf conf, Path inDir, Path outDir, int numMaps, int numReds, String input) throws IOException\n{\r\n    FileSystem fs = FileSystem.get(conf);\r\n    if (fs.exists(outDir)) {\r\n        fs.delete(outDir, true);\r\n    }\r\n    if (!fs.exists(inDir)) {\r\n        fs.mkdirs(inDir);\r\n    }\r\n    for (int i = 0; i < numMaps; ++i) {\r\n        DataOutputStream file = fs.create(new Path(inDir, \"part-\" + i));\r\n        file.writeBytes(input);\r\n        file.close();\r\n    }\r\n    conf.setInputFormat(TextInputFormat.class);\r\n    conf.setOutputKeyClass(LongWritable.class);\r\n    conf.setOutputValueClass(Text.class);\r\n    FileInputFormat.setInputPaths(conf, inDir);\r\n    FileOutputFormat.setOutputPath(conf, outDir);\r\n    conf.setNumMapTasks(numMaps);\r\n    conf.setNumReduceTasks(numReds);\r\n    JobClient jobClient = new JobClient(conf);\r\n    RunningJob job = jobClient.submitJob(conf);\r\n    return job;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "waitForAppFinished",
  "errType" : [ "TimeoutException|InterruptedException" ],
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void waitForAppFinished(RunningJob job, MiniMRYarnCluster cluster) throws IOException\n{\r\n    ApplicationId appId = ApplicationId.newInstance(Long.parseLong(job.getID().getJtIdentifier()), job.getID().getId());\r\n    ConcurrentMap<ApplicationId, RMApp> rmApps = cluster.getResourceManager().getRMContext().getRMApps();\r\n    if (!rmApps.containsKey(appId)) {\r\n        throw new IOException(\"Job not found\");\r\n    }\r\n    final RMApp rmApp = rmApps.get(appId);\r\n    try {\r\n        GenericTestUtils.waitFor(new Supplier<Boolean>() {\r\n\r\n            @Override\r\n            public Boolean get() {\r\n                return RMAppImpl.isAppInFinalState(rmApp);\r\n            }\r\n        }, 1000, 1000 * 180);\r\n    } catch (TimeoutException | InterruptedException e1) {\r\n        throw new IOException(\"Yarn application with \" + appId + \" didn't finish \" + \"did not reach finale State\", e1);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "runJobSucceed",
  "errType" : [ "InterruptedException" ],
  "containingMethodsNum" : 6,
  "sourceCodeText" : "RunningJob runJobSucceed(JobConf conf, Path inDir, Path outDir) throws IOException\n{\r\n    conf.setJobName(\"test-job-succeed\");\r\n    conf.setMapperClass(IdentityMapper.class);\r\n    conf.setReducerClass(IdentityReducer.class);\r\n    RunningJob job = UtilsForTests.runJob(conf, inDir, outDir);\r\n    long sleepCount = 0;\r\n    while (!job.isComplete()) {\r\n        try {\r\n            if (sleepCount > 300) {\r\n                throw new IOException(\"Job didn't finish in 30 seconds\");\r\n            }\r\n            Thread.sleep(100);\r\n            sleepCount++;\r\n        } catch (InterruptedException e) {\r\n            break;\r\n        }\r\n    }\r\n    return job;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "runJobFail",
  "errType" : [ "InterruptedException" ],
  "containingMethodsNum" : 7,
  "sourceCodeText" : "RunningJob runJobFail(JobConf conf, Path inDir, Path outDir) throws IOException\n{\r\n    conf.setJobName(\"test-job-fail\");\r\n    conf.setMapperClass(FailMapper.class);\r\n    conf.setReducerClass(IdentityReducer.class);\r\n    conf.setMaxMapAttempts(1);\r\n    RunningJob job = UtilsForTests.runJob(conf, inDir, outDir);\r\n    long sleepCount = 0;\r\n    while (!job.isComplete()) {\r\n        try {\r\n            if (sleepCount > 300) {\r\n                throw new IOException(\"Job didn't finish in 30 seconds\");\r\n            }\r\n            Thread.sleep(100);\r\n            sleepCount++;\r\n        } catch (InterruptedException e) {\r\n            break;\r\n        }\r\n    }\r\n    return job;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "runJobKill",
  "errType" : [ "InterruptedException", "InterruptedException" ],
  "containingMethodsNum" : 9,
  "sourceCodeText" : "RunningJob runJobKill(JobConf conf, Path inDir, Path outDir) throws IOException\n{\r\n    conf.setJobName(\"test-job-kill\");\r\n    conf.setMapperClass(KillMapper.class);\r\n    conf.setReducerClass(IdentityReducer.class);\r\n    RunningJob job = UtilsForTests.runJob(conf, inDir, outDir);\r\n    long sleepCount = 0;\r\n    while (job.getJobState() != JobStatus.RUNNING) {\r\n        try {\r\n            if (sleepCount > 300) {\r\n                throw new IOException(\"Job didn't finish in 30 seconds\");\r\n            }\r\n            Thread.sleep(100);\r\n            sleepCount++;\r\n        } catch (InterruptedException e) {\r\n            break;\r\n        }\r\n    }\r\n    job.killJob();\r\n    sleepCount = 0;\r\n    while (job.cleanupProgress() == 0.0f) {\r\n        try {\r\n            if (sleepCount > 2000) {\r\n                throw new IOException(\"Job cleanup didn't start in 20 seconds\");\r\n            }\r\n            Thread.sleep(10);\r\n            sleepCount++;\r\n        } catch (InterruptedException ie) {\r\n            break;\r\n        }\r\n    }\r\n    return job;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 2,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "setUpConfigFile",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void setUpConfigFile(Properties confProps, File configFile) throws IOException\n{\r\n    Configuration config = new Configuration(false);\r\n    FileOutputStream fos = new FileOutputStream(configFile);\r\n    for (Enumeration<?> e = confProps.propertyNames(); e.hasMoreElements(); ) {\r\n        String key = (String) e.nextElement();\r\n        config.set(key, confProps.getProperty(key));\r\n    }\r\n    config.writeXml(fos);\r\n    fos.close();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "createTmpFileDFS",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "DataOutputStream createTmpFileDFS(FileSystem dfs, Path URIPATH, FsPermission permission, String input) throws Exception\n{\r\n    DataOutputStream file = FileSystem.create(dfs, URIPATH, permission);\r\n    file.writeBytes(input);\r\n    file.close();\r\n    return file;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getFQDNofTT",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "String getFQDNofTT(String taskTrackerLong) throws Exception\n{\r\n    String[] firstSplit = taskTrackerLong.split(\"_\");\r\n    String tmpOutput = firstSplit[1];\r\n    String[] secondSplit = tmpOutput.split(\":\");\r\n    String tmpTaskTracker = secondSplit[0];\r\n    return tmpTaskTracker;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "testMovingWindow",
  "errType" : null,
  "containingMethodsNum" : 55,
  "sourceCodeText" : "void testMovingWindow() throws Exception\n{\r\n    StatisticsCollector collector = new StatisticsCollector(1);\r\n    TimeWindow window = new TimeWindow(\"test\", 6, 2);\r\n    TimeWindow sincStart = StatisticsCollector.SINCE_START;\r\n    TimeWindow[] windows = { sincStart, window };\r\n    Stat stat = collector.createStat(\"m1\", windows);\r\n    stat.inc(3);\r\n    collector.update();\r\n    assertEquals(0, stat.getValues().get(window).getValue());\r\n    assertEquals(3, stat.getValues().get(sincStart).getValue());\r\n    stat.inc(3);\r\n    collector.update();\r\n    assertEquals((3 + 3), stat.getValues().get(window).getValue());\r\n    assertEquals(6, stat.getValues().get(sincStart).getValue());\r\n    stat.inc(10);\r\n    collector.update();\r\n    assertEquals((3 + 3), stat.getValues().get(window).getValue());\r\n    assertEquals(16, stat.getValues().get(sincStart).getValue());\r\n    stat.inc(10);\r\n    collector.update();\r\n    assertEquals((3 + 3 + 10 + 10), stat.getValues().get(window).getValue());\r\n    assertEquals(26, stat.getValues().get(sincStart).getValue());\r\n    stat.inc(10);\r\n    collector.update();\r\n    stat.inc(10);\r\n    collector.update();\r\n    assertEquals((3 + 3 + 10 + 10 + 10 + 10), stat.getValues().get(window).getValue());\r\n    assertEquals(46, stat.getValues().get(sincStart).getValue());\r\n    stat.inc(10);\r\n    collector.update();\r\n    assertEquals((3 + 3 + 10 + 10 + 10 + 10), stat.getValues().get(window).getValue());\r\n    assertEquals(56, stat.getValues().get(sincStart).getValue());\r\n    stat.inc(12);\r\n    collector.update();\r\n    assertEquals((10 + 10 + 10 + 10 + 10 + 12), stat.getValues().get(window).getValue());\r\n    assertEquals(68, stat.getValues().get(sincStart).getValue());\r\n    stat.inc(13);\r\n    collector.update();\r\n    assertEquals((10 + 10 + 10 + 10 + 10 + 12), stat.getValues().get(window).getValue());\r\n    assertEquals(81, stat.getValues().get(sincStart).getValue());\r\n    stat.inc(14);\r\n    collector.update();\r\n    assertEquals((10 + 10 + 10 + 12 + 13 + 14), stat.getValues().get(window).getValue());\r\n    assertEquals(95, stat.getValues().get(sincStart).getValue());\r\n    Map updaters = collector.getUpdaters();\r\n    assertThat(updaters.size()).isEqualTo(2);\r\n    Map<String, Stat> ststistics = collector.getStatistics();\r\n    assertNotNull(ststistics.get(\"m1\"));\r\n    Stat newStat = collector.createStat(\"m2\");\r\n    assertThat(newStat.name).isEqualTo(\"m2\");\r\n    Stat st = collector.removeStat(\"m1\");\r\n    assertThat(st.name).isEqualTo(\"m1\");\r\n    assertEquals((10 + 10 + 10 + 12 + 13 + 14), stat.getValues().get(window).getValue());\r\n    assertEquals(95, stat.getValues().get(sincStart).getValue());\r\n    st = collector.removeStat(\"m1\");\r\n    assertNull(st);\r\n    collector.start();\r\n    Thread.sleep(2500);\r\n    assertEquals(69, stat.getValues().get(window).getValue());\r\n    assertEquals(95, stat.getValues().get(sincStart).getValue());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\input",
  "methodName" : "testFormat",
  "errType" : null,
  "containingMethodsNum" : 28,
  "sourceCodeText" : "void testFormat() throws Exception\n{\r\n    Job job = Job.getInstance(conf);\r\n    FileSystem fs = FileSystem.getLocal(conf);\r\n    Path dir = new Path(System.getProperty(\"test.build.data\", \".\") + \"/mapred\");\r\n    Path file = new Path(dir, \"test.seq\");\r\n    int seed = new Random().nextInt();\r\n    Random random = new Random(seed);\r\n    fs.delete(dir, true);\r\n    FileInputFormat.setInputPaths(job, dir);\r\n    for (int length = 0; length < MAX_LENGTH; length += random.nextInt(MAX_LENGTH / 10) + 1) {\r\n        SequenceFile.Writer writer = SequenceFile.createWriter(fs, conf, file, IntWritable.class, LongWritable.class);\r\n        try {\r\n            for (int i = 0; i < length; i++) {\r\n                IntWritable key = new IntWritable(i);\r\n                LongWritable value = new LongWritable(10 * i);\r\n                writer.append(key, value);\r\n            }\r\n        } finally {\r\n            writer.close();\r\n        }\r\n        TaskAttemptContext context = MapReduceTestUtil.createDummyMapTaskAttemptContext(job.getConfiguration());\r\n        InputFormat<Text, Text> format = new SequenceFileAsTextInputFormat();\r\n        for (int i = 0; i < 3; i++) {\r\n            BitSet bits = new BitSet(length);\r\n            int numSplits = random.nextInt(MAX_LENGTH / (SequenceFile.SYNC_INTERVAL / 20)) + 1;\r\n            FileInputFormat.setMaxInputSplitSize(job, fs.getFileStatus(file).getLen() / numSplits);\r\n            for (InputSplit split : format.getSplits(job)) {\r\n                RecordReader<Text, Text> reader = format.createRecordReader(split, context);\r\n                MapContext<Text, Text, Text, Text> mcontext = new MapContextImpl<Text, Text, Text, Text>(job.getConfiguration(), context.getTaskAttemptID(), reader, null, null, MapReduceTestUtil.createDummyReporter(), split);\r\n                reader.initialize(split, mcontext);\r\n                Class<?> readerClass = reader.getClass();\r\n                assertEquals(\"reader class is SequenceFileAsTextRecordReader.\", SequenceFileAsTextRecordReader.class, readerClass);\r\n                Text key;\r\n                try {\r\n                    int count = 0;\r\n                    while (reader.nextKeyValue()) {\r\n                        key = reader.getCurrentKey();\r\n                        int keyInt = Integer.parseInt(key.toString());\r\n                        assertFalse(\"Key in multiple partitions.\", bits.get(keyInt));\r\n                        bits.set(keyInt);\r\n                        count++;\r\n                    }\r\n                } finally {\r\n                    reader.close();\r\n                }\r\n            }\r\n            assertEquals(\"Some keys in no partition.\", length, bits.cardinality());\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 2,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\fs\\slive",
  "methodName" : "getDirectory",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Path getDirectory()\n{\r\n    Path dir = getFinder().getDirectory();\r\n    return dir;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\fs\\slive",
  "methodName" : "run",
  "errType" : [ "FileNotFoundException", "IOException" ],
  "containingMethodsNum" : 14,
  "sourceCodeText" : "List<OperationOutput> run(FileSystem fs)\n{\r\n    List<OperationOutput> out = super.run(fs);\r\n    try {\r\n        Path dir = getDirectory();\r\n        boolean mkRes = false;\r\n        long timeTaken = 0;\r\n        {\r\n            long startTime = Timer.now();\r\n            mkRes = fs.mkdirs(dir);\r\n            timeTaken = Timer.elapsed(startTime);\r\n        }\r\n        if (mkRes) {\r\n            out.add(new OperationOutput(OutputType.LONG, getType(), ReportWriter.OK_TIME_TAKEN, timeTaken));\r\n            out.add(new OperationOutput(OutputType.LONG, getType(), ReportWriter.SUCCESSES, 1L));\r\n            LOG.info(\"Made directory \" + dir);\r\n        } else {\r\n            out.add(new OperationOutput(OutputType.LONG, getType(), ReportWriter.FAILURES, 1L));\r\n            LOG.warn(\"Could not make \" + dir);\r\n        }\r\n    } catch (FileNotFoundException e) {\r\n        out.add(new OperationOutput(OutputType.LONG, getType(), ReportWriter.NOT_FOUND, 1L));\r\n        LOG.warn(\"Error with mkdir\", e);\r\n    } catch (IOException e) {\r\n        out.add(new OperationOutput(OutputType.LONG, getType(), ReportWriter.FAILURES, 1L));\r\n        LOG.warn(\"Error with mkdir\", e);\r\n    }\r\n    return out;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\db",
  "methodName" : "formatArray",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "String formatArray(Object[] ar)\n{\r\n    StringBuilder sb = new StringBuilder();\r\n    sb.append(\"[\");\r\n    boolean first = true;\r\n    for (Object val : ar) {\r\n        if (!first) {\r\n            sb.append(\", \");\r\n        }\r\n        sb.append(val.toString());\r\n        first = false;\r\n    }\r\n    sb.append(\"]\");\r\n    return sb.toString();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\db",
  "methodName" : "assertArrayEquals",
  "errType" : [ "ArrayIndexOutOfBoundsException" ],
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void assertArrayEquals(Object[] expected, Object[] actual)\n{\r\n    for (int i = 0; i < expected.length; i++) {\r\n        try {\r\n            assertEquals(\"Failure at position \" + i + \"; got \" + actual[i] + \" instead of \" + expected[i] + \"; actual array is \" + formatArray(actual), expected[i], actual[i]);\r\n        } catch (ArrayIndexOutOfBoundsException oob) {\r\n            fail(\"Expected array with \" + expected.length + \" elements; got \" + actual.length + \". Actual array is \" + formatArray(actual));\r\n        }\r\n    }\r\n    if (actual.length > expected.length) {\r\n        fail(\"Actual array has \" + actual.length + \" elements; expected \" + expected.length + \". Actual array is \" + formatArray(actual));\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\db",
  "methodName" : "testStringConvertEmpty",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void testStringConvertEmpty()\n{\r\n    TextSplitter splitter = new TextSplitter();\r\n    BigDecimal emptyBigDec = splitter.stringToBigDecimal(\"\");\r\n    assertEquals(BigDecimal.ZERO, emptyBigDec);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\db",
  "methodName" : "testBigDecConvertEmpty",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void testBigDecConvertEmpty()\n{\r\n    TextSplitter splitter = new TextSplitter();\r\n    String emptyStr = splitter.bigDecimalToString(BigDecimal.ZERO);\r\n    assertEquals(\"\", emptyStr);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\db",
  "methodName" : "testConvertA",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void testConvertA()\n{\r\n    TextSplitter splitter = new TextSplitter();\r\n    String out = splitter.bigDecimalToString(splitter.stringToBigDecimal(\"A\"));\r\n    assertEquals(\"A\", out);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\db",
  "methodName" : "testConvertZ",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void testConvertZ()\n{\r\n    TextSplitter splitter = new TextSplitter();\r\n    String out = splitter.bigDecimalToString(splitter.stringToBigDecimal(\"Z\"));\r\n    assertEquals(\"Z\", out);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\db",
  "methodName" : "testConvertThreeChars",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void testConvertThreeChars()\n{\r\n    TextSplitter splitter = new TextSplitter();\r\n    String out = splitter.bigDecimalToString(splitter.stringToBigDecimal(\"abc\"));\r\n    assertEquals(\"abc\", out);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\db",
  "methodName" : "testConvertStr",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void testConvertStr()\n{\r\n    TextSplitter splitter = new TextSplitter();\r\n    String out = splitter.bigDecimalToString(splitter.stringToBigDecimal(\"big str\"));\r\n    assertEquals(\"big str\", out);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\db",
  "methodName" : "testConvertChomped",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void testConvertChomped()\n{\r\n    TextSplitter splitter = new TextSplitter();\r\n    String out = splitter.bigDecimalToString(splitter.stringToBigDecimal(\"AVeryLongStringIndeed\"));\r\n    assertEquals(\"AVeryLon\", out);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\db",
  "methodName" : "testAlphabetSplit",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void testAlphabetSplit() throws SQLException\n{\r\n    TextSplitter splitter = new TextSplitter();\r\n    List<String> splits = splitter.split(25, \"A\", \"Z\", \"\");\r\n    String[] expected = { \"A\", \"B\", \"C\", \"D\", \"E\", \"F\", \"G\", \"H\", \"I\", \"J\", \"K\", \"L\", \"M\", \"N\", \"O\", \"P\", \"Q\", \"R\", \"S\", \"T\", \"U\", \"V\", \"W\", \"X\", \"Y\", \"Z\" };\r\n    assertArrayEquals(expected, splits.toArray(new String[0]));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\db",
  "methodName" : "testCommonPrefix",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void testCommonPrefix() throws SQLException\n{\r\n    TextSplitter splitter = new TextSplitter();\r\n    List<String> splits = splitter.split(5, \"nd\", \"rdy\", \"Ha\");\r\n    assertEquals(\"Hand\", splits.get(0));\r\n    assertEquals(\"Hardy\", splits.get(splits.size() - 1));\r\n    assertEquals(6, splits.size());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "configure",
  "errType" : null,
  "containingMethodsNum" : 26,
  "sourceCodeText" : "void configure() throws Exception\n{\r\n    Path testdir = new Path(TEST_DIR.getAbsolutePath());\r\n    Path inDir = new Path(testdir, \"in\");\r\n    Path outDir = new Path(testdir, \"out\");\r\n    FileSystem fs = FileSystem.get(conf);\r\n    fs.delete(testdir, true);\r\n    conf.setInputFormat(SequenceFileInputFormat.class);\r\n    FileInputFormat.setInputPaths(conf, inDir);\r\n    FileOutputFormat.setOutputPath(conf, outDir);\r\n    conf.setOutputKeyClass(IntWritable.class);\r\n    conf.setOutputValueClass(Text.class);\r\n    conf.setMapOutputValueClass(IntWritable.class);\r\n    conf.setNumMapTasks(2);\r\n    conf.set(MRConfig.FRAMEWORK_NAME, MRConfig.LOCAL_FRAMEWORK_NAME);\r\n    conf.setOutputFormat(SequenceFileOutputFormat.class);\r\n    if (!fs.mkdirs(testdir)) {\r\n        throw new IOException(\"Mkdirs failed to create \" + testdir.toString());\r\n    }\r\n    if (!fs.mkdirs(inDir)) {\r\n        throw new IOException(\"Mkdirs failed to create \" + inDir.toString());\r\n    }\r\n    Path inFile = new Path(inDir, \"part0\");\r\n    SequenceFile.Writer writer = SequenceFile.createWriter(fs, conf, inFile, IntWritable.class, IntWritable.class);\r\n    writer.append(new IntWritable(11), new IntWritable(999));\r\n    writer.append(new IntWritable(23), new IntWritable(456));\r\n    writer.append(new IntWritable(10), new IntWritable(780));\r\n    writer.close();\r\n    inFile = new Path(inDir, \"part1\");\r\n    writer = SequenceFile.createWriter(fs, conf, inFile, IntWritable.class, IntWritable.class);\r\n    writer.append(new IntWritable(45), new IntWritable(100));\r\n    writer.append(new IntWritable(18), new IntWritable(200));\r\n    writer.append(new IntWritable(27), new IntWritable(300));\r\n    writer.close();\r\n    jc = new JobClient(conf);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "cleanup",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void cleanup()\n{\r\n    FileUtil.fullyDelete(TEST_DIR);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "testDefaultMRComparator",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void testDefaultMRComparator() throws Exception\n{\r\n    conf.setMapperClass(IdentityMapper.class);\r\n    conf.setReducerClass(AscendingKeysReducer.class);\r\n    RunningJob r_job = jc.submitJob(conf);\r\n    while (!r_job.isComplete()) {\r\n        Thread.sleep(1000);\r\n    }\r\n    if (!r_job.isSuccessful()) {\r\n        fail(\"Oops! The job broke due to an unexpected error\");\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "testUserMRComparator",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void testUserMRComparator() throws Exception\n{\r\n    conf.setMapperClass(IdentityMapper.class);\r\n    conf.setReducerClass(DescendingKeysReducer.class);\r\n    conf.setOutputKeyComparatorClass(DecreasingIntComparator.class);\r\n    RunningJob r_job = jc.submitJob(conf);\r\n    while (!r_job.isComplete()) {\r\n        Thread.sleep(1000);\r\n    }\r\n    if (!r_job.isSuccessful()) {\r\n        fail(\"Oops! The job broke due to an unexpected error\");\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "testUserValueGroupingComparator",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void testUserValueGroupingComparator() throws Exception\n{\r\n    conf.setMapperClass(RandomGenMapper.class);\r\n    conf.setReducerClass(AscendingGroupReducer.class);\r\n    conf.setOutputValueGroupingComparator(CompositeIntGroupFn.class);\r\n    RunningJob r_job = jc.submitJob(conf);\r\n    while (!r_job.isComplete()) {\r\n        Thread.sleep(1000);\r\n    }\r\n    if (!r_job.isSuccessful()) {\r\n        fail(\"Oops! The job broke due to an unexpected error\");\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "testAllUserComparators",
  "errType" : null,
  "containingMethodsNum" : 9,
  "sourceCodeText" : "void testAllUserComparators() throws Exception\n{\r\n    conf.setMapperClass(RandomGenMapper.class);\r\n    conf.setOutputKeyComparatorClass(DecreasingIntComparator.class);\r\n    conf.setReducerClass(DescendingGroupReducer.class);\r\n    conf.setOutputValueGroupingComparator(CompositeIntReverseGroupFn.class);\r\n    RunningJob r_job = jc.submitJob(conf);\r\n    while (!r_job.isComplete()) {\r\n        Thread.sleep(1000);\r\n    }\r\n    if (!r_job.isSuccessful()) {\r\n        fail(\"Oops! The job broke due to an unexpected error\");\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "testBakedUserComparator",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void testBakedUserComparator() throws Exception\n{\r\n    MyWritable a = new MyWritable(8, 8);\r\n    MyWritable b = new MyWritable(7, 9);\r\n    assertTrue(a.compareTo(b) > 0);\r\n    assertTrue(WritableComparator.get(MyWritable.class).compare(a, b) < 0);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\testjar",
  "methodName" : "getMessage",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getMessage()\n{\r\n    return message;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\testjar",
  "methodName" : "setMessage",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setMessage(String message)\n{\r\n    this.message = message;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\testjar",
  "methodName" : "readFields",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void readFields(DataInput in) throws IOException\n{\r\n    message = null;\r\n    boolean hasMessage = in.readBoolean();\r\n    if (hasMessage) {\r\n        message = in.readUTF();\r\n    }\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\testjar",
  "methodName" : "write",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void write(DataOutput out) throws IOException\n{\r\n    boolean hasMessage = (message != null && message.length() > 0);\r\n    out.writeBoolean(hasMessage);\r\n    if (hasMessage) {\r\n        out.writeUTF(message);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\testjar",
  "methodName" : "compareTo",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "int compareTo(Object o)\n{\r\n    if (!(o instanceof ExternalWritable)) {\r\n        throw new IllegalArgumentException(\"Input not an ExternalWritable\");\r\n    }\r\n    ExternalWritable that = (ExternalWritable) o;\r\n    return this.message.compareTo(that.message);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\testjar",
  "methodName" : "toString",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String toString()\n{\r\n    return this.message;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\fs\\slive",
  "methodName" : "getTrimmedStrings",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "String[] getTrimmedStrings(String str)\n{\r\n    if (null == str || \"\".equals(str.trim())) {\r\n        return emptyStringArray;\r\n    }\r\n    return str.trim().split(\"\\\\s*,\\\\s*\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\fs\\slive",
  "methodName" : "toByteInfo",
  "errType" : null,
  "containingMethodsNum" : 9,
  "sourceCodeText" : "String toByteInfo(long bytes)\n{\r\n    StringBuilder str = new StringBuilder();\r\n    if (bytes < 0) {\r\n        bytes = 0;\r\n    }\r\n    str.append(bytes);\r\n    str.append(\" bytes or \");\r\n    str.append(bytes / 1024);\r\n    str.append(\" kilobytes or \");\r\n    str.append(bytes / (1024 * 1024));\r\n    str.append(\" megabytes or \");\r\n    str.append(bytes / (1024 * 1024 * 1024));\r\n    str.append(\" gigabytes\");\r\n    return str.toString();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\fs\\slive",
  "methodName" : "stringifyArray",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "String stringifyArray(Object[] args, String sep)\n{\r\n    StringBuilder optStr = new StringBuilder();\r\n    for (int i = 0; i < args.length; ++i) {\r\n        optStr.append(args[i]);\r\n        if ((i + 1) != args.length) {\r\n            optStr.append(sep);\r\n        }\r\n    }\r\n    return optStr.toString();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\fs",
  "methodName" : "createControlFile",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void createControlFile(FileSystem fs, Path jhLogDir) throws IOException\n{\r\n    LOG.info(\"creating control file: JH log dir = \" + jhLogDir);\r\n    FileCreateDaemon.createControlFile(fs, jhLogDir);\r\n    LOG.info(\"created control file: JH log dir = \" + jhLogDir);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\fs",
  "methodName" : "getFileName",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String getFileName(int fIdx)\n{\r\n    return BASE_INPUT_FILE_NAME + Integer.toString(fIdx);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\fs",
  "methodName" : "getKeyValue",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String[] getKeyValue(String t) throws IOException\n{\r\n    String[] keyVal = t.split(\"=\\\"*|\\\"\");\r\n    return keyVal;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\fs",
  "methodName" : "runJHLA",
  "errType" : null,
  "containingMethodsNum" : 10,
  "sourceCodeText" : "void runJHLA(Class<? extends Mapper<Text, LongWritable, Text, Text>> mapperClass, Path outputDir, Configuration fsConfig) throws IOException\n{\r\n    JobConf job = new JobConf(fsConfig, JHLogAnalyzer.class);\r\n    job.setPartitionerClass(JHLAPartitioner.class);\r\n    FileInputFormat.setInputPaths(job, INPUT_DIR);\r\n    job.setInputFormat(SequenceFileInputFormat.class);\r\n    job.setMapperClass(mapperClass);\r\n    job.setReducerClass(AccumulatingReducer.class);\r\n    FileOutputFormat.setOutputPath(job, outputDir);\r\n    job.setOutputKeyClass(Text.class);\r\n    job.setOutputValueClass(Text.class);\r\n    job.setNumReduceTasks(JHLAPartitioner.NUM_REDUCERS);\r\n    JobClient.runJob(job);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\fs",
  "methodName" : "main",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 34,
  "sourceCodeText" : "void main(String[] args)\n{\r\n    Path resFileName = RESULT_FILE;\r\n    Configuration conf = new Configuration();\r\n    try {\r\n        conf.setInt(\"test.io.file.buffer.size\", 0);\r\n        Path historyDir = DEFAULT_HISTORY_DIR;\r\n        String testFile = null;\r\n        boolean cleanup = false;\r\n        boolean initControlFiles = true;\r\n        for (int i = 0; i < args.length; i++) {\r\n            if (args[i].equalsIgnoreCase(\"-historyDir\")) {\r\n                historyDir = new Path(args[++i]);\r\n            } else if (args[i].equalsIgnoreCase(\"-resFile\")) {\r\n                resFileName = new Path(args[++i]);\r\n            } else if (args[i].equalsIgnoreCase(\"-usersIncluded\")) {\r\n                conf.set(\"jhla.users.included\", args[++i]);\r\n            } else if (args[i].equalsIgnoreCase(\"-usersExcluded\")) {\r\n                conf.set(\"jhla.users.excluded\", args[++i]);\r\n            } else if (args[i].equalsIgnoreCase(\"-gzip\")) {\r\n                conf.set(\"jhla.compression.class\", GzipCodec.class.getCanonicalName());\r\n            } else if (args[i].equalsIgnoreCase(\"-jobDelimiter\")) {\r\n                conf.set(\"jhla.job.delimiter.pattern\", args[++i]);\r\n            } else if (args[i].equalsIgnoreCase(\"-jobDelimiterLength\")) {\r\n                conf.setInt(\"jhla.job.delimiter.length\", Integer.parseInt(args[++i]));\r\n            } else if (args[i].equalsIgnoreCase(\"-noInit\")) {\r\n                initControlFiles = false;\r\n            } else if (args[i].equalsIgnoreCase(\"-test\")) {\r\n                testFile = args[++i];\r\n            } else if (args[i].equalsIgnoreCase(\"-clean\")) {\r\n                cleanup = true;\r\n            } else if (args[i].equalsIgnoreCase(\"-jobQueue\")) {\r\n                conf.set(\"mapred.job.queue.name\", args[++i]);\r\n            } else if (args[i].startsWith(\"-Xmx\")) {\r\n                conf.set(\"mapred.child.java.opts\", args[i]);\r\n            } else {\r\n                printUsage();\r\n            }\r\n        }\r\n        if (cleanup) {\r\n            cleanup(conf);\r\n            return;\r\n        }\r\n        if (testFile != null) {\r\n            LOG.info(\"Start JHLA test ============ \");\r\n            LocalFileSystem lfs = FileSystem.getLocal(conf);\r\n            conf.set(\"fs.defaultFS\", \"file:///\");\r\n            JHLAMapper map = new JHLAMapper(conf);\r\n            map.parseLogFile(lfs, new Path(testFile), 0L, new LoggingCollector(), Reporter.NULL);\r\n            return;\r\n        }\r\n        FileSystem fs = FileSystem.get(conf);\r\n        if (initControlFiles)\r\n            createControlFile(fs, historyDir);\r\n        long tStart = System.currentTimeMillis();\r\n        runJHLA(JHLAMapper.class, OUTPUT_DIR, conf);\r\n        long execTime = System.currentTimeMillis() - tStart;\r\n        analyzeResult(fs, 0, execTime, resFileName);\r\n    } catch (IOException e) {\r\n        System.err.print(StringUtils.stringifyException(e));\r\n        System.exit(-1);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\fs",
  "methodName" : "printUsage",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void printUsage()\n{\r\n    String className = JHLogAnalyzer.class.getSimpleName();\r\n    System.err.println(\"Usage: \" + className + \"\\n\\t[-historyDir inputDir] | [-resFile resultFile] |\" + \"\\n\\t[-usersIncluded | -usersExcluded userList] |\" + \"\\n\\t[-gzip] | [-jobDelimiter pattern] |\" + \"\\n\\t[-help | -clean | -test testFile]\");\r\n    System.exit(-1);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\fs",
  "methodName" : "getUserList",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "Collection<String> getUserList(String users)\n{\r\n    if (users == null)\r\n        return null;\r\n    StringTokenizer tokens = new StringTokenizer(users, \",;\");\r\n    Collection<String> userList = new ArrayList<String>(tokens.countTokens());\r\n    while (tokens.hasMoreTokens()) userList.add(tokens.nextToken());\r\n    return userList;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\fs",
  "methodName" : "analyzeResult",
  "errType" : null,
  "containingMethodsNum" : 23,
  "sourceCodeText" : "void analyzeResult(FileSystem fs, int testType, long execTime, Path resFileName) throws IOException\n{\r\n    LOG.info(\"Analyzing results ...\");\r\n    DataOutputStream out = null;\r\n    BufferedWriter writer = null;\r\n    try {\r\n        out = new DataOutputStream(fs.create(resFileName));\r\n        writer = new BufferedWriter(new OutputStreamWriter(out));\r\n        writer.write(\"SERIES\\tPERIOD\\tTYPE\\tSLOT_HOUR\\n\");\r\n        FileStatus[] reduceFiles = fs.listStatus(OUTPUT_DIR);\r\n        assert reduceFiles.length == JHLAPartitioner.NUM_REDUCERS;\r\n        for (int i = 0; i < JHLAPartitioner.NUM_REDUCERS; i++) {\r\n            DataInputStream in = null;\r\n            BufferedReader lines = null;\r\n            try {\r\n                in = fs.open(reduceFiles[i].getPath());\r\n                lines = new BufferedReader(new InputStreamReader(in));\r\n                String line;\r\n                while ((line = lines.readLine()) != null) {\r\n                    StringTokenizer tokens = new StringTokenizer(line, \"\\t*\");\r\n                    String attr = tokens.nextToken();\r\n                    String dateTime = tokens.nextToken();\r\n                    String taskType = tokens.nextToken();\r\n                    double val = Long.parseLong(tokens.nextToken()) / (double) DEFAULT_TIME_INTERVAL_MSEC;\r\n                    writer.write(attr.substring(2));\r\n                    writer.write(\"\\t\");\r\n                    writer.write(dateTime);\r\n                    writer.write(\"\\t\");\r\n                    writer.write(taskType);\r\n                    writer.write(\"\\t\");\r\n                    writer.write(String.valueOf((float) val));\r\n                    writer.newLine();\r\n                }\r\n            } finally {\r\n                if (lines != null)\r\n                    lines.close();\r\n                if (in != null)\r\n                    in.close();\r\n            }\r\n        }\r\n    } finally {\r\n        if (writer != null)\r\n            writer.close();\r\n        if (out != null)\r\n            out.close();\r\n    }\r\n    LOG.info(\"Analyzing results ... done.\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 2,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\fs",
  "methodName" : "cleanup",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void cleanup(Configuration conf) throws IOException\n{\r\n    LOG.info(\"Cleaning up test files\");\r\n    FileSystem fs = FileSystem.get(conf);\r\n    fs.delete(new Path(JHLA_ROOT_DIR), true);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\fs\\loadGenerator",
  "methodName" : "run",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "int run(String[] args) throws Exception\n{\r\n    int exitCode = parseArgsMR(args);\r\n    if (exitCode != 0) {\r\n        return exitCode;\r\n    }\r\n    System.out.println(\"Running LoadGeneratorMR against fileSystem: \" + FileContext.getFileContext().getDefaultFileSystem().getUri());\r\n    return submitAsMapReduce();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\fs\\loadGenerator",
  "methodName" : "parseArgsMR",
  "errType" : [ "NumberFormatException" ],
  "containingMethodsNum" : 10,
  "sourceCodeText" : "int parseArgsMR(String[] args) throws IOException\n{\r\n    try {\r\n        if (args.length >= 3 && args[0].equals(\"-mr\")) {\r\n            numMapTasks = Integer.parseInt(args[1]);\r\n            mrOutDir = args[2];\r\n            if (mrOutDir.startsWith(\"-\")) {\r\n                System.err.println(\"Missing output file parameter, instead got: \" + mrOutDir);\r\n                System.err.println(USAGE);\r\n                return -1;\r\n            }\r\n        } else {\r\n            System.err.println(USAGE);\r\n            ToolRunner.printGenericCommandUsage(System.err);\r\n            return -1;\r\n        }\r\n        String[] strippedArgs = new String[args.length - 3];\r\n        for (int i = 0; i < strippedArgs.length; i++) {\r\n            strippedArgs[i] = args[i + 3];\r\n        }\r\n        super.parseArgs(true, strippedArgs);\r\n    } catch (NumberFormatException e) {\r\n        System.err.println(\"Illegal parameter: \" + e.getLocalizedMessage());\r\n        System.err.println(USAGE);\r\n        return -1;\r\n    }\r\n    return 0;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\fs\\loadGenerator",
  "methodName" : "main",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void main(String[] args) throws Exception\n{\r\n    int res = ToolRunner.run(new Configuration(), new LoadGeneratorMR(), args);\r\n    System.exit(res);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\fs\\loadGenerator",
  "methodName" : "submitAsMapReduce",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 29,
  "sourceCodeText" : "int submitAsMapReduce()\n{\r\n    System.out.println(\"Running as a MapReduce job with \" + numMapTasks + \" mapTasks;  Output to file \" + mrOutDir);\r\n    Configuration conf = new Configuration(getConf());\r\n    conf.set(LG_ROOT, root.toString());\r\n    conf.setInt(LG_MAXDELAYBETWEENOPS, maxDelayBetweenOps);\r\n    conf.setInt(LG_NUMOFTHREADS, numOfThreads);\r\n    conf.set(LG_READPR, readProbs[0] + \"\");\r\n    conf.set(LG_WRITEPR, writeProbs[0] + \"\");\r\n    conf.setLong(LG_SEED, seed);\r\n    conf.setInt(LG_NUMMAPTASKS, numMapTasks);\r\n    if (scriptFile == null && durations[0] <= 0) {\r\n        System.err.println(\"When run as a MapReduce job, elapsed Time or ScriptFile must be specified\");\r\n        System.exit(-1);\r\n    }\r\n    conf.setLong(LG_ELAPSEDTIME, durations[0]);\r\n    conf.setLong(LG_STARTTIME, startTime);\r\n    if (scriptFile != null) {\r\n        conf.set(LG_SCRIPTFILE, scriptFile);\r\n    }\r\n    conf.set(LG_FLAGFILE, flagFile.toString());\r\n    JobConf jobConf = new JobConf(conf, LoadGenerator.class);\r\n    jobConf.setJobName(\"NNLoadGeneratorViaMR\");\r\n    jobConf.setNumMapTasks(numMapTasks);\r\n    jobConf.setNumReduceTasks(1);\r\n    jobConf.setOutputKeyClass(Text.class);\r\n    jobConf.setOutputValueClass(IntWritable.class);\r\n    jobConf.setMapperClass(MapperThatRunsNNLoadGenerator.class);\r\n    jobConf.setReducerClass(ReducerThatCollectsLGdata.class);\r\n    jobConf.setInputFormat(DummyInputFormat.class);\r\n    jobConf.setOutputFormat(TextOutputFormat.class);\r\n    jobConf.setMaxMapAttempts(1);\r\n    jobConf.setSpeculativeExecution(false);\r\n    FileOutputFormat.setOutputPath(jobConf, new Path(mrOutDir));\r\n    try {\r\n        JobClient.runJob(jobConf);\r\n    } catch (IOException e) {\r\n        System.err.println(\"Failed to run job: \" + e.getMessage());\r\n        return -1;\r\n    }\r\n    return 0;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "printUsage",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void printUsage()\n{\r\n    System.err.println(\"sortvalidate [-m <maps>] [-r <reduces>] [-deep] \" + \"-sortInput <sort-input-dir> -sortOutput <sort-output-dir>\");\r\n    System.exit(1);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "deduceInputFile",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "IntWritable deduceInputFile(JobConf job)\n{\r\n    Path[] inputPaths = FileInputFormat.getInputPaths(job);\r\n    Path inputFile = new Path(job.get(JobContext.MAP_INPUT_FILE));\r\n    return (inputFile.getParent().equals(inputPaths[0])) ? sortInput : sortOutput;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "pair",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "byte[] pair(BytesWritable a, BytesWritable b)\n{\r\n    byte[] pairData = new byte[a.getLength() + b.getLength()];\r\n    System.arraycopy(a.getBytes(), 0, pairData, 0, a.getLength());\r\n    System.arraycopy(b.getBytes(), 0, pairData, a.getLength(), b.getLength());\r\n    return pairData;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "run",
  "errType" : [ "NumberFormatException", "ArrayIndexOutOfBoundsException" ],
  "containingMethodsNum" : 17,
  "sourceCodeText" : "int run(String[] args) throws Exception\n{\r\n    Configuration defaults = getConf();\r\n    int noMaps = -1, noReduces = -1;\r\n    Path sortInput = null, sortOutput = null;\r\n    boolean deepTest = false;\r\n    for (int i = 0; i < args.length; ++i) {\r\n        try {\r\n            if (\"-m\".equals(args[i])) {\r\n                noMaps = Integer.parseInt(args[++i]);\r\n            } else if (\"-r\".equals(args[i])) {\r\n                noReduces = Integer.parseInt(args[++i]);\r\n            } else if (\"-sortInput\".equals(args[i])) {\r\n                sortInput = new Path(args[++i]);\r\n            } else if (\"-sortOutput\".equals(args[i])) {\r\n                sortOutput = new Path(args[++i]);\r\n            } else if (\"-deep\".equals(args[i])) {\r\n                deepTest = true;\r\n            } else {\r\n                printUsage();\r\n                return -1;\r\n            }\r\n        } catch (NumberFormatException except) {\r\n            System.err.println(\"ERROR: Integer expected instead of \" + args[i]);\r\n            printUsage();\r\n            return -1;\r\n        } catch (ArrayIndexOutOfBoundsException except) {\r\n            System.err.println(\"ERROR: Required parameter missing from \" + args[i - 1]);\r\n            printUsage();\r\n            return -1;\r\n        }\r\n    }\r\n    if (sortInput == null || sortOutput == null) {\r\n        printUsage();\r\n        return -2;\r\n    }\r\n    RecordStatsChecker.checkRecords(defaults, sortInput, sortOutput);\r\n    if (deepTest) {\r\n        RecordChecker.checkRecords(defaults, noMaps, noReduces, sortInput, sortOutput);\r\n    }\r\n    System.out.println(\"\\nSUCCESS! Validated the MapReduce framework's 'sort'\" + \" successfully.\");\r\n    return 0;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "main",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void main(String[] args) throws Exception\n{\r\n    int res = ToolRunner.run(new Configuration(), new SortValidator(), args);\r\n    System.exit(res);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\fs\\slive",
  "methodName" : "toString",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String toString()\n{\r\n    return getKeyString() + \" (\" + this.value + \")\";\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\fs\\slive",
  "methodName" : "merge",
  "errType" : [ "NumberFormatException", "NumberFormatException", "NumberFormatException", "NumberFormatException" ],
  "containingMethodsNum" : 26,
  "sourceCodeText" : "OperationOutput merge(OperationOutput o1, OperationOutput o2)\n{\r\n    if (o1.getMeasurementType().equals(o2.getMeasurementType()) && o1.getOperationType().equals(o2.getOperationType())) {\r\n        Object newvalue = null;\r\n        OutputType newtype = null;\r\n        String opType = o1.getOperationType();\r\n        String mType = o1.getMeasurementType();\r\n        if (o1.getOutputType() == OutputType.STRING || o2.getOutputType() == OutputType.STRING) {\r\n            newtype = OutputType.STRING;\r\n            StringBuilder str = new StringBuilder();\r\n            str.append(o1.getValue());\r\n            str.append(STRING_SEP);\r\n            str.append(o2.getValue());\r\n            newvalue = str.toString();\r\n        } else if (o1.getOutputType() == OutputType.DOUBLE || o2.getOutputType() == OutputType.DOUBLE) {\r\n            newtype = OutputType.DOUBLE;\r\n            try {\r\n                newvalue = Double.parseDouble(o1.getValue().toString()) + Double.parseDouble(o2.getValue().toString());\r\n            } catch (NumberFormatException e) {\r\n                throw new IllegalArgumentException(\"Unable to combine a type with a double \" + o1 + \" & \" + o2, e);\r\n            }\r\n        } else if (o1.getOutputType() == OutputType.FLOAT || o2.getOutputType() == OutputType.FLOAT) {\r\n            newtype = OutputType.FLOAT;\r\n            try {\r\n                newvalue = Float.parseFloat(o1.getValue().toString()) + Float.parseFloat(o2.getValue().toString());\r\n            } catch (NumberFormatException e) {\r\n                throw new IllegalArgumentException(\"Unable to combine a type with a float \" + o1 + \" & \" + o2, e);\r\n            }\r\n        } else if (o1.getOutputType() == OutputType.LONG || o2.getOutputType() == OutputType.LONG) {\r\n            newtype = OutputType.LONG;\r\n            try {\r\n                newvalue = Long.parseLong(o1.getValue().toString()) + Long.parseLong(o2.getValue().toString());\r\n            } catch (NumberFormatException e) {\r\n                throw new IllegalArgumentException(\"Unable to combine a type with a long \" + o1 + \" & \" + o2, e);\r\n            }\r\n        } else if (o1.getOutputType() == OutputType.INTEGER || o2.getOutputType() == OutputType.INTEGER) {\r\n            newtype = OutputType.INTEGER;\r\n            try {\r\n                newvalue = Integer.parseInt(o1.getValue().toString()) + Integer.parseInt(o2.getValue().toString());\r\n            } catch (NumberFormatException e) {\r\n                throw new IllegalArgumentException(\"Unable to combine a type with an int \" + o1 + \" & \" + o2, e);\r\n            }\r\n        }\r\n        return new OperationOutput(newtype, opType, mType, newvalue);\r\n    } else {\r\n        throw new IllegalArgumentException(\"Unable to combine dissimilar types \" + o1 + \" & \" + o2);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 4,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\fs\\slive",
  "methodName" : "getKeyString",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "String getKeyString()\n{\r\n    StringBuilder str = new StringBuilder();\r\n    str.append(getOutputType().name());\r\n    str.append(TYPE_SEP);\r\n    str.append(getOperationType());\r\n    str.append(MEASUREMENT_SEP);\r\n    str.append(getMeasurementType());\r\n    return str.toString();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\fs\\slive",
  "methodName" : "getKey",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Text getKey()\n{\r\n    return new Text(getKeyString());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\fs\\slive",
  "methodName" : "getOutputValue",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "Text getOutputValue()\n{\r\n    StringBuilder valueStr = new StringBuilder();\r\n    valueStr.append(getValue());\r\n    return new Text(valueStr.toString());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\fs\\slive",
  "methodName" : "getValue",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Object getValue()\n{\r\n    return value;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\fs\\slive",
  "methodName" : "getOutputType",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "OutputType getOutputType()\n{\r\n    return dataType;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\fs\\slive",
  "methodName" : "getOperationType",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getOperationType()\n{\r\n    return opType;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\fs\\slive",
  "methodName" : "getMeasurementType",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getMeasurementType()\n{\r\n    return measurementType;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "testOutputFilesFilter",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void testOutputFilesFilter()\n{\r\n    PathFilter filter = new Utils.OutputFileUtils.OutputFilesFilter();\r\n    for (Path p : LOG_PATHS) {\r\n        assertFalse(filter.accept(p));\r\n    }\r\n    for (Path p : SUCCEEDED_PATHS) {\r\n        assertFalse(filter.accept(p));\r\n    }\r\n    for (Path p : PASS_PATHS) {\r\n        assertTrue(filter.accept(p));\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "testLogFilter",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void testLogFilter()\n{\r\n    PathFilter filter = new Utils.OutputFileUtils.OutputLogFilter();\r\n    for (Path p : LOG_PATHS) {\r\n        assertFalse(filter.accept(p));\r\n    }\r\n    for (Path p : SUCCEEDED_PATHS) {\r\n        assertTrue(filter.accept(p));\r\n    }\r\n    for (Path p : PASS_PATHS) {\r\n        assertTrue(filter.accept(p));\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\db",
  "methodName" : "testConstructQuery",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void testConstructQuery()\n{\r\n    String actual = format.constructQuery(\"hadoop_output\", fieldNames);\r\n    assertEquals(expected, actual);\r\n    actual = format.constructQuery(\"hadoop_output\", nullFieldNames);\r\n    assertEquals(nullExpected, actual);\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\db",
  "methodName" : "testDB2ConstructQuery",
  "errType" : [ "IllegalAccessException|NoSuchFieldException" ],
  "containingMethodsNum" : 11,
  "sourceCodeText" : "void testDB2ConstructQuery()\n{\r\n    String db2expected = StringUtils.removeEnd(expected, \";\");\r\n    String db2nullExpected = StringUtils.removeEnd(nullExpected, \";\");\r\n    try {\r\n        Class<?> clazz = this.format.getClass();\r\n        Field field = clazz.getDeclaredField(\"dbProductName\");\r\n        field.setAccessible(true);\r\n        field.set(format, \"DB2\");\r\n    } catch (IllegalAccessException | NoSuchFieldException e) {\r\n        fail(e.getMessage());\r\n    }\r\n    String actual = format.constructQuery(\"hadoop_output\", fieldNames);\r\n    assertEquals(db2expected, actual);\r\n    actual = format.constructQuery(\"hadoop_output\", nullFieldNames);\r\n    assertEquals(db2nullExpected, actual);\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\db",
  "methodName" : "testORACLEConstructQuery",
  "errType" : [ "IllegalAccessException|NoSuchFieldException" ],
  "containingMethodsNum" : 11,
  "sourceCodeText" : "void testORACLEConstructQuery()\n{\r\n    String oracleExpected = StringUtils.removeEnd(expected, \";\");\r\n    String oracleNullExpected = StringUtils.removeEnd(nullExpected, \";\");\r\n    try {\r\n        Class<?> clazz = this.format.getClass();\r\n        Field field = clazz.getDeclaredField(\"dbProductName\");\r\n        field.setAccessible(true);\r\n        field.set(format, \"ORACLE\");\r\n    } catch (IllegalAccessException | NoSuchFieldException e) {\r\n        fail(e.getMessage());\r\n    }\r\n    String actual = format.constructQuery(\"hadoop_output\", fieldNames);\r\n    assertEquals(oracleExpected, actual);\r\n    actual = format.constructQuery(\"hadoop_output\", nullFieldNames);\r\n    assertEquals(oracleNullExpected, actual);\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\db",
  "methodName" : "testSetOutput",
  "errType" : null,
  "containingMethodsNum" : 12,
  "sourceCodeText" : "void testSetOutput() throws IOException\n{\r\n    Job job = Job.getInstance(new Configuration());\r\n    DBOutputFormat.setOutput(job, \"hadoop_output\", fieldNames);\r\n    DBConfiguration dbConf = new DBConfiguration(job.getConfiguration());\r\n    String actual = format.constructQuery(dbConf.getOutputTableName(), dbConf.getOutputFieldNames());\r\n    assertEquals(expected, actual);\r\n    job = Job.getInstance(new Configuration());\r\n    dbConf = new DBConfiguration(job.getConfiguration());\r\n    DBOutputFormat.setOutput(job, \"hadoop_output\", nullFieldNames.length);\r\n    assertNull(dbConf.getOutputFieldNames());\r\n    assertEquals(nullFieldNames.length, dbConf.getOutputFieldCount());\r\n    actual = format.constructQuery(dbConf.getOutputTableName(), new String[dbConf.getOutputFieldCount()]);\r\n    assertEquals(nullExpected, actual);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\fs\\slive",
  "methodName" : "run",
  "errType" : [ "Exception", "Exception", "Exception", "Exception", "Exception", "Exception" ],
  "containingMethodsNum" : 21,
  "sourceCodeText" : "int run(String[] args)\n{\r\n    ParsedOutput parsedOpts = null;\r\n    try {\r\n        ArgumentParser argHolder = new ArgumentParser(args);\r\n        parsedOpts = argHolder.parse();\r\n        if (parsedOpts.shouldOutputHelp()) {\r\n            parsedOpts.outputHelp();\r\n            return 1;\r\n        }\r\n    } catch (Exception e) {\r\n        LOG.error(\"Unable to parse arguments due to error: \", e);\r\n        return 1;\r\n    }\r\n    LOG.info(\"Running with option list \" + Helper.stringifyArray(args, \" \"));\r\n    ConfigExtractor config = null;\r\n    try {\r\n        ConfigMerger cfgMerger = new ConfigMerger();\r\n        Configuration cfg = cfgMerger.getMerged(parsedOpts, new Configuration(base));\r\n        if (cfg != null) {\r\n            config = new ConfigExtractor(cfg);\r\n        }\r\n    } catch (Exception e) {\r\n        LOG.error(\"Unable to merge config due to error: \", e);\r\n        return 1;\r\n    }\r\n    if (config == null) {\r\n        LOG.error(\"Unable to merge config & options!\");\r\n        return 1;\r\n    }\r\n    try {\r\n        LOG.info(\"Options are:\");\r\n        ConfigExtractor.dumpOptions(config);\r\n    } catch (Exception e) {\r\n        LOG.error(\"Unable to dump options due to error: \", e);\r\n        return 1;\r\n    }\r\n    boolean jobOk = false;\r\n    try {\r\n        LOG.info(\"Running job:\");\r\n        runJob(config);\r\n        jobOk = true;\r\n    } catch (Exception e) {\r\n        LOG.error(\"Unable to run job due to error: \", e);\r\n    }\r\n    if (jobOk) {\r\n        try {\r\n            LOG.info(\"Reporting on job:\");\r\n            writeReport(config);\r\n        } catch (Exception e) {\r\n            LOG.error(\"Unable to report on job due to error: \", e);\r\n        }\r\n    }\r\n    boolean cleanUp = getBool(parsedOpts.getValue(ConfigOption.CLEANUP.getOpt()));\r\n    if (cleanUp) {\r\n        try {\r\n            LOG.info(\"Cleaning up job:\");\r\n            cleanup(config);\r\n        } catch (Exception e) {\r\n            LOG.error(\"Unable to cleanup job due to error: \", e);\r\n        }\r\n    }\r\n    if (jobOk) {\r\n        return 0;\r\n    }\r\n    return 1;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 6,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\fs\\slive",
  "methodName" : "getBool",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "boolean getBool(String val)\n{\r\n    if (val == null) {\r\n        return false;\r\n    }\r\n    String cleanupOpt = StringUtils.toLowerCase(val).trim();\r\n    if (cleanupOpt.equals(\"true\") || cleanupOpt.equals(\"1\")) {\r\n        return true;\r\n    } else {\r\n        return false;\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\fs\\slive",
  "methodName" : "getJob",
  "errType" : null,
  "containingMethodsNum" : 12,
  "sourceCodeText" : "JobConf getJob(ConfigExtractor config) throws IOException\n{\r\n    JobConf job = new JobConf(config.getConfig(), SliveTest.class);\r\n    job.setInputFormat(DummyInputFormat.class);\r\n    FileOutputFormat.setOutputPath(job, config.getOutputPath());\r\n    job.setMapperClass(SliveMapper.class);\r\n    job.setPartitionerClass(SlivePartitioner.class);\r\n    job.setReducerClass(SliveReducer.class);\r\n    job.setOutputKeyClass(Text.class);\r\n    job.setOutputValueClass(Text.class);\r\n    job.setOutputFormat(TextOutputFormat.class);\r\n    TextOutputFormat.setCompressOutput(job, false);\r\n    job.setNumReduceTasks(config.getReducerAmount());\r\n    job.setNumMapTasks(config.getMapAmount());\r\n    return job;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\fs\\slive",
  "methodName" : "runJob",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void runJob(ConfigExtractor config) throws IOException\n{\r\n    JobClient.runJob(getJob(config));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\fs\\slive",
  "methodName" : "writeReport",
  "errType" : null,
  "containingMethodsNum" : 24,
  "sourceCodeText" : "void writeReport(ConfigExtractor cfg) throws Exception\n{\r\n    Path dn = cfg.getOutputPath();\r\n    LOG.info(\"Writing report using contents of \" + dn);\r\n    FileSystem fs = dn.getFileSystem(cfg.getConfig());\r\n    FileStatus[] reduceFiles = fs.listStatus(dn);\r\n    BufferedReader fileReader = null;\r\n    PrintWriter reportWriter = null;\r\n    try {\r\n        List<OperationOutput> noOperations = new ArrayList<OperationOutput>();\r\n        Map<String, List<OperationOutput>> splitTypes = new TreeMap<String, List<OperationOutput>>();\r\n        for (FileStatus fn : reduceFiles) {\r\n            if (!fn.getPath().getName().startsWith(\"part\"))\r\n                continue;\r\n            fileReader = new BufferedReader(new InputStreamReader(new DataInputStream(fs.open(fn.getPath()))));\r\n            String line;\r\n            while ((line = fileReader.readLine()) != null) {\r\n                String[] pieces = line.split(\"\\t\", 2);\r\n                if (pieces.length == 2) {\r\n                    OperationOutput data = new OperationOutput(pieces[0], pieces[1]);\r\n                    String op = (data.getOperationType());\r\n                    if (op != null) {\r\n                        List<OperationOutput> opList = splitTypes.get(op);\r\n                        if (opList == null) {\r\n                            opList = new ArrayList<OperationOutput>();\r\n                        }\r\n                        opList.add(data);\r\n                        splitTypes.put(op, opList);\r\n                    } else {\r\n                        noOperations.add(data);\r\n                    }\r\n                } else {\r\n                    throw new IOException(\"Unparseable line \" + line);\r\n                }\r\n            }\r\n            fileReader.close();\r\n            fileReader = null;\r\n        }\r\n        File resFile = null;\r\n        if (cfg.getResultFile() != null) {\r\n            resFile = new File(cfg.getResultFile());\r\n        }\r\n        if (resFile != null) {\r\n            LOG.info(\"Report results being placed to logging output and to file \" + resFile.getCanonicalPath());\r\n            reportWriter = new PrintWriter(new FileOutputStream(resFile));\r\n        } else {\r\n            LOG.info(\"Report results being placed to logging output\");\r\n        }\r\n        ReportWriter reporter = new ReportWriter();\r\n        if (!noOperations.isEmpty()) {\r\n            reporter.basicReport(noOperations, reportWriter);\r\n        }\r\n        for (String opType : splitTypes.keySet()) {\r\n            reporter.opReport(opType, splitTypes.get(opType), reportWriter);\r\n        }\r\n    } finally {\r\n        if (fileReader != null) {\r\n            fileReader.close();\r\n        }\r\n        if (reportWriter != null) {\r\n            reportWriter.close();\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\fs\\slive",
  "methodName" : "cleanup",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void cleanup(ConfigExtractor cfg) throws IOException\n{\r\n    Path base = cfg.getBaseDirectory();\r\n    if (base != null) {\r\n        LOG.info(\"Attempting to recursively delete \" + base);\r\n        FileSystem fs = base.getFileSystem(cfg.getConfig());\r\n        fs.delete(base, true);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\fs\\slive",
  "methodName" : "main",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void main(String[] args) throws Exception\n{\r\n    Configuration startCfg = new Configuration(true);\r\n    SliveTest runner = new SliveTest(startCfg);\r\n    int ec = ToolRunner.run(runner, args);\r\n    System.exit(ec);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\fs\\slive",
  "methodName" : "getConf",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Configuration getConf()\n{\r\n    return this.base;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\fs\\slive",
  "methodName" : "setConf",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setConf(Configuration conf)\n{\r\n    this.base = conf;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\join",
  "methodName" : "makeTuple",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "TupleWritable makeTuple(Writable[] writs)\n{\r\n    Writable[] sub1 = { writs[1], writs[2] };\r\n    Writable[] sub3 = { writs[4], writs[5] };\r\n    Writable[] sub2 = { writs[3], new TupleWritable(sub3), writs[6] };\r\n    Writable[] vals = { writs[0], new TupleWritable(sub1), new TupleWritable(sub2), writs[7], writs[8], writs[9] };\r\n    TupleWritable ret = new TupleWritable(vals);\r\n    for (int i = 0; i < 6; ++i) {\r\n        ret.setWritten(i);\r\n    }\r\n    ((TupleWritable) sub2[1]).setWritten(0);\r\n    ((TupleWritable) sub2[1]).setWritten(1);\r\n    ((TupleWritable) vals[1]).setWritten(0);\r\n    ((TupleWritable) vals[1]).setWritten(1);\r\n    for (int i = 0; i < 3; ++i) {\r\n        ((TupleWritable) vals[2]).setWritten(i);\r\n    }\r\n    return ret;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\join",
  "methodName" : "makeRandomWritables",
  "errType" : null,
  "containingMethodsNum" : 10,
  "sourceCodeText" : "Writable[] makeRandomWritables()\n{\r\n    Random r = new Random();\r\n    Writable[] writs = { new BooleanWritable(r.nextBoolean()), new FloatWritable(r.nextFloat()), new FloatWritable(r.nextFloat()), new IntWritable(r.nextInt()), new LongWritable(r.nextLong()), new BytesWritable(\"dingo\".getBytes()), new LongWritable(r.nextLong()), new IntWritable(r.nextInt()), new BytesWritable(\"yak\".getBytes()), new IntWritable(r.nextInt()) };\r\n    return writs;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\join",
  "methodName" : "makeRandomWritables",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Writable[] makeRandomWritables(int numWrits)\n{\r\n    Writable[] writs = makeRandomWritables();\r\n    Writable[] manyWrits = new Writable[numWrits];\r\n    for (int i = 0; i < manyWrits.length; i++) {\r\n        manyWrits[i] = writs[i % writs.length];\r\n    }\r\n    return manyWrits;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\join",
  "methodName" : "verifIter",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "int verifIter(Writable[] writs, TupleWritable t, int i)\n{\r\n    for (Writable w : t) {\r\n        if (w instanceof TupleWritable) {\r\n            i = verifIter(writs, ((TupleWritable) w), i);\r\n            continue;\r\n        }\r\n        assertTrue(\"Bad value\", w.equals(writs[i++]));\r\n    }\r\n    return i;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\join",
  "methodName" : "testIterable",
  "errType" : null,
  "containingMethodsNum" : 12,
  "sourceCodeText" : "void testIterable() throws Exception\n{\r\n    Random r = new Random();\r\n    Writable[] writs = { new BooleanWritable(r.nextBoolean()), new FloatWritable(r.nextFloat()), new FloatWritable(r.nextFloat()), new IntWritable(r.nextInt()), new LongWritable(r.nextLong()), new BytesWritable(\"dingo\".getBytes()), new LongWritable(r.nextLong()), new IntWritable(r.nextInt()), new BytesWritable(\"yak\".getBytes()), new IntWritable(r.nextInt()) };\r\n    TupleWritable t = new TupleWritable(writs);\r\n    for (int i = 0; i < 6; ++i) {\r\n        t.setWritten(i);\r\n    }\r\n    verifIter(writs, t, 0);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\join",
  "methodName" : "testNestedIterable",
  "errType" : null,
  "containingMethodsNum" : 12,
  "sourceCodeText" : "void testNestedIterable() throws Exception\n{\r\n    Random r = new Random();\r\n    Writable[] writs = { new BooleanWritable(r.nextBoolean()), new FloatWritable(r.nextFloat()), new FloatWritable(r.nextFloat()), new IntWritable(r.nextInt()), new LongWritable(r.nextLong()), new BytesWritable(\"dingo\".getBytes()), new LongWritable(r.nextLong()), new IntWritable(r.nextInt()), new BytesWritable(\"yak\".getBytes()), new IntWritable(r.nextInt()) };\r\n    TupleWritable sTuple = makeTuple(writs);\r\n    assertTrue(\"Bad count\", writs.length == verifIter(writs, sTuple, 0));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\join",
  "methodName" : "testWritable",
  "errType" : null,
  "containingMethodsNum" : 15,
  "sourceCodeText" : "void testWritable() throws Exception\n{\r\n    Random r = new Random();\r\n    Writable[] writs = { new BooleanWritable(r.nextBoolean()), new FloatWritable(r.nextFloat()), new FloatWritable(r.nextFloat()), new IntWritable(r.nextInt()), new LongWritable(r.nextLong()), new BytesWritable(\"dingo\".getBytes()), new LongWritable(r.nextLong()), new IntWritable(r.nextInt()), new BytesWritable(\"yak\".getBytes()), new IntWritable(r.nextInt()) };\r\n    TupleWritable sTuple = makeTuple(writs);\r\n    ByteArrayOutputStream out = new ByteArrayOutputStream();\r\n    sTuple.write(new DataOutputStream(out));\r\n    ByteArrayInputStream in = new ByteArrayInputStream(out.toByteArray());\r\n    TupleWritable dTuple = new TupleWritable();\r\n    dTuple.readFields(new DataInputStream(in));\r\n    assertTrue(\"Failed to write/read tuple\", sTuple.equals(dTuple));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\join",
  "methodName" : "testWideWritable",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void testWideWritable() throws Exception\n{\r\n    Writable[] manyWrits = makeRandomWritables(131);\r\n    TupleWritable sTuple = new TupleWritable(manyWrits);\r\n    for (int i = 0; i < manyWrits.length; i++) {\r\n        if (i % 3 == 0) {\r\n            sTuple.setWritten(i);\r\n        }\r\n    }\r\n    ByteArrayOutputStream out = new ByteArrayOutputStream();\r\n    sTuple.write(new DataOutputStream(out));\r\n    ByteArrayInputStream in = new ByteArrayInputStream(out.toByteArray());\r\n    TupleWritable dTuple = new TupleWritable();\r\n    dTuple.readFields(new DataInputStream(in));\r\n    assertThat(dTuple).withFailMessage(\"Failed to write/read tuple\").isEqualTo(sTuple);\r\n    assertEquals(\"All tuple data has not been read from the stream\", -1, in.read());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\join",
  "methodName" : "testWideWritable2",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void testWideWritable2() throws Exception\n{\r\n    Writable[] manyWrits = makeRandomWritables(71);\r\n    TupleWritable sTuple = new TupleWritable(manyWrits);\r\n    for (int i = 0; i < manyWrits.length; i++) {\r\n        sTuple.setWritten(i);\r\n    }\r\n    ByteArrayOutputStream out = new ByteArrayOutputStream();\r\n    sTuple.write(new DataOutputStream(out));\r\n    ByteArrayInputStream in = new ByteArrayInputStream(out.toByteArray());\r\n    TupleWritable dTuple = new TupleWritable();\r\n    dTuple.readFields(new DataInputStream(in));\r\n    assertThat(dTuple).withFailMessage(\"Failed to write/read tuple\").isEqualTo(sTuple);\r\n    assertEquals(\"All tuple data has not been read from the stream\", -1, in.read());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\join",
  "methodName" : "testSparseWideWritable",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void testSparseWideWritable() throws Exception\n{\r\n    Writable[] manyWrits = makeRandomWritables(131);\r\n    TupleWritable sTuple = new TupleWritable(manyWrits);\r\n    for (int i = 0; i < manyWrits.length; i++) {\r\n        if (i % 65 == 0) {\r\n            sTuple.setWritten(i);\r\n        }\r\n    }\r\n    ByteArrayOutputStream out = new ByteArrayOutputStream();\r\n    sTuple.write(new DataOutputStream(out));\r\n    ByteArrayInputStream in = new ByteArrayInputStream(out.toByteArray());\r\n    TupleWritable dTuple = new TupleWritable();\r\n    dTuple.readFields(new DataInputStream(in));\r\n    assertThat(dTuple).withFailMessage(\"Failed to write/read tuple\").isEqualTo(sTuple);\r\n    assertEquals(\"All tuple data has not been read from the stream\", -1, in.read());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\join",
  "methodName" : "testWideTuple",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void testWideTuple() throws Exception\n{\r\n    Text emptyText = new Text(\"Should be empty\");\r\n    Writable[] values = new Writable[64];\r\n    Arrays.fill(values, emptyText);\r\n    values[42] = new Text(\"Number 42\");\r\n    TupleWritable tuple = new TupleWritable(values);\r\n    tuple.setWritten(42);\r\n    for (int pos = 0; pos < tuple.size(); pos++) {\r\n        boolean has = tuple.has(pos);\r\n        if (pos == 42) {\r\n            assertTrue(has);\r\n        } else {\r\n            assertFalse(\"Tuple position is incorrectly labelled as set: \" + pos, has);\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\join",
  "methodName" : "testWideTuple2",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void testWideTuple2() throws Exception\n{\r\n    Text emptyText = new Text(\"Should be empty\");\r\n    Writable[] values = new Writable[64];\r\n    Arrays.fill(values, emptyText);\r\n    values[9] = new Text(\"Number 9\");\r\n    TupleWritable tuple = new TupleWritable(values);\r\n    tuple.setWritten(9);\r\n    for (int pos = 0; pos < tuple.size(); pos++) {\r\n        boolean has = tuple.has(pos);\r\n        if (pos == 9) {\r\n            assertTrue(has);\r\n        } else {\r\n            assertFalse(\"Tuple position is incorrectly labelled as set: \" + pos, has);\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\join",
  "methodName" : "testWideTupleBoundary",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void testWideTupleBoundary() throws Exception\n{\r\n    Text emptyText = new Text(\"Should not be set written\");\r\n    Writable[] values = new Writable[65];\r\n    Arrays.fill(values, emptyText);\r\n    values[64] = new Text(\"Should be the only value set written\");\r\n    TupleWritable tuple = new TupleWritable(values);\r\n    tuple.setWritten(64);\r\n    for (int pos = 0; pos < tuple.size(); pos++) {\r\n        boolean has = tuple.has(pos);\r\n        if (pos == 64) {\r\n            assertTrue(has);\r\n        } else {\r\n            assertFalse(\"Tuple position is incorrectly labelled as set: \" + pos, has);\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "newDFSCluster",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "MiniDFSCluster newDFSCluster(JobConf conf) throws Exception\n{\r\n    return new MiniDFSCluster.Builder(conf).numDataNodes(4).racks(new String[] { \"/rack0\", \"/rack0\", \"/rack1\", \"/rack1\" }).hosts(new String[] { \"host0\", \"host1\", \"host2\", \"host3\" }).build();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "testLocality",
  "errType" : null,
  "containingMethodsNum" : 22,
  "sourceCodeText" : "void testLocality() throws Exception\n{\r\n    JobConf job = new JobConf(conf);\r\n    dfs = newDFSCluster(job);\r\n    FileSystem fs = dfs.getFileSystem();\r\n    System.out.println(\"FileSystem \" + fs.getUri());\r\n    Path inputDir = new Path(\"/foo/\");\r\n    String fileName = \"part-0000\";\r\n    createInputs(fs, inputDir, fileName);\r\n    TextInputFormat.addInputPath(job, inputDir);\r\n    TextInputFormat inFormat = new TextInputFormat();\r\n    inFormat.configure(job);\r\n    InputSplit[] splits = inFormat.getSplits(job, 1);\r\n    FileStatus fileStatus = fs.getFileStatus(new Path(inputDir, fileName));\r\n    BlockLocation[] locations = fs.getFileBlockLocations(fileStatus, 0, fileStatus.getLen());\r\n    System.out.println(\"Made splits\");\r\n    for (int i = 0; i < splits.length; ++i) {\r\n        FileSplit fileSplit = (FileSplit) splits[i];\r\n        System.out.println(\"File split: \" + fileSplit);\r\n        for (String h : fileSplit.getLocations()) {\r\n            System.out.println(\"Location: \" + h);\r\n        }\r\n        System.out.println(\"Block: \" + locations[i]);\r\n        assertEquals(locations[i].getOffset(), fileSplit.getStart());\r\n        assertEquals(locations[i].getLength(), fileSplit.getLength());\r\n        String[] blockLocs = locations[i].getHosts();\r\n        String[] splitLocs = fileSplit.getLocations();\r\n        assertEquals(2, blockLocs.length);\r\n        assertEquals(2, splitLocs.length);\r\n        assertTrue((blockLocs[0].equals(splitLocs[0]) && blockLocs[1].equals(splitLocs[1])) || (blockLocs[1].equals(splitLocs[0]) && blockLocs[0].equals(splitLocs[1])));\r\n    }\r\n    assertEquals(\"Expected value of \" + FileInputFormat.NUM_INPUT_FILES, 1, job.getLong(FileInputFormat.NUM_INPUT_FILES, 0));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "createInputs",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void createInputs(FileSystem fs, Path inDir, String fileName) throws IOException, TimeoutException, InterruptedException\n{\r\n    Path path = new Path(inDir, fileName);\r\n    final short replication = 2;\r\n    DataOutputStream out = fs.create(path, true, 4096, replication, 512, null);\r\n    for (int i = 0; i < 1000; ++i) {\r\n        out.writeChars(\"Hello\\n\");\r\n    }\r\n    out.close();\r\n    System.out.println(\"Wrote file\");\r\n    DFSTestUtil.waitReplication(fs, path, replication);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "testNumInputs",
  "errType" : null,
  "containingMethodsNum" : 10,
  "sourceCodeText" : "void testNumInputs() throws Exception\n{\r\n    JobConf job = new JobConf(conf);\r\n    dfs = newDFSCluster(job);\r\n    FileSystem fs = dfs.getFileSystem();\r\n    System.out.println(\"FileSystem \" + fs.getUri());\r\n    Path inputDir = new Path(\"/foo/\");\r\n    final int numFiles = 10;\r\n    String fileNameBase = \"part-0000\";\r\n    for (int i = 0; i < numFiles; ++i) {\r\n        createInputs(fs, inputDir, fileNameBase + String.valueOf(i));\r\n    }\r\n    createInputs(fs, inputDir, \"_meta\");\r\n    createInputs(fs, inputDir, \"_temp\");\r\n    TextInputFormat.addInputPath(job, inputDir);\r\n    TextInputFormat inFormat = new TextInputFormat();\r\n    inFormat.configure(job);\r\n    InputSplit[] splits = inFormat.getSplits(job, 1);\r\n    assertEquals(\"Expected value of \" + FileInputFormat.NUM_INPUT_FILES, numFiles, job.getLong(FileInputFormat.NUM_INPUT_FILES, 0));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "testMultiLevelInput",
  "errType" : [ "Exception" ],
  "containingMethodsNum" : 16,
  "sourceCodeText" : "void testMultiLevelInput() throws Exception\n{\r\n    JobConf job = new JobConf(conf);\r\n    job.setBoolean(\"dfs.replication.considerLoad\", false);\r\n    dfs = new MiniDFSCluster.Builder(job).racks(rack1).hosts(hosts1).build();\r\n    dfs.waitActive();\r\n    String namenode = (dfs.getFileSystem()).getUri().getHost() + \":\" + (dfs.getFileSystem()).getUri().getPort();\r\n    FileSystem fileSys = dfs.getFileSystem();\r\n    if (!fileSys.mkdirs(dir1)) {\r\n        throw new IOException(\"Mkdirs failed to create \" + root.toString());\r\n    }\r\n    writeFile(job, file1, (short) 1, 1);\r\n    writeFile(job, file2, (short) 1, 1);\r\n    DummyFileInputFormat inFormat = new DummyFileInputFormat();\r\n    inFormat.setInputPaths(job, root);\r\n    boolean exceptionThrown = false;\r\n    try {\r\n        InputSplit[] splits = inFormat.getSplits(job, 1);\r\n    } catch (Exception e) {\r\n        exceptionThrown = true;\r\n    }\r\n    assertTrue(\"Exception should be thrown by default for scanning a \" + \"directory with directories inside.\", exceptionThrown);\r\n    job.setBoolean(FileInputFormat.INPUT_DIR_RECURSIVE, true);\r\n    InputSplit[] splits = inFormat.getSplits(job, 1);\r\n    assertThat(splits.length).isEqualTo(2);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "testLastInputSplitAtSplitBoundary",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void testLastInputSplitAtSplitBoundary() throws Exception\n{\r\n    FileInputFormat fif = new FileInputFormatForTest(1024l * 1024 * 1024, 128l * 1024 * 1024);\r\n    JobConf job = new JobConf();\r\n    InputSplit[] splits = fif.getSplits(job, 8);\r\n    assertEquals(8, splits.length);\r\n    for (int i = 0; i < splits.length; i++) {\r\n        InputSplit split = splits[i];\r\n        assertEquals((\"host\" + i), split.getLocations()[0]);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "testLastInputSplitExceedingSplitBoundary",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void testLastInputSplitExceedingSplitBoundary() throws Exception\n{\r\n    FileInputFormat fif = new FileInputFormatForTest(1027l * 1024 * 1024, 128l * 1024 * 1024);\r\n    JobConf job = new JobConf();\r\n    InputSplit[] splits = fif.getSplits(job, 8);\r\n    assertEquals(8, splits.length);\r\n    for (int i = 0; i < splits.length; i++) {\r\n        InputSplit split = splits[i];\r\n        assertEquals((\"host\" + i), split.getLocations()[0]);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "testLastInputSplitSingleSplit",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void testLastInputSplitSingleSplit() throws Exception\n{\r\n    FileInputFormat fif = new FileInputFormatForTest(100l * 1024 * 1024, 128l * 1024 * 1024);\r\n    JobConf job = new JobConf();\r\n    InputSplit[] splits = fif.getSplits(job, 1);\r\n    assertEquals(1, splits.length);\r\n    for (int i = 0; i < splits.length; i++) {\r\n        InputSplit split = splits[i];\r\n        assertEquals((\"host\" + i), split.getLocations()[0]);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "writeFile",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void writeFile(Configuration conf, Path name, short replication, int numBlocks) throws IOException, TimeoutException, InterruptedException\n{\r\n    FileSystem fileSys = FileSystem.get(conf);\r\n    FSDataOutputStream stm = fileSys.create(name, true, conf.getInt(\"io.file.buffer.size\", 4096), replication, (long) BLOCKSIZE);\r\n    for (int i = 0; i < numBlocks; i++) {\r\n        stm.write(databuf);\r\n    }\r\n    stm.close();\r\n    DFSTestUtil.waitReplication(fileSys, name, replication);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "tearDown",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void tearDown() throws Exception\n{\r\n    if (dfs != null) {\r\n        dfs.shutdown();\r\n        dfs = null;\r\n    }\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "cleanData",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void cleanData(FileSystem fs, Path dirPath) throws IOException\n{\r\n    fs.delete(dirPath, true);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "generateRandomWord",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String generateRandomWord()\n{\r\n    return idFormat.format(rand.nextLong());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "generateRandomLine",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "String generateRandomLine()\n{\r\n    long r = rand.nextLong() % 7;\r\n    long n = r + 20;\r\n    StringBuffer sb = new StringBuffer();\r\n    for (int i = 0; i < n; i++) {\r\n        sb.append(generateRandomWord()).append(\" \");\r\n    }\r\n    sb.append(\"\\n\");\r\n    return sb.toString();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "generateData",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void generateData(FileSystem fs, Path dirPath) throws IOException\n{\r\n    FSDataOutputStream out = fs.create(new Path(dirPath, \"data.txt\"));\r\n    for (int i = 0; i < 10000; i++) {\r\n        String line = generateRandomLine();\r\n        out.write(line.getBytes(\"UTF-8\"));\r\n    }\r\n    out.close();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "createCopyJob",
  "errType" : null,
  "containingMethodsNum" : 10,
  "sourceCodeText" : "Job createCopyJob(Configuration conf, Path outdir, Path... indirs) throws Exception\n{\r\n    conf.setInt(MRJobConfig.NUM_MAPS, 3);\r\n    Job theJob = Job.getInstance(conf);\r\n    theJob.setJobName(\"DataMoveJob\");\r\n    FileInputFormat.setInputPaths(theJob, indirs);\r\n    theJob.setMapperClass(DataCopyMapper.class);\r\n    FileOutputFormat.setOutputPath(theJob, outdir);\r\n    theJob.setOutputKeyClass(Text.class);\r\n    theJob.setOutputValueClass(Text.class);\r\n    theJob.setReducerClass(DataCopyReducer.class);\r\n    theJob.setNumReduceTasks(1);\r\n    return theJob;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "createFailJob",
  "errType" : null,
  "containingMethodsNum" : 13,
  "sourceCodeText" : "Job createFailJob(Configuration conf, Path outdir, Path... indirs) throws Exception\n{\r\n    FileSystem fs = outdir.getFileSystem(conf);\r\n    if (fs.exists(outdir)) {\r\n        fs.delete(outdir, true);\r\n    }\r\n    conf.setInt(MRJobConfig.MAP_MAX_ATTEMPTS, 2);\r\n    Job theJob = Job.getInstance(conf);\r\n    theJob.setJobName(\"Fail-Job\");\r\n    FileInputFormat.setInputPaths(theJob, indirs);\r\n    theJob.setMapperClass(FailMapper.class);\r\n    theJob.setReducerClass(Reducer.class);\r\n    theJob.setNumReduceTasks(0);\r\n    FileOutputFormat.setOutputPath(theJob, outdir);\r\n    theJob.setOutputKeyClass(Text.class);\r\n    theJob.setOutputValueClass(Text.class);\r\n    return theJob;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "createKillJob",
  "errType" : null,
  "containingMethodsNum" : 9,
  "sourceCodeText" : "Job createKillJob(Configuration conf, Path outdir, Path... indirs) throws Exception\n{\r\n    Job theJob = Job.getInstance(conf);\r\n    theJob.setJobName(\"Kill-Job\");\r\n    FileInputFormat.setInputPaths(theJob, indirs);\r\n    theJob.setMapperClass(KillMapper.class);\r\n    theJob.setReducerClass(Reducer.class);\r\n    theJob.setNumReduceTasks(0);\r\n    FileOutputFormat.setOutputPath(theJob, outdir);\r\n    theJob.setOutputKeyClass(Text.class);\r\n    theJob.setOutputValueClass(Text.class);\r\n    return theJob;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "createJob",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Job createJob(Configuration conf, Path inDir, Path outDir, int numInputFiles, int numReds) throws IOException\n{\r\n    String input = \"The quick brown fox\\n\" + \"has many silly\\n\" + \"red fox sox\\n\";\r\n    return createJob(conf, inDir, outDir, numInputFiles, numReds, input);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "createJob",
  "errType" : null,
  "containingMethodsNum" : 13,
  "sourceCodeText" : "Job createJob(Configuration conf, Path inDir, Path outDir, int numInputFiles, int numReds, String input) throws IOException\n{\r\n    Job job = Job.getInstance(conf);\r\n    FileSystem fs = FileSystem.get(conf);\r\n    if (fs.exists(outDir)) {\r\n        fs.delete(outDir, true);\r\n    }\r\n    if (fs.exists(inDir)) {\r\n        fs.delete(inDir, true);\r\n    }\r\n    fs.mkdirs(inDir);\r\n    for (int i = 0; i < numInputFiles; ++i) {\r\n        DataOutputStream file = fs.create(new Path(inDir, \"part-\" + i));\r\n        file.writeBytes(input);\r\n        file.close();\r\n    }\r\n    FileInputFormat.setInputPaths(job, inDir);\r\n    FileOutputFormat.setOutputPath(job, outDir);\r\n    job.setNumReduceTasks(numReds);\r\n    return job;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "createDummyMapTaskAttemptContext",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "TaskAttemptContext createDummyMapTaskAttemptContext(Configuration conf)\n{\r\n    TaskAttemptID tid = new TaskAttemptID(\"jt\", 1, TaskType.MAP, 0, 0);\r\n    conf.set(MRJobConfig.TASK_ATTEMPT_ID, tid.toString());\r\n    return new TaskAttemptContextImpl(conf, tid);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "createDummyReporter",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "StatusReporter createDummyReporter()\n{\r\n    return new StatusReporter() {\r\n\r\n        public void setStatus(String s) {\r\n        }\r\n\r\n        public void progress() {\r\n        }\r\n\r\n        @Override\r\n        public float getProgress() {\r\n            return 0;\r\n        }\r\n\r\n        public Counter getCounter(Enum<?> name) {\r\n            return new Counters().findCounter(name);\r\n        }\r\n\r\n        public Counter getCounter(String group, String name) {\r\n            return new Counters().findCounter(group, name);\r\n        }\r\n    };\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "readOutput",
  "errType" : null,
  "containingMethodsNum" : 10,
  "sourceCodeText" : "String readOutput(Path outDir, Configuration conf) throws IOException\n{\r\n    FileSystem fs = outDir.getFileSystem(conf);\r\n    StringBuffer result = new StringBuffer();\r\n    Path[] fileList = FileUtil.stat2Paths(fs.listStatus(outDir, new Utils.OutputFileUtils.OutputFilesFilter()));\r\n    for (Path outputFile : fileList) {\r\n        LOG.info(\"Path\" + \": \" + outputFile);\r\n        BufferedReader file = new BufferedReader(new InputStreamReader(fs.open(outputFile)));\r\n        String line = file.readLine();\r\n        while (line != null) {\r\n            result.append(line);\r\n            result.append(\"\\n\");\r\n            line = file.readLine();\r\n        }\r\n        file.close();\r\n    }\r\n    return result.toString();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "readTaskLog",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "String readTaskLog(TaskLog.LogName filter, org.apache.hadoop.mapred.TaskAttemptID taskId, boolean isCleanup) throws IOException\n{\r\n    StringBuffer result = new StringBuffer();\r\n    int res;\r\n    InputStream taskLogReader = new TaskLog.Reader(taskId, filter, 0, -1, isCleanup);\r\n    byte[] b = new byte[65536];\r\n    while (true) {\r\n        res = taskLogReader.read(b);\r\n        if (res > 0) {\r\n            result.append(new String(b));\r\n        } else {\r\n            break;\r\n        }\r\n    }\r\n    taskLogReader.close();\r\n    String str = result.toString();\r\n    str = str.trim();\r\n    return str;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "initFiles",
  "errType" : null,
  "containingMethodsNum" : 12,
  "sourceCodeText" : "Path initFiles(FileSystem fs, int numFiles, int numBytes) throws IOException\n{\r\n    Path dir = new Path(System.getProperty(\"test.build.data\", \".\") + \"/mapred\");\r\n    Path multiFileDir = new Path(dir, \"test.multifile\");\r\n    fs.delete(multiFileDir, true);\r\n    fs.mkdirs(multiFileDir);\r\n    LOG.info(\"Creating \" + numFiles + \" file(s) in \" + multiFileDir);\r\n    for (int i = 0; i < numFiles; i++) {\r\n        Path path = new Path(multiFileDir, \"file_\" + i);\r\n        FSDataOutputStream out = fs.create(path);\r\n        if (numBytes == -1) {\r\n            numBytes = rand.nextInt(MAX_BYTES);\r\n        }\r\n        for (int j = 0; j < numBytes; j++) {\r\n            out.write(rand.nextInt());\r\n        }\r\n        out.close();\r\n        if (LOG.isDebugEnabled()) {\r\n            LOG.debug(\"Created file \" + path + \" with length \" + numBytes);\r\n        }\r\n        lengths.put(path.getName(), new Long(numBytes));\r\n    }\r\n    FileInputFormat.setInputPaths(job, multiFileDir);\r\n    return multiFileDir;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "testFormat",
  "errType" : null,
  "containingMethodsNum" : 24,
  "sourceCodeText" : "void testFormat() throws IOException\n{\r\n    LOG.info(\"Test started\");\r\n    LOG.info(\"Max split count           = \" + MAX_SPLIT_COUNT);\r\n    LOG.info(\"Split count increment     = \" + SPLIT_COUNT_INCR);\r\n    LOG.info(\"Max bytes per file        = \" + MAX_BYTES);\r\n    LOG.info(\"Max number of files       = \" + MAX_NUM_FILES);\r\n    LOG.info(\"Number of files increment = \" + NUM_FILES_INCR);\r\n    MultiFileInputFormat<Text, Text> format = new DummyMultiFileInputFormat();\r\n    FileSystem fs = FileSystem.getLocal(job);\r\n    for (int numFiles = 1; numFiles < MAX_NUM_FILES; numFiles += (NUM_FILES_INCR / 2) + rand.nextInt(NUM_FILES_INCR / 2)) {\r\n        Path dir = initFiles(fs, numFiles, -1);\r\n        BitSet bits = new BitSet(numFiles);\r\n        for (int i = 1; i < MAX_SPLIT_COUNT; i += rand.nextInt(SPLIT_COUNT_INCR) + 1) {\r\n            LOG.info(\"Running for Num Files=\" + numFiles + \", split count=\" + i);\r\n            MultiFileSplit[] splits = (MultiFileSplit[]) format.getSplits(job, i);\r\n            bits.clear();\r\n            for (MultiFileSplit split : splits) {\r\n                long splitLength = 0;\r\n                for (Path p : split.getPaths()) {\r\n                    long length = fs.getContentSummary(p).getLength();\r\n                    assertEquals(length, lengths.get(p.getName()).longValue());\r\n                    splitLength += length;\r\n                    String name = p.getName();\r\n                    int index = Integer.parseInt(name.substring(name.lastIndexOf(\"file_\") + 5));\r\n                    assertFalse(bits.get(index));\r\n                    bits.set(index);\r\n                }\r\n                assertEquals(splitLength, split.getLength());\r\n            }\r\n        }\r\n        assertEquals(bits.cardinality(), numFiles);\r\n        fs.delete(dir, true);\r\n    }\r\n    LOG.info(\"Test Finished\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "testFormatWithLessPathsThanSplits",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void testFormatWithLessPathsThanSplits() throws Exception\n{\r\n    MultiFileInputFormat<Text, Text> format = new DummyMultiFileInputFormat();\r\n    FileSystem fs = FileSystem.getLocal(job);\r\n    initFiles(fs, 0, -1);\r\n    assertEquals(0, format.getSplits(job, 2).length);\r\n    initFiles(fs, 2, 500);\r\n    assertEquals(2, format.getSplits(job, 4).length);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "testTimelineServiceStartInMiniCluster",
  "errType" : null,
  "containingMethodsNum" : 14,
  "sourceCodeText" : "void testTimelineServiceStartInMiniCluster() throws Exception\n{\r\n    Configuration conf = new YarnConfiguration();\r\n    conf.setBoolean(YarnConfiguration.TIMELINE_SERVICE_ENABLED, false);\r\n    conf.setBoolean(MRJobConfig.MAPREDUCE_JOB_EMIT_TIMELINE_DATA, true);\r\n    MiniMRYarnCluster cluster = null;\r\n    try {\r\n        cluster = new MiniMRYarnCluster(TestMRTimelineEventHandling.class.getSimpleName(), 1);\r\n        cluster.init(conf);\r\n        cluster.start();\r\n        Assert.assertNull(\"Timeline Service should not have been started\", cluster.getApplicationHistoryServer());\r\n    } finally {\r\n        if (cluster != null) {\r\n            cluster.stop();\r\n        }\r\n    }\r\n    conf.setBoolean(YarnConfiguration.TIMELINE_SERVICE_ENABLED, false);\r\n    conf.setBoolean(MRJobConfig.MAPREDUCE_JOB_EMIT_TIMELINE_DATA, false);\r\n    cluster = null;\r\n    try {\r\n        cluster = new MiniMRYarnCluster(TestJobHistoryEventHandler.class.getSimpleName(), 1);\r\n        cluster.init(conf);\r\n        cluster.start();\r\n        Assert.assertNull(\"Timeline Service should not have been started\", cluster.getApplicationHistoryServer());\r\n    } finally {\r\n        if (cluster != null) {\r\n            cluster.stop();\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 2,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "testMRTimelineEventHandling",
  "errType" : null,
  "containingMethodsNum" : 27,
  "sourceCodeText" : "void testMRTimelineEventHandling() throws Exception\n{\r\n    Configuration conf = new YarnConfiguration();\r\n    conf.setBoolean(YarnConfiguration.TIMELINE_SERVICE_ENABLED, true);\r\n    conf.setBoolean(MRJobConfig.MAPREDUCE_JOB_EMIT_TIMELINE_DATA, true);\r\n    MiniMRYarnCluster cluster = null;\r\n    try {\r\n        cluster = new MiniMRYarnCluster(TestMRTimelineEventHandling.class.getSimpleName(), 1);\r\n        cluster.init(conf);\r\n        cluster.start();\r\n        conf.set(YarnConfiguration.TIMELINE_SERVICE_WEBAPP_ADDRESS, MiniYARNCluster.getHostname() + \":\" + cluster.getApplicationHistoryServer().getPort());\r\n        TimelineStore ts = cluster.getApplicationHistoryServer().getTimelineStore();\r\n        String localPathRoot = System.getProperty(\"test.build.data\", \"build/test/data\");\r\n        Path inDir = new Path(localPathRoot, \"input\");\r\n        Path outDir = new Path(localPathRoot, \"output\");\r\n        RunningJob job = UtilsForTests.runJobSucceed(new JobConf(conf), inDir, outDir);\r\n        Assert.assertEquals(JobStatus.SUCCEEDED, job.getJobStatus().getState().getValue());\r\n        TimelineEntities entities = ts.getEntities(\"MAPREDUCE_JOB\", null, null, null, null, null, null, null, null, null);\r\n        Assert.assertEquals(1, entities.getEntities().size());\r\n        TimelineEntity tEntity = entities.getEntities().get(0);\r\n        Assert.assertEquals(job.getID().toString(), tEntity.getEntityId());\r\n        Assert.assertEquals(\"MAPREDUCE_JOB\", tEntity.getEntityType());\r\n        Assert.assertEquals(EventType.AM_STARTED.toString(), tEntity.getEvents().get(tEntity.getEvents().size() - 1).getEventType());\r\n        Assert.assertEquals(EventType.JOB_FINISHED.toString(), tEntity.getEvents().get(0).getEventType());\r\n        job = UtilsForTests.runJobFail(new JobConf(conf), inDir, outDir);\r\n        Assert.assertEquals(JobStatus.FAILED, job.getJobStatus().getState().getValue());\r\n        entities = ts.getEntities(\"MAPREDUCE_JOB\", null, null, null, null, null, null, null, null, null);\r\n        Assert.assertEquals(2, entities.getEntities().size());\r\n        tEntity = entities.getEntities().get(0);\r\n        Assert.assertEquals(job.getID().toString(), tEntity.getEntityId());\r\n        Assert.assertEquals(\"MAPREDUCE_JOB\", tEntity.getEntityType());\r\n        Assert.assertEquals(EventType.AM_STARTED.toString(), tEntity.getEvents().get(tEntity.getEvents().size() - 1).getEventType());\r\n        Assert.assertEquals(EventType.JOB_FAILED.toString(), tEntity.getEvents().get(0).getEventType());\r\n    } finally {\r\n        if (cluster != null) {\r\n            cluster.stop();\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "testMRNewTimelineServiceEventHandling",
  "errType" : null,
  "containingMethodsNum" : 43,
  "sourceCodeText" : "void testMRNewTimelineServiceEventHandling() throws Exception\n{\r\n    LOG.info(\"testMRNewTimelineServiceEventHandling start.\");\r\n    String testDir = new File(\"target\", getClass().getSimpleName() + \"-test_dir\").getAbsolutePath();\r\n    String storageDir = testDir + File.separator + \"timeline_service_data\";\r\n    Configuration conf = new YarnConfiguration();\r\n    conf.setBoolean(YarnConfiguration.TIMELINE_SERVICE_ENABLED, true);\r\n    conf.setFloat(YarnConfiguration.TIMELINE_SERVICE_VERSION, 2.0f);\r\n    conf.setClass(YarnConfiguration.TIMELINE_SERVICE_WRITER_CLASS, FileSystemTimelineWriterImpl.class, TimelineWriter.class);\r\n    conf.setBoolean(MRJobConfig.MAPREDUCE_JOB_EMIT_TIMELINE_DATA, true);\r\n    conf.set(FileSystemTimelineWriterImpl.TIMELINE_SERVICE_STORAGE_DIR_ROOT, storageDir);\r\n    conf.set(YarnConfiguration.NM_AUX_SERVICES, TIMELINE_AUX_SERVICE_NAME);\r\n    conf.set(YarnConfiguration.NM_AUX_SERVICES + \".\" + TIMELINE_AUX_SERVICE_NAME + \".class\", PerNodeTimelineCollectorsAuxService.class.getName());\r\n    conf.setBoolean(YarnConfiguration.SYSTEM_METRICS_PUBLISHER_ENABLED, true);\r\n    MiniMRYarnCluster cluster = null;\r\n    try {\r\n        cluster = new MiniMRYarnCluster(TestMRTimelineEventHandling.class.getSimpleName(), 1, true);\r\n        cluster.init(conf);\r\n        cluster.start();\r\n        LOG.info(\"A MiniMRYarnCluster get start.\");\r\n        Path inDir = new Path(testDir, \"input\");\r\n        Path outDir = new Path(testDir, \"output\");\r\n        LOG.info(\"Run 1st job which should be successful.\");\r\n        JobConf successConf = new JobConf(conf);\r\n        successConf.set(\"dummy_conf1\", UtilsForTests.createConfigValue(51 * 1024));\r\n        successConf.set(\"dummy_conf2\", UtilsForTests.createConfigValue(51 * 1024));\r\n        successConf.set(\"huge_dummy_conf1\", UtilsForTests.createConfigValue(101 * 1024));\r\n        successConf.set(\"huge_dummy_conf2\", UtilsForTests.createConfigValue(101 * 1024));\r\n        RunningJob job = UtilsForTests.runJobSucceed(successConf, inDir, outDir);\r\n        Assert.assertEquals(JobStatus.SUCCEEDED, job.getJobStatus().getState().getValue());\r\n        YarnClient yarnClient = YarnClient.createYarnClient();\r\n        yarnClient.init(new Configuration(cluster.getConfig()));\r\n        yarnClient.start();\r\n        EnumSet<YarnApplicationState> appStates = EnumSet.allOf(YarnApplicationState.class);\r\n        ApplicationId firstAppId = null;\r\n        List<ApplicationReport> apps = yarnClient.getApplications(appStates);\r\n        Assert.assertEquals(apps.size(), 1);\r\n        ApplicationReport appReport = apps.get(0);\r\n        firstAppId = appReport.getApplicationId();\r\n        UtilsForTests.waitForAppFinished(job, cluster);\r\n        checkNewTimelineEvent(firstAppId, appReport, storageDir);\r\n        LOG.info(\"Run 2nd job which should be failed.\");\r\n        job = UtilsForTests.runJobFail(new JobConf(conf), inDir, outDir);\r\n        Assert.assertEquals(JobStatus.FAILED, job.getJobStatus().getState().getValue());\r\n        apps = yarnClient.getApplications(appStates);\r\n        Assert.assertEquals(apps.size(), 2);\r\n        appReport = apps.get(0).getApplicationId().equals(firstAppId) ? apps.get(0) : apps.get(1);\r\n        checkNewTimelineEvent(firstAppId, appReport, storageDir);\r\n    } finally {\r\n        if (cluster != null) {\r\n            cluster.stop();\r\n        }\r\n        File testDirFolder = new File(testDir);\r\n        if (testDirFolder.isDirectory()) {\r\n            FileUtils.deleteDirectory(testDirFolder);\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "checkNewTimelineEvent",
  "errType" : null,
  "containingMethodsNum" : 24,
  "sourceCodeText" : "void checkNewTimelineEvent(ApplicationId appId, ApplicationReport appReport, String storageDir) throws IOException\n{\r\n    String tmpRoot = storageDir + File.separator + \"entities\" + File.separator;\r\n    File tmpRootFolder = new File(tmpRoot);\r\n    Assert.assertTrue(tmpRootFolder.isDirectory());\r\n    String basePath = tmpRoot + YarnConfiguration.DEFAULT_RM_CLUSTER_ID + File.separator + UserGroupInformation.getCurrentUser().getShortUserName() + File.separator + appReport.getName() + File.separator + TimelineUtils.DEFAULT_FLOW_VERSION + File.separator + appReport.getStartTime() + File.separator + appId.toString();\r\n    String outputDirJob = basePath + File.separator + \"MAPREDUCE_JOB\" + File.separator;\r\n    File entityFolder = new File(outputDirJob);\r\n    Assert.assertTrue(\"Job output directory: \" + outputDirJob + \" does not exist.\", entityFolder.isDirectory());\r\n    String jobEventFileName = appId.toString().replaceAll(\"application\", \"job\") + FileSystemTimelineWriterImpl.TIMELINE_SERVICE_STORAGE_EXTENSION;\r\n    String jobEventFilePath = outputDirJob + jobEventFileName;\r\n    File jobEventFile = new File(jobEventFilePath);\r\n    Assert.assertTrue(\"jobEventFilePath: \" + jobEventFilePath + \" does not exist.\", jobEventFile.exists());\r\n    verifyEntity(jobEventFile, EventType.JOB_FINISHED.name(), true, false, null, false);\r\n    Set<String> cfgsToCheck = Sets.newHashSet(\"dummy_conf1\", \"dummy_conf2\", \"huge_dummy_conf1\", \"huge_dummy_conf2\");\r\n    verifyEntity(jobEventFile, null, false, true, cfgsToCheck, false);\r\n    String outputAppDir = basePath + File.separator + \"YARN_APPLICATION\" + File.separator;\r\n    entityFolder = new File(outputAppDir);\r\n    Assert.assertTrue(\"Job output directory: \" + outputAppDir + \" does not exist.\", entityFolder.isDirectory());\r\n    String appEventFileName = appId.toString() + FileSystemTimelineWriterImpl.TIMELINE_SERVICE_STORAGE_EXTENSION;\r\n    String appEventFilePath = outputAppDir + appEventFileName;\r\n    File appEventFile = new File(appEventFilePath);\r\n    Assert.assertTrue(\"appEventFilePath: \" + appEventFilePath + \" does not exist.\", appEventFile.exists());\r\n    verifyEntity(appEventFile, null, true, false, null, false);\r\n    verifyEntity(appEventFile, null, false, true, cfgsToCheck, false);\r\n    String outputDirTask = basePath + File.separator + \"MAPREDUCE_TASK\" + File.separator;\r\n    File taskFolder = new File(outputDirTask);\r\n    Assert.assertTrue(\"Task output directory: \" + outputDirTask + \" does not exist.\", taskFolder.isDirectory());\r\n    String taskEventFileName = appId.toString().replaceAll(\"application\", \"task\") + \"_m_000000\" + FileSystemTimelineWriterImpl.TIMELINE_SERVICE_STORAGE_EXTENSION;\r\n    String taskEventFilePath = outputDirTask + taskEventFileName;\r\n    File taskEventFile = new File(taskEventFilePath);\r\n    Assert.assertTrue(\"taskEventFileName: \" + taskEventFilePath + \" does not exist.\", taskEventFile.exists());\r\n    verifyEntity(taskEventFile, EventType.TASK_FINISHED.name(), true, false, null, true);\r\n    String outputDirTaskAttempt = basePath + File.separator + \"MAPREDUCE_TASK_ATTEMPT\" + File.separator;\r\n    File taskAttemptFolder = new File(outputDirTaskAttempt);\r\n    Assert.assertTrue(\"TaskAttempt output directory: \" + outputDirTaskAttempt + \" does not exist.\", taskAttemptFolder.isDirectory());\r\n    String taskAttemptEventFileName = appId.toString().replaceAll(\"application\", \"attempt\") + \"_m_000000_0\" + FileSystemTimelineWriterImpl.TIMELINE_SERVICE_STORAGE_EXTENSION;\r\n    String taskAttemptEventFilePath = outputDirTaskAttempt + taskAttemptEventFileName;\r\n    File taskAttemptEventFile = new File(taskAttemptEventFilePath);\r\n    Assert.assertTrue(\"taskAttemptEventFileName: \" + taskAttemptEventFilePath + \" does not exist.\", taskAttemptEventFile.exists());\r\n    verifyEntity(taskAttemptEventFile, EventType.MAP_ATTEMPT_FINISHED.name(), true, false, null, true);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "verifyEntity",
  "errType" : null,
  "containingMethodsNum" : 23,
  "sourceCodeText" : "void verifyEntity(File entityFile, String eventId, boolean chkMetrics, boolean chkCfg, Set<String> cfgsToVerify, boolean checkIdPrefix) throws IOException\n{\r\n    BufferedReader reader = null;\r\n    String strLine;\r\n    try {\r\n        reader = new BufferedReader(new FileReader(entityFile));\r\n        long idPrefix = -1;\r\n        while ((strLine = reader.readLine()) != null) {\r\n            if (strLine.trim().length() > 0) {\r\n                org.apache.hadoop.yarn.api.records.timelineservice.TimelineEntity entity = FileSystemTimelineReaderImpl.getTimelineRecordFromJSON(strLine.trim(), org.apache.hadoop.yarn.api.records.timelineservice.TimelineEntity.class);\r\n                LOG.info(\"strLine.trim()= \" + strLine.trim());\r\n                if (checkIdPrefix) {\r\n                    Assert.assertTrue(\"Entity ID prefix expected to be > 0\", entity.getIdPrefix() > 0);\r\n                    if (idPrefix == -1) {\r\n                        idPrefix = entity.getIdPrefix();\r\n                    } else {\r\n                        Assert.assertEquals(\"Entity ID prefix should be same across \" + \"each publish of same entity\", idPrefix, entity.getIdPrefix());\r\n                    }\r\n                }\r\n                if (eventId == null) {\r\n                    if (chkMetrics && entity.getMetrics().size() > 0) {\r\n                        return;\r\n                    }\r\n                    if (chkCfg && entity.getConfigs().size() > 0) {\r\n                        if (cfgsToVerify == null) {\r\n                            return;\r\n                        } else {\r\n                            for (Iterator<String> itr = cfgsToVerify.iterator(); itr.hasNext(); ) {\r\n                                String config = itr.next();\r\n                                if (entity.getConfigs().containsKey(config)) {\r\n                                    itr.remove();\r\n                                }\r\n                            }\r\n                            if (cfgsToVerify.isEmpty()) {\r\n                                return;\r\n                            }\r\n                        }\r\n                    }\r\n                } else {\r\n                    for (TimelineEvent event : entity.getEvents()) {\r\n                        if (event.getId().equals(eventId)) {\r\n                            if (chkMetrics) {\r\n                                assertTrue(entity.getMetrics().size() > 0);\r\n                            }\r\n                            if (chkCfg) {\r\n                                assertTrue(entity.getConfigs().size() > 0);\r\n                                if (cfgsToVerify != null) {\r\n                                    for (String cfg : cfgsToVerify) {\r\n                                        assertTrue(entity.getConfigs().containsKey(cfg));\r\n                                    }\r\n                                }\r\n                            }\r\n                            return;\r\n                        }\r\n                    }\r\n                }\r\n            }\r\n        }\r\n        if (cfgsToVerify != null) {\r\n            assertTrue(cfgsToVerify.isEmpty());\r\n            return;\r\n        }\r\n        fail(\"Expected event : \" + eventId + \" not found in the file \" + entityFile);\r\n    } finally {\r\n        reader.close();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "testMapreduceJobTimelineServiceEnabled",
  "errType" : null,
  "containingMethodsNum" : 44,
  "sourceCodeText" : "void testMapreduceJobTimelineServiceEnabled() throws Exception\n{\r\n    Configuration conf = new YarnConfiguration();\r\n    conf.setBoolean(YarnConfiguration.TIMELINE_SERVICE_ENABLED, true);\r\n    conf.setBoolean(MRJobConfig.MAPREDUCE_JOB_EMIT_TIMELINE_DATA, false);\r\n    MiniMRYarnCluster cluster = null;\r\n    FileSystem fs = null;\r\n    Path inDir = new Path(GenericTestUtils.getTempPath(\"input\"));\r\n    Path outDir = new Path(GenericTestUtils.getTempPath(\"output\"));\r\n    try {\r\n        fs = FileSystem.get(conf);\r\n        cluster = new MiniMRYarnCluster(TestMRTimelineEventHandling.class.getSimpleName(), 1);\r\n        cluster.init(conf);\r\n        cluster.start();\r\n        conf.set(YarnConfiguration.TIMELINE_SERVICE_WEBAPP_ADDRESS, MiniYARNCluster.getHostname() + \":\" + cluster.getApplicationHistoryServer().getPort());\r\n        TimelineStore ts = cluster.getApplicationHistoryServer().getTimelineStore();\r\n        RunningJob job = UtilsForTests.runJobSucceed(new JobConf(conf), inDir, outDir);\r\n        Assert.assertEquals(JobStatus.SUCCEEDED, job.getJobStatus().getState().getValue());\r\n        TimelineEntities entities = ts.getEntities(\"MAPREDUCE_JOB\", null, null, null, null, null, null, null, null, null);\r\n        Assert.assertEquals(0, entities.getEntities().size());\r\n        conf.setBoolean(MRJobConfig.MAPREDUCE_JOB_EMIT_TIMELINE_DATA, true);\r\n        job = UtilsForTests.runJobSucceed(new JobConf(conf), inDir, outDir);\r\n        Assert.assertEquals(JobStatus.SUCCEEDED, job.getJobStatus().getState().getValue());\r\n        entities = ts.getEntities(\"MAPREDUCE_JOB\", null, null, null, null, null, null, null, null, null);\r\n        Assert.assertEquals(1, entities.getEntities().size());\r\n        TimelineEntity tEntity = entities.getEntities().get(0);\r\n        Assert.assertEquals(job.getID().toString(), tEntity.getEntityId());\r\n    } finally {\r\n        if (cluster != null) {\r\n            cluster.stop();\r\n        }\r\n        deletePaths(fs, inDir, outDir);\r\n    }\r\n    conf = new YarnConfiguration();\r\n    conf.setBoolean(YarnConfiguration.TIMELINE_SERVICE_ENABLED, true);\r\n    conf.setBoolean(MRJobConfig.MAPREDUCE_JOB_EMIT_TIMELINE_DATA, true);\r\n    cluster = null;\r\n    try {\r\n        cluster = new MiniMRYarnCluster(TestJobHistoryEventHandler.class.getSimpleName(), 1);\r\n        cluster.init(conf);\r\n        cluster.start();\r\n        conf.set(YarnConfiguration.TIMELINE_SERVICE_WEBAPP_ADDRESS, MiniYARNCluster.getHostname() + \":\" + cluster.getApplicationHistoryServer().getPort());\r\n        TimelineStore ts = cluster.getApplicationHistoryServer().getTimelineStore();\r\n        conf.setBoolean(MRJobConfig.MAPREDUCE_JOB_EMIT_TIMELINE_DATA, false);\r\n        RunningJob job = UtilsForTests.runJobSucceed(new JobConf(conf), inDir, outDir);\r\n        Assert.assertEquals(JobStatus.SUCCEEDED, job.getJobStatus().getState().getValue());\r\n        TimelineEntities entities = ts.getEntities(\"MAPREDUCE_JOB\", null, null, null, null, null, null, null, null, null);\r\n        Assert.assertEquals(0, entities.getEntities().size());\r\n        conf.setBoolean(MRJobConfig.MAPREDUCE_JOB_EMIT_TIMELINE_DATA, true);\r\n        job = UtilsForTests.runJobSucceed(new JobConf(conf), inDir, outDir);\r\n        Assert.assertEquals(JobStatus.SUCCEEDED, job.getJobStatus().getState().getValue());\r\n        entities = ts.getEntities(\"MAPREDUCE_JOB\", null, null, null, null, null, null, null, null, null);\r\n        Assert.assertEquals(1, entities.getEntities().size());\r\n        TimelineEntity tEntity = entities.getEntities().get(0);\r\n        Assert.assertEquals(job.getID().toString(), tEntity.getEntityId());\r\n    } finally {\r\n        if (cluster != null) {\r\n            cluster.stop();\r\n        }\r\n        deletePaths(fs, inDir, outDir);\r\n    }\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 2,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "deletePaths",
  "errType" : [ "Exception" ],
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void deletePaths(FileSystem fs, Path... paths)\n{\r\n    if (fs == null) {\r\n        return;\r\n    }\r\n    for (Path path : paths) {\r\n        try {\r\n            fs.delete(path, true);\r\n        } catch (Exception ignored) {\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\fi",
  "methodName" : "init",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void init()\n{\r\n    if (conf == null) {\r\n        conf = new HdfsConfiguration(false);\r\n        String configName = System.getProperty(CONFIG_PARAMETER, DEFAULT_CONFIG);\r\n        conf.addResource(configName);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\fi",
  "methodName" : "getConfig",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Configuration getConfig()\n{\r\n    return conf;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\input",
  "methodName" : "createInputFile",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void createInputFile(Configuration conf) throws IOException\n{\r\n    FileSystem localFs = FileSystem.getLocal(conf);\r\n    Path file = new Path(inputDir, \"test.txt\");\r\n    Writer writer = new OutputStreamWriter(localFs.create(file));\r\n    writer.write(\"abc\\ndef\\t\\nghi\\njkl\");\r\n    writer.close();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\input",
  "methodName" : "readOutputFile",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "String readOutputFile(Configuration conf) throws IOException\n{\r\n    FileSystem localFs = FileSystem.getLocal(conf);\r\n    Path file = new Path(outputDir, \"part-r-00000\");\r\n    return UtilsForTests.slurpHadoop(file, localFs);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\input",
  "methodName" : "createAndRunJob",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void createAndRunJob(Configuration conf) throws IOException, InterruptedException, ClassNotFoundException\n{\r\n    Job job = Job.getInstance(conf);\r\n    job.setJarByClass(TestLineRecordReaderJobs.class);\r\n    job.setMapperClass(Mapper.class);\r\n    job.setReducerClass(Reducer.class);\r\n    FileInputFormat.addInputPath(job, inputDir);\r\n    FileOutputFormat.setOutputPath(job, outputDir);\r\n    job.waitForCompletion(true);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\input",
  "methodName" : "testCustomRecordDelimiters",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void testCustomRecordDelimiters() throws IOException, InterruptedException, ClassNotFoundException\n{\r\n    Configuration conf = new Configuration();\r\n    conf.set(\"textinputformat.record.delimiter\", \"\\t\\n\");\r\n    FileSystem localFs = FileSystem.getLocal(conf);\r\n    localFs.delete(workDir, true);\r\n    createInputFile(conf);\r\n    createAndRunJob(conf);\r\n    String expected = \"0\\tabc\\ndef\\n9\\tghi\\njkl\\n\";\r\n    assertEquals(expected, readOutputFile(conf));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\input",
  "methodName" : "testDefaultRecordDelimiters",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void testDefaultRecordDelimiters() throws IOException, InterruptedException, ClassNotFoundException\n{\r\n    Configuration conf = new Configuration();\r\n    FileSystem localFs = FileSystem.getLocal(conf);\r\n    localFs.delete(workDir, true);\r\n    createInputFile(conf);\r\n    createAndRunJob(conf);\r\n    String expected = \"0\\tabc\\n4\\tdef\\t\\n9\\tghi\\n13\\tjkl\\n\";\r\n    assertEquals(expected, readOutputFile(conf));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\fs\\slive",
  "methodName" : "getCfgOption",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getCfgOption()\n{\r\n    return cfgOption;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\fs\\slive",
  "methodName" : "getDefault",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "T getDefault()\n{\r\n    return defaultValue;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "testMapTaskReportsWithNullJob",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void testMapTaskReportsWithNullJob() throws Exception\n{\r\n    TestJobClient client = new TestJobClient(new JobConf());\r\n    Cluster mockCluster = mock(Cluster.class);\r\n    client.setCluster(mockCluster);\r\n    JobID id = new JobID(\"test\", 0);\r\n    when(mockCluster.getJob(id)).thenReturn(null);\r\n    TaskReport[] result = client.getMapTaskReports(id);\r\n    assertEquals(0, result.length);\r\n    verify(mockCluster).getJob(id);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "testReduceTaskReportsWithNullJob",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void testReduceTaskReportsWithNullJob() throws Exception\n{\r\n    TestJobClient client = new TestJobClient(new JobConf());\r\n    Cluster mockCluster = mock(Cluster.class);\r\n    client.setCluster(mockCluster);\r\n    JobID id = new JobID(\"test\", 0);\r\n    when(mockCluster.getJob(id)).thenReturn(null);\r\n    TaskReport[] result = client.getReduceTaskReports(id);\r\n    assertEquals(0, result.length);\r\n    verify(mockCluster).getJob(id);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "testSetupTaskReportsWithNullJob",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void testSetupTaskReportsWithNullJob() throws Exception\n{\r\n    TestJobClient client = new TestJobClient(new JobConf());\r\n    Cluster mockCluster = mock(Cluster.class);\r\n    client.setCluster(mockCluster);\r\n    JobID id = new JobID(\"test\", 0);\r\n    when(mockCluster.getJob(id)).thenReturn(null);\r\n    TaskReport[] result = client.getSetupTaskReports(id);\r\n    assertEquals(0, result.length);\r\n    verify(mockCluster).getJob(id);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "testCleanupTaskReportsWithNullJob",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void testCleanupTaskReportsWithNullJob() throws Exception\n{\r\n    TestJobClient client = new TestJobClient(new JobConf());\r\n    Cluster mockCluster = mock(Cluster.class);\r\n    client.setCluster(mockCluster);\r\n    JobID id = new JobID(\"test\", 0);\r\n    when(mockCluster.getJob(id)).thenReturn(null);\r\n    TaskReport[] result = client.getCleanupTaskReports(id);\r\n    assertEquals(0, result.length);\r\n    verify(mockCluster).getJob(id);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "testShowJob",
  "errType" : null,
  "containingMethodsNum" : 39,
  "sourceCodeText" : "void testShowJob() throws Exception\n{\r\n    TestJobClient client = new TestJobClient(new JobConf());\r\n    long startTime = System.currentTimeMillis();\r\n    JobID jobID = new JobID(String.valueOf(startTime), 12345);\r\n    JobStatus mockJobStatus = mock(JobStatus.class);\r\n    when(mockJobStatus.getJobID()).thenReturn(jobID);\r\n    when(mockJobStatus.getJobName()).thenReturn(jobID.toString());\r\n    when(mockJobStatus.getState()).thenReturn(JobStatus.State.RUNNING);\r\n    when(mockJobStatus.getStartTime()).thenReturn(startTime);\r\n    when(mockJobStatus.getUsername()).thenReturn(\"mockuser\");\r\n    when(mockJobStatus.getQueue()).thenReturn(\"mockqueue\");\r\n    when(mockJobStatus.getPriority()).thenReturn(JobPriority.NORMAL);\r\n    when(mockJobStatus.getNumUsedSlots()).thenReturn(1);\r\n    when(mockJobStatus.getNumReservedSlots()).thenReturn(1);\r\n    when(mockJobStatus.getUsedMem()).thenReturn(1024);\r\n    when(mockJobStatus.getReservedMem()).thenReturn(512);\r\n    when(mockJobStatus.getNeededMem()).thenReturn(2048);\r\n    when(mockJobStatus.getSchedulingInfo()).thenReturn(\"NA\");\r\n    Job mockJob = mock(Job.class);\r\n    when(mockJob.getTaskReports(isA(TaskType.class))).thenReturn(new TaskReport[5]);\r\n    Cluster mockCluster = mock(Cluster.class);\r\n    when(mockCluster.getJob(jobID)).thenReturn(mockJob);\r\n    client.setCluster(mockCluster);\r\n    ByteArrayOutputStream out = new ByteArrayOutputStream();\r\n    client.displayJobList(new JobStatus[] { mockJobStatus }, new PrintWriter(out));\r\n    String commandLineOutput = out.toString();\r\n    System.out.println(commandLineOutput);\r\n    Assert.assertTrue(commandLineOutput.contains(\"Total jobs:1\"));\r\n    verify(mockJobStatus, atLeastOnce()).getJobID();\r\n    verify(mockJobStatus).getState();\r\n    verify(mockJobStatus).getStartTime();\r\n    verify(mockJobStatus).getUsername();\r\n    verify(mockJobStatus).getQueue();\r\n    verify(mockJobStatus).getPriority();\r\n    verify(mockJobStatus).getNumUsedSlots();\r\n    verify(mockJobStatus).getNumReservedSlots();\r\n    verify(mockJobStatus).getUsedMem();\r\n    verify(mockJobStatus).getReservedMem();\r\n    verify(mockJobStatus).getNeededMem();\r\n    verify(mockJobStatus).getSchedulingInfo();\r\n    verify(mockCluster, never()).getJob(jobID);\r\n    verify(mockJob, never()).getTaskReports(isA(TaskType.class));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "testGetJobWithUnknownJob",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void testGetJobWithUnknownJob() throws Exception\n{\r\n    TestJobClient client = new TestJobClient(new JobConf());\r\n    Cluster mockCluster = mock(Cluster.class);\r\n    client.setCluster(mockCluster);\r\n    JobID id = new JobID(\"unknown\", 0);\r\n    when(mockCluster.getJob(id)).thenReturn(null);\r\n    assertNull(client.getJob(id));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "testGetJobRetry",
  "errType" : null,
  "containingMethodsNum" : 10,
  "sourceCodeText" : "void testGetJobRetry() throws Exception\n{\r\n    JobConf conf = new JobConf();\r\n    conf.setInt(MRJobConfig.MR_CLIENT_JOB_MAX_RETRIES, 2);\r\n    TestJobClientGetJob client = new TestJobClientGetJob(conf);\r\n    JobID id = new JobID(\"ajob\", 1);\r\n    RunningJob rj = mock(RunningJob.class);\r\n    client.setRunningJob(rj);\r\n    assertNotNull(client.getJob(id));\r\n    assertThat(client.getLastGetJobRetriesCounter()).isEqualTo(0);\r\n    client.setGetJobRetries(2);\r\n    assertNotNull(client.getJob(id));\r\n    assertThat(client.getLastGetJobRetriesCounter()).isEqualTo(2);\r\n    client.setGetJobRetries(3);\r\n    assertNull(client.getJob(id));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "testGetJobRetryDefault",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void testGetJobRetryDefault() throws Exception\n{\r\n    JobConf conf = new JobConf();\r\n    TestJobClientGetJob client = new TestJobClientGetJob(conf);\r\n    JobID id = new JobID(\"ajob\", 1);\r\n    RunningJob rj = mock(RunningJob.class);\r\n    client.setRunningJob(rj);\r\n    client.setGetJobRetries(MRJobConfig.DEFAULT_MR_CLIENT_JOB_MAX_RETRIES);\r\n    assertNotNull(client.getJob(id));\r\n    assertThat(client.getLastGetJobRetriesCounter()).isEqualTo(MRJobConfig.DEFAULT_MR_CLIENT_JOB_MAX_RETRIES);\r\n    client.setGetJobRetries(MRJobConfig.DEFAULT_MR_CLIENT_JOB_MAX_RETRIES + 1);\r\n    assertNull(client.getJob(id));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "setupClassBase",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setupClassBase(Class<?> testClass) throws Exception\n{\r\n    testRootDir = GenericTestUtils.setupTestRootDir(testClass);\r\n    dfsFolder = new File(testRootDir, \"dfs\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "setUp",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setUp() throws Exception\n{\r\n    startCluster(true, null);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "startCluster",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void startCluster(boolean reformatDFS, Properties props) throws Exception\n{\r\n    if (dfsCluster == null) {\r\n        JobConf conf = new JobConf();\r\n        if (props != null) {\r\n            for (Map.Entry entry : props.entrySet()) {\r\n                conf.set((String) entry.getKey(), (String) entry.getValue());\r\n            }\r\n        }\r\n        dfsCluster = new MiniDFSCluster.Builder(conf, dfsFolder).numDataNodes(2).format(reformatDFS).racks(null).build();\r\n        mrCluster = MiniMRClientClusterFactory.create(this.getClass(), 2, conf);\r\n    }\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "stopCluster",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void stopCluster() throws Exception\n{\r\n    if (mrCluster != null) {\r\n        mrCluster.stop();\r\n        mrCluster = null;\r\n    }\r\n    if (dfsCluster != null) {\r\n        dfsCluster.shutdown();\r\n        dfsCluster = null;\r\n    }\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "tearDown",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void tearDown() throws Exception\n{\r\n    stopCluster();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getFileSystem",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "FileSystem getFileSystem() throws IOException\n{\r\n    return dfsCluster.getFileSystem();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getTestRootDir",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Path getTestRootDir()\n{\r\n    return new Path(testRootDir.getPath());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getInputDir",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Path getInputDir()\n{\r\n    return new Path(\"target/input\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "getOutputDir",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Path getOutputDir()\n{\r\n    return new Path(\"target/output\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "createJobConf",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "JobConf createJobConf() throws IOException\n{\r\n    return new JobConf(mrCluster.getConfig());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\join",
  "methodName" : "testClassLoader",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void testClassLoader() throws Exception\n{\r\n    Configuration conf = new Configuration();\r\n    Fake_ClassLoader classLoader = new Fake_ClassLoader();\r\n    conf.setClassLoader(classLoader);\r\n    assertTrue(conf.getClassLoader() instanceof Fake_ClassLoader);\r\n    FileSystem fs = FileSystem.get(conf);\r\n    Path testdir = fs.makeQualified(new Path(System.getProperty(\"test.build.data\", \"/tmp\")));\r\n    Path base = new Path(testdir, \"/empty\");\r\n    Path[] src = { new Path(base, \"i0\"), new Path(\"i1\"), new Path(\"i2\") };\r\n    conf.set(CompositeInputFormat.JOIN_EXPR, CompositeInputFormat.compose(\"outer\", IF_ClassLoaderChecker.class, src));\r\n    CompositeInputFormat<NullWritable> inputFormat = new CompositeInputFormat<NullWritable>();\r\n    TaskAttemptID tid = new TaskAttemptID(\"jt\", 1, TaskType.MAP, 0, 0);\r\n    conf.set(MRJobConfig.TASK_ATTEMPT_ID, tid.toString());\r\n    inputFormat.createRecordReader(inputFormat.getSplits(Job.getInstance(conf)).get(0), new TaskAttemptContextImpl(conf, tid));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred\\jobcontrol",
  "methodName" : "doJobControlTest",
  "errType" : [ "Exception" ],
  "containingMethodsNum" : 71,
  "sourceCodeText" : "void doJobControlTest() throws Exception\n{\r\n    Configuration defaults = new Configuration();\r\n    FileSystem fs = FileSystem.get(defaults);\r\n    Path rootDataDir = new Path(System.getProperty(\"test.build.data\", \".\"), \"TestJobControlData\");\r\n    Path indir = new Path(rootDataDir, \"indir\");\r\n    Path outdir_1 = new Path(rootDataDir, \"outdir_1\");\r\n    Path outdir_2 = new Path(rootDataDir, \"outdir_2\");\r\n    Path outdir_3 = new Path(rootDataDir, \"outdir_3\");\r\n    Path outdir_4 = new Path(rootDataDir, \"outdir_4\");\r\n    JobControlTestUtils.cleanData(fs, indir);\r\n    JobControlTestUtils.generateData(fs, indir);\r\n    JobControlTestUtils.cleanData(fs, outdir_1);\r\n    JobControlTestUtils.cleanData(fs, outdir_2);\r\n    JobControlTestUtils.cleanData(fs, outdir_3);\r\n    JobControlTestUtils.cleanData(fs, outdir_4);\r\n    ArrayList<Job> dependingJobs = null;\r\n    ArrayList<Path> inPaths_1 = new ArrayList<Path>();\r\n    inPaths_1.add(indir);\r\n    JobConf jobConf_1 = JobControlTestUtils.createCopyJob(inPaths_1, outdir_1);\r\n    Job job_1 = new Job(jobConf_1, dependingJobs);\r\n    ArrayList<Path> inPaths_2 = new ArrayList<Path>();\r\n    inPaths_2.add(indir);\r\n    JobConf jobConf_2 = JobControlTestUtils.createCopyJob(inPaths_2, outdir_2);\r\n    Job job_2 = new Job(jobConf_2, dependingJobs);\r\n    ArrayList<Path> inPaths_3 = new ArrayList<Path>();\r\n    inPaths_3.add(outdir_1);\r\n    inPaths_3.add(outdir_2);\r\n    JobConf jobConf_3 = JobControlTestUtils.createCopyJob(inPaths_3, outdir_3);\r\n    dependingJobs = new ArrayList<Job>();\r\n    dependingJobs.add(job_1);\r\n    dependingJobs.add(job_2);\r\n    Job job_3 = new Job(jobConf_3, dependingJobs);\r\n    ArrayList<Path> inPaths_4 = new ArrayList<Path>();\r\n    inPaths_4.add(outdir_3);\r\n    JobConf jobConf_4 = JobControlTestUtils.createCopyJob(inPaths_4, outdir_4);\r\n    dependingJobs = new ArrayList<Job>();\r\n    dependingJobs.add(job_3);\r\n    Job job_4 = new Job(jobConf_4, dependingJobs);\r\n    JobControl theControl = new JobControl(\"Test\");\r\n    theControl.addJob((ControlledJob) job_1);\r\n    theControl.addJob((ControlledJob) job_2);\r\n    theControl.addJob(job_3);\r\n    theControl.addJob(job_4);\r\n    Thread theController = new Thread(theControl);\r\n    theController.start();\r\n    while (!theControl.allFinished()) {\r\n        System.out.println(\"Jobs in waiting state: \" + theControl.getWaitingJobs().size());\r\n        System.out.println(\"Jobs in ready state: \" + theControl.getReadyJobs().size());\r\n        System.out.println(\"Jobs in running state: \" + theControl.getRunningJobs().size());\r\n        System.out.println(\"Jobs in success state: \" + theControl.getSuccessfulJobs().size());\r\n        System.out.println(\"Jobs in failed state: \" + theControl.getFailedJobs().size());\r\n        System.out.println(\"\\n\");\r\n        try {\r\n            Thread.sleep(5000);\r\n        } catch (Exception e) {\r\n        }\r\n    }\r\n    System.out.println(\"Jobs are all done???\");\r\n    System.out.println(\"Jobs in waiting state: \" + theControl.getWaitingJobs().size());\r\n    System.out.println(\"Jobs in ready state: \" + theControl.getReadyJobs().size());\r\n    System.out.println(\"Jobs in running state: \" + theControl.getRunningJobs().size());\r\n    System.out.println(\"Jobs in success state: \" + theControl.getSuccessfulJobs().size());\r\n    System.out.println(\"Jobs in failed state: \" + theControl.getFailedJobs().size());\r\n    System.out.println(\"\\n\");\r\n    if (job_1.getState() != Job.FAILED && job_1.getState() != Job.DEPENDENT_FAILED && job_1.getState() != Job.SUCCESS) {\r\n        String states = \"job_1:  \" + job_1.getState() + \"\\n\";\r\n        throw new Exception(\"The state of job_1 is not in a complete state\\n\" + states);\r\n    }\r\n    if (job_2.getState() != Job.FAILED && job_2.getState() != Job.DEPENDENT_FAILED && job_2.getState() != Job.SUCCESS) {\r\n        String states = \"job_2:  \" + job_2.getState() + \"\\n\";\r\n        throw new Exception(\"The state of job_2 is not in a complete state\\n\" + states);\r\n    }\r\n    if (job_3.getState() != Job.FAILED && job_3.getState() != Job.DEPENDENT_FAILED && job_3.getState() != Job.SUCCESS) {\r\n        String states = \"job_3:  \" + job_3.getState() + \"\\n\";\r\n        throw new Exception(\"The state of job_3 is not in a complete state\\n\" + states);\r\n    }\r\n    if (job_4.getState() != Job.FAILED && job_4.getState() != Job.DEPENDENT_FAILED && job_4.getState() != Job.SUCCESS) {\r\n        String states = \"job_4:  \" + job_4.getState() + \"\\n\";\r\n        throw new Exception(\"The state of job_4 is not in a complete state\\n\" + states);\r\n    }\r\n    if (job_1.getState() == Job.FAILED || job_2.getState() == Job.FAILED || job_1.getState() == Job.DEPENDENT_FAILED || job_2.getState() == Job.DEPENDENT_FAILED) {\r\n        if (job_3.getState() != Job.DEPENDENT_FAILED) {\r\n            String states = \"job_1:  \" + job_1.getState() + \"\\n\";\r\n            states = \"job_2:  \" + job_2.getState() + \"\\n\";\r\n            states = \"job_3:  \" + job_3.getState() + \"\\n\";\r\n            states = \"job_4:  \" + job_4.getState() + \"\\n\";\r\n            throw new Exception(\"The states of jobs 1, 2, 3, 4 are not consistent\\n\" + states);\r\n        }\r\n    }\r\n    if (job_3.getState() == Job.FAILED || job_3.getState() == Job.DEPENDENT_FAILED) {\r\n        if (job_4.getState() != Job.DEPENDENT_FAILED) {\r\n            String states = \"job_3:  \" + job_3.getState() + \"\\n\";\r\n            states = \"job_4:  \" + job_4.getState() + \"\\n\";\r\n            throw new Exception(\"The states of jobs 3, 4 are not consistent\\n\" + states);\r\n        }\r\n    }\r\n    theControl.stop();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred\\jobcontrol",
  "methodName" : "testJobState",
  "errType" : null,
  "containingMethodsNum" : 12,
  "sourceCodeText" : "void testJobState() throws Exception\n{\r\n    Job job_1 = getCopyJob();\r\n    JobControl jc = new JobControl(\"Test\");\r\n    jc.addJob(job_1);\r\n    Assert.assertEquals(Job.WAITING, job_1.getState());\r\n    job_1.setState(Job.SUCCESS);\r\n    Assert.assertEquals(Job.WAITING, job_1.getState());\r\n    org.apache.hadoop.mapreduce.Job mockjob = mock(org.apache.hadoop.mapreduce.Job.class);\r\n    org.apache.hadoop.mapreduce.JobID jid = new org.apache.hadoop.mapreduce.JobID(\"test\", 0);\r\n    when(mockjob.getJobID()).thenReturn(jid);\r\n    job_1.setJob(mockjob);\r\n    Assert.assertEquals(\"job_test_0000\", job_1.getMapredJobID());\r\n    job_1.setMapredJobID(\"job_test_0001\");\r\n    Assert.assertEquals(\"job_test_0000\", job_1.getMapredJobID());\r\n    jc.stop();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred\\jobcontrol",
  "methodName" : "testAddingDependingJob",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void testAddingDependingJob() throws Exception\n{\r\n    Job job_1 = getCopyJob();\r\n    ArrayList<Job> dependingJobs = new ArrayList<Job>();\r\n    JobControl jc = new JobControl(\"Test\");\r\n    jc.addJob(job_1);\r\n    Assert.assertEquals(Job.WAITING, job_1.getState());\r\n    Assert.assertTrue(job_1.addDependingJob(new Job(job_1.getJobConf(), dependingJobs)));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred\\jobcontrol",
  "methodName" : "getCopyJob",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "Job getCopyJob() throws Exception\n{\r\n    Configuration defaults = new Configuration();\r\n    FileSystem fs = FileSystem.get(defaults);\r\n    Path rootDataDir = new Path(System.getProperty(\"test.build.data\", \".\"), \"TestJobControlData\");\r\n    Path indir = new Path(rootDataDir, \"indir\");\r\n    Path outdir_1 = new Path(rootDataDir, \"outdir_1\");\r\n    JobControlTestUtils.cleanData(fs, indir);\r\n    JobControlTestUtils.generateData(fs, indir);\r\n    JobControlTestUtils.cleanData(fs, outdir_1);\r\n    ArrayList<Job> dependingJobs = null;\r\n    ArrayList<Path> inPaths_1 = new ArrayList<Path>();\r\n    inPaths_1.add(indir);\r\n    JobConf jobConf_1 = JobControlTestUtils.createCopyJob(inPaths_1, outdir_1);\r\n    Job job_1 = new Job(jobConf_1, dependingJobs);\r\n    return job_1;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred\\jobcontrol",
  "methodName" : "testJobControl",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testJobControl() throws Exception\n{\r\n    doJobControlTest();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred\\jobcontrol",
  "methodName" : "testGetAssignedJobId",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void testGetAssignedJobId() throws Exception\n{\r\n    JobConf jc = new JobConf();\r\n    Job j = new Job(jc);\r\n    Assert.assertNull(j.getAssignedJobID());\r\n    org.apache.hadoop.mapreduce.Job mockjob = mock(org.apache.hadoop.mapreduce.Job.class);\r\n    org.apache.hadoop.mapreduce.JobID jid = new org.apache.hadoop.mapreduce.JobID(\"test\", 0);\r\n    when(mockjob.getJobID()).thenReturn(jid);\r\n    j.setJob(mockjob);\r\n    JobID expected = new JobID(\"test\", 0);\r\n    Assert.assertEquals(expected, j.getAssignedJobID());\r\n    verify(mockjob).getJobID();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred\\jobcontrol",
  "methodName" : "main",
  "errType" : [ "Exception" ],
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void main(String[] args)\n{\r\n    TestJobControl test = new TestJobControl();\r\n    try {\r\n        test.testJobControl();\r\n    } catch (Exception e) {\r\n        e.printStackTrace();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\fs\\slive",
  "methodName" : "getPath",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "Path getPath(int curId, int limitPerDir, Type type)\n{\r\n    if (curId <= 0) {\r\n        return basePath;\r\n    }\r\n    String name = \"\";\r\n    switch(type) {\r\n        case FILE:\r\n            name = FILE_PREFIX + new Integer(curId % limitPerDir).toString();\r\n            break;\r\n        case DIRECTORY:\r\n            name = DIR_PREFIX + new Integer(curId % limitPerDir).toString();\r\n            break;\r\n    }\r\n    Path base = getPath((curId / limitPerDir), limitPerDir, Type.DIRECTORY);\r\n    return new Path(base, name);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\fs\\slive",
  "methodName" : "getFile",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "Path getFile()\n{\r\n    int fileLimit = config.getTotalFiles();\r\n    int dirLimit = config.getDirSize();\r\n    int startPoint = 1 + rnd.nextInt(fileLimit);\r\n    return getPath(startPoint, dirLimit, Type.FILE);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\fs\\slive",
  "methodName" : "getDirectory",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "Path getDirectory()\n{\r\n    int fileLimit = config.getTotalFiles();\r\n    int dirLimit = config.getDirSize();\r\n    int startPoint = rnd.nextInt(fileLimit);\r\n    return getPath(startPoint, dirLimit, Type.DIRECTORY);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "generateTextFile",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void generateTextFile(FileSystem fs, Path inputFile, long numLines, Order sortOrder) throws IOException\n{\r\n    LOG.info(\"creating control file: \" + numLines + \" numLines, \" + sortOrder + \" sortOrder\");\r\n    PrintStream output = null;\r\n    try {\r\n        output = new PrintStream(fs.create(inputFile));\r\n        int padding = String.valueOf(numLines).length();\r\n        switch(sortOrder) {\r\n            case RANDOM:\r\n                for (long l = 0; l < numLines; l++) {\r\n                    output.println(pad((new Random()).nextLong(), padding));\r\n                }\r\n                break;\r\n            case ASCENDING:\r\n                for (long l = 0; l < numLines; l++) {\r\n                    output.println(pad(l, padding));\r\n                }\r\n                break;\r\n            case DESCENDING:\r\n                for (long l = numLines; l > 0; l--) {\r\n                    output.println(pad(l, padding));\r\n                }\r\n                break;\r\n        }\r\n    } finally {\r\n        if (output != null)\r\n            output.close();\r\n    }\r\n    LOG.info(\"created control file: \" + inputFile);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "pad",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "String pad(long number, int length)\n{\r\n    String str = String.valueOf(number);\r\n    StringBuffer value = new StringBuffer();\r\n    for (int i = str.length(); i < length; i++) {\r\n        value.append(\"0\");\r\n    }\r\n    value.append(str);\r\n    return value.toString();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "setupJob",
  "errType" : null,
  "containingMethodsNum" : 14,
  "sourceCodeText" : "JobConf setupJob(int numMaps, int numReduces, String jarFile)\n{\r\n    JobConf jobConf = new JobConf(getConf());\r\n    jobConf.setJarByClass(MRBench.class);\r\n    FileInputFormat.addInputPath(jobConf, INPUT_DIR);\r\n    jobConf.setInputFormat(TextInputFormat.class);\r\n    jobConf.setOutputFormat(TextOutputFormat.class);\r\n    jobConf.setOutputValueClass(Text.class);\r\n    jobConf.setMapOutputKeyClass(Text.class);\r\n    jobConf.setMapOutputValueClass(Text.class);\r\n    if (null != jarFile) {\r\n        jobConf.setJar(jarFile);\r\n    }\r\n    jobConf.setMapperClass(Map.class);\r\n    jobConf.setReducerClass(Reduce.class);\r\n    jobConf.setNumMapTasks(numMaps);\r\n    jobConf.setNumReduceTasks(numReduces);\r\n    jobConf.setBoolean(\"mapreduce.job.complete.cancel.delegation.tokens\", false);\r\n    return jobConf;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "runJobInSequence",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "ArrayList<Long> runJobInSequence(JobConf masterJobConf, int numRuns) throws IOException\n{\r\n    Random rand = new Random();\r\n    ArrayList<Long> execTimes = new ArrayList<Long>();\r\n    for (int i = 0; i < numRuns; i++) {\r\n        JobConf jobConf = new JobConf(masterJobConf);\r\n        jobConf.setJar(masterJobConf.getJar());\r\n        FileOutputFormat.setOutputPath(jobConf, new Path(OUTPUT_DIR, \"output_\" + rand.nextInt()));\r\n        LOG.info(\"Running job \" + i + \":\" + \" input=\" + FileInputFormat.getInputPaths(jobConf)[0] + \" output=\" + FileOutputFormat.getOutputPath(jobConf));\r\n        long curTime = System.currentTimeMillis();\r\n        JobClient.runJob(jobConf);\r\n        execTimes.add(new Long(System.currentTimeMillis() - curTime));\r\n    }\r\n    return execTimes;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "main",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void main(String[] args) throws Exception\n{\r\n    int res = ToolRunner.run(new MRBench(), args);\r\n    System.exit(res);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "run",
  "errType" : null,
  "containingMethodsNum" : 34,
  "sourceCodeText" : "int run(String[] args) throws Exception\n{\r\n    String version = \"MRBenchmark.0.0.2\";\r\n    System.out.println(version);\r\n    String usage = \"Usage: mrbench \" + \"[-baseDir <base DFS path for output/input, default is /benchmarks/MRBench>] \" + \"[-jar <local path to job jar file containing Mapper and Reducer implementations, default is current jar file>] \" + \"[-numRuns <number of times to run the job, default is 1>] \" + \"[-maps <number of maps for each run, default is 2>] \" + \"[-reduces <number of reduces for each run, default is 1>] \" + \"[-inputLines <number of input lines to generate, default is 1>] \" + \"[-inputType <type of input to generate, one of ascending (default), descending, random>] \" + \"[-verbose]\";\r\n    String jarFile = null;\r\n    long inputLines = 1;\r\n    int numRuns = 1;\r\n    int numMaps = 2;\r\n    int numReduces = 1;\r\n    boolean verbose = false;\r\n    Order inputSortOrder = Order.ASCENDING;\r\n    for (int i = 0; i < args.length; i++) {\r\n        if (args[i].equals(\"-jar\")) {\r\n            jarFile = args[++i];\r\n        } else if (args[i].equals(\"-numRuns\")) {\r\n            numRuns = Integer.parseInt(args[++i]);\r\n        } else if (args[i].equals(\"-baseDir\")) {\r\n            BASE_DIR = new Path(args[++i]);\r\n            INPUT_DIR = new Path(BASE_DIR, DEFAULT_INPUT_SUB);\r\n            OUTPUT_DIR = new Path(BASE_DIR, DEFAULT_OUTPUT_SUB);\r\n        } else if (args[i].equals(\"-maps\")) {\r\n            numMaps = Integer.parseInt(args[++i]);\r\n        } else if (args[i].equals(\"-reduces\")) {\r\n            numReduces = Integer.parseInt(args[++i]);\r\n        } else if (args[i].equals(\"-inputLines\")) {\r\n            inputLines = Long.parseLong(args[++i]);\r\n        } else if (args[i].equals(\"-inputType\")) {\r\n            String s = args[++i];\r\n            if (s.equalsIgnoreCase(\"ascending\")) {\r\n                inputSortOrder = Order.ASCENDING;\r\n            } else if (s.equalsIgnoreCase(\"descending\")) {\r\n                inputSortOrder = Order.DESCENDING;\r\n            } else if (s.equalsIgnoreCase(\"random\")) {\r\n                inputSortOrder = Order.RANDOM;\r\n            } else {\r\n                inputSortOrder = null;\r\n            }\r\n        } else if (args[i].equals(\"-verbose\")) {\r\n            verbose = true;\r\n        } else {\r\n            System.err.println(usage);\r\n            System.exit(-1);\r\n        }\r\n    }\r\n    if (numRuns < 1 || numMaps < 1 || numReduces < 1 || inputLines < 0 || inputSortOrder == null) {\r\n        System.err.println(usage);\r\n        return -1;\r\n    }\r\n    JobConf jobConf = setupJob(numMaps, numReduces, jarFile);\r\n    FileSystem fs = BASE_DIR.getFileSystem(jobConf);\r\n    Path inputFile = new Path(INPUT_DIR, \"input_\" + (new Random()).nextInt() + \".txt\");\r\n    generateTextFile(fs, inputFile, inputLines, inputSortOrder);\r\n    ArrayList<Long> execTimes = new ArrayList<Long>();\r\n    try {\r\n        execTimes = runJobInSequence(jobConf, numRuns);\r\n    } finally {\r\n        fs.delete(OUTPUT_DIR, true);\r\n        fs.delete(INPUT_DIR, true);\r\n    }\r\n    if (verbose) {\r\n        System.out.println(\"Total MapReduce jobs executed: \" + numRuns);\r\n        System.out.println(\"Total lines of data per job: \" + inputLines);\r\n        System.out.println(\"Maps per job: \" + numMaps);\r\n        System.out.println(\"Reduces per job: \" + numReduces);\r\n    }\r\n    int i = 0;\r\n    long totalTime = 0;\r\n    for (Long time : execTimes) {\r\n        totalTime += time.longValue();\r\n        if (verbose) {\r\n            System.out.println(\"Total milliseconds for task: \" + (++i) + \" = \" + time);\r\n        }\r\n    }\r\n    long avgTime = totalTime / numRuns;\r\n    System.out.println(\"DataLines\\tMaps\\tReduces\\tAvgTime (milliseconds)\");\r\n    System.out.println(inputLines + \"\\t\\t\" + numMaps + \"\\t\" + numReduces + \"\\t\" + avgTime);\r\n    return 0;\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\fs\\slive",
  "methodName" : "getConfig",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "ConfigExtractor getConfig()\n{\r\n    return this.config;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\fs\\slive",
  "methodName" : "getRandom",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Random getRandom()\n{\r\n    return this.rnd;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\fs\\slive",
  "methodName" : "getType",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getType()\n{\r\n    return type;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\fs\\slive",
  "methodName" : "getFinder",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "PathFinder getFinder()\n{\r\n    return this.finder;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\fs\\slive",
  "methodName" : "toString",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String toString()\n{\r\n    return getType();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\fs\\slive",
  "methodName" : "run",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "List<OperationOutput> run(FileSystem fs)\n{\r\n    List<OperationOutput> out = new LinkedList<OperationOutput>();\r\n    out.add(new OperationOutput(OutputType.LONG, getType(), ReportWriter.OP_COUNT, 1L));\r\n    return out;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "setupClass",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setupClass() throws Exception\n{\r\n    setupClassBase(TestMRCJCJobClient.class);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "runJob",
  "errType" : null,
  "containingMethodsNum" : 19,
  "sourceCodeText" : "String runJob() throws Exception\n{\r\n    OutputStream os = getFileSystem().create(new Path(getInputDir(), \"text.txt\"));\r\n    Writer wr = new OutputStreamWriter(os);\r\n    wr.write(\"hello1\\n\");\r\n    wr.write(\"hello2\\n\");\r\n    wr.write(\"hello3\\n\");\r\n    wr.close();\r\n    JobConf conf = createJobConf();\r\n    conf.setJobName(\"mr\");\r\n    conf.setJobPriority(JobPriority.HIGH);\r\n    conf.setInputFormat(TextInputFormat.class);\r\n    conf.setMapOutputKeyClass(LongWritable.class);\r\n    conf.setMapOutputValueClass(Text.class);\r\n    conf.setOutputFormat(TextOutputFormat.class);\r\n    conf.setOutputKeyClass(LongWritable.class);\r\n    conf.setOutputValueClass(Text.class);\r\n    conf.setMapperClass(org.apache.hadoop.mapred.lib.IdentityMapper.class);\r\n    conf.setReducerClass(org.apache.hadoop.mapred.lib.IdentityReducer.class);\r\n    FileInputFormat.setInputPaths(conf, getInputDir());\r\n    FileOutputFormat.setOutputPath(conf, getOutputDir());\r\n    return JobClient.runJob(conf).getID().toString();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "runTool",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "int runTool(Configuration conf, Tool tool, String[] args, OutputStream out) throws Exception\n{\r\n    return TestMRJobClient.runTool(conf, tool, args, out);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "verifyJobPriority",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void verifyJobPriority(String jobId, String priority, JobConf conf) throws Exception\n{\r\n    TestMRCJCJobClient test = new TestMRCJCJobClient();\r\n    test.verifyJobPriority(jobId, priority, conf, test.createJobClient());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "testJobClient",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void testJobClient() throws Exception\n{\r\n    Configuration conf = createJobConf();\r\n    String jobId = runJob();\r\n    testGetCounter(jobId, conf);\r\n    testAllJobList(jobId, conf);\r\n    testChangingJobPriority(jobId, conf);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "createJobClient",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "CLI createJobClient() throws IOException\n{\r\n    return new JobClient();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred\\lib\\aggregate",
  "methodName" : "generateKeyValPairs",
  "errType" : null,
  "containingMethodsNum" : 16,
  "sourceCodeText" : "ArrayList<Entry<Text, Text>> generateKeyValPairs(Object key, Object val)\n{\r\n    ArrayList<Entry<Text, Text>> retv = new ArrayList<Entry<Text, Text>>();\r\n    String[] words = val.toString().split(\" \");\r\n    String countType;\r\n    String id;\r\n    Entry<Text, Text> e;\r\n    for (String word : words) {\r\n        long numVal = Long.parseLong(word);\r\n        countType = LONG_VALUE_SUM;\r\n        id = \"count_\" + word;\r\n        e = generateEntry(countType, id, ValueAggregatorDescriptor.ONE);\r\n        if (e != null) {\r\n            retv.add(e);\r\n        }\r\n        countType = LONG_VALUE_MAX;\r\n        id = \"max\";\r\n        e = generateEntry(countType, id, new Text(word));\r\n        if (e != null) {\r\n            retv.add(e);\r\n        }\r\n        countType = LONG_VALUE_MIN;\r\n        id = \"min\";\r\n        e = generateEntry(countType, id, new Text(word));\r\n        if (e != null) {\r\n            retv.add(e);\r\n        }\r\n        countType = STRING_VALUE_MAX;\r\n        id = \"value_as_string_max\";\r\n        e = generateEntry(countType, id, new Text(\"\" + numVal));\r\n        if (e != null) {\r\n            retv.add(e);\r\n        }\r\n        countType = STRING_VALUE_MIN;\r\n        id = \"value_as_string_min\";\r\n        e = generateEntry(countType, id, new Text(\"\" + numVal));\r\n        if (e != null) {\r\n            retv.add(e);\r\n        }\r\n        countType = UNIQ_VALUE_COUNT;\r\n        id = \"uniq_count\";\r\n        e = generateEntry(countType, id, new Text(word));\r\n        if (e != null) {\r\n            retv.add(e);\r\n        }\r\n        countType = VALUE_HISTOGRAM;\r\n        id = \"histogram\";\r\n        e = generateEntry(countType, id, new Text(word));\r\n        if (e != null) {\r\n            retv.add(e);\r\n        }\r\n    }\r\n    return retv;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "writeEntities",
  "errType" : [ "Exception" ],
  "containingMethodsNum" : 38,
  "sourceCodeText" : "void writeEntities(Configuration tlConf, TimelineCollectorManager manager, Context context) throws IOException\n{\r\n    Configuration conf = context.getConfiguration();\r\n    int taskId = context.getTaskAttemptID().getTaskID().getId();\r\n    long timestamp = conf.getLong(TIMELINE_SERVICE_PERFORMANCE_RUN_ID, 0);\r\n    ApplicationId appId = ApplicationId.newInstance(timestamp, taskId);\r\n    AppLevelTimelineCollector collector = new AppLevelTimelineCollector(appId);\r\n    manager.putIfAbsent(appId, collector);\r\n    try {\r\n        TimelineCollectorContext tlContext = collector.getTimelineEntityContext();\r\n        tlContext.setFlowName(context.getJobName());\r\n        tlContext.setFlowRunId(timestamp);\r\n        tlContext.setUserId(context.getUser());\r\n        final int kbs = conf.getInt(KBS_SENT, KBS_SENT_DEFAULT);\r\n        long totalTime = 0;\r\n        final int testtimes = conf.getInt(TEST_TIMES, TEST_TIMES_DEFAULT);\r\n        final Random rand = new Random();\r\n        final TaskAttemptID taskAttemptId = context.getTaskAttemptID();\r\n        final char[] payLoad = new char[kbs * 1024];\r\n        for (int i = 0; i < testtimes; i++) {\r\n            for (int xx = 0; xx < kbs * 1024; xx++) {\r\n                int alphaNumIdx = rand.nextInt(ALPHA_NUMS.length);\r\n                payLoad[xx] = ALPHA_NUMS[alphaNumIdx];\r\n            }\r\n            String entId = taskAttemptId + \"_\" + Integer.toString(i);\r\n            final TimelineEntity entity = new TimelineEntity();\r\n            entity.setId(entId);\r\n            entity.setType(\"FOO_ATTEMPT\");\r\n            entity.addInfo(\"PERF_TEST\", payLoad);\r\n            TimelineEvent event = new TimelineEvent();\r\n            event.setId(\"foo_event_id\");\r\n            event.setTimestamp(System.currentTimeMillis());\r\n            event.addInfo(\"foo_event\", \"test\");\r\n            entity.addEvent(event);\r\n            TimelineMetric metric = new TimelineMetric();\r\n            metric.setId(\"foo_metric\");\r\n            metric.addValue(System.currentTimeMillis(), 123456789L);\r\n            entity.addMetric(metric);\r\n            entity.addConfig(\"foo\", \"bar\");\r\n            TimelineEntities entities = new TimelineEntities();\r\n            entities.addEntity(entity);\r\n            UserGroupInformation ugi = UserGroupInformation.getCurrentUser();\r\n            long startWrite = System.nanoTime();\r\n            try {\r\n                collector.putEntities(entities, ugi);\r\n            } catch (Exception e) {\r\n                context.getCounter(PerfCounters.TIMELINE_SERVICE_WRITE_FAILURES).increment(1);\r\n                LOG.error(\"writing to the timeline service failed\", e);\r\n            }\r\n            long endWrite = System.nanoTime();\r\n            totalTime += TimeUnit.NANOSECONDS.toMillis(endWrite - startWrite);\r\n        }\r\n        LOG.info(\"wrote \" + testtimes + \" entities (\" + kbs * testtimes + \" kB) in \" + totalTime + \" ms\");\r\n        context.getCounter(PerfCounters.TIMELINE_SERVICE_WRITE_TIME).increment(totalTime);\r\n        context.getCounter(PerfCounters.TIMELINE_SERVICE_WRITE_COUNTER).increment(testtimes);\r\n        context.getCounter(PerfCounters.TIMELINE_SERVICE_WRITE_KBS).increment(kbs * testtimes);\r\n    } finally {\r\n        manager.remove(appId);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 2,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred\\pipes",
  "methodName" : "getSplits",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "InputSplit[] getSplits(JobConf conf, int numSplits) throws IOException\n{\r\n    ArrayList<InputSplit> result = new ArrayList<InputSplit>();\r\n    FileSystem local = FileSystem.getLocal(conf);\r\n    for (Path dir : getInputPaths(conf)) {\r\n        for (FileStatus file : local.listStatus(dir)) {\r\n            result.add(new WordCountInputSplit(file.getPath()));\r\n        }\r\n    }\r\n    return result.toArray(new InputSplit[result.size()]);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred\\pipes",
  "methodName" : "getRecordReader",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "RecordReader<IntWritable, Text> getRecordReader(InputSplit split, JobConf conf, Reporter reporter)\n{\r\n    return new RecordReader<IntWritable, Text>() {\r\n\r\n        public boolean next(IntWritable key, Text value) throws IOException {\r\n            return false;\r\n        }\r\n\r\n        public IntWritable createKey() {\r\n            return new IntWritable();\r\n        }\r\n\r\n        public Text createValue() {\r\n            return new Text();\r\n        }\r\n\r\n        public long getPos() {\r\n            return 0;\r\n        }\r\n\r\n        public void close() {\r\n        }\r\n\r\n        public float getProgress() {\r\n            return 0.0f;\r\n        }\r\n    };\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "configure",
  "errType" : null,
  "containingMethodsNum" : 17,
  "sourceCodeText" : "void configure(JobConf conf, Path inDir, Path outDir, String input, Class<? extends Mapper> map, Class<? extends Reducer> reduce) throws IOException\n{\r\n    FileSystem inFs = inDir.getFileSystem(conf);\r\n    FileSystem outFs = outDir.getFileSystem(conf);\r\n    outFs.delete(outDir, true);\r\n    if (!inFs.mkdirs(inDir)) {\r\n        throw new IOException(\"Mkdirs failed to create \" + inDir.toString());\r\n    }\r\n    {\r\n        DataOutputStream file = inFs.create(new Path(inDir, \"part-0\"));\r\n        file.writeBytes(input);\r\n        file.close();\r\n    }\r\n    conf.setJobName(\"testmap\");\r\n    conf.setMapperClass(map);\r\n    conf.setReducerClass(reduce);\r\n    conf.setNumMapTasks(1);\r\n    conf.setNumReduceTasks(0);\r\n    FileInputFormat.setInputPaths(conf, inDir);\r\n    FileOutputFormat.setOutputPath(conf, outDir);\r\n    String TEST_ROOT_DIR = new Path(System.getProperty(\"test.build.data\", \"/tmp\")).toString().replace(' ', '+');\r\n    conf.set(\"test.build.data\", TEST_ROOT_DIR);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "checkEnv",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void checkEnv(String envName, String expValue, String mode)\n{\r\n    String envValue = System.getenv(envName);\r\n    if (\"append\".equals(mode)) {\r\n        if (envValue == null || !envValue.contains(File.pathSeparator)) {\r\n            throw new RuntimeException(\"Missing env variable\");\r\n        } else {\r\n            String[] parts = envValue.trim().split(File.pathSeparator);\r\n            if (!parts[parts.length - 1].equals(expValue)) {\r\n                throw new RuntimeException(\"Wrong env variable in append mode\");\r\n            }\r\n        }\r\n    } else {\r\n        if (envValue == null || !envValue.trim().equals(expValue)) {\r\n            throw new RuntimeException(\"Wrong env variable in noappend mode\");\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "setup",
  "errType" : null,
  "containingMethodsNum" : 9,
  "sourceCodeText" : "void setup() throws IOException\n{\r\n    dfs = new MiniDFSCluster.Builder(conf).build();\r\n    fileSys = dfs.getFileSystem();\r\n    if (!(new File(MiniMRYarnCluster.APPJAR)).exists()) {\r\n        LOG.info(\"MRAppJar \" + MiniMRYarnCluster.APPJAR + \" not found. Not running test.\");\r\n        return;\r\n    }\r\n    if (mr == null) {\r\n        mr = new MiniMRYarnCluster(TestMiniMRChildTask.class.getName());\r\n        Configuration conf = new Configuration();\r\n        mr.init(conf);\r\n        mr.start();\r\n    }\r\n    localFs.copyFromLocalFile(new Path(MiniMRYarnCluster.APPJAR), APP_JAR);\r\n    localFs.setPermission(APP_JAR, new FsPermission(\"700\"));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "tearDown",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void tearDown()\n{\r\n    try {\r\n        if (fileSys != null) {\r\n            fileSys.close();\r\n        }\r\n        if (dfs != null) {\r\n            dfs.shutdown();\r\n        }\r\n        if (mr != null) {\r\n            mr.stop();\r\n            mr = null;\r\n        }\r\n    } catch (IOException ioe) {\r\n        LOG.info(\"IO exception in closing file system)\");\r\n        ioe.printStackTrace();\r\n    }\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "testTaskEnv",
  "errType" : [ "Exception" ],
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void testTaskEnv()\n{\r\n    try {\r\n        JobConf conf = new JobConf(mr.getConfig());\r\n        String baseDir = System.getProperty(\"test.build.data\", \"build/test/data\");\r\n        Path inDir = new Path(baseDir + \"/testing/wc/input1\");\r\n        Path outDir = new Path(baseDir + \"/testing/wc/output1\");\r\n        FileSystem outFs = outDir.getFileSystem(conf);\r\n        runTestTaskEnv(conf, inDir, outDir, false);\r\n        outFs.delete(outDir, true);\r\n    } catch (Exception e) {\r\n        e.printStackTrace();\r\n        fail(\"Exception in testing child env\");\r\n        tearDown();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "testTaskOldEnv",
  "errType" : [ "Exception" ],
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void testTaskOldEnv()\n{\r\n    try {\r\n        JobConf conf = new JobConf(mr.getConfig());\r\n        String baseDir = System.getProperty(\"test.build.data\", \"build/test/data\");\r\n        Path inDir = new Path(baseDir + \"/testing/wc/input1\");\r\n        Path outDir = new Path(baseDir + \"/testing/wc/output1\");\r\n        FileSystem outFs = outDir.getFileSystem(conf);\r\n        runTestTaskEnv(conf, inDir, outDir, true);\r\n        outFs.delete(outDir, true);\r\n    } catch (Exception e) {\r\n        e.printStackTrace();\r\n        fail(\"Exception in testing child env\");\r\n        tearDown();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "runTestTaskEnv",
  "errType" : null,
  "containingMethodsNum" : 13,
  "sourceCodeText" : "void runTestTaskEnv(JobConf config, Path inDir, Path outDir, boolean oldConfigs) throws IOException, InterruptedException, ClassNotFoundException\n{\r\n    String input = \"The input\";\r\n    configure(config, inDir, outDir, input, EnvCheckMapper.class, EnvCheckReducer.class);\r\n    String mapTaskEnvKey = JobConf.MAPRED_MAP_TASK_ENV;\r\n    String reduceTaskEnvKey = JobConf.MAPRED_MAP_TASK_ENV;\r\n    String mapTaskJavaOptsKey = JobConf.MAPRED_MAP_TASK_JAVA_OPTS;\r\n    String reduceTaskJavaOptsKey = JobConf.MAPRED_REDUCE_TASK_JAVA_OPTS;\r\n    String mapTaskJavaOpts = MAP_OPTS_VAL;\r\n    String reduceTaskJavaOpts = REDUCE_OPTS_VAL;\r\n    config.setBoolean(OLD_CONFIGS, oldConfigs);\r\n    if (oldConfigs) {\r\n        mapTaskEnvKey = reduceTaskEnvKey = JobConf.MAPRED_TASK_ENV;\r\n        mapTaskJavaOptsKey = reduceTaskJavaOptsKey = JobConf.MAPRED_TASK_JAVA_OPTS;\r\n        mapTaskJavaOpts = reduceTaskJavaOpts = TASK_OPTS_VAL;\r\n    }\r\n    config.set(mapTaskEnvKey, Shell.WINDOWS ? \"MY_PATH=/tmp,LANG=en_us_8859_1,NEW_PATH=%MY_PATH%;/tmp\" : \"MY_PATH=/tmp,LANG=en_us_8859_1,NEW_PATH=$NEW_PATH:/tmp\");\r\n    config.set(reduceTaskEnvKey, Shell.WINDOWS ? \"MY_PATH=/tmp,LANG=en_us_8859_1,NEW_PATH=%MY_PATH%;/tmp\" : \"MY_PATH=/tmp,LANG=en_us_8859_1,NEW_PATH=$NEW_PATH:/tmp\");\r\n    config.set(mapTaskJavaOptsKey, mapTaskJavaOpts);\r\n    config.set(reduceTaskJavaOptsKey, reduceTaskJavaOpts);\r\n    Job job = Job.getInstance(config);\r\n    job.addFileToClassPath(APP_JAR);\r\n    job.setJarByClass(TestMiniMRChildTask.class);\r\n    job.setMaxMapAttempts(1);\r\n    job.waitForCompletion(true);\r\n    boolean succeeded = job.waitForCompletion(true);\r\n    assertTrue(\"The environment checker job failed.\", succeeded);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\util",
  "methodName" : "testRunjar",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void testRunjar() throws Throwable\n{\r\n    File outFile = new File(TEST_ROOT_DIR, \"out\");\r\n    if (outFile.exists()) {\r\n        outFile.delete();\r\n    }\r\n    File makeTestJar = makeTestJar();\r\n    String[] args = new String[3];\r\n    args[0] = makeTestJar.getAbsolutePath();\r\n    args[1] = \"org.apache.hadoop.util.Hello\";\r\n    args[2] = outFile.toString();\r\n    RunJar.main(args);\r\n    Assert.assertTrue(\"RunJar failed\", outFile.exists());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\util",
  "methodName" : "makeTestJar",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "File makeTestJar() throws IOException\n{\r\n    File jarFile = new File(TEST_ROOT_DIR, TEST_JAR_NAME);\r\n    JarOutputStream jstream = new JarOutputStream(new FileOutputStream(jarFile));\r\n    InputStream entryInputStream = this.getClass().getResourceAsStream(CLASS_NAME);\r\n    ZipEntry entry = new ZipEntry(\"org/apache/hadoop/util/\" + CLASS_NAME);\r\n    jstream.putNextEntry(entry);\r\n    BufferedInputStream bufInputStream = new BufferedInputStream(entryInputStream, 2048);\r\n    int count;\r\n    byte[] data = new byte[2048];\r\n    while ((count = bufInputStream.read(data, 0, 2048)) != -1) {\r\n        jstream.write(data, 0, count);\r\n    }\r\n    jstream.closeEntry();\r\n    jstream.close();\r\n    return jarFile;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2",
  "methodName" : "setup",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void setup() throws IOException\n{\r\n    if (!(new File(MiniMRYarnCluster.APPJAR)).exists()) {\r\n        LOG.info(\"MRAppJar \" + MiniMRYarnCluster.APPJAR + \" not found. Not running test.\");\r\n        return;\r\n    }\r\n    if (mrCluster == null) {\r\n        mrCluster = new MiniMRYarnCluster(TestMRJobs.class.getName(), 3);\r\n        Configuration conf = new Configuration();\r\n        mrCluster.init(conf);\r\n        mrCluster.start();\r\n    }\r\n    localFs.copyFromLocalFile(new Path(MiniMRYarnCluster.APPJAR), TestMRJobs.APP_JAR);\r\n    localFs.setPermission(TestMRJobs.APP_JAR, new FsPermission(\"700\"));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2",
  "methodName" : "tearDown",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void tearDown()\n{\r\n    if (mrCluster != null) {\r\n        mrCluster.stop();\r\n        mrCluster = null;\r\n    }\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2",
  "methodName" : "testCombinerShouldUpdateTheReporter",
  "errType" : null,
  "containingMethodsNum" : 18,
  "sourceCodeText" : "void testCombinerShouldUpdateTheReporter() throws Exception\n{\r\n    JobConf conf = new JobConf(mrCluster.getConfig());\r\n    int numMaps = 5;\r\n    int numReds = 2;\r\n    Path in = new Path(mrCluster.getTestWorkDir().getAbsolutePath(), \"testCombinerShouldUpdateTheReporter-in\");\r\n    Path out = new Path(mrCluster.getTestWorkDir().getAbsolutePath(), \"testCombinerShouldUpdateTheReporter-out\");\r\n    createInputOutPutFolder(in, out, numMaps);\r\n    conf.setJobName(\"test-job-with-combiner\");\r\n    conf.setMapperClass(IdentityMapper.class);\r\n    conf.setCombinerClass(MyCombinerToCheckReporter.class);\r\n    conf.setReducerClass(IdentityReducer.class);\r\n    Job.addFileToClassPath(TestMRJobs.APP_JAR, conf, TestMRJobs.APP_JAR.getFileSystem(conf));\r\n    conf.setOutputCommitter(CustomOutputCommitter.class);\r\n    conf.setInputFormat(TextInputFormat.class);\r\n    conf.setOutputKeyClass(LongWritable.class);\r\n    conf.setOutputValueClass(Text.class);\r\n    FileInputFormat.setInputPaths(conf, in);\r\n    FileOutputFormat.setOutputPath(conf, out);\r\n    conf.setNumMapTasks(numMaps);\r\n    conf.setNumReduceTasks(numReds);\r\n    runJob(conf);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2",
  "methodName" : "createInputOutPutFolder",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void createInputOutPutFolder(Path inDir, Path outDir, int numMaps) throws Exception\n{\r\n    FileSystem fs = FileSystem.get(conf);\r\n    if (fs.exists(outDir)) {\r\n        fs.delete(outDir, true);\r\n    }\r\n    if (!fs.exists(inDir)) {\r\n        fs.mkdirs(inDir);\r\n    }\r\n    String input = \"The quick brown fox\\n\" + \"has many silly\\n\" + \"red fox sox\\n\";\r\n    for (int i = 0; i < numMaps; ++i) {\r\n        DataOutputStream file = fs.create(new Path(inDir, \"part-\" + i));\r\n        file.writeBytes(input);\r\n        file.close();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2",
  "methodName" : "runJob",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "boolean runJob(JobConf conf) throws Exception\n{\r\n    JobClient jobClient = new JobClient(conf);\r\n    RunningJob job = jobClient.submitJob(conf);\r\n    return jobClient.monitorAndPrintJob(conf, job);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\db",
  "methodName" : "startHsqldbServer",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void startHsqldbServer()\n{\r\n    if (null == server) {\r\n        server = new Server();\r\n        server.setDatabasePath(0, System.getProperty(\"test.build.data\", \"/tmp\") + \"/\" + DB_NAME);\r\n        server.setDatabaseName(0, DB_NAME);\r\n        server.start();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\db",
  "methodName" : "createConnection",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void createConnection(String driverClassName, String url) throws Exception\n{\r\n    Class.forName(driverClassName);\r\n    connection = DriverManager.getConnection(url);\r\n    connection.setAutoCommit(false);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\db",
  "methodName" : "shutdown",
  "errType" : [ "Throwable", "Throwable" ],
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void shutdown()\n{\r\n    try {\r\n        connection.commit();\r\n        connection.close();\r\n        connection = null;\r\n    } catch (Throwable ex) {\r\n        LOG.warn(\"Exception occurred while closing connection :\" + StringUtils.stringifyException(ex));\r\n    } finally {\r\n        try {\r\n            if (server != null) {\r\n                server.shutdown();\r\n            }\r\n        } catch (Throwable ex) {\r\n            LOG.warn(\"Exception occurred while shutting down HSQLDB :\" + StringUtils.stringifyException(ex));\r\n        }\r\n        server = null;\r\n    }\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 2,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\db",
  "methodName" : "initialize",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void initialize(String driverClassName, String url) throws Exception\n{\r\n    startHsqldbServer();\r\n    createConnection(driverClassName, url);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\db",
  "methodName" : "setUp",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void setUp() throws Exception\n{\r\n    initialize(DRIVER_CLASS, DB_URL);\r\n    super.setUp();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\db",
  "methodName" : "tearDown",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void tearDown() throws Exception\n{\r\n    super.tearDown();\r\n    shutdown();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\db",
  "methodName" : "testDateSplits",
  "errType" : [ "SQLException" ],
  "containingMethodsNum" : 26,
  "sourceCodeText" : "void testDateSplits() throws Exception\n{\r\n    Statement s = connection.createStatement();\r\n    final String DATE_TABLE = \"datetable\";\r\n    final String COL = \"foo\";\r\n    try {\r\n        s.executeUpdate(\"DROP TABLE \" + DATE_TABLE);\r\n    } catch (SQLException e) {\r\n    }\r\n    s.executeUpdate(\"CREATE TABLE \" + DATE_TABLE + \"(\" + COL + \" DATE)\");\r\n    s.executeUpdate(\"INSERT INTO \" + DATE_TABLE + \" VALUES('2010-04-01')\");\r\n    s.executeUpdate(\"INSERT INTO \" + DATE_TABLE + \" VALUES('2010-04-02')\");\r\n    s.executeUpdate(\"INSERT INTO \" + DATE_TABLE + \" VALUES('2010-05-01')\");\r\n    s.executeUpdate(\"INSERT INTO \" + DATE_TABLE + \" VALUES('2011-04-01')\");\r\n    connection.commit();\r\n    Configuration conf = new Configuration();\r\n    conf.set(\"fs.defaultFS\", \"file:///\");\r\n    FileSystem fs = FileSystem.getLocal(conf);\r\n    fs.delete(new Path(OUT_DIR), true);\r\n    Job job = Job.getInstance(conf);\r\n    job.setMapperClass(ValMapper.class);\r\n    job.setReducerClass(Reducer.class);\r\n    job.setMapOutputKeyClass(DateCol.class);\r\n    job.setMapOutputValueClass(NullWritable.class);\r\n    job.setOutputKeyClass(DateCol.class);\r\n    job.setOutputValueClass(NullWritable.class);\r\n    job.setNumReduceTasks(1);\r\n    job.getConfiguration().setInt(\"mapreduce.map.tasks\", 2);\r\n    FileOutputFormat.setOutputPath(job, new Path(OUT_DIR));\r\n    DBConfiguration.configureDB(job.getConfiguration(), DRIVER_CLASS, DB_URL, null, null);\r\n    DataDrivenDBInputFormat.setInput(job, DateCol.class, DATE_TABLE, null, COL, COL);\r\n    boolean ret = job.waitForCompletion(true);\r\n    assertTrue(\"job failed\", ret);\r\n    assertEquals(\"Did not get all the records\", 4, job.getCounters().findCounter(TaskCounter.REDUCE_OUTPUT_RECORDS).getValue());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\security",
  "methodName" : "testDelegationToken",
  "errType" : [ "IOException", "IOException", "IOException", "IOException", "IOException" ],
  "containingMethodsNum" : 51,
  "sourceCodeText" : "void testDelegationToken() throws IOException, InterruptedException\n{\r\n    org.apache.log4j.Logger rootLogger = LogManager.getRootLogger();\r\n    rootLogger.setLevel(Level.DEBUG);\r\n    final YarnConfiguration conf = new YarnConfiguration(new JobConf());\r\n    conf.set(JHAdminConfig.MR_HISTORY_PRINCIPAL, \"RandomOrc/localhost@apache.org\");\r\n    conf.set(CommonConfigurationKeysPublic.HADOOP_SECURITY_AUTHENTICATION, \"kerberos\");\r\n    UserGroupInformation.setConfiguration(conf);\r\n    final long initialInterval = 10000l;\r\n    final long maxLifetime = 20000l;\r\n    final long renewInterval = 10000l;\r\n    JobHistoryServer jobHistoryServer = null;\r\n    MRClientProtocol clientUsingDT = null;\r\n    long tokenFetchTime;\r\n    try {\r\n        jobHistoryServer = new JobHistoryServer() {\r\n\r\n            protected void doSecureLogin(Configuration conf) throws IOException {\r\n            }\r\n\r\n            @Override\r\n            protected JHSDelegationTokenSecretManager createJHSSecretManager(Configuration conf, HistoryServerStateStoreService store) {\r\n                return new JHSDelegationTokenSecretManager(initialInterval, maxLifetime, renewInterval, 3600000, store);\r\n            }\r\n        };\r\n        jobHistoryServer.init(conf);\r\n        jobHistoryServer.start();\r\n        final MRClientProtocol hsService = jobHistoryServer.getClientService().getClientHandler();\r\n        UserGroupInformation loggedInUser = UserGroupInformation.createRemoteUser(\"testrenewer@APACHE.ORG\");\r\n        Assert.assertEquals(\"testrenewer\", loggedInUser.getShortUserName());\r\n        loggedInUser.setAuthenticationMethod(AuthenticationMethod.KERBEROS);\r\n        Token token = getDelegationToken(loggedInUser, hsService, loggedInUser.getShortUserName());\r\n        tokenFetchTime = System.currentTimeMillis();\r\n        LOG.info(\"Got delegation token at: \" + tokenFetchTime);\r\n        clientUsingDT = getMRClientProtocol(token, jobHistoryServer.getClientService().getBindAddress(), \"TheDarkLord\", conf);\r\n        GetJobReportRequest jobReportRequest = Records.newRecord(GetJobReportRequest.class);\r\n        jobReportRequest.setJobId(MRBuilderUtils.newJobId(123456, 1, 1));\r\n        try {\r\n            clientUsingDT.getJobReport(jobReportRequest);\r\n        } catch (IOException e) {\r\n            Assert.assertEquals(\"Unknown job job_123456_0001\", e.getMessage());\r\n        }\r\n        while (System.currentTimeMillis() < tokenFetchTime + initialInterval / 2) {\r\n            Thread.sleep(500l);\r\n        }\r\n        long nextExpTime = renewDelegationToken(loggedInUser, hsService, token);\r\n        long renewalTime = System.currentTimeMillis();\r\n        LOG.info(\"Renewed token at: \" + renewalTime + \", NextExpiryTime: \" + nextExpTime);\r\n        while (System.currentTimeMillis() > tokenFetchTime + initialInterval && System.currentTimeMillis() < nextExpTime) {\r\n            Thread.sleep(500l);\r\n        }\r\n        Thread.sleep(50l);\r\n        try {\r\n            clientUsingDT.getJobReport(jobReportRequest);\r\n        } catch (IOException e) {\r\n            Assert.assertEquals(\"Unknown job job_123456_0001\", e.getMessage());\r\n        }\r\n        while (System.currentTimeMillis() < renewalTime + renewInterval) {\r\n            Thread.sleep(500l);\r\n        }\r\n        Thread.sleep(50l);\r\n        LOG.info(\"At time: \" + System.currentTimeMillis() + \", token should be invalid\");\r\n        try {\r\n            clientUsingDT.getJobReport(jobReportRequest);\r\n            fail(\"Should not have succeeded with an expired token\");\r\n        } catch (IOException e) {\r\n            assertTrue(e.getCause().getMessage().contains(\"is expired\"));\r\n        }\r\n        if (clientUsingDT != null) {\r\n            clientUsingDT = null;\r\n        }\r\n        token = getDelegationToken(loggedInUser, hsService, loggedInUser.getShortUserName());\r\n        tokenFetchTime = System.currentTimeMillis();\r\n        LOG.info(\"Got delegation token at: \" + tokenFetchTime);\r\n        clientUsingDT = getMRClientProtocol(token, jobHistoryServer.getClientService().getBindAddress(), \"loginuser2\", conf);\r\n        try {\r\n            clientUsingDT.getJobReport(jobReportRequest);\r\n        } catch (IOException e) {\r\n            fail(\"Unexpected exception\" + e);\r\n        }\r\n        cancelDelegationToken(loggedInUser, hsService, token);\r\n        Token tokenWithDifferentRenewer = getDelegationToken(loggedInUser, hsService, \"yarn\");\r\n        cancelDelegationToken(loggedInUser, hsService, tokenWithDifferentRenewer);\r\n        if (clientUsingDT != null) {\r\n            clientUsingDT = null;\r\n        }\r\n        clientUsingDT = getMRClientProtocol(token, jobHistoryServer.getClientService().getBindAddress(), \"loginuser2\", conf);\r\n        LOG.info(\"Cancelled delegation token at: \" + System.currentTimeMillis());\r\n        try {\r\n            clientUsingDT.getJobReport(jobReportRequest);\r\n            fail(\"Should not have succeeded with a cancelled delegation token\");\r\n        } catch (IOException e) {\r\n        }\r\n    } finally {\r\n        jobHistoryServer.stop();\r\n    }\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 6,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\security",
  "methodName" : "getDelegationToken",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Token getDelegationToken(final UserGroupInformation loggedInUser, final MRClientProtocol hsService, final String renewerString) throws IOException, InterruptedException\n{\r\n    Token token = loggedInUser.doAs(new PrivilegedExceptionAction<Token>() {\r\n\r\n        @Override\r\n        public Token run() throws IOException {\r\n            GetDelegationTokenRequest request = Records.newRecord(GetDelegationTokenRequest.class);\r\n            request.setRenewer(renewerString);\r\n            return hsService.getDelegationToken(request).getDelegationToken();\r\n        }\r\n    });\r\n    return token;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\security",
  "methodName" : "renewDelegationToken",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "long renewDelegationToken(final UserGroupInformation loggedInUser, final MRClientProtocol hsService, final Token dToken) throws IOException, InterruptedException\n{\r\n    long nextExpTime = loggedInUser.doAs(new PrivilegedExceptionAction<Long>() {\r\n\r\n        @Override\r\n        public Long run() throws IOException {\r\n            RenewDelegationTokenRequest request = Records.newRecord(RenewDelegationTokenRequest.class);\r\n            request.setDelegationToken(dToken);\r\n            return hsService.renewDelegationToken(request).getNextExpirationTime();\r\n        }\r\n    });\r\n    return nextExpTime;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\security",
  "methodName" : "cancelDelegationToken",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void cancelDelegationToken(final UserGroupInformation loggedInUser, final MRClientProtocol hsService, final Token dToken) throws IOException, InterruptedException\n{\r\n    loggedInUser.doAs(new PrivilegedExceptionAction<Void>() {\r\n\r\n        @Override\r\n        public Void run() throws IOException {\r\n            CancelDelegationTokenRequest request = Records.newRecord(CancelDelegationTokenRequest.class);\r\n            request.setDelegationToken(dToken);\r\n            hsService.cancelDelegationToken(request);\r\n            return null;\r\n        }\r\n    });\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\security",
  "methodName" : "getMRClientProtocol",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "MRClientProtocol getMRClientProtocol(Token token, final InetSocketAddress hsAddress, String user, final Configuration conf)\n{\r\n    UserGroupInformation ugi = UserGroupInformation.createRemoteUser(user);\r\n    ugi.addToken(ConverterUtils.convertFromYarn(token, hsAddress));\r\n    final YarnRPC rpc = YarnRPC.create(conf);\r\n    MRClientProtocol hsWithDT = ugi.doAs(new PrivilegedAction<MRClientProtocol>() {\r\n\r\n        @Override\r\n        public MRClientProtocol run() {\r\n            return (MRClientProtocol) rpc.getProxy(HSClientProtocol.class, hsAddress, conf);\r\n        }\r\n    });\r\n    return hsWithDT;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\fs\\slive",
  "methodName" : "getTruncateFile",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Path getTruncateFile()\n{\r\n    Path fn = getFinder().getFile();\r\n    return fn;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\fs\\slive",
  "methodName" : "run",
  "errType" : [ "FileNotFoundException", "IOException|UnsupportedOperationException" ],
  "containingMethodsNum" : 21,
  "sourceCodeText" : "List<OperationOutput> run(FileSystem fs)\n{\r\n    List<OperationOutput> out = super.run(fs);\r\n    try {\r\n        Path fn = getTruncateFile();\r\n        boolean waitOnTruncate = getConfig().shouldWaitOnTruncate();\r\n        long currentSize = fs.getFileStatus(fn).getLen();\r\n        Range<Long> truncateSizeRange = getConfig().getTruncateSize();\r\n        if (getConfig().shouldTruncateUseBlockSize()) {\r\n            truncateSizeRange = getConfig().getBlockSize();\r\n        }\r\n        long truncateSize = Math.max(0L, currentSize - Range.betweenPositive(getRandom(), truncateSizeRange));\r\n        long timeTaken = 0;\r\n        LOG.info(\"Attempting to truncate file at \" + fn + \" to size \" + Helper.toByteInfo(truncateSize));\r\n        {\r\n            long startTime = Timer.now();\r\n            boolean completed = fs.truncate(fn, truncateSize);\r\n            if (!completed && waitOnTruncate)\r\n                waitForRecovery(fs, fn, truncateSize);\r\n            timeTaken += Timer.elapsed(startTime);\r\n        }\r\n        out.add(new OperationOutput(OutputType.LONG, getType(), ReportWriter.BYTES_WRITTEN, 0));\r\n        out.add(new OperationOutput(OutputType.LONG, getType(), ReportWriter.OK_TIME_TAKEN, timeTaken));\r\n        out.add(new OperationOutput(OutputType.LONG, getType(), ReportWriter.SUCCESSES, 1L));\r\n        LOG.info(\"Truncate file \" + fn + \" to \" + Helper.toByteInfo(truncateSize) + \" in \" + timeTaken + \" milliseconds\");\r\n    } catch (FileNotFoundException e) {\r\n        out.add(new OperationOutput(OutputType.LONG, getType(), ReportWriter.NOT_FOUND, 1L));\r\n        LOG.warn(\"Error with truncating\", e);\r\n    } catch (IOException | UnsupportedOperationException e) {\r\n        out.add(new OperationOutput(OutputType.LONG, getType(), ReportWriter.FAILURES, 1L));\r\n        LOG.warn(\"Error with truncating\", e);\r\n    }\r\n    return out;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\fs\\slive",
  "methodName" : "waitForRecovery",
  "errType" : [ "InterruptedException" ],
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void waitForRecovery(FileSystem fs, Path fn, long newLength) throws IOException\n{\r\n    LOG.info(\"Waiting on truncate file recovery for \" + fn);\r\n    for (; ; ) {\r\n        FileStatus stat = fs.getFileStatus(fn);\r\n        if (stat.getLen() == newLength)\r\n            break;\r\n        try {\r\n            Thread.sleep(1000);\r\n        } catch (InterruptedException ignored) {\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\input",
  "methodName" : "testAddInputPath",
  "errType" : null,
  "containingMethodsNum" : 16,
  "sourceCodeText" : "void testAddInputPath() throws IOException\n{\r\n    final Configuration conf = new Configuration();\r\n    conf.set(\"fs.defaultFS\", \"file:///abc/\");\r\n    final Job j = Job.getInstance(conf);\r\n    final FileSystem defaultfs = FileSystem.get(conf);\r\n    System.out.println(\"defaultfs.getUri() = \" + defaultfs.getUri());\r\n    {\r\n        final Path original = new Path(\"file:/foo\");\r\n        System.out.println(\"original = \" + original);\r\n        FileInputFormat.addInputPath(j, original);\r\n        final Path[] results = FileInputFormat.getInputPaths(j);\r\n        System.out.println(\"results = \" + Arrays.asList(results));\r\n        assertEquals(1, results.length);\r\n        assertEquals(original, results[0]);\r\n    }\r\n    {\r\n        final Path original = new Path(\"file:/bar\");\r\n        System.out.println(\"original = \" + original);\r\n        FileInputFormat.setInputPaths(j, original);\r\n        final Path[] results = FileInputFormat.getInputPaths(j);\r\n        System.out.println(\"results = \" + Arrays.asList(results));\r\n        assertEquals(1, results.length);\r\n        assertEquals(original, results[0]);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\input",
  "methodName" : "testNumInputFiles",
  "errType" : null,
  "containingMethodsNum" : 9,
  "sourceCodeText" : "void testNumInputFiles() throws Exception\n{\r\n    Configuration conf = spy(new Configuration());\r\n    Job mockedJob = mock(Job.class);\r\n    when(mockedJob.getConfiguration()).thenReturn(conf);\r\n    FileStatus stat = mock(FileStatus.class);\r\n    when(stat.getLen()).thenReturn(0L);\r\n    TextInputFormat ispy = spy(new TextInputFormat());\r\n    doReturn(Arrays.asList(stat)).when(ispy).listStatus(mockedJob);\r\n    ispy.getSplits(mockedJob);\r\n    verify(conf).setLong(FileInputFormat.NUM_INPUT_FILES, 1);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\input",
  "methodName" : "testLastInputSplitAtSplitBoundary",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void testLastInputSplitAtSplitBoundary() throws Exception\n{\r\n    FileInputFormat fif = new FileInputFormatForTest(1024l * 1024 * 1024, 128l * 1024 * 1024);\r\n    Configuration conf = new Configuration();\r\n    JobContext jobContext = mock(JobContext.class);\r\n    when(jobContext.getConfiguration()).thenReturn(conf);\r\n    List<InputSplit> splits = fif.getSplits(jobContext);\r\n    assertEquals(8, splits.size());\r\n    for (int i = 0; i < splits.size(); i++) {\r\n        InputSplit split = splits.get(i);\r\n        assertEquals((\"host\" + i), split.getLocations()[0]);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\input",
  "methodName" : "testLastInputSplitExceedingSplitBoundary",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void testLastInputSplitExceedingSplitBoundary() throws Exception\n{\r\n    FileInputFormat fif = new FileInputFormatForTest(1027l * 1024 * 1024, 128l * 1024 * 1024);\r\n    Configuration conf = new Configuration();\r\n    JobContext jobContext = mock(JobContext.class);\r\n    when(jobContext.getConfiguration()).thenReturn(conf);\r\n    List<InputSplit> splits = fif.getSplits(jobContext);\r\n    assertEquals(8, splits.size());\r\n    for (int i = 0; i < splits.size(); i++) {\r\n        InputSplit split = splits.get(i);\r\n        assertEquals((\"host\" + i), split.getLocations()[0]);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\input",
  "methodName" : "testLastInputSplitSingleSplit",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void testLastInputSplitSingleSplit() throws Exception\n{\r\n    FileInputFormat fif = new FileInputFormatForTest(100l * 1024 * 1024, 128l * 1024 * 1024);\r\n    Configuration conf = new Configuration();\r\n    JobContext jobContext = mock(JobContext.class);\r\n    when(jobContext.getConfiguration()).thenReturn(conf);\r\n    List<InputSplit> splits = fif.getSplits(jobContext);\r\n    assertEquals(1, splits.size());\r\n    for (int i = 0; i < splits.size(); i++) {\r\n        InputSplit split = splits.get(i);\r\n        assertEquals((\"host\" + i), split.getLocations()[0]);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\lib\\input",
  "methodName" : "testForEmptyFile",
  "errType" : null,
  "containingMethodsNum" : 14,
  "sourceCodeText" : "void testForEmptyFile() throws Exception\n{\r\n    Configuration conf = new Configuration();\r\n    FileSystem fileSys = FileSystem.get(conf);\r\n    Path file = new Path(\"test\" + \"/file\");\r\n    FSDataOutputStream out = fileSys.create(file, true, conf.getInt(\"io.file.buffer.size\", 4096), (short) 1, (long) 1024);\r\n    out.write(new byte[0]);\r\n    out.close();\r\n    DummyInputFormat inFormat = new DummyInputFormat();\r\n    Job job = Job.getInstance(conf);\r\n    FileInputFormat.setInputPaths(job, \"test\");\r\n    List<InputSplit> splits = inFormat.getSplits(job);\r\n    assertEquals(1, splits.size());\r\n    FileSplit fileSplit = (FileSplit) splits.get(0);\r\n    assertEquals(0, fileSplit.getLocations().length);\r\n    assertEquals(file.getName(), fileSplit.getPath().getName());\r\n    assertEquals(0, fileSplit.getStart());\r\n    assertEquals(0, fileSplit.getLength());\r\n    fileSys.delete(file.getParent(), true);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2",
  "methodName" : "copyAppJarIntoTestDir",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String copyAppJarIntoTestDir(String testSubdir)\n{\r\n    return JarFinder.getJar(LocalContainerLauncher.class, testSubdir);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2",
  "methodName" : "getResolvedMRHistoryWebAppURLWithoutScheme",
  "errType" : [ "UnknownHostException" ],
  "containingMethodsNum" : 12,
  "sourceCodeText" : "String getResolvedMRHistoryWebAppURLWithoutScheme(Configuration conf, boolean isSSLEnabled)\n{\r\n    InetSocketAddress address = null;\r\n    if (isSSLEnabled) {\r\n        address = conf.getSocketAddr(JHAdminConfig.MR_HISTORY_WEBAPP_HTTPS_ADDRESS, JHAdminConfig.DEFAULT_MR_HISTORY_WEBAPP_HTTPS_ADDRESS, JHAdminConfig.DEFAULT_MR_HISTORY_WEBAPP_HTTPS_PORT);\r\n    } else {\r\n        address = conf.getSocketAddr(JHAdminConfig.MR_HISTORY_WEBAPP_ADDRESS, JHAdminConfig.DEFAULT_MR_HISTORY_WEBAPP_ADDRESS, JHAdminConfig.DEFAULT_MR_HISTORY_WEBAPP_PORT);\r\n    }\r\n    address = NetUtils.getConnectAddress(address);\r\n    StringBuffer sb = new StringBuffer();\r\n    InetAddress resolved = address.getAddress();\r\n    if (resolved == null || resolved.isAnyLocalAddress() || resolved.isLoopbackAddress()) {\r\n        String lh = address.getHostName();\r\n        try {\r\n            lh = InetAddress.getLocalHost().getCanonicalHostName();\r\n        } catch (UnknownHostException e) {\r\n        }\r\n        sb.append(lh);\r\n    } else {\r\n        sb.append(address.getHostName());\r\n    }\r\n    sb.append(\":\").append(address.getPort());\r\n    return sb.toString();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2",
  "methodName" : "serviceInit",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 32,
  "sourceCodeText" : "void serviceInit(Configuration conf) throws Exception\n{\r\n    if (conf.getBoolean(MR_HISTORY_MINICLUSTER_ENABLED, true)) {\r\n        historyServerWrapper = new JobHistoryServerWrapper();\r\n        addService(historyServerWrapper);\r\n    }\r\n    conf.set(MRConfig.FRAMEWORK_NAME, MRConfig.YARN_FRAMEWORK_NAME);\r\n    String stagingDir = conf.get(MRJobConfig.MR_AM_STAGING_DIR);\r\n    if (stagingDir == null || stagingDir.equals(MRJobConfig.DEFAULT_MR_AM_STAGING_DIR)) {\r\n        conf.set(MRJobConfig.MR_AM_STAGING_DIR, new File(getTestWorkDir(), \"apps_staging_dir/\").getAbsolutePath());\r\n    }\r\n    if (!conf.getBoolean(MRConfig.MAPREDUCE_MINICLUSTER_CONTROL_RESOURCE_MONITORING, MRConfig.DEFAULT_MAPREDUCE_MINICLUSTER_CONTROL_RESOURCE_MONITORING)) {\r\n        conf.setBoolean(YarnConfiguration.NM_PMEM_CHECK_ENABLED, false);\r\n        conf.setBoolean(YarnConfiguration.NM_VMEM_CHECK_ENABLED, false);\r\n    }\r\n    conf.set(CommonConfigurationKeys.FS_PERMISSIONS_UMASK_KEY, \"000\");\r\n    try {\r\n        Path stagingPath = FileContext.getFileContext(conf).makeQualified(new Path(conf.get(MRJobConfig.MR_AM_STAGING_DIR)));\r\n        if (Path.WINDOWS) {\r\n            if (LocalFileSystem.class.isInstance(stagingPath.getFileSystem(conf))) {\r\n                conf.set(MRJobConfig.MR_AM_STAGING_DIR, new File(conf.get(MRJobConfig.MR_AM_STAGING_DIR)).getAbsolutePath());\r\n            }\r\n        }\r\n        FileContext fc = FileContext.getFileContext(stagingPath.toUri(), conf);\r\n        if (fc.util().exists(stagingPath)) {\r\n            LOG.info(stagingPath + \" exists! deleting...\");\r\n            fc.delete(stagingPath, true);\r\n        }\r\n        LOG.info(\"mkdir: \" + stagingPath);\r\n        fc.mkdir(stagingPath, null, true);\r\n        String doneDir = JobHistoryUtils.getConfiguredHistoryServerDoneDirPrefix(conf);\r\n        Path doneDirPath = fc.makeQualified(new Path(doneDir));\r\n        fc.mkdir(doneDirPath, null, true);\r\n    } catch (IOException e) {\r\n        throw new YarnRuntimeException(\"Could not create staging directory. \", e);\r\n    }\r\n    conf.set(MRConfig.MASTER_ADDRESS, \"test\");\r\n    String[] nmAuxServices = conf.getStrings(YarnConfiguration.NM_AUX_SERVICES);\r\n    boolean enableTimelineAuxService = false;\r\n    if (nmAuxServices != null) {\r\n        for (String nmAuxService : nmAuxServices) {\r\n            if (nmAuxService.equals(TIMELINE_AUX_SERVICE_NAME)) {\r\n                enableTimelineAuxService = true;\r\n                break;\r\n            }\r\n        }\r\n    }\r\n    if (enableTimelineAuxService) {\r\n        conf.setStrings(YarnConfiguration.NM_AUX_SERVICES, new String[] { ShuffleHandler.MAPREDUCE_SHUFFLE_SERVICEID, TIMELINE_AUX_SERVICE_NAME });\r\n    } else {\r\n        conf.setStrings(YarnConfiguration.NM_AUX_SERVICES, new String[] { ShuffleHandler.MAPREDUCE_SHUFFLE_SERVICEID });\r\n    }\r\n    conf.setClass(String.format(YarnConfiguration.NM_AUX_SERVICE_FMT, ShuffleHandler.MAPREDUCE_SHUFFLE_SERVICEID), ShuffleHandler.class, Service.class);\r\n    conf.setInt(ShuffleHandler.SHUFFLE_PORT_CONFIG_KEY, 0);\r\n    conf.setClass(YarnConfiguration.NM_CONTAINER_EXECUTOR, DefaultContainerExecutor.class, ContainerExecutor.class);\r\n    conf.setBoolean(MRJobConfig.JOB_UBERTASK_ENABLE, false);\r\n    super.serviceInit(conf);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2",
  "methodName" : "serviceStart",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void serviceStart() throws Exception\n{\r\n    super.serviceStart();\r\n    if (historyServer != null) {\r\n        getConfig().set(JHAdminConfig.MR_HISTORY_ADDRESS, historyServer.getConfig().get(JHAdminConfig.MR_HISTORY_ADDRESS));\r\n        MRWebAppUtil.setJHSWebappURLWithoutScheme(getConfig(), MRWebAppUtil.getJHSWebappURLWithoutScheme(historyServer.getConfig()));\r\n    }\r\n    LOG.info(\"MiniMRYARN ResourceManager address: \" + getConfig().get(YarnConfiguration.RM_ADDRESS));\r\n    LOG.info(\"MiniMRYARN ResourceManager web address: \" + WebAppUtils.getRMWebAppURLWithoutScheme(getConfig()));\r\n    LOG.info(\"MiniMRYARN HistoryServer address: \" + getConfig().get(JHAdminConfig.MR_HISTORY_ADDRESS));\r\n    LOG.info(\"MiniMRYARN HistoryServer web address: \" + getResolvedMRHistoryWebAppURLWithoutScheme(getConfig(), MRWebAppUtil.getJHSHttpPolicy() == HttpConfig.Policy.HTTPS_ONLY));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2",
  "methodName" : "getHistoryServer",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "JobHistoryServer getHistoryServer()\n{\r\n    return this.historyServer;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "createTimelineEntities",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "Set<TimelineEntity> createTimelineEntities(JobInfo jobInfo, Configuration conf)\n{\r\n    Set<TimelineEntity> entities = new HashSet<>();\r\n    TimelineEntity job = createJobEntity(jobInfo, conf);\r\n    entities.add(job);\r\n    Set<TimelineEntity> tasksAndAttempts = createTaskAndTaskAttemptEntities(jobInfo);\r\n    entities.addAll(tasksAndAttempts);\r\n    return entities;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "createJobEntity",
  "errType" : null,
  "containingMethodsNum" : 16,
  "sourceCodeText" : "TimelineEntity createJobEntity(JobInfo jobInfo, Configuration conf)\n{\r\n    TimelineEntity job = new TimelineEntity();\r\n    job.setEntityType(JOB);\r\n    job.setEntityId(jobInfo.getJobId().toString());\r\n    job.setStartTime(jobInfo.getSubmitTime());\r\n    job.addPrimaryFilter(\"JOBNAME\", jobInfo.getJobname());\r\n    job.addPrimaryFilter(\"USERNAME\", jobInfo.getUsername());\r\n    job.addOtherInfo(\"JOB_QUEUE_NAME\", jobInfo.getJobQueueName());\r\n    job.addOtherInfo(\"SUBMIT_TIME\", jobInfo.getSubmitTime());\r\n    job.addOtherInfo(\"LAUNCH_TIME\", jobInfo.getLaunchTime());\r\n    job.addOtherInfo(\"FINISH_TIME\", jobInfo.getFinishTime());\r\n    job.addOtherInfo(\"JOB_STATUS\", jobInfo.getJobStatus());\r\n    job.addOtherInfo(\"PRIORITY\", jobInfo.getPriority());\r\n    job.addOtherInfo(\"TOTAL_MAPS\", jobInfo.getTotalMaps());\r\n    job.addOtherInfo(\"TOTAL_REDUCES\", jobInfo.getTotalReduces());\r\n    job.addOtherInfo(\"UBERIZED\", jobInfo.getUberized());\r\n    job.addOtherInfo(\"ERROR_INFO\", jobInfo.getErrorInfo());\r\n    LOG.info(\"converted job \" + jobInfo.getJobId() + \" to a timeline entity\");\r\n    return job;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "createTaskAndTaskAttemptEntities",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "Set<TimelineEntity> createTaskAndTaskAttemptEntities(JobInfo jobInfo)\n{\r\n    Set<TimelineEntity> entities = new HashSet<>();\r\n    Map<TaskID, TaskInfo> taskInfoMap = jobInfo.getAllTasks();\r\n    LOG.info(\"job \" + jobInfo.getJobId() + \" has \" + taskInfoMap.size() + \" tasks\");\r\n    for (TaskInfo taskInfo : taskInfoMap.values()) {\r\n        TimelineEntity task = createTaskEntity(taskInfo);\r\n        entities.add(task);\r\n        Set<TimelineEntity> taskAttempts = createTaskAttemptEntities(taskInfo);\r\n        entities.addAll(taskAttempts);\r\n    }\r\n    return entities;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "createTaskEntity",
  "errType" : null,
  "containingMethodsNum" : 9,
  "sourceCodeText" : "TimelineEntity createTaskEntity(TaskInfo taskInfo)\n{\r\n    TimelineEntity task = new TimelineEntity();\r\n    task.setEntityType(TASK);\r\n    task.setEntityId(taskInfo.getTaskId().toString());\r\n    task.setStartTime(taskInfo.getStartTime());\r\n    task.addOtherInfo(\"START_TIME\", taskInfo.getStartTime());\r\n    task.addOtherInfo(\"FINISH_TIME\", taskInfo.getFinishTime());\r\n    task.addOtherInfo(\"TASK_TYPE\", taskInfo.getTaskType());\r\n    task.addOtherInfo(\"TASK_STATUS\", taskInfo.getTaskStatus());\r\n    task.addOtherInfo(\"ERROR_INFO\", taskInfo.getError());\r\n    LOG.info(\"converted task \" + taskInfo.getTaskId() + \" to a timeline entity\");\r\n    return task;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "createTaskAttemptEntities",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "Set<TimelineEntity> createTaskAttemptEntities(TaskInfo taskInfo)\n{\r\n    Set<TimelineEntity> taskAttempts = new HashSet<TimelineEntity>();\r\n    Map<TaskAttemptID, TaskAttemptInfo> taskAttemptInfoMap = taskInfo.getAllTaskAttempts();\r\n    LOG.info(\"task \" + taskInfo.getTaskId() + \" has \" + taskAttemptInfoMap.size() + \" task attempts\");\r\n    for (TaskAttemptInfo taskAttemptInfo : taskAttemptInfoMap.values()) {\r\n        TimelineEntity taskAttempt = createTaskAttemptEntity(taskAttemptInfo);\r\n        taskAttempts.add(taskAttempt);\r\n    }\r\n    return taskAttempts;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "createTaskAttemptEntity",
  "errType" : null,
  "containingMethodsNum" : 13,
  "sourceCodeText" : "TimelineEntity createTaskAttemptEntity(TaskAttemptInfo taskAttemptInfo)\n{\r\n    TimelineEntity taskAttempt = new TimelineEntity();\r\n    taskAttempt.setEntityType(TASK_ATTEMPT);\r\n    taskAttempt.setEntityId(taskAttemptInfo.getAttemptId().toString());\r\n    taskAttempt.setStartTime(taskAttemptInfo.getStartTime());\r\n    taskAttempt.addOtherInfo(\"START_TIME\", taskAttemptInfo.getStartTime());\r\n    taskAttempt.addOtherInfo(\"FINISH_TIME\", taskAttemptInfo.getFinishTime());\r\n    taskAttempt.addOtherInfo(\"MAP_FINISH_TIME\", taskAttemptInfo.getMapFinishTime());\r\n    taskAttempt.addOtherInfo(\"SHUFFLE_FINISH_TIME\", taskAttemptInfo.getShuffleFinishTime());\r\n    taskAttempt.addOtherInfo(\"SORT_FINISH_TIME\", taskAttemptInfo.getSortFinishTime());\r\n    taskAttempt.addOtherInfo(\"TASK_STATUS\", taskAttemptInfo.getTaskStatus());\r\n    taskAttempt.addOtherInfo(\"STATE\", taskAttemptInfo.getState());\r\n    taskAttempt.addOtherInfo(\"ERROR\", taskAttemptInfo.getError());\r\n    taskAttempt.addOtherInfo(\"CONTAINER_ID\", taskAttemptInfo.getContainerId().toString());\r\n    LOG.info(\"converted task attempt \" + taskAttemptInfo.getAttemptId() + \" to a timeline entity\");\r\n    return taskAttempt;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "testCombiner",
  "errType" : null,
  "containingMethodsNum" : 42,
  "sourceCodeText" : "void testCombiner() throws Exception\n{\r\n    if (!new File(TEST_ROOT_DIR).mkdirs()) {\r\n        throw new RuntimeException(\"Could not create test dir: \" + TEST_ROOT_DIR);\r\n    }\r\n    File in = new File(TEST_ROOT_DIR, \"input\");\r\n    if (!in.mkdirs()) {\r\n        throw new RuntimeException(\"Could not create test dir: \" + in);\r\n    }\r\n    File out = new File(TEST_ROOT_DIR, \"output\");\r\n    PrintWriter pw = new PrintWriter(new FileWriter(new File(in, \"data.txt\")));\r\n    pw.println(\"A|a,1\");\r\n    pw.println(\"A|b,2\");\r\n    pw.println(\"B|a,3\");\r\n    pw.println(\"B|b,4\");\r\n    pw.println(\"B|c,5\");\r\n    pw.close();\r\n    JobConf conf = new JobConf();\r\n    conf.set(\"mapreduce.framework.name\", \"local\");\r\n    Job job = new Job(conf);\r\n    TextInputFormat.setInputPaths(job, new Path(in.getPath()));\r\n    TextOutputFormat.setOutputPath(job, new Path(out.getPath()));\r\n    job.setMapperClass(Map.class);\r\n    job.setReducerClass(Reduce.class);\r\n    job.setInputFormatClass(TextInputFormat.class);\r\n    job.setMapOutputKeyClass(Text.class);\r\n    job.setMapOutputValueClass(LongWritable.class);\r\n    job.setOutputFormatClass(TextOutputFormat.class);\r\n    job.setGroupingComparatorClass(GroupComparator.class);\r\n    job.setCombinerKeyGroupingComparatorClass(GroupComparator.class);\r\n    job.setCombinerClass(Combiner.class);\r\n    job.getConfiguration().setInt(\"min.num.spills.for.combine\", 0);\r\n    job.submit();\r\n    job.waitForCompletion(false);\r\n    if (job.isSuccessful()) {\r\n        Counters counters = job.getCounters();\r\n        long combinerInputRecords = counters.findCounter(\"org.apache.hadoop.mapreduce.TaskCounter\", \"COMBINE_INPUT_RECORDS\").getValue();\r\n        long combinerOutputRecords = counters.findCounter(\"org.apache.hadoop.mapreduce.TaskCounter\", \"COMBINE_OUTPUT_RECORDS\").getValue();\r\n        Assert.assertTrue(combinerInputRecords > 0);\r\n        Assert.assertTrue(combinerInputRecords > combinerOutputRecords);\r\n        BufferedReader br = new BufferedReader(new FileReader(new File(out, \"part-r-00000\")));\r\n        Set<String> output = new HashSet<String>();\r\n        String line = br.readLine();\r\n        Assert.assertNotNull(line);\r\n        output.add(line.substring(0, 1) + line.substring(4, 5));\r\n        line = br.readLine();\r\n        Assert.assertNotNull(line);\r\n        output.add(line.substring(0, 1) + line.substring(4, 5));\r\n        line = br.readLine();\r\n        Assert.assertNull(line);\r\n        br.close();\r\n        Set<String> expected = new HashSet<String>();\r\n        expected.add(\"A2\");\r\n        expected.add(\"B5\");\r\n        Assert.assertEquals(expected, output);\r\n    } else {\r\n        Assert.fail(\"Job failed\");\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "create",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "MiniMRClientCluster create(Class<?> caller, int noOfNMs, Configuration conf) throws IOException\n{\r\n    return create(caller, caller.getSimpleName(), noOfNMs, conf);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "create",
  "errType" : null,
  "containingMethodsNum" : 16,
  "sourceCodeText" : "MiniMRClientCluster create(Class<?> caller, String identifier, int noOfNMs, Configuration conf) throws IOException\n{\r\n    if (conf == null) {\r\n        conf = new Configuration();\r\n    }\r\n    FileSystem fs = FileSystem.get(conf);\r\n    Path testRootDir = fs.makeQualified(new Path(\"target\", identifier + \"-tmpDir\"));\r\n    Path appJar = new Path(testRootDir, \"MRAppJar.jar\");\r\n    Path appMasterJar = new Path(MiniMRYarnCluster.copyAppJarIntoTestDir(identifier));\r\n    fs.copyFromLocalFile(appMasterJar, appJar);\r\n    fs.setPermission(appJar, new FsPermission(\"744\"));\r\n    Job job = Job.getInstance(conf);\r\n    job.addFileToClassPath(appJar);\r\n    Path callerJar = new Path(JarFinder.getJar(caller, identifier));\r\n    Path remoteCallerJar = new Path(testRootDir, callerJar.getName());\r\n    fs.copyFromLocalFile(callerJar, remoteCallerJar);\r\n    fs.setPermission(remoteCallerJar, new FsPermission(\"744\"));\r\n    job.addFileToClassPath(remoteCallerJar);\r\n    MiniMRYarnCluster miniMRYarnCluster = new MiniMRYarnCluster(identifier, noOfNMs);\r\n    job.getConfiguration().set(\"minimrclientcluster.caller.name\", identifier);\r\n    job.getConfiguration().setInt(\"minimrclientcluster.nodemanagers.number\", noOfNMs);\r\n    miniMRYarnCluster.init(job.getConfiguration());\r\n    miniMRYarnCluster.start();\r\n    return new MiniMRYarnClusterAdapter(miniMRYarnCluster);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\fs\\slive",
  "methodName" : "getOptionList",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Options getOptionList()\n{\r\n    return optList;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\fs\\slive",
  "methodName" : "parse",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "ParsedOutput parse() throws Exception\n{\r\n    if (parsed == null) {\r\n        PosixParser parser = new PosixParser();\r\n        CommandLine popts = parser.parse(getOptionList(), argumentList, true);\r\n        if (popts.hasOption(ConfigOption.HELP.getOpt())) {\r\n            parsed = new ParsedOutput(null, this, true);\r\n        } else {\r\n            parsed = new ParsedOutput(popts, this, false);\r\n        }\r\n    }\r\n    return parsed;\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\fs\\slive",
  "methodName" : "getOptions",
  "errType" : null,
  "containingMethodsNum" : 29,
  "sourceCodeText" : "Options getOptions()\n{\r\n    Options cliopt = new Options();\r\n    cliopt.addOption(ConfigOption.MAPS);\r\n    cliopt.addOption(ConfigOption.REDUCES);\r\n    cliopt.addOption(ConfigOption.PACKET_SIZE);\r\n    cliopt.addOption(ConfigOption.OPS);\r\n    cliopt.addOption(ConfigOption.DURATION);\r\n    cliopt.addOption(ConfigOption.EXIT_ON_ERROR);\r\n    cliopt.addOption(ConfigOption.SLEEP_TIME);\r\n    cliopt.addOption(ConfigOption.TRUNCATE_WAIT);\r\n    cliopt.addOption(ConfigOption.FILES);\r\n    cliopt.addOption(ConfigOption.DIR_SIZE);\r\n    cliopt.addOption(ConfigOption.BASE_DIR);\r\n    cliopt.addOption(ConfigOption.RESULT_FILE);\r\n    cliopt.addOption(ConfigOption.CLEANUP);\r\n    {\r\n        String[] distStrs = new String[Distribution.values().length];\r\n        Distribution[] distValues = Distribution.values();\r\n        for (int i = 0; i < distValues.length; ++i) {\r\n            distStrs[i] = distValues[i].lowerName();\r\n        }\r\n        String opdesc = String.format(Constants.OP_DESCR, StringUtils.arrayToString(distStrs));\r\n        for (OperationType type : OperationType.values()) {\r\n            String opname = type.lowerName();\r\n            cliopt.addOption(new Option(opname, true, opdesc));\r\n        }\r\n    }\r\n    cliopt.addOption(ConfigOption.REPLICATION_AM);\r\n    cliopt.addOption(ConfigOption.BLOCK_SIZE);\r\n    cliopt.addOption(ConfigOption.READ_SIZE);\r\n    cliopt.addOption(ConfigOption.WRITE_SIZE);\r\n    cliopt.addOption(ConfigOption.APPEND_SIZE);\r\n    cliopt.addOption(ConfigOption.TRUNCATE_SIZE);\r\n    cliopt.addOption(ConfigOption.RANDOM_SEED);\r\n    cliopt.addOption(ConfigOption.QUEUE_NAME);\r\n    cliopt.addOption(ConfigOption.HELP);\r\n    return cliopt;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "writeEntities",
  "errType" : [ "Exception" ],
  "containingMethodsNum" : 37,
  "sourceCodeText" : "void writeEntities(Configuration tlConf, TimelineCollectorManager manager, Context context) throws IOException\n{\r\n    JobHistoryFileReplayHelper helper = new JobHistoryFileReplayHelper(context);\r\n    int replayMode = helper.getReplayMode();\r\n    JobHistoryFileParser parser = helper.getParser();\r\n    TimelineEntityConverterV2 converter = new TimelineEntityConverterV2();\r\n    Collection<JobFiles> jobs = helper.getJobFiles();\r\n    if (jobs.isEmpty()) {\r\n        LOG.info(context.getTaskAttemptID().getTaskID() + \" will process no jobs\");\r\n    } else {\r\n        LOG.info(context.getTaskAttemptID().getTaskID() + \" will process \" + jobs.size() + \" jobs\");\r\n    }\r\n    for (JobFiles job : jobs) {\r\n        String jobIdStr = job.getJobId();\r\n        if (job.getJobConfFilePath() == null || job.getJobHistoryFilePath() == null) {\r\n            LOG.info(jobIdStr + \" missing either the job history file or the \" + \"configuration file. Skipping.\");\r\n            continue;\r\n        }\r\n        LOG.info(\"processing \" + jobIdStr + \"...\");\r\n        JobId jobId = TypeConverter.toYarn(JobID.forName(jobIdStr));\r\n        ApplicationId appId = jobId.getAppId();\r\n        AppLevelTimelineCollector collector = new AppLevelTimelineCollector(appId);\r\n        manager.putIfAbsent(appId, collector);\r\n        try {\r\n            JobInfo jobInfo = parser.parseHistoryFile(job.getJobHistoryFilePath());\r\n            Configuration jobConf = parser.parseConfiguration(job.getJobConfFilePath());\r\n            LOG.info(\"parsed the job history file and the configuration file \" + \"for job \" + jobIdStr);\r\n            TimelineCollectorContext tlContext = collector.getTimelineEntityContext();\r\n            tlContext.setFlowName(jobInfo.getJobname());\r\n            tlContext.setFlowRunId(jobInfo.getSubmitTime());\r\n            tlContext.setUserId(jobInfo.getUsername());\r\n            long totalTime = 0;\r\n            List<TimelineEntity> entitySet = converter.createTimelineEntities(jobInfo, jobConf);\r\n            LOG.info(\"converted them into timeline entities for job \" + jobIdStr);\r\n            UserGroupInformation ugi = UserGroupInformation.getCurrentUser();\r\n            long startWrite = System.nanoTime();\r\n            try {\r\n                switch(replayMode) {\r\n                    case JobHistoryFileReplayHelper.WRITE_ALL_AT_ONCE:\r\n                        writeAllEntities(collector, entitySet, ugi);\r\n                        break;\r\n                    case JobHistoryFileReplayHelper.WRITE_PER_ENTITY:\r\n                        writePerEntity(collector, entitySet, ugi);\r\n                        break;\r\n                    default:\r\n                        break;\r\n                }\r\n            } catch (Exception e) {\r\n                context.getCounter(PerfCounters.TIMELINE_SERVICE_WRITE_FAILURES).increment(1);\r\n                LOG.error(\"writing to the timeline service failed\", e);\r\n            }\r\n            long endWrite = System.nanoTime();\r\n            totalTime += TimeUnit.NANOSECONDS.toMillis(endWrite - startWrite);\r\n            int numEntities = entitySet.size();\r\n            LOG.info(\"wrote \" + numEntities + \" entities in \" + totalTime + \" ms\");\r\n            context.getCounter(PerfCounters.TIMELINE_SERVICE_WRITE_TIME).increment(totalTime);\r\n            context.getCounter(PerfCounters.TIMELINE_SERVICE_WRITE_COUNTER).increment(numEntities);\r\n        } finally {\r\n            manager.remove(appId);\r\n            context.progress();\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 2,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "writeAllEntities",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void writeAllEntities(AppLevelTimelineCollector collector, List<TimelineEntity> entitySet, UserGroupInformation ugi) throws IOException\n{\r\n    TimelineEntities entities = new TimelineEntities();\r\n    entities.setEntities(entitySet);\r\n    collector.putEntities(entities, ugi);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-jobclient\\src\\test\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "writePerEntity",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void writePerEntity(AppLevelTimelineCollector collector, List<TimelineEntity> entitySet, UserGroupInformation ugi) throws IOException\n{\r\n    for (TimelineEntity entity : entitySet) {\r\n        TimelineEntities entities = new TimelineEntities();\r\n        entities.addEntity(entity);\r\n        collector.putEntities(entities, ugi);\r\n        LOG.info(\"wrote entity \" + entity.getId());\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
} ]
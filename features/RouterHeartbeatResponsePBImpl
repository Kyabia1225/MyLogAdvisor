[ {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\protocol\\impl\\pb",
  "methodName" : "getProto",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "RouterHeartbeatResponseProto getProto()\n{\r\n    return this.translator.build();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\protocol\\impl\\pb",
  "methodName" : "setProto",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setProto(Message proto)\n{\r\n    this.translator.setProto(proto);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\protocol\\impl\\pb",
  "methodName" : "readInstance",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void readInstance(String base64String) throws IOException\n{\r\n    this.translator.readInstance(base64String);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\protocol\\impl\\pb",
  "methodName" : "getStatus",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "boolean getStatus()\n{\r\n    return this.translator.getProtoOrBuilder().getStatus();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\protocol\\impl\\pb",
  "methodName" : "setStatus",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setStatus(boolean result)\n{\r\n    this.translator.getBuilder().setStatus(result);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "allowSnapshot",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void allowSnapshot(String snapshotRoot) throws IOException\n{\r\n    rpcServer.checkOperation(OperationCategory.WRITE);\r\n    final List<RemoteLocation> locations = rpcServer.getLocationsForPath(snapshotRoot, true, false);\r\n    RemoteMethod method = new RemoteMethod(\"allowSnapshot\", new Class<?>[] { String.class }, new RemoteParam());\r\n    if (rpcServer.isInvokeConcurrent(snapshotRoot)) {\r\n        rpcClient.invokeConcurrent(locations, method);\r\n    } else {\r\n        rpcClient.invokeSequential(locations, method);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "disallowSnapshot",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void disallowSnapshot(String snapshotRoot) throws IOException\n{\r\n    rpcServer.checkOperation(OperationCategory.WRITE);\r\n    final List<RemoteLocation> locations = rpcServer.getLocationsForPath(snapshotRoot, true, false);\r\n    RemoteMethod method = new RemoteMethod(\"disallowSnapshot\", new Class<?>[] { String.class }, new RemoteParam());\r\n    if (rpcServer.isInvokeConcurrent(snapshotRoot)) {\r\n        rpcClient.invokeConcurrent(locations, method);\r\n    } else {\r\n        rpcClient.invokeSequential(locations, method);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "createSnapshot",
  "errType" : null,
  "containingMethodsNum" : 12,
  "sourceCodeText" : "String createSnapshot(String snapshotRoot, String snapshotName) throws IOException\n{\r\n    rpcServer.checkOperation(OperationCategory.WRITE);\r\n    final List<RemoteLocation> locations = rpcServer.getLocationsForPath(snapshotRoot, true, false);\r\n    RemoteMethod method = new RemoteMethod(\"createSnapshot\", new Class<?>[] { String.class, String.class }, new RemoteParam(), snapshotName);\r\n    String result = null;\r\n    if (rpcServer.isInvokeConcurrent(snapshotRoot)) {\r\n        Map<RemoteLocation, String> results = rpcClient.invokeConcurrent(locations, method, String.class);\r\n        Entry<RemoteLocation, String> firstelement = results.entrySet().iterator().next();\r\n        RemoteLocation loc = firstelement.getKey();\r\n        result = firstelement.getValue();\r\n        result = result.replaceFirst(loc.getDest(), loc.getSrc());\r\n    } else {\r\n        RemoteResult<RemoteLocation, String> response = rpcClient.invokeSequential(method, locations, String.class, null);\r\n        RemoteLocation loc = response.getLocation();\r\n        String invokedResult = response.getResult();\r\n        result = invokedResult.replaceFirst(loc.getDest(), loc.getSrc());\r\n    }\r\n    return result;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "deleteSnapshot",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void deleteSnapshot(String snapshotRoot, String snapshotName) throws IOException\n{\r\n    rpcServer.checkOperation(OperationCategory.WRITE);\r\n    final List<RemoteLocation> locations = rpcServer.getLocationsForPath(snapshotRoot, true, false);\r\n    RemoteMethod method = new RemoteMethod(\"deleteSnapshot\", new Class<?>[] { String.class, String.class }, new RemoteParam(), snapshotName);\r\n    if (rpcServer.isInvokeConcurrent(snapshotRoot)) {\r\n        rpcClient.invokeConcurrent(locations, method);\r\n    } else {\r\n        rpcClient.invokeSequential(locations, method);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "renameSnapshot",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void renameSnapshot(String snapshotRoot, String oldSnapshotName, String newSnapshot) throws IOException\n{\r\n    rpcServer.checkOperation(OperationCategory.WRITE);\r\n    final List<RemoteLocation> locations = rpcServer.getLocationsForPath(snapshotRoot, true, false);\r\n    RemoteMethod method = new RemoteMethod(\"renameSnapshot\", new Class<?>[] { String.class, String.class, String.class }, new RemoteParam(), oldSnapshotName, newSnapshot);\r\n    if (rpcServer.isInvokeConcurrent(snapshotRoot)) {\r\n        rpcClient.invokeConcurrent(locations, method);\r\n    } else {\r\n        rpcClient.invokeSequential(locations, method);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "getSnapshottableDirListing",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "SnapshottableDirectoryStatus[] getSnapshottableDirListing() throws IOException\n{\r\n    rpcServer.checkOperation(NameNode.OperationCategory.READ);\r\n    RemoteMethod method = new RemoteMethod(\"getSnapshottableDirListing\");\r\n    Set<FederationNamespaceInfo> nss = namenodeResolver.getNamespaces();\r\n    Map<FederationNamespaceInfo, SnapshottableDirectoryStatus[]> ret = rpcClient.invokeConcurrent(nss, method, true, false, SnapshottableDirectoryStatus[].class);\r\n    return RouterRpcServer.merge(ret, SnapshottableDirectoryStatus.class);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "getSnapshotListing",
  "errType" : null,
  "containingMethodsNum" : 14,
  "sourceCodeText" : "SnapshotStatus[] getSnapshotListing(String snapshotRoot) throws IOException\n{\r\n    rpcServer.checkOperation(NameNode.OperationCategory.READ);\r\n    final List<RemoteLocation> locations = rpcServer.getLocationsForPath(snapshotRoot, true, false);\r\n    RemoteMethod remoteMethod = new RemoteMethod(\"getSnapshotListing\", new Class<?>[] { String.class }, new RemoteParam());\r\n    SnapshotStatus[] response;\r\n    if (rpcServer.isInvokeConcurrent(snapshotRoot)) {\r\n        Map<RemoteLocation, SnapshotStatus[]> ret = rpcClient.invokeConcurrent(locations, remoteMethod, true, false, SnapshotStatus[].class);\r\n        response = ret.values().iterator().next();\r\n        String src = ret.keySet().iterator().next().getSrc();\r\n        String dst = ret.keySet().iterator().next().getDest();\r\n        for (SnapshotStatus s : response) {\r\n            String mountPath = DFSUtil.bytes2String(s.getParentFullPath()).replaceFirst(src, dst);\r\n            s.setParentFullPath(DFSUtil.string2Bytes(mountPath));\r\n        }\r\n    } else {\r\n        RemoteResult<RemoteLocation, SnapshotStatus[]> invokedResponse = rpcClient.invokeSequential(remoteMethod, locations, SnapshotStatus[].class, null);\r\n        RemoteLocation loc = invokedResponse.getLocation();\r\n        response = invokedResponse.getResult();\r\n        for (SnapshotStatus s : response) {\r\n            String mountPath = DFSUtil.bytes2String(s.getParentFullPath()).replaceFirst(loc.getDest(), loc.getSrc());\r\n            s.setParentFullPath(DFSUtil.string2Bytes(mountPath));\r\n        }\r\n    }\r\n    return response;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "getSnapshotDiffReport",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "SnapshotDiffReport getSnapshotDiffReport(String snapshotRoot, String earlierSnapshotName, String laterSnapshotName) throws IOException\n{\r\n    rpcServer.checkOperation(NameNode.OperationCategory.READ);\r\n    final List<RemoteLocation> locations = rpcServer.getLocationsForPath(snapshotRoot, true, false);\r\n    RemoteMethod remoteMethod = new RemoteMethod(\"getSnapshotDiffReport\", new Class<?>[] { String.class, String.class, String.class }, new RemoteParam(), earlierSnapshotName, laterSnapshotName);\r\n    if (rpcServer.isInvokeConcurrent(snapshotRoot)) {\r\n        Map<RemoteLocation, SnapshotDiffReport> ret = rpcClient.invokeConcurrent(locations, remoteMethod, true, false, SnapshotDiffReport.class);\r\n        return ret.values().iterator().next();\r\n    } else {\r\n        return rpcClient.invokeSequential(locations, remoteMethod, SnapshotDiffReport.class, null);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "getSnapshotDiffReportListing",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "SnapshotDiffReportListing getSnapshotDiffReportListing(String snapshotRoot, String earlierSnapshotName, String laterSnapshotName, byte[] startPath, int index) throws IOException\n{\r\n    rpcServer.checkOperation(NameNode.OperationCategory.READ);\r\n    final List<RemoteLocation> locations = rpcServer.getLocationsForPath(snapshotRoot, true, false);\r\n    Class<?>[] params = new Class<?>[] { String.class, String.class, String.class, byte[].class, int.class };\r\n    RemoteMethod remoteMethod = new RemoteMethod(\"getSnapshotDiffReportListing\", params, new RemoteParam(), earlierSnapshotName, laterSnapshotName, startPath, index);\r\n    if (rpcServer.isInvokeConcurrent(snapshotRoot)) {\r\n        Map<RemoteLocation, SnapshotDiffReportListing> ret = rpcClient.invokeConcurrent(locations, remoteMethod, false, false, SnapshotDiffReportListing.class);\r\n        Collection<SnapshotDiffReportListing> listings = ret.values();\r\n        SnapshotDiffReportListing listing0 = listings.iterator().next();\r\n        return listing0;\r\n    } else {\r\n        return rpcClient.invokeSequential(locations, remoteMethod, SnapshotDiffReportListing.class, null);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\protocol\\impl\\pb",
  "methodName" : "getProto",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "LeaveSafeModeResponseProto getProto()\n{\r\n    return translator.build();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\protocol\\impl\\pb",
  "methodName" : "setProto",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setProto(Message proto)\n{\r\n    this.translator.setProto(proto);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\protocol\\impl\\pb",
  "methodName" : "readInstance",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void readInstance(String base64String) throws IOException\n{\r\n    this.translator.readInstance(base64String);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\protocol\\impl\\pb",
  "methodName" : "getStatus",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "boolean getStatus()\n{\r\n    return this.translator.getProtoOrBuilder().getStatus();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\protocol\\impl\\pb",
  "methodName" : "setStatus",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setStatus(boolean result)\n{\r\n    this.translator.getBuilder().setStatus(result);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\protocol\\impl\\pb",
  "methodName" : "getProto",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "GetDestinationResponseProto getProto()\n{\r\n    this.translator.getBuilder();\r\n    return this.translator.build();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\protocol\\impl\\pb",
  "methodName" : "setProto",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setProto(Message proto)\n{\r\n    this.translator.setProto(proto);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\protocol\\impl\\pb",
  "methodName" : "readInstance",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void readInstance(String base64String) throws IOException\n{\r\n    this.translator.readInstance(base64String);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\protocol\\impl\\pb",
  "methodName" : "getDestinations",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Collection<String> getDestinations()\n{\r\n    return new ArrayList<>(this.translator.getProtoOrBuilder().getDestinationsList());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\protocol\\impl\\pb",
  "methodName" : "setDestinations",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void setDestinations(Collection<String> nsIds)\n{\r\n    this.translator.getBuilder().clearDestinations();\r\n    for (String nsId : nsIds) {\r\n        this.translator.getBuilder().addDestinations(nsId);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "serviceInit",
  "errType" : null,
  "containingMethodsNum" : 20,
  "sourceCodeText" : "void serviceInit(Configuration configuration) throws Exception\n{\r\n    this.conf = DFSHAAdmin.addSecurityConfiguration(configuration);\r\n    String nnDesc = nameserviceId;\r\n    if (this.namenodeId != null && !this.namenodeId.isEmpty()) {\r\n        nnDesc += \"-\" + namenodeId;\r\n    } else {\r\n        this.localTarget = null;\r\n    }\r\n    if (originalNnId == null) {\r\n        originalNnId = namenodeId;\r\n    }\r\n    this.rpcAddress = getRpcAddress(conf, nameserviceId, originalNnId);\r\n    this.serviceAddress = DFSUtil.getNamenodeServiceAddr(conf, nameserviceId, originalNnId);\r\n    if (this.serviceAddress == null) {\r\n        LOG.error(\"Cannot locate RPC service address for NN {}, \" + \"using RPC address {}\", nnDesc, this.rpcAddress);\r\n        this.serviceAddress = this.rpcAddress;\r\n    }\r\n    this.lifelineAddress = DFSUtil.getNamenodeLifelineAddr(conf, nameserviceId, originalNnId);\r\n    if (this.lifelineAddress == null) {\r\n        this.lifelineAddress = this.serviceAddress;\r\n    }\r\n    this.webAddress = DFSUtil.getNamenodeWebAddr(conf, nameserviceId, originalNnId);\r\n    if (resolvedHost != null) {\r\n        rpcAddress = resolvedHost + \":\" + NetUtils.getPortFromHostPortString(rpcAddress);\r\n        serviceAddress = resolvedHost + \":\" + NetUtils.getPortFromHostPortString(serviceAddress);\r\n        lifelineAddress = resolvedHost + \":\" + NetUtils.getPortFromHostPortString(lifelineAddress);\r\n        webAddress = resolvedHost + \":\" + NetUtils.getPortFromHostPortString(webAddress);\r\n    }\r\n    LOG.info(\"{} RPC address: {}\", nnDesc, rpcAddress);\r\n    LOG.info(\"{} Service RPC address: {}\", nnDesc, serviceAddress);\r\n    LOG.info(\"{} Lifeline RPC address: {}\", nnDesc, lifelineAddress);\r\n    LOG.info(\"{} Web address: {}\", nnDesc, webAddress);\r\n    if (this.namenodeId != null && !this.namenodeId.isEmpty()) {\r\n        this.localTarget = new NNHAServiceTarget(conf, nameserviceId, namenodeId, serviceAddress, lifelineAddress);\r\n    }\r\n    this.connectionFactory = URLConnectionFactory.newDefaultURLConnectionFactory(conf);\r\n    this.scheme = DFSUtil.getHttpPolicy(conf).isHttpEnabled() ? \"http\" : \"https\";\r\n    this.setIntervalMs(conf.getLong(DFS_ROUTER_HEARTBEAT_INTERVAL_MS, DFS_ROUTER_HEARTBEAT_INTERVAL_MS_DEFAULT));\r\n    super.serviceInit(configuration);\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "periodicInvoke",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void periodicInvoke()\n{\r\n    updateState();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "getRpcAddress",
  "errType" : null,
  "containingMethodsNum" : 10,
  "sourceCodeText" : "String getRpcAddress(Configuration conf, String nsId, String nnId)\n{\r\n    String confKey = DFSConfigKeys.DFS_NAMENODE_RPC_ADDRESS_KEY;\r\n    String ret = conf.get(confKey);\r\n    if (nsId != null || nnId != null) {\r\n        confKey = DFSUtil.addKeySuffixes(confKey, nsId, nnId);\r\n        ret = conf.get(confKey);\r\n        if (ret == null) {\r\n            Map<String, InetSocketAddress> rpcAddresses = DFSUtil.getRpcAddressesForNameserviceId(conf, nsId, null);\r\n            InetSocketAddress sockAddr = null;\r\n            if (nnId != null) {\r\n                sockAddr = rpcAddresses.get(nnId);\r\n            } else if (rpcAddresses.size() == 1) {\r\n                sockAddr = rpcAddresses.values().iterator().next();\r\n            }\r\n            if (sockAddr != null) {\r\n                InetAddress addr = sockAddr.getAddress();\r\n                ret = addr.getHostName() + \":\" + sockAddr.getPort();\r\n            }\r\n        }\r\n    }\r\n    return ret;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "updateState",
  "errType" : [ "IOException", "Exception" ],
  "containingMethodsNum" : 12,
  "sourceCodeText" : "void updateState()\n{\r\n    NamenodeStatusReport report = getNamenodeStatusReport();\r\n    if (!report.registrationValid()) {\r\n        LOG.error(\"Namenode is not operational: {}\", getNamenodeDesc());\r\n    } else if (report.haStateValid()) {\r\n        if (LOG.isDebugEnabled()) {\r\n            LOG.debug(\"Received service state: {} from HA namenode: {}\", report.getState(), getNamenodeDesc());\r\n        }\r\n    } else if (localTarget == null) {\r\n        if (LOG.isDebugEnabled()) {\r\n            LOG.debug(\"Reporting non-HA namenode as operational: {}\", getNamenodeDesc());\r\n        }\r\n    } else {\r\n        return;\r\n    }\r\n    try {\r\n        if (!resolver.registerNamenode(report)) {\r\n            LOG.warn(\"Cannot register namenode {}\", report);\r\n        }\r\n    } catch (IOException e) {\r\n        LOG.info(\"Cannot register namenode in the State Store\");\r\n    } catch (Exception ex) {\r\n        LOG.error(\"Unhandled exception updating NN registration for {}\", getNamenodeDesc(), ex);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "getNamenodeStatusReport",
  "errType" : [ "IOException", "Throwable", "Exception", "Throwable" ],
  "containingMethodsNum" : 19,
  "sourceCodeText" : "NamenodeStatusReport getNamenodeStatusReport()\n{\r\n    NamenodeStatusReport report = new NamenodeStatusReport(nameserviceId, namenodeId, rpcAddress, serviceAddress, lifelineAddress, scheme, webAddress);\r\n    try {\r\n        LOG.debug(\"Probing NN at service address: {}\", serviceAddress);\r\n        URI serviceURI = new URI(\"hdfs://\" + serviceAddress);\r\n        NamenodeProtocol nn = NameNodeProxies.createProxy(this.conf, serviceURI, NamenodeProtocol.class).getProxy();\r\n        if (nn != null) {\r\n            NamespaceInfo info = nn.versionRequest();\r\n            if (info != null) {\r\n                report.setNamespaceInfo(info);\r\n            }\r\n        }\r\n        if (!report.registrationValid()) {\r\n            return report;\r\n        }\r\n        try {\r\n            ClientProtocol client = NameNodeProxies.createProxy(this.conf, serviceURI, ClientProtocol.class).getProxy();\r\n            if (client != null) {\r\n                boolean isSafeMode = client.setSafeMode(SafeModeAction.SAFEMODE_GET, false);\r\n                report.setSafeMode(isSafeMode);\r\n            }\r\n        } catch (Exception e) {\r\n            LOG.error(\"Cannot fetch safemode state for {}\", getNamenodeDesc(), e);\r\n        }\r\n        updateJMXParameters(webAddress, report);\r\n        if (localTarget != null) {\r\n            try {\r\n                if (localTargetHAProtocol == null) {\r\n                    localTargetHAProtocol = localTarget.getHealthMonitorProxy(conf, 30 * 1000);\r\n                    LOG.debug(\"Get HA status with address {}\", lifelineAddress);\r\n                }\r\n                HAServiceStatus status = localTargetHAProtocol.getServiceStatus();\r\n                report.setHAServiceState(status.getState());\r\n            } catch (Throwable e) {\r\n                if (e.getMessage().startsWith(\"HA for namenode is not enabled\")) {\r\n                    LOG.error(\"HA for {} is not enabled\", getNamenodeDesc());\r\n                    localTarget = null;\r\n                } else {\r\n                    LOG.error(\"Cannot fetch HA status for {}: {}\", getNamenodeDesc(), e.getMessage(), e);\r\n                }\r\n                localTargetHAProtocol = null;\r\n            }\r\n        }\r\n    } catch (IOException e) {\r\n        LOG.error(\"Cannot communicate with {}: {}\", getNamenodeDesc(), e.getMessage());\r\n    } catch (Throwable e) {\r\n        LOG.error(\"Unexpected exception while communicating with {}: {}\", getNamenodeDesc(), e.getMessage(), e);\r\n    }\r\n    return report;\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 3,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "getLocalTarget",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "NNHAServiceTarget getLocalTarget()\n{\r\n    return this.localTarget;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "getNamenodeDesc",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String getNamenodeDesc()\n{\r\n    if (namenodeId != null && !namenodeId.isEmpty()) {\r\n        return nameserviceId + \"-\" + namenodeId + \":\" + serviceAddress;\r\n    } else {\r\n        return nameserviceId + \":\" + serviceAddress;\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "getNnHeartBeatServiceName",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String getNnHeartBeatServiceName(String nsId, String nnId)\n{\r\n    return NamenodeHeartbeatService.class.getSimpleName() + (nsId == null ? \"\" : \" \" + nsId) + (nnId == null ? \"\" : \" \" + nnId);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "updateJMXParameters",
  "errType" : [ "Exception" ],
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void updateJMXParameters(String address, NamenodeStatusReport report)\n{\r\n    try {\r\n        getFsNamesystemMetrics(address, report);\r\n        getNamenodeInfoMetrics(address, report);\r\n    } catch (Exception e) {\r\n        LOG.error(\"Cannot get stat from {} using JMX\", getNamenodeDesc(), e);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "getNamenodeInfoMetrics",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void getNamenodeInfoMetrics(String address, NamenodeStatusReport report) throws JSONException\n{\r\n    String query = \"Hadoop:service=NameNode,name=NameNodeInfo\";\r\n    JSONArray aux = FederationUtil.getJmx(query, address, connectionFactory, scheme);\r\n    if (aux != null && aux.length() > 0) {\r\n        JSONObject jsonObject = aux.getJSONObject(0);\r\n        String name = jsonObject.getString(\"name\");\r\n        if (name.equals(\"Hadoop:service=NameNode,name=NameNodeInfo\")) {\r\n            report.setNamenodeInfo(jsonObject.optInt(\"CorruptFilesCount\"), jsonObject.optLong(\"NumberOfMissingBlocksWithReplicationFactorOne\"), jsonObject.optLong(\"HighestPriorityLowRedundancyReplicatedBlocks\"), jsonObject.optLong(\"HighestPriorityLowRedundancyECBlocks\"));\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "getFsNamesystemMetrics",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void getFsNamesystemMetrics(String address, NamenodeStatusReport report) throws JSONException\n{\r\n    String query = \"Hadoop:service=NameNode,name=FSNamesystem*\";\r\n    JSONArray aux = FederationUtil.getJmx(query, address, connectionFactory, scheme);\r\n    if (aux != null) {\r\n        for (int i = 0; i < aux.length(); i++) {\r\n            JSONObject jsonObject = aux.getJSONObject(i);\r\n            String name = jsonObject.getString(\"name\");\r\n            if (name.equals(\"Hadoop:service=NameNode,name=FSNamesystemState\")) {\r\n                report.setDatanodeInfo(jsonObject.getInt(\"NumLiveDataNodes\"), jsonObject.getInt(\"NumDeadDataNodes\"), jsonObject.getInt(\"NumStaleDataNodes\"), jsonObject.getInt(\"NumDecommissioningDataNodes\"), jsonObject.getInt(\"NumDecomLiveDataNodes\"), jsonObject.getInt(\"NumDecomDeadDataNodes\"), jsonObject.optInt(\"NumInMaintenanceLiveDataNodes\"), jsonObject.optInt(\"NumInMaintenanceDeadDataNodes\"), jsonObject.optInt(\"NumEnteringMaintenanceDataNodes\"), jsonObject.optLong(\"ScheduledReplicationBlocks\"));\r\n            } else if (name.equals(\"Hadoop:service=NameNode,name=FSNamesystem\")) {\r\n                report.setNamesystemInfo(jsonObject.getLong(\"CapacityRemaining\"), jsonObject.getLong(\"CapacityTotal\"), jsonObject.getLong(\"FilesTotal\"), jsonObject.getLong(\"BlocksTotal\"), jsonObject.getLong(\"MissingBlocks\"), jsonObject.getLong(\"PendingReplicationBlocks\"), jsonObject.getLong(\"UnderReplicatedBlocks\"), jsonObject.getLong(\"PendingDeletionBlocks\"), jsonObject.optLong(\"ProvidedCapacityTotal\"), jsonObject.getInt(\"PendingSPSPaths\"));\r\n            }\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "serviceStop",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void serviceStop() throws Exception\n{\r\n    LOG.info(\"Stopping NamenodeHeartbeat service for, NS {} NN {} \", this.nameserviceId, this.namenodeId);\r\n    if (this.connectionFactory != null) {\r\n        this.connectionFactory.destroy();\r\n    }\r\n    super.serviceStop();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\driver\\impl",
  "methodName" : "exists",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "boolean exists(String path)\n{\r\n    File test = new File(path);\r\n    return test.exists();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\driver\\impl",
  "methodName" : "mkdir",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "boolean mkdir(String path)\n{\r\n    File dir = new File(path);\r\n    return dir.mkdirs();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\driver\\impl",
  "methodName" : "rename",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 2,
  "sourceCodeText" : "boolean rename(String src, String dst)\n{\r\n    try {\r\n        Files.move(new File(src), new File(dst));\r\n        return true;\r\n    } catch (IOException e) {\r\n        LOG.error(\"Cannot rename {} to {}\", src, dst, e);\r\n        return false;\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\driver\\impl",
  "methodName" : "remove",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "boolean remove(String path)\n{\r\n    File file = new File(path);\r\n    return file.delete();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\driver\\impl",
  "methodName" : "getRootDir",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 6,
  "sourceCodeText" : "String getRootDir()\n{\r\n    if (this.rootDirectory == null) {\r\n        String dir = getConf().get(FEDERATION_STORE_FILE_DIRECTORY);\r\n        if (dir == null) {\r\n            File tempDirBase = new File(System.getProperty(\"java.io.tmpdir\"));\r\n            File tempDir = null;\r\n            try {\r\n                tempDir = java.nio.file.Files.createTempDirectory(tempDirBase.toPath(), System.currentTimeMillis() + \"-\").toFile();\r\n            } catch (IOException e) {\r\n                LOG.debug(\"Unable to create a temporary directory. Fall back to \" + \" the default system temp directory {}\", tempDirBase, e);\r\n                tempDir = tempDirBase;\r\n            }\r\n            dir = tempDir.getAbsolutePath();\r\n            LOG.warn(\"The root directory is not available, using {}\", dir);\r\n        }\r\n        this.rootDirectory = dir;\r\n    }\r\n    return this.rootDirectory;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\driver\\impl",
  "methodName" : "getReader",
  "errType" : [ "Exception" ],
  "containingMethodsNum" : 2,
  "sourceCodeText" : "BufferedReader getReader(String filename)\n{\r\n    BufferedReader reader = null;\r\n    try {\r\n        LOG.debug(\"Loading file: {}\", filename);\r\n        File file = new File(filename);\r\n        FileInputStream fis = new FileInputStream(file);\r\n        InputStreamReader isr = new InputStreamReader(fis, StandardCharsets.UTF_8);\r\n        reader = new BufferedReader(isr);\r\n    } catch (Exception ex) {\r\n        LOG.error(\"Cannot open read stream for record {}\", filename, ex);\r\n    }\r\n    return reader;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\driver\\impl",
  "methodName" : "getWriter",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 2,
  "sourceCodeText" : "BufferedWriter getWriter(String filename)\n{\r\n    BufferedWriter writer = null;\r\n    try {\r\n        LOG.debug(\"Writing file: {}\", filename);\r\n        File file = new File(filename);\r\n        FileOutputStream fos = new FileOutputStream(file, false);\r\n        OutputStreamWriter osw = new OutputStreamWriter(fos, StandardCharsets.UTF_8);\r\n        writer = new BufferedWriter(osw);\r\n    } catch (IOException e) {\r\n        LOG.error(\"Cannot open write stream for record {}\", filename, e);\r\n    }\r\n    return writer;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\driver\\impl",
  "methodName" : "close",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void close() throws Exception\n{\r\n    setInitialized(false);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\driver\\impl",
  "methodName" : "getChildren",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "List<String> getChildren(String path)\n{\r\n    File dir = new File(path);\r\n    File[] files = dir.listFiles();\r\n    if (ArrayUtils.isNotEmpty(files)) {\r\n        List<String> ret = new ArrayList<>(files.length);\r\n        for (File file : files) {\r\n            String filename = file.getName();\r\n            ret.add(filename);\r\n        }\r\n        return ret;\r\n    }\r\n    return Collections.emptyList();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\protocol",
  "methodName" : "newInstance",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "GetSafeModeRequest newInstance() throws IOException\n{\r\n    return StateStoreSerializer.newRecord(GetSafeModeRequest.class);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "addCacheDirective",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "long addCacheDirective(CacheDirectiveInfo path, EnumSet<CacheFlag> flags) throws IOException\n{\r\n    rpcServer.checkOperation(NameNode.OperationCategory.WRITE, true);\r\n    final List<RemoteLocation> locations = rpcServer.getLocationsForPath(path.getPath().toString(), true, false);\r\n    RemoteMethod method = new RemoteMethod(\"addCacheDirective\", new Class<?>[] { CacheDirectiveInfo.class, EnumSet.class }, new RemoteParam(getRemoteMap(path, locations)), flags);\r\n    Map<RemoteLocation, Long> response = rpcClient.invokeConcurrent(locations, method, false, false, long.class);\r\n    return response.values().iterator().next();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "modifyCacheDirective",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void modifyCacheDirective(CacheDirectiveInfo directive, EnumSet<CacheFlag> flags) throws IOException\n{\r\n    rpcServer.checkOperation(NameNode.OperationCategory.WRITE, true);\r\n    Path p = directive.getPath();\r\n    if (p != null) {\r\n        final List<RemoteLocation> locations = rpcServer.getLocationsForPath(directive.getPath().toString(), true, false);\r\n        RemoteMethod method = new RemoteMethod(\"modifyCacheDirective\", new Class<?>[] { CacheDirectiveInfo.class, EnumSet.class }, new RemoteParam(getRemoteMap(directive, locations)), flags);\r\n        rpcClient.invokeConcurrent(locations, method);\r\n        return;\r\n    }\r\n    RemoteMethod method = new RemoteMethod(\"modifyCacheDirective\", new Class<?>[] { CacheDirectiveInfo.class, EnumSet.class }, directive, flags);\r\n    Set<FederationNamespaceInfo> nss = namenodeResolver.getNamespaces();\r\n    rpcClient.invokeConcurrent(nss, method, false, false);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "removeCacheDirective",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void removeCacheDirective(long id) throws IOException\n{\r\n    rpcServer.checkOperation(NameNode.OperationCategory.WRITE, true);\r\n    RemoteMethod method = new RemoteMethod(\"removeCacheDirective\", new Class<?>[] { long.class }, id);\r\n    Set<FederationNamespaceInfo> nss = namenodeResolver.getNamespaces();\r\n    rpcClient.invokeConcurrent(nss, method, false, false);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "listCacheDirectives",
  "errType" : null,
  "containingMethodsNum" : 9,
  "sourceCodeText" : "BatchedEntries<CacheDirectiveEntry> listCacheDirectives(long prevId, CacheDirectiveInfo filter) throws IOException\n{\r\n    rpcServer.checkOperation(NameNode.OperationCategory.READ, true);\r\n    if (filter.getPath() != null) {\r\n        final List<RemoteLocation> locations = rpcServer.getLocationsForPath(filter.getPath().toString(), true, false);\r\n        RemoteMethod method = new RemoteMethod(\"listCacheDirectives\", new Class<?>[] { long.class, CacheDirectiveInfo.class }, prevId, new RemoteParam(getRemoteMap(filter, locations)));\r\n        Map<RemoteLocation, BatchedEntries> response = rpcClient.invokeConcurrent(locations, method, false, false, BatchedEntries.class);\r\n        return response.values().iterator().next();\r\n    }\r\n    RemoteMethod method = new RemoteMethod(\"listCacheDirectives\", new Class<?>[] { long.class, CacheDirectiveInfo.class }, prevId, filter);\r\n    Set<FederationNamespaceInfo> nss = namenodeResolver.getNamespaces();\r\n    Map<FederationNamespaceInfo, BatchedEntries> results = rpcClient.invokeConcurrent(nss, method, true, false, BatchedEntries.class);\r\n    return results.values().iterator().next();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "addCachePool",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void addCachePool(CachePoolInfo info) throws IOException\n{\r\n    rpcServer.checkOperation(NameNode.OperationCategory.WRITE, true);\r\n    RemoteMethod method = new RemoteMethod(\"addCachePool\", new Class<?>[] { CachePoolInfo.class }, info);\r\n    Set<FederationNamespaceInfo> nss = namenodeResolver.getNamespaces();\r\n    rpcClient.invokeConcurrent(nss, method, true, false);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "modifyCachePool",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void modifyCachePool(CachePoolInfo info) throws IOException\n{\r\n    rpcServer.checkOperation(NameNode.OperationCategory.WRITE, true);\r\n    RemoteMethod method = new RemoteMethod(\"modifyCachePool\", new Class<?>[] { CachePoolInfo.class }, info);\r\n    Set<FederationNamespaceInfo> nss = namenodeResolver.getNamespaces();\r\n    rpcClient.invokeConcurrent(nss, method, true, false);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "removeCachePool",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void removeCachePool(String cachePoolName) throws IOException\n{\r\n    rpcServer.checkOperation(NameNode.OperationCategory.WRITE, true);\r\n    RemoteMethod method = new RemoteMethod(\"removeCachePool\", new Class<?>[] { String.class }, cachePoolName);\r\n    Set<FederationNamespaceInfo> nss = namenodeResolver.getNamespaces();\r\n    rpcClient.invokeConcurrent(nss, method, true, false);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "listCachePools",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "BatchedEntries<CachePoolEntry> listCachePools(String prevKey) throws IOException\n{\r\n    rpcServer.checkOperation(NameNode.OperationCategory.READ, true);\r\n    RemoteMethod method = new RemoteMethod(\"listCachePools\", new Class<?>[] { String.class }, prevKey);\r\n    Set<FederationNamespaceInfo> nss = namenodeResolver.getNamespaces();\r\n    Map<FederationNamespaceInfo, BatchedEntries> results = rpcClient.invokeConcurrent(nss, method, true, false, BatchedEntries.class);\r\n    return results.values().iterator().next();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "getRemoteMap",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "Map<RemoteLocation, CacheDirectiveInfo> getRemoteMap(CacheDirectiveInfo path, final List<RemoteLocation> locations)\n{\r\n    final Map<RemoteLocation, CacheDirectiveInfo> dstMap = new HashMap<>();\r\n    Iterator<RemoteLocation> iterator = locations.iterator();\r\n    while (iterator.hasNext()) {\r\n        dstMap.put(iterator.next(), path);\r\n    }\r\n    return dstMap;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\impl",
  "methodName" : "disableNameservice",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "boolean disableNameservice(String nsId) throws IOException\n{\r\n    DisabledNameservice record = DisabledNameservice.newInstance(nsId);\r\n    return getDriver().put(record, false, false);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\impl",
  "methodName" : "enableNameservice",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "boolean enableNameservice(String nsId) throws IOException\n{\r\n    DisabledNameservice record = DisabledNameservice.newInstance(nsId);\r\n    return getDriver().remove(record);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\impl",
  "methodName" : "getDisabledNameservices",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "Set<String> getDisabledNameservices() throws IOException\n{\r\n    Set<String> disabledNameservices = new TreeSet<>();\r\n    for (DisabledNameservice record : getCachedRecords()) {\r\n        String nsId = record.getNameserviceId();\r\n        disabledNameservices.add(nsId);\r\n    }\r\n    return disabledNameservices;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\protocol\\impl\\pb",
  "methodName" : "getProto",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "RefreshMountTableEntriesResponseProto getProto()\n{\r\n    return this.translator.build();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\protocol\\impl\\pb",
  "methodName" : "setProto",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setProto(Message proto)\n{\r\n    this.translator.setProto(proto);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\protocol\\impl\\pb",
  "methodName" : "readInstance",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void readInstance(String base64String) throws IOException\n{\r\n    this.translator.readInstance(base64String);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\protocol\\impl\\pb",
  "methodName" : "getResult",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "boolean getResult()\n{\r\n    return this.translator.getProtoOrBuilder().getResult();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\protocol\\impl\\pb",
  "methodName" : "setResult",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setResult(boolean result)\n{\r\n    this.translator.getBuilder().setResult(result);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\metrics",
  "methodName" : "create",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "NameserviceRPCMetrics create(Configuration conf, String nameService)\n{\r\n    MetricsSystem ms = DefaultMetricsSystem.instance();\r\n    String name = NAMESERVICE_RPC_METRICS_PREFIX + (nameService.isEmpty() ? \"UndefinedNameService\" + ThreadLocalRandom.current().nextInt() : nameService);\r\n    return ms.register(name, \"HDFS Federation NameService RPC Metrics\", new NameserviceRPCMetrics(conf, name));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\metrics",
  "methodName" : "incrProxyOpFailureStandby",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void incrProxyOpFailureStandby()\n{\r\n    proxyOpFailureStandby.incr();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\metrics",
  "methodName" : "getProxyOpFailureStandby",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "long getProxyOpFailureStandby()\n{\r\n    return proxyOpFailureStandby.value();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\metrics",
  "methodName" : "incrProxyOpFailureCommunicate",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void incrProxyOpFailureCommunicate()\n{\r\n    proxyOpFailureCommunicate.incr();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\metrics",
  "methodName" : "getProxyOpFailureCommunicate",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "long getProxyOpFailureCommunicate()\n{\r\n    return proxyOpFailureCommunicate.value();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\metrics",
  "methodName" : "incrProxyOpNoNamenodes",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void incrProxyOpNoNamenodes()\n{\r\n    proxyOpNoNamenodes.incr();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\metrics",
  "methodName" : "getProxyOpNoNamenodes",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "long getProxyOpNoNamenodes()\n{\r\n    return proxyOpNoNamenodes.value();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\metrics",
  "methodName" : "addProxyTime",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void addProxyTime(long time)\n{\r\n    proxy.add(time);\r\n    proxyOp.incr();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\metrics",
  "methodName" : "getProxyAvg",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "double getProxyAvg()\n{\r\n    return proxy.lastStat().mean();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\metrics",
  "methodName" : "getProxyOps",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "long getProxyOps()\n{\r\n    return proxyOp.value();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\metrics",
  "methodName" : "getNsId",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getNsId()\n{\r\n    return this.nsId;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\utils",
  "methodName" : "addLocation",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void addLocation(String location)\n{\r\n    addLocation(location, 100);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\utils",
  "methodName" : "addLocation",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void addLocation(String location, int numVirtualNodes)\n{\r\n    writeLock.lock();\r\n    try {\r\n        entryToVirtualNodes.put(location, numVirtualNodes);\r\n        for (int i = 0; i < numVirtualNodes; i++) {\r\n            String key = String.format(VIRTUAL_NODE_FORMAT, location, i);\r\n            String hash = getHash(key);\r\n            ring.put(hash, key);\r\n        }\r\n    } finally {\r\n        writeLock.unlock();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\utils",
  "methodName" : "removeLocation",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void removeLocation(String location)\n{\r\n    writeLock.lock();\r\n    try {\r\n        Integer numVirtualNodes = entryToVirtualNodes.remove(location);\r\n        for (int i = 0; i < numVirtualNodes; i++) {\r\n            String key = String.format(VIRTUAL_NODE_FORMAT, location, i);\r\n            String hash = getHash(key);\r\n            ring.remove(hash);\r\n        }\r\n    } finally {\r\n        writeLock.unlock();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\utils",
  "methodName" : "getLocation",
  "errType" : null,
  "containingMethodsNum" : 12,
  "sourceCodeText" : "String getLocation(String item)\n{\r\n    readLock.lock();\r\n    try {\r\n        if (ring.isEmpty()) {\r\n            return null;\r\n        }\r\n        String hash = getHash(item);\r\n        if (!ring.containsKey(hash)) {\r\n            SortedMap<String, String> tailMap = ring.tailMap(hash);\r\n            hash = tailMap.isEmpty() ? ring.firstKey() : tailMap.firstKey();\r\n        }\r\n        String virtualNode = ring.get(hash);\r\n        int index = virtualNode.lastIndexOf(SEPARATOR);\r\n        if (index >= 0) {\r\n            return virtualNode.substring(0, index);\r\n        } else {\r\n            return virtualNode;\r\n        }\r\n    } finally {\r\n        readLock.unlock();\r\n    }\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\utils",
  "methodName" : "getHash",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String getHash(String key)\n{\r\n    return MD5Hash.digest(key).toString();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\utils",
  "methodName" : "getLocations",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Set<String> getLocations()\n{\r\n    return entryToVirtualNodes.keySet();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\protocol",
  "methodName" : "newInstance",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "DisableNameserviceResponse newInstance() throws IOException\n{\r\n    return StateStoreSerializer.newRecord(DisableNameserviceResponse.class);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\protocol",
  "methodName" : "newInstance",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "DisableNameserviceResponse newInstance(boolean status) throws IOException\n{\r\n    DisableNameserviceResponse response = newInstance();\r\n    response.setStatus(status);\r\n    return response;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\protocol",
  "methodName" : "getStatus",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean getStatus()",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\protocol",
  "methodName" : "setStatus",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setStatus(boolean result)",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\protocol",
  "methodName" : "newInstance",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "GetRouterRegistrationsResponse newInstance() throws IOException\n{\r\n    return StateStoreSerializer.newRecord(GetRouterRegistrationsResponse.class);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\protocol",
  "methodName" : "getRouters",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "List<RouterState> getRouters() throws IOException",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\protocol",
  "methodName" : "setRouters",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setRouters(List<RouterState> routers) throws IOException",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\protocol",
  "methodName" : "getTimestamp",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getTimestamp()",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\protocol",
  "methodName" : "setTimestamp",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setTimestamp(long time)",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store",
  "methodName" : "serviceInit",
  "errType" : [ "NotCompliantMBeanException", "MetricsException" ],
  "containingMethodsNum" : 19,
  "sourceCodeText" : "void serviceInit(Configuration config) throws Exception\n{\r\n    this.conf = config;\r\n    Class<? extends StateStoreDriver> driverClass = this.conf.getClass(RBFConfigKeys.FEDERATION_STORE_DRIVER_CLASS, RBFConfigKeys.FEDERATION_STORE_DRIVER_CLASS_DEFAULT, StateStoreDriver.class);\r\n    this.driver = ReflectionUtils.newInstance(driverClass, this.conf);\r\n    if (this.driver == null) {\r\n        throw new IOException(\"Cannot create driver for the State Store\");\r\n    }\r\n    addRecordStore(MembershipStoreImpl.class);\r\n    addRecordStore(MountTableStoreImpl.class);\r\n    addRecordStore(RouterStoreImpl.class);\r\n    addRecordStore(DisabledNameserviceStoreImpl.class);\r\n    this.monitorService = new StateStoreConnectionMonitorService(this);\r\n    this.addService(monitorService);\r\n    MembershipState.setExpirationMs(conf.getTimeDuration(RBFConfigKeys.FEDERATION_STORE_MEMBERSHIP_EXPIRATION_MS, RBFConfigKeys.FEDERATION_STORE_MEMBERSHIP_EXPIRATION_MS_DEFAULT, TimeUnit.MILLISECONDS));\r\n    MembershipState.setDeletionMs(conf.getTimeDuration(RBFConfigKeys.FEDERATION_STORE_MEMBERSHIP_EXPIRATION_DELETION_MS, RBFConfigKeys.FEDERATION_STORE_MEMBERSHIP_EXPIRATION_DELETION_MS_DEFAULT, TimeUnit.MILLISECONDS));\r\n    RouterState.setExpirationMs(conf.getTimeDuration(RBFConfigKeys.FEDERATION_STORE_ROUTER_EXPIRATION_MS, RBFConfigKeys.FEDERATION_STORE_ROUTER_EXPIRATION_MS_DEFAULT, TimeUnit.MILLISECONDS));\r\n    RouterState.setDeletionMs(conf.getTimeDuration(RBFConfigKeys.FEDERATION_STORE_ROUTER_EXPIRATION_DELETION_MS, RBFConfigKeys.FEDERATION_STORE_ROUTER_EXPIRATION_DELETION_MS_DEFAULT, TimeUnit.MILLISECONDS));\r\n    this.cacheUpdater = new StateStoreCacheUpdateService(this);\r\n    addService(this.cacheUpdater);\r\n    if (conf.getBoolean(RBFConfigKeys.DFS_ROUTER_METRICS_ENABLE, RBFConfigKeys.DFS_ROUTER_METRICS_ENABLE_DEFAULT)) {\r\n        this.metrics = StateStoreMetrics.create(conf);\r\n        try {\r\n            StandardMBean bean = new StandardMBean(metrics, StateStoreMBean.class);\r\n            ObjectName registeredObject = MBeans.register(\"Router\", \"StateStore\", bean);\r\n            LOG.info(\"Registered StateStoreMBean: {}\", registeredObject);\r\n        } catch (NotCompliantMBeanException e) {\r\n            throw new RuntimeException(\"Bad StateStoreMBean setup\", e);\r\n        } catch (MetricsException e) {\r\n            LOG.error(\"Failed to register State Store bean {}\", e.getMessage());\r\n        }\r\n    } else {\r\n        LOG.info(\"State Store metrics not enabled\");\r\n        this.metrics = new NullStateStoreMetrics();\r\n    }\r\n    super.serviceInit(this.conf);\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store",
  "methodName" : "serviceStart",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void serviceStart() throws Exception\n{\r\n    loadDriver();\r\n    super.serviceStart();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store",
  "methodName" : "serviceStop",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void serviceStop() throws Exception\n{\r\n    closeDriver();\r\n    if (metrics != null) {\r\n        metrics.shutdown();\r\n        metrics = null;\r\n    }\r\n    super.serviceStop();\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store",
  "methodName" : "addRecordStore",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void addRecordStore(final Class<T> clazz) throws ReflectiveOperationException\n{\r\n    assert this.getServiceState() == STATE.INITED : \"Cannot add record to the State Store once started\";\r\n    T recordStore = RecordStore.newInstance(clazz, this.getDriver());\r\n    Class<? extends BaseRecord> recordClass = recordStore.getRecordClass();\r\n    this.recordStores.put(recordClass, recordStore);\r\n    if (recordStore instanceof StateStoreCache) {\r\n        StateStoreCache cachedRecordStore = (StateStoreCache) recordStore;\r\n        this.cachesToUpdateInternal.add(cachedRecordStore);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store",
  "methodName" : "getRegisteredRecordStore",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "T getRegisteredRecordStore(final Class<T> recordStoreClass)\n{\r\n    for (RecordStore<? extends BaseRecord> recordStore : this.recordStores.values()) {\r\n        if (recordStoreClass.isInstance(recordStore)) {\r\n            @SuppressWarnings(\"unchecked\")\r\n            T recordStoreChecked = (T) recordStore;\r\n            return recordStoreChecked;\r\n        }\r\n    }\r\n    return null;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store",
  "methodName" : "getSupportedRecords",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Collection<Class<? extends BaseRecord>> getSupportedRecords()\n{\r\n    return this.recordStores.keySet();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store",
  "methodName" : "loadDriver",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void loadDriver()\n{\r\n    synchronized (this.driver) {\r\n        if (!isDriverReady()) {\r\n            String driverName = this.driver.getClass().getSimpleName();\r\n            if (this.driver.init(conf, getIdentifier(), getSupportedRecords(), metrics)) {\r\n                LOG.info(\"Connection to the State Store driver {} is open and ready\", driverName);\r\n                this.refreshCaches();\r\n            } else {\r\n                LOG.error(\"Cannot initialize State Store driver {}\", driverName);\r\n            }\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store",
  "methodName" : "isDriverReady",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "boolean isDriverReady()\n{\r\n    return this.driver.isDriverReady();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store",
  "methodName" : "closeDriver",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void closeDriver() throws Exception\n{\r\n    if (this.driver != null) {\r\n        this.driver.close();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store",
  "methodName" : "getDriver",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "StateStoreDriver getDriver()\n{\r\n    return this.driver;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store",
  "methodName" : "getIdentifier",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getIdentifier()\n{\r\n    return this.identifier;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store",
  "methodName" : "setIdentifier",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setIdentifier(String id)\n{\r\n    this.identifier = id;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store",
  "methodName" : "getCacheUpdateTime",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getCacheUpdateTime()\n{\r\n    return this.cacheLastUpdateTime;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store",
  "methodName" : "stopCacheUpdateService",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void stopCacheUpdateService()\n{\r\n    if (this.cacheUpdater != null) {\r\n        this.cacheUpdater.stop();\r\n        removeService(this.cacheUpdater);\r\n        this.cacheUpdater = null;\r\n    }\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store",
  "methodName" : "registerCacheExternal",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void registerCacheExternal(StateStoreCache client)\n{\r\n    this.cachesToUpdateExternal.add(client);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store",
  "methodName" : "refreshCaches",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void refreshCaches()\n{\r\n    refreshCaches(false);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store",
  "methodName" : "refreshCaches",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 9,
  "sourceCodeText" : "void refreshCaches(boolean force)\n{\r\n    boolean success = true;\r\n    if (isDriverReady()) {\r\n        List<StateStoreCache> cachesToUpdate = new LinkedList<>();\r\n        cachesToUpdate.addAll(cachesToUpdateInternal);\r\n        cachesToUpdate.addAll(cachesToUpdateExternal);\r\n        for (StateStoreCache cachedStore : cachesToUpdate) {\r\n            String cacheName = cachedStore.getClass().getSimpleName();\r\n            boolean result = false;\r\n            try {\r\n                result = cachedStore.loadCache(force);\r\n            } catch (IOException e) {\r\n                LOG.error(\"Error updating cache for {}\", cacheName, e);\r\n                result = false;\r\n            }\r\n            if (!result) {\r\n                success = false;\r\n                LOG.error(\"Cache update failed for cache {}\", cacheName);\r\n            }\r\n        }\r\n    } else {\r\n        success = false;\r\n        LOG.info(\"Skipping State Store cache update, driver is not ready.\");\r\n    }\r\n    if (success) {\r\n        this.cacheLastUpdateTime = Time.now();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store",
  "methodName" : "loadCache",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "boolean loadCache(final Class<?> clazz) throws IOException\n{\r\n    return loadCache(clazz, false);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store",
  "methodName" : "loadCache",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "boolean loadCache(Class<?> clazz, boolean force) throws IOException\n{\r\n    List<StateStoreCache> cachesToUpdate = new LinkedList<StateStoreCache>();\r\n    cachesToUpdate.addAll(this.cachesToUpdateInternal);\r\n    cachesToUpdate.addAll(this.cachesToUpdateExternal);\r\n    for (StateStoreCache cachedStore : cachesToUpdate) {\r\n        if (clazz.isInstance(cachedStore)) {\r\n            return cachedStore.loadCache(force);\r\n        }\r\n    }\r\n    throw new IOException(\"Registered cache was not found for \" + clazz);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store",
  "methodName" : "getMetrics",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "StateStoreMetrics getMetrics()\n{\r\n    return metrics;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\metrics",
  "methodName" : "init",
  "errType" : [ "NotCompliantMBeanException" ],
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void init(Configuration configuration, RouterRpcServer rpcServer, StateStoreService stateStore)\n{\r\n    this.conf = configuration;\r\n    this.server = rpcServer;\r\n    this.store = stateStore;\r\n    this.metrics = FederationRPCMetrics.create(conf, server);\r\n    for (String nameservice : FederationUtil.getAllConfiguredNS(conf)) {\r\n        LOG.info(\"Create Nameservice RPC Metrics for \" + nameservice);\r\n        this.nameserviceRPCMetricsMap.computeIfAbsent(nameservice, k -> NameserviceRPCMetrics.create(conf, k));\r\n    }\r\n    ThreadFactory threadFactory = new ThreadFactoryBuilder().setNameFormat(\"Federation RPC Performance Monitor-%d\").build();\r\n    this.executor = Executors.newFixedThreadPool(1, threadFactory);\r\n    try {\r\n        StandardMBean bean = new StandardMBean(this.metrics, FederationRPCMBean.class);\r\n        registeredBean = MBeans.register(\"Router\", \"FederationRPC\", bean);\r\n        LOG.info(\"Registered FederationRPCMBean: {}\", registeredBean);\r\n    } catch (NotCompliantMBeanException e) {\r\n        throw new RuntimeException(\"Bad FederationRPCMBean setup\", e);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\metrics",
  "methodName" : "close",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void close()\n{\r\n    if (registeredBean != null) {\r\n        MBeans.unregister(registeredBean);\r\n        registeredBean = null;\r\n    }\r\n    if (this.executor != null) {\r\n        this.executor.shutdown();\r\n    }\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\metrics",
  "methodName" : "resetPerfCounters",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void resetPerfCounters()\n{\r\n    if (registeredBean != null) {\r\n        MBeans.unregister(registeredBean);\r\n        registeredBean = null;\r\n    }\r\n    if (metrics != null) {\r\n        FederationRPCMetrics.reset();\r\n        metrics = null;\r\n    }\r\n    init(conf, server, store);\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\metrics",
  "methodName" : "startOp",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void startOp()\n{\r\n    START_TIME.set(monotonicNow());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\metrics",
  "methodName" : "proxyOp",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "long proxyOp()\n{\r\n    PROXY_TIME.set(monotonicNow());\r\n    long processingTime = getProcessingTime();\r\n    if (metrics != null && processingTime >= 0) {\r\n        metrics.addProcessingTime(processingTime);\r\n    }\r\n    return Thread.currentThread().getId();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\metrics",
  "methodName" : "proxyOpComplete",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void proxyOpComplete(boolean success, String nsId)\n{\r\n    if (success) {\r\n        long proxyTime = getProxyTime();\r\n        if (proxyTime >= 0) {\r\n            if (metrics != null) {\r\n                metrics.addProxyTime(proxyTime);\r\n            }\r\n            if (nameserviceRPCMetricsMap != null && nameserviceRPCMetricsMap.containsKey(nsId)) {\r\n                nameserviceRPCMetricsMap.get(nsId).addProxyTime(proxyTime);\r\n            }\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\metrics",
  "methodName" : "proxyOpFailureStandby",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void proxyOpFailureStandby(String nsId)\n{\r\n    if (metrics != null) {\r\n        metrics.incrProxyOpFailureStandby();\r\n    }\r\n    if (nameserviceRPCMetricsMap != null && nameserviceRPCMetricsMap.containsKey(nsId)) {\r\n        nameserviceRPCMetricsMap.get(nsId).incrProxyOpFailureStandby();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\metrics",
  "methodName" : "proxyOpFailureCommunicate",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void proxyOpFailureCommunicate(String nsId)\n{\r\n    if (metrics != null) {\r\n        metrics.incrProxyOpFailureCommunicate();\r\n    }\r\n    if (nameserviceRPCMetricsMap != null && nameserviceRPCMetricsMap.containsKey(nsId)) {\r\n        nameserviceRPCMetricsMap.get(nsId).incrProxyOpFailureCommunicate();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\metrics",
  "methodName" : "proxyOpFailureClientOverloaded",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void proxyOpFailureClientOverloaded()\n{\r\n    if (metrics != null) {\r\n        metrics.incrProxyOpFailureClientOverloaded();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\metrics",
  "methodName" : "proxyOpNotImplemented",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void proxyOpNotImplemented()\n{\r\n    if (metrics != null) {\r\n        metrics.incrProxyOpNotImplemented();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\metrics",
  "methodName" : "proxyOpRetries",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void proxyOpRetries()\n{\r\n    if (metrics != null) {\r\n        metrics.incrProxyOpRetries();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\metrics",
  "methodName" : "proxyOpNoNamenodes",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void proxyOpNoNamenodes(String nsId)\n{\r\n    if (metrics != null) {\r\n        metrics.incrProxyOpNoNamenodes();\r\n    }\r\n    if (nameserviceRPCMetricsMap != null && nameserviceRPCMetricsMap.containsKey(nsId)) {\r\n        nameserviceRPCMetricsMap.get(nsId).incrProxyOpNoNamenodes();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\metrics",
  "methodName" : "routerFailureStateStore",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void routerFailureStateStore()\n{\r\n    if (metrics != null) {\r\n        metrics.incrRouterFailureStateStore();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\metrics",
  "methodName" : "routerFailureSafemode",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void routerFailureSafemode()\n{\r\n    if (metrics != null) {\r\n        metrics.incrRouterFailureSafemode();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\metrics",
  "methodName" : "routerFailureReadOnly",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void routerFailureReadOnly()\n{\r\n    if (metrics != null) {\r\n        metrics.incrRouterFailureReadOnly();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\metrics",
  "methodName" : "routerFailureLocked",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void routerFailureLocked()\n{\r\n    if (metrics != null) {\r\n        metrics.incrRouterFailureLocked();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\metrics",
  "methodName" : "getProcessingTime",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "long getProcessingTime()\n{\r\n    if (START_TIME.get() != null && START_TIME.get() > 0 && PROXY_TIME.get() != null && PROXY_TIME.get() > 0) {\r\n        return PROXY_TIME.get() - START_TIME.get();\r\n    }\r\n    return -1;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\metrics",
  "methodName" : "getProxyTime",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "long getProxyTime()\n{\r\n    if (PROXY_TIME.get() != null && PROXY_TIME.get() > 0) {\r\n        return monotonicNow() - PROXY_TIME.get();\r\n    }\r\n    return -1;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\metrics",
  "methodName" : "getRPCMetrics",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "FederationRPCMetrics getRPCMetrics()\n{\r\n    return this.metrics;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\protocol\\impl\\pb",
  "methodName" : "getProto",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "EnterSafeModeRequestProto getProto()\n{\r\n    return translator.build();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\protocol\\impl\\pb",
  "methodName" : "setProto",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setProto(Message proto)\n{\r\n    this.translator.setProto(proto);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\protocol\\impl\\pb",
  "methodName" : "readInstance",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void readInstance(String base64String) throws IOException\n{\r\n    this.translator.readInstance(base64String);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "initRouterFedRename",
  "errType" : [ "URISyntaxException|NullPointerException" ],
  "containingMethodsNum" : 11,
  "sourceCodeText" : "void initRouterFedRename() throws IOException\n{\r\n    routerRenameOption = RouterRenameOption.valueOf(conf.get(DFS_ROUTER_FEDERATION_RENAME_OPTION, DFS_ROUTER_FEDERATION_RENAME_OPTION_DEFAULT).toUpperCase());\r\n    switch(routerRenameOption) {\r\n        case DISTCP:\r\n            RouterFederationRename.checkConfiguration(conf);\r\n            Configuration sConf = new Configuration(conf);\r\n            URI journalUri;\r\n            try {\r\n                journalUri = new URI(sConf.get(SCHEDULER_JOURNAL_URI));\r\n            } catch (URISyntaxException | NullPointerException e) {\r\n                throw new IOException(\"Bad journal uri. Please check configuration for \" + SCHEDULER_JOURNAL_URI);\r\n            }\r\n            Path child;\r\n            String nsId = DFSUtil.getNamenodeNameServiceId(conf);\r\n            String namenodeId = HAUtil.getNameNodeId(conf, nsId);\r\n            InetSocketAddress listenAddress = this.rpcServer.getListenerAddress();\r\n            if (nsId == null || namenodeId == null) {\r\n                child = new Path(listenAddress.getHostName() + \"_\" + listenAddress.getPort());\r\n            } else {\r\n                child = new Path(nsId, namenodeId);\r\n            }\r\n            String routerJournal = new Path(journalUri.toString(), child).toString();\r\n            sConf.set(SCHEDULER_JOURNAL_URI, routerJournal);\r\n            fedRenameScheduler = new BalanceProcedureScheduler(sConf);\r\n            fedRenameScheduler.init(true);\r\n            break;\r\n        case NONE:\r\n            fedRenameScheduler = null;\r\n            break;\r\n        default:\r\n            break;\r\n    }\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "serviceInit",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void serviceInit(Configuration configuration) throws Exception\n{\r\n    this.conf = configuration;\r\n    if (this.rpcMonitor == null) {\r\n        LOG.info(\"Do not start Router RPC metrics\");\r\n    } else {\r\n        this.rpcMonitor.init(this.conf, this, this.router.getStateStore());\r\n    }\r\n    super.serviceInit(configuration);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "serviceStart",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void serviceStart() throws Exception\n{\r\n    if (this.rpcServer != null) {\r\n        this.rpcServer.start();\r\n        LOG.info(\"Router RPC up at: {}\", this.getRpcAddress());\r\n    }\r\n    super.serviceStart();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "serviceStop",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void serviceStop() throws Exception\n{\r\n    if (this.rpcServer != null) {\r\n        this.rpcServer.stop();\r\n    }\r\n    if (rpcMonitor != null) {\r\n        this.rpcMonitor.close();\r\n    }\r\n    if (securityManager != null) {\r\n        this.securityManager.stop();\r\n    }\r\n    if (this.fedRenameScheduler != null) {\r\n        fedRenameScheduler.shutDown();\r\n    }\r\n    super.serviceStop();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "isEnableRenameAcrossNamespace",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean isEnableRenameAcrossNamespace()\n{\r\n    return routerRenameOption != RouterRenameOption.NONE;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "getFedRenameScheduler",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "BalanceProcedureScheduler getFedRenameScheduler()\n{\r\n    return this.fedRenameScheduler;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "getRouterSecurityManager",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "RouterSecurityManager getRouterSecurityManager()\n{\r\n    return this.securityManager;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "getRPCClient",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "RouterRpcClient getRPCClient()\n{\r\n    return rpcClient;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "getSubclusterResolver",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "FileSubclusterResolver getSubclusterResolver()\n{\r\n    return subclusterResolver;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "getNamenodeResolver",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "ActiveNamenodeResolver getNamenodeResolver()\n{\r\n    return namenodeResolver;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "getRPCMonitor",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "RouterRpcMonitor getRPCMonitor()\n{\r\n    return rpcMonitor;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "getServer",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Server getServer()\n{\r\n    return rpcServer;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "getRpcAddress",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "InetSocketAddress getRpcAddress()\n{\r\n    return rpcAddress;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "checkOperation",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void checkOperation(OperationCategory op, boolean supported) throws StandbyException, UnsupportedOperationException\n{\r\n    checkOperation(op);\r\n    if (!supported) {\r\n        if (rpcMonitor != null) {\r\n            rpcMonitor.proxyOpNotImplemented();\r\n        }\r\n        String methodName = getMethodName();\r\n        throw new UnsupportedOperationException(\"Operation \\\"\" + methodName + \"\\\" is not supported\");\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "checkOperation",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void checkOperation(OperationCategory op) throws StandbyException\n{\r\n    if (rpcMonitor != null) {\r\n        rpcMonitor.startOp();\r\n    }\r\n    if (LOG.isDebugEnabled()) {\r\n        String methodName = getMethodName();\r\n        LOG.debug(\"Proxying operation: {}\", methodName);\r\n    }\r\n    opCategory.set(op);\r\n    if (op == OperationCategory.UNCHECKED || op == OperationCategory.READ) {\r\n        return;\r\n    }\r\n    checkSafeMode();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "checkSafeMode",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void checkSafeMode() throws StandbyException\n{\r\n    if (isSafeMode()) {\r\n        if (rpcMonitor != null) {\r\n            rpcMonitor.routerFailureSafemode();\r\n        }\r\n        OperationCategory op = opCategory.get();\r\n        throw new StandbyException(\"Router \" + router.getRouterId() + \" is in safe mode and cannot handle \" + op + \" requests\");\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "isSafeMode",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "boolean isSafeMode()\n{\r\n    RouterSafemodeService safemodeService = router.getSafemodeService();\r\n    return (safemodeService != null && safemodeService.isInSafeMode());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "getMethodName",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "String getMethodName()\n{\r\n    final StackTraceElement[] stack = Thread.currentThread().getStackTrace();\r\n    String methodName = stack[3].getMethodName();\r\n    return methodName;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "invokeAtAvailableNs",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 9,
  "sourceCodeText" : "T invokeAtAvailableNs(RemoteMethod method, Class<T> clazz) throws IOException\n{\r\n    String nsId = subclusterResolver.getDefaultNamespace();\r\n    Set<FederationNamespaceInfo> nss = namenodeResolver.getNamespaces();\r\n    IOException io = new IOException(\"No namespace available.\");\r\n    if (!nsId.isEmpty()) {\r\n        try {\r\n            return rpcClient.invokeSingle(nsId, method, clazz);\r\n        } catch (IOException ioe) {\r\n            if (!clientProto.isUnavailableSubclusterException(ioe)) {\r\n                LOG.debug(\"{} exception cannot be retried\", ioe.getClass().getSimpleName());\r\n                throw ioe;\r\n            }\r\n            nss.removeIf(n -> n.getNameserviceId().equals(nsId));\r\n            return invokeOnNs(method, clazz, io, nss);\r\n        }\r\n    }\r\n    return invokeOnNs(method, clazz, io, nss);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "invokeOnNs",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 6,
  "sourceCodeText" : "T invokeOnNs(RemoteMethod method, Class<T> clazz, IOException ioe, Set<FederationNamespaceInfo> nss) throws IOException\n{\r\n    if (nss.isEmpty()) {\r\n        throw ioe;\r\n    }\r\n    for (FederationNamespaceInfo fnInfo : nss) {\r\n        String nsId = fnInfo.getNameserviceId();\r\n        LOG.debug(\"Invoking {} on namespace {}\", method, nsId);\r\n        try {\r\n            return rpcClient.invokeSingle(nsId, method, clazz);\r\n        } catch (IOException e) {\r\n            LOG.debug(\"Failed to invoke {} on namespace {}\", method, nsId, e);\r\n            if (!clientProto.isUnavailableSubclusterException(e)) {\r\n                throw e;\r\n            }\r\n        }\r\n    }\r\n    throw ioe;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "getDelegationToken",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Token<DelegationTokenIdentifier> getDelegationToken(Text renewer) throws IOException\n{\r\n    return clientProto.getDelegationToken(renewer);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "renewDelegationToken",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "long renewDelegationToken(Token<DelegationTokenIdentifier> token) throws IOException\n{\r\n    return clientProto.renewDelegationToken(token);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "cancelDelegationToken",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void cancelDelegationToken(Token<DelegationTokenIdentifier> token) throws IOException\n{\r\n    clientProto.cancelDelegationToken(token);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "getBlockLocations",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "LocatedBlocks getBlockLocations(String src, final long offset, final long length) throws IOException\n{\r\n    return clientProto.getBlockLocations(src, offset, length);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "getServerDefaults",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "FsServerDefaults getServerDefaults() throws IOException\n{\r\n    return clientProto.getServerDefaults();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "create",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "HdfsFileStatus create(String src, FsPermission masked, String clientName, EnumSetWritable<CreateFlag> flag, boolean createParent, short replication, long blockSize, CryptoProtocolVersion[] supportedVersions, String ecPolicyName, String storagePolicy) throws IOException\n{\r\n    return clientProto.create(src, masked, clientName, flag, createParent, replication, blockSize, supportedVersions, ecPolicyName, storagePolicy);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "getCreateLocation",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "RemoteLocation getCreateLocation(final String src) throws IOException\n{\r\n    final List<RemoteLocation> locations = getLocationsForPath(src, true);\r\n    return getCreateLocation(src, locations);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "getCreateLocation",
  "errType" : [ "FileNotFoundException" ],
  "containingMethodsNum" : 5,
  "sourceCodeText" : "RemoteLocation getCreateLocation(final String src, final List<RemoteLocation> locations) throws IOException\n{\r\n    if (locations == null || locations.isEmpty()) {\r\n        throw new IOException(\"Cannot get locations to create \" + src);\r\n    }\r\n    RemoteLocation createLocation = locations.get(0);\r\n    if (locations.size() > 1) {\r\n        try {\r\n            RemoteLocation existingLocation = getExistingLocation(src, locations);\r\n            if (existingLocation != null) {\r\n                LOG.debug(\"{} already exists in {}.\", src, existingLocation);\r\n                createLocation = existingLocation;\r\n            }\r\n        } catch (FileNotFoundException fne) {\r\n        }\r\n    }\r\n    return createLocation;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "getExistingLocation",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "RemoteLocation getExistingLocation(String src, List<RemoteLocation> locations) throws IOException\n{\r\n    RemoteMethod method = new RemoteMethod(\"getFileInfo\", new Class<?>[] { String.class }, new RemoteParam());\r\n    Map<RemoteLocation, HdfsFileStatus> results = rpcClient.invokeConcurrent(locations, method, true, false, HdfsFileStatus.class);\r\n    for (RemoteLocation loc : locations) {\r\n        if (results.get(loc) != null) {\r\n            return loc;\r\n        }\r\n    }\r\n    return null;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "append",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "LastBlockWithStatus append(String src, final String clientName, final EnumSetWritable<CreateFlag> flag) throws IOException\n{\r\n    return clientProto.append(src, clientName, flag);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "recoverLease",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "boolean recoverLease(String src, String clientName) throws IOException\n{\r\n    return clientProto.recoverLease(src, clientName);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "setReplication",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "boolean setReplication(String src, short replication) throws IOException\n{\r\n    return clientProto.setReplication(src, replication);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "setStoragePolicy",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setStoragePolicy(String src, String policyName) throws IOException\n{\r\n    clientProto.setStoragePolicy(src, policyName);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "getStoragePolicies",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "BlockStoragePolicy[] getStoragePolicies() throws IOException\n{\r\n    return clientProto.getStoragePolicies();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "setPermission",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setPermission(String src, FsPermission permissions) throws IOException\n{\r\n    clientProto.setPermission(src, permissions);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "setOwner",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setOwner(String src, String username, String groupname) throws IOException\n{\r\n    clientProto.setOwner(src, username, groupname);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "addBlock",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "LocatedBlock addBlock(String src, String clientName, ExtendedBlock previous, DatanodeInfo[] excludedNodes, long fileId, String[] favoredNodes, EnumSet<AddBlockFlag> addBlockFlags) throws IOException\n{\r\n    return clientProto.addBlock(src, clientName, previous, excludedNodes, fileId, favoredNodes, addBlockFlags);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "getAdditionalDatanode",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "LocatedBlock getAdditionalDatanode(final String src, final long fileId, final ExtendedBlock blk, final DatanodeInfo[] existings, final String[] existingStorageIDs, final DatanodeInfo[] excludes, final int numAdditionalNodes, final String clientName) throws IOException\n{\r\n    return clientProto.getAdditionalDatanode(src, fileId, blk, existings, existingStorageIDs, excludes, numAdditionalNodes, clientName);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "abandonBlock",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void abandonBlock(ExtendedBlock b, long fileId, String src, String holder) throws IOException\n{\r\n    clientProto.abandonBlock(b, fileId, src, holder);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "complete",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "boolean complete(String src, String clientName, ExtendedBlock last, long fileId) throws IOException\n{\r\n    return clientProto.complete(src, clientName, last, fileId);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "updateBlockForPipeline",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "LocatedBlock updateBlockForPipeline(ExtendedBlock block, String clientName) throws IOException\n{\r\n    return clientProto.updateBlockForPipeline(block, clientName);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "updatePipeline",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void updatePipeline(String clientName, ExtendedBlock oldBlock, ExtendedBlock newBlock, DatanodeID[] newNodes, String[] newStorageIDs) throws IOException\n{\r\n    clientProto.updatePipeline(clientName, oldBlock, newBlock, newNodes, newStorageIDs);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "getPreferredBlockSize",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "long getPreferredBlockSize(String src) throws IOException\n{\r\n    return clientProto.getPreferredBlockSize(src);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "rename",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "boolean rename(final String src, final String dst) throws IOException\n{\r\n    return clientProto.rename(src, dst);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "rename2",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void rename2(final String src, final String dst, final Options.Rename... options) throws IOException\n{\r\n    clientProto.rename2(src, dst, options);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "concat",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void concat(String trg, String[] src) throws IOException\n{\r\n    clientProto.concat(trg, src);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "truncate",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "boolean truncate(String src, long newLength, String clientName) throws IOException\n{\r\n    return clientProto.truncate(src, newLength, clientName);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "delete",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "boolean delete(String src, boolean recursive) throws IOException\n{\r\n    return clientProto.delete(src, recursive);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "mkdirs",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "boolean mkdirs(String src, FsPermission masked, boolean createParent) throws IOException\n{\r\n    return clientProto.mkdirs(src, masked, createParent);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "renewLease",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void renewLease(String clientName) throws IOException\n{\r\n    clientProto.renewLease(clientName);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "getListing",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "DirectoryListing getListing(String src, byte[] startAfter, boolean needLocation) throws IOException\n{\r\n    return clientProto.getListing(src, startAfter, needLocation);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "getBatchedListing",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "BatchedDirectoryListing getBatchedListing(String[] srcs, byte[] startAfter, boolean needLocation) throws IOException\n{\r\n    throw new UnsupportedOperationException();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "getFileInfo",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "HdfsFileStatus getFileInfo(String src) throws IOException\n{\r\n    return clientProto.getFileInfo(src);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "isFileClosed",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "boolean isFileClosed(String src) throws IOException\n{\r\n    return clientProto.isFileClosed(src);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "getFileLinkInfo",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "HdfsFileStatus getFileLinkInfo(String src) throws IOException\n{\r\n    return clientProto.getFileLinkInfo(src);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "getLocatedFileInfo",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "HdfsLocatedFileStatus getLocatedFileInfo(String src, boolean needBlockToken) throws IOException\n{\r\n    return clientProto.getLocatedFileInfo(src, needBlockToken);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "getStats",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "long[] getStats() throws IOException\n{\r\n    return clientProto.getStats();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "getDatanodeReport",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "DatanodeInfo[] getDatanodeReport(DatanodeReportType type) throws IOException\n{\r\n    return clientProto.getDatanodeReport(type);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "getCachedDatanodeReport",
  "errType" : [ "ExecutionException" ],
  "containingMethodsNum" : 6,
  "sourceCodeText" : "DatanodeInfo[] getCachedDatanodeReport(DatanodeReportType type) throws IOException\n{\r\n    try {\r\n        DatanodeInfo[] dns = this.dnCache.get(type);\r\n        if (dns == null) {\r\n            LOG.debug(\"Get null DN report from cache\");\r\n            dns = getCachedDatanodeReportImpl(type);\r\n            this.dnCache.put(type, dns);\r\n        }\r\n        return dns;\r\n    } catch (ExecutionException e) {\r\n        LOG.error(\"Cannot get the DN report for {}\", type, e);\r\n        Throwable cause = e.getCause();\r\n        if (cause instanceof IOException) {\r\n            throw (IOException) cause;\r\n        } else {\r\n            throw new IOException(cause);\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "getCachedDatanodeReportImpl",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "DatanodeInfo[] getCachedDatanodeReportImpl(final DatanodeReportType type) throws IOException\n{\r\n    UserGroupInformation loginUser = UserGroupInformation.getLoginUser();\r\n    RouterRpcServer.setCurrentUser(loginUser);\r\n    try {\r\n        DatanodeInfo[] dns = clientProto.getDatanodeReport(type);\r\n        LOG.debug(\"Refresh cached DN report with {} datanodes\", dns.length);\r\n        return dns;\r\n    } finally {\r\n        RouterRpcServer.resetCurrentUser();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "getDatanodeReport",
  "errType" : null,
  "containingMethodsNum" : 15,
  "sourceCodeText" : "DatanodeInfo[] getDatanodeReport(DatanodeReportType type, boolean requireResponse, long timeOutMs) throws IOException\n{\r\n    checkOperation(OperationCategory.UNCHECKED);\r\n    Map<String, DatanodeInfo> datanodesMap = new LinkedHashMap<>();\r\n    RemoteMethod method = new RemoteMethod(\"getDatanodeReport\", new Class<?>[] { DatanodeReportType.class }, type);\r\n    Set<FederationNamespaceInfo> nss = namenodeResolver.getNamespaces();\r\n    Map<FederationNamespaceInfo, DatanodeInfo[]> results = rpcClient.invokeConcurrent(nss, method, requireResponse, false, timeOutMs, DatanodeInfo[].class);\r\n    for (Entry<FederationNamespaceInfo, DatanodeInfo[]> entry : results.entrySet()) {\r\n        FederationNamespaceInfo ns = entry.getKey();\r\n        DatanodeInfo[] result = entry.getValue();\r\n        for (DatanodeInfo node : result) {\r\n            String nodeId = node.getXferAddr();\r\n            DatanodeInfo dn = datanodesMap.get(nodeId);\r\n            if (dn == null || node.getLastUpdate() > dn.getLastUpdate()) {\r\n                node.setNetworkLocation(NodeBase.PATH_SEPARATOR_STR + ns.getNameserviceId() + node.getNetworkLocation());\r\n                datanodesMap.put(nodeId, node);\r\n            } else {\r\n                LOG.debug(\"{} is in multiple subclusters\", nodeId);\r\n            }\r\n        }\r\n    }\r\n    Collection<DatanodeInfo> datanodes = datanodesMap.values();\r\n    return toArray(datanodes, DatanodeInfo.class);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "getDatanodeStorageReport",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "DatanodeStorageReport[] getDatanodeStorageReport(DatanodeReportType type) throws IOException\n{\r\n    return clientProto.getDatanodeStorageReport(type);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "getDatanodeStorageReportMap",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Map<String, DatanodeStorageReport[]> getDatanodeStorageReportMap(DatanodeReportType type) throws IOException\n{\r\n    return getDatanodeStorageReportMap(type, true, -1);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "getDatanodeStorageReportMap",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "Map<String, DatanodeStorageReport[]> getDatanodeStorageReportMap(DatanodeReportType type, boolean requireResponse, long timeOutMs) throws IOException\n{\r\n    Map<String, DatanodeStorageReport[]> ret = new LinkedHashMap<>();\r\n    RemoteMethod method = new RemoteMethod(\"getDatanodeStorageReport\", new Class<?>[] { DatanodeReportType.class }, type);\r\n    Set<FederationNamespaceInfo> nss = namenodeResolver.getNamespaces();\r\n    Map<FederationNamespaceInfo, DatanodeStorageReport[]> results = rpcClient.invokeConcurrent(nss, method, requireResponse, false, timeOutMs, DatanodeStorageReport[].class);\r\n    for (Entry<FederationNamespaceInfo, DatanodeStorageReport[]> entry : results.entrySet()) {\r\n        FederationNamespaceInfo ns = entry.getKey();\r\n        String nsId = ns.getNameserviceId();\r\n        DatanodeStorageReport[] result = entry.getValue();\r\n        ret.put(nsId, result);\r\n    }\r\n    return ret;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "setSafeMode",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "boolean setSafeMode(SafeModeAction action, boolean isChecked) throws IOException\n{\r\n    return clientProto.setSafeMode(action, isChecked);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "restoreFailedStorage",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "boolean restoreFailedStorage(String arg) throws IOException\n{\r\n    return clientProto.restoreFailedStorage(arg);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "saveNamespace",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "boolean saveNamespace(long timeWindow, long txGap) throws IOException\n{\r\n    return clientProto.saveNamespace(timeWindow, txGap);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "rollEdits",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "long rollEdits() throws IOException\n{\r\n    return clientProto.rollEdits();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "refreshNodes",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void refreshNodes() throws IOException\n{\r\n    clientProto.refreshNodes();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "finalizeUpgrade",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void finalizeUpgrade() throws IOException\n{\r\n    clientProto.finalizeUpgrade();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "upgradeStatus",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "boolean upgradeStatus() throws IOException\n{\r\n    return clientProto.upgradeStatus();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "rollingUpgrade",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "RollingUpgradeInfo rollingUpgrade(RollingUpgradeAction action) throws IOException\n{\r\n    return clientProto.rollingUpgrade(action);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "metaSave",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void metaSave(String filename) throws IOException\n{\r\n    clientProto.metaSave(filename);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "listCorruptFileBlocks",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "CorruptFileBlocks listCorruptFileBlocks(String path, String cookie) throws IOException\n{\r\n    return clientProto.listCorruptFileBlocks(path, cookie);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "setBalancerBandwidth",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setBalancerBandwidth(long bandwidth) throws IOException\n{\r\n    clientProto.setBalancerBandwidth(bandwidth);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "getContentSummary",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "ContentSummary getContentSummary(String path) throws IOException\n{\r\n    return clientProto.getContentSummary(path);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "fsync",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void fsync(String src, long fileId, String clientName, long lastBlockLength) throws IOException\n{\r\n    clientProto.fsync(src, fileId, clientName, lastBlockLength);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "setTimes",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setTimes(String src, long mtime, long atime) throws IOException\n{\r\n    clientProto.setTimes(src, mtime, atime);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "createSymlink",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void createSymlink(String target, String link, FsPermission dirPerms, boolean createParent) throws IOException\n{\r\n    clientProto.createSymlink(target, link, dirPerms, createParent);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "getLinkTarget",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String getLinkTarget(String path) throws IOException\n{\r\n    return clientProto.getLinkTarget(path);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "allowSnapshot",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void allowSnapshot(String snapshotRoot) throws IOException\n{\r\n    clientProto.allowSnapshot(snapshotRoot);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "disallowSnapshot",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void disallowSnapshot(String snapshot) throws IOException\n{\r\n    clientProto.disallowSnapshot(snapshot);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "renameSnapshot",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void renameSnapshot(String snapshotRoot, String snapshotOldName, String snapshotNewName) throws IOException\n{\r\n    clientProto.renameSnapshot(snapshotRoot, snapshotOldName, snapshotNewName);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "getSnapshottableDirListing",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "SnapshottableDirectoryStatus[] getSnapshottableDirListing() throws IOException\n{\r\n    return clientProto.getSnapshottableDirListing();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "getSnapshotListing",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "SnapshotStatus[] getSnapshotListing(String snapshotRoot) throws IOException\n{\r\n    return clientProto.getSnapshotListing(snapshotRoot);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "getSnapshotDiffReport",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "SnapshotDiffReport getSnapshotDiffReport(String snapshotRoot, String earlierSnapshotName, String laterSnapshotName) throws IOException\n{\r\n    return clientProto.getSnapshotDiffReport(snapshotRoot, earlierSnapshotName, laterSnapshotName);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "getSnapshotDiffReportListing",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "SnapshotDiffReportListing getSnapshotDiffReportListing(String snapshotRoot, String earlierSnapshotName, String laterSnapshotName, byte[] startPath, int index) throws IOException\n{\r\n    return clientProto.getSnapshotDiffReportListing(snapshotRoot, earlierSnapshotName, laterSnapshotName, startPath, index);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "addCacheDirective",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "long addCacheDirective(CacheDirectiveInfo path, EnumSet<CacheFlag> flags) throws IOException\n{\r\n    return clientProto.addCacheDirective(path, flags);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "modifyCacheDirective",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void modifyCacheDirective(CacheDirectiveInfo directive, EnumSet<CacheFlag> flags) throws IOException\n{\r\n    clientProto.modifyCacheDirective(directive, flags);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "removeCacheDirective",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void removeCacheDirective(long id) throws IOException\n{\r\n    clientProto.removeCacheDirective(id);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "listCacheDirectives",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "BatchedEntries<CacheDirectiveEntry> listCacheDirectives(long prevId, CacheDirectiveInfo filter) throws IOException\n{\r\n    return clientProto.listCacheDirectives(prevId, filter);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "addCachePool",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void addCachePool(CachePoolInfo info) throws IOException\n{\r\n    clientProto.addCachePool(info);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "modifyCachePool",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void modifyCachePool(CachePoolInfo info) throws IOException\n{\r\n    clientProto.modifyCachePool(info);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "removeCachePool",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void removeCachePool(String cachePoolName) throws IOException\n{\r\n    clientProto.removeCachePool(cachePoolName);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "listCachePools",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "BatchedEntries<CachePoolEntry> listCachePools(String prevKey) throws IOException\n{\r\n    return clientProto.listCachePools(prevKey);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "modifyAclEntries",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void modifyAclEntries(String src, List<AclEntry> aclSpec) throws IOException\n{\r\n    clientProto.modifyAclEntries(src, aclSpec);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "removeAclEntries",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void removeAclEntries(String src, List<AclEntry> aclSpec) throws IOException\n{\r\n    clientProto.removeAclEntries(src, aclSpec);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "removeDefaultAcl",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void removeDefaultAcl(String src) throws IOException\n{\r\n    clientProto.removeDefaultAcl(src);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "removeAcl",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void removeAcl(String src) throws IOException\n{\r\n    clientProto.removeAcl(src);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "setAcl",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setAcl(String src, List<AclEntry> aclSpec) throws IOException\n{\r\n    clientProto.setAcl(src, aclSpec);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "getAclStatus",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "AclStatus getAclStatus(String src) throws IOException\n{\r\n    return clientProto.getAclStatus(src);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "createEncryptionZone",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void createEncryptionZone(String src, String keyName) throws IOException\n{\r\n    clientProto.createEncryptionZone(src, keyName);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "getEZForPath",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "EncryptionZone getEZForPath(String src) throws IOException\n{\r\n    return clientProto.getEZForPath(src);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "listEncryptionZones",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "BatchedEntries<EncryptionZone> listEncryptionZones(long prevId) throws IOException\n{\r\n    return clientProto.listEncryptionZones(prevId);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "reencryptEncryptionZone",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void reencryptEncryptionZone(String zone, ReencryptAction action) throws IOException\n{\r\n    clientProto.reencryptEncryptionZone(zone, action);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "listReencryptionStatus",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "BatchedEntries<ZoneReencryptionStatus> listReencryptionStatus(long prevId) throws IOException\n{\r\n    return clientProto.listReencryptionStatus(prevId);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "setXAttr",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setXAttr(String src, XAttr xAttr, EnumSet<XAttrSetFlag> flag) throws IOException\n{\r\n    clientProto.setXAttr(src, xAttr, flag);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "getXAttrs",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "List<XAttr> getXAttrs(String src, List<XAttr> xAttrs) throws IOException\n{\r\n    return clientProto.getXAttrs(src, xAttrs);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "listXAttrs",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "List<XAttr> listXAttrs(String src) throws IOException\n{\r\n    return clientProto.listXAttrs(src);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "removeXAttr",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void removeXAttr(String src, XAttr xAttr) throws IOException\n{\r\n    clientProto.removeXAttr(src, xAttr);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "checkAccess",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void checkAccess(String path, FsAction mode) throws IOException\n{\r\n    clientProto.checkAccess(path, mode);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "getCurrentEditLogTxid",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "long getCurrentEditLogTxid() throws IOException\n{\r\n    return clientProto.getCurrentEditLogTxid();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "getEditsFromTxid",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "EventBatchList getEditsFromTxid(long txid) throws IOException\n{\r\n    return clientProto.getEditsFromTxid(txid);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "getDataEncryptionKey",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "DataEncryptionKey getDataEncryptionKey() throws IOException\n{\r\n    return clientProto.getDataEncryptionKey();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "createSnapshot",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String createSnapshot(String snapshotRoot, String snapshotName) throws IOException\n{\r\n    return clientProto.createSnapshot(snapshotRoot, snapshotName);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "deleteSnapshot",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void deleteSnapshot(String snapshotRoot, String snapshotName) throws IOException\n{\r\n    clientProto.deleteSnapshot(snapshotRoot, snapshotName);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "setQuota",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setQuota(String path, long namespaceQuota, long storagespaceQuota, StorageType type) throws IOException\n{\r\n    clientProto.setQuota(path, namespaceQuota, storagespaceQuota, type);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "getQuotaUsage",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "QuotaUsage getQuotaUsage(String path) throws IOException\n{\r\n    return clientProto.getQuotaUsage(path);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "reportBadBlocks",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void reportBadBlocks(LocatedBlock[] blocks) throws IOException\n{\r\n    clientProto.reportBadBlocks(blocks);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "unsetStoragePolicy",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void unsetStoragePolicy(String src) throws IOException\n{\r\n    clientProto.unsetStoragePolicy(src);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "getStoragePolicy",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "BlockStoragePolicy getStoragePolicy(String path) throws IOException\n{\r\n    return clientProto.getStoragePolicy(path);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "getErasureCodingPolicies",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "ErasureCodingPolicyInfo[] getErasureCodingPolicies() throws IOException\n{\r\n    return clientProto.getErasureCodingPolicies();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "getErasureCodingCodecs",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Map<String, String> getErasureCodingCodecs() throws IOException\n{\r\n    return clientProto.getErasureCodingCodecs();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "addErasureCodingPolicies",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "AddErasureCodingPolicyResponse[] addErasureCodingPolicies(ErasureCodingPolicy[] policies) throws IOException\n{\r\n    return clientProto.addErasureCodingPolicies(policies);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "removeErasureCodingPolicy",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void removeErasureCodingPolicy(String ecPolicyName) throws IOException\n{\r\n    clientProto.removeErasureCodingPolicy(ecPolicyName);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "disableErasureCodingPolicy",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void disableErasureCodingPolicy(String ecPolicyName) throws IOException\n{\r\n    clientProto.disableErasureCodingPolicy(ecPolicyName);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "enableErasureCodingPolicy",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void enableErasureCodingPolicy(String ecPolicyName) throws IOException\n{\r\n    clientProto.enableErasureCodingPolicy(ecPolicyName);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "getErasureCodingPolicy",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "ErasureCodingPolicy getErasureCodingPolicy(String src) throws IOException\n{\r\n    return clientProto.getErasureCodingPolicy(src);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "setErasureCodingPolicy",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setErasureCodingPolicy(String src, String ecPolicyName) throws IOException\n{\r\n    clientProto.setErasureCodingPolicy(src, ecPolicyName);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "unsetErasureCodingPolicy",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void unsetErasureCodingPolicy(String src) throws IOException\n{\r\n    clientProto.unsetErasureCodingPolicy(src);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "getECTopologyResultForPolicies",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "ECTopologyVerifierResult getECTopologyResultForPolicies(String... policyNames) throws IOException\n{\r\n    return clientProto.getECTopologyResultForPolicies(policyNames);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "getECBlockGroupStats",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "ECBlockGroupStats getECBlockGroupStats() throws IOException\n{\r\n    return clientProto.getECBlockGroupStats();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "getReplicatedBlockStats",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "ReplicatedBlockStats getReplicatedBlockStats() throws IOException\n{\r\n    return clientProto.getReplicatedBlockStats();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "listOpenFiles",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "BatchedEntries<OpenFileEntry> listOpenFiles(long prevId) throws IOException\n{\r\n    return clientProto.listOpenFiles(prevId);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "getHAServiceState",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "HAServiceProtocol.HAServiceState getHAServiceState() throws IOException\n{\r\n    return clientProto.getHAServiceState();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "listOpenFiles",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "BatchedEntries<OpenFileEntry> listOpenFiles(long prevId, EnumSet<OpenFilesType> openFilesTypes, String path) throws IOException\n{\r\n    return clientProto.listOpenFiles(prevId, openFilesTypes, path);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "msync",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void msync() throws IOException\n{\r\n    clientProto.msync();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "satisfyStoragePolicy",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void satisfyStoragePolicy(String path) throws IOException\n{\r\n    clientProto.satisfyStoragePolicy(path);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "getBlocks",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "BlocksWithLocations getBlocks(DatanodeInfo datanode, long size, long minBlockSize, long hotBlockTimeInterval) throws IOException\n{\r\n    return nnProto.getBlocks(datanode, size, minBlockSize, hotBlockTimeInterval);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "getBlockKeys",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "ExportedBlockKeys getBlockKeys() throws IOException\n{\r\n    return nnProto.getBlockKeys();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "getTransactionID",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "long getTransactionID() throws IOException\n{\r\n    return nnProto.getTransactionID();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "getMostRecentCheckpointTxId",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "long getMostRecentCheckpointTxId() throws IOException\n{\r\n    return nnProto.getMostRecentCheckpointTxId();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "rollEditLog",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "CheckpointSignature rollEditLog() throws IOException\n{\r\n    return nnProto.rollEditLog();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "versionRequest",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "NamespaceInfo versionRequest() throws IOException\n{\r\n    return nnProto.versionRequest();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "errorReport",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void errorReport(NamenodeRegistration registration, int errorCode, String msg) throws IOException\n{\r\n    nnProto.errorReport(registration, errorCode, msg);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "registerSubordinateNamenode",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "NamenodeRegistration registerSubordinateNamenode(NamenodeRegistration registration) throws IOException\n{\r\n    return nnProto.registerSubordinateNamenode(registration);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "startCheckpoint",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "NamenodeCommand startCheckpoint(NamenodeRegistration registration) throws IOException\n{\r\n    return nnProto.startCheckpoint(registration);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "endCheckpoint",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void endCheckpoint(NamenodeRegistration registration, CheckpointSignature sig) throws IOException\n{\r\n    nnProto.endCheckpoint(registration, sig);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "getEditLogManifest",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "RemoteEditLogManifest getEditLogManifest(long sinceTxId) throws IOException\n{\r\n    return nnProto.getEditLogManifest(sinceTxId);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "isUpgradeFinalized",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "boolean isUpgradeFinalized() throws IOException\n{\r\n    return nnProto.isUpgradeFinalized();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "isRollingUpgrade",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "boolean isRollingUpgrade() throws IOException\n{\r\n    return nnProto.isRollingUpgrade();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "getNextSPSPath",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Long getNextSPSPath() throws IOException\n{\r\n    return nnProto.getNextSPSPath();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "getLocationForPath",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "RemoteLocation getLocationForPath(String path, boolean failIfLocked, String blockPoolId) throws IOException\n{\r\n    final List<RemoteLocation> locations = getLocationsForPath(path, failIfLocked);\r\n    String nameserviceId = null;\r\n    Set<FederationNamespaceInfo> namespaces = this.namenodeResolver.getNamespaces();\r\n    for (FederationNamespaceInfo namespace : namespaces) {\r\n        if (namespace.getBlockPoolId().equals(blockPoolId)) {\r\n            nameserviceId = namespace.getNameserviceId();\r\n            break;\r\n        }\r\n    }\r\n    if (nameserviceId != null) {\r\n        for (RemoteLocation location : locations) {\r\n            if (location.getNameserviceId().equals(nameserviceId)) {\r\n                return location;\r\n            }\r\n        }\r\n    }\r\n    throw new IOException(\"Cannot locate a nameservice for block pool \" + blockPoolId);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "getLocationsForPath",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "List<RemoteLocation> getLocationsForPath(String path, boolean failIfLocked) throws IOException\n{\r\n    return getLocationsForPath(path, failIfLocked, true);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "getLocationsForPath",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 22,
  "sourceCodeText" : "List<RemoteLocation> getLocationsForPath(String path, boolean failIfLocked, boolean needQuotaVerify) throws IOException\n{\r\n    try {\r\n        if (failIfLocked) {\r\n            final List<String> mountPoints = this.subclusterResolver.getMountPoints(path);\r\n            if (mountPoints != null) {\r\n                StringBuilder sb = new StringBuilder();\r\n                sb.append(\"The operation is not allowed because \");\r\n                if (mountPoints.isEmpty()) {\r\n                    sb.append(\"the path: \").append(path).append(\" is a mount point\");\r\n                } else {\r\n                    sb.append(\"there are mount points: \").append(String.join(\",\", mountPoints)).append(\" under the path: \").append(path);\r\n                }\r\n                throw new AccessControlException(sb.toString());\r\n            }\r\n        }\r\n        final PathLocation location = this.subclusterResolver.getDestinationForPath(path);\r\n        if (location == null) {\r\n            throw new IOException(\"Cannot find locations for \" + path + \" in \" + this.subclusterResolver.getClass().getSimpleName());\r\n        }\r\n        if (opCategory.get() == OperationCategory.WRITE) {\r\n            if (isPathReadOnly(path)) {\r\n                if (this.rpcMonitor != null) {\r\n                    this.rpcMonitor.routerFailureReadOnly();\r\n                }\r\n                throw new IOException(path + \" is in a read only mount point\");\r\n            }\r\n            if (this.router.isQuotaEnabled() && needQuotaVerify) {\r\n                RouterQuotaUsage quotaUsage = this.router.getQuotaManager().getQuotaUsage(path);\r\n                if (quotaUsage != null) {\r\n                    quotaUsage.verifyNamespaceQuota();\r\n                    quotaUsage.verifyStoragespaceQuota();\r\n                    quotaUsage.verifyQuotaByStorageType();\r\n                }\r\n            }\r\n        }\r\n        Set<String> disabled = namenodeResolver.getDisabledNamespaces();\r\n        List<RemoteLocation> locs = new ArrayList<>();\r\n        for (RemoteLocation loc : location.getDestinations()) {\r\n            if (!disabled.contains(loc.getNameserviceId())) {\r\n                locs.add(loc);\r\n            }\r\n        }\r\n        return locs;\r\n    } catch (IOException ioe) {\r\n        if (this.rpcMonitor != null) {\r\n            this.rpcMonitor.routerFailureStateStore();\r\n        }\r\n        if (ioe instanceof StateStoreUnavailableException) {\r\n            checkSafeMode();\r\n        }\r\n        throw ioe;\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "isPathReadOnly",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 3,
  "sourceCodeText" : "boolean isPathReadOnly(final String path)\n{\r\n    if (subclusterResolver instanceof MountTableResolver) {\r\n        try {\r\n            MountTableResolver mountTable = (MountTableResolver) subclusterResolver;\r\n            MountTable entry = mountTable.getMountPoint(path);\r\n            if (entry != null && entry.isReadOnly()) {\r\n                return true;\r\n            }\r\n        } catch (IOException e) {\r\n            LOG.error(\"Cannot get mount point\", e);\r\n        }\r\n    }\r\n    return false;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "getRemoteUser",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "UserGroupInformation getRemoteUser() throws IOException\n{\r\n    UserGroupInformation ugi = CUR_USER.get();\r\n    ugi = (ugi != null) ? ugi : Server.getRemoteUser();\r\n    return (ugi != null) ? ugi : UserGroupInformation.getCurrentUser();\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "setCurrentUser",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setCurrentUser(UserGroupInformation ugi)\n{\r\n    CUR_USER.set(ugi);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "resetCurrentUser",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void resetCurrentUser()\n{\r\n    CUR_USER.set(null);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "merge",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "T[] merge(Map<FederationNamespaceInfo, T[]> map, Class<T> clazz)\n{\r\n    Set<T> ret = new LinkedHashSet<>();\r\n    for (T[] values : map.values()) {\r\n        if (values != null) {\r\n            for (T val : values) {\r\n                ret.add(val);\r\n            }\r\n        }\r\n    }\r\n    return toArray(ret, clazz);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "toArray",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "T[] toArray(Collection<T> set, Class<T> clazz)\n{\r\n    @SuppressWarnings(\"unchecked\")\r\n    T[] combinedData = (T[]) Array.newInstance(clazz, set.size());\r\n    combinedData = set.toArray(combinedData);\r\n    return combinedData;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "getQuotaModule",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Quota getQuotaModule()\n{\r\n    return this.quotaCall;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "getClientProtocolModule",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "RouterClientProtocol getClientProtocolModule()\n{\r\n    return this.clientProto;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "getRPCMetrics",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "FederationRPCMetrics getRPCMetrics()\n{\r\n    return this.rpcMonitor.getRPCMetrics();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "isPathAll",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 3,
  "sourceCodeText" : "boolean isPathAll(final String path)\n{\r\n    if (subclusterResolver instanceof MountTableResolver) {\r\n        try {\r\n            MountTableResolver mountTable = (MountTableResolver) subclusterResolver;\r\n            MountTable entry = mountTable.getMountPoint(path);\r\n            if (entry != null) {\r\n                return entry.isAll();\r\n            }\r\n        } catch (IOException e) {\r\n            LOG.error(\"Cannot get mount point\", e);\r\n        }\r\n    }\r\n    return false;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "isPathFaultTolerant",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 3,
  "sourceCodeText" : "boolean isPathFaultTolerant(final String path)\n{\r\n    if (subclusterResolver instanceof MountTableResolver) {\r\n        try {\r\n            MountTableResolver mountTable = (MountTableResolver) subclusterResolver;\r\n            MountTable entry = mountTable.getMountPoint(path);\r\n            if (entry != null) {\r\n                return entry.isFaultTolerant();\r\n            }\r\n        } catch (IOException e) {\r\n            LOG.error(\"Cannot get mount point\", e);\r\n        }\r\n    }\r\n    return false;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "isInvokeConcurrent",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "boolean isInvokeConcurrent(final String path) throws IOException\n{\r\n    if (subclusterResolver instanceof MountTableResolver) {\r\n        MountTableResolver mountTableResolver = (MountTableResolver) subclusterResolver;\r\n        List<String> mountPoints = mountTableResolver.getMountPoints(path);\r\n        if (mountPoints != null) {\r\n            return true;\r\n        }\r\n        return isPathAll(path);\r\n    }\r\n    return false;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "refreshUserToGroupsMappings",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void refreshUserToGroupsMappings() throws IOException\n{\r\n    routerProto.refreshUserToGroupsMappings();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "refreshSuperUserGroupsConfiguration",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void refreshSuperUserGroupsConfiguration() throws IOException\n{\r\n    routerProto.refreshSuperUserGroupsConfiguration();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "getGroupsForUser",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String[] getGroupsForUser(String user) throws IOException\n{\r\n    return routerProto.getGroupsForUser(user);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "getRouterFederationRenameCount",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "int getRouterFederationRenameCount()\n{\r\n    return clientProto.getRouterFederationRenameCount();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "getSchedulerJobCount",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "int getSchedulerJobCount()\n{\r\n    if (fedRenameScheduler == null) {\r\n        return 0;\r\n    }\r\n    return fedRenameScheduler.getAllJobs().size();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store",
  "methodName" : "namenodeHeartbeat",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "NamenodeHeartbeatResponse namenodeHeartbeat(NamenodeHeartbeatRequest request) throws IOException",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store",
  "methodName" : "getNamenodeRegistrations",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "GetNamenodeRegistrationsResponse getNamenodeRegistrations(GetNamenodeRegistrationsRequest request) throws IOException",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store",
  "methodName" : "getExpiredNamenodeRegistrations",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "GetNamenodeRegistrationsResponse getExpiredNamenodeRegistrations(GetNamenodeRegistrationsRequest request) throws IOException",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store",
  "methodName" : "getNamespaceInfo",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "GetNamespaceInfoResponse getNamespaceInfo(GetNamespaceInfoRequest request) throws IOException",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store",
  "methodName" : "updateNamenodeRegistration",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "UpdateNamenodeRegistrationResponse updateNamenodeRegistration(UpdateNamenodeRegistrationRequest request) throws IOException",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\protocol",
  "methodName" : "newInstance",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "RouterHeartbeatRequest newInstance() throws IOException\n{\r\n    return StateStoreSerializer.newRecord(RouterHeartbeatRequest.class);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\protocol",
  "methodName" : "newInstance",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "RouterHeartbeatRequest newInstance(RouterState router) throws IOException\n{\r\n    RouterHeartbeatRequest request = newInstance();\r\n    request.setRouter(router);\r\n    return request;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\protocol",
  "methodName" : "getRouter",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "RouterState getRouter() throws IOException",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\protocol",
  "methodName" : "setRouter",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setRouter(RouterState routerState)",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "init",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void init(final UserGroupInformation ugi, final DelegationParam delegation, final UserParam username, final DoAsParam doAsUser, final UriFsPathParam path, final HttpOpParam<?> op, final Param<?, ?>... parameters)\n{\r\n    super.init(ugi, delegation, username, doAsUser, path, op, parameters);\r\n    remoteAddr = JspHelper.getRemoteAddr(request);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "getRpcClientProtocol",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "ClientProtocol getRpcClientProtocol() throws IOException\n{\r\n    final Router router = getRouter();\r\n    final RouterRpcServer routerRpcServer = router.getRpcServer();\r\n    if (routerRpcServer == null) {\r\n        throw new RetriableException(\"Router is in startup mode\");\r\n    }\r\n    return routerRpcServer;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "reset",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void reset()\n{\r\n    remoteAddr = null;\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "getRemoteAddr",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getRemoteAddr()\n{\r\n    return remoteAddr;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "queueExternalCall",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void queueExternalCall(ExternalCall call) throws IOException, InterruptedException\n{\r\n    getRouter().getRpcServer().getServer().queueCall(call);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "getRouter",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Router getRouter()\n{\r\n    return (Router) getContext().getAttribute(\"name.node\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "getRPCServer",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "RouterRpcServer getRPCServer(final Router router) throws IOException\n{\r\n    final RouterRpcServer routerRpcServer = router.getRpcServer();\r\n    if (routerRpcServer == null) {\r\n        throw new RetriableException(\"Router is in startup mode\");\r\n    }\r\n    return routerRpcServer;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "put",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "Response put(final UserGroupInformation ugi, final DelegationParam delegation, final UserParam username, final DoAsParam doAsUser, final String fullpath, final PutOpParam op, final DestinationParam destination, final OwnerParam owner, final GroupParam group, final PermissionParam permission, final UnmaskedPermissionParam unmaskedPermission, final OverwriteParam overwrite, final BufferSizeParam bufferSize, final ReplicationParam replication, final BlockSizeParam blockSize, final ModificationTimeParam modificationTime, final AccessTimeParam accessTime, final RenameOptionSetParam renameOptions, final CreateParentParam createParent, final TokenArgumentParam delegationTokenArgument, final AclPermissionParam aclPermission, final XAttrNameParam xattrName, final XAttrValueParam xattrValue, final XAttrSetFlagParam xattrSetFlag, final SnapshotNameParam snapshotName, final OldSnapshotNameParam oldSnapshotName, final ExcludeDatanodesParam exclDatanodes, final CreateFlagParam createFlagParam, final NoRedirectParam noredirectParam, final StoragePolicyParam policyName, final ECPolicyParam ecpolicy, final NameSpaceQuotaParam namespaceQuota, final StorageSpaceQuotaParam storagespaceQuota, final StorageTypeParam storageType) throws IOException, URISyntaxException\n{\r\n    switch(op.getValue()) {\r\n        case CREATE:\r\n            {\r\n                final Router router = getRouter();\r\n                final URI uri = redirectURI(router, ugi, delegation, username, doAsUser, fullpath, op.getValue(), -1L, exclDatanodes.getValue(), permission, unmaskedPermission, overwrite, bufferSize, replication, blockSize, createParent, createFlagParam);\r\n                if (!noredirectParam.getValue()) {\r\n                    return Response.temporaryRedirect(uri).type(MediaType.APPLICATION_OCTET_STREAM).build();\r\n                } else {\r\n                    final String js = JsonUtil.toJsonString(\"Location\", uri);\r\n                    return Response.ok(js).type(MediaType.APPLICATION_JSON).build();\r\n                }\r\n            }\r\n        case MKDIRS:\r\n        case CREATESYMLINK:\r\n        case RENAME:\r\n        case SETREPLICATION:\r\n        case SETOWNER:\r\n        case SETPERMISSION:\r\n        case SETTIMES:\r\n        case RENEWDELEGATIONTOKEN:\r\n        case CANCELDELEGATIONTOKEN:\r\n        case MODIFYACLENTRIES:\r\n        case REMOVEACLENTRIES:\r\n        case REMOVEDEFAULTACL:\r\n        case REMOVEACL:\r\n        case SETACL:\r\n        case SETXATTR:\r\n        case REMOVEXATTR:\r\n        case ALLOWSNAPSHOT:\r\n        case CREATESNAPSHOT:\r\n        case RENAMESNAPSHOT:\r\n        case DISALLOWSNAPSHOT:\r\n        case SETSTORAGEPOLICY:\r\n        case ENABLEECPOLICY:\r\n        case DISABLEECPOLICY:\r\n        case SATISFYSTORAGEPOLICY:\r\n            {\r\n                return super.put(ugi, delegation, username, doAsUser, fullpath, op, destination, owner, group, permission, unmaskedPermission, overwrite, bufferSize, replication, blockSize, modificationTime, accessTime, renameOptions, createParent, delegationTokenArgument, aclPermission, xattrName, xattrValue, xattrSetFlag, snapshotName, oldSnapshotName, exclDatanodes, createFlagParam, noredirectParam, policyName, ecpolicy, namespaceQuota, storagespaceQuota, storageType);\r\n            }\r\n        default:\r\n            throw new UnsupportedOperationException(op + \" is not supported\");\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "post",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "Response post(final UserGroupInformation ugi, final DelegationParam delegation, final UserParam username, final DoAsParam doAsUser, final String fullpath, final PostOpParam op, final ConcatSourcesParam concatSrcs, final BufferSizeParam bufferSize, final ExcludeDatanodesParam excludeDatanodes, final NewLengthParam newLength, final NoRedirectParam noRedirectParam) throws IOException, URISyntaxException\n{\r\n    switch(op.getValue()) {\r\n        case APPEND:\r\n            {\r\n                final Router router = getRouter();\r\n                final URI uri = redirectURI(router, ugi, delegation, username, doAsUser, fullpath, op.getValue(), -1L, excludeDatanodes.getValue(), bufferSize);\r\n                if (!noRedirectParam.getValue()) {\r\n                    return Response.temporaryRedirect(uri).type(MediaType.APPLICATION_OCTET_STREAM).build();\r\n                } else {\r\n                    final String js = JsonUtil.toJsonString(\"Location\", uri);\r\n                    return Response.ok(js).type(MediaType.APPLICATION_JSON).build();\r\n                }\r\n            }\r\n        case CONCAT:\r\n        case TRUNCATE:\r\n        case UNSETSTORAGEPOLICY:\r\n            {\r\n                return super.post(ugi, delegation, username, doAsUser, fullpath, op, concatSrcs, bufferSize, excludeDatanodes, newLength, noRedirectParam);\r\n            }\r\n        default:\r\n            throw new UnsupportedOperationException(op + \" is not supported\");\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "get",
  "errType" : null,
  "containingMethodsNum" : 14,
  "sourceCodeText" : "Response get(final UserGroupInformation ugi, final DelegationParam delegation, final UserParam username, final DoAsParam doAsUser, final String fullpath, final GetOpParam op, final OffsetParam offset, final LengthParam length, final RenewerParam renewer, final BufferSizeParam bufferSize, final List<XAttrNameParam> xattrNames, final XAttrEncodingParam xattrEncoding, final ExcludeDatanodesParam excludeDatanodes, final FsActionParam fsAction, final SnapshotNameParam snapshotName, final OldSnapshotNameParam oldSnapshotName, final SnapshotDiffStartPathParam snapshotDiffStartPath, final SnapshotDiffIndexParam snapshotDiffIndex, final TokenKindParam tokenKind, final TokenServiceParam tokenService, final NoRedirectParam noredirectParam, final StartAfterParam startAfter) throws IOException, URISyntaxException\n{\r\n    try {\r\n        final Router router = getRouter();\r\n        switch(op.getValue()) {\r\n            case OPEN:\r\n                {\r\n                    final URI uri = redirectURI(router, ugi, delegation, username, doAsUser, fullpath, op.getValue(), offset.getValue(), excludeDatanodes.getValue(), offset, length, bufferSize);\r\n                    if (!noredirectParam.getValue()) {\r\n                        return Response.temporaryRedirect(uri).type(MediaType.APPLICATION_OCTET_STREAM).build();\r\n                    } else {\r\n                        final String js = JsonUtil.toJsonString(\"Location\", uri);\r\n                        return Response.ok(js).type(MediaType.APPLICATION_JSON).build();\r\n                    }\r\n                }\r\n            case GETFILECHECKSUM:\r\n                {\r\n                    final URI uri = redirectURI(router, ugi, delegation, username, doAsUser, fullpath, op.getValue(), -1L, null);\r\n                    if (!noredirectParam.getValue()) {\r\n                        return Response.temporaryRedirect(uri).type(MediaType.APPLICATION_OCTET_STREAM).build();\r\n                    } else {\r\n                        final String js = JsonUtil.toJsonString(\"Location\", uri);\r\n                        return Response.ok(js).type(MediaType.APPLICATION_JSON).build();\r\n                    }\r\n                }\r\n            case GETDELEGATIONTOKEN:\r\n            case GET_BLOCK_LOCATIONS:\r\n            case GETFILESTATUS:\r\n            case LISTSTATUS:\r\n            case GETCONTENTSUMMARY:\r\n            case GETHOMEDIRECTORY:\r\n            case GETACLSTATUS:\r\n            case GETXATTRS:\r\n            case LISTXATTRS:\r\n            case CHECKACCESS:\r\n                {\r\n                    return super.get(ugi, delegation, username, doAsUser, fullpath, op, offset, length, renewer, bufferSize, xattrNames, xattrEncoding, excludeDatanodes, fsAction, snapshotName, oldSnapshotName, snapshotDiffStartPath, snapshotDiffIndex, tokenKind, tokenService, noredirectParam, startAfter);\r\n                }\r\n            default:\r\n                throw new UnsupportedOperationException(op + \" is not supported\");\r\n        }\r\n    } finally {\r\n        reset();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "redirectURI",
  "errType" : null,
  "containingMethodsNum" : 17,
  "sourceCodeText" : "URI redirectURI(final Router router, final UserGroupInformation ugi, final DelegationParam delegation, final UserParam username, final DoAsParam doAsUser, final String path, final HttpOpParam.Op op, final long openOffset, final String excludeDatanodes, final Param<?, ?>... parameters) throws URISyntaxException, IOException\n{\r\n    if (!DFSUtil.isValidName(path)) {\r\n        throw new InvalidPathException(path);\r\n    }\r\n    final DatanodeInfo dn = chooseDatanode(router, path, op, openOffset, excludeDatanodes);\r\n    if (dn == null) {\r\n        throw new IOException(\"Failed to find datanode, suggest to check cluster\" + \" health. excludeDatanodes=\" + excludeDatanodes);\r\n    }\r\n    final String delegationQuery;\r\n    if (!UserGroupInformation.isSecurityEnabled()) {\r\n        delegationQuery = Param.toSortedString(\"&\", doAsUser, username);\r\n    } else if (delegation.getValue() != null) {\r\n        delegationQuery = \"&\" + delegation;\r\n    } else {\r\n        final Token<? extends TokenIdentifier> t = generateDelegationToken(ugi, ugi.getUserName());\r\n        delegationQuery = \"&delegation=\" + t.encodeToUrlString();\r\n    }\r\n    final String redirectQuery = op.toQueryString() + delegationQuery + \"&namenoderpcaddress=\" + router.getRouterId() + Param.toSortedString(\"&\", parameters);\r\n    final String uripath = WebHdfsFileSystem.PATH_PREFIX + path;\r\n    int port = \"http\".equals(getScheme()) ? dn.getInfoPort() : dn.getInfoSecurePort();\r\n    final URI uri = new URI(getScheme(), null, dn.getHostName(), port, uripath, redirectQuery, null);\r\n    if (LOG.isTraceEnabled()) {\r\n        LOG.trace(\"redirectURI={}\", uri);\r\n    }\r\n    return uri;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "chooseDatanode",
  "errType" : [ "IOException", "IOException" ],
  "containingMethodsNum" : 19,
  "sourceCodeText" : "DatanodeInfo chooseDatanode(final Router router, final String path, final HttpOpParam.Op op, final long openOffset, final String excludeDatanodes) throws IOException\n{\r\n    final RouterRpcServer rpcServer = getRPCServer(router);\r\n    DatanodeInfo[] dns = {};\r\n    String resolvedNs = \"\";\r\n    try {\r\n        dns = rpcServer.getCachedDatanodeReport(DatanodeReportType.LIVE);\r\n    } catch (IOException e) {\r\n        LOG.error(\"Cannot get the datanodes from the RPC server\", e);\r\n    }\r\n    if (op == PutOpParam.Op.CREATE) {\r\n        try {\r\n            resolvedNs = rpcServer.getCreateLocation(path).getNameserviceId();\r\n        } catch (IOException e) {\r\n            LOG.error(\"Cannot get the name service \" + \"to create file for path {} \", path, e);\r\n        }\r\n    }\r\n    HashSet<Node> excludes = new HashSet<Node>();\r\n    Collection<String> collection = getTrimmedStringCollection(excludeDatanodes);\r\n    for (DatanodeInfo dn : dns) {\r\n        String ns = getNsFromDataNodeNetworkLocation(dn.getNetworkLocation());\r\n        if (collection.contains(dn.getName())) {\r\n            excludes.add(dn);\r\n        } else if (op == PutOpParam.Op.CREATE && !ns.equals(resolvedNs)) {\r\n            excludes.add(dn);\r\n        }\r\n    }\r\n    if (op == GetOpParam.Op.OPEN || op == PostOpParam.Op.APPEND || op == GetOpParam.Op.GETFILECHECKSUM) {\r\n        final ClientProtocol cp = getRpcClientProtocol();\r\n        final HdfsFileStatus status = cp.getFileInfo(path);\r\n        if (status == null) {\r\n            throw new FileNotFoundException(\"File \" + path + \" not found.\");\r\n        }\r\n        final long len = status.getLen();\r\n        if (op == GetOpParam.Op.OPEN) {\r\n            if (openOffset < 0L || (openOffset >= len && len > 0)) {\r\n                throw new IOException(\"Offset=\" + openOffset + \" out of the range [0, \" + len + \"); \" + op + \", path=\" + path);\r\n            }\r\n        }\r\n        if (len > 0) {\r\n            final long offset = op == GetOpParam.Op.OPEN ? openOffset : len - 1;\r\n            final LocatedBlocks locations = cp.getBlockLocations(path, offset, 1);\r\n            final int count = locations.locatedBlockCount();\r\n            if (count > 0) {\r\n                LocatedBlock location0 = locations.get(0);\r\n                return bestNode(location0.getLocations(), excludes);\r\n            }\r\n        }\r\n    }\r\n    return getRandomDatanode(dns, excludes);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 2,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "getNsFromDataNodeNetworkLocation",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "String getNsFromDataNodeNetworkLocation(String location)\n{\r\n    Pattern pattern = Pattern.compile(\"^/([^/]*)/\");\r\n    Matcher matcher = pattern.matcher(location);\r\n    if (matcher.find()) {\r\n        return matcher.group(1);\r\n    }\r\n    return \"\";\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "getRandomDatanode",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "DatanodeInfo getRandomDatanode(final DatanodeInfo[] dns, final HashSet<Node> excludes)\n{\r\n    DatanodeInfo dn = null;\r\n    if (dns == null) {\r\n        return dn;\r\n    }\r\n    int numDNs = dns.length;\r\n    int availableNodes = 0;\r\n    if (excludes.isEmpty()) {\r\n        availableNodes = numDNs;\r\n    } else {\r\n        for (DatanodeInfo di : dns) {\r\n            if (!excludes.contains(di)) {\r\n                availableNodes++;\r\n            }\r\n        }\r\n    }\r\n    if (availableNodes > 0) {\r\n        while (dn == null || excludes.contains(dn)) {\r\n            Random rnd = new Random();\r\n            int idx = rnd.nextInt(numDNs);\r\n            dn = dns[idx];\r\n        }\r\n    }\r\n    return dn;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "createCredentials",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "Credentials createCredentials(final UserGroupInformation ugi, final String renewer) throws IOException\n{\r\n    final Router router = (Router) getContext().getAttribute(\"name.node\");\r\n    final Credentials c = RouterSecurityManager.createCredentials(router, ugi, renewer != null ? renewer : ugi.getShortUserName());\r\n    return c;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\resolver",
  "methodName" : "compare",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "int compare(FederationNamenodeContext o1, FederationNamenodeContext o2)\n{\r\n    FederationNamenodeServiceState state1 = o1.getState();\r\n    FederationNamenodeServiceState state2 = o2.getState();\r\n    if (state1 == state2) {\r\n        return compareModDates(o1, o2);\r\n    } else {\r\n        return state1.compareTo(state2);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\resolver",
  "methodName" : "compareModDates",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "int compareModDates(FederationNamenodeContext o1, FederationNamenodeContext o2)\n{\r\n    return (int) (o2.getDateModified() - o1.getDateModified());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "isActive",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "boolean isActive()\n{\r\n    final ServletContext context = getServletContext();\r\n    final Router router = RouterHttpServer.getRouterFromContext(context);\r\n    final RouterServiceState routerState = router.getRouterState();\r\n    return routerState == RouterServiceState.RUNNING;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\driver\\impl",
  "methodName" : "getReader",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "BufferedReader getReader(String path)",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\driver\\impl",
  "methodName" : "getWriter",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "BufferedWriter getWriter(String path)",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\driver\\impl",
  "methodName" : "exists",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean exists(String path)",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\driver\\impl",
  "methodName" : "mkdir",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean mkdir(String path)",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\driver\\impl",
  "methodName" : "rename",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean rename(String src, String dst)",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\driver\\impl",
  "methodName" : "remove",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean remove(String path)",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\driver\\impl",
  "methodName" : "getChildren",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "List<String> getChildren(String path)",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\driver\\impl",
  "methodName" : "getRootDir",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getRootDir()",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\driver\\impl",
  "methodName" : "setInitialized",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setInitialized(boolean ini)\n{\r\n    this.initialized = ini;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\driver\\impl",
  "methodName" : "initDriver",
  "errType" : [ "Exception" ],
  "containingMethodsNum" : 7,
  "sourceCodeText" : "boolean initDriver()\n{\r\n    String rootDir = getRootDir();\r\n    try {\r\n        if (rootDir == null) {\r\n            LOG.error(\"Invalid root directory, unable to initialize driver.\");\r\n            return false;\r\n        }\r\n        if (!exists(rootDir)) {\r\n            if (!mkdir(rootDir)) {\r\n                LOG.error(\"Cannot create State Store root directory {}\", rootDir);\r\n                return false;\r\n            }\r\n        }\r\n    } catch (Exception ex) {\r\n        LOG.error(\"Cannot initialize filesystem using root directory {}\", rootDir, ex);\r\n        return false;\r\n    }\r\n    setInitialized(true);\r\n    return true;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\driver\\impl",
  "methodName" : "initRecordStorage",
  "errType" : [ "Exception" ],
  "containingMethodsNum" : 6,
  "sourceCodeText" : "boolean initRecordStorage(String className, Class<T> recordClass)\n{\r\n    String dataDirPath = getRootDir() + \"/\" + className;\r\n    try {\r\n        if (!exists(dataDirPath)) {\r\n            LOG.info(\"{} data directory doesn't exist, creating it\", dataDirPath);\r\n            if (!mkdir(dataDirPath)) {\r\n                LOG.error(\"Cannot create data directory {}\", dataDirPath);\r\n                return false;\r\n            }\r\n        }\r\n    } catch (Exception ex) {\r\n        LOG.error(\"Cannot create data directory {}\", dataDirPath, ex);\r\n        return false;\r\n    }\r\n    return true;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\driver\\impl",
  "methodName" : "get",
  "errType" : [ "Exception" ],
  "containingMethodsNum" : 17,
  "sourceCodeText" : "QueryResult<T> get(Class<T> clazz) throws IOException\n{\r\n    verifyDriverReady();\r\n    long start = monotonicNow();\r\n    StateStoreMetrics metrics = getMetrics();\r\n    List<T> ret = new ArrayList<>();\r\n    try {\r\n        String path = getPathForClass(clazz);\r\n        List<String> children = getChildren(path);\r\n        for (String child : children) {\r\n            String pathRecord = path + \"/\" + child;\r\n            if (child.endsWith(TMP_MARK)) {\r\n                LOG.debug(\"There is a temporary file {} in {}\", child, path);\r\n                if (isOldTempRecord(child)) {\r\n                    LOG.warn(\"Removing {} as it's an old temporary record\", child);\r\n                    remove(pathRecord);\r\n                }\r\n            } else {\r\n                T record = getRecord(pathRecord, clazz);\r\n                ret.add(record);\r\n            }\r\n        }\r\n    } catch (Exception e) {\r\n        if (metrics != null) {\r\n            metrics.addFailure(monotonicNow() - start);\r\n        }\r\n        String msg = \"Cannot fetch records for \" + clazz.getSimpleName();\r\n        LOG.error(msg, e);\r\n        throw new IOException(msg, e);\r\n    }\r\n    if (metrics != null) {\r\n        metrics.addRead(monotonicNow() - start);\r\n    }\r\n    return new QueryResult<T>(ret, getTime());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\driver\\impl",
  "methodName" : "isOldTempRecord",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "boolean isOldTempRecord(final String pathRecord)\n{\r\n    if (!pathRecord.endsWith(TMP_MARK)) {\r\n        return false;\r\n    }\r\n    Matcher m = OLD_TMP_RECORD_PATTERN.matcher(pathRecord);\r\n    if (m.find()) {\r\n        long time = Long.parseLong(m.group(1));\r\n        return now() - time > OLD_TMP_RECORD_MS;\r\n    }\r\n    return false;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\driver\\impl",
  "methodName" : "getRecord",
  "errType" : [ "Exception" ],
  "containingMethodsNum" : 8,
  "sourceCodeText" : "T getRecord(final String path, final Class<T> clazz) throws IOException\n{\r\n    BufferedReader reader = getReader(path);\r\n    try {\r\n        String line;\r\n        while ((line = reader.readLine()) != null) {\r\n            if (!line.startsWith(\"#\") && line.length() > 0) {\r\n                try {\r\n                    T record = newRecord(line, clazz, false);\r\n                    return record;\r\n                } catch (Exception ex) {\r\n                    LOG.error(\"Cannot parse line {} in file {}\", line, path, ex);\r\n                }\r\n            }\r\n        }\r\n    } finally {\r\n        if (reader != null) {\r\n            reader.close();\r\n        }\r\n    }\r\n    throw new IOException(\"Cannot read \" + path + \" for record \" + clazz.getSimpleName());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 2,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\driver\\impl",
  "methodName" : "getPathForClass",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "String getPathForClass(final Class<T> clazz)\n{\r\n    String className = StateStoreUtils.getRecordName(clazz);\r\n    StringBuilder sb = new StringBuilder();\r\n    sb.append(getRootDir());\r\n    if (sb.charAt(sb.length() - 1) != '/') {\r\n        sb.append(\"/\");\r\n    }\r\n    sb.append(className);\r\n    return sb.toString();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\driver\\impl",
  "methodName" : "isDriverReady",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean isDriverReady()\n{\r\n    return this.initialized;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\driver\\impl",
  "methodName" : "putAll",
  "errType" : [ "IOException", "IOException" ],
  "containingMethodsNum" : 29,
  "sourceCodeText" : "boolean putAll(List<T> records, boolean allowUpdate, boolean errorIfExists) throws StateStoreUnavailableException\n{\r\n    verifyDriverReady();\r\n    if (records.isEmpty()) {\r\n        return true;\r\n    }\r\n    long start = monotonicNow();\r\n    StateStoreMetrics metrics = getMetrics();\r\n    Map<String, T> toWrite = new HashMap<>();\r\n    for (T record : records) {\r\n        Class<? extends BaseRecord> recordClass = record.getClass();\r\n        String path = getPathForClass(recordClass);\r\n        String primaryKey = getPrimaryKey(record);\r\n        String recordPath = path + \"/\" + primaryKey;\r\n        if (exists(recordPath)) {\r\n            if (allowUpdate) {\r\n                record.setDateModified(this.getTime());\r\n                toWrite.put(recordPath, record);\r\n            } else if (errorIfExists) {\r\n                LOG.error(\"Attempt to insert record {} that already exists\", recordPath);\r\n                if (metrics != null) {\r\n                    metrics.addFailure(monotonicNow() - start);\r\n                }\r\n                return false;\r\n            } else {\r\n                LOG.debug(\"Not updating {}\", record);\r\n            }\r\n        } else {\r\n            toWrite.put(recordPath, record);\r\n        }\r\n    }\r\n    boolean success = true;\r\n    for (Entry<String, T> entry : toWrite.entrySet()) {\r\n        String recordPath = entry.getKey();\r\n        String recordPathTemp = recordPath + \".\" + now() + TMP_MARK;\r\n        BufferedWriter writer = getWriter(recordPathTemp);\r\n        try {\r\n            T record = entry.getValue();\r\n            String line = serializeString(record);\r\n            writer.write(line);\r\n        } catch (IOException e) {\r\n            LOG.error(\"Cannot write {}\", recordPathTemp, e);\r\n            success = false;\r\n        } finally {\r\n            if (writer != null) {\r\n                try {\r\n                    writer.close();\r\n                } catch (IOException e) {\r\n                    LOG.error(\"Cannot close the writer for {}\", recordPathTemp, e);\r\n                }\r\n            }\r\n        }\r\n        if (!rename(recordPathTemp, recordPath)) {\r\n            LOG.error(\"Failed committing record into {}\", recordPath);\r\n            success = false;\r\n        }\r\n    }\r\n    long end = monotonicNow();\r\n    if (metrics != null) {\r\n        if (success) {\r\n            metrics.addWrite(end - start);\r\n        } else {\r\n            metrics.addFailure(end - start);\r\n        }\r\n    }\r\n    return success;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 2,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\driver\\impl",
  "methodName" : "remove",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 15,
  "sourceCodeText" : "int remove(Class<T> clazz, Query<T> query) throws StateStoreUnavailableException\n{\r\n    verifyDriverReady();\r\n    if (query == null) {\r\n        return 0;\r\n    }\r\n    long start = Time.monotonicNow();\r\n    StateStoreMetrics metrics = getMetrics();\r\n    int removed = 0;\r\n    try {\r\n        final QueryResult<T> result = get(clazz);\r\n        final List<T> existingRecords = result.getRecords();\r\n        final List<T> recordsToRemove = filterMultiple(query, existingRecords);\r\n        boolean success = true;\r\n        for (T recordToRemove : recordsToRemove) {\r\n            String path = getPathForClass(clazz);\r\n            String primaryKey = getPrimaryKey(recordToRemove);\r\n            String recordToRemovePath = path + \"/\" + primaryKey;\r\n            if (remove(recordToRemovePath)) {\r\n                removed++;\r\n            } else {\r\n                LOG.error(\"Cannot remove record {}\", recordToRemovePath);\r\n                success = false;\r\n            }\r\n        }\r\n        if (!success) {\r\n            LOG.error(\"Cannot remove records {} query {}\", clazz, query);\r\n            if (metrics != null) {\r\n                metrics.addFailure(monotonicNow() - start);\r\n            }\r\n        }\r\n    } catch (IOException e) {\r\n        LOG.error(\"Cannot remove records {} query {}\", clazz, query, e);\r\n        if (metrics != null) {\r\n            metrics.addFailure(monotonicNow() - start);\r\n        }\r\n    }\r\n    if (removed > 0 && metrics != null) {\r\n        metrics.addRemove(monotonicNow() - start);\r\n    }\r\n    return removed;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\driver\\impl",
  "methodName" : "removeAll",
  "errType" : null,
  "containingMethodsNum" : 9,
  "sourceCodeText" : "boolean removeAll(Class<T> clazz) throws StateStoreUnavailableException\n{\r\n    verifyDriverReady();\r\n    long start = Time.monotonicNow();\r\n    StateStoreMetrics metrics = getMetrics();\r\n    boolean success = true;\r\n    String path = getPathForClass(clazz);\r\n    List<String> children = getChildren(path);\r\n    for (String child : children) {\r\n        String pathRecord = path + \"/\" + child;\r\n        if (!remove(pathRecord)) {\r\n            success = false;\r\n        }\r\n    }\r\n    if (metrics != null) {\r\n        long time = Time.monotonicNow() - start;\r\n        if (success) {\r\n            metrics.addRemove(time);\r\n        } else {\r\n            metrics.addFailure(time);\r\n        }\r\n    }\r\n    return success;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\resolver\\order",
  "methodName" : "getSubclusterInfo",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "Map<String, String> getSubclusterInfo(MembershipStore membershipStore)\n{\r\n    Map<String, String> mapping = new HashMap<>();\r\n    Map<String, String> dnSubcluster = getDatanodesSubcluster();\r\n    if (dnSubcluster != null) {\r\n        mapping.putAll(dnSubcluster);\r\n    }\r\n    Map<String, String> nnSubcluster = getNamenodesSubcluster(membershipStore);\r\n    if (nnSubcluster != null) {\r\n        mapping.putAll(nnSubcluster);\r\n    }\r\n    return mapping;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\resolver\\order",
  "methodName" : "chooseFirstNamespace",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "String chooseFirstNamespace(String path, PathLocation loc)\n{\r\n    String localSubcluster = null;\r\n    String clientAddr = getClientAddr();\r\n    Map<String, String> subclusterInfo = getSubclusterMapping();\r\n    if (subclusterInfo != null) {\r\n        localSubcluster = subclusterInfo.get(clientAddr);\r\n        if (localSubcluster != null) {\r\n            LOG.debug(\"Local namespace for {} is {}\", clientAddr, localSubcluster);\r\n        } else {\r\n            LOG.error(\"Cannot get local namespace for {}\", clientAddr);\r\n        }\r\n    } else {\r\n        LOG.error(\"Cannot get node mapping when resolving {} at {} from {}\", path, loc, clientAddr);\r\n    }\r\n    return localSubcluster;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\resolver\\order",
  "methodName" : "getClientAddr",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String getClientAddr()\n{\r\n    return Server.getRemoteAddress();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\resolver\\order",
  "methodName" : "getDatanodesSubcluster",
  "errType" : [ "IOException", "IOException" ],
  "containingMethodsNum" : 11,
  "sourceCodeText" : "Map<String, String> getDatanodesSubcluster()\n{\r\n    final RouterRpcServer rpcServer = getRpcServer();\r\n    if (rpcServer == null) {\r\n        LOG.error(\"Cannot access the Router RPC server\");\r\n        return null;\r\n    }\r\n    Map<String, String> ret = new HashMap<>();\r\n    try {\r\n        UserGroupInformation loginUser = UserGroupInformation.getLoginUser();\r\n        Map<String, DatanodeStorageReport[]> dnMap = loginUser.doAs(new PrivilegedAction<Map<String, DatanodeStorageReport[]>>() {\r\n\r\n            @Override\r\n            public Map<String, DatanodeStorageReport[]> run() {\r\n                try {\r\n                    return rpcServer.getDatanodeStorageReportMap(DatanodeReportType.ALL);\r\n                } catch (IOException e) {\r\n                    LOG.error(\"Cannot get the datanodes from the RPC server\", e);\r\n                    return null;\r\n                }\r\n            }\r\n        });\r\n        for (Entry<String, DatanodeStorageReport[]> entry : dnMap.entrySet()) {\r\n            String nsId = entry.getKey();\r\n            DatanodeStorageReport[] dns = entry.getValue();\r\n            for (DatanodeStorageReport dn : dns) {\r\n                DatanodeInfo dnInfo = dn.getDatanodeInfo();\r\n                String ipAddr = dnInfo.getIpAddr();\r\n                ret.put(ipAddr, nsId);\r\n            }\r\n        }\r\n    } catch (IOException e) {\r\n        LOG.error(\"Cannot get Datanodes from the Namenodes: {}\", e.getMessage());\r\n    }\r\n    return ret;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 2,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\resolver\\order",
  "methodName" : "getNamenodesSubcluster",
  "errType" : [ "UnknownHostException", "IOException", "Exception" ],
  "containingMethodsNum" : 16,
  "sourceCodeText" : "Map<String, String> getNamenodesSubcluster(MembershipStore membershipStore)\n{\r\n    String localIp = \"127.0.0.1\";\r\n    String localHostname = localIp;\r\n    try {\r\n        localHostname = InetAddress.getLocalHost().getHostName();\r\n    } catch (UnknownHostException e) {\r\n        LOG.error(\"Cannot get local host name\");\r\n    }\r\n    Map<String, String> ret = new HashMap<>();\r\n    try {\r\n        GetNamenodeRegistrationsRequest request = GetNamenodeRegistrationsRequest.newInstance();\r\n        GetNamenodeRegistrationsResponse response = membershipStore.getNamenodeRegistrations(request);\r\n        final List<MembershipState> nns = response.getNamenodeMemberships();\r\n        for (MembershipState nn : nns) {\r\n            try {\r\n                String nsId = nn.getNameserviceId();\r\n                String rpcAddress = nn.getRpcAddress();\r\n                String hostname = HostAndPort.fromString(rpcAddress).getHost();\r\n                ret.put(hostname, nsId);\r\n                if (hostname.equals(localHostname)) {\r\n                    ret.put(localIp, nsId);\r\n                }\r\n                InetAddress addr = InetAddress.getByName(hostname);\r\n                String ipAddr = addr.getHostAddress();\r\n                ret.put(ipAddr, nsId);\r\n            } catch (Exception e) {\r\n                LOG.error(\"Cannot get address for {}: {}\", nn, e.getMessage());\r\n            }\r\n        }\r\n    } catch (IOException ioe) {\r\n        LOG.error(\"Cannot get Namenodes from the State Store\", ioe);\r\n    }\r\n    return ret;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 3,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "serviceInit",
  "errType" : null,
  "containingMethodsNum" : 41,
  "sourceCodeText" : "void serviceInit(Configuration configuration) throws Exception\n{\r\n    this.conf = configuration;\r\n    updateRouterState(RouterServiceState.INITIALIZING);\r\n    UserGroupInformation.setConfiguration(conf);\r\n    SecurityUtil.login(conf, DFS_ROUTER_KEYTAB_FILE_KEY, DFS_ROUTER_KERBEROS_PRINCIPAL_KEY, getHostName(conf));\r\n    if (conf.getBoolean(RBFConfigKeys.DFS_ROUTER_STORE_ENABLE, RBFConfigKeys.DFS_ROUTER_STORE_ENABLE_DEFAULT)) {\r\n        this.stateStore = new StateStoreService();\r\n        addService(this.stateStore);\r\n    }\r\n    this.namenodeResolver = newActiveNamenodeResolver(this.conf, this.stateStore);\r\n    if (this.namenodeResolver == null) {\r\n        throw new IOException(\"Cannot find namenode resolver.\");\r\n    }\r\n    this.subclusterResolver = newFileSubclusterResolver(this.conf, this);\r\n    if (this.subclusterResolver == null) {\r\n        throw new IOException(\"Cannot find subcluster resolver\");\r\n    }\r\n    if (conf.getBoolean(RBFConfigKeys.DFS_ROUTER_RPC_ENABLE, RBFConfigKeys.DFS_ROUTER_RPC_ENABLE_DEFAULT)) {\r\n        this.rpcServer = createRpcServer();\r\n        addService(this.rpcServer);\r\n        this.setRpcServerAddress(rpcServer.getRpcAddress());\r\n    }\r\n    checkRouterId();\r\n    if (conf.getBoolean(RBFConfigKeys.DFS_ROUTER_ADMIN_ENABLE, RBFConfigKeys.DFS_ROUTER_ADMIN_ENABLE_DEFAULT)) {\r\n        this.adminServer = createAdminServer();\r\n        addService(this.adminServer);\r\n    }\r\n    if (conf.getBoolean(RBFConfigKeys.DFS_ROUTER_HTTP_ENABLE, RBFConfigKeys.DFS_ROUTER_HTTP_ENABLE_DEFAULT)) {\r\n        this.httpServer = createHttpServer();\r\n        addService(this.httpServer);\r\n    }\r\n    boolean isRouterHeartbeatEnabled = conf.getBoolean(RBFConfigKeys.DFS_ROUTER_HEARTBEAT_ENABLE, RBFConfigKeys.DFS_ROUTER_HEARTBEAT_ENABLE_DEFAULT);\r\n    boolean isNamenodeHeartbeatEnable = conf.getBoolean(RBFConfigKeys.DFS_ROUTER_NAMENODE_HEARTBEAT_ENABLE, isRouterHeartbeatEnabled);\r\n    if (isNamenodeHeartbeatEnable) {\r\n        this.namenodeHeartbeatServices = createNamenodeHeartbeatServices();\r\n        for (NamenodeHeartbeatService heartbeatService : this.namenodeHeartbeatServices) {\r\n            addService(heartbeatService);\r\n        }\r\n        if (this.namenodeHeartbeatServices.isEmpty()) {\r\n            LOG.error(\"Heartbeat is enabled but there are no namenodes to monitor\");\r\n        }\r\n    }\r\n    if (isRouterHeartbeatEnabled) {\r\n        this.routerHeartbeatService = new RouterHeartbeatService(this);\r\n        addService(this.routerHeartbeatService);\r\n    }\r\n    if (conf.getBoolean(RBFConfigKeys.DFS_ROUTER_METRICS_ENABLE, RBFConfigKeys.DFS_ROUTER_METRICS_ENABLE_DEFAULT)) {\r\n        DefaultMetricsSystem.initialize(\"Router\");\r\n        this.metrics = new RouterMetricsService(this);\r\n        addService(this.metrics);\r\n        this.pauseMonitor = new JvmPauseMonitor();\r\n        this.pauseMonitor.init(conf);\r\n    }\r\n    if (conf.getBoolean(RBFConfigKeys.DFS_ROUTER_QUOTA_ENABLE, RBFConfigKeys.DFS_ROUTER_QUOTA_ENABLED_DEFAULT)) {\r\n        this.quotaManager = new RouterQuotaManager();\r\n        this.quotaUpdateService = new RouterQuotaUpdateService(this);\r\n        addService(this.quotaUpdateService);\r\n    }\r\n    if (conf.getBoolean(RBFConfigKeys.DFS_ROUTER_SAFEMODE_ENABLE, RBFConfigKeys.DFS_ROUTER_SAFEMODE_ENABLE_DEFAULT)) {\r\n        this.safemodeService = new RouterSafemodeService(this);\r\n        addService(this.safemodeService);\r\n    }\r\n    if (conf.getBoolean(RBFConfigKeys.MOUNT_TABLE_CACHE_UPDATE, RBFConfigKeys.MOUNT_TABLE_CACHE_UPDATE_DEFAULT)) {\r\n        String disabledDependentServices = getDisabledDependentServices();\r\n        if (disabledDependentServices == null) {\r\n            MountTableRefresherService refreshService = new MountTableRefresherService(this);\r\n            addService(refreshService);\r\n            LOG.info(\"Service {} is enabled.\", MountTableRefresherService.class.getSimpleName());\r\n        } else {\r\n            LOG.warn(\"Service {} not enabled: dependent service(s) {} not enabled.\", MountTableRefresherService.class.getSimpleName(), disabledDependentServices);\r\n        }\r\n    }\r\n    super.serviceInit(conf);\r\n    if (stateStore != null) {\r\n        MountTableStore mountstore = this.stateStore.getRegisteredRecordStore(MountTableStore.class);\r\n        mountstore.setQuotaManager(this.quotaManager);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "checkRouterId",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void checkRouterId()\n{\r\n    if (this.routerId == null) {\r\n        InetSocketAddress confRpcAddress = conf.getSocketAddr(RBFConfigKeys.DFS_ROUTER_RPC_BIND_HOST_KEY, RBFConfigKeys.DFS_ROUTER_RPC_ADDRESS_KEY, RBFConfigKeys.DFS_ROUTER_RPC_ADDRESS_DEFAULT, RBFConfigKeys.DFS_ROUTER_RPC_PORT_DEFAULT);\r\n        setRpcServerAddress(confRpcAddress);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "getDisabledDependentServices",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "String getDisabledDependentServices()\n{\r\n    if (this.stateStore == null && this.adminServer == null) {\r\n        return StateStoreService.class.getSimpleName() + \",\" + RouterAdminServer.class.getSimpleName();\r\n    } else if (this.stateStore == null) {\r\n        return StateStoreService.class.getSimpleName();\r\n    } else if (this.adminServer == null) {\r\n        return RouterAdminServer.class.getSimpleName();\r\n    }\r\n    return null;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "getHostName",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "String getHostName(Configuration config) throws UnknownHostException\n{\r\n    String name = config.get(DFS_ROUTER_KERBEROS_PRINCIPAL_HOSTNAME_KEY);\r\n    if (name == null) {\r\n        name = InetAddress.getLocalHost().getHostName();\r\n    }\r\n    return name;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "serviceStart",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void serviceStart() throws Exception\n{\r\n    if (this.safemodeService == null) {\r\n        updateRouterState(RouterServiceState.RUNNING);\r\n    }\r\n    if (this.pauseMonitor != null) {\r\n        this.pauseMonitor.start();\r\n        JvmMetrics jvmMetrics = this.metrics.getJvmMetrics();\r\n        if (jvmMetrics != null) {\r\n            jvmMetrics.setPauseMonitor(pauseMonitor);\r\n        }\r\n    }\r\n    super.serviceStart();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "serviceStop",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void serviceStop() throws Exception\n{\r\n    updateRouterState(RouterServiceState.SHUTDOWN);\r\n    if (this.pauseMonitor != null) {\r\n        this.pauseMonitor.stop();\r\n    }\r\n    super.serviceStop();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "shutDown",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void shutDown()\n{\r\n    new Thread() {\r\n\r\n        @Override\r\n        public void run() {\r\n            Router.this.stop();\r\n        }\r\n    }.start();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "createRpcServer",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "RouterRpcServer createRpcServer() throws IOException\n{\r\n    return new RouterRpcServer(this.conf, this, this.getNamenodeResolver(), this.getSubclusterResolver());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "getRpcServer",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "RouterRpcServer getRpcServer()\n{\r\n    return this.rpcServer;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "setRpcServerAddress",
  "errType" : [ "UnknownHostException" ],
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void setRpcServerAddress(InetSocketAddress address)\n{\r\n    this.rpcAddress = address;\r\n    if (this.rpcAddress != null) {\r\n        try {\r\n            String hostname = InetAddress.getLocalHost().getHostName();\r\n            setRouterId(hostname + \":\" + this.rpcAddress.getPort());\r\n        } catch (UnknownHostException ex) {\r\n            LOG.error(\"Cannot set unique router ID, address not resolvable {}\", this.rpcAddress);\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "getRpcServerAddress",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "InetSocketAddress getRpcServerAddress()\n{\r\n    return this.rpcAddress;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "createAdminServer",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "RouterAdminServer createAdminServer() throws IOException\n{\r\n    return new RouterAdminServer(this.conf, this);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "setAdminServerAddress",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setAdminServerAddress(InetSocketAddress address)\n{\r\n    this.adminAddress = address;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "getAdminServerAddress",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "InetSocketAddress getAdminServerAddress()\n{\r\n    return adminAddress;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "createHttpServer",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "RouterHttpServer createHttpServer()\n{\r\n    return new RouterHttpServer(this);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "getHttpServerAddress",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "InetSocketAddress getHttpServerAddress()\n{\r\n    if (httpServer != null) {\r\n        return httpServer.getHttpAddress();\r\n    }\r\n    return null;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "verifyToken",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void verifyToken(DelegationTokenIdentifier tokenId, byte[] password) throws IOException\n{\r\n    getRpcServer().getRouterSecurityManager().verifyToken(tokenId, password);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "createNamenodeHeartbeatServices",
  "errType" : null,
  "containingMethodsNum" : 18,
  "sourceCodeText" : "Collection<NamenodeHeartbeatService> createNamenodeHeartbeatServices()\n{\r\n    Map<String, NamenodeHeartbeatService> ret = new HashMap<>();\r\n    if (conf.getBoolean(RBFConfigKeys.DFS_ROUTER_MONITOR_LOCAL_NAMENODE, RBFConfigKeys.DFS_ROUTER_MONITOR_LOCAL_NAMENODE_DEFAULT)) {\r\n        NamenodeHeartbeatService localHeartbeatService = createLocalNamenodeHeartbeatService();\r\n        if (localHeartbeatService != null) {\r\n            String nnDesc = localHeartbeatService.getNamenodeDesc();\r\n            ret.put(nnDesc, localHeartbeatService);\r\n        }\r\n    }\r\n    Collection<String> namenodes = this.conf.getTrimmedStringCollection(RBFConfigKeys.DFS_ROUTER_MONITOR_NAMENODE);\r\n    for (String namenode : namenodes) {\r\n        String[] namenodeSplit = namenode.split(\"\\\\.\");\r\n        String nsId = null;\r\n        String nnId = null;\r\n        if (namenodeSplit.length == 2) {\r\n            nsId = namenodeSplit[0];\r\n            nnId = namenodeSplit[1];\r\n        } else if (namenodeSplit.length == 1) {\r\n            nsId = namenode;\r\n        } else {\r\n            LOG.error(\"Wrong Namenode to monitor: {}\", namenode);\r\n        }\r\n        if (nsId != null) {\r\n            String configKeyWithHost = RBFConfigKeys.DFS_ROUTER_MONITOR_NAMENODE_RESOLUTION_ENABLED + \".\" + nsId;\r\n            boolean resolveNeeded = conf.getBoolean(configKeyWithHost, RBFConfigKeys.DFS_ROUTER_MONITOR_NAMENODE_RESOLUTION_ENABLED_DEFAULT);\r\n            if (nnId != null && resolveNeeded) {\r\n                DomainNameResolver dnr = DomainNameResolverFactory.newInstance(conf, nsId, RBFConfigKeys.DFS_ROUTER_MONITOR_NAMENODE_RESOLVER_IMPL);\r\n                Map<String, InetSocketAddress> hosts = Maps.newLinkedHashMap();\r\n                Map<String, InetSocketAddress> resolvedHosts = DFSUtilClient.getResolvedAddressesForNnId(conf, nsId, nnId, dnr, null, DFS_NAMENODE_RPC_ADDRESS_KEY, DFS_NAMENODE_SERVICE_RPC_ADDRESS_KEY);\r\n                hosts.putAll(resolvedHosts);\r\n                for (InetSocketAddress isa : hosts.values()) {\r\n                    NamenodeHeartbeatService heartbeatService = createNamenodeHeartbeatService(nsId, nnId, isa.getHostName());\r\n                    if (heartbeatService != null) {\r\n                        ret.put(heartbeatService.getNamenodeDesc(), heartbeatService);\r\n                    }\r\n                }\r\n            } else {\r\n                NamenodeHeartbeatService heartbeatService = createNamenodeHeartbeatService(nsId, nnId);\r\n                if (heartbeatService != null) {\r\n                    ret.put(heartbeatService.getNamenodeDesc(), heartbeatService);\r\n                }\r\n            }\r\n        }\r\n    }\r\n    return ret.values();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "createLocalNamenodeHeartbeatService",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "NamenodeHeartbeatService createLocalNamenodeHeartbeatService()\n{\r\n    String nsId = DFSUtil.getNamenodeNameServiceId(conf);\r\n    if (nsId == null) {\r\n        LOG.error(\"Cannot find local nameservice id\");\r\n        return null;\r\n    }\r\n    String nnId = null;\r\n    if (HAUtil.isHAEnabled(conf, nsId)) {\r\n        nnId = HAUtil.getNameNodeId(conf, nsId);\r\n        if (nnId == null) {\r\n            LOG.error(\"Cannot find namenode id for local {}\", nsId);\r\n            return null;\r\n        }\r\n    }\r\n    return createNamenodeHeartbeatService(nsId, nnId);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "createNamenodeHeartbeatService",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "NamenodeHeartbeatService createNamenodeHeartbeatService(String nsId, String nnId)\n{\r\n    LOG.info(\"Creating heartbeat service for Namenode {} in {}\", nnId, nsId);\r\n    NamenodeHeartbeatService ret = new NamenodeHeartbeatService(namenodeResolver, nsId, nnId);\r\n    return ret;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "createNamenodeHeartbeatService",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "NamenodeHeartbeatService createNamenodeHeartbeatService(String nsId, String nnId, String resolvedHost)\n{\r\n    LOG.info(\"Creating heartbeat service for\" + \" Namenode {}, resolved host {}, in {}\", nnId, resolvedHost, nsId);\r\n    NamenodeHeartbeatService ret = new NamenodeHeartbeatService(namenodeResolver, nsId, nnId, resolvedHost);\r\n    return ret;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "updateRouterState",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void updateRouterState(RouterServiceState newState)\n{\r\n    this.state = newState;\r\n    if (this.routerHeartbeatService != null) {\r\n        this.routerHeartbeatService.updateStateAsync();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "getRouterState",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "RouterServiceState getRouterState()\n{\r\n    return this.state;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "isRouterState",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "boolean isRouterState(RouterServiceState routerState)\n{\r\n    return routerState.equals(this.state);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "getStateStore",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "StateStoreService getStateStore()\n{\r\n    return this.stateStore;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "getRouterMetrics",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "RouterMetrics getRouterMetrics()\n{\r\n    if (this.metrics != null) {\r\n        return this.metrics.getRouterMetrics();\r\n    }\r\n    return null;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "getRouterClientMetrics",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "RouterClientMetrics getRouterClientMetrics()\n{\r\n    if (this.metrics != null) {\r\n        return this.metrics.getRouterClientMetrics();\r\n    }\r\n    return null;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "getMetrics",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "RBFMetrics getMetrics()\n{\r\n    if (this.metrics != null) {\r\n        return this.metrics.getRBFMetrics();\r\n    }\r\n    return null;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "getNamenodeMetrics",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "NamenodeBeanMetrics getNamenodeMetrics() throws IOException\n{\r\n    if (this.metrics == null) {\r\n        throw new IOException(\"Namenode metrics is not initialized\");\r\n    }\r\n    return this.metrics.getNamenodeMetrics();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "getSubclusterResolver",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "FileSubclusterResolver getSubclusterResolver()\n{\r\n    return this.subclusterResolver;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "getNamenodeResolver",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "ActiveNamenodeResolver getNamenodeResolver()\n{\r\n    return this.namenodeResolver;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "getRouterStateManager",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "RouterStore getRouterStateManager()\n{\r\n    if (this.routerStateManager == null && this.stateStore != null) {\r\n        this.routerStateManager = this.stateStore.getRegisteredRecordStore(RouterStore.class);\r\n    }\r\n    return this.routerStateManager;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "getStartTime",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getStartTime()\n{\r\n    return this.startTime;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "getRouterId",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getRouterId()\n{\r\n    return this.routerId;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "setRouterId",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void setRouterId(String id)\n{\r\n    this.routerId = id;\r\n    if (this.stateStore != null) {\r\n        this.stateStore.setIdentifier(this.routerId);\r\n    }\r\n    if (this.namenodeResolver != null) {\r\n        this.namenodeResolver.setRouterId(this.routerId);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "isQuotaEnabled",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean isQuotaEnabled()\n{\r\n    return this.quotaManager != null;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "getQuotaManager",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "RouterQuotaManager getQuotaManager()\n{\r\n    return this.quotaManager;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "getQuotaCacheUpdateService",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "RouterQuotaUpdateService getQuotaCacheUpdateService()\n{\r\n    return this.quotaUpdateService;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "getNamenodeHeartbeatServices",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Collection<NamenodeHeartbeatService> getNamenodeHeartbeatServices()\n{\r\n    return this.namenodeHeartbeatServices;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "getRouterHeartbeatService",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "RouterHeartbeatService getRouterHeartbeatService()\n{\r\n    return this.routerHeartbeatService;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "getSafemodeService",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "RouterSafemodeService getSafemodeService()\n{\r\n    return this.safemodeService;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "getAdminServer",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "RouterAdminServer getAdminServer()\n{\r\n    return adminServer;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "setConf",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setConf(Configuration conf)\n{\r\n    this.conf = conf;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\protocol\\impl\\pb",
  "methodName" : "getProto",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "EnableNameserviceRequestProto getProto()\n{\r\n    return translator.build();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\protocol\\impl\\pb",
  "methodName" : "setProto",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setProto(Message proto)\n{\r\n    this.translator.setProto(proto);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\protocol\\impl\\pb",
  "methodName" : "readInstance",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void readInstance(String base64String) throws IOException\n{\r\n    this.translator.readInstance(base64String);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\protocol\\impl\\pb",
  "methodName" : "getNameServiceId",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String getNameServiceId()\n{\r\n    return this.translator.getProtoOrBuilder().getNameServiceId();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\protocol\\impl\\pb",
  "methodName" : "setNameServiceId",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setNameServiceId(String nsId)\n{\r\n    this.translator.getBuilder().setNameServiceId(nsId);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\protocol",
  "methodName" : "newInstance",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "EnterSafeModeResponse newInstance() throws IOException\n{\r\n    return StateStoreSerializer.newRecord(EnterSafeModeResponse.class);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\protocol",
  "methodName" : "newInstance",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "EnterSafeModeResponse newInstance(boolean status) throws IOException\n{\r\n    EnterSafeModeResponse response = newInstance();\r\n    response.setStatus(status);\r\n    return response;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\protocol",
  "methodName" : "getStatus",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean getStatus()",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\protocol",
  "methodName" : "setStatus",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setStatus(boolean result)",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\protocol\\impl\\pb",
  "methodName" : "getProto",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "RemoveMountTableEntryRequestProto getProto()\n{\r\n    return this.translator.build();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\protocol\\impl\\pb",
  "methodName" : "setProto",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setProto(Message proto)\n{\r\n    this.translator.setProto(proto);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\protocol\\impl\\pb",
  "methodName" : "readInstance",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void readInstance(String base64String) throws IOException\n{\r\n    this.translator.readInstance(base64String);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\protocol\\impl\\pb",
  "methodName" : "getSrcPath",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String getSrcPath()\n{\r\n    return this.translator.getProtoOrBuilder().getSrcPath();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\protocol\\impl\\pb",
  "methodName" : "setSrcPath",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setSrcPath(String path)\n{\r\n    this.translator.getBuilder().setSrcPath(path);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\protocol\\impl\\pb",
  "methodName" : "getProto",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "UpdateNamenodeRegistrationResponseProto getProto()\n{\r\n    return this.translator.build();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\protocol\\impl\\pb",
  "methodName" : "setProto",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setProto(Message proto)\n{\r\n    this.translator.setProto(proto);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\protocol\\impl\\pb",
  "methodName" : "readInstance",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void readInstance(String base64String) throws IOException\n{\r\n    this.translator.readInstance(base64String);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\protocol\\impl\\pb",
  "methodName" : "getResult",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "boolean getResult()\n{\r\n    return this.translator.getProtoOrBuilder().getStatus();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\protocol\\impl\\pb",
  "methodName" : "setResult",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setResult(boolean result)\n{\r\n    this.translator.getBuilder().setStatus(result);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\rbfbalance",
  "methodName" : "run",
  "errType" : null,
  "containingMethodsNum" : 9,
  "sourceCodeText" : "int run(String[] args) throws Exception\n{\r\n    CommandLineParser parser = new GnuParser();\r\n    CommandLine command = parser.parse(CLI_OPTIONS, args, true);\r\n    String[] leftOverArgs = command.getArgs();\r\n    if (leftOverArgs == null || leftOverArgs.length < 1) {\r\n        printUsage();\r\n        return -1;\r\n    }\r\n    String cmd = leftOverArgs[0];\r\n    if (cmd.equals(SUBMIT_COMMAND)) {\r\n        if (leftOverArgs.length < 3) {\r\n            printUsage();\r\n            return -1;\r\n        }\r\n        String inputSrc = leftOverArgs[1];\r\n        String inputDst = leftOverArgs[2];\r\n        return submit(command, inputSrc, inputDst);\r\n    } else if (cmd.equals(CONTINUE_COMMAND)) {\r\n        return continueJob();\r\n    } else {\r\n        printUsage();\r\n        return -1;\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\rbfbalance",
  "methodName" : "continueJob",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 8,
  "sourceCodeText" : "int continueJob() throws InterruptedException\n{\r\n    BalanceProcedureScheduler scheduler = new BalanceProcedureScheduler(getConf());\r\n    try {\r\n        scheduler.init(true);\r\n        while (true) {\r\n            Collection<BalanceJob> jobs = scheduler.getAllJobs();\r\n            int unfinished = 0;\r\n            for (BalanceJob job : jobs) {\r\n                if (!job.isJobDone()) {\r\n                    unfinished++;\r\n                }\r\n                LOG.info(job.toString());\r\n            }\r\n            if (unfinished == 0) {\r\n                break;\r\n            }\r\n            Thread.sleep(TimeUnit.SECONDS.toMillis(10));\r\n        }\r\n    } catch (IOException e) {\r\n        LOG.error(\"Continue balance job failed.\", e);\r\n        return -1;\r\n    } finally {\r\n        scheduler.shutDown();\r\n    }\r\n    return 0;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\rbfbalance",
  "methodName" : "submit",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 25,
  "sourceCodeText" : "int submit(CommandLine command, String inputSrc, String inputDst) throws IOException\n{\r\n    Builder builder = new Builder(inputSrc, inputDst);\r\n    builder.setForceCloseOpen(command.hasOption(FORCE_CLOSE_OPEN.getOpt()));\r\n    if (command.hasOption(MAP.getOpt())) {\r\n        builder.setMap(Integer.parseInt(command.getOptionValue(MAP.getOpt())));\r\n    }\r\n    if (command.hasOption(BANDWIDTH.getOpt())) {\r\n        builder.setBandWidth(Integer.parseInt(command.getOptionValue(BANDWIDTH.getOpt())));\r\n    }\r\n    if (command.hasOption(DELAY_DURATION.getOpt())) {\r\n        builder.setDelayDuration(Long.parseLong(command.getOptionValue(DELAY_DURATION.getOpt())));\r\n    }\r\n    if (command.hasOption(DIFF_THRESHOLD.getOpt())) {\r\n        builder.setDiffThreshold(Integer.parseInt(command.getOptionValue(DIFF_THRESHOLD.getOpt())));\r\n    }\r\n    if (command.hasOption(TRASH.getOpt())) {\r\n        String val = command.getOptionValue(TRASH.getOpt());\r\n        if (val.equalsIgnoreCase(\"skip\")) {\r\n            builder.setTrashOpt(TrashOption.SKIP);\r\n        } else if (val.equalsIgnoreCase(\"trash\")) {\r\n            builder.setTrashOpt(TrashOption.TRASH);\r\n        } else if (val.equalsIgnoreCase(\"delete\")) {\r\n            builder.setTrashOpt(TrashOption.DELETE);\r\n        } else {\r\n            printUsage();\r\n            return -1;\r\n        }\r\n    }\r\n    BalanceProcedureScheduler scheduler = new BalanceProcedureScheduler(getConf());\r\n    scheduler.init(false);\r\n    try {\r\n        BalanceJob balanceJob = builder.build();\r\n        scheduler.submit(balanceJob);\r\n        scheduler.waitUntilDone(balanceJob);\r\n    } catch (IOException e) {\r\n        LOG.error(\"Submit balance job failed.\", e);\r\n        return -1;\r\n    } finally {\r\n        scheduler.shutDown();\r\n    }\r\n    return 0;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\rbfbalance",
  "methodName" : "getSrcPath",
  "errType" : null,
  "containingMethodsNum" : 9,
  "sourceCodeText" : "Path getSrcPath(String fedPath) throws IOException\n{\r\n    String address = getConf().getTrimmed(RBFConfigKeys.DFS_ROUTER_ADMIN_ADDRESS_KEY, RBFConfigKeys.DFS_ROUTER_ADMIN_ADDRESS_DEFAULT);\r\n    InetSocketAddress routerSocket = NetUtils.createSocketAddr(address);\r\n    RouterClient rClient = new RouterClient(routerSocket, getConf());\r\n    try {\r\n        MountTableManager mountTable = rClient.getMountTableManager();\r\n        MountTable entry = MountTableProcedure.getMountEntry(fedPath, mountTable);\r\n        if (entry == null) {\r\n            throw new IllegalArgumentException(\"The mount point doesn't exist. path=\" + fedPath);\r\n        } else if (entry.getDestinations().size() > 1) {\r\n            throw new IllegalArgumentException(\"The mount point has more than one destination. path=\" + fedPath);\r\n        } else {\r\n            String ns = entry.getDestinations().get(0).getNameserviceId();\r\n            String path = entry.getDestinations().get(0).getDest();\r\n            return new Path(\"hdfs://\" + ns + path);\r\n        }\r\n    } finally {\r\n        rClient.close();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\rbfbalance",
  "methodName" : "printUsage",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void printUsage()\n{\r\n    HelpFormatter formatter = new HelpFormatter();\r\n    formatter.printHelp(\"rbfbalance OPTIONS [submit|continue] <src> <target>\\n\\nOPTIONS\", CLI_OPTIONS);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\rbfbalance",
  "methodName" : "getDefaultConf",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "Configuration getDefaultConf()\n{\r\n    Configuration config = new HdfsConfiguration();\r\n    config.addResource(FED_BALANCE_DEFAULT_XML);\r\n    config.addResource(FED_BALANCE_SITE_XML);\r\n    return config;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\rbfbalance",
  "methodName" : "main",
  "errType" : [ "Exception" ],
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void main(String[] argv)\n{\r\n    Configuration conf = getDefaultConf();\r\n    RouterFedBalance fedBalance = new RouterFedBalance();\r\n    fedBalance.setConf(conf);\r\n    int exitCode;\r\n    try {\r\n        exitCode = ToolRunner.run(fedBalance, argv);\r\n    } catch (Exception e) {\r\n        LOG.warn(\"Couldn't complete RouterFedBalance operation.\", e);\r\n        exitCode = -1;\r\n    }\r\n    System.exit(exitCode);\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\fairness",
  "methodName" : "init",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void init(Configuration conf)\n{\r\n    this.permits = new HashMap<>();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\fairness",
  "methodName" : "acquirePermit",
  "errType" : [ "InterruptedException" ],
  "containingMethodsNum" : 3,
  "sourceCodeText" : "boolean acquirePermit(String nsId)\n{\r\n    try {\r\n        LOG.debug(\"Taking lock for nameservice {}\", nsId);\r\n        return this.permits.get(nsId).tryAcquire(1, TimeUnit.SECONDS);\r\n    } catch (InterruptedException e) {\r\n        LOG.debug(\"Cannot get a permit for nameservice {}\", nsId);\r\n    }\r\n    return false;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\fairness",
  "methodName" : "releasePermit",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void releasePermit(String nsId)\n{\r\n    this.permits.get(nsId).release();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\fairness",
  "methodName" : "shutdown",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void shutdown()\n{\r\n    for (Semaphore sema : this.permits.values()) {\r\n        sema.drainPermits();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\fairness",
  "methodName" : "insertNameServiceWithPermits",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void insertNameServiceWithPermits(String nsId, int maxPermits)\n{\r\n    this.permits.put(nsId, new Semaphore(maxPermits));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\fairness",
  "methodName" : "getAvailablePermits",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "int getAvailablePermits(String nsId)\n{\r\n    return this.permits.get(nsId).availablePermits();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\fairness",
  "methodName" : "getAvailableHandlerOnPerNs",
  "errType" : [ "JSONException" ],
  "containingMethodsNum" : 6,
  "sourceCodeText" : "String getAvailableHandlerOnPerNs()\n{\r\n    JSONObject json = new JSONObject();\r\n    for (Map.Entry<String, Semaphore> entry : permits.entrySet()) {\r\n        try {\r\n            String nsId = entry.getKey();\r\n            int availableHandler = entry.getValue().availablePermits();\r\n            json.put(nsId, availableHandler);\r\n        } catch (JSONException e) {\r\n            LOG.warn(\"Cannot put {} into JSONObject\", entry.getKey(), e);\r\n        }\r\n    }\r\n    return json.toString();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\protocol\\impl\\pb",
  "methodName" : "getProto",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "NamenodeHeartbeatRequestProto getProto()\n{\r\n    return this.translator.build();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\protocol\\impl\\pb",
  "methodName" : "setProto",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setProto(Message proto)\n{\r\n    this.translator.setProto(proto);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\protocol\\impl\\pb",
  "methodName" : "readInstance",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void readInstance(String base64String) throws IOException\n{\r\n    this.translator.readInstance(base64String);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\protocol\\impl\\pb",
  "methodName" : "getNamenodeMembership",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "MembershipState getNamenodeMembership() throws IOException\n{\r\n    NamenodeMembershipRecordProto membershipProto = this.translator.getProtoOrBuilder().getNamenodeMembership();\r\n    MembershipState membership = StateStoreSerializer.newRecord(MembershipState.class);\r\n    if (membership instanceof MembershipStatePBImpl) {\r\n        MembershipStatePBImpl membershipPB = (MembershipStatePBImpl) membership;\r\n        membershipPB.setProto(membershipProto);\r\n        return membershipPB;\r\n    } else {\r\n        throw new IOException(\"Cannot get membership from request\");\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\protocol\\impl\\pb",
  "methodName" : "setNamenodeMembership",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void setNamenodeMembership(MembershipState membership) throws IOException\n{\r\n    if (membership instanceof MembershipStatePBImpl) {\r\n        MembershipStatePBImpl membershipPB = (MembershipStatePBImpl) membership;\r\n        NamenodeMembershipRecordProto membershipProto = (NamenodeMembershipRecordProto) membershipPB.getProto();\r\n        this.translator.getBuilder().setNamenodeMembership(membershipProto);\r\n    } else {\r\n        throw new IOException(\"Cannot set mount table entry\");\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "getClientConfiguration",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "Configuration getClientConfiguration(final Configuration conf)\n{\r\n    Configuration clientConf = new Configuration(conf);\r\n    int maxRetries = conf.getInt(RBFConfigKeys.DFS_ROUTER_CLIENT_MAX_RETRIES_TIME_OUT, RBFConfigKeys.DFS_ROUTER_CLIENT_MAX_RETRIES_TIME_OUT_DEFAULT);\r\n    if (maxRetries >= 0) {\r\n        clientConf.setInt(IPC_CLIENT_CONNECT_MAX_RETRIES_ON_SOCKET_TIMEOUTS_KEY, maxRetries);\r\n    }\r\n    long connectTimeOut = conf.getTimeDuration(RBFConfigKeys.DFS_ROUTER_CLIENT_CONNECT_TIMEOUT, RBFConfigKeys.DFS_ROUTER_CLIENT_CONNECT_TIMEOUT_DEFAULT, TimeUnit.MILLISECONDS);\r\n    if (connectTimeOut >= 0) {\r\n        clientConf.setLong(IPC_CLIENT_CONNECT_TIMEOUT_KEY, connectTimeOut);\r\n    }\r\n    return clientConf;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "getNamenodeResolver",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "ActiveNamenodeResolver getNamenodeResolver()\n{\r\n    return this.namenodeResolver;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "shutdown",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void shutdown()\n{\r\n    if (this.connectionManager != null) {\r\n        this.connectionManager.close();\r\n    }\r\n    if (this.executorService != null) {\r\n        this.executorService.shutdownNow();\r\n    }\r\n    if (this.routerRpcFairnessPolicyController != null) {\r\n        this.routerRpcFairnessPolicyController.shutdown();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "getNumConnections",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "int getNumConnections()\n{\r\n    return this.connectionManager.getNumConnections();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "getNumActiveConnections",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "int getNumActiveConnections()\n{\r\n    return this.connectionManager.getNumActiveConnections();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "getNumIdleConnections",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "int getNumIdleConnections()\n{\r\n    return this.connectionManager.getNumIdleConnections();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "getNumActiveConnectionsRecently",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "int getNumActiveConnectionsRecently()\n{\r\n    return this.connectionManager.getNumActiveConnectionsRecently();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "getNumConnectionPools",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "int getNumConnectionPools()\n{\r\n    return this.connectionManager.getNumConnectionPools();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "getNumCreatingConnections",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "int getNumCreatingConnections()\n{\r\n    return this.connectionManager.getNumCreatingConnections();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "getJSON",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String getJSON()\n{\r\n    return this.connectionManager.getJSON();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "getAsyncCallerPoolJson",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "String getAsyncCallerPoolJson()\n{\r\n    final Map<String, Integer> info = new LinkedHashMap<>();\r\n    info.put(\"active\", executorService.getActiveCount());\r\n    info.put(\"total\", executorService.getPoolSize());\r\n    info.put(\"max\", executorService.getMaximumPoolSize());\r\n    return JSON.toString(info);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "getRejectedPermitsPerNsJSON",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String getRejectedPermitsPerNsJSON()\n{\r\n    return JSON.toString(rejectedPermitsPerNs);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "getAcceptedPermitsPerNsJSON",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String getAcceptedPermitsPerNsJSON()\n{\r\n    return JSON.toString(acceptedPermitsPerNs);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "getConnection",
  "errType" : [ "Exception" ],
  "containingMethodsNum" : 6,
  "sourceCodeText" : "ConnectionContext getConnection(UserGroupInformation ugi, String nsId, String rpcAddress, Class<?> proto) throws IOException\n{\r\n    ConnectionContext connection = null;\r\n    try {\r\n        UserGroupInformation connUGI = ugi;\r\n        if (UserGroupInformation.isSecurityEnabled()) {\r\n            UserGroupInformation routerUser = UserGroupInformation.getLoginUser();\r\n            connUGI = UserGroupInformation.createProxyUser(ugi.getUserName(), routerUser);\r\n        }\r\n        connection = this.connectionManager.getConnection(connUGI, rpcAddress, proto);\r\n        LOG.debug(\"User {} NN {} is using connection {}\", ugi.getUserName(), rpcAddress, connection);\r\n    } catch (Exception ex) {\r\n        LOG.error(\"Cannot open NN client to address: {}\", rpcAddress, ex);\r\n    }\r\n    if (connection == null) {\r\n        throw new ConnectionNullException(\"Cannot get a connection to \" + rpcAddress);\r\n    }\r\n    return connection;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "toIOException",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "IOException toIOException(Exception e)\n{\r\n    if (e instanceof RemoteException) {\r\n        return ((RemoteException) e).unwrapRemoteException();\r\n    }\r\n    if (e instanceof IOException) {\r\n        return (IOException) e;\r\n    }\r\n    return new IOException(e);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "shouldRetry",
  "errType" : [ "Exception" ],
  "containingMethodsNum" : 4,
  "sourceCodeText" : "RetryDecision shouldRetry(final IOException ioe, final int retryCount, final String nsId) throws IOException\n{\r\n    if (isClusterUnAvailable(nsId)) {\r\n        if (retryCount == 0) {\r\n            return RetryDecision.RETRY;\r\n        } else {\r\n            throw new NoNamenodesAvailableException(nsId, ioe);\r\n        }\r\n    }\r\n    try {\r\n        final RetryPolicy.RetryAction a = this.retryPolicy.shouldRetry(ioe, retryCount, 0, true);\r\n        return a.action;\r\n    } catch (Exception ex) {\r\n        LOG.error(\"Re-throwing API exception, no more retries\", ex);\r\n        throw toIOException(ex);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "invokeMethod",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 48,
  "sourceCodeText" : "Object invokeMethod(final UserGroupInformation ugi, final List<? extends FederationNamenodeContext> namenodes, final Class<?> protocol, final Method method, final Object... params) throws ConnectException, StandbyException, IOException\n{\r\n    if (namenodes == null || namenodes.isEmpty()) {\r\n        throw new IOException(\"No namenodes to invoke \" + method.getName() + \" with params \" + Arrays.deepToString(params) + \" from \" + router.getRouterId());\r\n    }\r\n    addClientIpToCallerContext();\r\n    Object ret = null;\r\n    if (rpcMonitor != null) {\r\n        rpcMonitor.proxyOp();\r\n    }\r\n    boolean failover = false;\r\n    Map<FederationNamenodeContext, IOException> ioes = new LinkedHashMap<>();\r\n    for (FederationNamenodeContext namenode : namenodes) {\r\n        ConnectionContext connection = null;\r\n        String nsId = namenode.getNameserviceId();\r\n        String rpcAddress = namenode.getRpcAddress();\r\n        try {\r\n            connection = this.getConnection(ugi, nsId, rpcAddress, protocol);\r\n            ProxyAndInfo<?> client = connection.getClient();\r\n            final Object proxy = client.getProxy();\r\n            ret = invoke(nsId, 0, method, proxy, params);\r\n            if (failover) {\r\n                InetSocketAddress address = client.getAddress();\r\n                namenodeResolver.updateActiveNamenode(nsId, address);\r\n            }\r\n            if (this.rpcMonitor != null) {\r\n                this.rpcMonitor.proxyOpComplete(true, nsId);\r\n            }\r\n            if (this.router.getRouterClientMetrics() != null) {\r\n                this.router.getRouterClientMetrics().incInvokedMethod(method);\r\n            }\r\n            return ret;\r\n        } catch (IOException ioe) {\r\n            ioes.put(namenode, ioe);\r\n            if (ioe instanceof StandbyException) {\r\n                if (this.rpcMonitor != null) {\r\n                    this.rpcMonitor.proxyOpFailureStandby(nsId);\r\n                }\r\n                failover = true;\r\n            } else if (isUnavailableException(ioe)) {\r\n                if (this.rpcMonitor != null) {\r\n                    this.rpcMonitor.proxyOpFailureCommunicate(nsId);\r\n                }\r\n                failover = true;\r\n            } else if (ioe instanceof RemoteException) {\r\n                if (this.rpcMonitor != null) {\r\n                    this.rpcMonitor.proxyOpComplete(true, nsId);\r\n                }\r\n                RemoteException re = (RemoteException) ioe;\r\n                ioe = re.unwrapRemoteException();\r\n                ioe = getCleanException(ioe);\r\n                throw ioe;\r\n            } else if (ioe instanceof ConnectionNullException) {\r\n                if (this.rpcMonitor != null) {\r\n                    this.rpcMonitor.proxyOpFailureCommunicate(nsId);\r\n                }\r\n                LOG.error(\"Get connection for {} {} error: {}\", nsId, rpcAddress, ioe.getMessage());\r\n                StandbyException se = new StandbyException(ioe.getMessage());\r\n                se.initCause(ioe);\r\n                throw se;\r\n            } else if (ioe instanceof NoNamenodesAvailableException) {\r\n                if (this.rpcMonitor != null) {\r\n                    this.rpcMonitor.proxyOpNoNamenodes(nsId);\r\n                }\r\n                LOG.error(\"Cannot get available namenode for {} {} error: {}\", nsId, rpcAddress, ioe.getMessage());\r\n                throw new RetriableException(ioe);\r\n            } else {\r\n                if (this.rpcMonitor != null) {\r\n                    this.rpcMonitor.proxyOpFailureCommunicate(nsId);\r\n                    this.rpcMonitor.proxyOpComplete(false, nsId);\r\n                }\r\n                throw ioe;\r\n            }\r\n        } finally {\r\n            if (connection != null) {\r\n                connection.release();\r\n            }\r\n        }\r\n    }\r\n    if (this.rpcMonitor != null) {\r\n        this.rpcMonitor.proxyOpComplete(false, null);\r\n    }\r\n    String msg = \"No namenode available to invoke \" + method.getName() + \" \" + Arrays.deepToString(params) + \" in \" + namenodes + \" from \" + router.getRouterId();\r\n    LOG.error(msg);\r\n    int exConnect = 0;\r\n    for (Entry<FederationNamenodeContext, IOException> entry : ioes.entrySet()) {\r\n        FederationNamenodeContext namenode = entry.getKey();\r\n        String nnKey = namenode.getNamenodeKey();\r\n        String addr = namenode.getRpcAddress();\r\n        IOException ioe = entry.getValue();\r\n        if (ioe instanceof StandbyException) {\r\n            LOG.error(\"{} at {} is in Standby: {}\", nnKey, addr, ioe.getMessage());\r\n        } else if (isUnavailableException(ioe)) {\r\n            exConnect++;\r\n            LOG.error(\"{} at {} cannot be reached: {}\", nnKey, addr, ioe.getMessage());\r\n        } else {\r\n            LOG.error(\"{} at {} error: \\\"{}\\\"\", nnKey, addr, ioe.getMessage());\r\n        }\r\n    }\r\n    if (exConnect == ioes.size()) {\r\n        throw new ConnectException(msg);\r\n    } else {\r\n        throw new StandbyException(msg);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "addClientIpToCallerContext",
  "errType" : null,
  "containingMethodsNum" : 9,
  "sourceCodeText" : "void addClientIpToCallerContext()\n{\r\n    CallerContext ctx = CallerContext.getCurrent();\r\n    String origContext = ctx == null ? null : ctx.getContext();\r\n    byte[] origSignature = ctx == null ? null : ctx.getSignature();\r\n    CallerContext.Builder builder = new CallerContext.Builder(\"\", contextFieldSeparator).append(CallerContext.CLIENT_IP_STR, Server.getRemoteAddress()).append(CallerContext.CLIENT_PORT_STR, Integer.toString(Server.getRemotePort())).setSignature(origSignature);\r\n    if (origContext != null) {\r\n        for (String part : origContext.split(contextFieldSeparator)) {\r\n            String[] keyValue = part.split(CallerContext.Builder.KEY_VALUE_SEPARATOR, 2);\r\n            if (keyValue.length == 2) {\r\n                builder.appendIfAbsent(keyValue[0], keyValue[1]);\r\n            } else if (keyValue.length == 1) {\r\n                builder.append(keyValue[0]);\r\n            }\r\n        }\r\n    }\r\n    CallerContext.setCurrent(builder.build());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "invoke",
  "errType" : [ "IllegalAccessException", "IllegalArgumentException", "InvocationTargetException" ],
  "containingMethodsNum" : 9,
  "sourceCodeText" : "Object invoke(String nsId, int retryCount, final Method method, final Object obj, final Object... params) throws IOException\n{\r\n    try {\r\n        return method.invoke(obj, params);\r\n    } catch (IllegalAccessException e) {\r\n        LOG.error(\"Unexpected exception while proxying API\", e);\r\n        return null;\r\n    } catch (IllegalArgumentException e) {\r\n        LOG.error(\"Unexpected exception while proxying API\", e);\r\n        return null;\r\n    } catch (InvocationTargetException e) {\r\n        Throwable cause = e.getCause();\r\n        if (cause instanceof IOException) {\r\n            IOException ioe = (IOException) cause;\r\n            RetryDecision decision = shouldRetry(ioe, retryCount, nsId);\r\n            if (decision == RetryDecision.RETRY) {\r\n                if (this.rpcMonitor != null) {\r\n                    this.rpcMonitor.proxyOpRetries();\r\n                }\r\n                return invoke(nsId, ++retryCount, method, obj, params);\r\n            } else if (decision == RetryDecision.FAILOVER_AND_RETRY) {\r\n                if (ioe instanceof StandbyException) {\r\n                    throw ioe;\r\n                } else if (isUnavailableException(ioe)) {\r\n                    throw ioe;\r\n                } else {\r\n                    throw new StandbyException(ioe.getMessage());\r\n                }\r\n            } else {\r\n                throw ioe;\r\n            }\r\n        } else {\r\n            throw new IOException(e);\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "isUnavailableException",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "boolean isUnavailableException(IOException ioe)\n{\r\n    if (ioe instanceof ConnectTimeoutException || ioe instanceof EOFException || ioe instanceof SocketException || ioe instanceof StandbyException) {\r\n        return true;\r\n    }\r\n    if (ioe instanceof RetriableException) {\r\n        Throwable cause = ioe.getCause();\r\n        if (cause instanceof NoNamenodesAvailableException) {\r\n            return true;\r\n        }\r\n    }\r\n    return false;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "isClusterUnAvailable",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "boolean isClusterUnAvailable(String nsId) throws IOException\n{\r\n    List<? extends FederationNamenodeContext> nnState = this.namenodeResolver.getNamenodesForNameserviceId(nsId);\r\n    if (nnState != null) {\r\n        for (FederationNamenodeContext nnContext : nnState) {\r\n            if (nnContext.getState() == FederationNamenodeServiceState.ACTIVE) {\r\n                return false;\r\n            }\r\n        }\r\n    }\r\n    return true;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "getCleanException",
  "errType" : [ "ReflectiveOperationException" ],
  "containingMethodsNum" : 20,
  "sourceCodeText" : "IOException getCleanException(IOException ioe)\n{\r\n    IOException ret = null;\r\n    String msg = ioe.getMessage();\r\n    Throwable cause = ioe.getCause();\r\n    StackTraceElement[] stackTrace = ioe.getStackTrace();\r\n    int index = msg.indexOf(\"\\n\");\r\n    if (index > 0) {\r\n        String[] msgSplit = msg.split(\"\\n\");\r\n        msg = msgSplit[0];\r\n        List<StackTraceElement> elements = new LinkedList<>();\r\n        for (int i = 1; i < msgSplit.length; i++) {\r\n            String line = msgSplit[i];\r\n            Matcher matcher = STACK_TRACE_PATTERN.matcher(line);\r\n            if (matcher.find()) {\r\n                String declaringClass = matcher.group(1);\r\n                String methodName = matcher.group(2);\r\n                String fileName = matcher.group(3);\r\n                int lineNumber = Integer.parseInt(matcher.group(4));\r\n                StackTraceElement element = new StackTraceElement(declaringClass, methodName, fileName, lineNumber);\r\n                elements.add(element);\r\n            }\r\n        }\r\n        stackTrace = elements.toArray(new StackTraceElement[elements.size()]);\r\n    }\r\n    if (ioe instanceof RemoteException) {\r\n        RemoteException re = (RemoteException) ioe;\r\n        ret = new RemoteException(re.getClassName(), msg);\r\n    } else {\r\n        Class<? extends IOException> ioeClass = ioe.getClass();\r\n        try {\r\n            Constructor<? extends IOException> constructor = ioeClass.getDeclaredConstructor(String.class);\r\n            ret = constructor.newInstance(msg);\r\n        } catch (ReflectiveOperationException e) {\r\n            LOG.error(\"Could not create exception {}\", ioeClass.getSimpleName(), e);\r\n            ret = ioe;\r\n        }\r\n    }\r\n    if (ret != null) {\r\n        ret.initCause(cause);\r\n        ret.setStackTrace(stackTrace);\r\n    }\r\n    return ret;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "invokeSingle",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "Object invokeSingle(final ExtendedBlock block, RemoteMethod method) throws IOException\n{\r\n    String bpId = block.getBlockPoolId();\r\n    return invokeSingleBlockPool(bpId, method);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "invokeSingleBlockPool",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "Object invokeSingleBlockPool(final String bpId, RemoteMethod method) throws IOException\n{\r\n    String nsId = getNameserviceForBlockPoolId(bpId);\r\n    return invokeSingle(nsId, method);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "invokeSingle",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "Object invokeSingle(final String nsId, RemoteMethod method) throws IOException\n{\r\n    UserGroupInformation ugi = RouterRpcServer.getRemoteUser();\r\n    acquirePermit(nsId, ugi, method);\r\n    try {\r\n        List<? extends FederationNamenodeContext> nns = getNamenodesForNameservice(nsId);\r\n        RemoteLocationContext loc = new RemoteLocation(nsId, \"/\", \"/\");\r\n        Class<?> proto = method.getProtocol();\r\n        Method m = method.getMethod();\r\n        Object[] params = method.getParams(loc);\r\n        return invokeMethod(ugi, nns, proto, m, params);\r\n    } finally {\r\n        releasePermit(nsId, ugi, method);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "invokeSingle",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "T invokeSingle(final String nsId, RemoteMethod method, Class<T> clazz) throws IOException\n{\r\n    @SuppressWarnings(\"unchecked\")\r\n    T ret = (T) invokeSingle(nsId, method);\r\n    return ret;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "invokeSingle",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "T invokeSingle(final ExtendedBlock extendedBlock, RemoteMethod method, Class<T> clazz) throws IOException\n{\r\n    String nsId = getNameserviceForBlockPoolId(extendedBlock.getBlockPoolId());\r\n    @SuppressWarnings(\"unchecked\")\r\n    T ret = (T) invokeSingle(nsId, method);\r\n    return ret;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "invokeSingle",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "T invokeSingle(final RemoteLocationContext location, RemoteMethod remoteMethod, Class<T> clazz) throws IOException\n{\r\n    List<RemoteLocationContext> locations = Collections.singletonList(location);\r\n    @SuppressWarnings(\"unchecked\")\r\n    T ret = (T) invokeSequential(locations, remoteMethod);\r\n    return ret;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "invokeSequential",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Object invokeSequential(final List<? extends RemoteLocationContext> locations, final RemoteMethod remoteMethod) throws IOException\n{\r\n    return invokeSequential(locations, remoteMethod, null, null);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "invokeSequential",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "T invokeSequential(final List<? extends RemoteLocationContext> locations, final RemoteMethod remoteMethod, Class<T> expectedResultClass, Object expectedResultValue) throws IOException\n{\r\n    return (T) invokeSequential(remoteMethod, locations, expectedResultClass, expectedResultValue).getResult();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "invokeSequential",
  "errType" : [ "IOException", "Exception" ],
  "containingMethodsNum" : 22,
  "sourceCodeText" : "RemoteResult invokeSequential(final RemoteMethod remoteMethod, final List<R> locations, Class<T> expectedResultClass, Object expectedResultValue) throws IOException\n{\r\n    final UserGroupInformation ugi = RouterRpcServer.getRemoteUser();\r\n    final Method m = remoteMethod.getMethod();\r\n    List<IOException> thrownExceptions = new ArrayList<>();\r\n    Object firstResult = null;\r\n    for (final RemoteLocationContext loc : locations) {\r\n        String ns = loc.getNameserviceId();\r\n        acquirePermit(ns, ugi, remoteMethod);\r\n        List<? extends FederationNamenodeContext> namenodes = getNamenodesForNameservice(ns);\r\n        try {\r\n            Class<?> proto = remoteMethod.getProtocol();\r\n            Object[] params = remoteMethod.getParams(loc);\r\n            Object result = invokeMethod(ugi, namenodes, proto, m, params);\r\n            if (isExpectedClass(expectedResultClass, result) && isExpectedValue(expectedResultValue, result)) {\r\n                @SuppressWarnings(\"unchecked\")\r\n                R location = (R) loc;\r\n                @SuppressWarnings(\"unchecked\")\r\n                T ret = (T) result;\r\n                return new RemoteResult<>(location, ret);\r\n            }\r\n            if (firstResult == null) {\r\n                firstResult = result;\r\n            }\r\n        } catch (IOException ioe) {\r\n            ioe = processException(ioe, loc);\r\n            thrownExceptions.add(ioe);\r\n        } catch (Exception e) {\r\n            LOG.error(\"Unexpected exception {} proxying {} to {}\", e.getClass(), m.getName(), ns, e);\r\n            IOException ioe = new IOException(\"Unexpected exception proxying API \" + e.getMessage(), e);\r\n            thrownExceptions.add(ioe);\r\n        } finally {\r\n            releasePermit(ns, ugi, remoteMethod);\r\n        }\r\n    }\r\n    if (!thrownExceptions.isEmpty()) {\r\n        for (int i = 0; i < thrownExceptions.size(); i++) {\r\n            IOException ioe = thrownExceptions.get(i);\r\n            if (isUnavailableException(ioe)) {\r\n                throw ioe;\r\n            }\r\n        }\r\n        throw thrownExceptions.get(0);\r\n    }\r\n    @SuppressWarnings(\"unchecked\")\r\n    T ret = (T) firstResult;\r\n    return new RemoteResult<>(locations.get(0), ret);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "processException",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "IOException processException(IOException ioe, RemoteLocationContext loc)\n{\r\n    if (ioe instanceof RemoteException) {\r\n        RemoteException re = (RemoteException) ioe;\r\n        String newMsg = processExceptionMsg(re.getMessage(), loc.getDest(), loc.getSrc());\r\n        RemoteException newException = new RemoteException(re.getClassName(), newMsg);\r\n        newException.setStackTrace(ioe.getStackTrace());\r\n        return newException;\r\n    }\r\n    if (ioe instanceof FileNotFoundException) {\r\n        String newMsg = processExceptionMsg(ioe.getMessage(), loc.getDest(), loc.getSrc());\r\n        FileNotFoundException newException = new FileNotFoundException(newMsg);\r\n        newException.setStackTrace(ioe.getStackTrace());\r\n        return newException;\r\n    }\r\n    if (ioe instanceof SnapshotException) {\r\n        String newMsg = processExceptionMsg(ioe.getMessage(), loc.getDest(), loc.getSrc());\r\n        SnapshotException newException = new SnapshotException(newMsg);\r\n        newException.setStackTrace(ioe.getStackTrace());\r\n        return newException;\r\n    }\r\n    return ioe;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "processExceptionMsg",
  "errType" : null,
  "containingMethodsNum" : 9,
  "sourceCodeText" : "String processExceptionMsg(final String msg, final String dst, final String src)\n{\r\n    if (dst.equals(src) || !dst.startsWith(\"/\") || !src.startsWith(\"/\")) {\r\n        return msg;\r\n    }\r\n    String newMsg = msg.replaceFirst(dst, src);\r\n    int minLen = Math.min(dst.length(), src.length());\r\n    for (int i = 0; newMsg.equals(msg) && i < minLen; i++) {\r\n        String dst1 = dst.substring(0, dst.length() - 1 - i);\r\n        String src1 = src.substring(0, src.length() - 1 - i);\r\n        newMsg = msg.replaceFirst(dst1, src1);\r\n    }\r\n    return newMsg;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "isExpectedClass",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "boolean isExpectedClass(Class<?> expectedClass, Object clazz)\n{\r\n    if (expectedClass == null) {\r\n        return true;\r\n    } else if (clazz == null) {\r\n        return false;\r\n    } else {\r\n        return expectedClass.isInstance(clazz);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "isExpectedValue",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "boolean isExpectedValue(Object expectedValue, Object value)\n{\r\n    if (expectedValue == null) {\r\n        return true;\r\n    } else if (value == null) {\r\n        return false;\r\n    } else {\r\n        return value.equals(expectedValue);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "invokeAll",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "boolean invokeAll(final Collection<T> locations, final RemoteMethod method) throws IOException\n{\r\n    Map<T, Boolean> results = invokeConcurrent(locations, method, false, false, Boolean.class);\r\n    return results.containsValue(true);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "invokeConcurrent",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void invokeConcurrent(final Collection<T> locations, final RemoteMethod method) throws IOException\n{\r\n    invokeConcurrent(locations, method, void.class);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "invokeConcurrent",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Map<T, R> invokeConcurrent(final Collection<T> locations, final RemoteMethod method, Class<R> clazz) throws IOException\n{\r\n    return invokeConcurrent(locations, method, false, false, clazz);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "invokeConcurrent",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void invokeConcurrent(final Collection<T> locations, final RemoteMethod method, boolean requireResponse, boolean standby) throws IOException\n{\r\n    invokeConcurrent(locations, method, requireResponse, standby, void.class);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "invokeConcurrent",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Map<T, R> invokeConcurrent(final Collection<T> locations, final RemoteMethod method, boolean requireResponse, boolean standby, Class<R> clazz) throws IOException\n{\r\n    return invokeConcurrent(locations, method, requireResponse, standby, -1, clazz);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "invokeConcurrent",
  "errType" : null,
  "containingMethodsNum" : 10,
  "sourceCodeText" : "Map<T, R> invokeConcurrent(final Collection<T> locations, final RemoteMethod method, boolean requireResponse, boolean standby, long timeOutMs, Class<R> clazz) throws IOException\n{\r\n    final List<RemoteResult<T, R>> results = invokeConcurrent(locations, method, standby, timeOutMs, clazz);\r\n    final Map<T, R> ret = new TreeMap<>();\r\n    final List<IOException> thrownExceptions = new ArrayList<>();\r\n    IOException firstUnavailableException = null;\r\n    for (final RemoteResult<T, R> result : results) {\r\n        if (result.hasException()) {\r\n            IOException ioe = result.getException();\r\n            thrownExceptions.add(ioe);\r\n            if (isUnavailableException(ioe)) {\r\n                firstUnavailableException = ioe;\r\n            }\r\n        }\r\n        if (result.hasResult()) {\r\n            ret.put(result.getLocation(), result.getResult());\r\n        }\r\n    }\r\n    if (!thrownExceptions.isEmpty()) {\r\n        if (requireResponse || ret.isEmpty()) {\r\n            if (firstUnavailableException != null) {\r\n                throw firstUnavailableException;\r\n            } else {\r\n                throw thrownExceptions.get(0);\r\n            }\r\n        }\r\n    }\r\n    return ret;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "invokeConcurrent",
  "errType" : [ "IOException", "RejectedExecutionException", "InterruptedException", "CancellationException", "ExecutionException" ],
  "containingMethodsNum" : 55,
  "sourceCodeText" : "List<RemoteResult<T, R>> invokeConcurrent(final Collection<T> locations, final RemoteMethod method, boolean standby, long timeOutMs, Class<R> clazz) throws IOException\n{\r\n    final UserGroupInformation ugi = RouterRpcServer.getRemoteUser();\r\n    final Method m = method.getMethod();\r\n    if (locations.isEmpty()) {\r\n        throw new IOException(\"No remote locations available\");\r\n    } else if (locations.size() == 1 && timeOutMs <= 0) {\r\n        T location = locations.iterator().next();\r\n        String ns = location.getNameserviceId();\r\n        acquirePermit(ns, ugi, method);\r\n        final List<? extends FederationNamenodeContext> namenodes = getNamenodesForNameservice(ns);\r\n        try {\r\n            Class<?> proto = method.getProtocol();\r\n            Object[] paramList = method.getParams(location);\r\n            R result = (R) invokeMethod(ugi, namenodes, proto, m, paramList);\r\n            RemoteResult<T, R> remoteResult = new RemoteResult<>(location, result);\r\n            return Collections.singletonList(remoteResult);\r\n        } catch (IOException ioe) {\r\n            throw processException(ioe, location);\r\n        } finally {\r\n            releasePermit(ns, ugi, method);\r\n        }\r\n    }\r\n    List<T> orderedLocations = new ArrayList<>();\r\n    List<Callable<Object>> callables = new ArrayList<>();\r\n    final Call originCall = Server.getCurCall().get();\r\n    final CallerContext originContext = CallerContext.getCurrent();\r\n    for (final T location : locations) {\r\n        String nsId = location.getNameserviceId();\r\n        final List<? extends FederationNamenodeContext> namenodes = getNamenodesForNameservice(nsId);\r\n        final Class<?> proto = method.getProtocol();\r\n        final Object[] paramList = method.getParams(location);\r\n        if (standby) {\r\n            for (final FederationNamenodeContext nn : namenodes) {\r\n                String nnId = nn.getNamenodeId();\r\n                final List<FederationNamenodeContext> nnList = Collections.singletonList(nn);\r\n                T nnLocation = location;\r\n                if (location instanceof RemoteLocation) {\r\n                    nnLocation = (T) new RemoteLocation(nsId, nnId, location.getDest());\r\n                }\r\n                orderedLocations.add(nnLocation);\r\n                callables.add(() -> {\r\n                    transferThreadLocalContext(originCall, originContext);\r\n                    return invokeMethod(ugi, nnList, proto, m, paramList);\r\n                });\r\n            }\r\n        } else {\r\n            orderedLocations.add(location);\r\n            callables.add(() -> {\r\n                transferThreadLocalContext(originCall, originContext);\r\n                return invokeMethod(ugi, namenodes, proto, m, paramList);\r\n            });\r\n        }\r\n    }\r\n    if (rpcMonitor != null) {\r\n        rpcMonitor.proxyOp();\r\n    }\r\n    if (this.router.getRouterClientMetrics() != null) {\r\n        this.router.getRouterClientMetrics().incInvokedConcurrent(m);\r\n    }\r\n    acquirePermit(CONCURRENT_NS, ugi, method);\r\n    try {\r\n        List<Future<Object>> futures = null;\r\n        if (timeOutMs > 0) {\r\n            futures = executorService.invokeAll(callables, timeOutMs, TimeUnit.MILLISECONDS);\r\n        } else {\r\n            futures = executorService.invokeAll(callables);\r\n        }\r\n        List<RemoteResult<T, R>> results = new ArrayList<>();\r\n        for (int i = 0; i < futures.size(); i++) {\r\n            T location = orderedLocations.get(i);\r\n            try {\r\n                Future<Object> future = futures.get(i);\r\n                R result = (R) future.get();\r\n                results.add(new RemoteResult<>(location, result));\r\n            } catch (CancellationException ce) {\r\n                T loc = orderedLocations.get(i);\r\n                String msg = \"Invocation to \\\"\" + loc + \"\\\" for \\\"\" + method.getMethodName() + \"\\\" timed out\";\r\n                LOG.error(msg);\r\n                IOException ioe = new SubClusterTimeoutException(msg);\r\n                results.add(new RemoteResult<>(location, ioe));\r\n            } catch (ExecutionException ex) {\r\n                Throwable cause = ex.getCause();\r\n                LOG.debug(\"Canot execute {} in {}: {}\", m.getName(), location, cause.getMessage());\r\n                IOException ioe = null;\r\n                if (cause instanceof IOException) {\r\n                    ioe = (IOException) cause;\r\n                } else {\r\n                    ioe = new IOException(\"Unhandled exception while proxying API \" + m.getName() + \": \" + cause.getMessage(), cause);\r\n                }\r\n                results.add(new RemoteResult<>(location, ioe));\r\n            }\r\n        }\r\n        return results;\r\n    } catch (RejectedExecutionException e) {\r\n        if (rpcMonitor != null) {\r\n            rpcMonitor.proxyOpFailureClientOverloaded();\r\n        }\r\n        int active = executorService.getActiveCount();\r\n        int total = executorService.getMaximumPoolSize();\r\n        String msg = \"Not enough client threads \" + active + \"/\" + total;\r\n        LOG.error(msg);\r\n        throw new StandbyException(\"Router \" + router.getRouterId() + \" is overloaded: \" + msg);\r\n    } catch (InterruptedException ex) {\r\n        LOG.error(\"Unexpected error while invoking API: {}\", ex.getMessage());\r\n        throw new IOException(\"Unexpected error while invoking API \" + ex.getMessage(), ex);\r\n    } finally {\r\n        releasePermit(CONCURRENT_NS, ugi, method);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 3,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "transferThreadLocalContext",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void transferThreadLocalContext(final Call originCall, final CallerContext originContext)\n{\r\n    Server.getCurCall().set(originCall);\r\n    CallerContext.setCurrent(originContext);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "getNamenodesForNameservice",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "List<? extends FederationNamenodeContext> getNamenodesForNameservice(final String nsId) throws IOException\n{\r\n    final List<? extends FederationNamenodeContext> namenodes = namenodeResolver.getNamenodesForNameserviceId(nsId);\r\n    if (namenodes == null || namenodes.isEmpty()) {\r\n        throw new IOException(\"Cannot locate a registered namenode for \" + nsId + \" from \" + router.getRouterId());\r\n    }\r\n    return namenodes;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "getNamenodesForBlockPoolId",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "List<? extends FederationNamenodeContext> getNamenodesForBlockPoolId(final String bpId) throws IOException\n{\r\n    List<? extends FederationNamenodeContext> namenodes = namenodeResolver.getNamenodesForBlockPoolId(bpId);\r\n    if (namenodes == null || namenodes.isEmpty()) {\r\n        throw new IOException(\"Cannot locate a registered namenode for \" + bpId + \" from \" + router.getRouterId());\r\n    }\r\n    return namenodes;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "getNameserviceForBlockPoolId",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "String getNameserviceForBlockPoolId(final String bpId) throws IOException\n{\r\n    List<? extends FederationNamenodeContext> namenodes = getNamenodesForBlockPoolId(bpId);\r\n    FederationNamenodeContext namenode = namenodes.get(0);\r\n    return namenode.getNameserviceId();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "acquirePermit",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void acquirePermit(final String nsId, final UserGroupInformation ugi, final RemoteMethod m) throws IOException\n{\r\n    if (routerRpcFairnessPolicyController != null) {\r\n        if (!routerRpcFairnessPolicyController.acquirePermit(nsId)) {\r\n            if (rpcMonitor != null) {\r\n                rpcMonitor.getRPCMetrics().incrProxyOpPermitRejected();\r\n            }\r\n            incrRejectedPermitForNs(nsId);\r\n            LOG.debug(\"Permit denied for ugi: {} for method: {}\", ugi, m.getMethodName());\r\n            String msg = \"Router \" + router.getRouterId() + \" is overloaded for NS: \" + nsId;\r\n            throw new StandbyException(msg);\r\n        }\r\n        incrAcceptedPermitForNs(nsId);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "releasePermit",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void releasePermit(final String nsId, final UserGroupInformation ugi, final RemoteMethod m)\n{\r\n    if (routerRpcFairnessPolicyController != null) {\r\n        routerRpcFairnessPolicyController.releasePermit(nsId);\r\n        LOG.trace(\"Permit released for ugi: {} for method: {}\", ugi, m.getMethodName());\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "getRouterRpcFairnessPolicyController",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "RouterRpcFairnessPolicyController getRouterRpcFairnessPolicyController()\n{\r\n    return routerRpcFairnessPolicyController;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "incrRejectedPermitForNs",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void incrRejectedPermitForNs(String ns)\n{\r\n    rejectedPermitsPerNs.computeIfAbsent(ns, k -> new LongAdder()).increment();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "getRejectedPermitForNs",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "Long getRejectedPermitForNs(String ns)\n{\r\n    return rejectedPermitsPerNs.containsKey(ns) ? rejectedPermitsPerNs.get(ns).longValue() : 0L;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "incrAcceptedPermitForNs",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void incrAcceptedPermitForNs(String ns)\n{\r\n    acceptedPermitsPerNs.computeIfAbsent(ns, k -> new LongAdder()).increment();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "getAcceptedPermitForNs",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "Long getAcceptedPermitForNs(String ns)\n{\r\n    return acceptedPermitsPerNs.containsKey(ns) ? acceptedPermitsPerNs.get(ns).longValue() : 0L;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\records",
  "methodName" : "getPartial",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "T getPartial()\n{\r\n    return this.partial;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\records",
  "methodName" : "matches",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "boolean matches(T other)\n{\r\n    if (this.partial == null) {\r\n        return false;\r\n    }\r\n    return this.partial.like(other);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\records",
  "methodName" : "toString",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String toString()\n{\r\n    return \"Checking: \" + this.partial;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\protocol\\impl\\pb",
  "methodName" : "getProto",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "RefreshMountTableEntriesRequestProto getProto()\n{\r\n    this.translator.getBuilder();\r\n    return this.translator.build();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\protocol\\impl\\pb",
  "methodName" : "setProto",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setProto(Message proto)\n{\r\n    this.translator.setProto(proto);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\protocol\\impl\\pb",
  "methodName" : "readInstance",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void readInstance(String base64String) throws IOException\n{\r\n    this.translator.readInstance(base64String);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "doGet",
  "errType" : [ "InterruptedException" ],
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void doGet(HttpServletRequest request, HttpServletResponse response) throws IOException\n{\r\n    final Map<String, String[]> pmap = request.getParameterMap();\r\n    final PrintWriter out = response.getWriter();\r\n    final InetAddress remoteAddress = InetAddress.getByName(request.getRemoteAddr());\r\n    final ServletContext context = getServletContext();\r\n    final Configuration conf = RouterHttpServer.getConfFromContext(context);\r\n    final UserGroupInformation ugi = getUGI(request, conf);\r\n    try {\r\n        ugi.doAs((PrivilegedExceptionAction<Object>) () -> {\r\n            Router router = RouterHttpServer.getRouterFromContext(context);\r\n            new RouterFsck(router, pmap, out, remoteAddress).fsck();\r\n            return null;\r\n        });\r\n    } catch (InterruptedException e) {\r\n        response.sendError(HttpURLConnection.HTTP_BAD_REQUEST, e.getMessage());\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "getUGI",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "UserGroupInformation getUGI(HttpServletRequest request, Configuration conf) throws IOException\n{\r\n    return JspHelper.getUGI(getServletContext(), request, conf);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\protocol\\impl\\pb",
  "methodName" : "getProto",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "GetRouterRegistrationResponseProto getProto()\n{\r\n    return this.translator.build();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\protocol\\impl\\pb",
  "methodName" : "setProto",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setProto(Message protocol)\n{\r\n    this.translator.setProto(protocol);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\protocol\\impl\\pb",
  "methodName" : "readInstance",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void readInstance(String base64String) throws IOException\n{\r\n    this.translator.readInstance(base64String);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\protocol\\impl\\pb",
  "methodName" : "getRouter",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "RouterState getRouter()\n{\r\n    RouterRecordProto proto = this.translator.getProtoOrBuilder().getRouter();\r\n    return new RouterStatePBImpl(proto);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\protocol\\impl\\pb",
  "methodName" : "setRouter",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void setRouter(RouterState router)\n{\r\n    if (router instanceof RouterStatePBImpl) {\r\n        RouterStatePBImpl routerPB = (RouterStatePBImpl) router;\r\n        RouterRecordProto routerProto = routerPB.getProto();\r\n        this.translator.getBuilder().setRouter(routerProto);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocolPB",
  "methodName" : "close",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void close()\n{\r\n    RPC.stopProxy(rpcProxy);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocolPB",
  "methodName" : "getUnderlyingProxyObject",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Object getUnderlyingProxyObject()\n{\r\n    return rpcProxy;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocolPB",
  "methodName" : "isMethodSupported",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "boolean isMethodSupported(String methodName) throws IOException\n{\r\n    return RpcClientUtil.isMethodSupported(rpcProxy, RouterAdminProtocolPB.class, RPC.RpcKind.RPC_PROTOCOL_BUFFER, RPC.getProtocolVersion(RouterAdminProtocolPB.class), methodName);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocolPB",
  "methodName" : "addMountTableEntry",
  "errType" : [ "ServiceException" ],
  "containingMethodsNum" : 3,
  "sourceCodeText" : "AddMountTableEntryResponse addMountTableEntry(AddMountTableEntryRequest request) throws IOException\n{\r\n    AddMountTableEntryRequestPBImpl requestPB = (AddMountTableEntryRequestPBImpl) request;\r\n    AddMountTableEntryRequestProto proto = requestPB.getProto();\r\n    try {\r\n        AddMountTableEntryResponseProto response = rpcProxy.addMountTableEntry(null, proto);\r\n        return new AddMountTableEntryResponsePBImpl(response);\r\n    } catch (ServiceException e) {\r\n        throw new IOException(ProtobufHelper.getRemoteException(e).getMessage());\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocolPB",
  "methodName" : "updateMountTableEntry",
  "errType" : [ "ServiceException" ],
  "containingMethodsNum" : 3,
  "sourceCodeText" : "UpdateMountTableEntryResponse updateMountTableEntry(UpdateMountTableEntryRequest request) throws IOException\n{\r\n    UpdateMountTableEntryRequestPBImpl requestPB = (UpdateMountTableEntryRequestPBImpl) request;\r\n    UpdateMountTableEntryRequestProto proto = requestPB.getProto();\r\n    try {\r\n        UpdateMountTableEntryResponseProto response = rpcProxy.updateMountTableEntry(null, proto);\r\n        return new UpdateMountTableEntryResponsePBImpl(response);\r\n    } catch (ServiceException e) {\r\n        throw new IOException(ProtobufHelper.getRemoteException(e).getMessage());\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocolPB",
  "methodName" : "removeMountTableEntry",
  "errType" : [ "ServiceException" ],
  "containingMethodsNum" : 3,
  "sourceCodeText" : "RemoveMountTableEntryResponse removeMountTableEntry(RemoveMountTableEntryRequest request) throws IOException\n{\r\n    RemoveMountTableEntryRequestPBImpl requestPB = (RemoveMountTableEntryRequestPBImpl) request;\r\n    RemoveMountTableEntryRequestProto proto = requestPB.getProto();\r\n    try {\r\n        RemoveMountTableEntryResponseProto responseProto = rpcProxy.removeMountTableEntry(null, proto);\r\n        return new RemoveMountTableEntryResponsePBImpl(responseProto);\r\n    } catch (ServiceException e) {\r\n        throw new IOException(ProtobufHelper.getRemoteException(e).getMessage());\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocolPB",
  "methodName" : "getMountTableEntries",
  "errType" : [ "ServiceException" ],
  "containingMethodsNum" : 3,
  "sourceCodeText" : "GetMountTableEntriesResponse getMountTableEntries(GetMountTableEntriesRequest request) throws IOException\n{\r\n    GetMountTableEntriesRequestPBImpl requestPB = (GetMountTableEntriesRequestPBImpl) request;\r\n    GetMountTableEntriesRequestProto proto = requestPB.getProto();\r\n    try {\r\n        GetMountTableEntriesResponseProto response = rpcProxy.getMountTableEntries(null, proto);\r\n        return new GetMountTableEntriesResponsePBImpl(response);\r\n    } catch (ServiceException e) {\r\n        throw new IOException(ProtobufHelper.getRemoteException(e).getMessage());\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocolPB",
  "methodName" : "enterSafeMode",
  "errType" : [ "ServiceException" ],
  "containingMethodsNum" : 3,
  "sourceCodeText" : "EnterSafeModeResponse enterSafeMode(EnterSafeModeRequest request) throws IOException\n{\r\n    EnterSafeModeRequestProto proto = EnterSafeModeRequestProto.newBuilder().build();\r\n    try {\r\n        EnterSafeModeResponseProto response = rpcProxy.enterSafeMode(null, proto);\r\n        return new EnterSafeModeResponsePBImpl(response);\r\n    } catch (ServiceException e) {\r\n        throw new IOException(ProtobufHelper.getRemoteException(e).getMessage());\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocolPB",
  "methodName" : "leaveSafeMode",
  "errType" : [ "ServiceException" ],
  "containingMethodsNum" : 3,
  "sourceCodeText" : "LeaveSafeModeResponse leaveSafeMode(LeaveSafeModeRequest request) throws IOException\n{\r\n    LeaveSafeModeRequestProto proto = LeaveSafeModeRequestProto.newBuilder().build();\r\n    try {\r\n        LeaveSafeModeResponseProto response = rpcProxy.leaveSafeMode(null, proto);\r\n        return new LeaveSafeModeResponsePBImpl(response);\r\n    } catch (ServiceException e) {\r\n        throw new IOException(ProtobufHelper.getRemoteException(e).getMessage());\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocolPB",
  "methodName" : "getSafeMode",
  "errType" : [ "ServiceException" ],
  "containingMethodsNum" : 3,
  "sourceCodeText" : "GetSafeModeResponse getSafeMode(GetSafeModeRequest request) throws IOException\n{\r\n    GetSafeModeRequestProto proto = GetSafeModeRequestProto.newBuilder().build();\r\n    try {\r\n        GetSafeModeResponseProto response = rpcProxy.getSafeMode(null, proto);\r\n        return new GetSafeModeResponsePBImpl(response);\r\n    } catch (ServiceException e) {\r\n        throw new IOException(ProtobufHelper.getRemoteException(e).getMessage());\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocolPB",
  "methodName" : "disableNameservice",
  "errType" : [ "ServiceException" ],
  "containingMethodsNum" : 3,
  "sourceCodeText" : "DisableNameserviceResponse disableNameservice(DisableNameserviceRequest request) throws IOException\n{\r\n    DisableNameserviceRequestPBImpl requestPB = (DisableNameserviceRequestPBImpl) request;\r\n    DisableNameserviceRequestProto proto = requestPB.getProto();\r\n    try {\r\n        DisableNameserviceResponseProto response = rpcProxy.disableNameservice(null, proto);\r\n        return new DisableNameserviceResponsePBImpl(response);\r\n    } catch (ServiceException e) {\r\n        throw new IOException(ProtobufHelper.getRemoteException(e).getMessage());\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocolPB",
  "methodName" : "enableNameservice",
  "errType" : [ "ServiceException" ],
  "containingMethodsNum" : 3,
  "sourceCodeText" : "EnableNameserviceResponse enableNameservice(EnableNameserviceRequest request) throws IOException\n{\r\n    EnableNameserviceRequestPBImpl requestPB = (EnableNameserviceRequestPBImpl) request;\r\n    EnableNameserviceRequestProto proto = requestPB.getProto();\r\n    try {\r\n        EnableNameserviceResponseProto response = rpcProxy.enableNameservice(null, proto);\r\n        return new EnableNameserviceResponsePBImpl(response);\r\n    } catch (ServiceException e) {\r\n        throw new IOException(ProtobufHelper.getRemoteException(e).getMessage());\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocolPB",
  "methodName" : "getDisabledNameservices",
  "errType" : [ "ServiceException" ],
  "containingMethodsNum" : 3,
  "sourceCodeText" : "GetDisabledNameservicesResponse getDisabledNameservices(GetDisabledNameservicesRequest request) throws IOException\n{\r\n    GetDisabledNameservicesRequestProto proto = GetDisabledNameservicesRequestProto.newBuilder().build();\r\n    try {\r\n        GetDisabledNameservicesResponseProto response = rpcProxy.getDisabledNameservices(null, proto);\r\n        return new GetDisabledNameservicesResponsePBImpl(response);\r\n    } catch (ServiceException e) {\r\n        throw new IOException(ProtobufHelper.getRemoteException(e).getMessage());\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocolPB",
  "methodName" : "refreshMountTableEntries",
  "errType" : [ "ServiceException" ],
  "containingMethodsNum" : 3,
  "sourceCodeText" : "RefreshMountTableEntriesResponse refreshMountTableEntries(RefreshMountTableEntriesRequest request) throws IOException\n{\r\n    RefreshMountTableEntriesRequestPBImpl requestPB = (RefreshMountTableEntriesRequestPBImpl) request;\r\n    RefreshMountTableEntriesRequestProto proto = requestPB.getProto();\r\n    try {\r\n        RefreshMountTableEntriesResponseProto response = rpcProxy.refreshMountTableEntries(null, proto);\r\n        return new RefreshMountTableEntriesResponsePBImpl(response);\r\n    } catch (ServiceException e) {\r\n        throw new IOException(ProtobufHelper.getRemoteException(e).getMessage());\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocolPB",
  "methodName" : "getDestination",
  "errType" : [ "ServiceException" ],
  "containingMethodsNum" : 3,
  "sourceCodeText" : "GetDestinationResponse getDestination(GetDestinationRequest request) throws IOException\n{\r\n    GetDestinationRequestPBImpl requestPB = (GetDestinationRequestPBImpl) request;\r\n    GetDestinationRequestProto proto = requestPB.getProto();\r\n    try {\r\n        GetDestinationResponseProto response = rpcProxy.getDestination(null, proto);\r\n        return new GetDestinationResponsePBImpl(response);\r\n    } catch (ServiceException e) {\r\n        throw new IOException(ProtobufHelper.getRemoteException(e).getMessage());\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocolPB",
  "methodName" : "refreshSuperUserGroupsConfiguration",
  "errType" : [ "ServiceException" ],
  "containingMethodsNum" : 4,
  "sourceCodeText" : "boolean refreshSuperUserGroupsConfiguration() throws IOException\n{\r\n    RefreshSuperUserGroupsConfigurationRequestProto proto = RefreshSuperUserGroupsConfigurationRequestProto.newBuilder().build();\r\n    try {\r\n        RefreshSuperUserGroupsConfigurationResponseProto response = rpcProxy.refreshSuperUserGroupsConfiguration(null, proto);\r\n        return new RefreshSuperUserGroupsConfigurationResponsePBImpl(response).getStatus();\r\n    } catch (ServiceException e) {\r\n        throw new IOException(ProtobufHelper.getRemoteException(e).getMessage());\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\protocol",
  "methodName" : "newInstance",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "LeaveSafeModeResponse newInstance() throws IOException\n{\r\n    return StateStoreSerializer.newRecord(LeaveSafeModeResponse.class);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\protocol",
  "methodName" : "newInstance",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "LeaveSafeModeResponse newInstance(boolean status) throws IOException\n{\r\n    LeaveSafeModeResponse response = newInstance();\r\n    response.setStatus(status);\r\n    return response;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\protocol",
  "methodName" : "getStatus",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean getStatus()",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\protocol",
  "methodName" : "setStatus",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setStatus(boolean result)",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\records",
  "methodName" : "newInstance",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "MountTable newInstance()\n{\r\n    MountTable record = StateStoreSerializer.newRecord(MountTable.class);\r\n    record.init();\r\n    return record;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\records",
  "methodName" : "newInstance",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "MountTable newInstance(final String src, final Map<String, String> destinations, long dateCreated, long dateModified) throws IOException\n{\r\n    MountTable record = newInstance(src, destinations);\r\n    record.setDateCreated(dateCreated);\r\n    record.setDateModified(dateModified);\r\n    return record;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\records",
  "methodName" : "newInstance",
  "errType" : null,
  "containingMethodsNum" : 17,
  "sourceCodeText" : "MountTable newInstance(final String src, final Map<String, String> destinations) throws IOException\n{\r\n    MountTable record = newInstance();\r\n    record.setSourcePath(normalizeFileSystemPath(src));\r\n    final List<RemoteLocation> locations = new LinkedList<>();\r\n    for (Entry<String, String> entry : destinations.entrySet()) {\r\n        String nsId = entry.getKey();\r\n        String path = normalizeFileSystemPath(entry.getValue());\r\n        RemoteLocation location = new RemoteLocation(nsId, path, src);\r\n        locations.add(location);\r\n    }\r\n    record.setDestinations(locations);\r\n    UserGroupInformation ugi = NameNode.getRemoteUser();\r\n    record.setOwnerName(ugi.getShortUserName());\r\n    String group = ugi.getGroupsSet().isEmpty() ? ugi.getShortUserName() : ugi.getPrimaryGroupName();\r\n    record.setGroupName(group);\r\n    record.setMode(new FsPermission(RouterPermissionChecker.MOUNT_TABLE_PERMISSION_DEFAULT));\r\n    RouterQuotaUsage quota = new RouterQuotaUsage.Builder().fileAndDirectoryCount(RouterQuotaUsage.QUOTA_USAGE_COUNT_DEFAULT).quota(HdfsConstants.QUOTA_RESET).spaceConsumed(RouterQuotaUsage.QUOTA_USAGE_COUNT_DEFAULT).spaceQuota(HdfsConstants.QUOTA_RESET).build();\r\n    record.setQuota(quota);\r\n    record.validate();\r\n    return record;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\records",
  "methodName" : "getSourcePath",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getSourcePath()",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\records",
  "methodName" : "setSourcePath",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setSourcePath(String path)",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\records",
  "methodName" : "getDestinations",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "List<RemoteLocation> getDestinations()",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\records",
  "methodName" : "setDestinations",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setDestinations(List<RemoteLocation> dests)",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\records",
  "methodName" : "addDestination",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean addDestination(String nsId, String path)",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\records",
  "methodName" : "isReadOnly",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean isReadOnly()",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\records",
  "methodName" : "setReadOnly",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setReadOnly(boolean ro)",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\records",
  "methodName" : "getDestOrder",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "DestinationOrder getDestOrder()",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\records",
  "methodName" : "setDestOrder",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setDestOrder(DestinationOrder order)",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\records",
  "methodName" : "isFaultTolerant",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean isFaultTolerant()",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\records",
  "methodName" : "setFaultTolerant",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setFaultTolerant(boolean faultTolerant)",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\records",
  "methodName" : "getOwnerName",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getOwnerName()",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\records",
  "methodName" : "setOwnerName",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setOwnerName(String owner)",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\records",
  "methodName" : "getGroupName",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getGroupName()",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\records",
  "methodName" : "setGroupName",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setGroupName(String group)",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\records",
  "methodName" : "getMode",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "FsPermission getMode()",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\records",
  "methodName" : "setMode",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setMode(FsPermission mode)",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\records",
  "methodName" : "getQuota",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "RouterQuotaUsage getQuota()",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\records",
  "methodName" : "setQuota",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setQuota(RouterQuotaUsage quota)",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\records",
  "methodName" : "getDefaultLocation",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "RemoteLocation getDefaultLocation()\n{\r\n    List<RemoteLocation> dests = this.getDestinations();\r\n    if (dests == null || dests.isEmpty()) {\r\n        return null;\r\n    }\r\n    return dests.get(0);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\records",
  "methodName" : "like",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "boolean like(final BaseRecord o)\n{\r\n    if (o instanceof MountTable) {\r\n        MountTable other = (MountTable) o;\r\n        if (getSourcePath() != null && !getSourcePath().equals(other.getSourcePath())) {\r\n            return false;\r\n        }\r\n        if (getDestinations() != null && !getDestinations().equals(other.getDestinations())) {\r\n            return false;\r\n        }\r\n        return true;\r\n    }\r\n    return false;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\records",
  "methodName" : "toString",
  "errType" : null,
  "containingMethodsNum" : 19,
  "sourceCodeText" : "String toString()\n{\r\n    StringBuilder sb = new StringBuilder();\r\n    sb.append(this.getSourcePath());\r\n    sb.append(\"->\");\r\n    List<RemoteLocation> destinations = this.getDestinations();\r\n    sb.append(destinations);\r\n    if (destinations != null && destinations.size() > 1) {\r\n        sb.append(\"[\").append(this.getDestOrder()).append(\"]\");\r\n    }\r\n    if (this.isReadOnly()) {\r\n        sb.append(\"[RO]\");\r\n    }\r\n    if (this.isFaultTolerant()) {\r\n        sb.append(\"[FT]\");\r\n    }\r\n    if (this.getOwnerName() != null) {\r\n        sb.append(\"[owner:\").append(this.getOwnerName()).append(\"]\");\r\n    }\r\n    if (this.getGroupName() != null) {\r\n        sb.append(\"[group:\").append(this.getGroupName()).append(\"]\");\r\n    }\r\n    if (this.getMode() != null) {\r\n        sb.append(\"[mode:\").append(this.getMode()).append(\"]\");\r\n    }\r\n    if (this.getQuota() != null) {\r\n        sb.append(\"[quota:\").append(this.getQuota()).append(\"]\");\r\n    }\r\n    return sb.toString();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\records",
  "methodName" : "getPrimaryKeys",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "SortedMap<String, String> getPrimaryKeys()\n{\r\n    SortedMap<String, String> map = new TreeMap<>();\r\n    map.put(\"sourcePath\", this.getSourcePath());\r\n    return map;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\records",
  "methodName" : "validate",
  "errType" : null,
  "containingMethodsNum" : 15,
  "sourceCodeText" : "void validate()\n{\r\n    super.validate();\r\n    if (this.getSourcePath() == null || this.getSourcePath().length() == 0) {\r\n        throw new IllegalArgumentException(ERROR_MSG_NO_SOURCE_PATH + this);\r\n    }\r\n    if (!this.getSourcePath().startsWith(\"/\")) {\r\n        throw new IllegalArgumentException(ERROR_MSG_MUST_START_WITH_BACK_SLASH + this);\r\n    }\r\n    if (this.getDestinations() == null || this.getDestinations().size() == 0) {\r\n        throw new IllegalArgumentException(ERROR_MSG_NO_DEST_PATH_SPECIFIED + this);\r\n    }\r\n    for (RemoteLocation loc : getDestinations()) {\r\n        String nsId = loc.getNameserviceId();\r\n        if (nsId == null || nsId.length() == 0) {\r\n            throw new IllegalArgumentException(ERROR_MSG_INVALID_DEST_NS + this);\r\n        }\r\n        if (loc.getDest() == null || loc.getDest().length() == 0) {\r\n            throw new IllegalArgumentException(ERROR_MSG_INVALID_DEST_PATH + this);\r\n        }\r\n        if (!loc.getDest().startsWith(\"/\")) {\r\n            throw new IllegalArgumentException(ERROR_MSG_ALL_DEST_MUST_START_WITH_BACK_SLASH + this);\r\n        }\r\n    }\r\n    if (isFaultTolerant()) {\r\n        if (getDestinations().size() < 2) {\r\n            throw new IllegalArgumentException(ERROR_MSG_FAULT_TOLERANT_MULTI_DEST + this);\r\n        }\r\n        if (!isAll()) {\r\n            throw new IllegalArgumentException(ERROR_MSG_FAULT_TOLERANT_ALL + this);\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\records",
  "methodName" : "getExpirationMs",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getExpirationMs()\n{\r\n    return 0;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\records",
  "methodName" : "hashCode",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "int hashCode()\n{\r\n    return new HashCodeBuilder(17, 31).append(this.getSourcePath()).append(this.getDestinations()).append(this.isReadOnly()).append(this.getDestOrder()).append(this.isFaultTolerant()).append(this.getQuota().getQuota()).append(this.getQuota().getSpaceQuota()).toHashCode();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\records",
  "methodName" : "equals",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "boolean equals(Object obj)\n{\r\n    if (obj instanceof MountTable) {\r\n        MountTable other = (MountTable) obj;\r\n        return new EqualsBuilder().append(this.getSourcePath(), other.getSourcePath()).append(this.getDestinations(), other.getDestinations()).append(this.isReadOnly(), other.isReadOnly()).append(this.getDestOrder(), other.getDestOrder()).append(this.isFaultTolerant(), other.isFaultTolerant()).append(this.getQuota().getQuota(), other.getQuota().getQuota()).append(this.getQuota().getSpaceQuota(), other.getQuota().getSpaceQuota()).isEquals();\r\n    }\r\n    return false;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\records",
  "methodName" : "isAll",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "boolean isAll()\n{\r\n    DestinationOrder order = getDestOrder();\r\n    return DestinationOrder.FOLDER_ALL.contains(order);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\records",
  "methodName" : "normalizeFileSystemPath",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String normalizeFileSystemPath(final String path)\n{\r\n    Path normalizedPath = new Path(path);\r\n    return normalizedPath.toString();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\records\\impl\\pb",
  "methodName" : "getProto",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "RouterRecordProto getProto()\n{\r\n    return this.translator.build();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\records\\impl\\pb",
  "methodName" : "setProto",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setProto(Message proto)\n{\r\n    this.translator.setProto(proto);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\records\\impl\\pb",
  "methodName" : "readInstance",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void readInstance(String base64String) throws IOException\n{\r\n    this.translator.readInstance(base64String);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\records\\impl\\pb",
  "methodName" : "setAddress",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void setAddress(String address)\n{\r\n    RouterRecordProto.Builder builder = this.translator.getBuilder();\r\n    if (address == null) {\r\n        builder.clearAddress();\r\n    } else {\r\n        builder.setAddress(address);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\records\\impl\\pb",
  "methodName" : "getAddress",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "String getAddress()\n{\r\n    RouterRecordProtoOrBuilder proto = this.translator.getProtoOrBuilder();\r\n    if (!proto.hasAddress()) {\r\n        return null;\r\n    }\r\n    return proto.getAddress();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\records\\impl\\pb",
  "methodName" : "setStateStoreVersion",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void setStateStoreVersion(StateStoreVersion version)\n{\r\n    RouterRecordProto.Builder builder = this.translator.getBuilder();\r\n    if (version instanceof StateStoreVersionPBImpl) {\r\n        StateStoreVersionPBImpl versionPB = (StateStoreVersionPBImpl) version;\r\n        StateStoreVersionRecordProto versionProto = (StateStoreVersionRecordProto) versionPB.getProto();\r\n        builder.setStateStoreVersion(versionProto);\r\n    } else {\r\n        builder.clearStateStoreVersion();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\records\\impl\\pb",
  "methodName" : "getStateStoreVersion",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "StateStoreVersion getStateStoreVersion() throws IOException\n{\r\n    RouterRecordProtoOrBuilder proto = this.translator.getProtoOrBuilder();\r\n    if (!proto.hasStateStoreVersion()) {\r\n        return null;\r\n    }\r\n    StateStoreVersionRecordProto versionProto = proto.getStateStoreVersion();\r\n    StateStoreVersion version = StateStoreSerializer.newRecord(StateStoreVersion.class);\r\n    if (version instanceof StateStoreVersionPBImpl) {\r\n        StateStoreVersionPBImpl versionPB = (StateStoreVersionPBImpl) version;\r\n        versionPB.setProto(versionProto);\r\n        return versionPB;\r\n    } else {\r\n        throw new IOException(\"Cannot get State Store version\");\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\records\\impl\\pb",
  "methodName" : "getStatus",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "RouterServiceState getStatus()\n{\r\n    RouterRecordProtoOrBuilder proto = this.translator.getProtoOrBuilder();\r\n    if (!proto.hasStatus()) {\r\n        return null;\r\n    }\r\n    return RouterServiceState.valueOf(proto.getStatus());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\records\\impl\\pb",
  "methodName" : "setStatus",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void setStatus(RouterServiceState newStatus)\n{\r\n    RouterRecordProto.Builder builder = this.translator.getBuilder();\r\n    if (newStatus == null) {\r\n        builder.clearStatus();\r\n    } else {\r\n        builder.setStatus(newStatus.toString());\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\records\\impl\\pb",
  "methodName" : "getVersion",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "String getVersion()\n{\r\n    RouterRecordProtoOrBuilder proto = this.translator.getProtoOrBuilder();\r\n    if (!proto.hasVersion()) {\r\n        return null;\r\n    }\r\n    return proto.getVersion();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\records\\impl\\pb",
  "methodName" : "setVersion",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void setVersion(String version)\n{\r\n    RouterRecordProto.Builder builder = this.translator.getBuilder();\r\n    if (version == null) {\r\n        builder.clearVersion();\r\n    } else {\r\n        builder.setVersion(version);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\records\\impl\\pb",
  "methodName" : "getCompileInfo",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "String getCompileInfo()\n{\r\n    RouterRecordProtoOrBuilder proto = this.translator.getProtoOrBuilder();\r\n    if (!proto.hasCompileInfo()) {\r\n        return null;\r\n    }\r\n    return proto.getCompileInfo();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\records\\impl\\pb",
  "methodName" : "setCompileInfo",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void setCompileInfo(String info)\n{\r\n    RouterRecordProto.Builder builder = this.translator.getBuilder();\r\n    if (info == null) {\r\n        builder.clearCompileInfo();\r\n    } else {\r\n        builder.setCompileInfo(info);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\records\\impl\\pb",
  "methodName" : "setDateStarted",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setDateStarted(long dateStarted)\n{\r\n    this.translator.getBuilder().setDateStarted(dateStarted);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\records\\impl\\pb",
  "methodName" : "getDateStarted",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "long getDateStarted()\n{\r\n    return this.translator.getProtoOrBuilder().getDateStarted();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\records\\impl\\pb",
  "methodName" : "setDateModified",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void setDateModified(long time)\n{\r\n    if (getStatus() != RouterServiceState.EXPIRED) {\r\n        this.translator.getBuilder().setDateModified(time);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\records\\impl\\pb",
  "methodName" : "getDateModified",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "long getDateModified()\n{\r\n    return this.translator.getProtoOrBuilder().getDateModified();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\records\\impl\\pb",
  "methodName" : "setDateCreated",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setDateCreated(long time)\n{\r\n    this.translator.getBuilder().setDateCreated(time);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\records\\impl\\pb",
  "methodName" : "getDateCreated",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "long getDateCreated()\n{\r\n    return this.translator.getProtoOrBuilder().getDateCreated();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\records\\impl\\pb",
  "methodName" : "setAdminAddress",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setAdminAddress(String adminAddress)\n{\r\n    this.translator.getBuilder().setAdminAddress(adminAddress);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\records\\impl\\pb",
  "methodName" : "getAdminAddress",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String getAdminAddress()\n{\r\n    return this.translator.getProtoOrBuilder().getAdminAddress();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\protocol",
  "methodName" : "newInstance",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "GetRouterRegistrationsRequest newInstance() throws IOException\n{\r\n    return StateStoreSerializer.newRecord(GetRouterRegistrationsRequest.class);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\protocol\\impl\\pb",
  "methodName" : "getProto",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "GetSafeModeResponseProto getProto()\n{\r\n    return translator.build();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\protocol\\impl\\pb",
  "methodName" : "setProto",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setProto(Message proto)\n{\r\n    this.translator.setProto(proto);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\protocol\\impl\\pb",
  "methodName" : "readInstance",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void readInstance(String base64String) throws IOException\n{\r\n    this.translator.readInstance(base64String);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\protocol\\impl\\pb",
  "methodName" : "isInSafeMode",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "boolean isInSafeMode()\n{\r\n    return this.translator.getProtoOrBuilder().getIsInSafeMode();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\protocol\\impl\\pb",
  "methodName" : "setSafeMode",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setSafeMode(boolean isInSafeMode)\n{\r\n    this.translator.getBuilder().setIsInSafeMode(isInSafeMode);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\protocol\\impl\\pb",
  "methodName" : "getProto",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "DisableNameserviceRequestProto getProto()\n{\r\n    return translator.build();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\protocol\\impl\\pb",
  "methodName" : "setProto",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setProto(Message proto)\n{\r\n    this.translator.setProto(proto);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\protocol\\impl\\pb",
  "methodName" : "readInstance",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void readInstance(String base64String) throws IOException\n{\r\n    this.translator.readInstance(base64String);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\protocol\\impl\\pb",
  "methodName" : "getNameServiceId",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String getNameServiceId()\n{\r\n    return this.translator.getProtoOrBuilder().getNameServiceId();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\protocol\\impl\\pb",
  "methodName" : "setNameServiceId",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setNameServiceId(String nsId)\n{\r\n    this.translator.getBuilder().setNameServiceId(nsId);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "create",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "RouterClientMetrics create(Configuration conf)\n{\r\n    String sessionId = conf.get(DFSConfigKeys.DFS_METRICS_SESSION_ID_KEY);\r\n    String processName = \"Router\";\r\n    MetricsSystem ms = DefaultMetricsSystem.instance();\r\n    return ms.register(new RouterClientMetrics(processName, sessionId));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "shutdown",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void shutdown()\n{\r\n    DefaultMetricsSystem.shutdown();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "incInvokedMethod",
  "errType" : null,
  "containingMethodsNum" : 108,
  "sourceCodeText" : "void incInvokedMethod(Method method)\n{\r\n    switch(method.getName()) {\r\n        case \"getBlockLocations\":\r\n            getBlockLocationsOps.incr();\r\n            break;\r\n        case \"getServerDefaults\":\r\n            getServerDefaultsOps.incr();\r\n            break;\r\n        case \"create\":\r\n            createOps.incr();\r\n            break;\r\n        case \"append\":\r\n            appendOps.incr();\r\n            break;\r\n        case \"recoverLease\":\r\n            recoverLeaseOps.incr();\r\n            break;\r\n        case \"setReplication\":\r\n            setReplicationOps.incr();\r\n            break;\r\n        case \"setStoragePolicy\":\r\n            setStoragePolicyOps.incr();\r\n            break;\r\n        case \"getStoragePolicies\":\r\n            getStoragePoliciesOps.incr();\r\n            break;\r\n        case \"setPermission\":\r\n            setPermissionOps.incr();\r\n            break;\r\n        case \"setOwner\":\r\n            setOwnerOps.incr();\r\n            break;\r\n        case \"addBlock\":\r\n            addBlockOps.incr();\r\n            break;\r\n        case \"getAdditionalDatanode\":\r\n            getAdditionalDatanodeOps.incr();\r\n            break;\r\n        case \"abandonBlock\":\r\n            abandonBlockOps.incr();\r\n            break;\r\n        case \"complete\":\r\n            completeOps.incr();\r\n            break;\r\n        case \"updateBlockForPipeline\":\r\n            updateBlockForPipelineOps.incr();\r\n            break;\r\n        case \"updatePipeline\":\r\n            updatePipelineOps.incr();\r\n            break;\r\n        case \"getPreferredBlockSize\":\r\n            getPreferredBlockSizeOps.incr();\r\n            break;\r\n        case \"rename\":\r\n            renameOps.incr();\r\n            break;\r\n        case \"rename2\":\r\n            rename2Ops.incr();\r\n            break;\r\n        case \"concat\":\r\n            concatOps.incr();\r\n            break;\r\n        case \"truncate\":\r\n            truncateOps.incr();\r\n            break;\r\n        case \"delete\":\r\n            deleteOps.incr();\r\n            break;\r\n        case \"mkdirs\":\r\n            mkdirsOps.incr();\r\n            break;\r\n        case \"renewLease\":\r\n            renewLeaseOps.incr();\r\n            break;\r\n        case \"getListing\":\r\n            getListingOps.incr();\r\n            break;\r\n        case \"getBatchedListing\":\r\n            getBatchedListingOps.incr();\r\n            break;\r\n        case \"getFileInfo\":\r\n            getFileInfoOps.incr();\r\n            break;\r\n        case \"isFileClosed\":\r\n            isFileClosedOps.incr();\r\n            break;\r\n        case \"getFileLinkInfo\":\r\n            getFileLinkInfoOps.incr();\r\n            break;\r\n        case \"getLocatedFileInfo\":\r\n            getLocatedFileInfoOps.incr();\r\n            break;\r\n        case \"getStats\":\r\n            getStatsOps.incr();\r\n            break;\r\n        case \"getDatanodeReport\":\r\n            getDatanodeReportOps.incr();\r\n            break;\r\n        case \"getDatanodeStorageReport\":\r\n            getDatanodeStorageReportOps.incr();\r\n            break;\r\n        case \"setSafeMode\":\r\n            setSafeModeOps.incr();\r\n            break;\r\n        case \"restoreFailedStorage\":\r\n            restoreFailedStorageOps.incr();\r\n            break;\r\n        case \"saveNamespace\":\r\n            saveNamespaceOps.incr();\r\n            break;\r\n        case \"rollEdits\":\r\n            rollEditsOps.incr();\r\n            break;\r\n        case \"refreshNodes\":\r\n            refreshNodesOps.incr();\r\n            break;\r\n        case \"finalizeUpgrade\":\r\n            finalizeUpgradeOps.incr();\r\n            break;\r\n        case \"upgradeStatus\":\r\n            upgradeStatusOps.incr();\r\n            break;\r\n        case \"rollingUpgrade\":\r\n            rollingUpgradeOps.incr();\r\n            break;\r\n        case \"metaSave\":\r\n            metaSaveOps.incr();\r\n            break;\r\n        case \"listCorruptFileBlocks\":\r\n            listCorruptFileBlocksOps.incr();\r\n            break;\r\n        case \"setBalancerBandwidth\":\r\n            setBalancerBandwidthOps.incr();\r\n            break;\r\n        case \"getContentSummary\":\r\n            getContentSummaryOps.incr();\r\n            break;\r\n        case \"fsync\":\r\n            fsyncOps.incr();\r\n            break;\r\n        case \"setTimes\":\r\n            setTimesOps.incr();\r\n            break;\r\n        case \"createSymlink\":\r\n            createSymlinkOps.incr();\r\n            break;\r\n        case \"getLinkTarget\":\r\n            getLinkTargetOps.incr();\r\n            break;\r\n        case \"allowSnapshot\":\r\n            allowSnapshotOps.incr();\r\n            break;\r\n        case \"disallowSnapshot\":\r\n            disallowSnapshotOps.incr();\r\n            break;\r\n        case \"renameSnapshot\":\r\n            renameSnapshotOps.incr();\r\n            break;\r\n        case \"getSnapshottableDirListing\":\r\n            getSnapshottableDirListingOps.incr();\r\n            break;\r\n        case \"getSnapshotListing\":\r\n            getSnapshotListingOps.incr();\r\n            break;\r\n        case \"getSnapshotDiffReport\":\r\n            getSnapshotDiffReportOps.incr();\r\n            break;\r\n        case \"getSnapshotDiffReportListing\":\r\n            getSnapshotDiffReportListingOps.incr();\r\n            break;\r\n        case \"addCacheDirective\":\r\n            addCacheDirectiveOps.incr();\r\n            break;\r\n        case \"modifyCacheDirective\":\r\n            modifyCacheDirectiveOps.incr();\r\n            break;\r\n        case \"removeCacheDirective\":\r\n            removeCacheDirectiveOps.incr();\r\n            break;\r\n        case \"listCacheDirectives\":\r\n            listCacheDirectivesOps.incr();\r\n            break;\r\n        case \"addCachePool\":\r\n            addCachePoolOps.incr();\r\n            break;\r\n        case \"modifyCachePool\":\r\n            modifyCachePoolOps.incr();\r\n            break;\r\n        case \"removeCachePool\":\r\n            removeCachePoolOps.incr();\r\n            break;\r\n        case \"listCachePools\":\r\n            listCachePoolsOps.incr();\r\n            break;\r\n        case \"modifyAclEntries\":\r\n            modifyAclEntriesOps.incr();\r\n            break;\r\n        case \"removeAclEntries\":\r\n            removeAclEntriesOps.incr();\r\n            break;\r\n        case \"removeDefaultAcl\":\r\n            removeDefaultAclOps.incr();\r\n            break;\r\n        case \"removeAcl\":\r\n            removeAclOps.incr();\r\n            break;\r\n        case \"setAcl\":\r\n            setAclOps.incr();\r\n            break;\r\n        case \"getAclStatus\":\r\n            getAclStatusOps.incr();\r\n            break;\r\n        case \"createEncryptionZone\":\r\n            createEncryptionZoneOps.incr();\r\n            break;\r\n        case \"getEZForPath\":\r\n            getEZForPathOps.incr();\r\n            break;\r\n        case \"listEncryptionZones\":\r\n            listEncryptionZonesOps.incr();\r\n            break;\r\n        case \"reencryptEncryptionZone\":\r\n            reencryptEncryptionZoneOps.incr();\r\n            break;\r\n        case \"listReencryptionStatus\":\r\n            listReencryptionStatusOps.incr();\r\n            break;\r\n        case \"setXAttr\":\r\n            setXAttrOps.incr();\r\n            break;\r\n        case \"getXAttrs\":\r\n            getXAttrsOps.incr();\r\n            break;\r\n        case \"listXAttrs\":\r\n            listXAttrsOps.incr();\r\n            break;\r\n        case \"removeXAttr\":\r\n            removeXAttrsOps.incr();\r\n            break;\r\n        case \"checkAccess\":\r\n            checkAccessOps.incr();\r\n            break;\r\n        case \"getCurrentEditLogTxid\":\r\n            getCurrentEditLogTxidOps.incr();\r\n            break;\r\n        case \"getEditsFromTxid\":\r\n            getEditsFromTxidOps.incr();\r\n            break;\r\n        case \"getDataEncryptionKey\":\r\n            getDataEncryptionKeyOps.incr();\r\n            break;\r\n        case \"createSnapshot\":\r\n            createSnapshotOps.incr();\r\n            break;\r\n        case \"deleteSnapshot\":\r\n            deleteSnapshotOps.incr();\r\n            break;\r\n        case \"setQuota\":\r\n            setQuotaOps.incr();\r\n            break;\r\n        case \"getQuotaUsage\":\r\n            getQuotaUsageOps.incr();\r\n            break;\r\n        case \"reportBadBlocks\":\r\n            reportBadBlocksOps.incr();\r\n            break;\r\n        case \"unsetStoragePolicy\":\r\n            unsetStoragePolicyOps.incr();\r\n            break;\r\n        case \"getStoragePolicy\":\r\n            getStoragePolicyOps.incr();\r\n            break;\r\n        case \"getErasureCodingPolicies\":\r\n            getErasureCodingPoliciesOps.incr();\r\n            break;\r\n        case \"getErasureCodingCodecs\":\r\n            getErasureCodingCodecsOps.incr();\r\n            break;\r\n        case \"addErasureCodingPolicies\":\r\n            addErasureCodingPoliciesOps.incr();\r\n            break;\r\n        case \"removeErasureCodingPolicy\":\r\n            removeErasureCodingPolicyOps.incr();\r\n            break;\r\n        case \"disableErasureCodingPolicy\":\r\n            disableErasureCodingPolicyOps.incr();\r\n            break;\r\n        case \"enableErasureCodingPolicy\":\r\n            enableErasureCodingPolicyOps.incr();\r\n            break;\r\n        case \"getErasureCodingPolicy\":\r\n            getErasureCodingPolicyOps.incr();\r\n            break;\r\n        case \"setErasureCodingPolicy\":\r\n            setErasureCodingPolicyOps.incr();\r\n            break;\r\n        case \"unsetErasureCodingPolicy\":\r\n            unsetErasureCodingPolicyOps.incr();\r\n            break;\r\n        case \"getECTopologyResultForPolicies\":\r\n            getECTopologyResultForPoliciesOps.incr();\r\n            break;\r\n        case \"getECBlockGroupStats\":\r\n            getECBlockGroupStatsOps.incr();\r\n            break;\r\n        case \"getReplicatedBlockStats\":\r\n            getReplicatedBlockStatsOps.incr();\r\n            break;\r\n        case \"listOpenFiles\":\r\n            listOpenFilesOps.incr();\r\n            break;\r\n        case \"msync\":\r\n            msyncOps.incr();\r\n            break;\r\n        case \"satisfyStoragePolicy\":\r\n            satisfyStoragePolicyOps.incr();\r\n            break;\r\n        case \"getHAServiceState\":\r\n            getHAServiceStateOps.incr();\r\n            break;\r\n        default:\r\n            otherOps.incr();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "incInvokedConcurrent",
  "errType" : null,
  "containingMethodsNum" : 36,
  "sourceCodeText" : "void incInvokedConcurrent(Method method)\n{\r\n    switch(method.getName()) {\r\n        case \"setReplication\":\r\n            concurrentSetReplicationOps.incr();\r\n            break;\r\n        case \"setPermission\":\r\n            concurrentSetPermissionOps.incr();\r\n            break;\r\n        case \"setOwner\":\r\n            concurrentSetOwnerOps.incr();\r\n            break;\r\n        case \"rename\":\r\n            concurrentRenameOps.incr();\r\n            break;\r\n        case \"rename2\":\r\n            concurrentRename2Ops.incr();\r\n            break;\r\n        case \"delete\":\r\n            concurrentDeleteOps.incr();\r\n            break;\r\n        case \"mkdirs\":\r\n            concurrentMkdirsOps.incr();\r\n            break;\r\n        case \"renewLease\":\r\n            concurrentRenewLeaseOps.incr();\r\n            break;\r\n        case \"getListing\":\r\n            concurrentGetListingOps.incr();\r\n            break;\r\n        case \"getFileInfo\":\r\n            concurrentGetFileInfoOps.incr();\r\n            break;\r\n        case \"getStats\":\r\n            concurrentGetStatsOps.incr();\r\n            break;\r\n        case \"getDatanodeReport\":\r\n            concurrentGetDatanodeReportOps.incr();\r\n            break;\r\n        case \"setSafeMode\":\r\n            concurrentSetSafeModeOps.incr();\r\n            break;\r\n        case \"restoreFailedStorage\":\r\n            concurrentRestoreFailedStorageOps.incr();\r\n            break;\r\n        case \"saveNamespace\":\r\n            concurrentSaveNamespaceOps.incr();\r\n            break;\r\n        case \"rollEdits\":\r\n            concurrentRollEditsOps.incr();\r\n            break;\r\n        case \"refreshNodes\":\r\n            concurrentRefreshNodesOps.incr();\r\n            break;\r\n        case \"finalizeUpgrade\":\r\n            concurrentFinalizeUpgradeOps.incr();\r\n            break;\r\n        case \"rollingUpgrade\":\r\n            concurrentRollingUpgradeOps.incr();\r\n            break;\r\n        case \"metaSave\":\r\n            concurrentMetaSaveOps.incr();\r\n            break;\r\n        case \"listCorruptFileBlocks\":\r\n            concurrentListCorruptFileBlocksOps.incr();\r\n            break;\r\n        case \"setBalancerBandwidth\":\r\n            concurrentSetBalancerBandwidthOps.incr();\r\n            break;\r\n        case \"getContentSummary\":\r\n            concurrentGetContentSummaryOps.incr();\r\n            break;\r\n        case \"modifyAclEntries\":\r\n            concurrentModifyAclEntriesOps.incr();\r\n            break;\r\n        case \"removeAclEntries\":\r\n            concurrentRemoveAclEntriesOps.incr();\r\n            break;\r\n        case \"removeDefaultAcl\":\r\n            concurrentRemoveDefaultAclOps.incr();\r\n            break;\r\n        case \"removeAcl\":\r\n            concurrentRemoveAclOps.incr();\r\n            break;\r\n        case \"setAcl\":\r\n            concurrentSetAclOps.incr();\r\n            break;\r\n        case \"setXAttr\":\r\n            concurrentSetXAttrOps.incr();\r\n            break;\r\n        case \"removeXAttr\":\r\n            concurrentRemoveXAttrOps.incr();\r\n            break;\r\n        case \"getCurrentEditLogTxid\":\r\n            concurrentGetCurrentEditLogTxidOps.incr();\r\n            break;\r\n        case \"getReplicatedBlockStats\":\r\n            concurrentGetReplicatedBlockStatsOps.incr();\r\n            break;\r\n        case \"setQuota\":\r\n            concurrentSetQuotaOps.incr();\r\n            break;\r\n        case \"getQuotaUsage\":\r\n            concurrentGetQuotaUsageOps.incr();\r\n            break;\r\n        default:\r\n            concurrentOtherOps.incr();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "verifyNamespaceQuota",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void verifyNamespaceQuota() throws NSQuotaExceededException\n{\r\n    long quota = getQuota();\r\n    long fileAndDirectoryCount = getFileAndDirectoryCount();\r\n    if (Quota.isViolated(quota, fileAndDirectoryCount)) {\r\n        throw new NSQuotaExceededException(quota, fileAndDirectoryCount);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "verifyStoragespaceQuota",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void verifyStoragespaceQuota() throws DSQuotaExceededException\n{\r\n    long spaceQuota = getSpaceQuota();\r\n    long spaceConsumed = getSpaceConsumed();\r\n    if (Quota.isViolated(spaceQuota, spaceConsumed)) {\r\n        throw new DSQuotaExceededException(spaceQuota, spaceConsumed);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "verifyQuotaByStorageType",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void verifyQuotaByStorageType() throws DSQuotaExceededException\n{\r\n    for (StorageType t : StorageType.getTypesSupportingQuota()) {\r\n        long typeQuota = getTypeQuota(t);\r\n        if (typeQuota == HdfsConstants.QUOTA_RESET) {\r\n            continue;\r\n        }\r\n        long typeConsumed = getTypeConsumed(t);\r\n        if (Quota.isViolated(typeQuota, typeConsumed)) {\r\n            throw new DSQuotaExceededException(typeQuota, typeConsumed);\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "toString",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "String toString()\n{\r\n    String nsQuota = \"-\";\r\n    String nsCount = \"-\";\r\n    long quota = getQuota();\r\n    if (quota != HdfsConstants.QUOTA_RESET) {\r\n        nsQuota = String.valueOf(quota);\r\n        nsCount = String.valueOf(getFileAndDirectoryCount());\r\n    }\r\n    String ssQuota = \"-\";\r\n    String ssCount = \"-\";\r\n    long spaceQuota = getSpaceQuota();\r\n    if (spaceQuota != HdfsConstants.QUOTA_RESET) {\r\n        ssQuota = StringUtils.byteDesc(spaceQuota);\r\n        ssCount = StringUtils.byteDesc(getSpaceConsumed());\r\n    }\r\n    StringBuilder str = new StringBuilder();\r\n    str.append(\"[NsQuota: \").append(nsQuota).append(\"/\").append(nsCount).append(\", SsQuota: \").append(ssQuota).append(\"/\").append(ssCount).append(\"]\");\r\n    return str.toString();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\metrics",
  "methodName" : "create",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "StateStoreMetrics create(Configuration conf)\n{\r\n    MetricsSystem ms = DefaultMetricsSystem.instance();\r\n    return ms.register(new StateStoreMetrics(conf));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\metrics",
  "methodName" : "shutdown",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void shutdown()\n{\r\n    DefaultMetricsSystem.shutdown();\r\n    reset();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\metrics",
  "methodName" : "addRead",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void addRead(long latency)\n{\r\n    reads.add(latency);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\metrics",
  "methodName" : "getReadOps",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "long getReadOps()\n{\r\n    return reads.lastStat().numSamples();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\metrics",
  "methodName" : "getReadAvg",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "double getReadAvg()\n{\r\n    return reads.lastStat().mean();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\metrics",
  "methodName" : "addWrite",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void addWrite(long latency)\n{\r\n    writes.add(latency);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\metrics",
  "methodName" : "getWriteOps",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "long getWriteOps()\n{\r\n    return writes.lastStat().numSamples();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\metrics",
  "methodName" : "getWriteAvg",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "double getWriteAvg()\n{\r\n    return writes.lastStat().mean();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\metrics",
  "methodName" : "addFailure",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void addFailure(long latency)\n{\r\n    failures.add(latency);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\metrics",
  "methodName" : "getFailureOps",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "long getFailureOps()\n{\r\n    return failures.lastStat().numSamples();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\metrics",
  "methodName" : "getFailureAvg",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "double getFailureAvg()\n{\r\n    return failures.lastStat().mean();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\metrics",
  "methodName" : "addRemove",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void addRemove(long latency)\n{\r\n    removes.add(latency);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\metrics",
  "methodName" : "getRemoveOps",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "long getRemoveOps()\n{\r\n    return removes.lastStat().numSamples();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\metrics",
  "methodName" : "getRemoveAvg",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "double getRemoveAvg()\n{\r\n    return removes.lastStat().mean();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\metrics",
  "methodName" : "setCacheSize",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void setCacheSize(String name, int size)\n{\r\n    String counterName = \"Cache\" + name + \"Size\";\r\n    MutableGaugeInt counter = cacheSizes.get(counterName);\r\n    if (counter == null) {\r\n        counter = registry.newGauge(counterName, name, size);\r\n        cacheSizes.put(counterName, counter);\r\n    }\r\n    counter.set(size);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\metrics",
  "methodName" : "reset",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void reset()\n{\r\n    reads.resetMinMax();\r\n    writes.resetMinMax();\r\n    removes.resetMinMax();\r\n    failures.resetMinMax();\r\n    reads.lastStat().reset();\r\n    writes.lastStat().reset();\r\n    removes.lastStat().reset();\r\n    failures.lastStat().reset();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\protocol\\impl\\pb",
  "methodName" : "getProto",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "GetDisabledNameservicesResponseProto getProto()\n{\r\n    return this.translator.build();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\protocol\\impl\\pb",
  "methodName" : "setProto",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setProto(Message proto)\n{\r\n    this.translator.setProto(proto);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\protocol\\impl\\pb",
  "methodName" : "readInstance",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void readInstance(String base64String) throws IOException\n{\r\n    this.translator.readInstance(base64String);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\protocol\\impl\\pb",
  "methodName" : "getNameservices",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Set<String> getNameservices()\n{\r\n    List<String> nsIds = this.translator.getProtoOrBuilder().getNameServiceIdsList();\r\n    return new TreeSet<>(nsIds);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\protocol\\impl\\pb",
  "methodName" : "setNameservices",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void setNameservices(Set<String> nameservices)\n{\r\n    this.translator.getBuilder().clearNameServiceIds();\r\n    for (String nsId : nameservices) {\r\n        this.translator.getBuilder().addNameServiceIds(nsId);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\protocol",
  "methodName" : "newInstance",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "GetDestinationRequest newInstance() throws IOException\n{\r\n    return StateStoreSerializer.newRecord(GetDestinationRequest.class);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\protocol",
  "methodName" : "newInstance",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "GetDestinationRequest newInstance(String srcPath) throws IOException\n{\r\n    GetDestinationRequest request = newInstance();\r\n    request.setSrcPath(srcPath);\r\n    return request;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\protocol",
  "methodName" : "newInstance",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "GetDestinationRequest newInstance(Path srcPath) throws IOException\n{\r\n    return newInstance(srcPath.toString());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\protocol",
  "methodName" : "getSrcPath",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getSrcPath()",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\protocol",
  "methodName" : "setSrcPath",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setSrcPath(String srcPath)",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\protocol\\impl\\pb",
  "methodName" : "getProto",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "GetMountTableEntriesRequestProto getProto()\n{\r\n    return this.translator.build();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\protocol\\impl\\pb",
  "methodName" : "setProto",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setProto(Message proto)\n{\r\n    this.translator.setProto(proto);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\protocol\\impl\\pb",
  "methodName" : "readInstance",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void readInstance(String base64String) throws IOException\n{\r\n    this.translator.readInstance(base64String);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\protocol\\impl\\pb",
  "methodName" : "getSrcPath",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String getSrcPath()\n{\r\n    return this.translator.getProtoOrBuilder().getSrcPath();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\protocol\\impl\\pb",
  "methodName" : "setSrcPath",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setSrcPath(String path)\n{\r\n    this.translator.getBuilder().setSrcPath(path);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocolPB",
  "methodName" : "getServices",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Service[] getServices()\n{\r\n    return Arrays.copyOf(services, services.length);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\resolver",
  "methodName" : "statsValid",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean statsValid()\n{\r\n    return this.statsValid;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\resolver",
  "methodName" : "registrationValid",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean registrationValid()\n{\r\n    return this.registrationValid;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\resolver",
  "methodName" : "haStateValid",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean haStateValid()\n{\r\n    return this.haStateValid;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\resolver",
  "methodName" : "getState",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "FederationNamenodeServiceState getState()\n{\r\n    if (!registrationValid) {\r\n        return FederationNamenodeServiceState.UNAVAILABLE;\r\n    } else if (haStateValid) {\r\n        return FederationNamenodeServiceState.getState(status);\r\n    } else {\r\n        return FederationNamenodeServiceState.ACTIVE;\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\resolver",
  "methodName" : "getNameserviceId",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getNameserviceId()\n{\r\n    return this.nameserviceId;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\resolver",
  "methodName" : "getNamenodeId",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getNamenodeId()\n{\r\n    return this.namenodeId;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\resolver",
  "methodName" : "getClusterId",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getClusterId()\n{\r\n    return this.clusterId;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\resolver",
  "methodName" : "getBlockPoolId",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getBlockPoolId()\n{\r\n    return this.blockPoolId;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\resolver",
  "methodName" : "getRpcAddress",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getRpcAddress()\n{\r\n    return this.rpcAddress;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\resolver",
  "methodName" : "getServiceAddress",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getServiceAddress()\n{\r\n    return this.serviceAddress;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\resolver",
  "methodName" : "getLifelineAddress",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getLifelineAddress()\n{\r\n    return this.lifelineAddress;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\resolver",
  "methodName" : "getWebAddress",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getWebAddress()\n{\r\n    return this.webAddress;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\resolver",
  "methodName" : "getWebScheme",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getWebScheme()\n{\r\n    return this.webScheme;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\resolver",
  "methodName" : "setHAServiceState",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setHAServiceState(HAServiceState state)\n{\r\n    this.status = state;\r\n    this.haStateValid = true;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\resolver",
  "methodName" : "setNamespaceInfo",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void setNamespaceInfo(NamespaceInfo info)\n{\r\n    this.clusterId = info.getClusterID();\r\n    this.blockPoolId = info.getBlockPoolID();\r\n    this.registrationValid = true;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\resolver",
  "methodName" : "setSafeMode",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setSafeMode(boolean safemode)\n{\r\n    this.safeMode = safemode;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\resolver",
  "methodName" : "getSafemode",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean getSafemode()\n{\r\n    return this.safeMode;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\resolver",
  "methodName" : "setDatanodeInfo",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setDatanodeInfo(int numLive, int numDead, int numStale, int numDecom, int numLiveDecom, int numDeadDecom, int numInMaintenanceLive, int numInMaintenanceDead, int numEnteringMaintenance, long numScheduledReplicationBlocks)\n{\r\n    this.liveDatanodes = numLive;\r\n    this.deadDatanodes = numDead;\r\n    this.staleDatanodes = numStale;\r\n    this.decomDatanodes = numDecom;\r\n    this.liveDecomDatanodes = numLiveDecom;\r\n    this.deadDecomDatanodes = numDeadDecom;\r\n    this.inMaintenanceLiveDataNodes = numInMaintenanceLive;\r\n    this.inMaintenanceDeadDataNodes = numInMaintenanceDead;\r\n    this.enteringMaintenanceDataNodes = numEnteringMaintenance;\r\n    this.statsValid = true;\r\n    this.scheduledReplicationBlocks = numScheduledReplicationBlocks;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\resolver",
  "methodName" : "getNumLiveDatanodes",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int getNumLiveDatanodes()\n{\r\n    return this.liveDatanodes;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\resolver",
  "methodName" : "getNumDeadDatanodes",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int getNumDeadDatanodes()\n{\r\n    return this.deadDatanodes;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\resolver",
  "methodName" : "getNumStaleDatanodes",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int getNumStaleDatanodes()\n{\r\n    return this.staleDatanodes;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\resolver",
  "methodName" : "getNumDecommissioningDatanodes",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int getNumDecommissioningDatanodes()\n{\r\n    return this.decomDatanodes;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\resolver",
  "methodName" : "getNumDecomLiveDatanodes",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int getNumDecomLiveDatanodes()\n{\r\n    return this.liveDecomDatanodes;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\resolver",
  "methodName" : "getNumDecomDeadDatanodes",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int getNumDecomDeadDatanodes()\n{\r\n    return this.deadDecomDatanodes;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\resolver",
  "methodName" : "getNumInMaintenanceLiveDataNodes",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int getNumInMaintenanceLiveDataNodes()\n{\r\n    return this.inMaintenanceLiveDataNodes;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\resolver",
  "methodName" : "getNumInMaintenanceDeadDataNodes",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int getNumInMaintenanceDeadDataNodes()\n{\r\n    return this.inMaintenanceDeadDataNodes;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\resolver",
  "methodName" : "getNumEnteringMaintenanceDataNodes",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int getNumEnteringMaintenanceDataNodes()\n{\r\n    return this.enteringMaintenanceDataNodes;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\resolver",
  "methodName" : "setNamesystemInfo",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setNamesystemInfo(long available, long total, long numFiles, long numBlocks, long numBlocksMissing, long numBlocksPendingReplication, long numBlocksUnderReplicated, long numBlocksPendingDeletion, long providedStorageSpace, int numPendingSPSPaths)\n{\r\n    this.totalSpace = total;\r\n    this.availableSpace = available;\r\n    this.numOfBlocks = numBlocks;\r\n    this.numOfBlocksMissing = numBlocksMissing;\r\n    this.numOfBlocksPendingReplication = numBlocksPendingReplication;\r\n    this.numOfBlocksUnderReplicated = numBlocksUnderReplicated;\r\n    this.numOfBlocksPendingDeletion = numBlocksPendingDeletion;\r\n    this.numOfFiles = numFiles;\r\n    this.statsValid = true;\r\n    this.providedSpace = providedStorageSpace;\r\n    this.pendingSPSPaths = numPendingSPSPaths;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\resolver",
  "methodName" : "setNamenodeInfo",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setNamenodeInfo(int numCorruptFiles, long numOfMissingBlocksWithReplicationFactorOne, long highestPriorityLowRedundancyRepBlocks, long highPriorityLowRedundancyECBlocks)\n{\r\n    this.corruptFilesCount = numCorruptFiles;\r\n    this.numberOfMissingBlocksWithReplicationFactorOne = numOfMissingBlocksWithReplicationFactorOne;\r\n    this.highestPriorityLowRedundancyReplicatedBlocks = highestPriorityLowRedundancyRepBlocks;\r\n    this.highestPriorityLowRedundancyECBlocks = highPriorityLowRedundancyECBlocks;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\resolver",
  "methodName" : "getCorruptFilesCount",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int getCorruptFilesCount()\n{\r\n    return this.corruptFilesCount;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\resolver",
  "methodName" : "getScheduledReplicationBlocks",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getScheduledReplicationBlocks()\n{\r\n    return this.scheduledReplicationBlocks;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\resolver",
  "methodName" : "getNumberOfMissingBlocksWithReplicationFactorOne",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getNumberOfMissingBlocksWithReplicationFactorOne()\n{\r\n    return this.numberOfMissingBlocksWithReplicationFactorOne;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\resolver",
  "methodName" : "getHighestPriorityLowRedundancyReplicatedBlocks",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getHighestPriorityLowRedundancyReplicatedBlocks()\n{\r\n    return this.highestPriorityLowRedundancyReplicatedBlocks;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\resolver",
  "methodName" : "getHighestPriorityLowRedundancyECBlocks",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getHighestPriorityLowRedundancyECBlocks()\n{\r\n    return this.highestPriorityLowRedundancyECBlocks;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\resolver",
  "methodName" : "getPendingSPSPaths",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int getPendingSPSPaths()\n{\r\n    return this.pendingSPSPaths;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\resolver",
  "methodName" : "getNumBlocks",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getNumBlocks()\n{\r\n    return this.numOfBlocks;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\resolver",
  "methodName" : "getNumFiles",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getNumFiles()\n{\r\n    return this.numOfFiles;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\resolver",
  "methodName" : "getTotalSpace",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getTotalSpace()\n{\r\n    return this.totalSpace;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\resolver",
  "methodName" : "getAvailableSpace",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getAvailableSpace()\n{\r\n    return this.availableSpace;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\resolver",
  "methodName" : "getProvidedSpace",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getProvidedSpace()\n{\r\n    return this.providedSpace;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\resolver",
  "methodName" : "getNumBlocksMissing",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getNumBlocksMissing()\n{\r\n    return this.numOfBlocksMissing;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\resolver",
  "methodName" : "getNumOfBlocksPendingReplication",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getNumOfBlocksPendingReplication()\n{\r\n    return this.numOfBlocksPendingReplication;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\resolver",
  "methodName" : "getNumOfBlocksUnderReplicated",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getNumOfBlocksUnderReplicated()\n{\r\n    return this.numOfBlocksUnderReplicated;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\resolver",
  "methodName" : "getNumOfBlocksPendingDeletion",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getNumOfBlocksPendingDeletion()\n{\r\n    return this.numOfBlocksPendingDeletion;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\resolver",
  "methodName" : "setRegistrationValid",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setRegistrationValid(boolean isValid)\n{\r\n    this.registrationValid = isValid;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\resolver",
  "methodName" : "toString",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String toString()\n{\r\n    return String.format(\"%s-%s:%s\", nameserviceId, namenodeId, serviceAddress);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\protocol\\impl\\pb",
  "methodName" : "getProto",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "AddMountTableEntryResponseProto getProto()\n{\r\n    return this.translator.build();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\protocol\\impl\\pb",
  "methodName" : "setProto",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setProto(Message proto)\n{\r\n    this.translator.setProto(proto);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\protocol\\impl\\pb",
  "methodName" : "readInstance",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void readInstance(String base64String) throws IOException\n{\r\n    this.translator.readInstance(base64String);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\protocol\\impl\\pb",
  "methodName" : "getStatus",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "boolean getStatus()\n{\r\n    return this.translator.getProtoOrBuilder().getStatus();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\protocol\\impl\\pb",
  "methodName" : "setStatus",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setStatus(boolean result)\n{\r\n    this.translator.getBuilder().setStatus(result);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store",
  "methodName" : "getRecordClass",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "Class<? extends BaseRecord> getRecordClass(final Class<T> clazz)\n{\r\n    Class<? extends BaseRecord> actualClazz = clazz;\r\n    while (actualClazz.getSimpleName().endsWith(\"Impl\")) {\r\n        actualClazz = (Class<? extends BaseRecord>) actualClazz.getSuperclass();\r\n    }\r\n    if (actualClazz.equals(BaseRecord.class)) {\r\n        LOG.error(\"We went too far ({}) with {}\", actualClazz, clazz);\r\n        actualClazz = clazz;\r\n    }\r\n    return actualClazz;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store",
  "methodName" : "getRecordClass",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Class<? extends BaseRecord> getRecordClass(final T record)\n{\r\n    return getRecordClass(record.getClass());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store",
  "methodName" : "getRecordName",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String getRecordName(final Class<T> clazz)\n{\r\n    return getRecordClass(clazz).getSimpleName();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store",
  "methodName" : "filterMultiple",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "List<T> filterMultiple(final Query<T> query, final Iterable<T> records)\n{\r\n    List<T> matchingList = new ArrayList<>();\r\n    for (T record : records) {\r\n        if (query.matches(record)) {\r\n            matchingList.add(record);\r\n        }\r\n    }\r\n    return matchingList;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store",
  "methodName" : "getHostPortString",
  "errType" : [ "UnknownHostException" ],
  "containingMethodsNum" : 5,
  "sourceCodeText" : "String getHostPortString(InetSocketAddress address)\n{\r\n    if (null == address) {\r\n        return \"\";\r\n    }\r\n    String hostName = address.getHostName();\r\n    if (hostName.equals(\"0.0.0.0\")) {\r\n        try {\r\n            hostName = InetAddress.getLocalHost().getHostName();\r\n        } catch (UnknownHostException e) {\r\n            LOG.error(\"Failed to get local host name\", e);\r\n            return \"\";\r\n        }\r\n    }\r\n    return hostName + \":\" + address.getPort();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\protocol",
  "methodName" : "newInstance",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "RemoveMountTableEntryRequest newInstance() throws IOException\n{\r\n    return StateStoreSerializer.newRecord(RemoveMountTableEntryRequest.class);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\protocol",
  "methodName" : "newInstance",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "RemoveMountTableEntryRequest newInstance(String path) throws IOException\n{\r\n    RemoveMountTableEntryRequest request = newInstance();\r\n    request.setSrcPath(path);\r\n    return request;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\protocol",
  "methodName" : "getSrcPath",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getSrcPath()",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\protocol",
  "methodName" : "setSrcPath",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setSrcPath(String path)",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "getProtocol",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Class<?> getProtocol()\n{\r\n    return this.protocol;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "getMethod",
  "errType" : [ "NoSuchMethodException", "SecurityException" ],
  "containingMethodsNum" : 4,
  "sourceCodeText" : "Method getMethod() throws IOException\n{\r\n    try {\r\n        if (types != null) {\r\n            return protocol.getDeclaredMethod(methodName, types);\r\n        } else {\r\n            return protocol.getDeclaredMethod(methodName);\r\n        }\r\n    } catch (NoSuchMethodException e) {\r\n        LOG.error(\"Cannot get method {} with types {} from {}\", methodName, Arrays.toString(types), protocol.getSimpleName(), e);\r\n        throw new IOException(e);\r\n    } catch (SecurityException e) {\r\n        LOG.error(\"Cannot access method {} with types {} from {}\", methodName, Arrays.toString(types), protocol.getSimpleName(), e);\r\n        throw new IOException(e);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "getTypes",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Class<?>[] getTypes()\n{\r\n    return Arrays.copyOf(this.types, this.types.length);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "getParams",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Object[] getParams()\n{\r\n    return this.getParams(null);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "getMethodName",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getMethodName()\n{\r\n    return this.methodName;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "getParams",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "Object[] getParams(RemoteLocationContext context)\n{\r\n    if (this.params == null) {\r\n        return new Object[] {};\r\n    }\r\n    Object[] objList = new Object[this.params.length];\r\n    for (int i = 0; i < this.params.length; i++) {\r\n        Object currentObj = this.params[i];\r\n        if (currentObj instanceof RemoteParam) {\r\n            RemoteParam paramGetter = (RemoteParam) currentObj;\r\n            if (this.types[i] == CacheDirectiveInfo.class) {\r\n                CacheDirectiveInfo path = (CacheDirectiveInfo) paramGetter.getParameterForContext(context);\r\n                objList[i] = new CacheDirectiveInfo.Builder(path).setPath(new Path(context.getDest())).build();\r\n            } else {\r\n                objList[i] = paramGetter.getParameterForContext(context);\r\n            }\r\n        } else {\r\n            objList[i] = currentObj;\r\n        }\r\n    }\r\n    return objList;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "toString",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String toString()\n{\r\n    return new StringBuilder().append(this.protocol.getSimpleName()).append(\"#\").append(this.methodName).append(\"(\").append(Arrays.deepToString(this.params)).append(\")\").toString();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\protocol",
  "methodName" : "newInstance",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "DisableNameserviceRequest newInstance()\n{\r\n    return StateStoreSerializer.newRecord(DisableNameserviceRequest.class);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\protocol",
  "methodName" : "newInstance",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "DisableNameserviceRequest newInstance(String nsId)\n{\r\n    DisableNameserviceRequest request = newInstance();\r\n    request.setNameServiceId(nsId);\r\n    return request;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\protocol",
  "methodName" : "getNameServiceId",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getNameServiceId()",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\protocol",
  "methodName" : "setNameServiceId",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setNameServiceId(String nsId)",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\protocol",
  "methodName" : "newInstance",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "AddMountTableEntryRequest newInstance()\n{\r\n    return StateStoreSerializer.newRecord(AddMountTableEntryRequest.class);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\protocol",
  "methodName" : "newInstance",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "AddMountTableEntryRequest newInstance(MountTable newEntry)\n{\r\n    AddMountTableEntryRequest request = newInstance();\r\n    request.setEntry(newEntry);\r\n    return request;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\protocol",
  "methodName" : "getEntry",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "MountTable getEntry()",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\protocol",
  "methodName" : "setEntry",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setEntry(MountTable mount)",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\records",
  "methodName" : "newInstance",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "MembershipState newInstance()\n{\r\n    MembershipState record = StateStoreSerializer.newRecord(MembershipState.class);\r\n    record.init();\r\n    return record;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\records",
  "methodName" : "newInstance",
  "errType" : null,
  "containingMethodsNum" : 14,
  "sourceCodeText" : "MembershipState newInstance(String router, String nameservice, String namenode, String clusterId, String blockPoolId, String rpcAddress, String serviceAddress, String lifelineAddress, String webScheme, String webAddress, FederationNamenodeServiceState state, boolean safemode)\n{\r\n    MembershipState record = MembershipState.newInstance();\r\n    record.setRouterId(router);\r\n    record.setNameserviceId(nameservice);\r\n    record.setNamenodeId(namenode);\r\n    record.setRpcAddress(rpcAddress);\r\n    record.setServiceAddress(serviceAddress);\r\n    record.setLifelineAddress(lifelineAddress);\r\n    record.setWebAddress(webAddress);\r\n    record.setIsSafeMode(safemode);\r\n    record.setState(state);\r\n    record.setClusterId(clusterId);\r\n    record.setBlockPoolId(blockPoolId);\r\n    record.setWebScheme(webScheme);\r\n    record.validate();\r\n    return record;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\records",
  "methodName" : "setRouterId",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setRouterId(String routerId)",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\records",
  "methodName" : "getRouterId",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getRouterId()",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\records",
  "methodName" : "setNameserviceId",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setNameserviceId(String nameserviceId)",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\records",
  "methodName" : "setNamenodeId",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setNamenodeId(String namenodeId)",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\records",
  "methodName" : "setWebAddress",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setWebAddress(String webAddress)",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\records",
  "methodName" : "setRpcAddress",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setRpcAddress(String rpcAddress)",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\records",
  "methodName" : "setServiceAddress",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setServiceAddress(String serviceAddress)",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\records",
  "methodName" : "setLifelineAddress",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setLifelineAddress(String lifelineAddress)",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\records",
  "methodName" : "setIsSafeMode",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setIsSafeMode(boolean isSafeMode)",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\records",
  "methodName" : "setClusterId",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setClusterId(String clusterId)",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\records",
  "methodName" : "setBlockPoolId",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setBlockPoolId(String blockPoolId)",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\records",
  "methodName" : "setState",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setState(FederationNamenodeServiceState state)",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\records",
  "methodName" : "setWebScheme",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setWebScheme(String webScheme)",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\records",
  "methodName" : "getNameserviceId",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getNameserviceId()",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\records",
  "methodName" : "getNamenodeId",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getNamenodeId()",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\records",
  "methodName" : "getClusterId",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getClusterId()",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\records",
  "methodName" : "getBlockPoolId",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getBlockPoolId()",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\records",
  "methodName" : "getRpcAddress",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getRpcAddress()",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\records",
  "methodName" : "getServiceAddress",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getServiceAddress()",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\records",
  "methodName" : "getLifelineAddress",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getLifelineAddress()",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\records",
  "methodName" : "getWebAddress",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getWebAddress()",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\records",
  "methodName" : "getIsSafeMode",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean getIsSafeMode()",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\records",
  "methodName" : "getWebScheme",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getWebScheme()",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\records",
  "methodName" : "getState",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "FederationNamenodeServiceState getState()",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\records",
  "methodName" : "setStats",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setStats(MembershipStats stats)",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\records",
  "methodName" : "getStats",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "MembershipStats getStats()",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\records",
  "methodName" : "setLastContact",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setLastContact(long contact)",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\records",
  "methodName" : "getLastContact",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getLastContact()",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\records",
  "methodName" : "like",
  "errType" : null,
  "containingMethodsNum" : 14,
  "sourceCodeText" : "boolean like(BaseRecord o)\n{\r\n    if (o instanceof MembershipState) {\r\n        MembershipState other = (MembershipState) o;\r\n        if (getRouterId() != null && !getRouterId().equals(other.getRouterId())) {\r\n            return false;\r\n        }\r\n        if (getNameserviceId() != null && !getNameserviceId().equals(other.getNameserviceId())) {\r\n            return false;\r\n        }\r\n        if (getNamenodeId() != null && !getNamenodeId().equals(other.getNamenodeId())) {\r\n            return false;\r\n        }\r\n        if (getRpcAddress() != null && !getRpcAddress().equals(other.getRpcAddress())) {\r\n            return false;\r\n        }\r\n        if (getClusterId() != null && !getClusterId().equals(other.getClusterId())) {\r\n            return false;\r\n        }\r\n        if (getBlockPoolId() != null && !getBlockPoolId().equals(other.getBlockPoolId())) {\r\n            return false;\r\n        }\r\n        if (getState() != null && !getState().equals(other.getState())) {\r\n            return false;\r\n        }\r\n        return true;\r\n    }\r\n    return false;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\records",
  "methodName" : "toString",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "String toString()\n{\r\n    return getRouterId() + \"->\" + getNameserviceId() + \":\" + getNamenodeId() + \":\" + getRpcAddress() + \"-\" + getState();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\records",
  "methodName" : "getPrimaryKeys",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "SortedMap<String, String> getPrimaryKeys()\n{\r\n    SortedMap<String, String> map = new TreeMap<String, String>();\r\n    map.put(\"routerId\", getRouterId());\r\n    map.put(\"nameserviceId\", getNameserviceId());\r\n    map.put(\"namenodeId\", getNamenodeId());\r\n    return map;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\records",
  "methodName" : "isAvailable",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "boolean isAvailable()\n{\r\n    return getState() == ACTIVE;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\records",
  "methodName" : "validate",
  "errType" : null,
  "containingMethodsNum" : 10,
  "sourceCodeText" : "void validate()\n{\r\n    super.validate();\r\n    if (getNameserviceId() == null || getNameserviceId().length() == 0) {\r\n        throw new IllegalArgumentException(ERROR_MSG_NO_NS_SPECIFIED + this);\r\n    }\r\n    if (getWebAddress() == null || getWebAddress().length() == 0) {\r\n        throw new IllegalArgumentException(ERROR_MSG_NO_WEB_ADDR_SPECIFIED + this);\r\n    }\r\n    if (getRpcAddress() == null || getRpcAddress().length() == 0) {\r\n        throw new IllegalArgumentException(ERROR_MSG_NO_RPC_ADDR_SPECIFIED + this);\r\n    }\r\n    if (!isBadState() && (getBlockPoolId().isEmpty() || getBlockPoolId().length() == 0)) {\r\n        throw new IllegalArgumentException(ERROR_MSG_NO_BP_SPECIFIED + this);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\records",
  "methodName" : "overrideState",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void overrideState(FederationNamenodeServiceState newState)\n{\r\n    this.setState(newState);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\records",
  "methodName" : "compareNameTo",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "int compareNameTo(MembershipState other)\n{\r\n    int ret = this.getNameserviceId().compareTo(other.getNameserviceId());\r\n    if (ret == 0) {\r\n        ret = this.getNamenodeId().compareTo(other.getNamenodeId());\r\n    }\r\n    if (ret == 0) {\r\n        ret = this.getRouterId().compareTo(other.getRouterId());\r\n    }\r\n    return ret;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\records",
  "methodName" : "getNamenodeKey",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String getNamenodeKey()\n{\r\n    return getNamenodeKey(this.getNameserviceId(), this.getNamenodeId());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\records",
  "methodName" : "getNamenodeKey",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getNamenodeKey(String nsId, String nnId)\n{\r\n    return nsId + \"-\" + nnId;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\records",
  "methodName" : "isBadState",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "boolean isBadState()\n{\r\n    return this.getState() == EXPIRED || this.getState() == UNAVAILABLE;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\records",
  "methodName" : "checkExpired",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "boolean checkExpired(long currentTime)\n{\r\n    if (super.checkExpired(currentTime)) {\r\n        this.setState(EXPIRED);\r\n        return true;\r\n    }\r\n    return false;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\records",
  "methodName" : "getExpirationMs",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getExpirationMs()\n{\r\n    return MembershipState.expirationMs;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\records",
  "methodName" : "setExpirationMs",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setExpirationMs(long time)\n{\r\n    MembershipState.expirationMs = time;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\records",
  "methodName" : "isExpired",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "boolean isExpired()\n{\r\n    return getState() == EXPIRED;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\records",
  "methodName" : "getDeletionMs",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getDeletionMs()\n{\r\n    return MembershipState.deletionMs;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\records",
  "methodName" : "setDeletionMs",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setDeletionMs(long time)\n{\r\n    MembershipState.deletionMs = time;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\driver\\impl",
  "methodName" : "newRecordInstance",
  "errType" : [ "ClassNotFoundException" ],
  "containingMethodsNum" : 3,
  "sourceCodeText" : "T newRecordInstance(Class<T> clazz)\n{\r\n    try {\r\n        String clazzPBImpl = getPBImplClassName(clazz);\r\n        Class<?> pbClazz = localConf.getClassByName(clazzPBImpl);\r\n        Object retObject = ReflectionUtils.newInstance(pbClazz, localConf);\r\n        return (T) retObject;\r\n    } catch (ClassNotFoundException e) {\r\n        throw new RuntimeException(e);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\driver\\impl",
  "methodName" : "getPBImplClassName",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "String getPBImplClassName(Class<?> clazz)\n{\r\n    String srcPackagePart = getPackageName(clazz);\r\n    String srcClassName = getClassName(clazz);\r\n    String destPackagePart = srcPackagePart + \".\" + PB_IMPL_PACKAGE_SUFFIX;\r\n    String destClassPart = srcClassName + PB_IMPL_CLASS_SUFFIX;\r\n    return destPackagePart + \".\" + destClassPart;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\driver\\impl",
  "methodName" : "getClassName",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "String getClassName(Class<?> clazz)\n{\r\n    String fqName = clazz.getName();\r\n    return (fqName.substring(fqName.lastIndexOf(\".\") + 1, fqName.length()));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\driver\\impl",
  "methodName" : "getPackageName",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String getPackageName(Class<?> clazz)\n{\r\n    return clazz.getPackage().getName();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\driver\\impl",
  "methodName" : "serialize",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "byte[] serialize(BaseRecord record)\n{\r\n    byte[] byteArray64 = null;\r\n    if (record instanceof PBRecord) {\r\n        PBRecord recordPB = (PBRecord) record;\r\n        Message msg = recordPB.getProto();\r\n        byte[] byteArray = msg.toByteArray();\r\n        byteArray64 = Base64.encodeBase64(byteArray, false);\r\n    }\r\n    return byteArray64;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\driver\\impl",
  "methodName" : "serializeString",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "String serializeString(BaseRecord record)\n{\r\n    byte[] byteArray64 = serialize(record);\r\n    String base64Encoded = StringUtils.newStringUtf8(byteArray64);\r\n    return base64Encoded;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\driver\\impl",
  "methodName" : "deserialize",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "T deserialize(byte[] byteArray, Class<T> clazz) throws IOException\n{\r\n    T record = newRecord(clazz);\r\n    if (record instanceof PBRecord) {\r\n        PBRecord pbRecord = (PBRecord) record;\r\n        byte[] byteArray64 = Base64.encodeBase64(byteArray, false);\r\n        String base64Encoded = StringUtils.newStringUtf8(byteArray64);\r\n        pbRecord.readInstance(base64Encoded);\r\n    }\r\n    return record;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\driver\\impl",
  "methodName" : "deserialize",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "T deserialize(String data, Class<T> clazz) throws IOException\n{\r\n    byte[] byteArray64 = Base64.decodeBase64(data);\r\n    return deserialize(byteArray64, clazz);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "create",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "RouterMetrics create(Configuration conf)\n{\r\n    String sessionId = conf.get(DFSConfigKeys.DFS_METRICS_SESSION_ID_KEY);\r\n    String processName = \"Router\";\r\n    MetricsSystem ms = DefaultMetricsSystem.instance();\r\n    JvmMetrics jm = JvmMetrics.create(processName, sessionId, ms);\r\n    return ms.register(new RouterMetrics(processName, sessionId, jm));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "getJvmMetrics",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "JvmMetrics getJvmMetrics()\n{\r\n    return jvmMetrics;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "shutdown",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void shutdown()\n{\r\n    DefaultMetricsSystem.shutdown();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "setSafeModeTime",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setSafeModeTime(long elapsed)\n{\r\n    safeModeTime.set((int) elapsed);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\protocol\\impl\\pb",
  "methodName" : "getProto",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "RouterHeartbeatRequestProto getProto()\n{\r\n    return this.translator.build();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\protocol\\impl\\pb",
  "methodName" : "setProto",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setProto(Message proto)\n{\r\n    this.translator.setProto(proto);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\protocol\\impl\\pb",
  "methodName" : "readInstance",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void readInstance(String base64String) throws IOException\n{\r\n    this.translator.readInstance(base64String);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\protocol\\impl\\pb",
  "methodName" : "getRouter",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "RouterState getRouter() throws IOException\n{\r\n    RouterRecordProto routerProto = this.translator.getProtoOrBuilder().getRouter();\r\n    return new RouterStatePBImpl(routerProto);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\protocol\\impl\\pb",
  "methodName" : "setRouter",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setRouter(RouterState routerState)\n{\r\n    if (routerState instanceof RouterStatePBImpl) {\r\n        RouterStatePBImpl routerStatePB = (RouterStatePBImpl) routerState;\r\n        this.translator.getBuilder().setRouter(routerStatePB.getProto());\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\resolver\\order",
  "methodName" : "getSubclusterInfo",
  "errType" : [ "IOException", "Exception" ],
  "containingMethodsNum" : 8,
  "sourceCodeText" : "Map<String, SubclusterAvailableSpace> getSubclusterInfo(MembershipStore membershipStore)\n{\r\n    Map<String, SubclusterAvailableSpace> mapping = new HashMap<>();\r\n    try {\r\n        GetNamenodeRegistrationsRequest request = GetNamenodeRegistrationsRequest.newInstance();\r\n        GetNamenodeRegistrationsResponse response = membershipStore.getNamenodeRegistrations(request);\r\n        final List<MembershipState> nns = response.getNamenodeMemberships();\r\n        for (MembershipState nn : nns) {\r\n            try {\r\n                String nsId = nn.getNameserviceId();\r\n                long availableSpace = nn.getStats().getAvailableSpace();\r\n                mapping.put(nsId, new SubclusterAvailableSpace(nsId, availableSpace));\r\n            } catch (Exception e) {\r\n                LOG.error(\"Cannot get stats info for {}: {}.\", nn, e.getMessage());\r\n            }\r\n        }\r\n    } catch (IOException ioe) {\r\n        LOG.error(\"Cannot get Namenodes from the State Store.\", ioe);\r\n    }\r\n    return mapping;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 2,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\resolver\\order",
  "methodName" : "chooseFirstNamespace",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "String chooseFirstNamespace(String path, PathLocation loc)\n{\r\n    Map<String, SubclusterAvailableSpace> subclusterInfo = getSubclusterMapping();\r\n    List<SubclusterAvailableSpace> subclusterList = new LinkedList<>(subclusterInfo.values());\r\n    Collections.sort(subclusterList, comparator);\r\n    return subclusterList.size() > 0 ? subclusterList.get(0).getNameserviceId() : null;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "serviceInit",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void serviceInit(Configuration configuration) throws Exception\n{\r\n    this.conf = configuration;\r\n    this.httpAddress = conf.getSocketAddr(RBFConfigKeys.DFS_ROUTER_HTTP_BIND_HOST_KEY, RBFConfigKeys.DFS_ROUTER_HTTP_ADDRESS_KEY, RBFConfigKeys.DFS_ROUTER_HTTP_ADDRESS_DEFAULT, RBFConfigKeys.DFS_ROUTER_HTTP_PORT_DEFAULT);\r\n    this.httpsAddress = conf.getSocketAddr(RBFConfigKeys.DFS_ROUTER_HTTPS_BIND_HOST_KEY, RBFConfigKeys.DFS_ROUTER_HTTPS_ADDRESS_KEY, RBFConfigKeys.DFS_ROUTER_HTTPS_ADDRESS_DEFAULT, RBFConfigKeys.DFS_ROUTER_HTTPS_PORT_DEFAULT);\r\n    super.serviceInit(conf);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "serviceStart",
  "errType" : null,
  "containingMethodsNum" : 12,
  "sourceCodeText" : "void serviceStart() throws Exception\n{\r\n    String webApp = \"router\";\r\n    HttpServer2.Builder builder = DFSUtil.httpServerTemplateForNNAndJN(this.conf, this.httpAddress, this.httpsAddress, webApp, RBFConfigKeys.DFS_ROUTER_KERBEROS_INTERNAL_SPNEGO_PRINCIPAL_KEY, RBFConfigKeys.DFS_ROUTER_KEYTAB_FILE_KEY);\r\n    this.httpServer = builder.build();\r\n    String httpKeytab = conf.get(DFSUtil.getSpnegoKeytabKey(conf, RBFConfigKeys.DFS_ROUTER_KEYTAB_FILE_KEY));\r\n    NameNodeHttpServer.initWebHdfs(conf, httpAddress.getHostName(), httpKeytab, httpServer, RouterWebHdfsMethods.class.getPackage().getName());\r\n    this.httpServer.setAttribute(NAMENODE_ATTRIBUTE_KEY, this.router);\r\n    this.httpServer.setAttribute(JspHelper.CURRENT_CONF, this.conf);\r\n    setupServlets(this.httpServer, this.conf);\r\n    this.httpServer.start();\r\n    InetSocketAddress listenAddress = this.httpServer.getConnectorAddress(0);\r\n    if (listenAddress != null) {\r\n        this.httpAddress = new InetSocketAddress(this.httpAddress.getHostName(), listenAddress.getPort());\r\n    }\r\n    super.serviceStart();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "serviceStop",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void serviceStop() throws Exception\n{\r\n    if (this.httpServer != null) {\r\n        this.httpServer.stop();\r\n    }\r\n    super.serviceStop();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "setupServlets",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void setupServlets(HttpServer2 httpServer, Configuration conf)\n{\r\n    httpServer.addInternalServlet(IsRouterActiveServlet.SERVLET_NAME, IsRouterActiveServlet.PATH_SPEC, IsRouterActiveServlet.class);\r\n    httpServer.addInternalServlet(RouterFsckServlet.SERVLET_NAME, RouterFsckServlet.PATH_SPEC, RouterFsckServlet.class, true);\r\n    httpServer.addInternalServlet(RouterNetworkTopologyServlet.SERVLET_NAME, RouterNetworkTopologyServlet.PATH_SPEC, RouterNetworkTopologyServlet.class);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "getHttpAddress",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "InetSocketAddress getHttpAddress()\n{\r\n    return this.httpAddress;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "getHttpsAddress",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "InetSocketAddress getHttpsAddress()\n{\r\n    return this.httpsAddress;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "getConfFromContext",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Configuration getConfFromContext(ServletContext context)\n{\r\n    return (Configuration) context.getAttribute(JspHelper.CURRENT_CONF);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "getRouterFromContext",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Router getRouterFromContext(ServletContext context)\n{\r\n    return (Router) context.getAttribute(NAMENODE_ATTRIBUTE_KEY);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "getMaxSize",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int getMaxSize()\n{\r\n    return this.maxSize;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "getMinSize",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int getMinSize()\n{\r\n    return this.minSize;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "getMinActiveRatio",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "float getMinActiveRatio()\n{\r\n    return this.minActiveRatio;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "getConnectionPoolId",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "ConnectionPoolId getConnectionPoolId()\n{\r\n    return this.connectionPoolId;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "getClientIndex",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "AtomicInteger getClientIndex()\n{\r\n    return this.clientIndex;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "getConnection",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "ConnectionContext getConnection()\n{\r\n    this.lastActiveTime = Time.now();\r\n    ConnectionContext conn = null;\r\n    List<ConnectionContext> tmpConnections = this.connections;\r\n    int size = tmpConnections.size();\r\n    int threadIndex = this.clientIndex.getAndIncrement() & 0x7FFFFFFF;\r\n    for (int i = 0; i < size; i++) {\r\n        int index = (threadIndex + i) % size;\r\n        conn = tmpConnections.get(index);\r\n        if (conn != null && conn.isUsable()) {\r\n            return conn;\r\n        }\r\n    }\r\n    return conn;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "addConnection",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void addConnection(ConnectionContext conn)\n{\r\n    List<ConnectionContext> tmpConnections = new ArrayList<>(this.connections);\r\n    tmpConnections.add(conn);\r\n    this.connections = tmpConnections;\r\n    this.lastActiveTime = Time.now();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "removeConnections",
  "errType" : null,
  "containingMethodsNum" : 9,
  "sourceCodeText" : "List<ConnectionContext> removeConnections(int num)\n{\r\n    List<ConnectionContext> removed = new LinkedList<>();\r\n    if (this.connections.size() > this.minSize) {\r\n        int targetCount = Math.min(num, this.connections.size() - this.minSize);\r\n        List<ConnectionContext> tmpConnections = new ArrayList<>();\r\n        for (int i = 0; i < this.connections.size(); i++) {\r\n            ConnectionContext conn = this.connections.get(i);\r\n            if (removed.size() < targetCount && conn.isUsable()) {\r\n                removed.add(conn);\r\n            } else {\r\n                tmpConnections.add(conn);\r\n            }\r\n        }\r\n        this.connections = tmpConnections;\r\n    }\r\n    LOG.debug(\"Expected to remove {} connection \" + \"and actually removed {} connections\", num, removed.size());\r\n    return removed;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "close",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void close()\n{\r\n    long timeSinceLastActive = TimeUnit.MILLISECONDS.toSeconds(Time.now() - getLastActiveTime());\r\n    LOG.debug(\"Shutting down connection pool \\\"{}\\\" used {} seconds ago\", this.connectionPoolId, timeSinceLastActive);\r\n    for (ConnectionContext connection : this.connections) {\r\n        connection.close(true);\r\n    }\r\n    this.connections.clear();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "getNumConnections",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "int getNumConnections()\n{\r\n    return this.connections.size();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "getNumActiveConnections",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "int getNumActiveConnections()\n{\r\n    int ret = 0;\r\n    List<ConnectionContext> tmpConnections = this.connections;\r\n    for (ConnectionContext conn : tmpConnections) {\r\n        if (conn.isActive()) {\r\n            ret++;\r\n        }\r\n    }\r\n    return ret;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "getNumIdleConnections",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "int getNumIdleConnections()\n{\r\n    int ret = 0;\r\n    List<ConnectionContext> tmpConnections = this.connections;\r\n    for (ConnectionContext conn : tmpConnections) {\r\n        if (conn.isUsable()) {\r\n            ret++;\r\n        }\r\n    }\r\n    return ret;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "getNumActiveConnectionsRecently",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "int getNumActiveConnectionsRecently()\n{\r\n    int ret = 0;\r\n    List<ConnectionContext> tmpConnections = this.connections;\r\n    for (ConnectionContext conn : tmpConnections) {\r\n        if (conn.isActiveRecently()) {\r\n            ret++;\r\n        }\r\n    }\r\n    return ret;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "getLastActiveTime",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getLastActiveTime()\n{\r\n    return this.lastActiveTime;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "toString",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String toString()\n{\r\n    return this.connectionPoolId.toString();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "getJSON",
  "errType" : null,
  "containingMethodsNum" : 12,
  "sourceCodeText" : "String getJSON()\n{\r\n    final Map<String, String> info = new LinkedHashMap<>();\r\n    info.put(\"active\", Integer.toString(getNumActiveConnections()));\r\n    info.put(\"recent_active\", Integer.toString(getNumActiveConnectionsRecently()));\r\n    info.put(\"idle\", Integer.toString(getNumIdleConnections()));\r\n    info.put(\"total\", Integer.toString(getNumConnections()));\r\n    if (LOG.isDebugEnabled()) {\r\n        List<ConnectionContext> tmpConnections = this.connections;\r\n        for (int i = 0; i < tmpConnections.size(); i++) {\r\n            ConnectionContext connection = tmpConnections.get(i);\r\n            info.put(i + \" active\", Boolean.toString(connection.isActive()));\r\n            info.put(i + \" recent_active\", Integer.toString(getNumActiveConnectionsRecently()));\r\n            info.put(i + \" idle\", Boolean.toString(connection.isUsable()));\r\n            info.put(i + \" closed\", Boolean.toString(connection.isClosed()));\r\n        }\r\n    }\r\n    return JSON.toString(info);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "newConnection",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "ConnectionContext newConnection() throws IOException\n{\r\n    return newConnection(this.conf, this.namenodeAddress, this.ugi, this.protocol);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "newConnection",
  "errType" : null,
  "containingMethodsNum" : 14,
  "sourceCodeText" : "ConnectionContext newConnection(Configuration conf, String nnAddress, UserGroupInformation ugi, Class<T> proto) throws IOException\n{\r\n    if (!PROTO_MAP.containsKey(proto)) {\r\n        String msg = \"Unsupported protocol for connection to NameNode: \" + ((proto != null) ? proto.getName() : \"null\");\r\n        LOG.error(msg);\r\n        throw new IllegalStateException(msg);\r\n    }\r\n    ProtoImpl classes = PROTO_MAP.get(proto);\r\n    RPC.setProtocolEngine(conf, classes.protoPb, ProtobufRpcEngine2.class);\r\n    final RetryPolicy defaultPolicy = RetryUtils.getDefaultRetryPolicy(conf, HdfsClientConfigKeys.Retry.POLICY_ENABLED_KEY, HdfsClientConfigKeys.Retry.POLICY_ENABLED_DEFAULT, HdfsClientConfigKeys.Retry.POLICY_SPEC_KEY, HdfsClientConfigKeys.Retry.POLICY_SPEC_DEFAULT, HdfsConstants.SAFEMODE_EXCEPTION_CLASS_NAME);\r\n    SocketFactory factory = SocketFactory.getDefault();\r\n    if (UserGroupInformation.isSecurityEnabled()) {\r\n        SaslRpcServer.init(conf);\r\n    }\r\n    InetSocketAddress socket = NetUtils.createSocketAddr(nnAddress);\r\n    final long version = RPC.getProtocolVersion(classes.protoPb);\r\n    Object proxy = RPC.getProtocolProxy(classes.protoPb, version, socket, ugi, conf, factory, RPC.getRpcTimeout(conf), defaultPolicy, null).getProxy();\r\n    T client = newProtoClient(proto, classes, proxy);\r\n    Text dtService = SecurityUtil.buildTokenService(socket);\r\n    ProxyAndInfo<T> clientProxy = new ProxyAndInfo<T>(client, dtService, socket);\r\n    ConnectionContext connection = new ConnectionContext(clientProxy);\r\n    return connection;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "newProtoClient",
  "errType" : [ "Exception" ],
  "containingMethodsNum" : 4,
  "sourceCodeText" : "T newProtoClient(Class<T> proto, ProtoImpl classes, Object proxy)\n{\r\n    try {\r\n        Constructor<?> constructor = classes.protoClientPb.getConstructor(classes.protoPb);\r\n        Object o = constructor.newInstance(new Object[] { proxy });\r\n        if (proto.isAssignableFrom(o.getClass())) {\r\n            @SuppressWarnings(\"unchecked\")\r\n            T client = (T) o;\r\n            return client;\r\n        }\r\n    } catch (Exception e) {\r\n        LOG.error(e.getMessage());\r\n    }\r\n    return null;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\protocol",
  "methodName" : "newInstance",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "GetNamespaceInfoRequest newInstance()\n{\r\n    return StateStoreSerializer.newRecord(GetNamespaceInfoRequest.class);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\fairness",
  "methodName" : "init",
  "errType" : null,
  "containingMethodsNum" : 23,
  "sourceCodeText" : "void init(Configuration conf) throws IllegalArgumentException\n{\r\n    super.init(conf);\r\n    int handlerCount = conf.getInt(DFS_ROUTER_HANDLER_COUNT_KEY, DFS_ROUTER_HANDLER_COUNT_DEFAULT);\r\n    LOG.info(\"Handlers available for fairness assignment {} \", handlerCount);\r\n    Set<String> allConfiguredNS = FederationUtil.getAllConfiguredNS(conf);\r\n    Set<String> unassignedNS = new HashSet<>();\r\n    allConfiguredNS.add(CONCURRENT_NS);\r\n    validateHandlersCount(conf, handlerCount, allConfiguredNS);\r\n    for (String nsId : allConfiguredNS) {\r\n        int dedicatedHandlers = conf.getInt(DFS_ROUTER_FAIR_HANDLER_COUNT_KEY_PREFIX + nsId, 0);\r\n        LOG.info(\"Dedicated handlers {} for ns {} \", dedicatedHandlers, nsId);\r\n        if (dedicatedHandlers > 0) {\r\n            handlerCount -= dedicatedHandlers;\r\n            insertNameServiceWithPermits(nsId, dedicatedHandlers);\r\n            logAssignment(nsId, dedicatedHandlers);\r\n        } else {\r\n            unassignedNS.add(nsId);\r\n        }\r\n    }\r\n    if (!unassignedNS.isEmpty()) {\r\n        LOG.info(\"Unassigned ns {}\", unassignedNS.toString());\r\n        int handlersPerNS = handlerCount / unassignedNS.size();\r\n        LOG.info(\"Handlers available per ns {}\", handlersPerNS);\r\n        for (String nsId : unassignedNS) {\r\n            insertNameServiceWithPermits(nsId, handlersPerNS);\r\n            logAssignment(nsId, handlersPerNS);\r\n        }\r\n    }\r\n    int leftOverHandlers = unassignedNS.isEmpty() ? handlerCount : handlerCount % unassignedNS.size();\r\n    int existingPermits = getAvailablePermits(CONCURRENT_NS);\r\n    if (leftOverHandlers > 0) {\r\n        LOG.info(\"Assigned extra {} handlers to commons pool\", leftOverHandlers);\r\n        insertNameServiceWithPermits(CONCURRENT_NS, existingPermits + leftOverHandlers);\r\n    }\r\n    LOG.info(\"Final permit allocation for concurrent ns: {}\", getAvailablePermits(CONCURRENT_NS));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\fairness",
  "methodName" : "logAssignment",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void logAssignment(String nsId, int count)\n{\r\n    LOG.info(\"Assigned {} handlers to nsId {} \", count, nsId);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\fairness",
  "methodName" : "validateHandlersCount",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void validateHandlersCount(Configuration conf, int handlerCount, Set<String> allConfiguredNS)\n{\r\n    int totalDedicatedHandlers = 0;\r\n    for (String nsId : allConfiguredNS) {\r\n        int dedicatedHandlers = conf.getInt(DFS_ROUTER_FAIR_HANDLER_COUNT_KEY_PREFIX + nsId, 0);\r\n        if (dedicatedHandlers > 0) {\r\n            totalDedicatedHandlers += dedicatedHandlers;\r\n        } else {\r\n            totalDedicatedHandlers++;\r\n        }\r\n    }\r\n    if (totalDedicatedHandlers > handlerCount) {\r\n        String msg = String.format(ERROR_MSG, handlerCount, totalDedicatedHandlers);\r\n        LOG.error(msg);\r\n        throw new IllegalArgumentException(msg);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\protocol\\impl\\pb",
  "methodName" : "getProto",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "UpdateMountTableEntryRequestProto getProto()\n{\r\n    return this.translator.build();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\protocol\\impl\\pb",
  "methodName" : "setProto",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setProto(Message proto)\n{\r\n    this.translator.setProto(proto);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\protocol\\impl\\pb",
  "methodName" : "readInstance",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void readInstance(String base64String) throws IOException\n{\r\n    this.translator.readInstance(base64String);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\protocol\\impl\\pb",
  "methodName" : "getEntry",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "MountTable getEntry() throws IOException\n{\r\n    MountTableRecordProto statsProto = this.translator.getProtoOrBuilder().getEntry();\r\n    MountTable stats = StateStoreSerializer.newRecord(MountTable.class);\r\n    if (stats instanceof MountTablePBImpl) {\r\n        MountTablePBImpl entryPB = (MountTablePBImpl) stats;\r\n        entryPB.setProto(statsProto);\r\n        return entryPB;\r\n    } else {\r\n        throw new IOException(\"Cannot get stats for the membership\");\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\protocol\\impl\\pb",
  "methodName" : "setEntry",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void setEntry(MountTable mount) throws IOException\n{\r\n    if (mount instanceof MountTablePBImpl) {\r\n        MountTablePBImpl mountPB = (MountTablePBImpl) mount;\r\n        MountTableRecordProto mountProto = (MountTableRecordProto) mountPB.getProto();\r\n        this.translator.getBuilder().setEntry(mountProto);\r\n    } else {\r\n        throw new IOException(\"Cannot set mount table entry\");\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\protocol",
  "methodName" : "newInstance",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "GetMountTableEntriesRequest newInstance() throws IOException\n{\r\n    return StateStoreSerializer.newRecord(GetMountTableEntriesRequest.class);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\protocol",
  "methodName" : "newInstance",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "GetMountTableEntriesRequest newInstance(String srcPath) throws IOException\n{\r\n    GetMountTableEntriesRequest request = newInstance();\r\n    request.setSrcPath(srcPath);\r\n    return request;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\protocol",
  "methodName" : "getSrcPath",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getSrcPath()",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\protocol",
  "methodName" : "setSrcPath",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setSrcPath(String path)",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\protocol",
  "methodName" : "newInstance",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "RouterHeartbeatResponse newInstance() throws IOException\n{\r\n    return StateStoreSerializer.newRecord(RouterHeartbeatResponse.class);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\protocol",
  "methodName" : "newInstance",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "RouterHeartbeatResponse newInstance(boolean status) throws IOException\n{\r\n    RouterHeartbeatResponse response = newInstance();\r\n    response.setStatus(status);\r\n    return response;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\protocol",
  "methodName" : "getStatus",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean getStatus()",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\protocol",
  "methodName" : "setStatus",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setStatus(boolean result)",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\protocol\\impl\\pb",
  "methodName" : "getProto",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "RefreshSuperUserGroupsConfigurationResponseProto getProto()\n{\r\n    return translator.build();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\protocol\\impl\\pb",
  "methodName" : "setProto",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setProto(Message proto)\n{\r\n    this.translator.setProto(proto);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\protocol\\impl\\pb",
  "methodName" : "readInstance",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void readInstance(String base64String) throws IOException\n{\r\n    this.translator.readInstance(base64String);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\protocol\\impl\\pb",
  "methodName" : "getStatus",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "boolean getStatus()\n{\r\n    return this.translator.getProtoOrBuilder().getStatus();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\protocol\\impl\\pb",
  "methodName" : "setStatus",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setStatus(boolean result)\n{\r\n    this.translator.getBuilder().setStatus(result);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\protocol\\impl\\pb",
  "methodName" : "getProto",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "GetRouterRegistrationsRequestProto getProto()\n{\r\n    return this.translator.build();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\protocol\\impl\\pb",
  "methodName" : "setProto",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setProto(Message proto)\n{\r\n    this.translator.setProto(proto);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\protocol\\impl\\pb",
  "methodName" : "readInstance",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void readInstance(String base64String) throws IOException\n{\r\n    this.translator.readInstance(base64String);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\protocol\\impl\\pb",
  "methodName" : "getProto",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "GetNamenodeRegistrationsResponseProto getProto()\n{\r\n    return this.translator.build();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\protocol\\impl\\pb",
  "methodName" : "setProto",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setProto(Message proto)\n{\r\n    this.translator.setProto(proto);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\protocol\\impl\\pb",
  "methodName" : "readInstance",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void readInstance(String base64String) throws IOException\n{\r\n    this.translator.readInstance(base64String);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\protocol\\impl\\pb",
  "methodName" : "getNamenodeMemberships",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "List<MembershipState> getNamenodeMemberships() throws IOException\n{\r\n    List<MembershipState> ret = new ArrayList<MembershipState>();\r\n    List<NamenodeMembershipRecordProto> memberships = this.translator.getProtoOrBuilder().getNamenodeMembershipsList();\r\n    for (NamenodeMembershipRecordProto memberProto : memberships) {\r\n        MembershipState membership = new MembershipStatePBImpl(memberProto);\r\n        ret.add(membership);\r\n    }\r\n    return ret;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\protocol\\impl\\pb",
  "methodName" : "setNamenodeMemberships",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setNamenodeMemberships(List<MembershipState> records) throws IOException\n{\r\n    for (MembershipState member : records) {\r\n        if (member instanceof MembershipStatePBImpl) {\r\n            MembershipStatePBImpl memberPB = (MembershipStatePBImpl) member;\r\n            this.translator.getBuilder().addNamenodeMemberships(memberPB.getProto());\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\protocol",
  "methodName" : "newInstance",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "EnterSafeModeRequest newInstance() throws IOException\n{\r\n    return StateStoreSerializer.newRecord(EnterSafeModeRequest.class);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "getParameterForContext",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "Object getParameterForContext(RemoteLocationContext context)\n{\r\n    if (context == null) {\r\n        return null;\r\n    } else if (this.paramMap != null) {\r\n        return this.paramMap.get(context);\r\n    } else {\r\n        return context.getDest();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "toString",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String toString()\n{\r\n    return new StringBuilder().append(\"RemoteParam(\").append(this.paramMap).append(\")\").toString();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\records\\impl\\pb",
  "methodName" : "getProto",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "NamenodeMembershipStatsRecordProto getProto()\n{\r\n    return this.translator.build();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\records\\impl\\pb",
  "methodName" : "setProto",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setProto(Message proto)\n{\r\n    this.translator.setProto(proto);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\records\\impl\\pb",
  "methodName" : "readInstance",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void readInstance(String base64String) throws IOException\n{\r\n    this.translator.readInstance(base64String);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\records\\impl\\pb",
  "methodName" : "setTotalSpace",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setTotalSpace(long space)\n{\r\n    this.translator.getBuilder().setTotalSpace(space);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\records\\impl\\pb",
  "methodName" : "getTotalSpace",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "long getTotalSpace()\n{\r\n    return this.translator.getProtoOrBuilder().getTotalSpace();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\records\\impl\\pb",
  "methodName" : "setAvailableSpace",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setAvailableSpace(long space)\n{\r\n    this.translator.getBuilder().setAvailableSpace(space);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\records\\impl\\pb",
  "methodName" : "getAvailableSpace",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "long getAvailableSpace()\n{\r\n    return this.translator.getProtoOrBuilder().getAvailableSpace();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\records\\impl\\pb",
  "methodName" : "setProvidedSpace",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setProvidedSpace(long capacity)\n{\r\n    this.translator.getBuilder().setProvidedSpace(capacity);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\records\\impl\\pb",
  "methodName" : "getProvidedSpace",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "long getProvidedSpace()\n{\r\n    return this.translator.getProtoOrBuilder().getProvidedSpace();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\records\\impl\\pb",
  "methodName" : "setNumOfFiles",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setNumOfFiles(long files)\n{\r\n    this.translator.getBuilder().setNumOfFiles(files);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\records\\impl\\pb",
  "methodName" : "getNumOfFiles",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "long getNumOfFiles()\n{\r\n    return this.translator.getProtoOrBuilder().getNumOfFiles();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\records\\impl\\pb",
  "methodName" : "setNumOfBlocks",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setNumOfBlocks(long blocks)\n{\r\n    this.translator.getBuilder().setNumOfBlocks(blocks);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\records\\impl\\pb",
  "methodName" : "getNumOfBlocks",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "long getNumOfBlocks()\n{\r\n    return this.translator.getProtoOrBuilder().getNumOfBlocks();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\records\\impl\\pb",
  "methodName" : "setNumOfBlocksMissing",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setNumOfBlocksMissing(long blocks)\n{\r\n    this.translator.getBuilder().setNumOfBlocksMissing(blocks);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\records\\impl\\pb",
  "methodName" : "getNumOfBlocksMissing",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "long getNumOfBlocksMissing()\n{\r\n    return this.translator.getProtoOrBuilder().getNumOfBlocksMissing();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\records\\impl\\pb",
  "methodName" : "setNumOfBlocksPendingReplication",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setNumOfBlocksPendingReplication(long blocks)\n{\r\n    this.translator.getBuilder().setNumOfBlocksPendingReplication(blocks);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\records\\impl\\pb",
  "methodName" : "getNumOfBlocksPendingReplication",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "long getNumOfBlocksPendingReplication()\n{\r\n    return this.translator.getProtoOrBuilder().getNumOfBlocksPendingReplication();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\records\\impl\\pb",
  "methodName" : "setNumOfBlocksUnderReplicated",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setNumOfBlocksUnderReplicated(long blocks)\n{\r\n    this.translator.getBuilder().setNumOfBlocksUnderReplicated(blocks);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\records\\impl\\pb",
  "methodName" : "getNumOfBlocksUnderReplicated",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "long getNumOfBlocksUnderReplicated()\n{\r\n    return this.translator.getProtoOrBuilder().getNumOfBlocksUnderReplicated();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\records\\impl\\pb",
  "methodName" : "setNumOfBlocksPendingDeletion",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setNumOfBlocksPendingDeletion(long blocks)\n{\r\n    this.translator.getBuilder().setNumOfBlocksPendingDeletion(blocks);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\records\\impl\\pb",
  "methodName" : "getNumOfBlocksPendingDeletion",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "long getNumOfBlocksPendingDeletion()\n{\r\n    return this.translator.getProtoOrBuilder().getNumOfBlocksPendingDeletion();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\records\\impl\\pb",
  "methodName" : "setNumOfActiveDatanodes",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setNumOfActiveDatanodes(int nodes)\n{\r\n    this.translator.getBuilder().setNumOfActiveDatanodes(nodes);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\records\\impl\\pb",
  "methodName" : "getNumOfActiveDatanodes",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "int getNumOfActiveDatanodes()\n{\r\n    return this.translator.getProtoOrBuilder().getNumOfActiveDatanodes();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\records\\impl\\pb",
  "methodName" : "setNumOfDeadDatanodes",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setNumOfDeadDatanodes(int nodes)\n{\r\n    this.translator.getBuilder().setNumOfDeadDatanodes(nodes);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\records\\impl\\pb",
  "methodName" : "getNumOfDeadDatanodes",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "int getNumOfDeadDatanodes()\n{\r\n    return this.translator.getProtoOrBuilder().getNumOfDeadDatanodes();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\records\\impl\\pb",
  "methodName" : "setNumOfStaleDatanodes",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setNumOfStaleDatanodes(int nodes)\n{\r\n    this.translator.getBuilder().setNumOfStaleDatanodes(nodes);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\records\\impl\\pb",
  "methodName" : "getNumOfStaleDatanodes",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "int getNumOfStaleDatanodes()\n{\r\n    return this.translator.getProtoOrBuilder().getNumOfStaleDatanodes();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\records\\impl\\pb",
  "methodName" : "setNumOfDecommissioningDatanodes",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setNumOfDecommissioningDatanodes(int nodes)\n{\r\n    this.translator.getBuilder().setNumOfDecommissioningDatanodes(nodes);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\records\\impl\\pb",
  "methodName" : "getNumOfDecommissioningDatanodes",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "int getNumOfDecommissioningDatanodes()\n{\r\n    return this.translator.getProtoOrBuilder().getNumOfDecommissioningDatanodes();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\records\\impl\\pb",
  "methodName" : "setNumOfDecomActiveDatanodes",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setNumOfDecomActiveDatanodes(int nodes)\n{\r\n    this.translator.getBuilder().setNumOfDecomActiveDatanodes(nodes);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\records\\impl\\pb",
  "methodName" : "getNumOfDecomActiveDatanodes",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "int getNumOfDecomActiveDatanodes()\n{\r\n    return this.translator.getProtoOrBuilder().getNumOfDecomActiveDatanodes();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\records\\impl\\pb",
  "methodName" : "setNumOfDecomDeadDatanodes",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setNumOfDecomDeadDatanodes(int nodes)\n{\r\n    this.translator.getBuilder().setNumOfDecomDeadDatanodes(nodes);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\records\\impl\\pb",
  "methodName" : "getNumOfDecomDeadDatanodes",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "int getNumOfDecomDeadDatanodes()\n{\r\n    return this.translator.getProtoOrBuilder().getNumOfDecomDeadDatanodes();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\records\\impl\\pb",
  "methodName" : "setNumOfInMaintenanceLiveDataNodes",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setNumOfInMaintenanceLiveDataNodes(int nodes)\n{\r\n    this.translator.getBuilder().setNumOfInMaintenanceLiveDataNodes(nodes);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\records\\impl\\pb",
  "methodName" : "getNumOfInMaintenanceLiveDataNodes",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "int getNumOfInMaintenanceLiveDataNodes()\n{\r\n    return this.translator.getProtoOrBuilder().getNumOfInMaintenanceLiveDataNodes();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\records\\impl\\pb",
  "methodName" : "setNumOfInMaintenanceDeadDataNodes",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setNumOfInMaintenanceDeadDataNodes(int nodes)\n{\r\n    this.translator.getBuilder().setNumOfInMaintenanceDeadDataNodes(nodes);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\records\\impl\\pb",
  "methodName" : "getNumOfInMaintenanceDeadDataNodes",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "int getNumOfInMaintenanceDeadDataNodes()\n{\r\n    return this.translator.getProtoOrBuilder().getNumOfInMaintenanceDeadDataNodes();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\records\\impl\\pb",
  "methodName" : "setNumOfEnteringMaintenanceDataNodes",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setNumOfEnteringMaintenanceDataNodes(int nodes)\n{\r\n    this.translator.getBuilder().setNumOfEnteringMaintenanceDataNodes(nodes);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\records\\impl\\pb",
  "methodName" : "getNumOfEnteringMaintenanceDataNodes",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "int getNumOfEnteringMaintenanceDataNodes()\n{\r\n    return this.translator.getProtoOrBuilder().getNumOfEnteringMaintenanceDataNodes();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\records\\impl\\pb",
  "methodName" : "setCorruptFilesCount",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setCorruptFilesCount(int num)\n{\r\n    this.translator.getBuilder().setCorruptFilesCount(num);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\records\\impl\\pb",
  "methodName" : "getCorruptFilesCount",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "int getCorruptFilesCount()\n{\r\n    return this.translator.getProtoOrBuilder().getCorruptFilesCount();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\records\\impl\\pb",
  "methodName" : "setScheduledReplicationBlocks",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setScheduledReplicationBlocks(long blocks)\n{\r\n    this.translator.getBuilder().setScheduledReplicationBlocks(blocks);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\records\\impl\\pb",
  "methodName" : "getScheduledReplicationBlocks",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "long getScheduledReplicationBlocks()\n{\r\n    return this.translator.getProtoOrBuilder().getScheduledReplicationBlocks();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\records\\impl\\pb",
  "methodName" : "setNumberOfMissingBlocksWithReplicationFactorOne",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setNumberOfMissingBlocksWithReplicationFactorOne(long blocks)\n{\r\n    this.translator.getBuilder().setNumberOfMissingBlocksWithReplicationFactorOne(blocks);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\records\\impl\\pb",
  "methodName" : "getNumberOfMissingBlocksWithReplicationFactorOne",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "long getNumberOfMissingBlocksWithReplicationFactorOne()\n{\r\n    return this.translator.getProtoOrBuilder().getNumberOfMissingBlocksWithReplicationFactorOne();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\records\\impl\\pb",
  "methodName" : "setHighestPriorityLowRedundancyReplicatedBlocks",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setHighestPriorityLowRedundancyReplicatedBlocks(long blocks)\n{\r\n    this.translator.getBuilder().setHighestPriorityLowRedundancyReplicatedBlocks(blocks);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\records\\impl\\pb",
  "methodName" : "getHighestPriorityLowRedundancyReplicatedBlocks",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "long getHighestPriorityLowRedundancyReplicatedBlocks()\n{\r\n    return this.translator.getProtoOrBuilder().getHighestPriorityLowRedundancyReplicatedBlocks();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\records\\impl\\pb",
  "methodName" : "setHighestPriorityLowRedundancyECBlocks",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setHighestPriorityLowRedundancyECBlocks(long blocks)\n{\r\n    this.translator.getBuilder().setHighestPriorityLowRedundancyECBlocks(blocks);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\records\\impl\\pb",
  "methodName" : "getHighestPriorityLowRedundancyECBlocks",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "long getHighestPriorityLowRedundancyECBlocks()\n{\r\n    return this.translator.getProtoOrBuilder().getHighestPriorityLowRedundancyECBlocks();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\records\\impl\\pb",
  "methodName" : "setPendingSPSPaths",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setPendingSPSPaths(int pendingSPSPaths)\n{\r\n    this.translator.getBuilder().setPendingSPSPaths(pendingSPSPaths);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\records\\impl\\pb",
  "methodName" : "getPendingSPSPaths",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "int getPendingSPSPaths()\n{\r\n    return this.translator.getProtoOrBuilder().getPendingSPSPaths();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "setStoragePolicy",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void setStoragePolicy(String src, String policyName) throws IOException\n{\r\n    rpcServer.checkOperation(NameNode.OperationCategory.WRITE);\r\n    List<RemoteLocation> locations = rpcServer.getLocationsForPath(src, false, false);\r\n    RemoteMethod method = new RemoteMethod(\"setStoragePolicy\", new Class<?>[] { String.class, String.class }, new RemoteParam(), policyName);\r\n    if (rpcServer.isInvokeConcurrent(src)) {\r\n        rpcClient.invokeConcurrent(locations, method);\r\n    } else {\r\n        rpcClient.invokeSequential(locations, method);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "getStoragePolicies",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "BlockStoragePolicy[] getStoragePolicies() throws IOException\n{\r\n    rpcServer.checkOperation(NameNode.OperationCategory.READ);\r\n    RemoteMethod method = new RemoteMethod(\"getStoragePolicies\");\r\n    return rpcServer.invokeAtAvailableNs(method, BlockStoragePolicy[].class);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "unsetStoragePolicy",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void unsetStoragePolicy(String src) throws IOException\n{\r\n    rpcServer.checkOperation(NameNode.OperationCategory.WRITE, true);\r\n    List<RemoteLocation> locations = rpcServer.getLocationsForPath(src, false, false);\r\n    RemoteMethod method = new RemoteMethod(\"unsetStoragePolicy\", new Class<?>[] { String.class }, new RemoteParam());\r\n    if (rpcServer.isInvokeConcurrent(src)) {\r\n        rpcClient.invokeConcurrent(locations, method);\r\n    } else {\r\n        rpcClient.invokeSequential(locations, method);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "getStoragePolicy",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "BlockStoragePolicy getStoragePolicy(String path) throws IOException\n{\r\n    rpcServer.checkOperation(NameNode.OperationCategory.READ, true);\r\n    List<RemoteLocation> locations = rpcServer.getLocationsForPath(path, false, false);\r\n    RemoteMethod method = new RemoteMethod(\"getStoragePolicy\", new Class<?>[] { String.class }, new RemoteParam());\r\n    return (BlockStoragePolicy) rpcClient.invokeSequential(locations, method);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "satisfyStoragePolicy",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void satisfyStoragePolicy(String path) throws IOException\n{\r\n    rpcServer.checkOperation(NameNode.OperationCategory.READ, true);\r\n    List<RemoteLocation> locations = rpcServer.getLocationsForPath(path, true, false);\r\n    RemoteMethod method = new RemoteMethod(\"satisfyStoragePolicy\", new Class<?>[] { String.class }, new RemoteParam());\r\n    rpcClient.invokeSequential(locations, method);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\protocol",
  "methodName" : "newInstance",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "RefreshMountTableEntriesResponse newInstance() throws IOException\n{\r\n    return StateStoreSerializer.newRecord(RefreshMountTableEntriesResponse.class);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\protocol",
  "methodName" : "getResult",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean getResult()",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\protocol",
  "methodName" : "setResult",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setResult(boolean result)",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\records\\impl\\pb",
  "methodName" : "getProto",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "StateStoreVersionRecordProto getProto()\n{\r\n    return this.translator.build();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\records\\impl\\pb",
  "methodName" : "setProto",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setProto(Message proto)\n{\r\n    this.translator.setProto(proto);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\records\\impl\\pb",
  "methodName" : "readInstance",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void readInstance(String base64String) throws IOException\n{\r\n    this.translator.readInstance(base64String);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\records\\impl\\pb",
  "methodName" : "getMembershipVersion",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "long getMembershipVersion()\n{\r\n    return this.translator.getProtoOrBuilder().getMembershipVersion();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\records\\impl\\pb",
  "methodName" : "setMembershipVersion",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setMembershipVersion(long version)\n{\r\n    this.translator.getBuilder().setMembershipVersion(version);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\records\\impl\\pb",
  "methodName" : "getMountTableVersion",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "long getMountTableVersion()\n{\r\n    return this.translator.getProtoOrBuilder().getMountTableVersion();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\records\\impl\\pb",
  "methodName" : "setMountTableVersion",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setMountTableVersion(long version)\n{\r\n    this.translator.getBuilder().setMountTableVersion(version);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\resolver",
  "methodName" : "getMembershipStore",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "MembershipStore getMembershipStore() throws IOException\n{\r\n    if (this.membershipInterface == null) {\r\n        this.membershipInterface = getStoreInterface(MembershipStore.class);\r\n    }\r\n    return this.membershipInterface;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\resolver",
  "methodName" : "getDisabledNameserviceStore",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "DisabledNameserviceStore getDisabledNameserviceStore() throws IOException\n{\r\n    if (this.disabledNameserviceInterface == null) {\r\n        this.disabledNameserviceInterface = getStoreInterface(DisabledNameserviceStore.class);\r\n    }\r\n    return this.disabledNameserviceInterface;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\resolver",
  "methodName" : "getStoreInterface",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "T getStoreInterface(Class<T> clazz) throws IOException\n{\r\n    T store = this.stateStore.getRegisteredRecordStore(clazz);\r\n    if (store == null) {\r\n        throw new IOException(\"State Store does not have an interface for \" + clazz.getSimpleName());\r\n    }\r\n    return store;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\resolver",
  "methodName" : "loadCache",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 7,
  "sourceCodeText" : "boolean loadCache(boolean force)\n{\r\n    try {\r\n        MembershipStore membership = getMembershipStore();\r\n        membership.loadCache(force);\r\n        DisabledNameserviceStore disabled = getDisabledNameserviceStore();\r\n        disabled.loadCache(force);\r\n    } catch (IOException e) {\r\n        LOG.error(\"Cannot update membership from the State Store\", e);\r\n    }\r\n    cacheBP.clear();\r\n    cacheNS.clear();\r\n    return true;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\resolver",
  "methodName" : "updateActiveNamenode",
  "errType" : [ "StateStoreUnavailableException" ],
  "containingMethodsNum" : 16,
  "sourceCodeText" : "void updateActiveNamenode(final String nsId, final InetSocketAddress address) throws IOException\n{\r\n    try {\r\n        MembershipState partial = MembershipState.newInstance();\r\n        String rpcAddress = address.getHostName() + \":\" + address.getPort();\r\n        partial.setRpcAddress(rpcAddress);\r\n        partial.setNameserviceId(nsId);\r\n        GetNamenodeRegistrationsRequest request = GetNamenodeRegistrationsRequest.newInstance(partial);\r\n        MembershipStore membership = getMembershipStore();\r\n        GetNamenodeRegistrationsResponse response = membership.getNamenodeRegistrations(request);\r\n        List<MembershipState> records = response.getNamenodeMemberships();\r\n        if (records != null && records.size() == 1) {\r\n            MembershipState record = records.get(0);\r\n            UpdateNamenodeRegistrationRequest updateRequest = UpdateNamenodeRegistrationRequest.newInstance(record.getNameserviceId(), record.getNamenodeId(), ACTIVE);\r\n            membership.updateNamenodeRegistration(updateRequest);\r\n            cacheNS.remove(nsId);\r\n            cacheBP.clear();\r\n        }\r\n    } catch (StateStoreUnavailableException e) {\r\n        LOG.error(\"Cannot update {} as active, State Store unavailable\", address);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\resolver",
  "methodName" : "getNamenodesForNameserviceId",
  "errType" : [ "StateStoreUnavailableException", "StateStoreUnavailableException" ],
  "containingMethodsNum" : 15,
  "sourceCodeText" : "List<? extends FederationNamenodeContext> getNamenodesForNameserviceId(final String nsId) throws IOException\n{\r\n    List<? extends FederationNamenodeContext> ret = cacheNS.get(nsId);\r\n    if (ret != null) {\r\n        return ret;\r\n    }\r\n    final List<MembershipState> result;\r\n    try {\r\n        MembershipState partial = MembershipState.newInstance();\r\n        partial.setNameserviceId(nsId);\r\n        GetNamenodeRegistrationsRequest request = GetNamenodeRegistrationsRequest.newInstance(partial);\r\n        result = getRecentRegistrationForQuery(request, true, false);\r\n    } catch (StateStoreUnavailableException e) {\r\n        LOG.error(\"Cannot get active NN for {}, State Store unavailable\", nsId);\r\n        return null;\r\n    }\r\n    if (result == null || result.isEmpty()) {\r\n        LOG.error(\"Cannot locate eligible NNs for {}\", nsId);\r\n        return null;\r\n    }\r\n    try {\r\n        Set<String> disabled = getDisabledNameserviceStore().getDisabledNameservices();\r\n        if (disabled == null) {\r\n            LOG.error(\"Cannot get disabled name services\");\r\n        } else {\r\n            for (MembershipState nn : result) {\r\n                if (disabled.contains(nn.getNameserviceId())) {\r\n                    nn.setState(FederationNamenodeServiceState.DISABLED);\r\n                }\r\n            }\r\n        }\r\n    } catch (StateStoreUnavailableException e) {\r\n        LOG.error(\"Cannot get disabled name services, State Store unavailable\");\r\n    }\r\n    ret = Collections.unmodifiableList(result);\r\n    cacheNS.put(nsId, result);\r\n    return ret;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 2,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\resolver",
  "methodName" : "getNamenodesForBlockPoolId",
  "errType" : [ "StateStoreUnavailableException" ],
  "containingMethodsNum" : 10,
  "sourceCodeText" : "List<? extends FederationNamenodeContext> getNamenodesForBlockPoolId(final String bpId) throws IOException\n{\r\n    List<? extends FederationNamenodeContext> ret = cacheBP.get(bpId);\r\n    if (ret == null) {\r\n        try {\r\n            MembershipState partial = MembershipState.newInstance();\r\n            partial.setBlockPoolId(bpId);\r\n            GetNamenodeRegistrationsRequest request = GetNamenodeRegistrationsRequest.newInstance(partial);\r\n            final List<MembershipState> result = getRecentRegistrationForQuery(request, true, false);\r\n            if (result == null || result.isEmpty()) {\r\n                LOG.error(\"Cannot locate eligible NNs for {}\", bpId);\r\n            } else {\r\n                cacheBP.put(bpId, result);\r\n                ret = result;\r\n            }\r\n        } catch (StateStoreUnavailableException e) {\r\n            LOG.error(\"Cannot get active NN for {}, State Store unavailable\", bpId);\r\n            return null;\r\n        }\r\n    }\r\n    if (ret == null) {\r\n        return null;\r\n    }\r\n    return Collections.unmodifiableList(ret);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\resolver",
  "methodName" : "registerNamenode",
  "errType" : null,
  "containingMethodsNum" : 34,
  "sourceCodeText" : "boolean registerNamenode(NamenodeStatusReport report) throws IOException\n{\r\n    if (this.routerId == null) {\r\n        LOG.warn(\"Cannot register namenode, router ID is not known {}\", report);\r\n        return false;\r\n    }\r\n    MembershipState record = MembershipState.newInstance(routerId, report.getNameserviceId(), report.getNamenodeId(), report.getClusterId(), report.getBlockPoolId(), NetUtils.normalizeIP2HostName(report.getRpcAddress()), report.getServiceAddress(), report.getLifelineAddress(), report.getWebScheme(), report.getWebAddress(), report.getState(), report.getSafemode());\r\n    if (report.statsValid()) {\r\n        MembershipStats stats = MembershipStats.newInstance();\r\n        stats.setNumOfFiles(report.getNumFiles());\r\n        stats.setNumOfBlocks(report.getNumBlocks());\r\n        stats.setNumOfBlocksMissing(report.getNumBlocksMissing());\r\n        stats.setNumOfBlocksPendingReplication(report.getNumOfBlocksPendingReplication());\r\n        stats.setNumOfBlocksUnderReplicated(report.getNumOfBlocksUnderReplicated());\r\n        stats.setNumOfBlocksPendingDeletion(report.getNumOfBlocksPendingDeletion());\r\n        stats.setAvailableSpace(report.getAvailableSpace());\r\n        stats.setTotalSpace(report.getTotalSpace());\r\n        stats.setProvidedSpace(report.getProvidedSpace());\r\n        stats.setNumOfDecommissioningDatanodes(report.getNumDecommissioningDatanodes());\r\n        stats.setNumOfActiveDatanodes(report.getNumLiveDatanodes());\r\n        stats.setNumOfDeadDatanodes(report.getNumDeadDatanodes());\r\n        stats.setNumOfStaleDatanodes(report.getNumStaleDatanodes());\r\n        stats.setNumOfDecomActiveDatanodes(report.getNumDecomLiveDatanodes());\r\n        stats.setNumOfDecomDeadDatanodes(report.getNumDecomDeadDatanodes());\r\n        stats.setNumOfInMaintenanceLiveDataNodes(report.getNumInMaintenanceLiveDataNodes());\r\n        stats.setNumOfInMaintenanceDeadDataNodes(report.getNumInMaintenanceDeadDataNodes());\r\n        stats.setNumOfEnteringMaintenanceDataNodes(report.getNumEnteringMaintenanceDataNodes());\r\n        stats.setCorruptFilesCount(report.getCorruptFilesCount());\r\n        stats.setScheduledReplicationBlocks(report.getScheduledReplicationBlocks());\r\n        stats.setNumberOfMissingBlocksWithReplicationFactorOne(report.getNumberOfMissingBlocksWithReplicationFactorOne());\r\n        stats.setHighestPriorityLowRedundancyReplicatedBlocks(report.getHighestPriorityLowRedundancyReplicatedBlocks());\r\n        stats.setHighestPriorityLowRedundancyECBlocks(report.getHighestPriorityLowRedundancyECBlocks());\r\n        stats.setPendingSPSPaths(report.getPendingSPSPaths());\r\n        record.setStats(stats);\r\n    }\r\n    if (report.getState() != UNAVAILABLE) {\r\n        record.setLastContact(Time.now());\r\n    }\r\n    NamenodeHeartbeatRequest request = NamenodeHeartbeatRequest.newInstance();\r\n    request.setNamenodeMembership(record);\r\n    return getMembershipStore().namenodeHeartbeat(request).getResult();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\resolver",
  "methodName" : "getNamespaces",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "Set<FederationNamespaceInfo> getNamespaces() throws IOException\n{\r\n    GetNamespaceInfoRequest request = GetNamespaceInfoRequest.newInstance();\r\n    GetNamespaceInfoResponse response = getMembershipStore().getNamespaceInfo(request);\r\n    Set<FederationNamespaceInfo> nss = response.getNamespaceInfo();\r\n    Set<FederationNamespaceInfo> ret = new TreeSet<>();\r\n    Set<String> disabled = getDisabledNamespaces();\r\n    for (FederationNamespaceInfo ns : nss) {\r\n        if (!disabled.contains(ns.getNameserviceId())) {\r\n            ret.add(ns);\r\n        }\r\n    }\r\n    return ret;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\resolver",
  "methodName" : "getDisabledNamespaces",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "Set<String> getDisabledNamespaces() throws IOException\n{\r\n    DisabledNameserviceStore store = getDisabledNameserviceStore();\r\n    return store.getDisabledNameservices();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\resolver",
  "methodName" : "getRecentRegistrationForQuery",
  "errType" : null,
  "containingMethodsNum" : 13,
  "sourceCodeText" : "List<MembershipState> getRecentRegistrationForQuery(GetNamenodeRegistrationsRequest request, boolean addUnavailable, boolean addExpired) throws IOException\n{\r\n    MembershipStore membershipStore = getMembershipStore();\r\n    GetNamenodeRegistrationsResponse response = membershipStore.getNamenodeRegistrations(request);\r\n    List<MembershipState> memberships = response.getNamenodeMemberships();\r\n    if (!addExpired || !addUnavailable) {\r\n        Iterator<MembershipState> iterator = memberships.iterator();\r\n        while (iterator.hasNext()) {\r\n            MembershipState membership = iterator.next();\r\n            if (membership.getState() == EXPIRED && !addExpired) {\r\n                iterator.remove();\r\n            } else if (membership.getState() == UNAVAILABLE && !addUnavailable) {\r\n                iterator.remove();\r\n            }\r\n        }\r\n    }\r\n    List<MembershipState> priorityList = new ArrayList<>();\r\n    priorityList.addAll(memberships);\r\n    Collections.sort(priorityList, new NamenodePriorityComparator());\r\n    LOG.debug(\"Selected most recent NN {} for query\", priorityList);\r\n    return priorityList;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\resolver",
  "methodName" : "setRouterId",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setRouterId(String router)\n{\r\n    this.routerId = router;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\protocol",
  "methodName" : "newInstance",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "GetRouterRegistrationRequest newInstance()\n{\r\n    return StateStoreSerializer.newRecord(GetRouterRegistrationRequest.class);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\protocol",
  "methodName" : "newInstance",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "GetRouterRegistrationRequest newInstance(String routerId)\n{\r\n    GetRouterRegistrationRequest request = newInstance();\r\n    request.setRouterId(routerId);\r\n    return request;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\protocol",
  "methodName" : "getRouterId",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getRouterId()",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\protocol",
  "methodName" : "setRouterId",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setRouterId(String routerId)",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\protocol",
  "methodName" : "newInstance",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "EnableNameserviceResponse newInstance() throws IOException\n{\r\n    return StateStoreSerializer.newRecord(EnableNameserviceResponse.class);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\protocol",
  "methodName" : "newInstance",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "EnableNameserviceResponse newInstance(boolean status) throws IOException\n{\r\n    EnableNameserviceResponse response = newInstance();\r\n    response.setStatus(status);\r\n    return response;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\protocol",
  "methodName" : "getStatus",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean getStatus()",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\protocol",
  "methodName" : "setStatus",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setStatus(boolean result)",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\protocol",
  "methodName" : "newInstance",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "UpdateMountTableEntryRequest newInstance() throws IOException\n{\r\n    return StateStoreSerializer.newRecord(UpdateMountTableEntryRequest.class);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\protocol",
  "methodName" : "newInstance",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "UpdateMountTableEntryRequest newInstance(MountTable entry) throws IOException\n{\r\n    UpdateMountTableEntryRequest request = newInstance();\r\n    request.setEntry(entry);\r\n    return request;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\protocol",
  "methodName" : "getEntry",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "MountTable getEntry() throws IOException",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\protocol",
  "methodName" : "setEntry",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setEntry(MountTable mount) throws IOException",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\protocol\\impl\\pb",
  "methodName" : "getProto",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "GetSafeModeRequestProto getProto()\n{\r\n    return translator.build();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\protocol\\impl\\pb",
  "methodName" : "setProto",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setProto(Message proto)\n{\r\n    this.translator.setProto(proto);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\protocol\\impl\\pb",
  "methodName" : "readInstance",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void readInstance(String base64String) throws IOException\n{\r\n    this.translator.readInstance(base64String);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\resolver",
  "methodName" : "getDestinationForPath",
  "errType" : null,
  "containingMethodsNum" : 10,
  "sourceCodeText" : "PathLocation getDestinationForPath(String path) throws IOException\n{\r\n    PathLocation mountTableResult = super.getDestinationForPath(path);\r\n    if (mountTableResult == null) {\r\n        LOG.error(\"The {} cannot find a location for {}\", super.getClass().getSimpleName(), path);\r\n    } else if (mountTableResult.hasMultipleDestinations()) {\r\n        DestinationOrder order = mountTableResult.getDestinationOrder();\r\n        OrderedResolver orderedResolver = orderedResolvers.get(order);\r\n        if (orderedResolver == null) {\r\n            LOG.error(\"Cannot find resolver for order {}\", order);\r\n        } else {\r\n            String firstNamespace = orderedResolver.getFirstNamespace(path, mountTableResult);\r\n            if (firstNamespace != null) {\r\n                mountTableResult = PathLocation.prioritizeDestination(mountTableResult, firstNamespace);\r\n                LOG.debug(\"Ordered locations following {} are {}\", order, mountTableResult);\r\n            } else {\r\n                LOG.error(\"Cannot get main namespace for path {} with order {}\", path, order);\r\n            }\r\n        }\r\n    }\r\n    return mountTableResult;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\resolver",
  "methodName" : "addResolver",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void addResolver(DestinationOrder order, OrderedResolver resolver)\n{\r\n    orderedResolvers.put(order, resolver);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\protocol",
  "methodName" : "newInstance",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "RefreshSuperUserGroupsConfigurationResponse newInstance() throws IOException\n{\r\n    return StateStoreSerializer.newRecord(RefreshSuperUserGroupsConfigurationResponse.class);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\protocol",
  "methodName" : "newInstance",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "RefreshSuperUserGroupsConfigurationResponse newInstance(boolean status) throws IOException\n{\r\n    RefreshSuperUserGroupsConfigurationResponse response = newInstance();\r\n    response.setStatus(status);\r\n    return response;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\protocol",
  "methodName" : "getStatus",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean getStatus()",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\protocol",
  "methodName" : "setStatus",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setStatus(boolean result)",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store",
  "methodName" : "disableNameservice",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean disableNameservice(String nsId) throws IOException",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store",
  "methodName" : "enableNameservice",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean enableNameservice(String nsId) throws IOException",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store",
  "methodName" : "getDisabledNameservices",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Set<String> getDisabledNameservices() throws IOException",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\protocol\\impl\\pb",
  "methodName" : "getProto",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "GetNamespaceInfoResponseProto getProto()\n{\r\n    return this.translator.build();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\protocol\\impl\\pb",
  "methodName" : "setProto",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setProto(Message protocol)\n{\r\n    this.translator.setProto(protocol);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\protocol\\impl\\pb",
  "methodName" : "readInstance",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void readInstance(String base64String) throws IOException\n{\r\n    this.translator.readInstance(base64String);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\protocol\\impl\\pb",
  "methodName" : "getNamespaceInfo",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "Set<FederationNamespaceInfo> getNamespaceInfo()\n{\r\n    Set<FederationNamespaceInfo> ret = new HashSet<FederationNamespaceInfo>();\r\n    List<FederationNamespaceInfoProto> namespaceList = this.translator.getProtoOrBuilder().getNamespaceInfosList();\r\n    for (FederationNamespaceInfoProto ns : namespaceList) {\r\n        FederationNamespaceInfo info = new FederationNamespaceInfo(ns.getBlockPoolId(), ns.getClusterId(), ns.getNameserviceId());\r\n        ret.add(info);\r\n    }\r\n    return ret;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\protocol\\impl\\pb",
  "methodName" : "setNamespaceInfo",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void setNamespaceInfo(Set<FederationNamespaceInfo> namespaceInfo)\n{\r\n    int index = 0;\r\n    for (FederationNamespaceInfo item : namespaceInfo) {\r\n        FederationNamespaceInfoProto.Builder itemBuilder = FederationNamespaceInfoProto.newBuilder();\r\n        itemBuilder.setClusterId(item.getClusterId());\r\n        itemBuilder.setBlockPoolId(item.getBlockPoolId());\r\n        itemBuilder.setNameserviceId(item.getNameserviceId());\r\n        this.translator.getBuilder().addNamespaceInfos(index, itemBuilder.build());\r\n        index++;\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\impl",
  "methodName" : "getExpiredNamenodeRegistrations",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "GetNamenodeRegistrationsResponse getExpiredNamenodeRegistrations(GetNamenodeRegistrationsRequest request) throws IOException\n{\r\n    GetNamenodeRegistrationsResponse response = GetNamenodeRegistrationsResponse.newInstance();\r\n    cacheReadLock.lock();\r\n    try {\r\n        Collection<MembershipState> vals = this.expiredRegistrations.values();\r\n        List<MembershipState> copyVals = new ArrayList<>(vals);\r\n        response.setNamenodeMemberships(copyVals);\r\n    } finally {\r\n        cacheReadLock.unlock();\r\n    }\r\n    return response;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\impl",
  "methodName" : "getNamespaceInfo",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "GetNamespaceInfoResponse getNamespaceInfo(GetNamespaceInfoRequest request) throws IOException\n{\r\n    Set<FederationNamespaceInfo> namespaces = new HashSet<>();\r\n    try {\r\n        cacheReadLock.lock();\r\n        namespaces.addAll(activeNamespaces);\r\n    } finally {\r\n        cacheReadLock.unlock();\r\n    }\r\n    GetNamespaceInfoResponse response = GetNamespaceInfoResponse.newInstance(namespaces);\r\n    return response;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\impl",
  "methodName" : "getNamenodeRegistrations",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "GetNamenodeRegistrationsResponse getNamenodeRegistrations(final GetNamenodeRegistrationsRequest request) throws IOException\n{\r\n    List<MembershipState> ret = null;\r\n    cacheReadLock.lock();\r\n    try {\r\n        Collection<MembershipState> registrations = activeRegistrations.values();\r\n        MembershipState partialMembership = request.getPartialMembership();\r\n        if (partialMembership == null) {\r\n            ret = new ArrayList<>(registrations);\r\n        } else {\r\n            Query<MembershipState> query = new Query<>(partialMembership);\r\n            ret = filterMultiple(query, registrations);\r\n        }\r\n    } finally {\r\n        cacheReadLock.unlock();\r\n    }\r\n    Collections.sort(ret);\r\n    GetNamenodeRegistrationsResponse response = GetNamenodeRegistrationsResponse.newInstance(ret);\r\n    return response;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\impl",
  "methodName" : "namenodeHeartbeat",
  "errType" : null,
  "containingMethodsNum" : 12,
  "sourceCodeText" : "NamenodeHeartbeatResponse namenodeHeartbeat(NamenodeHeartbeatRequest request) throws IOException\n{\r\n    MembershipState record = request.getNamenodeMembership();\r\n    String nnId = record.getNamenodeKey();\r\n    MembershipState existingEntry = null;\r\n    cacheReadLock.lock();\r\n    try {\r\n        existingEntry = this.activeRegistrations.get(nnId);\r\n    } finally {\r\n        cacheReadLock.unlock();\r\n    }\r\n    if (existingEntry != null) {\r\n        if (existingEntry.getState() != record.getState()) {\r\n            LOG.info(\"NN registration state has changed: {} -> {}\", existingEntry, record);\r\n        } else {\r\n            LOG.debug(\"Updating NN registration: {} -> {}\", existingEntry, record);\r\n        }\r\n    } else {\r\n        LOG.info(\"Inserting new NN registration: {}\", record);\r\n    }\r\n    boolean status = getDriver().put(record, true, false);\r\n    NamenodeHeartbeatResponse response = NamenodeHeartbeatResponse.newInstance(status);\r\n    return response;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\impl",
  "methodName" : "loadCache",
  "errType" : null,
  "containingMethodsNum" : 24,
  "sourceCodeText" : "boolean loadCache(boolean force) throws IOException\n{\r\n    super.loadCache(force);\r\n    cacheWriteLock.lock();\r\n    try {\r\n        this.activeRegistrations.clear();\r\n        this.expiredRegistrations.clear();\r\n        this.activeNamespaces.clear();\r\n        Map<String, List<MembershipState>> nnRegistrations = new HashMap<>();\r\n        List<MembershipState> cachedRecords = getCachedRecords();\r\n        for (MembershipState membership : cachedRecords) {\r\n            String nnId = membership.getNamenodeKey();\r\n            if (membership.getState() == FederationNamenodeServiceState.EXPIRED) {\r\n                String key = membership.getPrimaryKey();\r\n                this.expiredRegistrations.put(key, membership);\r\n            } else {\r\n                List<MembershipState> nnRegistration = nnRegistrations.get(nnId);\r\n                if (nnRegistration == null) {\r\n                    nnRegistration = new LinkedList<>();\r\n                    nnRegistrations.put(nnId, nnRegistration);\r\n                }\r\n                nnRegistration.add(membership);\r\n                if (membership.getState() != FederationNamenodeServiceState.UNAVAILABLE) {\r\n                    String bpId = membership.getBlockPoolId();\r\n                    String cId = membership.getClusterId();\r\n                    String nsId = membership.getNameserviceId();\r\n                    FederationNamespaceInfo nsInfo = new FederationNamespaceInfo(bpId, cId, nsId);\r\n                    this.activeNamespaces.add(nsInfo);\r\n                }\r\n            }\r\n        }\r\n        for (List<MembershipState> nnRegistration : nnRegistrations.values()) {\r\n            MembershipState representativeRecord = getRepresentativeQuorum(nnRegistration);\r\n            String nnKey = representativeRecord.getNamenodeKey();\r\n            this.activeRegistrations.put(nnKey, representativeRecord);\r\n        }\r\n        LOG.debug(\"Refreshed {} NN registrations from State Store\", cachedRecords.size());\r\n    } finally {\r\n        cacheWriteLock.unlock();\r\n    }\r\n    return true;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\impl",
  "methodName" : "updateNamenodeRegistration",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "UpdateNamenodeRegistrationResponse updateNamenodeRegistration(UpdateNamenodeRegistrationRequest request) throws IOException\n{\r\n    boolean status = false;\r\n    cacheWriteLock.lock();\r\n    try {\r\n        String namenode = MembershipState.getNamenodeKey(request.getNameserviceId(), request.getNamenodeId());\r\n        MembershipState member = this.activeRegistrations.get(namenode);\r\n        if (member != null) {\r\n            member.setState(request.getState());\r\n            status = true;\r\n        }\r\n    } finally {\r\n        cacheWriteLock.unlock();\r\n    }\r\n    UpdateNamenodeRegistrationResponse response = UpdateNamenodeRegistrationResponse.newInstance(status);\r\n    return response;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\impl",
  "methodName" : "getRepresentativeQuorum",
  "errType" : null,
  "containingMethodsNum" : 13,
  "sourceCodeText" : "MembershipState getRepresentativeQuorum(Collection<MembershipState> records)\n{\r\n    Map<FederationNamenodeServiceState, TreeSet<MembershipState>> occurenceMap = new HashMap<>();\r\n    for (MembershipState record : records) {\r\n        FederationNamenodeServiceState state = record.getState();\r\n        TreeSet<MembershipState> matchingSet = occurenceMap.get(state);\r\n        if (matchingSet == null) {\r\n            matchingSet = new TreeSet<>();\r\n            occurenceMap.put(state, matchingSet);\r\n        }\r\n        matchingSet.add(record);\r\n    }\r\n    TreeSet<MembershipState> largestSet = new TreeSet<>();\r\n    for (TreeSet<MembershipState> matchingSet : occurenceMap.values()) {\r\n        if (largestSet.size() < matchingSet.size()) {\r\n            largestSet = matchingSet;\r\n        }\r\n    }\r\n    if (largestSet.size() > records.size() / 2) {\r\n        return largestSet.first();\r\n    } else if (records.size() > 0) {\r\n        TreeSet<MembershipState> sortedList = new TreeSet<>(records);\r\n        LOG.debug(\"Quorum failed, using most recent: {}\", sortedList.first());\r\n        return sortedList.first();\r\n    } else {\r\n        return null;\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router\\security",
  "methodName" : "getSecretManager",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "AbstractDelegationTokenSecretManager<DelegationTokenIdentifier> getSecretManager()\n{\r\n    return this.dtSecretManager;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router\\security",
  "methodName" : "stop",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void stop()\n{\r\n    LOG.info(\"Stopping security manager\");\r\n    if (this.dtSecretManager != null) {\r\n        this.dtSecretManager.stopThreads();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router\\security",
  "methodName" : "getRemoteUser",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "UserGroupInformation getRemoteUser() throws IOException\n{\r\n    return RouterRpcServer.getRemoteUser();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router\\security",
  "methodName" : "getConnectionAuthenticationMethod",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "UserGroupInformation.AuthenticationMethod getConnectionAuthenticationMethod() throws IOException\n{\r\n    UserGroupInformation ugi = getRemoteUser();\r\n    UserGroupInformation.AuthenticationMethod authMethod = ugi.getAuthenticationMethod();\r\n    if (authMethod == UserGroupInformation.AuthenticationMethod.PROXY) {\r\n        authMethod = ugi.getRealUser().getAuthenticationMethod();\r\n    }\r\n    return authMethod;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router\\security",
  "methodName" : "isAllowedDelegationTokenOp",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "boolean isAllowedDelegationTokenOp() throws IOException\n{\r\n    AuthenticationMethod authMethod = getConnectionAuthenticationMethod();\r\n    if (UserGroupInformation.isSecurityEnabled() && (authMethod != AuthenticationMethod.KERBEROS) && (authMethod != AuthenticationMethod.KERBEROS_SSL) && (authMethod != AuthenticationMethod.CERTIFICATE)) {\r\n        return false;\r\n    }\r\n    return true;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router\\security",
  "methodName" : "getDelegationToken",
  "errType" : null,
  "containingMethodsNum" : 10,
  "sourceCodeText" : "Token<DelegationTokenIdentifier> getDelegationToken(Text renewer) throws IOException\n{\r\n    LOG.debug(\"Generate delegation token with renewer \" + renewer);\r\n    final String operationName = \"getDelegationToken\";\r\n    boolean success = false;\r\n    String tokenId = \"\";\r\n    Token<DelegationTokenIdentifier> token;\r\n    try {\r\n        if (!isAllowedDelegationTokenOp()) {\r\n            throw new IOException(\"Delegation Token can be issued only \" + \"with kerberos or web authentication\");\r\n        }\r\n        if (dtSecretManager == null || !dtSecretManager.isRunning()) {\r\n            LOG.warn(\"trying to get DT with no secret manager running\");\r\n            return null;\r\n        }\r\n        UserGroupInformation ugi = getRemoteUser();\r\n        String user = ugi.getUserName();\r\n        Text owner = new Text(user);\r\n        Text realUser = null;\r\n        if (ugi.getRealUser() != null) {\r\n            realUser = new Text(ugi.getRealUser().getUserName());\r\n        }\r\n        DelegationTokenIdentifier dtId = new DelegationTokenIdentifier(owner, renewer, realUser);\r\n        token = new Token<DelegationTokenIdentifier>(dtId, dtSecretManager);\r\n        tokenId = dtId.toStringStable();\r\n        success = true;\r\n    } finally {\r\n        logAuditEvent(success, operationName, tokenId);\r\n    }\r\n    return token;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router\\security",
  "methodName" : "renewDelegationToken",
  "errType" : [ "AccessControlException" ],
  "containingMethodsNum" : 9,
  "sourceCodeText" : "long renewDelegationToken(Token<DelegationTokenIdentifier> token) throws SecretManager.InvalidToken, IOException\n{\r\n    LOG.debug(\"Renew delegation token\");\r\n    final String operationName = \"renewDelegationToken\";\r\n    boolean success = false;\r\n    String tokenId = \"\";\r\n    long expiryTime;\r\n    try {\r\n        if (!isAllowedDelegationTokenOp()) {\r\n            throw new IOException(\"Delegation Token can be renewed only \" + \"with kerberos or web authentication\");\r\n        }\r\n        String renewer = getRemoteUser().getShortUserName();\r\n        expiryTime = dtSecretManager.renewToken(token, renewer);\r\n        final DelegationTokenIdentifier id = DFSUtil.decodeDelegationToken(token);\r\n        tokenId = id.toStringStable();\r\n        success = true;\r\n    } catch (AccessControlException ace) {\r\n        final DelegationTokenIdentifier id = DFSUtil.decodeDelegationToken(token);\r\n        tokenId = id.toStringStable();\r\n        throw ace;\r\n    } finally {\r\n        logAuditEvent(success, operationName, tokenId);\r\n    }\r\n    return expiryTime;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router\\security",
  "methodName" : "cancelDelegationToken",
  "errType" : [ "AccessControlException" ],
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void cancelDelegationToken(Token<DelegationTokenIdentifier> token) throws IOException\n{\r\n    LOG.debug(\"Cancel delegation token\");\r\n    final String operationName = \"cancelDelegationToken\";\r\n    boolean success = false;\r\n    String tokenId = \"\";\r\n    try {\r\n        String canceller = getRemoteUser().getUserName();\r\n        LOG.info(\"Cancel request by \" + canceller);\r\n        DelegationTokenIdentifier id = dtSecretManager.cancelToken(token, canceller);\r\n        tokenId = id.toStringStable();\r\n        success = true;\r\n    } catch (AccessControlException ace) {\r\n        final DelegationTokenIdentifier id = DFSUtil.decodeDelegationToken(token);\r\n        tokenId = id.toStringStable();\r\n        throw ace;\r\n    } finally {\r\n        logAuditEvent(success, operationName, tokenId);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router\\security",
  "methodName" : "createCredentials",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "Credentials createCredentials(final Router router, final UserGroupInformation ugi, final String renewer) throws IOException\n{\r\n    final Token<DelegationTokenIdentifier> token = router.getRpcServer().getDelegationToken(new Text(renewer));\r\n    if (token == null) {\r\n        return null;\r\n    }\r\n    final InetSocketAddress addr = router.getRpcServerAddress();\r\n    SecurityUtil.setTokenService(token, addr);\r\n    final Credentials c = new Credentials();\r\n    c.addToken(new Text(ugi.getShortUserName()), token);\r\n    return c;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router\\security",
  "methodName" : "verifyToken",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void verifyToken(DelegationTokenIdentifier identifier, byte[] password) throws SecretManager.InvalidToken\n{\r\n    this.dtSecretManager.verifyToken(identifier, password);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router\\security",
  "methodName" : "logAuditEvent",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void logAuditEvent(boolean succeeded, String cmd, String tokenId) throws IOException\n{\r\n    LOG.debug(\"Operation:\" + cmd + \" Status:\" + succeeded + \" TokenId:\" + tokenId);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\protocol",
  "methodName" : "newInstance",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "GetDisabledNameservicesRequest newInstance()\n{\r\n    return StateStoreSerializer.newRecord(GetDisabledNameservicesRequest.class);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store",
  "methodName" : "serviceInit",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void serviceInit(Configuration conf) throws Exception\n{\r\n    this.setIntervalMs(conf.getLong(RBFConfigKeys.FEDERATION_STORE_CONNECTION_TEST_MS, RBFConfigKeys.FEDERATION_STORE_CONNECTION_TEST_MS_DEFAULT));\r\n    super.serviceInit(conf);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store",
  "methodName" : "periodicInvoke",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void periodicInvoke()\n{\r\n    LOG.debug(\"Checking state store connection\");\r\n    if (!stateStore.isDriverReady()) {\r\n        LOG.info(\"Attempting to open state store driver.\");\r\n        stateStore.loadDriver();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\resolver",
  "methodName" : "getNameserviceId",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getNameserviceId()\n{\r\n    return this.nameserviceId;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\resolver",
  "methodName" : "getDest",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getDest()\n{\r\n    return this.nameserviceId;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\resolver",
  "methodName" : "getSrc",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getSrc()\n{\r\n    return null;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\resolver",
  "methodName" : "getClusterId",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getClusterId()\n{\r\n    return this.clusterId;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\resolver",
  "methodName" : "getBlockPoolId",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getBlockPoolId()\n{\r\n    return this.blockPoolId;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\resolver",
  "methodName" : "toString",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String toString()\n{\r\n    return this.nameserviceId + \"->\" + this.blockPoolId + \":\" + this.clusterId;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\resolver",
  "methodName" : "equals",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "boolean equals(Object obj)\n{\r\n    if (obj == null) {\r\n        return false;\r\n    }\r\n    if (obj == this) {\r\n        return true;\r\n    }\r\n    if (obj.getClass() != getClass()) {\r\n        return false;\r\n    }\r\n    FederationNamespaceInfo other = (FederationNamespaceInfo) obj;\r\n    return new EqualsBuilder().append(nameserviceId, other.nameserviceId).append(clusterId, other.clusterId).append(blockPoolId, other.blockPoolId).isEquals();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\resolver",
  "methodName" : "hashCode",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "int hashCode()\n{\r\n    return new HashCodeBuilder(17, 31).append(nameserviceId).append(clusterId).append(blockPoolId).toHashCode();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\resolver",
  "methodName" : "compareTo",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "int compareTo(RemoteLocationContext info)\n{\r\n    if (info instanceof FederationNamespaceInfo) {\r\n        FederationNamespaceInfo other = (FederationNamespaceInfo) info;\r\n        return new CompareToBuilder().append(nameserviceId, other.nameserviceId).append(clusterId, other.clusterId).append(blockPoolId, other.blockPoolId).toComparison();\r\n    }\r\n    return super.compareTo(info);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "isInSafeMode",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean isInSafeMode()\n{\r\n    return this.safeMode;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "setManualSafeMode",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setManualSafeMode(boolean mode)\n{\r\n    this.safeMode = mode;\r\n    this.isSafeModeSetManually = mode;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "enter",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void enter()\n{\r\n    LOG.info(\"Entering safe mode\");\r\n    enterSafeModeTime = now();\r\n    safeMode = true;\r\n    router.updateRouterState(RouterServiceState.SAFEMODE);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "leave",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void leave()\n{\r\n    long timeInSafemode = now() - enterSafeModeTime;\r\n    LOG.info(\"Leaving safe mode after {} milliseconds\", timeInSafemode);\r\n    RouterMetrics routerMetrics = router.getRouterMetrics();\r\n    if (routerMetrics == null) {\r\n        LOG.error(\"The Router metrics are not enabled\");\r\n    } else {\r\n        routerMetrics.setSafeModeTime(timeInSafemode);\r\n    }\r\n    safeMode = false;\r\n    router.updateRouterState(RouterServiceState.RUNNING);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "serviceInit",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void serviceInit(Configuration conf) throws Exception\n{\r\n    this.setIntervalMs(conf.getTimeDuration(RBFConfigKeys.DFS_ROUTER_CACHE_TIME_TO_LIVE_MS, RBFConfigKeys.DFS_ROUTER_CACHE_TIME_TO_LIVE_MS_DEFAULT, TimeUnit.MILLISECONDS));\r\n    this.startupInterval = conf.getTimeDuration(RBFConfigKeys.DFS_ROUTER_SAFEMODE_EXTENSION, RBFConfigKeys.DFS_ROUTER_SAFEMODE_EXTENSION_DEFAULT, TimeUnit.MILLISECONDS);\r\n    LOG.info(\"Leave startup safe mode after {} ms\", this.startupInterval);\r\n    this.staleInterval = conf.getTimeDuration(RBFConfigKeys.DFS_ROUTER_SAFEMODE_EXPIRATION, RBFConfigKeys.DFS_ROUTER_SAFEMODE_EXPIRATION_DEFAULT, TimeUnit.MILLISECONDS);\r\n    LOG.info(\"Enter safe mode after {} ms without reaching the State Store\", this.staleInterval);\r\n    this.startupTime = Time.now();\r\n    enter();\r\n    super.serviceInit(conf);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "periodicInvoke",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void periodicInvoke()\n{\r\n    long now = Time.now();\r\n    long delta = now - startupTime;\r\n    if (delta < startupInterval) {\r\n        LOG.info(\"Delaying safemode exit for {} milliseconds...\", this.startupInterval - delta);\r\n        return;\r\n    }\r\n    StateStoreService stateStore = router.getStateStore();\r\n    long cacheUpdateTime = stateStore.getCacheUpdateTime();\r\n    boolean isCacheStale = (now - cacheUpdateTime) > this.staleInterval;\r\n    if (isCacheStale) {\r\n        if (!safeMode) {\r\n            enter();\r\n        }\r\n    } else if (safeMode && !isSafeModeSetManually) {\r\n        leave();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "getNameserviceId",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getNameserviceId()",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "getDest",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getDest()",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "getSrc",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getSrc()",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "hashCode",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "int hashCode()\n{\r\n    return new HashCodeBuilder(17, 31).append(getNameserviceId()).append(getDest()).toHashCode();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "equals",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "boolean equals(Object obj)\n{\r\n    if (obj instanceof RemoteLocationContext) {\r\n        RemoteLocationContext other = (RemoteLocationContext) obj;\r\n        return this.getNameserviceId().equals(other.getNameserviceId()) && this.getDest().equals(other.getDest());\r\n    }\r\n    return false;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "compareTo",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "int compareTo(RemoteLocationContext info)\n{\r\n    int ret = this.getNameserviceId().compareTo(info.getNameserviceId());\r\n    if (ret == 0) {\r\n        ret = this.getDest().compareTo(info.getDest());\r\n    }\r\n    return ret;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\protocol",
  "methodName" : "newInstance",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "EnableNameserviceRequest newInstance()\n{\r\n    return StateStoreSerializer.newRecord(EnableNameserviceRequest.class);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\protocol",
  "methodName" : "newInstance",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "EnableNameserviceRequest newInstance(String nsId)\n{\r\n    EnableNameserviceRequest request = newInstance();\r\n    request.setNameServiceId(nsId);\r\n    return request;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\protocol",
  "methodName" : "getNameServiceId",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getNameServiceId()",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\protocol",
  "methodName" : "setNameServiceId",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setNameServiceId(String nsId)",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "setQuota",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void setQuota(String path, long namespaceQuota, long storagespaceQuota, StorageType type, boolean checkMountEntry) throws IOException\n{\r\n    if (!router.isQuotaEnabled()) {\r\n        throw new IOException(\"The quota system is disabled in Router.\");\r\n    }\r\n    if (checkMountEntry && isMountEntry(path)) {\r\n        throw new AccessControlException(\"Permission denied: \" + RouterRpcServer.getRemoteUser() + \" is not allowed to change quota of \" + path);\r\n    }\r\n    setQuotaInternal(path, null, namespaceQuota, storagespaceQuota, type);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "setQuotaInternal",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void setQuotaInternal(String path, List<RemoteLocation> locations, long namespaceQuota, long storagespaceQuota, StorageType type) throws IOException\n{\r\n    rpcServer.checkOperation(OperationCategory.WRITE);\r\n    if (locations == null) {\r\n        locations = getQuotaRemoteLocations(path);\r\n    }\r\n    if (LOG.isDebugEnabled()) {\r\n        for (RemoteLocation loc : locations) {\r\n            LOG.debug(\"Set quota for path: nsId: {}, dest: {}.\", loc.getNameserviceId(), loc.getDest());\r\n        }\r\n    }\r\n    RemoteMethod method = new RemoteMethod(\"setQuota\", new Class<?>[] { String.class, long.class, long.class, StorageType.class }, new RemoteParam(), namespaceQuota, storagespaceQuota, type);\r\n    rpcClient.invokeConcurrent(locations, method, false, false);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "getQuotaUsage",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "QuotaUsage getQuotaUsage(String path) throws IOException\n{\r\n    return aggregateQuota(path, getEachQuotaUsage(path));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "getEachQuotaUsage",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "Map<RemoteLocation, QuotaUsage> getEachQuotaUsage(String path) throws IOException\n{\r\n    rpcServer.checkOperation(OperationCategory.READ);\r\n    if (!router.isQuotaEnabled()) {\r\n        throw new IOException(\"The quota system is disabled in Router.\");\r\n    }\r\n    final List<RemoteLocation> quotaLocs = getValidQuotaLocations(path);\r\n    RemoteMethod method = new RemoteMethod(\"getQuotaUsage\", new Class<?>[] { String.class }, new RemoteParam());\r\n    Map<RemoteLocation, QuotaUsage> results = rpcClient.invokeConcurrent(quotaLocs, method, true, false, QuotaUsage.class);\r\n    return results;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "getGlobalQuota",
  "errType" : null,
  "containingMethodsNum" : 14,
  "sourceCodeText" : "QuotaUsage getGlobalQuota(String path) throws IOException\n{\r\n    if (!router.isQuotaEnabled()) {\r\n        throw new IOException(\"The quota system is disabled in Router.\");\r\n    }\r\n    long nQuota = HdfsConstants.QUOTA_RESET;\r\n    long sQuota = HdfsConstants.QUOTA_RESET;\r\n    long[] typeQuota = new long[StorageType.values().length];\r\n    eachByStorageType(t -> typeQuota[t.ordinal()] = HdfsConstants.QUOTA_RESET);\r\n    RouterQuotaManager manager = this.router.getQuotaManager();\r\n    TreeMap<String, RouterQuotaUsage> pts = manager.getParentsContainingQuota(path);\r\n    Entry<String, RouterQuotaUsage> entry = pts.lastEntry();\r\n    while (entry != null && (nQuota == HdfsConstants.QUOTA_RESET || sQuota == HdfsConstants.QUOTA_RESET || orByStorageType(t -> typeQuota[t.ordinal()] == HdfsConstants.QUOTA_RESET))) {\r\n        String ppath = entry.getKey();\r\n        QuotaUsage quota = entry.getValue();\r\n        if (nQuota == HdfsConstants.QUOTA_RESET) {\r\n            nQuota = quota.getQuota();\r\n        }\r\n        if (sQuota == HdfsConstants.QUOTA_RESET) {\r\n            sQuota = quota.getSpaceQuota();\r\n        }\r\n        eachByStorageType(t -> {\r\n            if (typeQuota[t.ordinal()] == HdfsConstants.QUOTA_RESET) {\r\n                typeQuota[t.ordinal()] = quota.getTypeQuota(t);\r\n            }\r\n        });\r\n        entry = pts.lowerEntry(ppath);\r\n    }\r\n    return new QuotaUsage.Builder().quota(nQuota).spaceQuota(sQuota).typeQuota(typeQuota).build();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "isMountEntry",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "boolean isMountEntry(String path)\n{\r\n    return router.getQuotaManager().isMountEntry(path);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "getValidQuotaLocations",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "List<RemoteLocation> getValidQuotaLocations(String path) throws IOException\n{\r\n    final List<RemoteLocation> locations = getQuotaRemoteLocations(path);\r\n    ListMultimap<String, RemoteLocation> validLocations = ArrayListMultimap.create();\r\n    for (RemoteLocation loc : locations) {\r\n        final String nsId = loc.getNameserviceId();\r\n        final Collection<RemoteLocation> dests = validLocations.get(nsId);\r\n        boolean isChildPath = false;\r\n        for (RemoteLocation d : dests) {\r\n            if (isParentEntry(loc.getDest(), d.getDest())) {\r\n                isChildPath = true;\r\n                break;\r\n            }\r\n        }\r\n        if (!isChildPath) {\r\n            validLocations.put(nsId, loc);\r\n        }\r\n    }\r\n    return Collections.unmodifiableList(new ArrayList<>(validLocations.values()));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "aggregateQuota",
  "errType" : null,
  "containingMethodsNum" : 28,
  "sourceCodeText" : "QuotaUsage aggregateQuota(String path, Map<RemoteLocation, QuotaUsage> results) throws IOException\n{\r\n    long nsCount = 0;\r\n    long ssCount = 0;\r\n    long[] typeCount = new long[StorageType.values().length];\r\n    long nsQuota = HdfsConstants.QUOTA_RESET;\r\n    long ssQuota = HdfsConstants.QUOTA_RESET;\r\n    long[] typeQuota = new long[StorageType.values().length];\r\n    eachByStorageType(t -> typeQuota[t.ordinal()] = HdfsConstants.QUOTA_RESET);\r\n    boolean hasQuotaUnset = false;\r\n    boolean isMountEntry = isMountEntry(path);\r\n    for (Map.Entry<RemoteLocation, QuotaUsage> entry : results.entrySet()) {\r\n        RemoteLocation loc = entry.getKey();\r\n        QuotaUsage usage = entry.getValue();\r\n        if (isMountEntry) {\r\n            nsCount += usage.getFileAndDirectoryCount();\r\n            ssCount += usage.getSpaceConsumed();\r\n            eachByStorageType(t -> typeCount[t.ordinal()] += usage.getTypeConsumed(t));\r\n        } else if (usage != null) {\r\n            if (!RouterQuotaManager.isQuotaSet(usage)) {\r\n                hasQuotaUnset = true;\r\n            }\r\n            nsQuota = usage.getQuota();\r\n            ssQuota = usage.getSpaceQuota();\r\n            eachByStorageType(t -> typeQuota[t.ordinal()] = usage.getTypeQuota(t));\r\n            nsCount += usage.getFileAndDirectoryCount();\r\n            ssCount += usage.getSpaceConsumed();\r\n            eachByStorageType(t -> typeCount[t.ordinal()] += usage.getTypeConsumed(t));\r\n            LOG.debug(\"Get quota usage for path: nsId: {}, dest: {},\" + \" nsCount: {}, ssCount: {}, typeCount: {}.\", loc.getNameserviceId(), loc.getDest(), usage.getFileAndDirectoryCount(), usage.getSpaceConsumed(), usage.toString(false, true, Arrays.asList(StorageType.values())));\r\n        }\r\n    }\r\n    if (isMountEntry) {\r\n        QuotaUsage quota = getGlobalQuota(path);\r\n        nsQuota = quota.getQuota();\r\n        ssQuota = quota.getSpaceQuota();\r\n        eachByStorageType(t -> typeQuota[t.ordinal()] = quota.getTypeQuota(t));\r\n    }\r\n    QuotaUsage.Builder builder = new QuotaUsage.Builder().fileAndDirectoryCount(nsCount).spaceConsumed(ssCount).typeConsumed(typeCount);\r\n    if (hasQuotaUnset) {\r\n        builder.quota(HdfsConstants.QUOTA_RESET).spaceQuota(HdfsConstants.QUOTA_RESET);\r\n        eachByStorageType(t -> builder.typeQuota(t, HdfsConstants.QUOTA_RESET));\r\n    } else {\r\n        builder.quota(nsQuota).spaceQuota(ssQuota);\r\n        eachByStorageType(t -> builder.typeQuota(t, typeQuota[t.ordinal()]));\r\n    }\r\n    return builder.build();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "eachByStorageType",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void eachByStorageType(Consumer<StorageType> consumer)\n{\r\n    for (StorageType type : StorageType.values()) {\r\n        consumer.accept(type);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "orByStorageType",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "boolean orByStorageType(Predicate<StorageType> predicate)\n{\r\n    boolean res = false;\r\n    for (StorageType type : StorageType.values()) {\r\n        res |= predicate.test(type);\r\n    }\r\n    return res;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "andByStorageType",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "boolean andByStorageType(Predicate<StorageType> predicate)\n{\r\n    boolean res = false;\r\n    for (StorageType type : StorageType.values()) {\r\n        res &= predicate.test(type);\r\n    }\r\n    return res;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "getQuotaRemoteLocations",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "List<RemoteLocation> getQuotaRemoteLocations(String path) throws IOException\n{\r\n    List<RemoteLocation> locations = new ArrayList<>();\r\n    RouterQuotaManager manager = this.router.getQuotaManager();\r\n    if (manager != null) {\r\n        Set<String> childrenPaths = manager.getPaths(path);\r\n        for (String childPath : childrenPaths) {\r\n            locations.addAll(rpcServer.getLocationsForPath(childPath, false, false));\r\n        }\r\n    }\r\n    if (locations.size() >= 1) {\r\n        return locations;\r\n    } else {\r\n        locations.addAll(rpcServer.getLocationsForPath(path, false, false));\r\n        return locations;\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\driver\\impl",
  "methodName" : "init",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "boolean init(final Configuration config, final String id, final Collection<Class<? extends BaseRecord>> records, final StateStoreMetrics metrics)\n{\r\n    boolean ret = super.init(config, id, records, metrics);\r\n    this.serializer = StateStoreSerializer.getSerializer(config);\r\n    return ret;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\driver\\impl",
  "methodName" : "serialize",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "byte[] serialize(T record)\n{\r\n    return serializer.serialize(record);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\driver\\impl",
  "methodName" : "serializeString",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String serializeString(T record)\n{\r\n    return serializer.serializeString(record);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\driver\\impl",
  "methodName" : "newRecord",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "T newRecord(String data, Class<T> clazz, boolean includeDates) throws IOException\n{\r\n    return serializer.deserialize(data, clazz);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\driver\\impl",
  "methodName" : "getPrimaryKey",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "String getPrimaryKey(BaseRecord record)\n{\r\n    String primaryKey = record.getPrimaryKey();\r\n    primaryKey = primaryKey.replaceAll(\"/\", SLASH_MARK);\r\n    primaryKey = primaryKey.replaceAll(\":\", COLON_MARK);\r\n    return primaryKey;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\protocol",
  "methodName" : "newInstance",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "NamenodeHeartbeatRequest newInstance() throws IOException\n{\r\n    return StateStoreSerializer.newRecord(NamenodeHeartbeatRequest.class);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\protocol",
  "methodName" : "newInstance",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "NamenodeHeartbeatRequest newInstance(MembershipState namenode) throws IOException\n{\r\n    NamenodeHeartbeatRequest request = newInstance();\r\n    request.setNamenodeMembership(namenode);\r\n    return request;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\protocol",
  "methodName" : "getNamenodeMembership",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "MembershipState getNamenodeMembership() throws IOException",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\protocol",
  "methodName" : "setNamenodeMembership",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setNamenodeMembership(MembershipState report) throws IOException",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router\\security\\token",
  "methodName" : "startThreads",
  "errType" : [ "Exception", "Exception", "Exception" ],
  "containingMethodsNum" : 15,
  "sourceCodeText" : "void startThreads() throws IOException\n{\r\n    super.startThreads();\r\n    if (!isTokenWatcherEnabled()) {\r\n        LOG.info(\"Watcher for tokens is disabled in this secret manager\");\r\n        try {\r\n            checkAgainstZkBeforeDeletion.set(true);\r\n            if (zkClient.checkExists().forPath(ZK_DTSM_TOKENS_ROOT) == null) {\r\n                zkClient.create().creatingParentsIfNeeded().withMode(CreateMode.PERSISTENT).forPath(ZK_DTSM_TOKENS_ROOT);\r\n            }\r\n            try {\r\n                zookeeper = zkClient.getZookeeperClient().getZooKeeper();\r\n            } catch (Exception e) {\r\n                LOG.info(\"Cannot get zookeeper client \", e);\r\n            } finally {\r\n                if (zookeeper == null) {\r\n                    throw new IOException(\"Zookeeper client is null\");\r\n                }\r\n            }\r\n            LOG.info(\"Start loading token cache\");\r\n            long start = Time.now();\r\n            rebuildTokenCache(true);\r\n            LOG.info(\"Loaded token cache in {} milliseconds\", Time.now() - start);\r\n            int syncInterval = conf.getInt(ZK_DTSM_ROUTER_TOKEN_SYNC_INTERVAL, ZK_DTSM_ROUTER_TOKEN_SYNC_INTERVAL_DEFAULT);\r\n            scheduler.scheduleAtFixedRate(new Runnable() {\r\n\r\n                @Override\r\n                public void run() {\r\n                    try {\r\n                        rebuildTokenCache(false);\r\n                    } catch (Exception e) {\r\n                    }\r\n                }\r\n            }, syncInterval, syncInterval, TimeUnit.SECONDS);\r\n        } catch (Exception e) {\r\n            LOG.error(\"Error rebuilding local cache for zkDelegationTokens \", e);\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 3,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router\\security\\token",
  "methodName" : "stopThreads",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void stopThreads()\n{\r\n    super.stopThreads();\r\n    scheduler.shutdown();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router\\security\\token",
  "methodName" : "createIdentifier",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "DelegationTokenIdentifier createIdentifier()\n{\r\n    return new DelegationTokenIdentifier();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router\\security\\token",
  "methodName" : "rebuildTokenCache",
  "errType" : [ "KeeperException|InterruptedException", "KeeperException.NoNodeException", "Exception" ],
  "containingMethodsNum" : 10,
  "sourceCodeText" : "void rebuildTokenCache(boolean initial) throws IOException\n{\r\n    localTokenCache.clear();\r\n    List<String> zkTokens;\r\n    try {\r\n        zkTokens = zookeeper.getChildren(TOKEN_PATH, false);\r\n    } catch (KeeperException | InterruptedException e) {\r\n        throw new IOException(\"Tokens cannot be fetched from path \" + TOKEN_PATH, e);\r\n    }\r\n    byte[] data;\r\n    for (String tokenPath : zkTokens) {\r\n        try {\r\n            data = zkClient.getData().forPath(ZK_DTSM_TOKENS_ROOT + \"/\" + tokenPath);\r\n        } catch (KeeperException.NoNodeException e) {\r\n            LOG.debug(\"No node in path [\" + tokenPath + \"]\");\r\n            continue;\r\n        } catch (Exception ex) {\r\n            throw new IOException(ex);\r\n        }\r\n        AbstractDelegationTokenIdentifier ident = processTokenAddOrUpdate(data);\r\n        localTokenCache.add(ident);\r\n    }\r\n    if (!initial) {\r\n        for (AbstractDelegationTokenIdentifier ident : currentTokens.keySet()) {\r\n            if (!localTokenCache.contains(ident)) {\r\n                currentTokens.remove(ident);\r\n            }\r\n        }\r\n    }\r\n    syncTokenOwnerStats();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 2,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router\\security\\token",
  "methodName" : "cancelToken",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "AbstractDelegationTokenIdentifier cancelToken(Token<AbstractDelegationTokenIdentifier> token, String canceller) throws IOException\n{\r\n    checkAgainstZkBeforeDeletion.set(false);\r\n    AbstractDelegationTokenIdentifier ident = super.cancelToken(token, canceller);\r\n    checkAgainstZkBeforeDeletion.set(true);\r\n    return ident;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router\\security\\token",
  "methodName" : "removeStoredToken",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void removeStoredToken(AbstractDelegationTokenIdentifier ident) throws IOException\n{\r\n    super.removeStoredToken(ident, checkAgainstZkBeforeDeletion.get());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router\\security\\token",
  "methodName" : "addOrUpdateToken",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void addOrUpdateToken(AbstractDelegationTokenIdentifier ident, DelegationTokenInformation info, boolean isUpdate) throws Exception\n{\r\n    currentTokens.put(ident, info);\r\n    super.addOrUpdateToken(ident, info, isUpdate);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "run",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void run()\n{\r\n    try {\r\n        SecurityUtil.doAsLoginUser(() -> {\r\n            if (UserGroupInformation.isSecurityEnabled()) {\r\n                UserGroupInformation.getLoginUser().checkTGTAndReloginFromKeytab();\r\n            }\r\n            RefreshMountTableEntriesResponse refreshMountTableEntries = manager.refreshMountTableEntries(RefreshMountTableEntriesRequest.newInstance());\r\n            success = refreshMountTableEntries.getResult();\r\n            return true;\r\n        });\r\n    } catch (IOException e) {\r\n        LOG.error(\"Failed to refresh mount table entries cache at router {}\", adminAddress, e);\r\n    } finally {\r\n        countDownLatch.countDown();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "isSuccess",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean isSuccess()\n{\r\n    return success;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "setCountDownLatch",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setCountDownLatch(CountDownLatch countDownLatch)\n{\r\n    this.countDownLatch = countDownLatch;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "toString",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String toString()\n{\r\n    return \"MountTableRefreshThread [success=\" + success + \", adminAddress=\" + adminAddress + \"]\";\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "getAdminAddress",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getAdminAddress()\n{\r\n    return adminAddress;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "fsck",
  "errType" : [ "Exception", "IOException" ],
  "containingMethodsNum" : 24,
  "sourceCodeText" : "void fsck()\n{\r\n    final long startTime = Time.monotonicNow();\r\n    try {\r\n        String warnMsg = \"Now FSCK to DFSRouter is unstable feature. \" + \"There may be incompatible changes between releases.\";\r\n        LOG.warn(warnMsg);\r\n        out.println(warnMsg);\r\n        String msg = \"Federated FSCK started by \" + UserGroupInformation.getCurrentUser() + \" from \" + remoteAddress + \" at \" + new Date();\r\n        LOG.info(msg);\r\n        out.println(msg);\r\n        StateStoreService stateStore = router.getStateStore();\r\n        MembershipStore membership = stateStore.getRegisteredRecordStore(MembershipStore.class);\r\n        GetNamenodeRegistrationsRequest request = GetNamenodeRegistrationsRequest.newInstance();\r\n        GetNamenodeRegistrationsResponse response = membership.getNamenodeRegistrations(request);\r\n        List<MembershipState> memberships = response.getNamenodeMemberships();\r\n        Collections.sort(memberships);\r\n        for (MembershipState nn : memberships) {\r\n            if (nn.getState() == FederationNamenodeServiceState.ACTIVE) {\r\n                try {\r\n                    String webAddress = nn.getWebAddress();\r\n                    out.write(\"Checking \" + nn + \" at \" + webAddress + \"\\n\");\r\n                    remoteFsck(nn);\r\n                } catch (IOException ioe) {\r\n                    out.println(\"Cannot query \" + nn + \": \" + ioe.getMessage() + \"\\n\");\r\n                }\r\n            }\r\n        }\r\n        out.println(\"Federated FSCK ended at \" + new Date() + \" in \" + (Time.monotonicNow() - startTime + \" milliseconds\"));\r\n    } catch (Exception e) {\r\n        String errMsg = \"Fsck \" + e.getMessage();\r\n        LOG.warn(errMsg, e);\r\n        out.println(\"Federated FSCK ended at \" + new Date() + \" in \" + (Time.monotonicNow() - startTime + \" milliseconds\"));\r\n        out.println(e.getMessage());\r\n        out.print(\"\\n\\n\" + errMsg);\r\n    } finally {\r\n        out.close();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 2,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "remoteFsck",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void remoteFsck(MembershipState nn) throws IOException\n{\r\n    final String scheme = nn.getWebScheme();\r\n    final String webAddress = nn.getWebAddress();\r\n    final String args = getURLArguments(pmap);\r\n    final URL url = new URL(scheme + \"://\" + webAddress + \"/fsck?\" + args);\r\n    final URLConnection conn = url.openConnection();\r\n    try (InputStream is = conn.getInputStream();\r\n        InputStreamReader isr = new InputStreamReader(is, StandardCharsets.UTF_8);\r\n        BufferedReader br = new BufferedReader(isr)) {\r\n        String line;\r\n        while ((line = br.readLine()) != null) {\r\n            out.write(line + \"\\n\");\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "getURLArguments",
  "errType" : null,
  "containingMethodsNum" : 9,
  "sourceCodeText" : "String getURLArguments(Map<String, String[]> map)\n{\r\n    StringBuilder sb = new StringBuilder();\r\n    for (Entry<String, String[]> entry : map.entrySet()) {\r\n        String key = entry.getKey();\r\n        String[] value = entry.getValue();\r\n        if (sb.length() > 0) {\r\n            sb.append(\"&\");\r\n        }\r\n        sb.append(key);\r\n        sb.append(\"=\");\r\n        sb.append(value[0]);\r\n    }\r\n    return sb.toString();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\protocol",
  "methodName" : "newInstance",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "RefreshSuperUserGroupsConfigurationRequest newInstance() throws IOException\n{\r\n    return StateStoreSerializer.newRecord(RefreshSuperUserGroupsConfigurationRequest.class);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\protocol",
  "methodName" : "newInstance",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "RefreshMountTableEntriesRequest newInstance() throws IOException\n{\r\n    return StateStoreSerializer.newRecord(RefreshMountTableEntriesRequest.class);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\resolver\\order",
  "methodName" : "getFirstNamespace",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "String getFirstNamespace(final String path, final PathLocation loc)\n{\r\n    String srcPath = loc.getSourcePath();\r\n    String trimmedPath = trimPathToChild(path, srcPath);\r\n    LOG.debug(\"Only using the first part of the path: {} -> {}\", path, trimmedPath);\r\n    return super.getFirstNamespace(trimmedPath, loc);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\resolver\\order",
  "methodName" : "trimPathToChild",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "String trimPathToChild(String path, String parent)\n{\r\n    if (path.length() <= parent.length()) {\r\n        return parent;\r\n    }\r\n    String remainder = path.substring(parent.length());\r\n    String[] components = remainder.replaceFirst(\"^/\", \"\").split(Path.SEPARATOR);\r\n    if (components.length > 0 && components[0].length() > 0) {\r\n        if (parent.endsWith(Path.SEPARATOR)) {\r\n            return parent + components[0];\r\n        } else {\r\n            return parent + Path.SEPARATOR + components[0];\r\n        }\r\n    } else {\r\n        return parent;\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\driver",
  "methodName" : "getSerializer",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "StateStoreSerializer getSerializer()\n{\r\n    return getSerializer(null);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\driver",
  "methodName" : "getSerializer",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "StateStoreSerializer getSerializer(Configuration conf)\n{\r\n    if (conf == null) {\r\n        synchronized (StateStoreSerializer.class) {\r\n            if (defaultSerializer == null) {\r\n                conf = new Configuration();\r\n                defaultSerializer = newSerializer(conf);\r\n            }\r\n        }\r\n        return defaultSerializer;\r\n    } else {\r\n        return newSerializer(conf);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\driver",
  "methodName" : "newSerializer",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "StateStoreSerializer newSerializer(final Configuration conf)\n{\r\n    Class<? extends StateStoreSerializer> serializerName = conf.getClass(RBFConfigKeys.FEDERATION_STORE_SERIALIZER_CLASS, RBFConfigKeys.FEDERATION_STORE_SERIALIZER_CLASS_DEFAULT, StateStoreSerializer.class);\r\n    return ReflectionUtils.newInstance(serializerName, conf);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\driver",
  "methodName" : "newRecord",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "T newRecord(Class<T> clazz)\n{\r\n    return getSerializer(null).newRecordInstance(clazz);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\driver",
  "methodName" : "newRecordInstance",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "T newRecordInstance(Class<T> clazz)",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\driver",
  "methodName" : "serialize",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "byte[] serialize(BaseRecord record)",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\driver",
  "methodName" : "serializeString",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String serializeString(BaseRecord record)",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\driver",
  "methodName" : "deserialize",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "T deserialize(byte[] byteArray, Class<T> clazz) throws IOException",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\driver",
  "methodName" : "deserialize",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "T deserialize(String data, Class<T> clazz) throws IOException",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\protocol",
  "methodName" : "newInstance",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "GetNamenodeRegistrationsResponse newInstance() throws IOException\n{\r\n    return StateStoreSerializer.newRecord(GetNamenodeRegistrationsResponse.class);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\protocol",
  "methodName" : "newInstance",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "GetNamenodeRegistrationsResponse newInstance(List<MembershipState> records) throws IOException\n{\r\n    GetNamenodeRegistrationsResponse response = newInstance();\r\n    response.setNamenodeMemberships(records);\r\n    return response;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\protocol",
  "methodName" : "getNamenodeMemberships",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "List<MembershipState> getNamenodeMemberships() throws IOException",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\protocol",
  "methodName" : "setNamenodeMemberships",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setNamenodeMemberships(List<MembershipState> records) throws IOException",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\protocol",
  "methodName" : "newInstance",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "NamenodeHeartbeatResponse newInstance() throws IOException\n{\r\n    return StateStoreSerializer.newRecord(NamenodeHeartbeatResponse.class);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\protocol",
  "methodName" : "newInstance",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "NamenodeHeartbeatResponse newInstance(boolean status) throws IOException\n{\r\n    NamenodeHeartbeatResponse response = newInstance();\r\n    response.setResult(status);\r\n    return response;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\protocol",
  "methodName" : "getResult",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean getResult()",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\protocol",
  "methodName" : "setResult",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setResult(boolean result)",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\records",
  "methodName" : "newInstance",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "StateStoreVersion newInstance()\n{\r\n    return StateStoreSerializer.newRecord(StateStoreVersion.class);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\records",
  "methodName" : "newInstance",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "StateStoreVersion newInstance(long membershipVersion, long mountTableVersion)\n{\r\n    StateStoreVersion record = newInstance();\r\n    record.setMembershipVersion(membershipVersion);\r\n    record.setMountTableVersion(mountTableVersion);\r\n    return record;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\records",
  "methodName" : "getMembershipVersion",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getMembershipVersion()",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\records",
  "methodName" : "setMembershipVersion",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setMembershipVersion(long version)",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\records",
  "methodName" : "getMountTableVersion",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getMountTableVersion()",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\records",
  "methodName" : "setMountTableVersion",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setMountTableVersion(long version)",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\records",
  "methodName" : "getPrimaryKeys",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "SortedMap<String, String> getPrimaryKeys()\n{\r\n    SortedMap<String, String> map = new TreeMap<String, String>();\r\n    return map;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\records",
  "methodName" : "getExpirationMs",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getExpirationMs()\n{\r\n    return -1;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\records",
  "methodName" : "setDateModified",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setDateModified(long time)\n{\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\records",
  "methodName" : "getDateModified",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getDateModified()\n{\r\n    return 0;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\records",
  "methodName" : "setDateCreated",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setDateCreated(long time)\n{\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\records",
  "methodName" : "getDateCreated",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getDateCreated()\n{\r\n    return 0;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\records",
  "methodName" : "toString",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "String toString()\n{\r\n    return \"Membership: \" + getMembershipVersion() + \" Mount Table: \" + getMountTableVersion();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\protocol\\impl\\pb",
  "methodName" : "getProto",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "GetDisabledNameservicesRequestProto getProto()\n{\r\n    return translator.build();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\protocol\\impl\\pb",
  "methodName" : "setProto",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setProto(Message proto)\n{\r\n    this.translator.setProto(proto);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\protocol\\impl\\pb",
  "methodName" : "readInstance",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void readInstance(String base64String) throws IOException\n{\r\n    this.translator.readInstance(base64String);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store",
  "methodName" : "getRouterRegistration",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "GetRouterRegistrationResponse getRouterRegistration(GetRouterRegistrationRequest request) throws IOException",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store",
  "methodName" : "getRouterRegistrations",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "GetRouterRegistrationsResponse getRouterRegistrations(GetRouterRegistrationsRequest request) throws IOException",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store",
  "methodName" : "routerHeartbeat",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "RouterHeartbeatResponse routerHeartbeat(RouterHeartbeatRequest request) throws IOException",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\protocol\\impl\\pb",
  "methodName" : "getProto",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "GetRouterRegistrationRequestProto getProto()\n{\r\n    return this.translator.build();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\protocol\\impl\\pb",
  "methodName" : "setProto",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setProto(Message proto)\n{\r\n    this.translator.setProto(proto);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\protocol\\impl\\pb",
  "methodName" : "readInstance",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void readInstance(String base64String) throws IOException\n{\r\n    this.translator.readInstance(base64String);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\protocol\\impl\\pb",
  "methodName" : "getRouterId",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String getRouterId()\n{\r\n    return this.translator.getProtoOrBuilder().getRouterId();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\protocol\\impl\\pb",
  "methodName" : "setRouterId",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setRouterId(String routerId)\n{\r\n    this.translator.getBuilder().setRouterId(routerId);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "isActive",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean isActive()\n{\r\n    return this.numThreads > 0;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "isActiveRecently",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "boolean isActiveRecently()\n{\r\n    return Time.monotonicNow() - this.lastActiveTs <= ACTIVE_WINDOW_TIME;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "isClosed",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean isClosed()\n{\r\n    return this.closed;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "isUsable",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "boolean isUsable()\n{\r\n    return !isActive() && !isClosed();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "getClient",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "ProxyAndInfo<?> getClient()\n{\r\n    this.numThreads++;\r\n    this.lastActiveTs = Time.monotonicNow();\r\n    return this.client;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "release",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void release()\n{\r\n    if (this.numThreads > 0) {\r\n        this.numThreads--;\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "close",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void close(boolean force)\n{\r\n    if (!force && this.numThreads > 0) {\r\n        LOG.error(\"Active connection with {} handlers will be closed\", this.numThreads);\r\n    }\r\n    this.closed = true;\r\n    Object proxy = this.client.getProxy();\r\n    RPC.stopProxy(proxy);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "close",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void close()\n{\r\n    close(false);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "toString",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "String toString()\n{\r\n    InetSocketAddress addr = this.client.getAddress();\r\n    Object proxy = this.client.getProxy();\r\n    Class<?> clazz = proxy.getClass();\r\n    StringBuilder sb = new StringBuilder();\r\n    sb.append(clazz.getSimpleName()).append(\"@\").append(addr).append(\"x\").append(numThreads);\r\n    if (closed) {\r\n        sb.append(\"[CLOSED]\");\r\n    }\r\n    return sb.toString();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\impl",
  "methodName" : "getRouterRegistration",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "GetRouterRegistrationResponse getRouterRegistration(GetRouterRegistrationRequest request) throws IOException\n{\r\n    final RouterState partial = RouterState.newInstance();\r\n    partial.setAddress(request.getRouterId());\r\n    final Query<RouterState> query = new Query<RouterState>(partial);\r\n    RouterState record = getDriver().get(getRecordClass(), query);\r\n    if (record != null) {\r\n        overrideExpiredRecord(record);\r\n    }\r\n    GetRouterRegistrationResponse response = GetRouterRegistrationResponse.newInstance();\r\n    response.setRouter(record);\r\n    return response;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\impl",
  "methodName" : "getRouterRegistrations",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "GetRouterRegistrationsResponse getRouterRegistrations(GetRouterRegistrationsRequest request) throws IOException\n{\r\n    QueryResult<RouterState> recordsAndTimeStamp = getCachedRecordsAndTimeStamp();\r\n    List<RouterState> records = recordsAndTimeStamp.getRecords();\r\n    long timestamp = recordsAndTimeStamp.getTimestamp();\r\n    GetRouterRegistrationsResponse response = GetRouterRegistrationsResponse.newInstance();\r\n    response.setRouters(records);\r\n    response.setTimestamp(timestamp);\r\n    return response;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\impl",
  "methodName" : "routerHeartbeat",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "RouterHeartbeatResponse routerHeartbeat(RouterHeartbeatRequest request) throws IOException\n{\r\n    RouterState record = request.getRouter();\r\n    boolean status = getDriver().put(record, true, false);\r\n    RouterHeartbeatResponse response = RouterHeartbeatResponse.newInstance(status);\r\n    return response;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\fairness",
  "methodName" : "acquirePermit",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean acquirePermit(String nsId)\n{\r\n    return true;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\fairness",
  "methodName" : "releasePermit",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void releasePermit(String nsId)\n{\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\fairness",
  "methodName" : "shutdown",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void shutdown()\n{\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\fairness",
  "methodName" : "getAvailableHandlerOnPerNs",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getAvailableHandlerOnPerNs()\n{\r\n    return \"N/A\";\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocolPB",
  "methodName" : "addMountTableEntry",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 2,
  "sourceCodeText" : "AddMountTableEntryResponseProto addMountTableEntry(RpcController controller, AddMountTableEntryRequestProto request) throws ServiceException\n{\r\n    try {\r\n        AddMountTableEntryRequest req = new AddMountTableEntryRequestPBImpl(request);\r\n        AddMountTableEntryResponse response = server.addMountTableEntry(req);\r\n        AddMountTableEntryResponsePBImpl responsePB = (AddMountTableEntryResponsePBImpl) response;\r\n        return responsePB.getProto();\r\n    } catch (IOException e) {\r\n        throw new ServiceException(e);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocolPB",
  "methodName" : "removeMountTableEntry",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 2,
  "sourceCodeText" : "RemoveMountTableEntryResponseProto removeMountTableEntry(RpcController controller, RemoveMountTableEntryRequestProto request) throws ServiceException\n{\r\n    try {\r\n        RemoveMountTableEntryRequest req = new RemoveMountTableEntryRequestPBImpl(request);\r\n        RemoveMountTableEntryResponse response = server.removeMountTableEntry(req);\r\n        RemoveMountTableEntryResponsePBImpl responsePB = (RemoveMountTableEntryResponsePBImpl) response;\r\n        return responsePB.getProto();\r\n    } catch (IOException e) {\r\n        throw new ServiceException(e);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocolPB",
  "methodName" : "getMountTableEntries",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 2,
  "sourceCodeText" : "GetMountTableEntriesResponseProto getMountTableEntries(RpcController controller, GetMountTableEntriesRequestProto request) throws ServiceException\n{\r\n    try {\r\n        GetMountTableEntriesRequest req = new GetMountTableEntriesRequestPBImpl(request);\r\n        GetMountTableEntriesResponse response = server.getMountTableEntries(req);\r\n        GetMountTableEntriesResponsePBImpl responsePB = (GetMountTableEntriesResponsePBImpl) response;\r\n        return responsePB.getProto();\r\n    } catch (IOException e) {\r\n        throw new ServiceException(e);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocolPB",
  "methodName" : "updateMountTableEntry",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 2,
  "sourceCodeText" : "UpdateMountTableEntryResponseProto updateMountTableEntry(RpcController controller, UpdateMountTableEntryRequestProto request) throws ServiceException\n{\r\n    try {\r\n        UpdateMountTableEntryRequest req = new UpdateMountTableEntryRequestPBImpl(request);\r\n        UpdateMountTableEntryResponse response = server.updateMountTableEntry(req);\r\n        UpdateMountTableEntryResponsePBImpl responsePB = (UpdateMountTableEntryResponsePBImpl) response;\r\n        return responsePB.getProto();\r\n    } catch (IOException e) {\r\n        throw new ServiceException(e);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocolPB",
  "methodName" : "refreshSuperUserGroupsConfiguration",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 3,
  "sourceCodeText" : "RefreshSuperUserGroupsConfigurationResponseProto refreshSuperUserGroupsConfiguration(RpcController controller, RefreshSuperUserGroupsConfigurationRequestProto request) throws ServiceException\n{\r\n    try {\r\n        boolean result = server.refreshSuperUserGroupsConfiguration();\r\n        RefreshSuperUserGroupsConfigurationResponse response = RefreshSuperUserGroupsConfigurationResponsePBImpl.newInstance(result);\r\n        RefreshSuperUserGroupsConfigurationResponsePBImpl responsePB = (RefreshSuperUserGroupsConfigurationResponsePBImpl) response;\r\n        return responsePB.getProto();\r\n    } catch (IOException e) {\r\n        throw new ServiceException(e);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocolPB",
  "methodName" : "enterSafeMode",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 2,
  "sourceCodeText" : "EnterSafeModeResponseProto enterSafeMode(RpcController controller, EnterSafeModeRequestProto request) throws ServiceException\n{\r\n    try {\r\n        EnterSafeModeRequest req = new EnterSafeModeRequestPBImpl(request);\r\n        EnterSafeModeResponse response = server.enterSafeMode(req);\r\n        EnterSafeModeResponsePBImpl responsePB = (EnterSafeModeResponsePBImpl) response;\r\n        return responsePB.getProto();\r\n    } catch (IOException e) {\r\n        throw new ServiceException(e);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocolPB",
  "methodName" : "leaveSafeMode",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 2,
  "sourceCodeText" : "LeaveSafeModeResponseProto leaveSafeMode(RpcController controller, LeaveSafeModeRequestProto request) throws ServiceException\n{\r\n    try {\r\n        LeaveSafeModeRequest req = new LeaveSafeModeRequestPBImpl(request);\r\n        LeaveSafeModeResponse response = server.leaveSafeMode(req);\r\n        LeaveSafeModeResponsePBImpl responsePB = (LeaveSafeModeResponsePBImpl) response;\r\n        return responsePB.getProto();\r\n    } catch (IOException e) {\r\n        throw new ServiceException(e);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocolPB",
  "methodName" : "getSafeMode",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 2,
  "sourceCodeText" : "GetSafeModeResponseProto getSafeMode(RpcController controller, GetSafeModeRequestProto request) throws ServiceException\n{\r\n    try {\r\n        GetSafeModeRequest req = new GetSafeModeRequestPBImpl(request);\r\n        GetSafeModeResponse response = server.getSafeMode(req);\r\n        GetSafeModeResponsePBImpl responsePB = (GetSafeModeResponsePBImpl) response;\r\n        return responsePB.getProto();\r\n    } catch (IOException e) {\r\n        throw new ServiceException(e);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocolPB",
  "methodName" : "disableNameservice",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 2,
  "sourceCodeText" : "DisableNameserviceResponseProto disableNameservice(RpcController controller, DisableNameserviceRequestProto request) throws ServiceException\n{\r\n    try {\r\n        DisableNameserviceRequest req = new DisableNameserviceRequestPBImpl(request);\r\n        DisableNameserviceResponse response = server.disableNameservice(req);\r\n        DisableNameserviceResponsePBImpl responsePB = (DisableNameserviceResponsePBImpl) response;\r\n        return responsePB.getProto();\r\n    } catch (IOException e) {\r\n        throw new ServiceException(e);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocolPB",
  "methodName" : "enableNameservice",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 2,
  "sourceCodeText" : "EnableNameserviceResponseProto enableNameservice(RpcController controller, EnableNameserviceRequestProto request) throws ServiceException\n{\r\n    try {\r\n        EnableNameserviceRequest req = new EnableNameserviceRequestPBImpl(request);\r\n        EnableNameserviceResponse response = server.enableNameservice(req);\r\n        EnableNameserviceResponsePBImpl responsePB = (EnableNameserviceResponsePBImpl) response;\r\n        return responsePB.getProto();\r\n    } catch (IOException e) {\r\n        throw new ServiceException(e);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocolPB",
  "methodName" : "getDisabledNameservices",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 2,
  "sourceCodeText" : "GetDisabledNameservicesResponseProto getDisabledNameservices(RpcController controller, GetDisabledNameservicesRequestProto request) throws ServiceException\n{\r\n    try {\r\n        GetDisabledNameservicesRequest req = new GetDisabledNameservicesRequestPBImpl(request);\r\n        GetDisabledNameservicesResponse response = server.getDisabledNameservices(req);\r\n        GetDisabledNameservicesResponsePBImpl responsePB = (GetDisabledNameservicesResponsePBImpl) response;\r\n        return responsePB.getProto();\r\n    } catch (IOException e) {\r\n        throw new ServiceException(e);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocolPB",
  "methodName" : "refreshMountTableEntries",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 2,
  "sourceCodeText" : "RefreshMountTableEntriesResponseProto refreshMountTableEntries(RpcController controller, RefreshMountTableEntriesRequestProto request) throws ServiceException\n{\r\n    try {\r\n        RefreshMountTableEntriesRequest req = new RefreshMountTableEntriesRequestPBImpl(request);\r\n        RefreshMountTableEntriesResponse response = server.refreshMountTableEntries(req);\r\n        RefreshMountTableEntriesResponsePBImpl responsePB = (RefreshMountTableEntriesResponsePBImpl) response;\r\n        return responsePB.getProto();\r\n    } catch (IOException e) {\r\n        throw new ServiceException(e);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\protocolPB",
  "methodName" : "getDestination",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 2,
  "sourceCodeText" : "GetDestinationResponseProto getDestination(RpcController controller, GetDestinationRequestProto request) throws ServiceException\n{\r\n    try {\r\n        GetDestinationRequest req = new GetDestinationRequestPBImpl(request);\r\n        GetDestinationResponse response = server.getDestination(req);\r\n        GetDestinationResponsePBImpl responsePB = (GetDestinationResponsePBImpl) response;\r\n        return responsePB.getProto();\r\n    } catch (IOException e) {\r\n        throw new ServiceException(e);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\records\\impl\\pb",
  "methodName" : "getProto",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "NamenodeMembershipRecordProto getProto()\n{\r\n    return this.translator.build();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\records\\impl\\pb",
  "methodName" : "setProto",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setProto(Message proto)\n{\r\n    this.translator.setProto(proto);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\records\\impl\\pb",
  "methodName" : "readInstance",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void readInstance(String base64String) throws IOException\n{\r\n    this.translator.readInstance(base64String);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\records\\impl\\pb",
  "methodName" : "setRouterId",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void setRouterId(String routerId)\n{\r\n    Builder builder = this.translator.getBuilder();\r\n    if (routerId == null) {\r\n        builder.clearRouterId();\r\n    } else {\r\n        builder.setRouterId(routerId);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\records\\impl\\pb",
  "methodName" : "setNameserviceId",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void setNameserviceId(String nameserviceId)\n{\r\n    Builder builder = this.translator.getBuilder();\r\n    if (nameserviceId == null) {\r\n        builder.clearNameserviceId();\r\n    } else {\r\n        builder.setNameserviceId(nameserviceId);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\records\\impl\\pb",
  "methodName" : "setNamenodeId",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void setNamenodeId(String namenodeId)\n{\r\n    Builder builder = this.translator.getBuilder();\r\n    if (namenodeId == null) {\r\n        builder.clearNamenodeId();\r\n    } else {\r\n        builder.setNamenodeId(namenodeId);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\records\\impl\\pb",
  "methodName" : "setWebAddress",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void setWebAddress(String webAddress)\n{\r\n    Builder builder = this.translator.getBuilder();\r\n    if (webAddress == null) {\r\n        builder.clearWebAddress();\r\n    } else {\r\n        builder.setWebAddress(webAddress);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\records\\impl\\pb",
  "methodName" : "setRpcAddress",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void setRpcAddress(String rpcAddress)\n{\r\n    Builder builder = this.translator.getBuilder();\r\n    if (rpcAddress == null) {\r\n        builder.clearRpcAddress();\r\n    } else {\r\n        builder.setRpcAddress(rpcAddress);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\records\\impl\\pb",
  "methodName" : "setServiceAddress",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setServiceAddress(String serviceAddress)\n{\r\n    this.translator.getBuilder().setServiceAddress(serviceAddress);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\records\\impl\\pb",
  "methodName" : "setLifelineAddress",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void setLifelineAddress(String lifelineAddress)\n{\r\n    Builder builder = this.translator.getBuilder();\r\n    if (lifelineAddress == null) {\r\n        builder.clearLifelineAddress();\r\n    } else {\r\n        builder.setLifelineAddress(lifelineAddress);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\records\\impl\\pb",
  "methodName" : "setIsSafeMode",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void setIsSafeMode(boolean isSafeMode)\n{\r\n    Builder builder = this.translator.getBuilder();\r\n    builder.setIsSafeMode(isSafeMode);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\records\\impl\\pb",
  "methodName" : "setClusterId",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void setClusterId(String clusterId)\n{\r\n    Builder builder = this.translator.getBuilder();\r\n    if (clusterId == null) {\r\n        builder.clearClusterId();\r\n    } else {\r\n        builder.setClusterId(clusterId);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\records\\impl\\pb",
  "methodName" : "setBlockPoolId",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void setBlockPoolId(String blockPoolId)\n{\r\n    Builder builder = this.translator.getBuilder();\r\n    if (blockPoolId == null) {\r\n        builder.clearBlockPoolId();\r\n    } else {\r\n        builder.setBlockPoolId(blockPoolId);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\records\\impl\\pb",
  "methodName" : "setState",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void setState(FederationNamenodeServiceState state)\n{\r\n    Builder builder = this.translator.getBuilder();\r\n    if (state == null) {\r\n        builder.clearState();\r\n    } else {\r\n        builder.setState(state.toString());\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\records\\impl\\pb",
  "methodName" : "setWebScheme",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void setWebScheme(String webScheme)\n{\r\n    Builder builder = this.translator.getBuilder();\r\n    if (webScheme == null) {\r\n        builder.clearWebScheme();\r\n    } else {\r\n        builder.setWebScheme(webScheme);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\records\\impl\\pb",
  "methodName" : "getRouterId",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "String getRouterId()\n{\r\n    NamenodeMembershipRecordProtoOrBuilder proto = this.translator.getProtoOrBuilder();\r\n    if (!proto.hasRouterId()) {\r\n        return null;\r\n    }\r\n    return proto.getRouterId();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\records\\impl\\pb",
  "methodName" : "getNameserviceId",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "String getNameserviceId()\n{\r\n    NamenodeMembershipRecordProtoOrBuilder proto = this.translator.getProtoOrBuilder();\r\n    if (!proto.hasNameserviceId()) {\r\n        return null;\r\n    }\r\n    return this.translator.getProtoOrBuilder().getNameserviceId();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\records\\impl\\pb",
  "methodName" : "getNamenodeId",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "String getNamenodeId()\n{\r\n    NamenodeMembershipRecordProtoOrBuilder proto = this.translator.getProtoOrBuilder();\r\n    if (!proto.hasNamenodeId()) {\r\n        return null;\r\n    }\r\n    return this.translator.getProtoOrBuilder().getNamenodeId();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\records\\impl\\pb",
  "methodName" : "getClusterId",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "String getClusterId()\n{\r\n    NamenodeMembershipRecordProtoOrBuilder proto = this.translator.getProtoOrBuilder();\r\n    if (!proto.hasClusterId()) {\r\n        return null;\r\n    }\r\n    return this.translator.getProtoOrBuilder().getClusterId();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\records\\impl\\pb",
  "methodName" : "getBlockPoolId",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "String getBlockPoolId()\n{\r\n    NamenodeMembershipRecordProtoOrBuilder proto = this.translator.getProtoOrBuilder();\r\n    if (!proto.hasBlockPoolId()) {\r\n        return null;\r\n    }\r\n    return this.translator.getProtoOrBuilder().getBlockPoolId();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\records\\impl\\pb",
  "methodName" : "getRpcAddress",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "String getRpcAddress()\n{\r\n    NamenodeMembershipRecordProtoOrBuilder proto = this.translator.getProtoOrBuilder();\r\n    if (!proto.hasRpcAddress()) {\r\n        return null;\r\n    }\r\n    return this.translator.getProtoOrBuilder().getRpcAddress();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\records\\impl\\pb",
  "methodName" : "getServiceAddress",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "String getServiceAddress()\n{\r\n    NamenodeMembershipRecordProtoOrBuilder proto = this.translator.getProtoOrBuilder();\r\n    if (!proto.hasServiceAddress()) {\r\n        return null;\r\n    }\r\n    return this.translator.getProtoOrBuilder().getServiceAddress();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\records\\impl\\pb",
  "methodName" : "getWebAddress",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "String getWebAddress()\n{\r\n    NamenodeMembershipRecordProtoOrBuilder proto = this.translator.getProtoOrBuilder();\r\n    if (!proto.hasWebAddress()) {\r\n        return null;\r\n    }\r\n    return this.translator.getProtoOrBuilder().getWebAddress();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\records\\impl\\pb",
  "methodName" : "getLifelineAddress",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "String getLifelineAddress()\n{\r\n    NamenodeMembershipRecordProtoOrBuilder proto = this.translator.getProtoOrBuilder();\r\n    if (!proto.hasLifelineAddress()) {\r\n        return null;\r\n    }\r\n    return this.translator.getProtoOrBuilder().getLifelineAddress();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\records\\impl\\pb",
  "methodName" : "getIsSafeMode",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "boolean getIsSafeMode()\n{\r\n    return this.translator.getProtoOrBuilder().getIsSafeMode();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\records\\impl\\pb",
  "methodName" : "getState",
  "errType" : [ "IllegalArgumentException" ],
  "containingMethodsNum" : 3,
  "sourceCodeText" : "FederationNamenodeServiceState getState()\n{\r\n    FederationNamenodeServiceState ret = FederationNamenodeServiceState.UNAVAILABLE;\r\n    NamenodeMembershipRecordProtoOrBuilder proto = this.translator.getProtoOrBuilder();\r\n    if (!proto.hasState()) {\r\n        return null;\r\n    }\r\n    try {\r\n        ret = FederationNamenodeServiceState.valueOf(proto.getState());\r\n    } catch (IllegalArgumentException e) {\r\n    }\r\n    return ret;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\records\\impl\\pb",
  "methodName" : "getWebScheme",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "String getWebScheme()\n{\r\n    NamenodeMembershipRecordProtoOrBuilder proto = this.translator.getProtoOrBuilder();\r\n    if (!proto.hasWebScheme()) {\r\n        return null;\r\n    }\r\n    return this.translator.getProtoOrBuilder().getWebScheme();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\records\\impl\\pb",
  "methodName" : "setStats",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void setStats(MembershipStats stats)\n{\r\n    if (stats instanceof MembershipStatsPBImpl) {\r\n        MembershipStatsPBImpl statsPB = (MembershipStatsPBImpl) stats;\r\n        NamenodeMembershipStatsRecordProto statsProto = (NamenodeMembershipStatsRecordProto) statsPB.getProto();\r\n        this.translator.getBuilder().setStats(statsProto);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\records\\impl\\pb",
  "methodName" : "getStats",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "MembershipStats getStats()\n{\r\n    NamenodeMembershipStatsRecordProto statsProto = this.translator.getProtoOrBuilder().getStats();\r\n    MembershipStats stats = StateStoreSerializer.newRecord(MembershipStats.class);\r\n    if (stats instanceof MembershipStatsPBImpl) {\r\n        MembershipStatsPBImpl statsPB = (MembershipStatsPBImpl) stats;\r\n        statsPB.setProto(statsProto);\r\n        return statsPB;\r\n    } else {\r\n        throw new IllegalArgumentException(\"Cannot get stats for the membership\");\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\records\\impl\\pb",
  "methodName" : "setLastContact",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setLastContact(long contact)\n{\r\n    this.translator.getBuilder().setLastContact(contact);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\records\\impl\\pb",
  "methodName" : "getLastContact",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "long getLastContact()\n{\r\n    return this.translator.getProtoOrBuilder().getLastContact();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\records\\impl\\pb",
  "methodName" : "setDateModified",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void setDateModified(long time)\n{\r\n    if (getState() != FederationNamenodeServiceState.EXPIRED) {\r\n        this.translator.getBuilder().setDateModified(time);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\records\\impl\\pb",
  "methodName" : "getDateModified",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "long getDateModified()\n{\r\n    return this.translator.getProtoOrBuilder().getDateModified();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\records\\impl\\pb",
  "methodName" : "setDateCreated",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setDateCreated(long time)\n{\r\n    this.translator.getBuilder().setDateCreated(time);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\records\\impl\\pb",
  "methodName" : "getDateCreated",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "long getDateCreated()\n{\r\n    return this.translator.getProtoOrBuilder().getDateCreated();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "hashCode",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "int hashCode()\n{\r\n    int hash = new HashCodeBuilder(17, 31).append(this.nnId).append(this.ugiString).append(this.getTokenIds()).append(this.protocol).toHashCode();\r\n    return hash;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "equals",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "boolean equals(Object o)\n{\r\n    if (o instanceof ConnectionPoolId) {\r\n        ConnectionPoolId other = (ConnectionPoolId) o;\r\n        if (!this.nnId.equals(other.nnId)) {\r\n            return false;\r\n        }\r\n        if (!this.ugiString.equals(other.ugiString)) {\r\n            return false;\r\n        }\r\n        String thisTokens = this.getTokenIds().toString();\r\n        String otherTokens = other.getTokenIds().toString();\r\n        if (!thisTokens.equals(otherTokens)) {\r\n            return false;\r\n        }\r\n        return this.protocol.equals(other.protocol);\r\n    }\r\n    return false;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "toString",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "String toString()\n{\r\n    return this.ugi + \" \" + this.getTokenIds() + \"->\" + this.nnId + \" [\" + this.protocol.getSimpleName() + \"]\";\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "compareTo",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "int compareTo(ConnectionPoolId other)\n{\r\n    int ret = this.nnId.compareTo(other.nnId);\r\n    if (ret == 0) {\r\n        ret = this.ugi.toString().compareTo(other.ugi.toString());\r\n    }\r\n    if (ret == 0) {\r\n        String thisTokens = this.getTokenIds().toString();\r\n        String otherTokens = other.getTokenIds().toString();\r\n        ret = thisTokens.compareTo(otherTokens);\r\n    }\r\n    if (ret == 0) {\r\n        ret = this.protocol.toString().compareTo(other.protocol.toString());\r\n    }\r\n    return ret;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "getUgi",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "UserGroupInformation getUgi()\n{\r\n    return this.ugi;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "getTokenIds",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "List<String> getTokenIds()\n{\r\n    List<String> tokenIds = new ArrayList<>();\r\n    Collection<Token<? extends TokenIdentifier>> tokens = this.ugi.getTokens();\r\n    for (Token<? extends TokenIdentifier> token : tokens) {\r\n        byte[] tokenIdBytes = token.getIdentifier();\r\n        String tokenId = Arrays.toString(tokenIdBytes);\r\n        tokenIds.add(tokenId);\r\n    }\r\n    Collections.sort(tokenIds);\r\n    return tokenIds;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\protocol\\impl\\pb",
  "methodName" : "getProto",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "GetMountTableEntriesResponseProto getProto()\n{\r\n    return this.translator.build();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\protocol\\impl\\pb",
  "methodName" : "setProto",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setProto(Message proto)\n{\r\n    this.translator.setProto(proto);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\protocol\\impl\\pb",
  "methodName" : "readInstance",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void readInstance(String base64String) throws IOException\n{\r\n    this.translator.readInstance(base64String);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\protocol\\impl\\pb",
  "methodName" : "getEntries",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "List<MountTable> getEntries() throws IOException\n{\r\n    List<MountTableRecordProto> entries = this.translator.getProtoOrBuilder().getEntriesList();\r\n    List<MountTable> ret = new ArrayList<MountTable>();\r\n    for (MountTableRecordProto entry : entries) {\r\n        MountTable record = new MountTablePBImpl(entry);\r\n        ret.add(record);\r\n    }\r\n    return ret;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\protocol\\impl\\pb",
  "methodName" : "setEntries",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void setEntries(List<MountTable> records) throws IOException\n{\r\n    this.translator.getBuilder().clearEntries();\r\n    for (MountTable entry : records) {\r\n        if (entry instanceof MountTablePBImpl) {\r\n            MountTablePBImpl entryPB = (MountTablePBImpl) entry;\r\n            this.translator.getBuilder().addEntries(entryPB.getProto());\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\protocol\\impl\\pb",
  "methodName" : "getTimestamp",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "long getTimestamp()\n{\r\n    return this.translator.getProtoOrBuilder().getTimestamp();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\protocol\\impl\\pb",
  "methodName" : "setTimestamp",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setTimestamp(long time)\n{\r\n    this.translator.getBuilder().setTimestamp(time);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\protocol\\impl\\pb",
  "methodName" : "getProto",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "GetNamespaceInfoRequestProto getProto()\n{\r\n    return this.translator.build();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\protocol\\impl\\pb",
  "methodName" : "setProto",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setProto(Message protocol)\n{\r\n    this.translator.setProto(protocol);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\protocol\\impl\\pb",
  "methodName" : "readInstance",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void readInstance(String base64String) throws IOException\n{\r\n    this.translator.readInstance(base64String);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "start",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void start()\n{\r\n    this.creator.start();\r\n    long recyleTimeMs = Math.min(poolCleanupPeriodMs, connectionCleanupPeriodMs);\r\n    LOG.info(\"Cleaning every {} seconds\", TimeUnit.MILLISECONDS.toSeconds(recyleTimeMs));\r\n    this.cleaner.scheduleAtFixedRate(new CleanupTask(), 0, recyleTimeMs, TimeUnit.MILLISECONDS);\r\n    this.running = true;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "close",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void close()\n{\r\n    this.creator.shutdown();\r\n    this.cleaner.shutdown();\r\n    this.running = false;\r\n    writeLock.lock();\r\n    try {\r\n        for (ConnectionPool pool : this.pools.values()) {\r\n            pool.close();\r\n        }\r\n        this.pools.clear();\r\n    } finally {\r\n        writeLock.unlock();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "getConnection",
  "errType" : null,
  "containingMethodsNum" : 14,
  "sourceCodeText" : "ConnectionContext getConnection(UserGroupInformation ugi, String nnAddress, Class<?> protocol) throws IOException\n{\r\n    if (!this.running) {\r\n        LOG.error(\"Cannot get a connection to {} because the manager isn't running\", nnAddress);\r\n        return null;\r\n    }\r\n    ConnectionPoolId connectionId = new ConnectionPoolId(ugi, nnAddress, protocol);\r\n    ConnectionPool pool = null;\r\n    readLock.lock();\r\n    try {\r\n        pool = this.pools.get(connectionId);\r\n    } finally {\r\n        readLock.unlock();\r\n    }\r\n    if (pool == null) {\r\n        writeLock.lock();\r\n        try {\r\n            pool = this.pools.get(connectionId);\r\n            if (pool == null) {\r\n                pool = new ConnectionPool(this.conf, nnAddress, ugi, this.minSize, this.maxSize, this.minActiveRatio, protocol);\r\n                this.pools.put(connectionId, pool);\r\n            }\r\n        } finally {\r\n            writeLock.unlock();\r\n        }\r\n    }\r\n    ConnectionContext conn = pool.getConnection();\r\n    if (conn == null || !conn.isUsable()) {\r\n        if (!this.creatorQueue.offer(pool)) {\r\n            LOG.error(\"Cannot add more than {} connections at the same time\", this.creatorQueueMaxSize);\r\n        }\r\n    }\r\n    if (conn != null && conn.isClosed()) {\r\n        LOG.error(\"We got a closed connection from {}\", pool);\r\n        conn = null;\r\n    }\r\n    return conn;\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 2,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "getNumConnectionPools",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "int getNumConnectionPools()\n{\r\n    readLock.lock();\r\n    try {\r\n        return pools.size();\r\n    } finally {\r\n        readLock.unlock();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "getNumConnections",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "int getNumConnections()\n{\r\n    int total = 0;\r\n    readLock.lock();\r\n    try {\r\n        for (ConnectionPool pool : this.pools.values()) {\r\n            total += pool.getNumConnections();\r\n        }\r\n    } finally {\r\n        readLock.unlock();\r\n    }\r\n    return total;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "getNumActiveConnections",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "int getNumActiveConnections()\n{\r\n    int total = 0;\r\n    readLock.lock();\r\n    try {\r\n        for (ConnectionPool pool : this.pools.values()) {\r\n            total += pool.getNumActiveConnections();\r\n        }\r\n    } finally {\r\n        readLock.unlock();\r\n    }\r\n    return total;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "getNumIdleConnections",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "int getNumIdleConnections()\n{\r\n    int total = 0;\r\n    readLock.lock();\r\n    try {\r\n        for (ConnectionPool pool : this.pools.values()) {\r\n            total += pool.getNumIdleConnections();\r\n        }\r\n    } finally {\r\n        readLock.unlock();\r\n    }\r\n    return total;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "getNumActiveConnectionsRecently",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "int getNumActiveConnectionsRecently()\n{\r\n    int total = 0;\r\n    readLock.lock();\r\n    try {\r\n        for (ConnectionPool pool : this.pools.values()) {\r\n            total += pool.getNumActiveConnectionsRecently();\r\n        }\r\n    } finally {\r\n        readLock.unlock();\r\n    }\r\n    return total;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "getNumCreatingConnections",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "int getNumCreatingConnections()\n{\r\n    return this.creatorQueue.size();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "getJSON",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "String getJSON()\n{\r\n    final Map<String, String> info = new TreeMap<>();\r\n    readLock.lock();\r\n    try {\r\n        for (Entry<ConnectionPoolId, ConnectionPool> entry : this.pools.entrySet()) {\r\n            ConnectionPoolId connectionPoolId = entry.getKey();\r\n            ConnectionPool pool = entry.getValue();\r\n            info.put(connectionPoolId.toString(), pool.getJSON());\r\n        }\r\n    } finally {\r\n        readLock.unlock();\r\n    }\r\n    return JSON.toString(info);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "getPools",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Map<ConnectionPoolId, ConnectionPool> getPools()\n{\r\n    return this.pools;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "cleanup",
  "errType" : null,
  "containingMethodsNum" : 11,
  "sourceCodeText" : "void cleanup(ConnectionPool pool)\n{\r\n    if (pool.getNumConnections() > pool.getMinSize()) {\r\n        long timeSinceLastActive = Time.now() - pool.getLastActiveTime();\r\n        int total = pool.getNumConnections();\r\n        int active = pool.getNumActiveConnectionsRecently();\r\n        float poolMinActiveRatio = pool.getMinActiveRatio();\r\n        if (timeSinceLastActive > connectionCleanupPeriodMs || active < poolMinActiveRatio * total) {\r\n            int targetConnectionsCount = Math.max(1, (int) (poolMinActiveRatio * total) - active);\r\n            List<ConnectionContext> conns = pool.removeConnections(targetConnectionsCount);\r\n            for (ConnectionContext conn : conns) {\r\n                conn.close();\r\n            }\r\n            LOG.debug(\"Removed connection {} used {} seconds ago. \" + \"Pool has {}/{} connections\", pool.getConnectionPoolId(), TimeUnit.MILLISECONDS.toSeconds(timeSinceLastActive), pool.getNumConnections(), pool.getMaxSize());\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\protocol",
  "methodName" : "newInstance",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "GetDisabledNameservicesResponse newInstance()\n{\r\n    return StateStoreSerializer.newRecord(GetDisabledNameservicesResponse.class);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\protocol",
  "methodName" : "newInstance",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "GetDisabledNameservicesResponse newInstance(Set<String> nsIds)\n{\r\n    GetDisabledNameservicesResponse response = newInstance();\r\n    response.setNameservices(nsIds);\r\n    return response;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\protocol",
  "methodName" : "getNameservices",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Set<String> getNameservices()",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\protocol",
  "methodName" : "setNameservices",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setNameservices(Set<String> nameservices)",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "serviceInit",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void serviceInit(Configuration conf) throws Exception\n{\r\n    this.setIntervalMs(conf.getTimeDuration(RBFConfigKeys.DFS_ROUTER_QUOTA_CACHE_UPDATE_INTERVAL, RBFConfigKeys.DFS_ROUTER_QUOTA_CACHE_UPDATE_INTERVAL_DEFAULT, TimeUnit.MILLISECONDS));\r\n    super.serviceInit(conf);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "periodicInvoke",
  "errType" : [ "IOException", "IOException" ],
  "containingMethodsNum" : 25,
  "sourceCodeText" : "void periodicInvoke()\n{\r\n    LOG.debug(\"Start to update quota cache.\");\r\n    try {\r\n        List<MountTable> mountTables = getQuotaSetMountTables();\r\n        Map<RemoteLocation, QuotaUsage> remoteQuotaUsage = new HashMap<>();\r\n        for (MountTable entry : mountTables) {\r\n            String src = entry.getSourcePath();\r\n            RouterQuotaUsage oldQuota = entry.getQuota();\r\n            long nsQuota = oldQuota.getQuota();\r\n            long ssQuota = oldQuota.getSpaceQuota();\r\n            long[] typeQuota = new long[StorageType.values().length];\r\n            Quota.eachByStorageType(t -> typeQuota[t.ordinal()] = oldQuota.getTypeQuota(t));\r\n            QuotaUsage currentQuotaUsage = null;\r\n            HdfsFileStatus ret = this.rpcServer.getFileInfo(src);\r\n            if (ret == null || ret.getModificationTime() == 0) {\r\n                long[] zeroConsume = new long[StorageType.values().length];\r\n                currentQuotaUsage = new RouterQuotaUsage.Builder().fileAndDirectoryCount(0).quota(nsQuota).spaceConsumed(0).spaceQuota(ssQuota).typeConsumed(zeroConsume).typeQuota(typeQuota).build();\r\n            } else {\r\n                try {\r\n                    Quota quotaModule = this.rpcServer.getQuotaModule();\r\n                    Map<RemoteLocation, QuotaUsage> usageMap = quotaModule.getEachQuotaUsage(src);\r\n                    currentQuotaUsage = quotaModule.aggregateQuota(src, usageMap);\r\n                    remoteQuotaUsage.putAll(usageMap);\r\n                } catch (IOException ioe) {\r\n                    LOG.error(\"Unable to get quota usage for \" + src, ioe);\r\n                    continue;\r\n                }\r\n            }\r\n            RouterQuotaUsage newQuota = generateNewQuota(oldQuota, currentQuotaUsage);\r\n            this.quotaManager.put(src, newQuota);\r\n            entry.setQuota(newQuota);\r\n        }\r\n        for (Entry<RemoteLocation, QuotaUsage> en : remoteQuotaUsage.entrySet()) {\r\n            RemoteLocation remoteLocation = en.getKey();\r\n            QuotaUsage currentQuota = en.getValue();\r\n            fixGlobalQuota(remoteLocation, currentQuota);\r\n        }\r\n    } catch (IOException e) {\r\n        LOG.error(\"Quota cache updated error.\", e);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 2,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "fixGlobalQuota",
  "errType" : null,
  "containingMethodsNum" : 12,
  "sourceCodeText" : "void fixGlobalQuota(RemoteLocation location, QuotaUsage remoteQuota) throws IOException\n{\r\n    QuotaUsage gQuota = this.rpcServer.getQuotaModule().getGlobalQuota(location.getSrc());\r\n    if (remoteQuota.getQuota() != gQuota.getQuota() || remoteQuota.getSpaceQuota() != gQuota.getSpaceQuota()) {\r\n        this.rpcServer.getQuotaModule().setQuotaInternal(location.getSrc(), Arrays.asList(location), gQuota.getQuota(), gQuota.getSpaceQuota(), null);\r\n        LOG.info(\"[Fix Quota] src={} dst={} oldQuota={}/{} newQuota={}/{}\", location.getSrc(), location, remoteQuota.getQuota(), remoteQuota.getSpaceQuota(), gQuota.getQuota(), gQuota.getSpaceQuota());\r\n    }\r\n    for (StorageType t : StorageType.values()) {\r\n        if (remoteQuota.getTypeQuota(t) != gQuota.getTypeQuota(t)) {\r\n            this.rpcServer.getQuotaModule().setQuotaInternal(location.getSrc(), Arrays.asList(location), HdfsConstants.QUOTA_DONT_SET, gQuota.getTypeQuota(t), t);\r\n            LOG.info(\"[Fix Quota] src={} dst={} type={} oldQuota={} newQuota={}\", location.getSrc(), location, t, remoteQuota.getTypeQuota(t), gQuota.getTypeQuota(t));\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "getMountTableStore",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "MountTableStore getMountTableStore() throws IOException\n{\r\n    if (this.mountTableStore == null) {\r\n        this.mountTableStore = router.getStateStore().getRegisteredRecordStore(MountTableStore.class);\r\n        if (this.mountTableStore == null) {\r\n            throw new IOException(\"Mount table state store is not available.\");\r\n        }\r\n    }\r\n    return this.mountTableStore;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "getMountTableEntries",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "List<MountTable> getMountTableEntries() throws IOException\n{\r\n    GetMountTableEntriesRequest getRequest = GetMountTableEntriesRequest.newInstance(\"/\");\r\n    GetMountTableEntriesResponse getResponse = getMountTableStore().getMountTableEntries(getRequest);\r\n    return getResponse.getEntries();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "getQuotaSetMountTables",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "List<MountTable> getQuotaSetMountTables() throws IOException\n{\r\n    List<MountTable> mountTables = getMountTableEntries();\r\n    Set<String> allPaths = this.quotaManager.getAll();\r\n    Set<String> stalePaths = new HashSet<>(allPaths);\r\n    List<MountTable> neededMountTables = new LinkedList<>();\r\n    for (MountTable entry : mountTables) {\r\n        if (isQuotaSet(entry)) {\r\n            neededMountTables.add(entry);\r\n        }\r\n        String src = entry.getSourcePath();\r\n        this.quotaManager.updateQuota(src, entry.getQuota());\r\n        stalePaths.remove(src);\r\n    }\r\n    for (String stalePath : stalePaths) {\r\n        this.quotaManager.remove(stalePath);\r\n    }\r\n    return neededMountTables;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "isQuotaSet",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "boolean isQuotaSet(MountTable mountTable)\n{\r\n    if (mountTable != null) {\r\n        return this.quotaManager.isQuotaSet(mountTable.getQuota());\r\n    }\r\n    return false;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "generateNewQuota",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "RouterQuotaUsage generateNewQuota(RouterQuotaUsage oldQuota, QuotaUsage currentQuotaUsage)\n{\r\n    RouterQuotaUsage.Builder newQuotaBuilder = new RouterQuotaUsage.Builder().fileAndDirectoryCount(currentQuotaUsage.getFileAndDirectoryCount()).quota(oldQuota.getQuota()).spaceConsumed(currentQuotaUsage.getSpaceConsumed()).spaceQuota(oldQuota.getSpaceQuota());\r\n    Quota.eachByStorageType(t -> {\r\n        newQuotaBuilder.typeQuota(t, oldQuota.getTypeQuota(t));\r\n        newQuotaBuilder.typeConsumed(t, currentQuotaUsage.getTypeConsumed(t));\r\n    });\r\n    return newQuotaBuilder.build();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\protocol",
  "methodName" : "newInstance",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "GetDestinationResponse newInstance() throws IOException\n{\r\n    return StateStoreSerializer.newRecord(GetDestinationResponse.class);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\protocol",
  "methodName" : "newInstance",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "GetDestinationResponse newInstance(Collection<String> nsIds) throws IOException\n{\r\n    GetDestinationResponse request = newInstance();\r\n    request.setDestinations(nsIds);\r\n    return request;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\protocol",
  "methodName" : "getDestinations",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Collection<String> getDestinations()",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\protocol",
  "methodName" : "setDestination",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setDestination(String nsId)\n{\r\n    setDestinations(Collections.singletonList(nsId));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\protocol",
  "methodName" : "setDestinations",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setDestinations(Collection<String> nsIds)",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\metrics",
  "methodName" : "create",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "FederationRPCMetrics create(Configuration conf, RouterRpcServer rpcServer)\n{\r\n    MetricsSystem ms = DefaultMetricsSystem.instance();\r\n    return ms.register(FederationRPCMetrics.class.getName(), \"HDFS Federation RPC Metrics\", new FederationRPCMetrics(conf, rpcServer));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\metrics",
  "methodName" : "reset",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void reset()\n{\r\n    MetricsSystem ms = DefaultMetricsSystem.instance();\r\n    ms.unregisterSource(FederationRPCMetrics.class.getName());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\metrics",
  "methodName" : "incrProxyOpFailureStandby",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void incrProxyOpFailureStandby()\n{\r\n    proxyOpFailureStandby.incr();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\metrics",
  "methodName" : "getProxyOpFailureStandby",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "long getProxyOpFailureStandby()\n{\r\n    return proxyOpFailureStandby.value();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\metrics",
  "methodName" : "incrProxyOpFailureCommunicate",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void incrProxyOpFailureCommunicate()\n{\r\n    proxyOpFailureCommunicate.incr();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\metrics",
  "methodName" : "getProxyOpFailureCommunicate",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "long getProxyOpFailureCommunicate()\n{\r\n    return proxyOpFailureCommunicate.value();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\metrics",
  "methodName" : "incrProxyOpFailureClientOverloaded",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void incrProxyOpFailureClientOverloaded()\n{\r\n    proxyOpFailureClientOverloaded.incr();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\metrics",
  "methodName" : "getProxyOpFailureClientOverloaded",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "long getProxyOpFailureClientOverloaded()\n{\r\n    return proxyOpFailureClientOverloaded.value();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\metrics",
  "methodName" : "incrProxyOpNotImplemented",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void incrProxyOpNotImplemented()\n{\r\n    proxyOpNotImplemented.incr();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\metrics",
  "methodName" : "getProxyOpNotImplemented",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "long getProxyOpNotImplemented()\n{\r\n    return proxyOpNotImplemented.value();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\metrics",
  "methodName" : "incrProxyOpRetries",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void incrProxyOpRetries()\n{\r\n    proxyOpRetries.incr();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\metrics",
  "methodName" : "getProxyOpRetries",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "long getProxyOpRetries()\n{\r\n    return proxyOpRetries.value();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\metrics",
  "methodName" : "incrProxyOpNoNamenodes",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void incrProxyOpNoNamenodes()\n{\r\n    proxyOpNoNamenodes.incr();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\metrics",
  "methodName" : "getProxyOpNoNamenodes",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "long getProxyOpNoNamenodes()\n{\r\n    return proxyOpNoNamenodes.value();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\metrics",
  "methodName" : "incrRouterFailureStateStore",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void incrRouterFailureStateStore()\n{\r\n    routerFailureStateStore.incr();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\metrics",
  "methodName" : "getRouterFailureStateStoreOps",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "long getRouterFailureStateStoreOps()\n{\r\n    return routerFailureStateStore.value();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\metrics",
  "methodName" : "incrRouterFailureSafemode",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void incrRouterFailureSafemode()\n{\r\n    routerFailureSafemode.incr();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\metrics",
  "methodName" : "getRouterFailureSafemodeOps",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "long getRouterFailureSafemodeOps()\n{\r\n    return routerFailureSafemode.value();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\metrics",
  "methodName" : "incrRouterFailureReadOnly",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void incrRouterFailureReadOnly()\n{\r\n    routerFailureReadOnly.incr();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\metrics",
  "methodName" : "getRouterFailureReadOnlyOps",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "long getRouterFailureReadOnlyOps()\n{\r\n    return routerFailureReadOnly.value();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\metrics",
  "methodName" : "incrRouterFailureLocked",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void incrRouterFailureLocked()\n{\r\n    routerFailureLocked.incr();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\metrics",
  "methodName" : "getRouterFailureLockedOps",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "long getRouterFailureLockedOps()\n{\r\n    return routerFailureLocked.value();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\metrics",
  "methodName" : "getRpcServerCallQueue",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "int getRpcServerCallQueue()\n{\r\n    return rpcServer.getServer().getCallQueueLen();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\metrics",
  "methodName" : "getRpcServerNumOpenConnections",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "int getRpcServerNumOpenConnections()\n{\r\n    return rpcServer.getServer().getNumOpenConnections();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\metrics",
  "methodName" : "getRpcClientNumConnections",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "int getRpcClientNumConnections()\n{\r\n    return rpcServer.getRPCClient().getNumConnections();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\metrics",
  "methodName" : "getRpcClientNumActiveConnections",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "int getRpcClientNumActiveConnections()\n{\r\n    return rpcServer.getRPCClient().getNumActiveConnections();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\metrics",
  "methodName" : "getRpcClientNumIdleConnections",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "int getRpcClientNumIdleConnections()\n{\r\n    return rpcServer.getRPCClient().getNumIdleConnections();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\metrics",
  "methodName" : "getRpcClientNumActiveConnectionsRecently",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "int getRpcClientNumActiveConnectionsRecently()\n{\r\n    return rpcServer.getRPCClient().getNumActiveConnectionsRecently();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\metrics",
  "methodName" : "getRpcClientNumCreatingConnections",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "int getRpcClientNumCreatingConnections()\n{\r\n    return rpcServer.getRPCClient().getNumCreatingConnections();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\metrics",
  "methodName" : "getRpcClientNumConnectionPools",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "int getRpcClientNumConnectionPools()\n{\r\n    return rpcServer.getRPCClient().getNumConnectionPools();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\metrics",
  "methodName" : "getRpcClientConnections",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String getRpcClientConnections()\n{\r\n    return rpcServer.getRPCClient().getJSON();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\metrics",
  "methodName" : "getAvailableHandlerOnPerNs",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String getAvailableHandlerOnPerNs()\n{\r\n    return rpcServer.getRPCClient().getRouterRpcFairnessPolicyController().getAvailableHandlerOnPerNs();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\metrics",
  "methodName" : "getAsyncCallerPool",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String getAsyncCallerPool()\n{\r\n    return rpcServer.getRPCClient().getAsyncCallerPoolJson();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\metrics",
  "methodName" : "addProxyTime",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void addProxyTime(long time)\n{\r\n    proxy.add(time);\r\n    proxyOp.incr();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\metrics",
  "methodName" : "getProxyAvg",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "double getProxyAvg()\n{\r\n    return proxy.lastStat().mean();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\metrics",
  "methodName" : "getProxyOps",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "long getProxyOps()\n{\r\n    return proxyOp.value();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\metrics",
  "methodName" : "addProcessingTime",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void addProcessingTime(long time)\n{\r\n    processing.add(time);\r\n    processingOp.incr();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\metrics",
  "methodName" : "getProcessingAvg",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "double getProcessingAvg()\n{\r\n    return processing.lastStat().mean();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\metrics",
  "methodName" : "getProcessingOps",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "long getProcessingOps()\n{\r\n    return processingOp.value();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\metrics",
  "methodName" : "incrProxyOpPermitRejected",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void incrProxyOpPermitRejected()\n{\r\n    proxyOpPermitRejected.incr();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\metrics",
  "methodName" : "getProxyOpPermitRejected",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "long getProxyOpPermitRejected()\n{\r\n    return proxyOpPermitRejected.value();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\metrics",
  "methodName" : "getProxyOpPermitRejectedPerNs",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String getProxyOpPermitRejectedPerNs()\n{\r\n    return rpcServer.getRPCClient().getRejectedPermitsPerNsJSON();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\metrics",
  "methodName" : "getProxyOpPermitAcceptedPerNs",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String getProxyOpPermitAcceptedPerNs()\n{\r\n    return rpcServer.getRPCClient().getAcceptedPermitsPerNsJSON();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\protocol",
  "methodName" : "newInstance",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "GetMountTableEntriesResponse newInstance() throws IOException\n{\r\n    return StateStoreSerializer.newRecord(GetMountTableEntriesResponse.class);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\protocol",
  "methodName" : "getEntries",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "List<MountTable> getEntries() throws IOException",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\protocol",
  "methodName" : "setEntries",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setEntries(List<MountTable> entries) throws IOException",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\protocol",
  "methodName" : "getTimestamp",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getTimestamp()",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\protocol",
  "methodName" : "setTimestamp",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setTimestamp(long time)",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\protocol\\impl\\pb",
  "methodName" : "getProto",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "EnterSafeModeResponseProto getProto()\n{\r\n    return translator.build();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\protocol\\impl\\pb",
  "methodName" : "setProto",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setProto(Message proto)\n{\r\n    this.translator.setProto(proto);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\protocol\\impl\\pb",
  "methodName" : "readInstance",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void readInstance(String base64String) throws IOException\n{\r\n    this.translator.readInstance(base64String);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\protocol\\impl\\pb",
  "methodName" : "getStatus",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "boolean getStatus()\n{\r\n    return this.translator.getProtoOrBuilder().getStatus();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\protocol\\impl\\pb",
  "methodName" : "setStatus",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setStatus(boolean result)\n{\r\n    this.translator.getBuilder().setStatus(result);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "updateStateAsync",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void updateStateAsync()\n{\r\n    Thread thread = new Thread(new Runnable() {\r\n\r\n        @Override\r\n        public void run() {\r\n            updateStateStore();\r\n        }\r\n    }, \"Router Heartbeat Async\");\r\n    thread.setDaemon(true);\r\n    thread.start();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "updateStateStore",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 16,
  "sourceCodeText" : "void updateStateStore()\n{\r\n    String routerId = router.getRouterId();\r\n    if (routerId == null) {\r\n        LOG.error(\"Cannot heartbeat for router: unknown router id\");\r\n        return;\r\n    }\r\n    if (isStoreAvailable()) {\r\n        RouterStore routerStore = router.getRouterStateManager();\r\n        try {\r\n            RouterState record = RouterState.newInstance(routerId, router.getStartTime(), router.getRouterState());\r\n            StateStoreVersion stateStoreVersion = StateStoreVersion.newInstance(getStateStoreVersion(MembershipStore.class), getStateStoreVersion(MountTableStore.class));\r\n            record.setStateStoreVersion(stateStoreVersion);\r\n            String hostPort = StateStoreUtils.getHostPortString(router.getAdminServerAddress());\r\n            record.setAdminAddress(hostPort);\r\n            RouterHeartbeatRequest request = RouterHeartbeatRequest.newInstance(record);\r\n            RouterHeartbeatResponse response = routerStore.routerHeartbeat(request);\r\n            if (!response.getStatus()) {\r\n                LOG.warn(\"Cannot heartbeat router {}\", routerId);\r\n            } else {\r\n                LOG.debug(\"Router heartbeat for router {}\", routerId);\r\n            }\r\n        } catch (IOException e) {\r\n            LOG.error(\"Cannot heartbeat router {}\", routerId, e);\r\n        }\r\n    } else {\r\n        LOG.warn(\"Cannot heartbeat router {}: State Store unavailable\", routerId);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "getStateStoreVersion",
  "errType" : [ "Exception" ],
  "containingMethodsNum" : 6,
  "sourceCodeText" : "long getStateStoreVersion(final Class<S> clazz)\n{\r\n    long version = -1;\r\n    try {\r\n        StateStoreService stateStore = router.getStateStore();\r\n        S recordStore = stateStore.getRegisteredRecordStore(clazz);\r\n        if (recordStore != null) {\r\n            if (recordStore instanceof CachedRecordStore) {\r\n                CachedRecordStore<R> cachedRecordStore = (CachedRecordStore<R>) recordStore;\r\n                List<R> records = cachedRecordStore.getCachedRecords();\r\n                for (BaseRecord record : records) {\r\n                    if (record.getDateModified() > version) {\r\n                        version = record.getDateModified();\r\n                    }\r\n                }\r\n            }\r\n        }\r\n    } catch (Exception e) {\r\n        LOG.error(\"Cannot get version for {}\", clazz, e);\r\n    }\r\n    return version;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "serviceInit",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void serviceInit(Configuration conf) throws Exception\n{\r\n    long interval = conf.getTimeDuration(RBFConfigKeys.DFS_ROUTER_HEARTBEAT_STATE_INTERVAL_MS, RBFConfigKeys.DFS_ROUTER_HEARTBEAT_STATE_INTERVAL_MS_DEFAULT, TimeUnit.MILLISECONDS);\r\n    this.setIntervalMs(interval);\r\n    super.serviceInit(conf);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "periodicInvoke",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void periodicInvoke()\n{\r\n    updateStateStore();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "isStoreAvailable",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "boolean isStoreAvailable()\n{\r\n    if (router.getRouterStateManager() == null) {\r\n        return false;\r\n    }\r\n    if (router.getStateStore() == null) {\r\n        return false;\r\n    }\r\n    return router.getStateStore().isDriverReady();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "routerFedRename",
  "errType" : [ "InterruptedException" ],
  "containingMethodsNum" : 11,
  "sourceCodeText" : "boolean routerFedRename(final String src, final String dst, final List<RemoteLocation> srcLocations, final List<RemoteLocation> dstLocations) throws IOException\n{\r\n    if (!rpcServer.isEnableRenameAcrossNamespace()) {\r\n        throw new IOException(\"Rename of \" + src + \" to \" + dst + \" is not allowed, no eligible destination in the same namespace was\" + \" found\");\r\n    }\r\n    if (srcLocations.size() != 1 || dstLocations.size() != 1) {\r\n        throw new IOException(\"Rename of \" + src + \" to \" + dst + \" is not\" + \" allowed. The remote location should be exactly one.\");\r\n    }\r\n    RemoteLocation srcLoc = srcLocations.get(0);\r\n    RemoteLocation dstLoc = dstLocations.get(0);\r\n    checkSnapshotPath(srcLoc, dstLoc);\r\n    checkPermission(srcLoc, dstLoc);\r\n    UserGroupInformation routerUser = UserGroupInformation.getLoginUser();\r\n    try {\r\n        return routerUser.doAs((PrivilegedExceptionAction<Boolean>) () -> {\r\n            BalanceJob job = buildRouterRenameJob(srcLoc.getNameserviceId(), dstLoc.getNameserviceId(), srcLoc.getDest(), dstLoc.getDest());\r\n            BalanceProcedureScheduler scheduler = rpcServer.getFedRenameScheduler();\r\n            countIncrement();\r\n            try {\r\n                scheduler.submit(job);\r\n                LOG.info(\"Rename {} to {} from namespace {} to {}. JobId={}.\", src, dst, srcLoc.getNameserviceId(), dstLoc.getNameserviceId(), job.getId());\r\n                scheduler.waitUntilDone(job);\r\n                if (job.getError() != null) {\r\n                    throw new IOException(\"Rename of \" + src + \" to \" + dst + \" failed.\", job.getError());\r\n                }\r\n                return true;\r\n            } finally {\r\n                countDecrement();\r\n            }\r\n        });\r\n    } catch (InterruptedException e) {\r\n        LOG.warn(\"Fed balance job is interrupted.\", e);\r\n        throw new InterruptedIOException(e.getMessage());\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 2,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "checkPermission",
  "errType" : [ "AccessControlException", "InterruptedException" ],
  "containingMethodsNum" : 9,
  "sourceCodeText" : "void checkPermission(RemoteLocation src, RemoteLocation dst) throws IOException\n{\r\n    try {\r\n        if (UserGroupInformation.isSecurityEnabled()) {\r\n            String remoteUserName = NameNode.getRemoteUser().getShortUserName();\r\n            UserGroupInformation proxyUser = UserGroupInformation.createProxyUser(remoteUserName, UserGroupInformation.getLoginUser());\r\n            proxyUser.doAs((PrivilegedExceptionAction<Object>) () -> {\r\n                checkRenamePermission(src, dst);\r\n                return null;\r\n            });\r\n        } else {\r\n            checkRenamePermission(src, dst);\r\n        }\r\n    } catch (AccessControlException e) {\r\n        throw new AccessControlException(\"Permission denied rename \" + src.getSrc() + \"(\" + src + \") to \" + dst.getSrc() + \"(\" + dst + \") Reason=\" + e.getMessage());\r\n    } catch (InterruptedException e) {\r\n        Thread.currentThread().interrupt();\r\n        throw new InterruptedIOException(\"Router Federation Rename is interrupted while checking permission.\");\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "checkRenamePermission",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void checkRenamePermission(RemoteLocation srcLoc, RemoteLocation dstLoc) throws IOException\n{\r\n    Path srcPath = new Path(\"hdfs://\" + srcLoc.getNameserviceId() + srcLoc.getDest());\r\n    srcPath.getFileSystem(conf).access(srcPath.getParent(), FsAction.WRITE);\r\n    Path dstPath = new Path(\"hdfs://\" + dstLoc.getNameserviceId() + dstLoc.getDest());\r\n    dstPath.getFileSystem(conf).access(dstPath.getParent(), FsAction.WRITE);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "checkSnapshotPath",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void checkSnapshotPath(RemoteLocation src, RemoteLocation dst) throws AccessControlException\n{\r\n    if (src.getDest().contains(HdfsConstants.SEPARATOR_DOT_SNAPSHOT_DIR + Path.SEPARATOR)) {\r\n        throw new AccessControlException(\"Router federation rename can't rename snapshot path. src=\" + src.getSrc() + \"(\" + src + \")\");\r\n    }\r\n    if (dst.getDest().contains(HdfsConstants.SEPARATOR_DOT_SNAPSHOT_DIR + Path.SEPARATOR)) {\r\n        throw new AccessControlException(\"Router federation rename can't rename snapshot path. dst=\" + dst.getSrc() + \"(\" + dst + \")\");\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "buildRouterRenameJob",
  "errType" : null,
  "containingMethodsNum" : 13,
  "sourceCodeText" : "BalanceJob buildRouterRenameJob(String srcNs, String dstNs, String src, String dst) throws IOException\n{\r\n    checkConfiguration(conf);\r\n    Path srcPath = new Path(\"hdfs://\" + srcNs + src);\r\n    Path dstPath = new Path(\"hdfs://\" + dstNs + dst);\r\n    boolean forceCloseOpen = conf.getBoolean(DFS_ROUTER_FEDERATION_RENAME_FORCE_CLOSE_OPEN_FILE, DFS_ROUTER_FEDERATION_RENAME_FORCE_CLOSE_OPEN_FILE_DEFAULT);\r\n    int map = conf.getInt(DFS_ROUTER_FEDERATION_RENAME_MAP, -1);\r\n    int bandwidth = conf.getInt(DFS_ROUTER_FEDERATION_RENAME_BANDWIDTH, -1);\r\n    long delay = conf.getLong(DFS_ROUTER_FEDERATION_RENAME_DELAY, DFS_ROUTER_FEDERATION_RENAME_DELAY_DEFAULT);\r\n    int diff = conf.getInt(DFS_ROUTER_FEDERATION_RENAME_DIFF, DFS_ROUTER_FEDERATION_RENAME_DIFF_DEFAULT);\r\n    String trashPolicy = conf.get(DFS_ROUTER_FEDERATION_RENAME_TRASH, DFS_ROUTER_FEDERATION_RENAME_TRASH_DEFAULT);\r\n    FedBalanceConfigs.TrashOption trashOpt = FedBalanceConfigs.TrashOption.valueOf(trashPolicy.toUpperCase());\r\n    FedBalanceContext context = new FedBalanceContext.Builder(srcPath, dstPath, NO_MOUNT, conf).setForceCloseOpenFiles(forceCloseOpen).setUseMountReadOnly(true).setMapNum(map).setBandwidthLimit(bandwidth).setTrash(trashOpt).setDelayDuration(delay).setDiffThreshold(diff).build();\r\n    LOG.info(context.toString());\r\n    BalanceJob.Builder<BalanceProcedure> builder = new BalanceJob.Builder<>();\r\n    DistCpProcedure dcp = new DistCpProcedure(DISTCP_PROCEDURE, null, delay, context);\r\n    builder.nextProcedure(dcp);\r\n    TrashProcedure tp = new TrashProcedure(TRASH_PROCEDURE, null, delay, context);\r\n    builder.nextProcedure(tp);\r\n    return builder.build();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "getRouterFederationRenameCount",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "int getRouterFederationRenameCount()\n{\r\n    return routerRenameCounter.get();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "countIncrement",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void countIncrement()\n{\r\n    routerRenameCounter.incrementAndGet();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "countDecrement",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void countDecrement()\n{\r\n    routerRenameCounter.decrementAndGet();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "checkConfiguration",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void checkConfiguration(Configuration conf) throws IOException\n{\r\n    int map = conf.getInt(DFS_ROUTER_FEDERATION_RENAME_MAP, -1);\r\n    int bandwidth = conf.getInt(DFS_ROUTER_FEDERATION_RENAME_BANDWIDTH, -1);\r\n    long delay = conf.getLong(DFS_ROUTER_FEDERATION_RENAME_DELAY, DFS_ROUTER_FEDERATION_RENAME_DELAY_DEFAULT);\r\n    int diff = conf.getInt(DFS_ROUTER_FEDERATION_RENAME_DIFF, DFS_ROUTER_FEDERATION_RENAME_DIFF_DEFAULT);\r\n    if (map < 0) {\r\n        throw new IOException(\"map=\" + map + \" is negative. Please check \" + DFS_ROUTER_FEDERATION_RENAME_MAP);\r\n    } else if (bandwidth < 0) {\r\n        throw new IOException(\"bandwidth=\" + bandwidth + \" is negative. Please check \" + DFS_ROUTER_FEDERATION_RENAME_BANDWIDTH);\r\n    } else if (delay < 0) {\r\n        throw new IOException(\"delay=\" + delay + \" is negative. Please check \" + DFS_ROUTER_FEDERATION_RENAME_DELAY);\r\n    } else if (diff < 0) {\r\n        throw new IOException(\"diff=\" + diff + \" is negative. Please check \" + DFS_ROUTER_FEDERATION_RENAME_DIFF);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "doGet",
  "errType" : [ "Throwable" ],
  "containingMethodsNum" : 14,
  "sourceCodeText" : "void doGet(HttpServletRequest request, HttpServletResponse response) throws IOException\n{\r\n    final ServletContext context = getServletContext();\r\n    String format = parseAcceptHeader(request);\r\n    if (FORMAT_TEXT.equals(format)) {\r\n        response.setContentType(\"text/plain; charset=UTF-8\");\r\n    } else if (FORMAT_JSON.equals(format)) {\r\n        response.setContentType(\"application/json; charset=UTF-8\");\r\n    }\r\n    Router router = RouterHttpServer.getRouterFromContext(context);\r\n    DatanodeInfo[] datanodeReport = router.getRpcServer().getDatanodeReport(HdfsConstants.DatanodeReportType.ALL);\r\n    List<Node> datanodeInfos = Arrays.asList(datanodeReport);\r\n    try (PrintStream out = new PrintStream(response.getOutputStream(), false, \"UTF-8\")) {\r\n        printTopology(out, datanodeInfos, format);\r\n    } catch (Throwable t) {\r\n        String errMsg = \"Print network topology failed. \" + StringUtils.stringifyException(t);\r\n        response.sendError(HttpServletResponse.SC_GONE, errMsg);\r\n        throw new IOException(errMsg);\r\n    } finally {\r\n        response.getOutputStream().close();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\protocol",
  "methodName" : "newInstance",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "AddMountTableEntryResponse newInstance() throws IOException\n{\r\n    return StateStoreSerializer.newRecord(AddMountTableEntryResponse.class);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\protocol",
  "methodName" : "getStatus",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean getStatus()",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\protocol",
  "methodName" : "setStatus",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setStatus(boolean result)",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\driver\\impl",
  "methodName" : "initDriver",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 6,
  "sourceCodeText" : "boolean initDriver()\n{\r\n    LOG.info(\"Initializing ZooKeeper connection\");\r\n    Configuration conf = getConf();\r\n    baseZNode = conf.get(FEDERATION_STORE_ZK_PARENT_PATH, FEDERATION_STORE_ZK_PARENT_PATH_DEFAULT);\r\n    try {\r\n        this.zkManager = new ZKCuratorManager(conf);\r\n        this.zkManager.start();\r\n        this.zkAcl = ZKCuratorManager.getZKAcls(conf);\r\n    } catch (IOException e) {\r\n        LOG.error(\"Cannot initialize the ZK connection\", e);\r\n        return false;\r\n    }\r\n    return true;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\driver\\impl",
  "methodName" : "initRecordStorage",
  "errType" : [ "Exception" ],
  "containingMethodsNum" : 3,
  "sourceCodeText" : "boolean initRecordStorage(String className, Class<T> clazz)\n{\r\n    try {\r\n        String checkPath = getNodePath(baseZNode, className);\r\n        zkManager.createRootDirRecursively(checkPath, zkAcl);\r\n        return true;\r\n    } catch (Exception e) {\r\n        LOG.error(\"Cannot initialize ZK node for {}: {}\", className, e.getMessage());\r\n        return false;\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\driver\\impl",
  "methodName" : "close",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void close() throws Exception\n{\r\n    if (zkManager != null) {\r\n        zkManager.close();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\driver\\impl",
  "methodName" : "isDriverReady",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "boolean isDriverReady()\n{\r\n    if (zkManager == null) {\r\n        return false;\r\n    }\r\n    CuratorFramework curator = zkManager.getCurator();\r\n    if (curator == null) {\r\n        return false;\r\n    }\r\n    return curator.getState() == CuratorFrameworkState.STARTED;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\driver\\impl",
  "methodName" : "get",
  "errType" : [ "Exception", "Exception", "IOException" ],
  "containingMethodsNum" : 19,
  "sourceCodeText" : "QueryResult<T> get(Class<T> clazz) throws IOException\n{\r\n    verifyDriverReady();\r\n    long start = monotonicNow();\r\n    List<T> ret = new ArrayList<>();\r\n    String znode = getZNodeForClass(clazz);\r\n    try {\r\n        List<String> children = zkManager.getChildren(znode);\r\n        for (String child : children) {\r\n            try {\r\n                String path = getNodePath(znode, child);\r\n                Stat stat = new Stat();\r\n                String data = zkManager.getStringData(path, stat);\r\n                boolean corrupted = false;\r\n                if (data == null || data.equals(\"\")) {\r\n                    corrupted = true;\r\n                } else {\r\n                    try {\r\n                        T record = createRecord(data, stat, clazz);\r\n                        ret.add(record);\r\n                    } catch (IOException e) {\r\n                        LOG.error(\"Cannot create record type \\\"{}\\\" from \\\"{}\\\": {}\", clazz.getSimpleName(), data, e.getMessage());\r\n                        corrupted = true;\r\n                    }\r\n                }\r\n                if (corrupted) {\r\n                    LOG.error(\"Cannot get data for {} at {}, cleaning corrupted data\", child, path);\r\n                    zkManager.delete(path);\r\n                }\r\n            } catch (Exception e) {\r\n                LOG.error(\"Cannot get data for {}: {}\", child, e.getMessage());\r\n            }\r\n        }\r\n    } catch (Exception e) {\r\n        getMetrics().addFailure(monotonicNow() - start);\r\n        String msg = \"Cannot get children for \\\"\" + znode + \"\\\": \" + e.getMessage();\r\n        LOG.error(msg);\r\n        throw new IOException(msg);\r\n    }\r\n    long end = monotonicNow();\r\n    getMetrics().addRead(end - start);\r\n    return new QueryResult<T>(ret, getTime());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 3,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\driver\\impl",
  "methodName" : "putAll",
  "errType" : null,
  "containingMethodsNum" : 13,
  "sourceCodeText" : "boolean putAll(List<T> records, boolean update, boolean error) throws IOException\n{\r\n    verifyDriverReady();\r\n    if (records.isEmpty()) {\r\n        return true;\r\n    }\r\n    T record0 = records.get(0);\r\n    Class<? extends BaseRecord> recordClass = record0.getClass();\r\n    String znode = getZNodeForClass(recordClass);\r\n    long start = monotonicNow();\r\n    boolean status = true;\r\n    for (T record : records) {\r\n        String primaryKey = getPrimaryKey(record);\r\n        String recordZNode = getNodePath(znode, primaryKey);\r\n        byte[] data = serialize(record);\r\n        if (!writeNode(recordZNode, data, update, error)) {\r\n            status = false;\r\n        }\r\n    }\r\n    long end = monotonicNow();\r\n    if (status) {\r\n        getMetrics().addWrite(end - start);\r\n    } else {\r\n        getMetrics().addFailure(end - start);\r\n    }\r\n    return status;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\driver\\impl",
  "methodName" : "remove",
  "errType" : [ "IOException", "Exception" ],
  "containingMethodsNum" : 17,
  "sourceCodeText" : "int remove(Class<T> clazz, Query<T> query) throws IOException\n{\r\n    verifyDriverReady();\r\n    if (query == null) {\r\n        return 0;\r\n    }\r\n    long start = monotonicNow();\r\n    List<T> records = null;\r\n    try {\r\n        QueryResult<T> result = get(clazz);\r\n        records = result.getRecords();\r\n    } catch (IOException ex) {\r\n        LOG.error(\"Cannot get existing records\", ex);\r\n        getMetrics().addFailure(monotonicNow() - start);\r\n        return 0;\r\n    }\r\n    String znode = getZNodeForClass(clazz);\r\n    List<T> recordsToRemove = filterMultiple(query, records);\r\n    int removed = 0;\r\n    for (T existingRecord : recordsToRemove) {\r\n        LOG.info(\"Removing \\\"{}\\\"\", existingRecord);\r\n        try {\r\n            String primaryKey = getPrimaryKey(existingRecord);\r\n            String path = getNodePath(znode, primaryKey);\r\n            if (zkManager.delete(path)) {\r\n                removed++;\r\n            } else {\r\n                LOG.error(\"Did not remove \\\"{}\\\"\", existingRecord);\r\n            }\r\n        } catch (Exception e) {\r\n            LOG.error(\"Cannot remove \\\"{}\\\"\", existingRecord, e);\r\n            getMetrics().addFailure(monotonicNow() - start);\r\n        }\r\n    }\r\n    long end = monotonicNow();\r\n    if (removed > 0) {\r\n        getMetrics().addRemove(end - start);\r\n    }\r\n    return removed;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 2,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\driver\\impl",
  "methodName" : "removeAll",
  "errType" : [ "Exception" ],
  "containingMethodsNum" : 11,
  "sourceCodeText" : "boolean removeAll(Class<T> clazz) throws IOException\n{\r\n    long start = monotonicNow();\r\n    boolean status = true;\r\n    String znode = getZNodeForClass(clazz);\r\n    LOG.info(\"Deleting all children under {}\", znode);\r\n    try {\r\n        List<String> children = zkManager.getChildren(znode);\r\n        for (String child : children) {\r\n            String path = getNodePath(znode, child);\r\n            LOG.info(\"Deleting {}\", path);\r\n            zkManager.delete(path);\r\n        }\r\n    } catch (Exception e) {\r\n        LOG.error(\"Cannot remove {}: {}\", znode, e.getMessage());\r\n        status = false;\r\n    }\r\n    long time = monotonicNow() - start;\r\n    if (status) {\r\n        getMetrics().addRemove(time);\r\n    } else {\r\n        getMetrics().addFailure(time);\r\n    }\r\n    return status;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\driver\\impl",
  "methodName" : "writeNode",
  "errType" : [ "Exception" ],
  "containingMethodsNum" : 4,
  "sourceCodeText" : "boolean writeNode(String znode, byte[] bytes, boolean update, boolean error)\n{\r\n    try {\r\n        boolean created = zkManager.create(znode);\r\n        if (!update && !created && error) {\r\n            LOG.info(\"Cannot write record \\\"{}\\\", it already exists\", znode);\r\n            return false;\r\n        }\r\n        zkManager.setData(znode, bytes, -1);\r\n        return true;\r\n    } catch (Exception e) {\r\n        LOG.error(\"Cannot write record \\\"{}\\\": {}\", znode, e.getMessage());\r\n    }\r\n    return false;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\driver\\impl",
  "methodName" : "getZNodeForClass",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "String getZNodeForClass(Class<T> clazz)\n{\r\n    String className = getRecordName(clazz);\r\n    return getNodePath(baseZNode, className);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\driver\\impl",
  "methodName" : "createRecord",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "T createRecord(String data, Stat stat, Class<T> clazz) throws IOException\n{\r\n    T record = newRecord(data, clazz, false);\r\n    record.setDateCreated(stat.getCtime());\r\n    record.setDateModified(stat.getMtime());\r\n    return record;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\protocol\\impl\\pb",
  "methodName" : "getProto",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "GetDestinationRequestProto getProto()\n{\r\n    return this.translator.build();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\protocol\\impl\\pb",
  "methodName" : "setProto",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setProto(Message proto)\n{\r\n    this.translator.setProto(proto);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\protocol\\impl\\pb",
  "methodName" : "readInstance",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void readInstance(String base64String) throws IOException\n{\r\n    this.translator.readInstance(base64String);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\protocol\\impl\\pb",
  "methodName" : "getSrcPath",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String getSrcPath()\n{\r\n    return this.translator.getProtoOrBuilder().getSrcPath();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\protocol\\impl\\pb",
  "methodName" : "setSrcPath",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setSrcPath(String path)\n{\r\n    this.translator.getBuilder().setSrcPath(path);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\records",
  "methodName" : "newInstance",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "DisabledNameservice newInstance() throws IOException\n{\r\n    DisabledNameservice record = StateStoreSerializer.newRecord(DisabledNameservice.class);\r\n    record.init();\r\n    return record;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\records",
  "methodName" : "newInstance",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "DisabledNameservice newInstance(String nsId) throws IOException\n{\r\n    DisabledNameservice record = newInstance();\r\n    record.setNameserviceId(nsId);\r\n    return record;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\records",
  "methodName" : "getNameserviceId",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getNameserviceId()",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\records",
  "methodName" : "setNameserviceId",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setNameserviceId(String nameServiceId)",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\records",
  "methodName" : "getPrimaryKeys",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "SortedMap<String, String> getPrimaryKeys()\n{\r\n    SortedMap<String, String> keyMap = new TreeMap<>();\r\n    keyMap.put(\"nameServiceId\", this.getNameserviceId());\r\n    return keyMap;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\records",
  "methodName" : "hasOtherFields",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean hasOtherFields()\n{\r\n    return false;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\records",
  "methodName" : "getExpirationMs",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getExpirationMs()\n{\r\n    return -1;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\records",
  "methodName" : "newInstance",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "RouterState newInstance()\n{\r\n    RouterState record = StateStoreSerializer.newRecord(RouterState.class);\r\n    record.init();\r\n    return record;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\records",
  "methodName" : "newInstance",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "RouterState newInstance(String addr, long startTime, RouterServiceState status)\n{\r\n    RouterState record = newInstance();\r\n    record.setDateStarted(startTime);\r\n    record.setAddress(addr);\r\n    record.setStatus(status);\r\n    record.setCompileInfo(FederationUtil.getCompileInfo());\r\n    record.setVersion(FederationUtil.getVersion());\r\n    return record;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\records",
  "methodName" : "setAddress",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setAddress(String address)",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\records",
  "methodName" : "setDateStarted",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setDateStarted(long dateStarted)",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\records",
  "methodName" : "getAddress",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getAddress()",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\records",
  "methodName" : "getStateStoreVersion",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "StateStoreVersion getStateStoreVersion() throws IOException",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\records",
  "methodName" : "setStateStoreVersion",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setStateStoreVersion(StateStoreVersion version)",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\records",
  "methodName" : "getStatus",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "RouterServiceState getStatus()",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\records",
  "methodName" : "setStatus",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setStatus(RouterServiceState newStatus)",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\records",
  "methodName" : "getVersion",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getVersion()",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\records",
  "methodName" : "setVersion",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setVersion(String version)",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\records",
  "methodName" : "getCompileInfo",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getCompileInfo()",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\records",
  "methodName" : "setCompileInfo",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setCompileInfo(String info)",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\records",
  "methodName" : "getDateStarted",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getDateStarted()",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\records",
  "methodName" : "setAdminAddress",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setAdminAddress(String adminAddress)",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\records",
  "methodName" : "getAdminAddress",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getAdminAddress()",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\records",
  "methodName" : "getRouterId",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String getRouterId()\n{\r\n    return getAddress();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\records",
  "methodName" : "like",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "boolean like(BaseRecord o)\n{\r\n    if (o instanceof RouterState) {\r\n        RouterState other = (RouterState) o;\r\n        if (getAddress() != null && !getAddress().equals(other.getAddress())) {\r\n            return false;\r\n        }\r\n        if (getStatus() != null && !getStatus().equals(other.getStatus())) {\r\n            return false;\r\n        }\r\n        return true;\r\n    }\r\n    return false;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\records",
  "methodName" : "toString",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "String toString()\n{\r\n    return getAddress() + \" -> \" + getStatus() + \",\" + getVersion();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\records",
  "methodName" : "getPrimaryKeys",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "SortedMap<String, String> getPrimaryKeys()\n{\r\n    SortedMap<String, String> map = new TreeMap<>();\r\n    map.put(\"address\", getAddress());\r\n    return map;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\records",
  "methodName" : "validate",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void validate()\n{\r\n    super.validate();\r\n    if ((getAddress() == null || getAddress().length() == 0) && getStatus() != RouterServiceState.INITIALIZING) {\r\n        throw new IllegalArgumentException(\"Invalid router entry, no address specified \" + this);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\records",
  "methodName" : "compareTo",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "int compareTo(BaseRecord other)\n{\r\n    if (other == null) {\r\n        return -1;\r\n    } else if (other instanceof RouterState) {\r\n        RouterState router = (RouterState) other;\r\n        return this.getAddress().compareTo(router.getAddress());\r\n    } else {\r\n        return super.compareTo(other);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\records",
  "methodName" : "checkExpired",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "boolean checkExpired(long currentTime)\n{\r\n    if (super.checkExpired(currentTime)) {\r\n        setStatus(RouterServiceState.EXPIRED);\r\n        return true;\r\n    }\r\n    return false;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\records",
  "methodName" : "getExpirationMs",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getExpirationMs()\n{\r\n    return RouterState.expirationMs;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\records",
  "methodName" : "setExpirationMs",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setExpirationMs(long time)\n{\r\n    RouterState.expirationMs = time;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\records",
  "methodName" : "isExpired",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "boolean isExpired()\n{\r\n    return getStatus() == RouterServiceState.EXPIRED;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\records",
  "methodName" : "getDeletionMs",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getDeletionMs()\n{\r\n    return RouterState.deletionMs;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\records",
  "methodName" : "setDeletionMs",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setDeletionMs(long time)\n{\r\n    RouterState.deletionMs = time;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\records",
  "methodName" : "setDateModified",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setDateModified(long time)",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\records",
  "methodName" : "getDateModified",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getDateModified()",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\records",
  "methodName" : "setDateCreated",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setDateCreated(long time)",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\records",
  "methodName" : "getDateCreated",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getDateCreated()",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\records",
  "methodName" : "getExpirationMs",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getExpirationMs()",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\records",
  "methodName" : "isExpired",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean isExpired()\n{\r\n    return false;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\records",
  "methodName" : "getDeletionMs",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getDeletionMs()\n{\r\n    return -1;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\records",
  "methodName" : "getPrimaryKeys",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Map<String, String> getPrimaryKeys()",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\records",
  "methodName" : "init",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void init()\n{\r\n    initDefaultTimes();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\records",
  "methodName" : "initDefaultTimes",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void initDefaultTimes()\n{\r\n    long now = Time.now();\r\n    this.setDateCreated(now);\r\n    this.setDateModified(now);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\records",
  "methodName" : "getPrimaryKey",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String getPrimaryKey()\n{\r\n    return generateMashupKey(getPrimaryKeys());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\records",
  "methodName" : "hasOtherFields",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean hasOtherFields()\n{\r\n    return true;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\records",
  "methodName" : "generateMashupKey",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "String generateMashupKey(final Map<String, String> keys)\n{\r\n    StringBuilder builder = new StringBuilder();\r\n    for (Object value : keys.values()) {\r\n        if (builder.length() > 0) {\r\n            builder.append(\"-\");\r\n        }\r\n        builder.append(value);\r\n    }\r\n    return builder.toString();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\records",
  "methodName" : "like",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "boolean like(BaseRecord other)\n{\r\n    if (other == null) {\r\n        return false;\r\n    }\r\n    Map<String, String> thisKeys = this.getPrimaryKeys();\r\n    Map<String, String> otherKeys = other.getPrimaryKeys();\r\n    if (thisKeys == null) {\r\n        return otherKeys == null;\r\n    }\r\n    return thisKeys.equals(otherKeys);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\records",
  "methodName" : "equals",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "boolean equals(Object obj)\n{\r\n    if (!(obj instanceof BaseRecord)) {\r\n        return false;\r\n    }\r\n    BaseRecord baseObject = (BaseRecord) obj;\r\n    Map<String, String> keyset1 = this.getPrimaryKeys();\r\n    Map<String, String> keyset2 = baseObject.getPrimaryKeys();\r\n    return keyset1.equals(keyset2);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\records",
  "methodName" : "hashCode",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "int hashCode()\n{\r\n    Map<String, String> keyset = this.getPrimaryKeys();\r\n    return keyset.hashCode();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\records",
  "methodName" : "compareTo",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "int compareTo(BaseRecord record)\n{\r\n    if (record == null) {\r\n        return -1;\r\n    }\r\n    return (int) (record.getDateModified() - this.getDateModified());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\records",
  "methodName" : "checkExpired",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "boolean checkExpired(long currentTime)\n{\r\n    long expiration = getExpirationMs();\r\n    long modifiedTime = getDateModified();\r\n    if (modifiedTime > 0 && expiration > 0) {\r\n        return (modifiedTime + expiration) < currentTime;\r\n    }\r\n    return false;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\records",
  "methodName" : "shouldBeDeleted",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "boolean shouldBeDeleted(long currentTime)\n{\r\n    long deletionTime = getDeletionMs();\r\n    if (isExpired() && deletionTime > 0) {\r\n        long elapsedTime = currentTime - (getDateModified() + getExpirationMs());\r\n        return elapsedTime > deletionTime;\r\n    } else {\r\n        return false;\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\records",
  "methodName" : "validate",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void validate()\n{\r\n    if (getDateCreated() <= 0) {\r\n        throw new IllegalArgumentException(ERROR_MSG_CREATION_TIME_NEGATIVE);\r\n    } else if (getDateModified() <= 0) {\r\n        throw new IllegalArgumentException(ERROR_MSG_MODIFICATION_TIME_NEGATIVE);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\records",
  "methodName" : "toString",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String toString()\n{\r\n    return getPrimaryKey();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store",
  "methodName" : "getRecordClass",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Class<R> getRecordClass()\n{\r\n    return this.recordClass;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store",
  "methodName" : "getDriver",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "StateStoreDriver getDriver()\n{\r\n    return this.driver;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store",
  "methodName" : "newInstance",
  "errType" : [ "Exception" ],
  "containingMethodsNum" : 3,
  "sourceCodeText" : "T newInstance(final Class<T> clazz, final StateStoreDriver driver)\n{\r\n    try {\r\n        Constructor<T> constructor = clazz.getConstructor(StateStoreDriver.class);\r\n        T recordStore = constructor.newInstance(driver);\r\n        return recordStore;\r\n    } catch (Exception e) {\r\n        LOG.error(\"Cannot create new instance for \" + clazz, e);\r\n        return null;\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\tools\\federation",
  "methodName" : "main",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void main(String[] argv) throws Exception\n{\r\n    Configuration conf = new HdfsConfiguration();\r\n    RouterAdmin admin = new RouterAdmin(conf);\r\n    int res = ToolRunner.run(admin, argv);\r\n    System.exit(res);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\tools\\federation",
  "methodName" : "printUsage",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void printUsage()\n{\r\n    String usage = getUsage(null);\r\n    System.out.println(usage);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\tools\\federation",
  "methodName" : "printUsage",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void printUsage(String cmd)\n{\r\n    String usage = getUsage(cmd);\r\n    System.out.println(usage);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\tools\\federation",
  "methodName" : "getUsage",
  "errType" : null,
  "containingMethodsNum" : 21,
  "sourceCodeText" : "String getUsage(String cmd)\n{\r\n    if (cmd == null) {\r\n        String[] commands = { \"-add\", \"-update\", \"-rm\", \"-ls\", \"-getDestination\", \"-setQuota\", \"-setStorageTypeQuota\", \"-clrQuota\", \"-clrStorageTypeQuota\", \"-safemode\", \"-nameservice\", \"-getDisabledNameservices\", \"-refresh\", \"-refreshRouterArgs\", \"-refreshSuperUserGroupsConfiguration\", \"-refreshCallQueue\" };\r\n        StringBuilder usage = new StringBuilder();\r\n        usage.append(\"Usage: hdfs dfsrouteradmin :\\n\");\r\n        for (int i = 0; i < commands.length; i++) {\r\n            usage.append(getUsage(commands[i]));\r\n            if (i + 1 < commands.length) {\r\n                usage.append(\"\\n\");\r\n            }\r\n        }\r\n        return usage.toString();\r\n    }\r\n    if (cmd.equals(\"-add\")) {\r\n        return \"\\t[-add <source> <nameservice1, nameservice2, ...> <destination> \" + \"[-readonly] [-faulttolerant] \" + \"[-order HASH|LOCAL|RANDOM|HASH_ALL|SPACE] \" + \"-owner <owner> -group <group> -mode <mode>]\";\r\n    } else if (cmd.equals(\"-update\")) {\r\n        return \"\\t[-update <source>\" + \" [<nameservice1, nameservice2, ...> <destination>] \" + \"[-readonly true|false] [-faulttolerant true|false] \" + \"[-order HASH|LOCAL|RANDOM|HASH_ALL|SPACE] \" + \"-owner <owner> -group <group> -mode <mode>]\";\r\n    } else if (cmd.equals(\"-rm\")) {\r\n        return \"\\t[-rm <source>]\";\r\n    } else if (cmd.equals(\"-ls\")) {\r\n        return \"\\t[-ls [-d] <path>]\";\r\n    } else if (cmd.equals(\"-getDestination\")) {\r\n        return \"\\t[-getDestination <path>]\";\r\n    } else if (cmd.equals(\"-setQuota\")) {\r\n        return \"\\t[-setQuota <path> -nsQuota <nsQuota> -ssQuota \" + \"<quota in bytes or quota size string>]\";\r\n    } else if (cmd.equals(\"-setStorageTypeQuota\")) {\r\n        return \"\\t[-setStorageTypeQuota <path> -storageType <storage type> \" + \"<quota in bytes or quota size string>]\";\r\n    } else if (cmd.equals(\"-clrQuota\")) {\r\n        return \"\\t[-clrQuota <path>]\";\r\n    } else if (cmd.equals(\"-clrStorageTypeQuota\")) {\r\n        return \"\\t[-clrStorageTypeQuota <path>]\";\r\n    } else if (cmd.equals(\"-safemode\")) {\r\n        return \"\\t[-safemode enter | leave | get]\";\r\n    } else if (cmd.equals(\"-nameservice\")) {\r\n        return \"\\t[-nameservice enable | disable <nameservice>]\";\r\n    } else if (cmd.equals(\"-getDisabledNameservices\")) {\r\n        return \"\\t[-getDisabledNameservices]\";\r\n    } else if (cmd.equals(\"-refresh\")) {\r\n        return \"\\t[-refresh]\";\r\n    } else if (cmd.equals(\"-refreshRouterArgs\")) {\r\n        return \"\\t[-refreshRouterArgs <host:ipc_port> <key> [arg1..argn]]\";\r\n    } else if (cmd.equals(\"-refreshSuperUserGroupsConfiguration\")) {\r\n        return \"\\t[-refreshSuperUserGroupsConfiguration]\";\r\n    } else if (cmd.equals(\"-refreshCallQueue\")) {\r\n        return \"\\t[-refreshCallQueue]\";\r\n    }\r\n    return getUsage(null);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\tools\\federation",
  "methodName" : "validateMax",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void validateMax(String[] arg)\n{\r\n    if (arg[0].equals(\"-ls\")) {\r\n        if (arg.length > 3) {\r\n            throw new IllegalArgumentException(\"Too many arguments, Max=2 argument allowed\");\r\n        }\r\n    } else if (arg[0].equals(\"-getDestination\")) {\r\n        if (arg.length > 2) {\r\n            throw new IllegalArgumentException(\"Too many arguments, Max=1 argument allowed only\");\r\n        }\r\n    } else if (arg[0].equals(\"-safemode\")) {\r\n        if (arg.length > 2) {\r\n            throw new IllegalArgumentException(\"Too many arguments, Max=1 argument allowed only\");\r\n        }\r\n    } else if (arg[0].equals(\"-nameservice\")) {\r\n        if (arg.length > 3) {\r\n            throw new IllegalArgumentException(\"Too many arguments, Max=2 arguments allowed\");\r\n        }\r\n    } else if (arg[0].equals(\"-getDisabledNameservices\")) {\r\n        if (arg.length > 1) {\r\n            throw new IllegalArgumentException(\"No arguments allowed\");\r\n        }\r\n    } else if (arg[0].equals(\"-refreshSuperUserGroupsConfiguration\")) {\r\n        if (arg.length > 1) {\r\n            throw new IllegalArgumentException(\"No arguments allowed\");\r\n        }\r\n    } else if (arg[0].equals(\"-refreshCallQueue\")) {\r\n        if (arg.length > 1) {\r\n            throw new IllegalArgumentException(\"No arguments allowed\");\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\tools\\federation",
  "methodName" : "validateMin",
  "errType" : null,
  "containingMethodsNum" : 11,
  "sourceCodeText" : "boolean validateMin(String[] argv)\n{\r\n    String cmd = argv[0];\r\n    if (\"-add\".equals(cmd)) {\r\n        if (argv.length < 4) {\r\n            return false;\r\n        }\r\n    } else if (\"-update\".equals(cmd)) {\r\n        if (argv.length < 4) {\r\n            return false;\r\n        }\r\n    } else if (\"-rm\".equals(cmd)) {\r\n        if (argv.length < 2) {\r\n            return false;\r\n        }\r\n    } else if (\"-getDestination\".equals(cmd)) {\r\n        if (argv.length < 2) {\r\n            return false;\r\n        }\r\n    } else if (\"-setQuota\".equals(cmd)) {\r\n        if (argv.length < 4) {\r\n            return false;\r\n        }\r\n    } else if (\"-setStorageTypeQuota\".equals(cmd)) {\r\n        if (argv.length < 5) {\r\n            return false;\r\n        }\r\n    } else if (\"-clrQuota\".equals(cmd)) {\r\n        if (argv.length < 2) {\r\n            return false;\r\n        }\r\n    } else if (\"-clrStorageTypeQuota\".equals(cmd)) {\r\n        if (argv.length < 2) {\r\n            return false;\r\n        }\r\n    } else if (\"-safemode\".equals(cmd)) {\r\n        if (argv.length < 2) {\r\n            return false;\r\n        }\r\n    } else if (\"-nameservice\".equals(cmd)) {\r\n        if (argv.length < 3) {\r\n            return false;\r\n        }\r\n    } else if (\"-refreshRouterArgs\".equals(cmd)) {\r\n        if (argv.length < 2) {\r\n            return false;\r\n        }\r\n    }\r\n    return true;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\tools\\federation",
  "methodName" : "run",
  "errType" : [ "RPC.VersionMismatch", "IOException", "IllegalArgumentException", "RemoteException", "IOException", "Exception", "Exception", "IOException" ],
  "containingMethodsNum" : 64,
  "sourceCodeText" : "int run(String[] argv) throws Exception\n{\r\n    if (argv.length < 1) {\r\n        System.err.println(\"Not enough parameters specified\");\r\n        printUsage();\r\n        return -1;\r\n    }\r\n    int exitCode = -1;\r\n    int i = 0;\r\n    String cmd = argv[i++];\r\n    if (!validateMin(argv)) {\r\n        System.err.println(\"Not enough parameters specificed for cmd \" + cmd);\r\n        printUsage(cmd);\r\n        return exitCode;\r\n    }\r\n    String address = null;\r\n    try {\r\n        address = getConf().getTrimmed(RBFConfigKeys.DFS_ROUTER_ADMIN_ADDRESS_KEY, RBFConfigKeys.DFS_ROUTER_ADMIN_ADDRESS_DEFAULT);\r\n        InetSocketAddress routerSocket = NetUtils.createSocketAddr(address);\r\n        client = new RouterClient(routerSocket, getConf());\r\n    } catch (RPC.VersionMismatch v) {\r\n        System.err.println(\"Version mismatch between client and server... command aborted\");\r\n        return exitCode;\r\n    } catch (IOException e) {\r\n        System.err.println(\"Bad connection to Router... command aborted\");\r\n        return exitCode;\r\n    }\r\n    Exception debugException = null;\r\n    exitCode = 0;\r\n    try {\r\n        validateMax(argv);\r\n        if (\"-add\".equals(cmd)) {\r\n            if (addMount(argv, i)) {\r\n                System.out.println(\"Successfully added mount point \" + argv[i]);\r\n            } else {\r\n                exitCode = -1;\r\n            }\r\n        } else if (\"-update\".equals(cmd)) {\r\n            if (updateMount(argv, i)) {\r\n                System.out.println(\"Successfully updated mount point \" + argv[i]);\r\n                System.out.println(\"WARN: Changing order/destinations may lead to inconsistencies\");\r\n            } else {\r\n                exitCode = -1;\r\n            }\r\n        } else if (\"-rm\".equals(cmd)) {\r\n            while (i < argv.length) {\r\n                try {\r\n                    if (removeMount(argv[i])) {\r\n                        System.out.println(\"Successfully removed mount point \" + argv[i]);\r\n                    }\r\n                } catch (IOException e) {\r\n                    exitCode = -1;\r\n                    System.err.println(cmd.substring(1) + \": \" + e.getLocalizedMessage());\r\n                }\r\n                i++;\r\n            }\r\n        } else if (\"-ls\".equals(cmd)) {\r\n            listMounts(argv, i);\r\n        } else if (\"-getDestination\".equals(cmd)) {\r\n            getDestination(argv[i]);\r\n        } else if (\"-setQuota\".equals(cmd)) {\r\n            if (setQuota(argv, i)) {\r\n                System.out.println(\"Successfully set quota for mount point \" + argv[i]);\r\n            }\r\n        } else if (\"-setStorageTypeQuota\".equals(cmd)) {\r\n            if (setStorageTypeQuota(argv, i)) {\r\n                System.out.println(\"Successfully set storage type quota for mount point \" + argv[i]);\r\n            }\r\n        } else if (\"-clrQuota\".equals(cmd)) {\r\n            while (i < argv.length) {\r\n                if (clrQuota(argv[i])) {\r\n                    System.out.println(\"Successfully clear quota for mount point \" + argv[i]);\r\n                    i++;\r\n                }\r\n            }\r\n        } else if (\"-clrStorageTypeQuota\".equals(cmd)) {\r\n            while (i < argv.length) {\r\n                if (clrStorageTypeQuota(argv[i])) {\r\n                    System.out.println(\"Successfully clear storage type quota for mount\" + \" point \" + argv[i]);\r\n                    i++;\r\n                }\r\n            }\r\n        } else if (\"-safemode\".equals(cmd)) {\r\n            manageSafeMode(argv[i]);\r\n        } else if (\"-nameservice\".equals(cmd)) {\r\n            String subcmd = argv[i];\r\n            String nsId = argv[i + 1];\r\n            manageNameservice(subcmd, nsId);\r\n        } else if (\"-getDisabledNameservices\".equals(cmd)) {\r\n            getDisabledNameservices();\r\n        } else if (\"-refresh\".equals(cmd)) {\r\n            refresh(address);\r\n        } else if (\"-refreshRouterArgs\".equals(cmd)) {\r\n            exitCode = genericRefresh(argv, i);\r\n        } else if (\"-refreshSuperUserGroupsConfiguration\".equals(cmd)) {\r\n            exitCode = refreshSuperUserGroupsConfiguration();\r\n        } else if (\"-refreshCallQueue\".equals(cmd)) {\r\n            exitCode = refreshCallQueue();\r\n        } else {\r\n            throw new IllegalArgumentException(\"Unknown Command: \" + cmd);\r\n        }\r\n    } catch (IllegalArgumentException arge) {\r\n        debugException = arge;\r\n        exitCode = -1;\r\n        System.err.println(cmd.substring(1) + \": \" + arge.getLocalizedMessage());\r\n        printUsage(cmd);\r\n    } catch (RemoteException e) {\r\n        exitCode = -1;\r\n        debugException = e;\r\n        try {\r\n            String[] content;\r\n            content = e.getLocalizedMessage().split(\"\\n\");\r\n            System.err.println(cmd.substring(1) + \": \" + content[0]);\r\n            e.printStackTrace();\r\n        } catch (Exception ex) {\r\n            System.err.println(cmd.substring(1) + \": \" + ex.getLocalizedMessage());\r\n            e.printStackTrace();\r\n            debugException = ex;\r\n        }\r\n    } catch (IOException ioe) {\r\n        exitCode = -1;\r\n        System.err.println(cmd.substring(1) + \": \" + ioe.getLocalizedMessage());\r\n        printUsage(cmd);\r\n    } catch (Exception e) {\r\n        exitCode = -1;\r\n        debugException = e;\r\n        System.err.println(cmd.substring(1) + \": \" + e.getLocalizedMessage());\r\n        e.printStackTrace();\r\n    }\r\n    if (debugException != null) {\r\n        LOG.debug(\"Exception encountered\", debugException);\r\n    }\r\n    return exitCode;\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 4,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\tools\\federation",
  "methodName" : "refreshSuperUserGroupsConfiguration",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "int refreshSuperUserGroupsConfiguration() throws IOException\n{\r\n    RouterGenericManager proxy = client.getRouterGenericManager();\r\n    String address = getConf().getTrimmed(RBFConfigKeys.DFS_ROUTER_ADMIN_ADDRESS_KEY, RBFConfigKeys.DFS_ROUTER_ADMIN_ADDRESS_DEFAULT);\r\n    if (proxy.refreshSuperUserGroupsConfiguration()) {\r\n        System.out.println(\"Successfully updated superuser proxy groups on router \" + address);\r\n        return 0;\r\n    }\r\n    return -1;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\tools\\federation",
  "methodName" : "refresh",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void refresh(String address) throws IOException\n{\r\n    if (refreshRouterCache()) {\r\n        System.out.println(\"Successfully updated mount table cache on router \" + address);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\tools\\federation",
  "methodName" : "refreshRouterCache",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "boolean refreshRouterCache() throws IOException\n{\r\n    RefreshMountTableEntriesResponse response = client.getMountTableManager().refreshMountTableEntries(RefreshMountTableEntriesRequest.newInstance());\r\n    return response.getResult();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\tools\\federation",
  "methodName" : "addMount",
  "errType" : [ "Exception" ],
  "containingMethodsNum" : 12,
  "sourceCodeText" : "boolean addMount(String[] parameters, int i) throws IOException\n{\r\n    String mount = parameters[i++];\r\n    String[] nss = parameters[i++].split(\",\");\r\n    String dest = parameters[i++];\r\n    boolean readOnly = false;\r\n    boolean faultTolerant = false;\r\n    String owner = null;\r\n    String group = null;\r\n    FsPermission mode = null;\r\n    DestinationOrder order = DestinationOrder.HASH;\r\n    while (i < parameters.length) {\r\n        if (parameters[i].equals(\"-readonly\")) {\r\n            readOnly = true;\r\n        } else if (parameters[i].equals(\"-faulttolerant\")) {\r\n            faultTolerant = true;\r\n        } else if (parameters[i].equals(\"-order\")) {\r\n            i++;\r\n            try {\r\n                order = DestinationOrder.valueOf(parameters[i]);\r\n            } catch (Exception e) {\r\n                System.err.println(\"Cannot parse order: \" + parameters[i]);\r\n            }\r\n        } else if (parameters[i].equals(\"-owner\")) {\r\n            i++;\r\n            owner = parameters[i];\r\n        } else if (parameters[i].equals(\"-group\")) {\r\n            i++;\r\n            group = parameters[i];\r\n        } else if (parameters[i].equals(\"-mode\")) {\r\n            i++;\r\n            short modeValue = Short.parseShort(parameters[i], 8);\r\n            mode = new FsPermission(modeValue);\r\n        } else {\r\n            printUsage(\"-add\");\r\n            return false;\r\n        }\r\n        i++;\r\n    }\r\n    return addMount(mount, nss, dest, readOnly, faultTolerant, order, new ACLEntity(owner, group, mode));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\tools\\federation",
  "methodName" : "addMount",
  "errType" : null,
  "containingMethodsNum" : 35,
  "sourceCodeText" : "boolean addMount(String mount, String[] nss, String dest, boolean readonly, boolean faultTolerant, DestinationOrder order, ACLEntity aclInfo) throws IOException\n{\r\n    mount = normalizeFileSystemPath(mount);\r\n    MountTableManager mountTable = client.getMountTableManager();\r\n    MountTable existingEntry = getMountEntry(mount, mountTable);\r\n    if (existingEntry == null) {\r\n        Map<String, String> destMap = new LinkedHashMap<>();\r\n        for (String ns : nss) {\r\n            destMap.put(ns, dest);\r\n        }\r\n        MountTable newEntry = MountTable.newInstance(mount, destMap);\r\n        if (readonly) {\r\n            newEntry.setReadOnly(true);\r\n        }\r\n        if (faultTolerant) {\r\n            newEntry.setFaultTolerant(true);\r\n        }\r\n        if (order != null) {\r\n            newEntry.setDestOrder(order);\r\n        }\r\n        if (aclInfo.getOwner() != null) {\r\n            newEntry.setOwnerName(aclInfo.getOwner());\r\n        }\r\n        if (aclInfo.getGroup() != null) {\r\n            newEntry.setGroupName(aclInfo.getGroup());\r\n        }\r\n        if (aclInfo.getMode() != null) {\r\n            newEntry.setMode(aclInfo.getMode());\r\n        }\r\n        newEntry.validate();\r\n        AddMountTableEntryRequest request = AddMountTableEntryRequest.newInstance(newEntry);\r\n        AddMountTableEntryResponse addResponse = mountTable.addMountTableEntry(request);\r\n        boolean added = addResponse.getStatus();\r\n        if (!added) {\r\n            System.err.println(\"Cannot add mount point \" + mount);\r\n        }\r\n        return added;\r\n    } else {\r\n        for (String nsId : nss) {\r\n            if (!existingEntry.addDestination(nsId, dest)) {\r\n                System.err.println(\"Cannot add destination at \" + nsId + \" \" + dest);\r\n                return false;\r\n            }\r\n        }\r\n        if (readonly) {\r\n            existingEntry.setReadOnly(true);\r\n        }\r\n        if (faultTolerant) {\r\n            existingEntry.setFaultTolerant(true);\r\n        }\r\n        if (order != null) {\r\n            existingEntry.setDestOrder(order);\r\n        }\r\n        if (aclInfo.getOwner() != null) {\r\n            existingEntry.setOwnerName(aclInfo.getOwner());\r\n        }\r\n        if (aclInfo.getGroup() != null) {\r\n            existingEntry.setGroupName(aclInfo.getGroup());\r\n        }\r\n        if (aclInfo.getMode() != null) {\r\n            existingEntry.setMode(aclInfo.getMode());\r\n        }\r\n        existingEntry.validate();\r\n        UpdateMountTableEntryRequest updateRequest = UpdateMountTableEntryRequest.newInstance(existingEntry);\r\n        UpdateMountTableEntryResponse updateResponse = mountTable.updateMountTableEntry(updateRequest);\r\n        boolean updated = updateResponse.getStatus();\r\n        if (!updated) {\r\n            System.err.println(\"Cannot update mount point \" + mount);\r\n        }\r\n        return updated;\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\tools\\federation",
  "methodName" : "updateMount",
  "errType" : [ "IllegalArgumentException", "Exception", "Exception" ],
  "containingMethodsNum" : 24,
  "sourceCodeText" : "boolean updateMount(String[] parameters, int i) throws IOException\n{\r\n    String mount = parameters[i++];\r\n    mount = normalizeFileSystemPath(mount);\r\n    MountTableManager mountTable = client.getMountTableManager();\r\n    MountTable existingEntry = getMountEntry(mount, mountTable);\r\n    if (existingEntry == null) {\r\n        throw new IOException(mount + \" doesn't exist.\");\r\n    }\r\n    if (!parameters[i].startsWith(\"-\")) {\r\n        String[] nss = parameters[i++].split(\",\");\r\n        String dest = parameters[i++];\r\n        Map<String, String> destMap = new LinkedHashMap<>();\r\n        for (String ns : nss) {\r\n            destMap.put(ns, dest);\r\n        }\r\n        final List<RemoteLocation> locations = new LinkedList<>();\r\n        for (Entry<String, String> entry : destMap.entrySet()) {\r\n            String nsId = entry.getKey();\r\n            String path = normalizeFileSystemPath(entry.getValue());\r\n            RemoteLocation location = new RemoteLocation(nsId, path, mount);\r\n            locations.add(location);\r\n        }\r\n        existingEntry.setDestinations(locations);\r\n    }\r\n    try {\r\n        while (i < parameters.length) {\r\n            switch(parameters[i]) {\r\n                case \"-readonly\":\r\n                    i++;\r\n                    existingEntry.setReadOnly(getBooleanValue(parameters[i]));\r\n                    break;\r\n                case \"-faulttolerant\":\r\n                    i++;\r\n                    existingEntry.setFaultTolerant(getBooleanValue(parameters[i]));\r\n                    break;\r\n                case \"-order\":\r\n                    i++;\r\n                    try {\r\n                        existingEntry.setDestOrder(DestinationOrder.valueOf(parameters[i]));\r\n                        break;\r\n                    } catch (Exception e) {\r\n                        throw new Exception(\"Cannot parse order: \" + parameters[i]);\r\n                    }\r\n                case \"-owner\":\r\n                    i++;\r\n                    existingEntry.setOwnerName(parameters[i]);\r\n                    break;\r\n                case \"-group\":\r\n                    i++;\r\n                    existingEntry.setGroupName(parameters[i]);\r\n                    break;\r\n                case \"-mode\":\r\n                    i++;\r\n                    short modeValue = Short.parseShort(parameters[i], 8);\r\n                    existingEntry.setMode(new FsPermission(modeValue));\r\n                    break;\r\n                default:\r\n                    printUsage(\"-update\");\r\n                    return false;\r\n            }\r\n            i++;\r\n        }\r\n    } catch (IllegalArgumentException iae) {\r\n        throw iae;\r\n    } catch (Exception e) {\r\n        String msg = \"Unable to parse arguments: \" + e.getMessage();\r\n        if (e instanceof ArrayIndexOutOfBoundsException) {\r\n            msg = \"Unable to parse arguments: no value provided for \" + parameters[i - 1];\r\n        }\r\n        throw new IOException(msg);\r\n    }\r\n    UpdateMountTableEntryRequest updateRequest = UpdateMountTableEntryRequest.newInstance(existingEntry);\r\n    UpdateMountTableEntryResponse updateResponse = mountTable.updateMountTableEntry(updateRequest);\r\n    boolean updated = updateResponse.getStatus();\r\n    if (!updated) {\r\n        System.err.println(\"Cannot update mount point \" + mount);\r\n    }\r\n    return updated;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 2,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\tools\\federation",
  "methodName" : "getBooleanValue",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "boolean getBooleanValue(String value) throws Exception\n{\r\n    if (value.equalsIgnoreCase(\"true\")) {\r\n        return true;\r\n    } else if (value.equalsIgnoreCase(\"false\")) {\r\n        return false;\r\n    }\r\n    throw new IllegalArgumentException(\"Invalid argument: \" + value + \". Please specify either true or false.\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\tools\\federation",
  "methodName" : "getMountEntry",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "MountTable getMountEntry(String mount, MountTableManager mountTable) throws IOException\n{\r\n    GetMountTableEntriesRequest getRequest = GetMountTableEntriesRequest.newInstance(mount);\r\n    GetMountTableEntriesResponse getResponse = mountTable.getMountTableEntries(getRequest);\r\n    List<MountTable> results = getResponse.getEntries();\r\n    MountTable existingEntry = null;\r\n    for (MountTable result : results) {\r\n        if (mount.equals(result.getSourcePath())) {\r\n            existingEntry = result;\r\n        }\r\n    }\r\n    return existingEntry;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\tools\\federation",
  "methodName" : "removeMount",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "boolean removeMount(String path) throws IOException\n{\r\n    path = normalizeFileSystemPath(path);\r\n    MountTableManager mountTable = client.getMountTableManager();\r\n    RemoveMountTableEntryRequest request = RemoveMountTableEntryRequest.newInstance(path);\r\n    RemoveMountTableEntryResponse response = mountTable.removeMountTableEntry(request);\r\n    boolean removed = response.getStatus();\r\n    if (!removed) {\r\n        System.out.println(\"Cannot remove mount point \" + path);\r\n    }\r\n    return removed;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\tools\\federation",
  "methodName" : "listMounts",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void listMounts(String[] argv, int i) throws IOException\n{\r\n    String path;\r\n    boolean detail = false;\r\n    if (argv.length == 1) {\r\n        path = \"/\";\r\n    } else if (argv[i].equals(\"-d\")) {\r\n        detail = true;\r\n        if (argv.length == 2) {\r\n            path = \"/\";\r\n        } else {\r\n            path = argv[++i];\r\n        }\r\n    } else {\r\n        path = argv[i];\r\n    }\r\n    path = normalizeFileSystemPath(path);\r\n    MountTableManager mountTable = client.getMountTableManager();\r\n    GetMountTableEntriesRequest request = GetMountTableEntriesRequest.newInstance(path);\r\n    GetMountTableEntriesResponse response = mountTable.getMountTableEntries(request);\r\n    List<MountTable> entries = response.getEntries();\r\n    printMounts(entries, detail);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\tools\\federation",
  "methodName" : "printMounts",
  "errType" : null,
  "containingMethodsNum" : 14,
  "sourceCodeText" : "void printMounts(List<MountTable> entries, boolean detail)\n{\r\n    System.out.println(\"Mount Table Entries:\");\r\n    if (detail) {\r\n        System.out.println(String.format(\"%-25s %-25s %-25s %-25s %-10s %-30s %-10s %-10s %-15s\", \"Source\", \"Destinations\", \"Owner\", \"Group\", \"Mode\", \"Quota/Usage\", \"Order\", \"ReadOnly\", \"FaultTolerant\"));\r\n    } else {\r\n        System.out.println(String.format(\"%-25s %-25s %-25s %-25s %-10s %-30s\", \"Source\", \"Destinations\", \"Owner\", \"Group\", \"Mode\", \"Quota/Usage\"));\r\n    }\r\n    for (MountTable entry : entries) {\r\n        StringBuilder destBuilder = new StringBuilder();\r\n        for (RemoteLocation location : entry.getDestinations()) {\r\n            if (destBuilder.length() > 0) {\r\n                destBuilder.append(\",\");\r\n            }\r\n            destBuilder.append(String.format(\"%s->%s\", location.getNameserviceId(), location.getDest()));\r\n        }\r\n        System.out.print(String.format(\"%-25s %-25s\", entry.getSourcePath(), destBuilder.toString()));\r\n        System.out.print(String.format(\" %-25s %-25s %-10s\", entry.getOwnerName(), entry.getGroupName(), entry.getMode()));\r\n        System.out.print(String.format(\" %-30s\", entry.getQuota()));\r\n        if (detail) {\r\n            System.out.print(String.format(\" %-10s\", entry.getDestOrder()));\r\n            System.out.print(String.format(\" %-10s\", entry.isReadOnly() ? \"Read-Only\" : \"\"));\r\n            System.out.print(String.format(\" %-15s\", entry.isFaultTolerant() ? \"Fault-Tolerant\" : \"\"));\r\n        }\r\n        System.out.println();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\tools\\federation",
  "methodName" : "getDestination",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void getDestination(String path) throws IOException\n{\r\n    path = normalizeFileSystemPath(path);\r\n    MountTableManager mountTable = client.getMountTableManager();\r\n    GetDestinationRequest request = GetDestinationRequest.newInstance(path);\r\n    GetDestinationResponse response = mountTable.getDestination(request);\r\n    System.out.println(\"Destination: \" + StringUtils.join(\",\", response.getDestinations()));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\tools\\federation",
  "methodName" : "setQuota",
  "errType" : [ "Exception", "Exception" ],
  "containingMethodsNum" : 5,
  "sourceCodeText" : "boolean setQuota(String[] parameters, int i) throws IOException\n{\r\n    long nsQuota = HdfsConstants.QUOTA_DONT_SET;\r\n    long ssQuota = HdfsConstants.QUOTA_DONT_SET;\r\n    String mount = parameters[i++];\r\n    while (i < parameters.length) {\r\n        if (parameters[i].equals(\"-nsQuota\")) {\r\n            i++;\r\n            try {\r\n                nsQuota = Long.parseLong(parameters[i]);\r\n            } catch (Exception e) {\r\n                throw new IllegalArgumentException(\"Cannot parse nsQuota: \" + parameters[i]);\r\n            }\r\n        } else if (parameters[i].equals(\"-ssQuota\")) {\r\n            i++;\r\n            try {\r\n                ssQuota = StringUtils.TraditionalBinaryPrefix.string2long(parameters[i]);\r\n            } catch (Exception e) {\r\n                throw new IllegalArgumentException(\"Cannot parse ssQuota: \" + parameters[i]);\r\n            }\r\n        } else {\r\n            throw new IllegalArgumentException(\"Invalid argument : \" + parameters[i]);\r\n        }\r\n        i++;\r\n    }\r\n    if (nsQuota <= 0 || ssQuota <= 0) {\r\n        throw new IllegalArgumentException(\"Input quota value should be a positive number.\");\r\n    }\r\n    if (nsQuota == HdfsConstants.QUOTA_DONT_SET && ssQuota == HdfsConstants.QUOTA_DONT_SET) {\r\n        throw new IllegalArgumentException(\"Must specify at least one of -nsQuota and -ssQuota.\");\r\n    }\r\n    return updateQuota(mount, nsQuota, ssQuota);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 2,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\tools\\federation",
  "methodName" : "setStorageTypeQuota",
  "errType" : null,
  "containingMethodsNum" : 9,
  "sourceCodeText" : "boolean setStorageTypeQuota(String[] parameters, int i) throws IOException\n{\r\n    long[] typeQuota = new long[StorageType.values().length];\r\n    eachByStorageType(t -> typeQuota[t.ordinal()] = HdfsConstants.QUOTA_DONT_SET);\r\n    String mount = parameters[i++];\r\n    if (parameters[i].equals(\"-storageType\")) {\r\n        i++;\r\n        StorageType type = StorageType.parseStorageType(parameters[i++]);\r\n        typeQuota[type.ordinal()] = Long.parseLong(parameters[i]);\r\n    } else {\r\n        throw new IllegalArgumentException(\"Invalid argument : \" + parameters[i]);\r\n    }\r\n    if (orByStorageType(t -> typeQuota[t.ordinal()] <= 0)) {\r\n        throw new IllegalArgumentException(\"Input quota value should be a positive number.\");\r\n    }\r\n    if (andByStorageType(t -> typeQuota[t.ordinal()] == HdfsConstants.QUOTA_DONT_SET)) {\r\n        throw new IllegalArgumentException(\"Must specify at least one of -nsQuota and -ssQuota.\");\r\n    }\r\n    return updateStorageTypeQuota(mount, typeQuota);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\tools\\federation",
  "methodName" : "clrQuota",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "boolean clrQuota(String mount) throws IOException\n{\r\n    return updateQuota(mount, HdfsConstants.QUOTA_RESET, HdfsConstants.QUOTA_RESET);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\tools\\federation",
  "methodName" : "clrStorageTypeQuota",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "boolean clrStorageTypeQuota(String mount) throws IOException\n{\r\n    long[] typeQuota = new long[StorageType.values().length];\r\n    eachByStorageType(t -> typeQuota[t.ordinal()] = HdfsConstants.QUOTA_RESET);\r\n    return updateStorageTypeQuota(mount, typeQuota);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\tools\\federation",
  "methodName" : "updateQuota",
  "errType" : null,
  "containingMethodsNum" : 14,
  "sourceCodeText" : "boolean updateQuota(String mount, long nsQuota, long ssQuota) throws IOException\n{\r\n    MountTableManager mountTable = client.getMountTableManager();\r\n    GetMountTableEntriesRequest getRequest = GetMountTableEntriesRequest.newInstance(mount);\r\n    GetMountTableEntriesResponse getResponse = mountTable.getMountTableEntries(getRequest);\r\n    List<MountTable> results = getResponse.getEntries();\r\n    MountTable existingEntry = null;\r\n    for (MountTable result : results) {\r\n        if (mount.equals(result.getSourcePath())) {\r\n            existingEntry = result;\r\n            break;\r\n        }\r\n    }\r\n    if (existingEntry == null) {\r\n        throw new IOException(mount + \" doesn't exist in mount table.\");\r\n    } else {\r\n        long nsCount = existingEntry.getQuota().getFileAndDirectoryCount();\r\n        long ssCount = existingEntry.getQuota().getSpaceConsumed();\r\n        if (nsQuota == HdfsConstants.QUOTA_RESET && ssQuota == HdfsConstants.QUOTA_RESET) {\r\n            nsCount = RouterQuotaUsage.QUOTA_USAGE_COUNT_DEFAULT;\r\n            ssCount = RouterQuotaUsage.QUOTA_USAGE_COUNT_DEFAULT;\r\n        } else {\r\n            if (nsQuota == HdfsConstants.QUOTA_DONT_SET) {\r\n                nsQuota = existingEntry.getQuota().getQuota();\r\n            }\r\n            if (ssQuota == HdfsConstants.QUOTA_DONT_SET) {\r\n                ssQuota = existingEntry.getQuota().getSpaceQuota();\r\n            }\r\n        }\r\n        RouterQuotaUsage updatedQuota = new RouterQuotaUsage.Builder().fileAndDirectoryCount(nsCount).quota(nsQuota).spaceConsumed(ssCount).spaceQuota(ssQuota).build();\r\n        existingEntry.setQuota(updatedQuota);\r\n    }\r\n    UpdateMountTableEntryRequest updateRequest = UpdateMountTableEntryRequest.newInstance(existingEntry);\r\n    UpdateMountTableEntryResponse updateResponse = mountTable.updateMountTableEntry(updateRequest);\r\n    return updateResponse.getStatus();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\tools\\federation",
  "methodName" : "updateStorageTypeQuota",
  "errType" : null,
  "containingMethodsNum" : 16,
  "sourceCodeText" : "boolean updateStorageTypeQuota(String mount, long[] typeQuota) throws IOException\n{\r\n    MountTableManager mountTable = client.getMountTableManager();\r\n    GetMountTableEntriesRequest getRequest = GetMountTableEntriesRequest.newInstance(mount);\r\n    GetMountTableEntriesResponse getResponse = mountTable.getMountTableEntries(getRequest);\r\n    List<MountTable> results = getResponse.getEntries();\r\n    MountTable existingEntry = null;\r\n    for (MountTable result : results) {\r\n        if (mount.equals(result.getSourcePath())) {\r\n            existingEntry = result;\r\n            break;\r\n        }\r\n    }\r\n    if (existingEntry == null) {\r\n        throw new IOException(mount + \" doesn't exist in mount table.\");\r\n    } else {\r\n        final RouterQuotaUsage quotaUsage = existingEntry.getQuota();\r\n        long[] typeCount = new long[StorageType.values().length];\r\n        eachByStorageType(t -> typeCount[t.ordinal()] = quotaUsage.getTypeQuota(t));\r\n        if (andByStorageType(t -> typeQuota[t.ordinal()] == HdfsConstants.QUOTA_RESET)) {\r\n            eachByStorageType(t -> typeCount[t.ordinal()] = RouterQuotaUsage.QUOTA_USAGE_COUNT_DEFAULT);\r\n        } else {\r\n            eachByStorageType(t -> {\r\n                if (typeQuota[t.ordinal()] == HdfsConstants.QUOTA_DONT_SET) {\r\n                    typeQuota[t.ordinal()] = quotaUsage.getTypeQuota(t);\r\n                }\r\n            });\r\n        }\r\n        RouterQuotaUsage updatedQuota = new RouterQuotaUsage.Builder().typeQuota(typeQuota).typeConsumed(typeCount).build();\r\n        existingEntry.setQuota(updatedQuota);\r\n    }\r\n    UpdateMountTableEntryRequest updateRequest = UpdateMountTableEntryRequest.newInstance(existingEntry);\r\n    UpdateMountTableEntryResponse updateResponse = mountTable.updateMountTableEntry(updateRequest);\r\n    return updateResponse.getStatus();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\tools\\federation",
  "methodName" : "manageSafeMode",
  "errType" : null,
  "containingMethodsNum" : 9,
  "sourceCodeText" : "void manageSafeMode(String cmd) throws IOException\n{\r\n    if (cmd.equals(\"enter\")) {\r\n        if (enterSafeMode()) {\r\n            System.out.println(\"Successfully enter safe mode.\");\r\n        }\r\n    } else if (cmd.equals(\"leave\")) {\r\n        if (leaveSafeMode()) {\r\n            System.out.println(\"Successfully leave safe mode.\");\r\n        }\r\n    } else if (cmd.equals(\"get\")) {\r\n        boolean result = getSafeMode();\r\n        System.out.println(\"Safe Mode: \" + result);\r\n    } else {\r\n        throw new IllegalArgumentException(\"Invalid argument: \" + cmd);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\tools\\federation",
  "methodName" : "enterSafeMode",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "boolean enterSafeMode() throws IOException\n{\r\n    RouterStateManager stateManager = client.getRouterStateManager();\r\n    EnterSafeModeResponse response = stateManager.enterSafeMode(EnterSafeModeRequest.newInstance());\r\n    return response.getStatus();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\tools\\federation",
  "methodName" : "leaveSafeMode",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "boolean leaveSafeMode() throws IOException\n{\r\n    RouterStateManager stateManager = client.getRouterStateManager();\r\n    LeaveSafeModeResponse response = stateManager.leaveSafeMode(LeaveSafeModeRequest.newInstance());\r\n    return response.getStatus();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\tools\\federation",
  "methodName" : "getSafeMode",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "boolean getSafeMode() throws IOException\n{\r\n    RouterStateManager stateManager = client.getRouterStateManager();\r\n    GetSafeModeResponse response = stateManager.getSafeMode(GetSafeModeRequest.newInstance());\r\n    return response.isInSafeMode();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\tools\\federation",
  "methodName" : "manageNameservice",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void manageNameservice(String cmd, String nsId) throws IOException\n{\r\n    if (cmd.equals(\"enable\")) {\r\n        if (enableNameservice(nsId)) {\r\n            System.out.println(\"Successfully enabled nameservice \" + nsId);\r\n        } else {\r\n            System.err.println(\"Cannot enable \" + nsId);\r\n        }\r\n    } else if (cmd.equals(\"disable\")) {\r\n        if (disableNameservice(nsId)) {\r\n            System.out.println(\"Successfully disabled nameservice \" + nsId);\r\n        } else {\r\n            System.err.println(\"Cannot disable \" + nsId);\r\n        }\r\n    } else {\r\n        throw new IllegalArgumentException(\"Unknown command: \" + cmd);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\tools\\federation",
  "methodName" : "disableNameservice",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "boolean disableNameservice(String nsId) throws IOException\n{\r\n    NameserviceManager nameserviceManager = client.getNameserviceManager();\r\n    DisableNameserviceResponse response = nameserviceManager.disableNameservice(DisableNameserviceRequest.newInstance(nsId));\r\n    return response.getStatus();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\tools\\federation",
  "methodName" : "enableNameservice",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "boolean enableNameservice(String nsId) throws IOException\n{\r\n    NameserviceManager nameserviceManager = client.getNameserviceManager();\r\n    EnableNameserviceResponse response = nameserviceManager.enableNameservice(EnableNameserviceRequest.newInstance(nsId));\r\n    return response.getStatus();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\tools\\federation",
  "methodName" : "getDisabledNameservices",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void getDisabledNameservices() throws IOException\n{\r\n    NameserviceManager nameserviceManager = client.getNameserviceManager();\r\n    GetDisabledNameservicesRequest request = GetDisabledNameservicesRequest.newInstance();\r\n    GetDisabledNameservicesResponse response = nameserviceManager.getDisabledNameservices(request);\r\n    System.out.println(\"List of disabled nameservices:\");\r\n    for (String nsId : response.getNameservices()) {\r\n        System.out.println(nsId);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\tools\\federation",
  "methodName" : "genericRefresh",
  "errType" : null,
  "containingMethodsNum" : 14,
  "sourceCodeText" : "int genericRefresh(String[] argv, int i) throws IOException\n{\r\n    String hostport = argv[i++];\r\n    String identifier = argv[i++];\r\n    String[] args = Arrays.copyOfRange(argv, i, argv.length);\r\n    Configuration conf = getConf();\r\n    conf.set(CommonConfigurationKeys.HADOOP_SECURITY_SERVICE_USER_NAME_KEY, conf.get(DFSConfigKeys.DFS_NAMENODE_KERBEROS_PRINCIPAL_KEY, \"\"));\r\n    Class<?> xface = GenericRefreshProtocolPB.class;\r\n    InetSocketAddress address = NetUtils.createSocketAddr(hostport);\r\n    UserGroupInformation ugi = UserGroupInformation.getCurrentUser();\r\n    RPC.setProtocolEngine(conf, xface, ProtobufRpcEngine2.class);\r\n    GenericRefreshProtocolPB proxy = (GenericRefreshProtocolPB) RPC.getProxy(xface, RPC.getProtocolVersion(xface), address, ugi, conf, NetUtils.getDefaultSocketFactory(conf), 0);\r\n    Collection<RefreshResponse> responses = null;\r\n    try (GenericRefreshProtocolClientSideTranslatorPB xlator = new GenericRefreshProtocolClientSideTranslatorPB(proxy)) {\r\n        responses = xlator.refresh(identifier, args);\r\n        int returnCode = 0;\r\n        System.out.println(\"Refresh Responses:\\n\");\r\n        for (RefreshResponse response : responses) {\r\n            System.out.println(response.toString());\r\n            if (returnCode == 0 && response.getReturnCode() != 0) {\r\n                returnCode = response.getReturnCode();\r\n            } else if (returnCode != 0 && response.getReturnCode() != 0) {\r\n                returnCode = -1;\r\n            }\r\n        }\r\n        return returnCode;\r\n    } finally {\r\n        if (responses == null) {\r\n            System.out.println(\"Failed to get response.\\n\");\r\n            return -1;\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\tools\\federation",
  "methodName" : "refreshCallQueue",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 9,
  "sourceCodeText" : "int refreshCallQueue() throws IOException\n{\r\n    Configuration conf = getConf();\r\n    String hostport = getConf().getTrimmed(RBFConfigKeys.DFS_ROUTER_ADMIN_ADDRESS_KEY, RBFConfigKeys.DFS_ROUTER_ADMIN_ADDRESS_DEFAULT);\r\n    Class<?> xface = RefreshCallQueueProtocolPB.class;\r\n    InetSocketAddress address = NetUtils.createSocketAddr(hostport);\r\n    UserGroupInformation ugi = UserGroupInformation.getCurrentUser();\r\n    RPC.setProtocolEngine(conf, xface, ProtobufRpcEngine2.class);\r\n    RefreshCallQueueProtocolPB proxy = (RefreshCallQueueProtocolPB) RPC.getProxy(xface, RPC.getProtocolVersion(xface), address, ugi, conf, NetUtils.getDefaultSocketFactory(conf), 0);\r\n    int returnCode = -1;\r\n    try (RefreshCallQueueProtocolClientSideTranslatorPB xlator = new RefreshCallQueueProtocolClientSideTranslatorPB(proxy)) {\r\n        xlator.refreshCallQueue();\r\n        System.out.println(\"Refresh call queue successfully for \" + hostport);\r\n        returnCode = 0;\r\n    } catch (IOException ioe) {\r\n        System.out.println(\"Refresh call queue unsuccessfully for \" + hostport);\r\n    }\r\n    return returnCode;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\tools\\federation",
  "methodName" : "normalizeFileSystemPath",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "String normalizeFileSystemPath(final String str)\n{\r\n    String path = SLASHES.matcher(str).replaceAll(\"/\");\r\n    if (path.length() > 1 && path.endsWith(\"/\")) {\r\n        path = path.substring(0, path.length() - 1);\r\n    }\r\n    return path;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\metrics",
  "methodName" : "close",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void close()\n{\r\n    if (this.routerBeanName != null) {\r\n        MBeans.unregister(routerBeanName);\r\n    }\r\n    if (this.federationBeanName != null) {\r\n        MBeans.unregister(federationBeanName);\r\n    }\r\n    MetricsSystem ms = DefaultMetricsSystem.instance();\r\n    ms.unregisterSource(RBFMetrics.class.getName());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\metrics",
  "methodName" : "getNamenodes",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 18,
  "sourceCodeText" : "String getNamenodes()\n{\r\n    final Map<String, Map<String, Object>> info = new LinkedHashMap<>();\r\n    if (membershipStore == null) {\r\n        return \"{}\";\r\n    }\r\n    try {\r\n        GetNamenodeRegistrationsRequest request = GetNamenodeRegistrationsRequest.newInstance();\r\n        GetNamenodeRegistrationsResponse response = membershipStore.getNamenodeRegistrations(request);\r\n        final List<MembershipState> namenodes = response.getNamenodeMemberships();\r\n        if (namenodes == null || namenodes.size() == 0) {\r\n            return JSON.toString(info);\r\n        }\r\n        List<MembershipState> namenodesOrder = new ArrayList<>(namenodes);\r\n        Collections.sort(namenodesOrder, MembershipState.NAME_COMPARATOR);\r\n        for (MembershipState namenode : namenodesOrder) {\r\n            Map<String, Object> innerInfo = new HashMap<>();\r\n            Map<String, Object> map = getJson(namenode);\r\n            innerInfo.putAll(map);\r\n            long dateModified = namenode.getDateModified();\r\n            long lastHeartbeat = getSecondsSince(dateModified);\r\n            innerInfo.put(\"lastHeartbeat\", lastHeartbeat);\r\n            MembershipStats stats = namenode.getStats();\r\n            long used = stats.getTotalSpace() - stats.getAvailableSpace();\r\n            innerInfo.put(\"used\", used);\r\n            info.put(namenode.getNamenodeKey(), Collections.unmodifiableMap(innerInfo));\r\n        }\r\n    } catch (IOException e) {\r\n        LOG.error(\"Enable to fetch json representation of namenodes {}\", e.getMessage());\r\n        return \"{}\";\r\n    }\r\n    return JSON.toString(info);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\metrics",
  "methodName" : "getNameservices",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 14,
  "sourceCodeText" : "String getNameservices()\n{\r\n    final Map<String, Map<String, Object>> info = new LinkedHashMap<>();\r\n    try {\r\n        final List<MembershipState> namenodes = getActiveNamenodeRegistrations();\r\n        List<MembershipState> namenodesOrder = new ArrayList<>(namenodes);\r\n        Collections.sort(namenodesOrder, MembershipState.NAME_COMPARATOR);\r\n        for (MembershipState namenode : namenodesOrder) {\r\n            Map<String, Object> innerInfo = new HashMap<>();\r\n            Map<String, Object> map = getJson(namenode);\r\n            innerInfo.putAll(map);\r\n            long dateModified = namenode.getDateModified();\r\n            long lastHeartbeat = getSecondsSince(dateModified);\r\n            innerInfo.put(\"lastHeartbeat\", lastHeartbeat);\r\n            MembershipStats stats = namenode.getStats();\r\n            long used = stats.getTotalSpace() - stats.getAvailableSpace();\r\n            innerInfo.put(\"used\", used);\r\n            info.put(namenode.getNamenodeKey(), Collections.unmodifiableMap(innerInfo));\r\n        }\r\n    } catch (IOException e) {\r\n        LOG.error(\"Cannot retrieve nameservices for JMX: {}\", e.getMessage());\r\n        return \"{}\";\r\n    }\r\n    return JSON.toString(info);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\metrics",
  "methodName" : "getMountTable",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 21,
  "sourceCodeText" : "String getMountTable()\n{\r\n    final List<Map<String, Object>> info = new LinkedList<>();\r\n    if (mountTableStore == null) {\r\n        return \"[]\";\r\n    }\r\n    try {\r\n        GetMountTableEntriesRequest request = GetMountTableEntriesRequest.newInstance(\"/\");\r\n        GetMountTableEntriesResponse response = mountTableStore.getMountTableEntries(request);\r\n        final List<MountTable> mounts = response.getEntries();\r\n        List<MountTable> orderedMounts = new ArrayList<>(mounts);\r\n        Collections.sort(orderedMounts, MountTable.SOURCE_COMPARATOR);\r\n        for (MountTable entry : orderedMounts) {\r\n            Set<String> nameservices = new LinkedHashSet<>();\r\n            Set<String> paths = new LinkedHashSet<>();\r\n            for (RemoteLocation location : entry.getDestinations()) {\r\n                nameservices.add(location.getNameserviceId());\r\n                paths.add(location.getDest());\r\n            }\r\n            Map<String, Object> map = getJson(entry);\r\n            map.put(\"dateCreated\", getDateString(entry.getDateCreated()));\r\n            map.put(\"dateModified\", getDateString(entry.getDateModified()));\r\n            Map<String, Object> innerInfo = new HashMap<>();\r\n            innerInfo.putAll(map);\r\n            innerInfo.put(\"nameserviceId\", StringUtils.join(\",\", nameservices));\r\n            innerInfo.put(\"path\", StringUtils.join(\",\", paths));\r\n            if (nameservices.size() > 1) {\r\n                innerInfo.put(\"order\", entry.getDestOrder().toString());\r\n            } else {\r\n                innerInfo.put(\"order\", \"\");\r\n            }\r\n            innerInfo.put(\"readonly\", entry.isReadOnly());\r\n            innerInfo.put(\"faulttolerant\", entry.isFaultTolerant());\r\n            info.add(Collections.unmodifiableMap(innerInfo));\r\n        }\r\n    } catch (IOException e) {\r\n        LOG.error(\"Cannot generate JSON of mount table from store: {}\", e.getMessage());\r\n        return \"[]\";\r\n    }\r\n    return JSON.toString(info);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\metrics",
  "methodName" : "getRouters",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 15,
  "sourceCodeText" : "String getRouters()\n{\r\n    final Map<String, Map<String, Object>> info = new LinkedHashMap<>();\r\n    if (routerStore == null) {\r\n        return \"{}\";\r\n    }\r\n    try {\r\n        GetRouterRegistrationsRequest request = GetRouterRegistrationsRequest.newInstance();\r\n        GetRouterRegistrationsResponse response = routerStore.getRouterRegistrations(request);\r\n        final List<RouterState> routers = response.getRouters();\r\n        List<RouterState> routersOrder = new ArrayList<>(routers);\r\n        Collections.sort(routersOrder);\r\n        for (RouterState record : routersOrder) {\r\n            Map<String, Object> innerInfo = new HashMap<>();\r\n            Map<String, Object> map = getJson(record);\r\n            innerInfo.putAll(map);\r\n            long dateModified = record.getDateModified();\r\n            long lastHeartbeat = getSecondsSince(dateModified);\r\n            innerInfo.put(\"lastHeartbeat\", lastHeartbeat);\r\n            StateStoreVersion stateStoreVersion = record.getStateStoreVersion();\r\n            if (stateStoreVersion == null) {\r\n                LOG.error(\"Cannot get State Store versions\");\r\n            } else {\r\n                setStateStoreVersions(innerInfo, stateStoreVersion);\r\n            }\r\n            info.put(record.getPrimaryKey(), Collections.unmodifiableMap(innerInfo));\r\n        }\r\n    } catch (IOException e) {\r\n        LOG.error(\"Cannot get Routers JSON from the State Store\", e);\r\n        return \"{}\";\r\n    }\r\n    return JSON.toString(info);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\metrics",
  "methodName" : "setStateStoreVersions",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void setStateStoreVersions(Map<String, Object> map, StateStoreVersion version)\n{\r\n    long membershipVersion = version.getMembershipVersion();\r\n    String lastMembershipUpdate = getDateString(membershipVersion);\r\n    map.put(\"lastMembershipUpdate\", lastMembershipUpdate);\r\n    long mountTableVersion = version.getMountTableVersion();\r\n    String lastMountTableDate = getDateString(mountTableVersion);\r\n    map.put(\"lastMountTableUpdate\", lastMountTableDate);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\metrics",
  "methodName" : "getTotalCapacity",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "long getTotalCapacity()\n{\r\n    return getNameserviceAggregatedLong(MembershipStats::getTotalSpace);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\metrics",
  "methodName" : "getRemainingCapacity",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "long getRemainingCapacity()\n{\r\n    return getNameserviceAggregatedLong(MembershipStats::getAvailableSpace);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\metrics",
  "methodName" : "getUsedCapacity",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "long getUsedCapacity()\n{\r\n    return getTotalCapacity() - getRemainingCapacity();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\metrics",
  "methodName" : "getTotalCapacityBigInt",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "BigInteger getTotalCapacityBigInt()\n{\r\n    return getNameserviceAggregatedBigInt(MembershipStats::getTotalSpace);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\metrics",
  "methodName" : "getRemainingCapacityBigInt",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "BigInteger getRemainingCapacityBigInt()\n{\r\n    return getNameserviceAggregatedBigInt(MembershipStats::getAvailableSpace);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\metrics",
  "methodName" : "getProvidedSpace",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "long getProvidedSpace()\n{\r\n    return getNameserviceAggregatedLong(MembershipStats::getProvidedSpace);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\metrics",
  "methodName" : "getUsedCapacityBigInt",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "BigInteger getUsedCapacityBigInt()\n{\r\n    return getTotalCapacityBigInt().subtract(getRemainingCapacityBigInt());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\metrics",
  "methodName" : "getNumNameservices",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 3,
  "sourceCodeText" : "int getNumNameservices()\n{\r\n    try {\r\n        Set<FederationNamespaceInfo> nss = namenodeResolver.getNamespaces();\r\n        return nss.size();\r\n    } catch (IOException e) {\r\n        LOG.error(\"Cannot fetch number of expired registrations from the store: {}\", e.getMessage());\r\n        return 0;\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\metrics",
  "methodName" : "getNumNamenodes",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 5,
  "sourceCodeText" : "int getNumNamenodes()\n{\r\n    if (membershipStore == null) {\r\n        return 0;\r\n    }\r\n    try {\r\n        GetNamenodeRegistrationsRequest request = GetNamenodeRegistrationsRequest.newInstance();\r\n        GetNamenodeRegistrationsResponse response = membershipStore.getNamenodeRegistrations(request);\r\n        List<MembershipState> memberships = response.getNamenodeMemberships();\r\n        return memberships.size();\r\n    } catch (IOException e) {\r\n        LOG.error(\"Cannot retrieve numNamenodes for JMX: {}\", e.getMessage());\r\n        return 0;\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\metrics",
  "methodName" : "getNumExpiredNamenodes",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 5,
  "sourceCodeText" : "int getNumExpiredNamenodes()\n{\r\n    if (membershipStore == null) {\r\n        return 0;\r\n    }\r\n    try {\r\n        GetNamenodeRegistrationsRequest request = GetNamenodeRegistrationsRequest.newInstance();\r\n        GetNamenodeRegistrationsResponse response = membershipStore.getExpiredNamenodeRegistrations(request);\r\n        List<MembershipState> expiredMemberships = response.getNamenodeMemberships();\r\n        return expiredMemberships.size();\r\n    } catch (IOException e) {\r\n        LOG.error(\"Cannot retrieve numExpiredNamenodes for JMX: {}\", e.getMessage());\r\n        return 0;\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\metrics",
  "methodName" : "getNumLiveNodes",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "int getNumLiveNodes()\n{\r\n    return getNameserviceAggregatedInt(MembershipStats::getNumOfActiveDatanodes);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\metrics",
  "methodName" : "getNumDeadNodes",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "int getNumDeadNodes()\n{\r\n    return getNameserviceAggregatedInt(MembershipStats::getNumOfDeadDatanodes);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\metrics",
  "methodName" : "getNumStaleNodes",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "int getNumStaleNodes()\n{\r\n    return getNameserviceAggregatedInt(MembershipStats::getNumOfStaleDatanodes);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\metrics",
  "methodName" : "getNumDecommissioningNodes",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "int getNumDecommissioningNodes()\n{\r\n    return getNameserviceAggregatedInt(MembershipStats::getNumOfDecommissioningDatanodes);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\metrics",
  "methodName" : "getNumDecomLiveNodes",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "int getNumDecomLiveNodes()\n{\r\n    return getNameserviceAggregatedInt(MembershipStats::getNumOfDecomActiveDatanodes);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\metrics",
  "methodName" : "getNumDecomDeadNodes",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "int getNumDecomDeadNodes()\n{\r\n    return getNameserviceAggregatedInt(MembershipStats::getNumOfDecomDeadDatanodes);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\metrics",
  "methodName" : "getNumInMaintenanceLiveDataNodes",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "int getNumInMaintenanceLiveDataNodes()\n{\r\n    return getNameserviceAggregatedInt(MembershipStats::getNumOfInMaintenanceLiveDataNodes);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\metrics",
  "methodName" : "getNumInMaintenanceDeadDataNodes",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "int getNumInMaintenanceDeadDataNodes()\n{\r\n    return getNameserviceAggregatedInt(MembershipStats::getNumOfInMaintenanceDeadDataNodes);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\metrics",
  "methodName" : "getNumEnteringMaintenanceDataNodes",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "int getNumEnteringMaintenanceDataNodes()\n{\r\n    return getNameserviceAggregatedInt(MembershipStats::getNumOfEnteringMaintenanceDataNodes);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\metrics",
  "methodName" : "getNodeUsage",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 13,
  "sourceCodeText" : "String getNodeUsage()\n{\r\n    float median = 0;\r\n    float max = 0;\r\n    float min = 0;\r\n    float dev = 0;\r\n    final Map<String, Map<String, Object>> info = new HashMap<>();\r\n    try {\r\n        RouterRpcServer rpcServer = this.router.getRpcServer();\r\n        DatanodeInfo[] live = rpcServer.getDatanodeReport(DatanodeReportType.LIVE, false, timeOut);\r\n        if (live.length > 0) {\r\n            float totalDfsUsed = 0;\r\n            float[] usages = new float[live.length];\r\n            int i = 0;\r\n            for (DatanodeInfo dn : live) {\r\n                usages[i++] = dn.getDfsUsedPercent();\r\n                totalDfsUsed += dn.getDfsUsedPercent();\r\n            }\r\n            totalDfsUsed /= live.length;\r\n            Arrays.sort(usages);\r\n            median = usages[usages.length / 2];\r\n            max = usages[usages.length - 1];\r\n            min = usages[0];\r\n            for (i = 0; i < usages.length; i++) {\r\n                dev += (usages[i] - totalDfsUsed) * (usages[i] - totalDfsUsed);\r\n            }\r\n            dev = (float) Math.sqrt(dev / usages.length);\r\n        }\r\n    } catch (IOException e) {\r\n        LOG.error(\"Cannot get the live nodes: {}\", e.getMessage());\r\n    }\r\n    final Map<String, Object> innerInfo = new HashMap<>();\r\n    innerInfo.put(\"min\", StringUtils.format(\"%.2f%%\", min));\r\n    innerInfo.put(\"median\", StringUtils.format(\"%.2f%%\", median));\r\n    innerInfo.put(\"max\", StringUtils.format(\"%.2f%%\", max));\r\n    innerInfo.put(\"stdDev\", StringUtils.format(\"%.2f%%\", dev));\r\n    info.put(\"nodeUsage\", innerInfo);\r\n    return JSON.toString(info);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\metrics",
  "methodName" : "getNumBlocks",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "long getNumBlocks()\n{\r\n    return getNameserviceAggregatedLong(MembershipStats::getNumOfBlocks);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\metrics",
  "methodName" : "getNumOfMissingBlocks",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "long getNumOfMissingBlocks()\n{\r\n    return getNameserviceAggregatedLong(MembershipStats::getNumOfBlocksMissing);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\metrics",
  "methodName" : "getNumOfBlocksPendingReplication",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "long getNumOfBlocksPendingReplication()\n{\r\n    return getNameserviceAggregatedLong(MembershipStats::getNumOfBlocksPendingReplication);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\metrics",
  "methodName" : "getNumOfBlocksUnderReplicated",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "long getNumOfBlocksUnderReplicated()\n{\r\n    return getNameserviceAggregatedLong(MembershipStats::getNumOfBlocksUnderReplicated);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\metrics",
  "methodName" : "getNumOfBlocksPendingDeletion",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "long getNumOfBlocksPendingDeletion()\n{\r\n    return getNameserviceAggregatedLong(MembershipStats::getNumOfBlocksPendingDeletion);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\metrics",
  "methodName" : "getNumFiles",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "long getNumFiles()\n{\r\n    return getNameserviceAggregatedLong(MembershipStats::getNumOfFiles);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\metrics",
  "methodName" : "getRouterStarted",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "String getRouterStarted()\n{\r\n    long startTime = this.router.getStartTime();\r\n    return new Date(startTime).toString();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\metrics",
  "methodName" : "getVersion",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "String getVersion()\n{\r\n    return VersionInfo.getVersion() + \", r\" + VersionInfo.getRevision();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\metrics",
  "methodName" : "getCompiledDate",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String getCompiledDate()\n{\r\n    return VersionInfo.getDate();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\metrics",
  "methodName" : "getCompileInfo",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "String getCompileInfo()\n{\r\n    return VersionInfo.getDate() + \" by \" + VersionInfo.getUser() + \" from \" + VersionInfo.getBranch();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\metrics",
  "methodName" : "getHostAndPort",
  "errType" : [ "UnknownHostException" ],
  "containingMethodsNum" : 3,
  "sourceCodeText" : "String getHostAndPort()\n{\r\n    InetSocketAddress address = this.router.getRpcServerAddress();\r\n    if (address != null) {\r\n        try {\r\n            String hostname = InetAddress.getLocalHost().getHostName();\r\n            int port = address.getPort();\r\n            return hostname + \":\" + port;\r\n        } catch (UnknownHostException ignored) {\r\n        }\r\n    }\r\n    return \"Unknown\";\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\metrics",
  "methodName" : "getRouterId",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String getRouterId()\n{\r\n    return this.router.getRouterId();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\metrics",
  "methodName" : "getClusterId",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 3,
  "sourceCodeText" : "String getClusterId()\n{\r\n    try {\r\n        Collection<String> clusterIds = getNamespaceInfo(FederationNamespaceInfo::getClusterId);\r\n        return clusterIds.toString();\r\n    } catch (IOException e) {\r\n        LOG.error(\"Cannot fetch cluster ID metrics: {}\", e.getMessage());\r\n        return \"\";\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\metrics",
  "methodName" : "getBlockPoolId",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 3,
  "sourceCodeText" : "String getBlockPoolId()\n{\r\n    try {\r\n        Collection<String> blockpoolIds = getNamespaceInfo(FederationNamespaceInfo::getBlockPoolId);\r\n        return blockpoolIds.toString();\r\n    } catch (IOException e) {\r\n        LOG.error(\"Cannot fetch block pool ID metrics: {}\", e.getMessage());\r\n        return \"\";\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\metrics",
  "methodName" : "getRouterStatus",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String getRouterStatus()\n{\r\n    return this.router.getRouterState().toString();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\metrics",
  "methodName" : "getCurrentTokensCount",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "long getCurrentTokensCount()\n{\r\n    RouterSecurityManager mgr = this.router.getRpcServer().getRouterSecurityManager();\r\n    if (mgr != null && mgr.getSecretManager() != null) {\r\n        return mgr.getSecretManager().getCurrentTokensSize();\r\n    }\r\n    return -1;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\metrics",
  "methodName" : "getTopTokenRealOwners",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "String getTopTokenRealOwners()\n{\r\n    RouterSecurityManager mgr = this.router.getRpcServer().getRouterSecurityManager();\r\n    if (mgr != null && mgr.getSecretManager() != null) {\r\n        return JSON.toString(mgr.getSecretManager().getTopTokenRealOwners(this.topTokenRealOwners));\r\n    }\r\n    return \"\";\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\metrics",
  "methodName" : "isSecurityEnabled",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "boolean isSecurityEnabled()\n{\r\n    return UserGroupInformation.isSecurityEnabled();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\metrics",
  "methodName" : "getCorruptFilesCount",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "int getCorruptFilesCount()\n{\r\n    return getNameserviceAggregatedInt(MembershipStats::getCorruptFilesCount);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\metrics",
  "methodName" : "getScheduledReplicationBlocks",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "long getScheduledReplicationBlocks()\n{\r\n    return getNameserviceAggregatedLong(MembershipStats::getScheduledReplicationBlocks);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\metrics",
  "methodName" : "getNumberOfMissingBlocksWithReplicationFactorOne",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "long getNumberOfMissingBlocksWithReplicationFactorOne()\n{\r\n    return getNameserviceAggregatedLong(MembershipStats::getNumberOfMissingBlocksWithReplicationFactorOne);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\metrics",
  "methodName" : "getHighestPriorityLowRedundancyReplicatedBlocks",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "long getHighestPriorityLowRedundancyReplicatedBlocks()\n{\r\n    return getNameserviceAggregatedLong(MembershipStats::getHighestPriorityLowRedundancyReplicatedBlocks);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\metrics",
  "methodName" : "getHighestPriorityLowRedundancyECBlocks",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "long getHighestPriorityLowRedundancyECBlocks()\n{\r\n    return getNameserviceAggregatedLong(MembershipStats::getHighestPriorityLowRedundancyECBlocks);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\metrics",
  "methodName" : "getPendingSPSPaths",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "int getPendingSPSPaths()\n{\r\n    return getNameserviceAggregatedInt(MembershipStats::getPendingSPSPaths);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\metrics",
  "methodName" : "getRouterFederationRenameCount",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "int getRouterFederationRenameCount()\n{\r\n    return this.router.getRpcServer().getRouterFederationRenameCount();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\metrics",
  "methodName" : "getSchedulerJobCount",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "int getSchedulerJobCount()\n{\r\n    return this.router.getRpcServer().getSchedulerJobCount();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\metrics",
  "methodName" : "getSafemode",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "String getSafemode()\n{\r\n    if (this.router.isRouterState(RouterServiceState.SAFEMODE)) {\r\n        return \"Safe mode is ON. \" + this.getSafeModeTip();\r\n    } else {\r\n        return \"\";\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\metrics",
  "methodName" : "getSafeModeTip",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "String getSafeModeTip()\n{\r\n    String cmd = \"Use \\\"hdfs dfsrouteradmin -safemode leave\\\" \" + \"to turn safe mode off.\";\r\n    if (this.router.isRouterState(RouterServiceState.INITIALIZING) || this.router.isRouterState(RouterServiceState.UNINITIALIZED)) {\r\n        return \"Router is in\" + this.router.getRouterState() + \"mode, the router will immediately return to \" + \"normal mode after some time. \" + cmd;\r\n    } else if (this.router.isRouterState(RouterServiceState.SAFEMODE)) {\r\n        return \"It was turned on manually. \" + cmd;\r\n    }\r\n    return \"\";\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\metrics",
  "methodName" : "getNamespaceInfo",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "Collection<String> getNamespaceInfo(Function<FederationNamespaceInfo, String> f) throws IOException\n{\r\n    if (membershipStore == null) {\r\n        return new HashSet<>();\r\n    }\r\n    GetNamespaceInfoRequest request = GetNamespaceInfoRequest.newInstance();\r\n    GetNamespaceInfoResponse response = membershipStore.getNamespaceInfo(request);\r\n    return response.getNamespaceInfo().stream().map(f).collect(Collectors.toSet());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\metrics",
  "methodName" : "getNameserviceAggregatedInt",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 2,
  "sourceCodeText" : "int getNameserviceAggregatedInt(ToIntFunction<MembershipStats> f)\n{\r\n    try {\r\n        return getActiveNamenodeRegistrations().stream().map(MembershipState::getStats).collect(Collectors.summingInt(f));\r\n    } catch (IOException e) {\r\n        LOG.error(\"Unable to extract metrics: {}\", e.getMessage());\r\n        return 0;\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\metrics",
  "methodName" : "getNameserviceAggregatedLong",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 2,
  "sourceCodeText" : "long getNameserviceAggregatedLong(ToLongFunction<MembershipStats> f)\n{\r\n    try {\r\n        return getActiveNamenodeRegistrations().stream().map(MembershipState::getStats).collect(Collectors.summingLong(f));\r\n    } catch (IOException e) {\r\n        LOG.error(\"Unable to extract metrics: {}\", e.getMessage());\r\n        return 0;\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\metrics",
  "methodName" : "getNameserviceAggregatedBigInt",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 5,
  "sourceCodeText" : "BigInteger getNameserviceAggregatedBigInt(ToLongFunction<MembershipStats> f)\n{\r\n    try {\r\n        List<MembershipState> states = getActiveNamenodeRegistrations();\r\n        BigInteger sum = BigInteger.valueOf(0);\r\n        for (MembershipState state : states) {\r\n            long lvalue = f.applyAsLong(state.getStats());\r\n            sum = sum.add(BigInteger.valueOf(lvalue));\r\n        }\r\n        return sum;\r\n    } catch (IOException e) {\r\n        LOG.error(\"Unable to extract metrics: {}\", e.getMessage());\r\n        return new BigInteger(\"0\");\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\metrics",
  "methodName" : "getActiveNamenodeRegistrations",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "List<MembershipState> getActiveNamenodeRegistrations() throws IOException\n{\r\n    List<MembershipState> resultList = new ArrayList<>();\r\n    if (membershipStore == null) {\r\n        return resultList;\r\n    }\r\n    GetNamespaceInfoRequest request = GetNamespaceInfoRequest.newInstance();\r\n    GetNamespaceInfoResponse response = membershipStore.getNamespaceInfo(request);\r\n    for (FederationNamespaceInfo nsInfo : response.getNamespaceInfo()) {\r\n        String nsId = nsInfo.getNameserviceId();\r\n        List<? extends FederationNamenodeContext> nns = namenodeResolver.getNamenodesForNameserviceId(nsId);\r\n        if (nns != null) {\r\n            FederationNamenodeContext nn = nns.get(0);\r\n            if (nn instanceof MembershipState) {\r\n                resultList.add((MembershipState) nn);\r\n            }\r\n        }\r\n    }\r\n    return resultList;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\metrics",
  "methodName" : "getDateString",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String getDateString(long time)\n{\r\n    if (time <= 0) {\r\n        return \"-\";\r\n    }\r\n    Date date = new Date(time);\r\n    SimpleDateFormat sdf = new SimpleDateFormat(DATE_FORMAT);\r\n    return sdf.format(date);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\metrics",
  "methodName" : "getSecondsSince",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "long getSecondsSince(long timeMs)\n{\r\n    if (timeMs < 0) {\r\n        return -1;\r\n    }\r\n    return (now() - timeMs) / 1000;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\metrics",
  "methodName" : "getJson",
  "errType" : [ "Exception" ],
  "containingMethodsNum" : 6,
  "sourceCodeText" : "Map<String, Object> getJson(BaseRecord record)\n{\r\n    Map<String, Object> json = new HashMap<>();\r\n    Map<String, Class<?>> fields = getFields(record);\r\n    for (String fieldName : fields.keySet()) {\r\n        if (!fieldName.equalsIgnoreCase(\"proto\")) {\r\n            try {\r\n                Object value = getField(record, fieldName);\r\n                if (value instanceof BaseRecord) {\r\n                    BaseRecord recordField = (BaseRecord) value;\r\n                    json.putAll(getJson(recordField));\r\n                } else {\r\n                    json.put(fieldName, value == null ? JSONObject.NULL : value);\r\n                }\r\n            } catch (Exception e) {\r\n                throw new IllegalArgumentException(\"Cannot serialize field \" + fieldName + \" into JSON\");\r\n            }\r\n        }\r\n    }\r\n    return json;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\metrics",
  "methodName" : "getFields",
  "errType" : [ "Exception" ],
  "containingMethodsNum" : 7,
  "sourceCodeText" : "Map<String, Class<?>> getFields(BaseRecord record)\n{\r\n    Map<String, Class<?>> getters = new HashMap<>();\r\n    for (Method m : record.getClass().getDeclaredMethods()) {\r\n        if (m.getName().startsWith(\"get\")) {\r\n            try {\r\n                Class<?> type = m.getReturnType();\r\n                char[] c = m.getName().substring(3).toCharArray();\r\n                c[0] = Character.toLowerCase(c[0]);\r\n                String key = new String(c);\r\n                getters.put(key, type);\r\n            } catch (Exception e) {\r\n                LOG.error(\"Cannot execute getter {} on {}\", m.getName(), record);\r\n            }\r\n        }\r\n    }\r\n    return getters;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\metrics",
  "methodName" : "getField",
  "errType" : [ "Exception" ],
  "containingMethodsNum" : 3,
  "sourceCodeText" : "Object getField(BaseRecord record, String fieldName)\n{\r\n    Object result = null;\r\n    Method m = locateGetter(record, fieldName);\r\n    if (m != null) {\r\n        try {\r\n            result = m.invoke(record);\r\n        } catch (Exception e) {\r\n            LOG.error(\"Cannot get field {} on {}\", fieldName, record);\r\n        }\r\n    }\r\n    return result;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\metrics",
  "methodName" : "locateGetter",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "Method locateGetter(BaseRecord record, String fieldName)\n{\r\n    for (Method m : record.getClass().getMethods()) {\r\n        if (m.getName().equalsIgnoreCase(\"get\" + fieldName)) {\r\n            return m;\r\n        }\r\n    }\r\n    return null;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\protocol",
  "methodName" : "newInstance",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "GetSafeModeResponse newInstance() throws IOException\n{\r\n    return StateStoreSerializer.newRecord(GetSafeModeResponse.class);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\protocol",
  "methodName" : "newInstance",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "GetSafeModeResponse newInstance(boolean isInSafeMode) throws IOException\n{\r\n    GetSafeModeResponse response = newInstance();\r\n    response.setSafeMode(isInSafeMode);\r\n    return response;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\protocol",
  "methodName" : "isInSafeMode",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean isInSafeMode()",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\protocol",
  "methodName" : "setSafeMode",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setSafeMode(boolean isInSafeMode)",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\protocol",
  "methodName" : "newInstance",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "GetNamenodeRegistrationsRequest newInstance() throws IOException\n{\r\n    return StateStoreSerializer.newRecord(GetNamenodeRegistrationsRequest.class);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\protocol",
  "methodName" : "newInstance",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "GetNamenodeRegistrationsRequest newInstance(MembershipState member) throws IOException\n{\r\n    GetNamenodeRegistrationsRequest request = newInstance();\r\n    request.setPartialMembership(member);\r\n    return request;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\protocol",
  "methodName" : "getPartialMembership",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "MembershipState getPartialMembership()",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\protocol",
  "methodName" : "setPartialMembership",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setPartialMembership(MembershipState member)",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "getJmx",
  "errType" : [ "IOException", "JSONException", "Exception", "IOException" ],
  "containingMethodsNum" : 17,
  "sourceCodeText" : "JSONArray getJmx(String beanQuery, String webAddress, URLConnectionFactory connectionFactory, String scheme)\n{\r\n    JSONArray ret = null;\r\n    BufferedReader reader = null;\r\n    try {\r\n        String host = webAddress;\r\n        int port = -1;\r\n        if (webAddress.indexOf(\":\") > 0) {\r\n            String[] webAddressSplit = webAddress.split(\":\");\r\n            host = webAddressSplit[0];\r\n            port = Integer.parseInt(webAddressSplit[1]);\r\n        }\r\n        URL jmxURL = new URL(scheme, host, port, \"/jmx?qry=\" + beanQuery);\r\n        LOG.debug(\"JMX URL: {}\", jmxURL);\r\n        URLConnection conn = connectionFactory.openConnection(jmxURL, UserGroupInformation.isSecurityEnabled());\r\n        conn.setConnectTimeout(5 * 1000);\r\n        conn.setReadTimeout(5 * 1000);\r\n        InputStream in = conn.getInputStream();\r\n        InputStreamReader isr = new InputStreamReader(in, \"UTF-8\");\r\n        reader = new BufferedReader(isr);\r\n        StringBuilder sb = new StringBuilder();\r\n        String line = null;\r\n        while ((line = reader.readLine()) != null) {\r\n            sb.append(line);\r\n        }\r\n        String jmxOutput = sb.toString();\r\n        JSONObject json = new JSONObject(jmxOutput);\r\n        ret = json.getJSONArray(\"beans\");\r\n    } catch (IOException e) {\r\n        LOG.error(\"Cannot read JMX bean {} from server {}\", beanQuery, webAddress, e);\r\n    } catch (JSONException e) {\r\n        LOG.error(\"Cannot parse JMX output for {} from server {}: {}\", beanQuery, webAddress, e.getMessage());\r\n    } catch (Exception e) {\r\n        LOG.error(\"Cannot parse JMX output for {} from server {}\", beanQuery, webAddress, e);\r\n    } finally {\r\n        if (reader != null) {\r\n            try {\r\n                reader.close();\r\n            } catch (IOException e) {\r\n                LOG.error(\"Problem closing {}\", webAddress, e);\r\n            }\r\n        }\r\n    }\r\n    return ret;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 2,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "getVersion",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String getVersion()\n{\r\n    return VersionInfo.getVersion();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "getCompileInfo",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "String getCompileInfo()\n{\r\n    return VersionInfo.getDate() + \" by \" + VersionInfo.getUser() + \" from \" + VersionInfo.getBranch();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "newInstance",
  "errType" : [ "ReflectiveOperationException" ],
  "containingMethodsNum" : 7,
  "sourceCodeText" : "T newInstance(final Configuration conf, final R context, final Class<R> contextClass, final Class<T> clazz)\n{\r\n    try {\r\n        if (contextClass == null) {\r\n            if (conf == null) {\r\n                Constructor<T> constructor = clazz.getConstructor();\r\n                return constructor.newInstance();\r\n            } else {\r\n                Constructor<T> constructor = clazz.getConstructor(Configuration.class);\r\n                return constructor.newInstance(conf);\r\n            }\r\n        } else {\r\n            Constructor<T> constructor = clazz.getConstructor(Configuration.class, contextClass);\r\n            return constructor.newInstance(conf, context);\r\n        }\r\n    } catch (ReflectiveOperationException e) {\r\n        LOG.error(\"Could not instantiate: {}\", clazz.getSimpleName(), e);\r\n        return null;\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "newFileSubclusterResolver",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "FileSubclusterResolver newFileSubclusterResolver(Configuration conf, Router router)\n{\r\n    Class<? extends FileSubclusterResolver> clazz = conf.getClass(RBFConfigKeys.FEDERATION_FILE_RESOLVER_CLIENT_CLASS, RBFConfigKeys.FEDERATION_FILE_RESOLVER_CLIENT_CLASS_DEFAULT, FileSubclusterResolver.class);\r\n    return newInstance(conf, router, Router.class, clazz);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "newActiveNamenodeResolver",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "ActiveNamenodeResolver newActiveNamenodeResolver(Configuration conf, StateStoreService stateStore)\n{\r\n    Class<? extends ActiveNamenodeResolver> clazz = conf.getClass(RBFConfigKeys.FEDERATION_NAMENODE_RESOLVER_CLIENT_CLASS, RBFConfigKeys.FEDERATION_NAMENODE_RESOLVER_CLIENT_CLASS_DEFAULT, ActiveNamenodeResolver.class);\r\n    return newInstance(conf, stateStore, StateStoreService.class, clazz);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "newSecretManager",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "AbstractDelegationTokenSecretManager<DelegationTokenIdentifier> newSecretManager(Configuration conf)\n{\r\n    Class<? extends AbstractDelegationTokenSecretManager> clazz = conf.getClass(RBFConfigKeys.DFS_ROUTER_DELEGATION_TOKEN_DRIVER_CLASS, RBFConfigKeys.DFS_ROUTER_DELEGATION_TOKEN_DRIVER_CLASS_DEFAULT, AbstractDelegationTokenSecretManager.class);\r\n    return newInstance(conf, null, null, clazz);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "updateMountPointStatus",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "HdfsFileStatus updateMountPointStatus(HdfsFileStatus dirStatus, int children)\n{\r\n    EnumSet<HdfsFileStatus.Flags> flags = DFSUtil.getFlags(dirStatus.isEncrypted(), dirStatus.isErasureCoded(), dirStatus.isSnapshotEnabled(), dirStatus.hasAcl());\r\n    EnumSet.noneOf(HdfsFileStatus.Flags.class);\r\n    return new HdfsFileStatus.Builder().atime(dirStatus.getAccessTime()).blocksize(dirStatus.getBlockSize()).children(children).ecPolicy(dirStatus.getErasureCodingPolicy()).feInfo(dirStatus.getFileEncryptionInfo()).fileId(dirStatus.getFileId()).group(dirStatus.getGroup()).isdir(dirStatus.isDir()).length(dirStatus.getLen()).mtime(dirStatus.getModificationTime()).owner(dirStatus.getOwner()).path(dirStatus.getLocalNameInBytes()).perm(dirStatus.getPermission()).replication(dirStatus.getReplication()).storagePolicy(dirStatus.getStoragePolicy()).symlink(dirStatus.getSymlinkInBytes()).flags(flags).build();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "newFairnessPolicyController",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "RouterRpcFairnessPolicyController newFairnessPolicyController(Configuration conf)\n{\r\n    Class<? extends RouterRpcFairnessPolicyController> clazz = conf.getClass(RBFConfigKeys.DFS_ROUTER_FAIRNESS_POLICY_CONTROLLER_CLASS, RBFConfigKeys.DFS_ROUTER_FAIRNESS_POLICY_CONTROLLER_CLASS_DEFAULT, RouterRpcFairnessPolicyController.class);\r\n    return newInstance(conf, null, null, clazz);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "getAllConfiguredNS",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "Set<String> getAllConfiguredNS(Configuration conf) throws IllegalArgumentException\n{\r\n    Collection<String> namenodes = conf.getTrimmedStringCollection(DFS_ROUTER_MONITOR_NAMENODE);\r\n    Set<String> nameservices = new HashSet();\r\n    for (String namenode : namenodes) {\r\n        String[] namenodeSplit = namenode.split(\"\\\\.\");\r\n        String nsId;\r\n        if (namenodeSplit.length == 2) {\r\n            nsId = namenodeSplit[0];\r\n        } else if (namenodeSplit.length == 1) {\r\n            nsId = namenode;\r\n        } else {\r\n            String errorMsg = \"Wrong name service specified : \" + namenode;\r\n            throw new IllegalArgumentException(errorMsg);\r\n        }\r\n        nameservices.add(nsId);\r\n    }\r\n    return nameservices;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\protocol\\impl\\pb",
  "methodName" : "getProto",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "LeaveSafeModeRequestProto getProto()\n{\r\n    return translator.build();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\protocol\\impl\\pb",
  "methodName" : "setProto",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setProto(Message proto)\n{\r\n    this.translator.setProto(proto);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\protocol\\impl\\pb",
  "methodName" : "readInstance",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void readInstance(String base64String) throws IOException\n{\r\n    this.translator.readInstance(base64String);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\protocol\\impl\\pb",
  "methodName" : "getProto",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "EnableNameserviceResponseProto getProto()\n{\r\n    return translator.build();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\protocol\\impl\\pb",
  "methodName" : "setProto",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setProto(Message proto)\n{\r\n    this.translator.setProto(proto);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\protocol\\impl\\pb",
  "methodName" : "readInstance",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void readInstance(String base64String) throws IOException\n{\r\n    this.translator.readInstance(base64String);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\protocol\\impl\\pb",
  "methodName" : "getStatus",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "boolean getStatus()\n{\r\n    return this.translator.getProtoOrBuilder().getStatus();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\protocol\\impl\\pb",
  "methodName" : "setStatus",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setStatus(boolean status)\n{\r\n    this.translator.getBuilder().setStatus(status);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\protocol\\impl\\pb",
  "methodName" : "getProto",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "UpdateNamenodeRegistrationRequestProto getProto()\n{\r\n    return this.translator.build();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\protocol\\impl\\pb",
  "methodName" : "setProto",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setProto(Message protocol)\n{\r\n    this.translator.setProto(protocol);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\protocol\\impl\\pb",
  "methodName" : "readInstance",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void readInstance(String base64String) throws IOException\n{\r\n    this.translator.readInstance(base64String);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\protocol\\impl\\pb",
  "methodName" : "getNameserviceId",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String getNameserviceId()\n{\r\n    return this.translator.getProtoOrBuilder().getNameserviceId();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\protocol\\impl\\pb",
  "methodName" : "getNamenodeId",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String getNamenodeId()\n{\r\n    return this.translator.getProtoOrBuilder().getNamenodeId();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\protocol\\impl\\pb",
  "methodName" : "getState",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "FederationNamenodeServiceState getState()\n{\r\n    return FederationNamenodeServiceState.valueOf(this.translator.getProtoOrBuilder().getState());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\protocol\\impl\\pb",
  "methodName" : "setNameserviceId",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setNameserviceId(String nsId)\n{\r\n    this.translator.getBuilder().setNameserviceId(nsId);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\protocol\\impl\\pb",
  "methodName" : "setNamenodeId",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setNamenodeId(String nnId)\n{\r\n    this.translator.getBuilder().setNamenodeId(nnId);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\protocol\\impl\\pb",
  "methodName" : "setState",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setState(FederationNamenodeServiceState state)\n{\r\n    this.translator.getBuilder().setState(state.toString());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\records\\impl\\pb",
  "methodName" : "getProto",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "DisabledNameserviceRecordProto getProto()\n{\r\n    return this.translator.build();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\records\\impl\\pb",
  "methodName" : "setProto",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setProto(Message proto)\n{\r\n    this.translator.setProto(proto);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\records\\impl\\pb",
  "methodName" : "readInstance",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void readInstance(String base64String) throws IOException\n{\r\n    this.translator.readInstance(base64String);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\records\\impl\\pb",
  "methodName" : "getNameserviceId",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String getNameserviceId()\n{\r\n    return this.translator.getProtoOrBuilder().getNameServiceId();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\records\\impl\\pb",
  "methodName" : "setNameserviceId",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setNameserviceId(String nameServiceId)\n{\r\n    this.translator.getBuilder().setNameServiceId(nameServiceId);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\records\\impl\\pb",
  "methodName" : "setDateModified",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setDateModified(long time)\n{\r\n    this.translator.getBuilder().setDateModified(time);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\records\\impl\\pb",
  "methodName" : "getDateModified",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "long getDateModified()\n{\r\n    return this.translator.getProtoOrBuilder().getDateModified();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\records\\impl\\pb",
  "methodName" : "setDateCreated",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setDateCreated(long time)\n{\r\n    this.translator.getBuilder().setDateCreated(time);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\records\\impl\\pb",
  "methodName" : "getDateCreated",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "long getDateCreated()\n{\r\n    return this.translator.getProtoOrBuilder().getDateCreated();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "checkPermission",
  "errType" : null,
  "containingMethodsNum" : 12,
  "sourceCodeText" : "void checkPermission(MountTable mountTable, FsAction access) throws AccessControlException\n{\r\n    if (isSuperUser()) {\r\n        return;\r\n    }\r\n    FsPermission mode = mountTable.getMode();\r\n    if (getUser().equals(mountTable.getOwnerName()) && mode.getUserAction().implies(access)) {\r\n        return;\r\n    }\r\n    if (isMemberOfGroup(mountTable.getGroupName()) && mode.getGroupAction().implies(access)) {\r\n        return;\r\n    }\r\n    if (!getUser().equals(mountTable.getOwnerName()) && !isMemberOfGroup(mountTable.getGroupName()) && mode.getOtherAction().implies(access)) {\r\n        return;\r\n    }\r\n    throw new AccessControlException(\"Permission denied while accessing mount table \" + mountTable.getSourcePath() + \": user \" + getUser() + \" does not have \" + access.toString() + \" permissions.\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "checkSuperuserPrivilege",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void checkSuperuserPrivilege() throws AccessControlException\n{\r\n    UserGroupInformation ugi = null;\r\n    try {\r\n        ugi = NameNode.getRemoteUser();\r\n    } catch (IOException e) {\r\n    }\r\n    if (ugi == null) {\r\n        LOG.error(\"Cannot get the remote user name\");\r\n        throw new AccessControlException(\"Cannot get the remote user name\");\r\n    }\r\n    if (ugi.getShortUserName().equals(superUser)) {\r\n        return;\r\n    }\r\n    if (ugi.getGroupsSet().contains(superGroup)) {\r\n        return;\r\n    }\r\n    throw new AccessControlException(ugi.getUserName() + \" is not a super user\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "getBlocks",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "BlocksWithLocations getBlocks(DatanodeInfo datanode, long size, long minBlockSize, long hotBlockTimeInterval) throws IOException\n{\r\n    rpcServer.checkOperation(OperationCategory.READ);\r\n    Map<String, DatanodeStorageReport[]> map = rpcServer.getDatanodeStorageReportMap(DatanodeReportType.ALL);\r\n    String nsId = null;\r\n    for (Entry<String, DatanodeStorageReport[]> entry : map.entrySet()) {\r\n        DatanodeStorageReport[] dns = entry.getValue();\r\n        for (DatanodeStorageReport dn : dns) {\r\n            DatanodeInfo dnInfo = dn.getDatanodeInfo();\r\n            if (dnInfo.getDatanodeUuid().equals(datanode.getDatanodeUuid())) {\r\n                nsId = entry.getKey();\r\n                break;\r\n            }\r\n        }\r\n        if (nsId != null) {\r\n            break;\r\n        }\r\n    }\r\n    if (nsId != null) {\r\n        RemoteMethod method = new RemoteMethod(NamenodeProtocol.class, \"getBlocks\", new Class<?>[] { DatanodeInfo.class, long.class, long.class, long.class }, datanode, size, minBlockSize, hotBlockTimeInterval);\r\n        return rpcClient.invokeSingle(nsId, method, BlocksWithLocations.class);\r\n    }\r\n    return null;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "getBlockKeys",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "ExportedBlockKeys getBlockKeys() throws IOException\n{\r\n    rpcServer.checkOperation(OperationCategory.READ);\r\n    RemoteMethod method = new RemoteMethod(NamenodeProtocol.class, \"getBlockKeys\");\r\n    return rpcServer.invokeAtAvailableNs(method, ExportedBlockKeys.class);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "getTransactionID",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "long getTransactionID() throws IOException\n{\r\n    rpcServer.checkOperation(OperationCategory.READ);\r\n    RemoteMethod method = new RemoteMethod(NamenodeProtocol.class, \"getTransactionID\");\r\n    return rpcServer.invokeAtAvailableNs(method, long.class);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "getMostRecentCheckpointTxId",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "long getMostRecentCheckpointTxId() throws IOException\n{\r\n    rpcServer.checkOperation(OperationCategory.READ);\r\n    RemoteMethod method = new RemoteMethod(NamenodeProtocol.class, \"getMostRecentCheckpointTxId\");\r\n    return rpcServer.invokeAtAvailableNs(method, long.class);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "rollEditLog",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "CheckpointSignature rollEditLog() throws IOException\n{\r\n    rpcServer.checkOperation(OperationCategory.WRITE, false);\r\n    return null;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "versionRequest",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "NamespaceInfo versionRequest() throws IOException\n{\r\n    rpcServer.checkOperation(OperationCategory.READ);\r\n    RemoteMethod method = new RemoteMethod(NamenodeProtocol.class, \"versionRequest\");\r\n    return rpcServer.invokeAtAvailableNs(method, NamespaceInfo.class);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "errorReport",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void errorReport(NamenodeRegistration registration, int errorCode, String msg) throws IOException\n{\r\n    rpcServer.checkOperation(OperationCategory.UNCHECKED, false);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "registerSubordinateNamenode",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "NamenodeRegistration registerSubordinateNamenode(NamenodeRegistration registration) throws IOException\n{\r\n    rpcServer.checkOperation(OperationCategory.WRITE, false);\r\n    return null;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "startCheckpoint",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "NamenodeCommand startCheckpoint(NamenodeRegistration registration) throws IOException\n{\r\n    rpcServer.checkOperation(OperationCategory.WRITE, false);\r\n    return null;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "endCheckpoint",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void endCheckpoint(NamenodeRegistration registration, CheckpointSignature sig) throws IOException\n{\r\n    rpcServer.checkOperation(OperationCategory.WRITE, false);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "getEditLogManifest",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "RemoteEditLogManifest getEditLogManifest(long sinceTxId) throws IOException\n{\r\n    rpcServer.checkOperation(OperationCategory.READ, false);\r\n    return null;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "isUpgradeFinalized",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "boolean isUpgradeFinalized() throws IOException\n{\r\n    rpcServer.checkOperation(OperationCategory.READ, false);\r\n    return false;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "isRollingUpgrade",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "boolean isRollingUpgrade() throws IOException\n{\r\n    rpcServer.checkOperation(OperationCategory.READ, false);\r\n    return false;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "getNextSPSPath",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Long getNextSPSPath() throws IOException\n{\r\n    rpcServer.checkOperation(OperationCategory.READ, false);\r\n    return null;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\metrics",
  "methodName" : "close",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void close()\n{\r\n    if (fsStateBeanName != null) {\r\n        MBeans.unregister(fsStateBeanName);\r\n        fsStateBeanName = null;\r\n    }\r\n    if (nnInfoBeanName != null) {\r\n        MBeans.unregister(nnInfoBeanName);\r\n        nnInfoBeanName = null;\r\n    }\r\n    if (nnStatusBeanName != null) {\r\n        MBeans.unregister(nnStatusBeanName);\r\n        nnStatusBeanName = null;\r\n    }\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\metrics",
  "methodName" : "getRBFMetrics",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "RBFMetrics getRBFMetrics() throws IOException\n{\r\n    RBFMetrics metrics = getRouter().getMetrics();\r\n    if (metrics == null) {\r\n        throw new IOException(\"Federated metrics is not initialized\");\r\n    }\r\n    return metrics;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\metrics",
  "methodName" : "getVersion",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "String getVersion()\n{\r\n    return VersionInfo.getVersion() + \", r\" + VersionInfo.getRevision();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\metrics",
  "methodName" : "getSoftwareVersion",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String getSoftwareVersion()\n{\r\n    return VersionInfo.getVersion();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\metrics",
  "methodName" : "getUsed",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 2,
  "sourceCodeText" : "long getUsed()\n{\r\n    try {\r\n        return getRBFMetrics().getUsedCapacity();\r\n    } catch (IOException e) {\r\n        LOG.debug(\"Failed to get the used capacity\", e);\r\n    }\r\n    return 0;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\metrics",
  "methodName" : "getFree",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 2,
  "sourceCodeText" : "long getFree()\n{\r\n    try {\r\n        return getRBFMetrics().getRemainingCapacity();\r\n    } catch (IOException e) {\r\n        LOG.debug(\"Failed to get remaining capacity\", e);\r\n    }\r\n    return 0;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\metrics",
  "methodName" : "getTotal",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 2,
  "sourceCodeText" : "long getTotal()\n{\r\n    try {\r\n        return getRBFMetrics().getTotalCapacity();\r\n    } catch (IOException e) {\r\n        LOG.debug(\"Failed to Get total capacity\", e);\r\n    }\r\n    return 0;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\metrics",
  "methodName" : "getProvidedCapacity",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 2,
  "sourceCodeText" : "long getProvidedCapacity()\n{\r\n    try {\r\n        return getRBFMetrics().getProvidedSpace();\r\n    } catch (IOException e) {\r\n        LOG.debug(\"Failed to get provided capacity\", e);\r\n    }\r\n    return 0;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\metrics",
  "methodName" : "getSafemode",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String getSafemode()\n{\r\n    try {\r\n        return getRBFMetrics().getSafemode();\r\n    } catch (IOException e) {\r\n        return \"Failed to get safemode status. Please check router\" + \"log for more detail.\";\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\metrics",
  "methodName" : "isUpgradeFinalized",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean isUpgradeFinalized()\n{\r\n    return true;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\metrics",
  "methodName" : "getRollingUpgradeStatus",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "RollingUpgradeInfo.Bean getRollingUpgradeStatus()\n{\r\n    return null;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\metrics",
  "methodName" : "getNonDfsUsedSpace",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getNonDfsUsedSpace()\n{\r\n    return 0;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\metrics",
  "methodName" : "getPercentUsed",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "float getPercentUsed()\n{\r\n    return DFSUtilClient.getPercentUsed(getCapacityUsed(), getCapacityTotal());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\metrics",
  "methodName" : "getPercentRemaining",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "float getPercentRemaining()\n{\r\n    return DFSUtilClient.getPercentUsed(getCapacityRemaining(), getCapacityTotal());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\metrics",
  "methodName" : "getCacheUsed",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getCacheUsed()\n{\r\n    return 0;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\metrics",
  "methodName" : "getCacheCapacity",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getCacheCapacity()\n{\r\n    return 0;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\metrics",
  "methodName" : "getBlockPoolUsedSpace",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getBlockPoolUsedSpace()\n{\r\n    return 0;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\metrics",
  "methodName" : "getPercentBlockPoolUsed",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "float getPercentBlockPoolUsed()\n{\r\n    return 0;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\metrics",
  "methodName" : "getTotalBlocks",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 2,
  "sourceCodeText" : "long getTotalBlocks()\n{\r\n    try {\r\n        return getRBFMetrics().getNumBlocks();\r\n    } catch (IOException e) {\r\n        LOG.debug(\"Failed to get number of blocks\", e);\r\n    }\r\n    return 0;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\metrics",
  "methodName" : "getNumberOfMissingBlocks",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 2,
  "sourceCodeText" : "long getNumberOfMissingBlocks()\n{\r\n    try {\r\n        return getRBFMetrics().getNumOfMissingBlocks();\r\n    } catch (IOException e) {\r\n        LOG.debug(\"Failed to get number of missing blocks\", e);\r\n    }\r\n    return 0;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\metrics",
  "methodName" : "getPendingReplicationBlocks",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 2,
  "sourceCodeText" : "long getPendingReplicationBlocks()\n{\r\n    try {\r\n        return getRBFMetrics().getNumOfBlocksPendingReplication();\r\n    } catch (IOException e) {\r\n        LOG.debug(\"Failed to get number of blocks pending replica\", e);\r\n    }\r\n    return 0;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\metrics",
  "methodName" : "getPendingReconstructionBlocks",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 2,
  "sourceCodeText" : "long getPendingReconstructionBlocks()\n{\r\n    try {\r\n        return getRBFMetrics().getNumOfBlocksPendingReplication();\r\n    } catch (IOException e) {\r\n        LOG.debug(\"Failed to get number of blocks pending replica\", e);\r\n    }\r\n    return 0;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\metrics",
  "methodName" : "getUnderReplicatedBlocks",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 2,
  "sourceCodeText" : "long getUnderReplicatedBlocks()\n{\r\n    try {\r\n        return getRBFMetrics().getNumOfBlocksUnderReplicated();\r\n    } catch (IOException e) {\r\n        LOG.debug(\"Failed to get number of blocks under replicated\", e);\r\n    }\r\n    return 0;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\metrics",
  "methodName" : "getLowRedundancyBlocks",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 2,
  "sourceCodeText" : "long getLowRedundancyBlocks()\n{\r\n    try {\r\n        return getRBFMetrics().getNumOfBlocksUnderReplicated();\r\n    } catch (IOException e) {\r\n        LOG.debug(\"Failed to get number of blocks under replicated\", e);\r\n    }\r\n    return 0;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\metrics",
  "methodName" : "getPendingDeletionBlocks",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 2,
  "sourceCodeText" : "long getPendingDeletionBlocks()\n{\r\n    try {\r\n        return getRBFMetrics().getNumOfBlocksPendingDeletion();\r\n    } catch (IOException e) {\r\n        LOG.debug(\"Failed to get number of blocks pending deletion\", e);\r\n    }\r\n    return 0;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\metrics",
  "methodName" : "getScheduledReplicationBlocks",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 2,
  "sourceCodeText" : "long getScheduledReplicationBlocks()\n{\r\n    try {\r\n        return getRBFMetrics().getScheduledReplicationBlocks();\r\n    } catch (IOException e) {\r\n        LOG.debug(\"Failed to get number of scheduled replication blocks\", e);\r\n    }\r\n    return 0;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\metrics",
  "methodName" : "getNumberOfMissingBlocksWithReplicationFactorOne",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 2,
  "sourceCodeText" : "long getNumberOfMissingBlocksWithReplicationFactorOne()\n{\r\n    try {\r\n        return getRBFMetrics().getNumberOfMissingBlocksWithReplicationFactorOne();\r\n    } catch (IOException e) {\r\n        LOG.debug(\"Failed to get number of missing blocks with replication \" + \"factor one.\", e);\r\n    }\r\n    return 0;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\metrics",
  "methodName" : "getHighestPriorityLowRedundancyReplicatedBlocks",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 2,
  "sourceCodeText" : "long getHighestPriorityLowRedundancyReplicatedBlocks()\n{\r\n    try {\r\n        return getRBFMetrics().getHighestPriorityLowRedundancyReplicatedBlocks();\r\n    } catch (IOException e) {\r\n        LOG.debug(\"Failed to get number of highest priority low redundancy \" + \"replicated blocks.\", e);\r\n    }\r\n    return 0;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\metrics",
  "methodName" : "getHighestPriorityLowRedundancyECBlocks",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 2,
  "sourceCodeText" : "long getHighestPriorityLowRedundancyECBlocks()\n{\r\n    try {\r\n        return getRBFMetrics().getHighestPriorityLowRedundancyECBlocks();\r\n    } catch (IOException e) {\r\n        LOG.debug(\"Failed to get number of highest priority low redundancy EC \" + \"blocks.\", e);\r\n    }\r\n    return 0;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\metrics",
  "methodName" : "getCorruptFiles",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getCorruptFiles()\n{\r\n    return \"N/A\";\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\metrics",
  "methodName" : "getCorruptFilesCount",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 2,
  "sourceCodeText" : "int getCorruptFilesCount()\n{\r\n    try {\r\n        return getRBFMetrics().getCorruptFilesCount();\r\n    } catch (IOException e) {\r\n        LOG.debug(\"Failed to get number of corrupt files\", e);\r\n    }\r\n    return 0;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\metrics",
  "methodName" : "getThreads",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "int getThreads()\n{\r\n    return ManagementFactory.getThreadMXBean().getThreadCount();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\metrics",
  "methodName" : "getLiveNodes",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String getLiveNodes()\n{\r\n    return this.getNodes(DatanodeReportType.LIVE);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\metrics",
  "methodName" : "getDeadNodes",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String getDeadNodes()\n{\r\n    return this.getNodes(DatanodeReportType.DEAD);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\metrics",
  "methodName" : "getDecomNodes",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String getDecomNodes()\n{\r\n    return this.getNodes(DatanodeReportType.DECOMMISSIONING);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\metrics",
  "methodName" : "getNodes",
  "errType" : [ "ExecutionException" ],
  "containingMethodsNum" : 2,
  "sourceCodeText" : "String getNodes(final DatanodeReportType type)\n{\r\n    try {\r\n        return this.dnCache.get(type);\r\n    } catch (ExecutionException e) {\r\n        LOG.error(\"Cannot get the DN storage report for {}\", type, e);\r\n    }\r\n    return \"{}\";\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\metrics",
  "methodName" : "getNodesImpl",
  "errType" : [ "StandbyException", "SubClusterTimeoutException", "IOException" ],
  "containingMethodsNum" : 27,
  "sourceCodeText" : "String getNodesImpl(final DatanodeReportType type)\n{\r\n    final Map<String, Map<String, Object>> info = new HashMap<>();\r\n    try {\r\n        RouterClientProtocol clientProtocol = this.router.getRpcServer().getClientProtocolModule();\r\n        DatanodeStorageReport[] datanodeStorageReports = clientProtocol.getDatanodeStorageReport(type, false, dnReportTimeOut);\r\n        for (DatanodeStorageReport datanodeStorageReport : datanodeStorageReports) {\r\n            DatanodeInfo node = datanodeStorageReport.getDatanodeInfo();\r\n            StorageReport[] storageReports = datanodeStorageReport.getStorageReports();\r\n            Map<String, Object> innerinfo = new HashMap<>();\r\n            innerinfo.put(\"infoAddr\", node.getInfoAddr());\r\n            innerinfo.put(\"infoSecureAddr\", node.getInfoSecureAddr());\r\n            innerinfo.put(\"xferaddr\", node.getXferAddr());\r\n            innerinfo.put(\"location\", node.getNetworkLocation());\r\n            innerinfo.put(\"lastContact\", getLastContact(node));\r\n            innerinfo.put(\"usedSpace\", node.getDfsUsed());\r\n            innerinfo.put(\"adminState\", node.getAdminState().toString());\r\n            innerinfo.put(\"nonDfsUsedSpace\", node.getNonDfsUsed());\r\n            innerinfo.put(\"capacity\", node.getCapacity());\r\n            innerinfo.put(\"numBlocks\", -1);\r\n            innerinfo.put(\"version\", (node.getSoftwareVersion() == null ? \"UNKNOWN\" : node.getSoftwareVersion()));\r\n            innerinfo.put(\"used\", node.getDfsUsed());\r\n            innerinfo.put(\"remaining\", node.getRemaining());\r\n            innerinfo.put(\"blockScheduled\", -1);\r\n            innerinfo.put(\"blockPoolUsed\", node.getBlockPoolUsed());\r\n            innerinfo.put(\"blockPoolUsedPercent\", node.getBlockPoolUsedPercent());\r\n            innerinfo.put(\"volfails\", -1);\r\n            innerinfo.put(\"blockPoolUsedPercentStdDev\", Util.getBlockPoolUsedPercentStdDev(storageReports));\r\n            info.put(node.getXferAddrWithHostname(), Collections.unmodifiableMap(innerinfo));\r\n        }\r\n    } catch (StandbyException e) {\r\n        LOG.error(\"Cannot get {} nodes, Router in safe mode\", type);\r\n    } catch (SubClusterTimeoutException e) {\r\n        LOG.error(\"Cannot get {} nodes, subclusters timed out responding\", type);\r\n    } catch (IOException e) {\r\n        LOG.error(\"Cannot get \" + type + \" nodes\", e);\r\n    }\r\n    return JSON.toString(info);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\metrics",
  "methodName" : "getClusterId",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 2,
  "sourceCodeText" : "String getClusterId()\n{\r\n    try {\r\n        return getNamespaceInfo(FederationNamespaceInfo::getClusterId).toString();\r\n    } catch (IOException e) {\r\n        LOG.error(\"Cannot fetch cluster ID metrics {}\", e.getMessage());\r\n        return \"\";\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\metrics",
  "methodName" : "getBlockPoolId",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 2,
  "sourceCodeText" : "String getBlockPoolId()\n{\r\n    try {\r\n        return getNamespaceInfo(FederationNamespaceInfo::getBlockPoolId).toString();\r\n    } catch (IOException e) {\r\n        LOG.error(\"Cannot fetch block pool ID metrics {}\", e.getMessage());\r\n        return \"\";\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\metrics",
  "methodName" : "getNamespaceInfo",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "Collection<String> getNamespaceInfo(Function<FederationNamespaceInfo, String> f) throws IOException\n{\r\n    StateStoreService stateStore = router.getStateStore();\r\n    MembershipStore membershipStore = stateStore.getRegisteredRecordStore(MembershipStore.class);\r\n    GetNamespaceInfoRequest request = GetNamespaceInfoRequest.newInstance();\r\n    GetNamespaceInfoResponse response = membershipStore.getNamespaceInfo(request);\r\n    return response.getNamespaceInfo().stream().map(f).collect(Collectors.toSet());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\metrics",
  "methodName" : "getNameDirStatuses",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getNameDirStatuses()\n{\r\n    return \"N/A\";\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\metrics",
  "methodName" : "getNodeUsage",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getNodeUsage()\n{\r\n    return \"N/A\";\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\metrics",
  "methodName" : "getNameJournalStatus",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getNameJournalStatus()\n{\r\n    return \"N/A\";\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\metrics",
  "methodName" : "getJournalTransactionInfo",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getJournalTransactionInfo()\n{\r\n    return \"N/A\";\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\metrics",
  "methodName" : "getNNStartedTimeInMillis",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 2,
  "sourceCodeText" : "long getNNStartedTimeInMillis()\n{\r\n    try {\r\n        return getRouter().getStartTime();\r\n    } catch (IOException e) {\r\n        LOG.debug(\"Failed to get the router startup time\", e);\r\n    }\r\n    return 0;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\metrics",
  "methodName" : "getCompileInfo",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "String getCompileInfo()\n{\r\n    return VersionInfo.getDate() + \" by \" + VersionInfo.getUser() + \" from \" + VersionInfo.getBranch();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\metrics",
  "methodName" : "getDistinctVersionCount",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int getDistinctVersionCount()\n{\r\n    return 0;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\metrics",
  "methodName" : "getDistinctVersions",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Map<String, Integer> getDistinctVersions()\n{\r\n    return null;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\metrics",
  "methodName" : "getFSState",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getFSState()\n{\r\n    return \"Operational\";\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\metrics",
  "methodName" : "getBlocksTotal",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "long getBlocksTotal()\n{\r\n    return this.getTotalBlocks();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\metrics",
  "methodName" : "getCapacityTotal",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "long getCapacityTotal()\n{\r\n    return this.getTotal();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\metrics",
  "methodName" : "getCapacityRemaining",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "long getCapacityRemaining()\n{\r\n    return this.getFree();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\metrics",
  "methodName" : "getCapacityUsed",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "long getCapacityUsed()\n{\r\n    return this.getUsed();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\metrics",
  "methodName" : "getProvidedCapacityTotal",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "long getProvidedCapacityTotal()\n{\r\n    return getProvidedCapacity();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\metrics",
  "methodName" : "getFilesTotal",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 2,
  "sourceCodeText" : "long getFilesTotal()\n{\r\n    try {\r\n        return getRBFMetrics().getNumFiles();\r\n    } catch (IOException e) {\r\n        LOG.debug(\"Failed to get number of files\", e);\r\n    }\r\n    return 0;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\metrics",
  "methodName" : "getTotalLoad",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int getTotalLoad()\n{\r\n    return -1;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\metrics",
  "methodName" : "getNumLiveDataNodes",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 2,
  "sourceCodeText" : "int getNumLiveDataNodes()\n{\r\n    try {\r\n        return getRBFMetrics().getNumLiveNodes();\r\n    } catch (IOException e) {\r\n        LOG.debug(\"Failed to get number of live nodes\", e);\r\n    }\r\n    return 0;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\metrics",
  "methodName" : "getNumDeadDataNodes",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 2,
  "sourceCodeText" : "int getNumDeadDataNodes()\n{\r\n    try {\r\n        return getRBFMetrics().getNumDeadNodes();\r\n    } catch (IOException e) {\r\n        LOG.debug(\"Failed to get number of dead nodes\", e);\r\n    }\r\n    return 0;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\metrics",
  "methodName" : "getNumStaleDataNodes",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 2,
  "sourceCodeText" : "int getNumStaleDataNodes()\n{\r\n    try {\r\n        return getRBFMetrics().getNumStaleNodes();\r\n    } catch (IOException e) {\r\n        LOG.debug(\"Failed to get number of stale nodes\", e);\r\n    }\r\n    return 0;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\metrics",
  "methodName" : "getNumDecomLiveDataNodes",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 2,
  "sourceCodeText" : "int getNumDecomLiveDataNodes()\n{\r\n    try {\r\n        return getRBFMetrics().getNumDecomLiveNodes();\r\n    } catch (IOException e) {\r\n        LOG.debug(\"Failed to get the number of live decommissioned datanodes\", e);\r\n    }\r\n    return 0;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\metrics",
  "methodName" : "getNumDecomDeadDataNodes",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 2,
  "sourceCodeText" : "int getNumDecomDeadDataNodes()\n{\r\n    try {\r\n        return getRBFMetrics().getNumDecomDeadNodes();\r\n    } catch (IOException e) {\r\n        LOG.debug(\"Failed to get the number of dead decommissioned datanodes\", e);\r\n    }\r\n    return 0;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\metrics",
  "methodName" : "getNumDecommissioningDataNodes",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 2,
  "sourceCodeText" : "int getNumDecommissioningDataNodes()\n{\r\n    try {\r\n        return getRBFMetrics().getNumDecommissioningNodes();\r\n    } catch (IOException e) {\r\n        LOG.debug(\"Failed to get number of decommissioning nodes\", e);\r\n    }\r\n    return 0;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\metrics",
  "methodName" : "getNumInMaintenanceLiveDataNodes",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 2,
  "sourceCodeText" : "int getNumInMaintenanceLiveDataNodes()\n{\r\n    try {\r\n        return getRBFMetrics().getNumInMaintenanceLiveDataNodes();\r\n    } catch (IOException e) {\r\n        LOG.debug(\"Failed to get number of live in maintenance nodes\", e);\r\n    }\r\n    return 0;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\metrics",
  "methodName" : "getNumInMaintenanceDeadDataNodes",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 2,
  "sourceCodeText" : "int getNumInMaintenanceDeadDataNodes()\n{\r\n    try {\r\n        return getRBFMetrics().getNumInMaintenanceDeadDataNodes();\r\n    } catch (IOException e) {\r\n        LOG.debug(\"Failed to get number of dead in maintenance nodes\", e);\r\n    }\r\n    return 0;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\metrics",
  "methodName" : "getNumEnteringMaintenanceDataNodes",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 2,
  "sourceCodeText" : "int getNumEnteringMaintenanceDataNodes()\n{\r\n    try {\r\n        return getRBFMetrics().getNumEnteringMaintenanceDataNodes();\r\n    } catch (IOException e) {\r\n        LOG.debug(\"Failed to get number of entering maintenance nodes\", e);\r\n    }\r\n    return 0;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\metrics",
  "methodName" : "getNumInServiceLiveDataNodes",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int getNumInServiceLiveDataNodes()\n{\r\n    return 0;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\metrics",
  "methodName" : "getVolumeFailuresTotal",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int getVolumeFailuresTotal()\n{\r\n    return 0;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\metrics",
  "methodName" : "getEstimatedCapacityLostTotal",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getEstimatedCapacityLostTotal()\n{\r\n    return 0;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\metrics",
  "methodName" : "getSnapshotStats",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getSnapshotStats()\n{\r\n    return null;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\metrics",
  "methodName" : "getMaxObjects",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getMaxObjects()\n{\r\n    return 0;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\metrics",
  "methodName" : "getBlockDeletionStartTime",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getBlockDeletionStartTime()\n{\r\n    return -1;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\metrics",
  "methodName" : "getNumStaleStorages",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int getNumStaleStorages()\n{\r\n    return -1;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\metrics",
  "methodName" : "getTopUserOpCounts",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getTopUserOpCounts()\n{\r\n    return \"N/A\";\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\metrics",
  "methodName" : "getFsLockQueueLength",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int getFsLockQueueLength()\n{\r\n    return 0;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\metrics",
  "methodName" : "getTotalSyncCount",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getTotalSyncCount()\n{\r\n    return 0;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\metrics",
  "methodName" : "getTotalSyncTimes",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getTotalSyncTimes()\n{\r\n    return \"\";\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\metrics",
  "methodName" : "getLastContact",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "long getLastContact(DatanodeInfo node)\n{\r\n    return (now() - node.getLastUpdate()) / 1000;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\metrics",
  "methodName" : "getNNRole",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String getNNRole()\n{\r\n    return NamenodeRole.NAMENODE.toString();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\metrics",
  "methodName" : "getState",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String getState()\n{\r\n    return HAServiceState.ACTIVE.toString();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\metrics",
  "methodName" : "getHostAndPort",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String getHostAndPort()\n{\r\n    return NetUtils.getHostPortString(router.getRpcServerAddress());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\metrics",
  "methodName" : "isSecurityEnabled",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 2,
  "sourceCodeText" : "boolean isSecurityEnabled()\n{\r\n    try {\r\n        return getRBFMetrics().isSecurityEnabled();\r\n    } catch (IOException e) {\r\n        LOG.debug(\"Failed to get security status\", e);\r\n    }\r\n    return false;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\metrics",
  "methodName" : "getLastHATransitionTime",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getLastHATransitionTime()\n{\r\n    return 0;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\metrics",
  "methodName" : "getBytesWithFutureGenerationStamps",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getBytesWithFutureGenerationStamps()\n{\r\n    return 0;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\metrics",
  "methodName" : "getSlowPeersReport",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getSlowPeersReport()\n{\r\n    return \"N/A\";\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\metrics",
  "methodName" : "getSlowDisksReport",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getSlowDisksReport()\n{\r\n    return \"N/A\";\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\metrics",
  "methodName" : "getNumberOfSnapshottableDirs",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getNumberOfSnapshottableDirs()\n{\r\n    return 0;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\metrics",
  "methodName" : "getEnteringMaintenanceNodes",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getEnteringMaintenanceNodes()\n{\r\n    return \"{}\";\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\metrics",
  "methodName" : "getNameDirSize",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getNameDirSize()\n{\r\n    return \"N/A\";\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\metrics",
  "methodName" : "getNumEncryptionZones",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int getNumEncryptionZones()\n{\r\n    return 0;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\metrics",
  "methodName" : "getVerifyECWithTopologyResult",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getVerifyECWithTopologyResult()\n{\r\n    return null;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\metrics",
  "methodName" : "getCurrentTokensCount",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getCurrentTokensCount()\n{\r\n    return 0;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\metrics",
  "methodName" : "getPendingSPSPaths",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 2,
  "sourceCodeText" : "int getPendingSPSPaths()\n{\r\n    try {\r\n        return getRBFMetrics().getPendingSPSPaths();\r\n    } catch (IOException e) {\r\n        LOG.debug(\"Failed to get number of paths to be processed by sps\", e);\r\n    }\r\n    return 0;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\metrics",
  "methodName" : "getRouter",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Router getRouter() throws IOException\n{\r\n    if (this.router == null) {\r\n        throw new IOException(\"Router is not initialized\");\r\n    }\r\n    return this.router;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\protocol\\impl\\pb",
  "methodName" : "getProto",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "GetNamenodeRegistrationsRequestProto getProto()\n{\r\n    return this.translator.build();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\protocol\\impl\\pb",
  "methodName" : "setProto",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setProto(Message proto)\n{\r\n    this.translator.setProto(proto);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\protocol\\impl\\pb",
  "methodName" : "readInstance",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void readInstance(String base64String) throws IOException\n{\r\n    this.translator.readInstance(base64String);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\protocol\\impl\\pb",
  "methodName" : "getPartialMembership",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "MembershipState getPartialMembership()\n{\r\n    GetNamenodeRegistrationsRequestProtoOrBuilder proto = this.translator.getProtoOrBuilder();\r\n    if (!proto.hasMembership()) {\r\n        return null;\r\n    }\r\n    NamenodeMembershipRecordProto memberProto = proto.getMembership();\r\n    return new MembershipStatePBImpl(memberProto);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\protocol\\impl\\pb",
  "methodName" : "setPartialMembership",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setPartialMembership(MembershipState member)\n{\r\n    MembershipStatePBImpl memberPB = (MembershipStatePBImpl) member;\r\n    this.translator.getBuilder().setMembership(memberPB.getProto());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\protocol",
  "methodName" : "newInstance",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "UpdateMountTableEntryResponse newInstance() throws IOException\n{\r\n    return StateStoreSerializer.newRecord(UpdateMountTableEntryResponse.class);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\protocol",
  "methodName" : "getStatus",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean getStatus()",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\protocol",
  "methodName" : "setStatus",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setStatus(boolean result)",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\impl",
  "methodName" : "checkMountTableEntryPermission",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void checkMountTableEntryPermission(String src, FsAction action) throws IOException\n{\r\n    final MountTable partial = MountTable.newInstance();\r\n    partial.setSourcePath(src);\r\n    final Query<MountTable> query = new Query<>(partial);\r\n    final MountTable entry = getDriver().get(getRecordClass(), query);\r\n    if (entry != null) {\r\n        RouterPermissionChecker pc = RouterAdminServer.getPermissionChecker();\r\n        if (pc != null) {\r\n            pc.checkPermission(entry, action);\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\impl",
  "methodName" : "checkMountTablePermission",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void checkMountTablePermission(final String src) throws IOException\n{\r\n    String parent = src.substring(0, src.lastIndexOf(Path.SEPARATOR));\r\n    checkMountTableEntryPermission(parent, FsAction.WRITE);\r\n    while (!parent.isEmpty()) {\r\n        parent = parent.substring(0, parent.lastIndexOf(Path.SEPARATOR));\r\n        checkMountTableEntryPermission(parent, FsAction.EXECUTE);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\impl",
  "methodName" : "addMountTableEntry",
  "errType" : null,
  "containingMethodsNum" : 10,
  "sourceCodeText" : "AddMountTableEntryResponse addMountTableEntry(AddMountTableEntryRequest request) throws IOException\n{\r\n    MountTable mountTable = request.getEntry();\r\n    if (mountTable != null) {\r\n        mountTable.validate();\r\n        final String src = mountTable.getSourcePath();\r\n        checkMountTablePermission(src);\r\n        boolean status = getDriver().put(mountTable, false, true);\r\n        AddMountTableEntryResponse response = AddMountTableEntryResponse.newInstance();\r\n        response.setStatus(status);\r\n        updateCacheAllRouters();\r\n        return response;\r\n    } else {\r\n        AddMountTableEntryResponse response = AddMountTableEntryResponse.newInstance();\r\n        response.setStatus(false);\r\n        return response;\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\impl",
  "methodName" : "updateMountTableEntry",
  "errType" : null,
  "containingMethodsNum" : 10,
  "sourceCodeText" : "UpdateMountTableEntryResponse updateMountTableEntry(UpdateMountTableEntryRequest request) throws IOException\n{\r\n    MountTable mountTable = request.getEntry();\r\n    if (mountTable != null) {\r\n        mountTable.validate();\r\n        final String srcPath = mountTable.getSourcePath();\r\n        checkMountTableEntryPermission(srcPath, FsAction.WRITE);\r\n        boolean status = getDriver().put(mountTable, true, true);\r\n        UpdateMountTableEntryResponse response = UpdateMountTableEntryResponse.newInstance();\r\n        response.setStatus(status);\r\n        updateCacheAllRouters();\r\n        return response;\r\n    } else {\r\n        UpdateMountTableEntryResponse response = UpdateMountTableEntryResponse.newInstance();\r\n        response.setStatus(false);\r\n        return response;\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\impl",
  "methodName" : "removeMountTableEntry",
  "errType" : null,
  "containingMethodsNum" : 10,
  "sourceCodeText" : "RemoveMountTableEntryResponse removeMountTableEntry(RemoveMountTableEntryRequest request) throws IOException\n{\r\n    final String srcPath = request.getSrcPath();\r\n    final MountTable partial = MountTable.newInstance();\r\n    partial.setSourcePath(srcPath);\r\n    final Query<MountTable> query = new Query<>(partial);\r\n    final MountTable deleteEntry = getDriver().get(getRecordClass(), query);\r\n    boolean status = false;\r\n    if (deleteEntry != null) {\r\n        RouterPermissionChecker pc = RouterAdminServer.getPermissionChecker();\r\n        if (pc != null) {\r\n            pc.checkPermission(deleteEntry, FsAction.WRITE);\r\n        }\r\n        status = getDriver().remove(deleteEntry);\r\n    }\r\n    RemoveMountTableEntryResponse response = RemoveMountTableEntryResponse.newInstance();\r\n    response.setStatus(status);\r\n    updateCacheAllRouters();\r\n    return response;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\impl",
  "methodName" : "getMountTableEntries",
  "errType" : [ "AccessControlException" ],
  "containingMethodsNum" : 22,
  "sourceCodeText" : "GetMountTableEntriesResponse getMountTableEntries(GetMountTableEntriesRequest request) throws IOException\n{\r\n    RouterPermissionChecker pc = RouterAdminServer.getPermissionChecker();\r\n    List<MountTable> records = getCachedRecords();\r\n    Collections.sort(records, MountTable.SOURCE_COMPARATOR);\r\n    String reqSrcPath = request.getSrcPath();\r\n    if (reqSrcPath != null && !reqSrcPath.isEmpty()) {\r\n        Iterator<MountTable> it = records.iterator();\r\n        while (it.hasNext()) {\r\n            MountTable record = it.next();\r\n            String srcPath = record.getSourcePath();\r\n            if (!isParentEntry(srcPath, reqSrcPath)) {\r\n                it.remove();\r\n            } else if (pc != null) {\r\n                try {\r\n                    pc.checkPermission(record, FsAction.READ);\r\n                } catch (AccessControlException ignored) {\r\n                    it.remove();\r\n                }\r\n            }\r\n            if (this.getQuotaManager() != null) {\r\n                RouterQuotaUsage quota = this.getQuotaManager().getQuotaUsage(record.getSourcePath());\r\n                if (quota != null) {\r\n                    RouterQuotaUsage oldquota = record.getQuota();\r\n                    RouterQuotaUsage.Builder builder = new RouterQuotaUsage.Builder().fileAndDirectoryCount(quota.getFileAndDirectoryCount()).quota(oldquota.getQuota()).spaceConsumed(quota.getSpaceConsumed()).spaceQuota(oldquota.getSpaceQuota());\r\n                    eachByStorageType(t -> {\r\n                        builder.typeQuota(t, oldquota.getTypeQuota(t));\r\n                        builder.typeConsumed(t, quota.getTypeConsumed(t));\r\n                    });\r\n                    record.setQuota(builder.build());\r\n                }\r\n            }\r\n        }\r\n    }\r\n    GetMountTableEntriesResponse response = GetMountTableEntriesResponse.newInstance();\r\n    response.setEntries(records);\r\n    response.setTimestamp(Time.now());\r\n    return response;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\impl",
  "methodName" : "refreshMountTableEntries",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "RefreshMountTableEntriesResponse refreshMountTableEntries(RefreshMountTableEntriesRequest request) throws IOException\n{\r\n    boolean result = loadCache(true);\r\n    RefreshMountTableEntriesResponse response = RefreshMountTableEntriesResponse.newInstance();\r\n    response.setResult(result);\r\n    return response;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\impl",
  "methodName" : "getDestination",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "GetDestinationResponse getDestination(GetDestinationRequest request) throws IOException\n{\r\n    throw new UnsupportedOperationException(\"Requires the RouterRpcServer\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "serviceInit",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void serviceInit(Configuration conf) throws Exception\n{\r\n    super.serviceInit(conf);\r\n    this.mountTableStore = getMountTableStore();\r\n    this.mountTableStore.setRefreshService(this);\r\n    this.localAdminAddress = StateStoreUtils.getHostPortString(router.getAdminServerAddress());\r\n    this.cacheUpdateTimeout = conf.getTimeDuration(RBFConfigKeys.MOUNT_TABLE_CACHE_UPDATE_TIMEOUT, RBFConfigKeys.MOUNT_TABLE_CACHE_UPDATE_TIMEOUT_DEFAULT, TimeUnit.MILLISECONDS);\r\n    long routerClientMaxLiveTime = conf.getTimeDuration(RBFConfigKeys.MOUNT_TABLE_CACHE_UPDATE_CLIENT_MAX_TIME, RBFConfigKeys.MOUNT_TABLE_CACHE_UPDATE_CLIENT_MAX_TIME_DEFAULT, TimeUnit.MILLISECONDS);\r\n    routerClientsCache = CacheBuilder.newBuilder().expireAfterWrite(routerClientMaxLiveTime, TimeUnit.MILLISECONDS).removalListener(getClientRemover()).build(getClientCreator());\r\n    initClientCacheCleaner(routerClientMaxLiveTime);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "initClientCacheCleaner",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void initClientCacheCleaner(long routerClientMaxLiveTime)\n{\r\n    clientCacheCleanerScheduler = Executors.newSingleThreadScheduledExecutor(new ThreadFactoryBuilder().setNameFormat(\"MountTableRefresh_ClientsCacheCleaner\").setDaemon(true).build());\r\n    clientCacheCleanerScheduler.scheduleWithFixedDelay(() -> routerClientsCache.cleanUp(), routerClientMaxLiveTime, routerClientMaxLiveTime, TimeUnit.MILLISECONDS);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "getClientRemover",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "RemovalListener<String, RouterClient> getClientRemover()\n{\r\n    return new RemovalListener<String, RouterClient>() {\r\n\r\n        @Override\r\n        public void onRemoval(RemovalNotification<String, RouterClient> notification) {\r\n            closeRouterClient(notification.getValue());\r\n        }\r\n    };\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "closeRouterClient",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void closeRouterClient(RouterClient client)\n{\r\n    try {\r\n        client.close();\r\n    } catch (IOException e) {\r\n        LOG.error(\"Error while closing RouterClient\", e);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "getClientCreator",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "CacheLoader<String, RouterClient> getClientCreator()\n{\r\n    return new CacheLoader<String, RouterClient>() {\r\n\r\n        public RouterClient load(String adminAddress) throws IOException {\r\n            InetSocketAddress routerSocket = NetUtils.createSocketAddr(adminAddress);\r\n            Configuration config = getConfig();\r\n            return createRouterClient(routerSocket, config);\r\n        }\r\n    };\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "createRouterClient",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "RouterClient createRouterClient(InetSocketAddress routerSocket, Configuration config) throws IOException\n{\r\n    return SecurityUtil.doAsLoginUser(() -> {\r\n        if (UserGroupInformation.isSecurityEnabled()) {\r\n            UserGroupInformation.getLoginUser().checkTGTAndReloginFromKeytab();\r\n        }\r\n        return new RouterClient(routerSocket, config);\r\n    });\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "serviceStart",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void serviceStart() throws Exception\n{\r\n    super.serviceStart();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "serviceStop",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void serviceStop() throws Exception\n{\r\n    super.serviceStop();\r\n    clientCacheCleanerScheduler.shutdown();\r\n    routerClientsCache.invalidateAll();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "getMountTableStore",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "MountTableStore getMountTableStore() throws IOException\n{\r\n    MountTableStore mountTblStore = router.getStateStore().getRegisteredRecordStore(MountTableStore.class);\r\n    if (mountTblStore == null) {\r\n        throw new IOException(\"Mount table state store is not available.\");\r\n    }\r\n    return mountTblStore;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "refresh",
  "errType" : [ "IOException", "ExecutionException" ],
  "containingMethodsNum" : 16,
  "sourceCodeText" : "void refresh() throws StateStoreUnavailableException\n{\r\n    RouterStore routerStore = router.getRouterStateManager();\r\n    try {\r\n        routerStore.loadCache(true);\r\n    } catch (IOException e) {\r\n        LOG.warn(\"RouterStore load cache failed,\", e);\r\n    }\r\n    List<RouterState> cachedRecords = routerStore.getCachedRecords();\r\n    List<MountTableRefresherThread> refreshThreads = new ArrayList<>();\r\n    for (RouterState routerState : cachedRecords) {\r\n        String adminAddress = routerState.getAdminAddress();\r\n        if (adminAddress == null || adminAddress.length() == 0) {\r\n            continue;\r\n        }\r\n        if (routerState.getStatus() != RouterServiceState.RUNNING) {\r\n            LOG.info(\"Router {} is not running. Mount table cache will not refresh.\", routerState.getAddress());\r\n            removeFromCache(adminAddress);\r\n        } else if (isLocalAdmin(adminAddress)) {\r\n            refreshThreads.add(getLocalRefresher(adminAddress));\r\n        } else {\r\n            try {\r\n                RouterClient client = routerClientsCache.get(adminAddress);\r\n                refreshThreads.add(new MountTableRefresherThread(client.getMountTableManager(), adminAddress));\r\n            } catch (ExecutionException execExcep) {\r\n                LOG.warn(ROUTER_CONNECT_ERROR_MSG, adminAddress, execExcep);\r\n            }\r\n        }\r\n    }\r\n    if (!refreshThreads.isEmpty()) {\r\n        invokeRefresh(refreshThreads);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 2,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "getLocalRefresher",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "MountTableRefresherThread getLocalRefresher(String adminAddress)\n{\r\n    return new MountTableRefresherThread(router.getAdminServer(), adminAddress);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "removeFromCache",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void removeFromCache(String adminAddress)\n{\r\n    routerClientsCache.invalidate(adminAddress);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "invokeRefresh",
  "errType" : [ "InterruptedException" ],
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void invokeRefresh(List<MountTableRefresherThread> refreshThreads)\n{\r\n    CountDownLatch countDownLatch = new CountDownLatch(refreshThreads.size());\r\n    for (MountTableRefresherThread refThread : refreshThreads) {\r\n        refThread.setCountDownLatch(countDownLatch);\r\n        refThread.start();\r\n    }\r\n    try {\r\n        boolean allReqCompleted = countDownLatch.await(cacheUpdateTimeout, TimeUnit.MILLISECONDS);\r\n        if (!allReqCompleted) {\r\n            LOG.warn(\"Not all router admins updated their cache\");\r\n        }\r\n    } catch (InterruptedException e) {\r\n        LOG.error(\"Mount table cache refresher was interrupted.\", e);\r\n    }\r\n    logResult(refreshThreads);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "isLocalAdmin",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "boolean isLocalAdmin(String adminAddress)\n{\r\n    return adminAddress.contentEquals(localAdminAddress);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "logResult",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void logResult(List<MountTableRefresherThread> refreshThreads)\n{\r\n    int successCount = 0;\r\n    int failureCount = 0;\r\n    for (MountTableRefresherThread mountTableRefreshThread : refreshThreads) {\r\n        if (mountTableRefreshThread.isSuccess()) {\r\n            successCount++;\r\n        } else {\r\n            failureCount++;\r\n            removeFromCache(mountTableRefreshThread.getAdminAddress());\r\n        }\r\n    }\r\n    LOG.info(\"Mount table entries cache refresh successCount={},failureCount={}\", successCount, failureCount);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\protocol\\impl\\pb",
  "methodName" : "getProto",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "GetRouterRegistrationsResponseProto getProto()\n{\r\n    return this.translator.build();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\protocol\\impl\\pb",
  "methodName" : "setProto",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setProto(Message proto)\n{\r\n    this.translator.setProto(proto);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\protocol\\impl\\pb",
  "methodName" : "readInstance",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void readInstance(String base64String) throws IOException\n{\r\n    this.translator.readInstance(base64String);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\protocol\\impl\\pb",
  "methodName" : "getRouters",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "List<RouterState> getRouters() throws IOException\n{\r\n    List<RouterState> ret = new ArrayList<RouterState>();\r\n    List<RouterRecordProto> memberships = this.translator.getProtoOrBuilder().getRoutersList();\r\n    for (RouterRecordProto memberProto : memberships) {\r\n        RouterState membership = new RouterStatePBImpl(memberProto);\r\n        ret.add(membership);\r\n    }\r\n    return ret;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\protocol\\impl\\pb",
  "methodName" : "setRouters",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void setRouters(List<RouterState> records) throws IOException\n{\r\n    this.translator.getBuilder().clearRouters();\r\n    for (RouterState router : records) {\r\n        if (router instanceof RouterStatePBImpl) {\r\n            RouterStatePBImpl routerPB = (RouterStatePBImpl) router;\r\n            this.translator.getBuilder().addRouters(routerPB.getProto());\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\protocol\\impl\\pb",
  "methodName" : "getTimestamp",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "long getTimestamp()\n{\r\n    return this.translator.getProtoOrBuilder().getTimestamp();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\protocol\\impl\\pb",
  "methodName" : "setTimestamp",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setTimestamp(long time)\n{\r\n    this.translator.getBuilder().setTimestamp(time);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\resolver\\order",
  "methodName" : "getFirstNamespace",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "String getFirstNamespace(final String path, final PathLocation loc)\n{\r\n    String finalPath = extractTempFileName(path);\r\n    Set<String> namespaces = loc.getNamespaces();\r\n    ConsistentHashRing locator = getHashResolver(namespaces);\r\n    String hashedSubcluster = locator.getLocation(finalPath);\r\n    if (hashedSubcluster == null) {\r\n        String srcPath = loc.getSourcePath();\r\n        LOG.error(\"Cannot find subcluster for {} ({} -> {})\", srcPath, path, finalPath);\r\n    }\r\n    LOG.debug(\"Namespace for {} ({}) is {}\", path, finalPath, hashedSubcluster);\r\n    return hashedSubcluster;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\resolver\\order",
  "methodName" : "getHashResolver",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "ConsistentHashRing getHashResolver(final Set<String> namespaces)\n{\r\n    final int hash = namespaces.hashCode();\r\n    return this.hashResolverMap.computeIfAbsent(hash, k -> new ConsistentHashRing(namespaces));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\resolver\\order",
  "methodName" : "extractTempFileName",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "String extractTempFileName(final String input)\n{\r\n    StringBuilder sb = new StringBuilder();\r\n    Matcher matcher = TEMP_FILE_PATTERN.matcher(input);\r\n    if (matcher.find()) {\r\n        for (int i = 1; i <= matcher.groupCount(); i++) {\r\n            String match = matcher.group(i);\r\n            if (match != null) {\r\n                sb.append(match);\r\n            }\r\n        }\r\n    }\r\n    if (sb.length() > 0) {\r\n        String ret = sb.toString();\r\n        LOG.debug(\"Extracted {} from {}\", ret, input);\r\n        return ret;\r\n    }\r\n    return input;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\resolver",
  "methodName" : "prioritizeDestination",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "PathLocation prioritizeDestination(PathLocation base, String firstNsId)\n{\r\n    List<RemoteLocation> prioritizedDestinations = orderedNamespaces(base.destinations, firstNsId);\r\n    return new PathLocation(base.sourcePath, prioritizedDestinations, base.destOrder);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\resolver",
  "methodName" : "orderedNamespaces",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "List<RemoteLocation> orderedNamespaces(final List<RemoteLocation> original, final String nsId)\n{\r\n    if (original.size() <= 1) {\r\n        return original;\r\n    }\r\n    LinkedList<RemoteLocation> newDestinations = new LinkedList<>();\r\n    boolean found = false;\r\n    for (RemoteLocation dest : original) {\r\n        if (dest.getNameserviceId().equals(nsId)) {\r\n            found = true;\r\n            newDestinations.addFirst(dest);\r\n        } else {\r\n            newDestinations.add(dest);\r\n        }\r\n    }\r\n    if (!found) {\r\n        LOG.debug(\"Cannot find location with namespace {} in {}\", nsId, original);\r\n    }\r\n    return Collections.unmodifiableList(newDestinations);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\resolver",
  "methodName" : "getSourcePath",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getSourcePath()\n{\r\n    return this.sourcePath;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\resolver",
  "methodName" : "getNamespaces",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "Set<String> getNamespaces()\n{\r\n    Set<String> namespaces = new HashSet<>();\r\n    List<RemoteLocation> locations = this.getDestinations();\r\n    for (RemoteLocation location : locations) {\r\n        String nsId = location.getNameserviceId();\r\n        namespaces.add(nsId);\r\n    }\r\n    return namespaces;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\resolver",
  "methodName" : "toString",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "String toString()\n{\r\n    StringBuilder sb = new StringBuilder();\r\n    for (RemoteLocation destination : this.destinations) {\r\n        String nsId = destination.getNameserviceId();\r\n        String path = destination.getDest();\r\n        if (sb.length() > 0) {\r\n            sb.append(\",\");\r\n        }\r\n        sb.append(nsId + \"->\" + path);\r\n    }\r\n    if (this.destinations.size() > 1) {\r\n        sb.append(\" [\").append(this.destOrder.toString()).append(\"]\");\r\n    }\r\n    return sb.toString();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\resolver",
  "methodName" : "hasMultipleDestinations",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "boolean hasMultipleDestinations()\n{\r\n    return this.destinations.size() > 1;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\resolver",
  "methodName" : "getDestinations",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "List<RemoteLocation> getDestinations()\n{\r\n    return Collections.unmodifiableList(this.destinations);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\resolver",
  "methodName" : "getDestinationOrder",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "DestinationOrder getDestinationOrder()\n{\r\n    return this.destOrder;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\resolver",
  "methodName" : "getDefaultLocation",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "RemoteLocation getDefaultLocation()\n{\r\n    if (destinations.isEmpty() || destinations.get(0).getDest() == null) {\r\n        throw new UnsupportedOperationException(\"Unsupported path \" + sourcePath + \" please check mount table\");\r\n    }\r\n    return destinations.get(0);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "getLocation",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "T getLocation()\n{\r\n    return loc;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "hasResult",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean hasResult()\n{\r\n    return resultSet;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "getResult",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "R getResult()\n{\r\n    return result;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "hasException",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "boolean hasException()\n{\r\n    return getException() != null;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "getException",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "IOException getException()\n{\r\n    return ioe;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "toString",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "String toString()\n{\r\n    StringBuilder sb = new StringBuilder().append(\"loc=\").append(getLocation());\r\n    if (hasResult()) {\r\n        sb.append(\" result=\").append(getResult());\r\n    }\r\n    if (hasException()) {\r\n        sb.append(\" exception=\").append(getException());\r\n    }\r\n    return sb.toString();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\protocol",
  "methodName" : "newInstance",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "GetRouterRegistrationResponse newInstance() throws IOException\n{\r\n    return StateStoreSerializer.newRecord(GetRouterRegistrationResponse.class);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\protocol",
  "methodName" : "getRouter",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "RouterState getRouter() throws IOException",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\protocol",
  "methodName" : "setRouter",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setRouter(RouterState router) throws IOException",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\records",
  "methodName" : "getRecords",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "List<T> getRecords()\n{\r\n    return this.records;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\records",
  "methodName" : "getTimestamp",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getTimestamp()\n{\r\n    return this.timestamp;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store",
  "methodName" : "setRefreshService",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setRefreshService(MountTableRefresherService refreshService)\n{\r\n    this.refreshService = refreshService;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store",
  "methodName" : "setQuotaManager",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setQuotaManager(RouterQuotaManager quotaManager)\n{\r\n    this.quotaManager = quotaManager;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store",
  "methodName" : "getQuotaManager",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "RouterQuotaManager getQuotaManager()\n{\r\n    return quotaManager;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store",
  "methodName" : "updateCacheAllRouters",
  "errType" : [ "StateStoreUnavailableException" ],
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void updateCacheAllRouters()\n{\r\n    if (refreshService != null) {\r\n        try {\r\n            refreshService.refresh();\r\n        } catch (StateStoreUnavailableException e) {\r\n            LOG.error(\"Cannot refresh mount table: state store not available\", e);\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\resolver\\order",
  "methodName" : "getFirstNamespace",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "String getFirstNamespace(String path, PathLocation loc)\n{\r\n    updateSubclusterMapping();\r\n    return chooseFirstNamespace(path, loc);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\resolver\\order",
  "methodName" : "getSubclusterInfo",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Map<K, V> getSubclusterInfo(MembershipStore membershipStore)",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\resolver\\order",
  "methodName" : "chooseFirstNamespace",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String chooseFirstNamespace(String path, PathLocation loc)",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\resolver\\order",
  "methodName" : "updateSubclusterMapping",
  "errType" : [ "InterruptedException" ],
  "containingMethodsNum" : 9,
  "sourceCodeText" : "void updateSubclusterMapping()\n{\r\n    if (subclusterMapping == null || (monotonicNow() - lastUpdated) > minUpdateTime) {\r\n        Thread updater = new Thread(new Runnable() {\r\n\r\n            @Override\r\n            public void run() {\r\n                final MembershipStore membershipStore = getMembershipStore();\r\n                if (membershipStore == null) {\r\n                    LOG.error(\"Cannot access the Membership store.\");\r\n                    return;\r\n                }\r\n                subclusterMapping = getSubclusterInfo(membershipStore);\r\n                lastUpdated = monotonicNow();\r\n            }\r\n        });\r\n        updater.start();\r\n        if (subclusterMapping == null) {\r\n            try {\r\n                LOG.debug(\"Wait to get the mapping for the first time\");\r\n                updater.join();\r\n            } catch (InterruptedException e) {\r\n                LOG.error(\"Cannot wait for the updater to finish\");\r\n            }\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\resolver\\order",
  "methodName" : "getRpcServer",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "RouterRpcServer getRpcServer()\n{\r\n    if (this.router == null) {\r\n        return null;\r\n    }\r\n    return router.getRpcServer();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\resolver\\order",
  "methodName" : "getMembershipStore",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "MembershipStore getMembershipStore()\n{\r\n    StateStoreService stateStore = router.getStateStore();\r\n    if (stateStore == null) {\r\n        return null;\r\n    }\r\n    return stateStore.getRegisteredRecordStore(MembershipStore.class);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\resolver\\order",
  "methodName" : "getSubclusterMapping",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Map<K, V> getSubclusterMapping()\n{\r\n    return this.subclusterMapping;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\protocol\\impl\\pb",
  "methodName" : "getProto",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "UpdateMountTableEntryResponseProto getProto()\n{\r\n    return this.translator.build();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\protocol\\impl\\pb",
  "methodName" : "setProto",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setProto(Message proto)\n{\r\n    this.translator.setProto(proto);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\protocol\\impl\\pb",
  "methodName" : "readInstance",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void readInstance(String base64String) throws IOException\n{\r\n    this.translator.readInstance(base64String);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\protocol\\impl\\pb",
  "methodName" : "getStatus",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "boolean getStatus()\n{\r\n    return this.translator.getProtoOrBuilder().getStatus();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\protocol\\impl\\pb",
  "methodName" : "setStatus",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setStatus(boolean result)\n{\r\n    this.translator.getBuilder().setStatus(result);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\driver\\impl",
  "methodName" : "get",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "T get(Class<T> clazz, Query<T> query) throws IOException\n{\r\n    List<T> records = getMultiple(clazz, query);\r\n    if (records.size() > 1) {\r\n        throw new IOException(\"Found more than one object in collection\");\r\n    } else if (records.size() == 1) {\r\n        return records.get(0);\r\n    } else {\r\n        return null;\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\driver\\impl",
  "methodName" : "getMultiple",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "List<T> getMultiple(Class<T> clazz, Query<T> query) throws IOException\n{\r\n    QueryResult<T> result = get(clazz);\r\n    List<T> records = result.getRecords();\r\n    List<T> ret = filterMultiple(query, records);\r\n    if (ret == null) {\r\n        throw new IOException(\"Cannot fetch records from the store\");\r\n    }\r\n    return ret;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\driver\\impl",
  "methodName" : "put",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "boolean put(T record, boolean allowUpdate, boolean errorIfExists) throws IOException\n{\r\n    List<T> singletonList = new ArrayList<>();\r\n    singletonList.add(record);\r\n    return putAll(singletonList, allowUpdate, errorIfExists);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\driver\\impl",
  "methodName" : "remove",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "boolean remove(T record) throws IOException\n{\r\n    final Query<T> query = new Query<T>(record);\r\n    Class<? extends BaseRecord> clazz = record.getClass();\r\n    @SuppressWarnings(\"unchecked\")\r\n    Class<T> recordClass = (Class<T>) StateStoreUtils.getRecordClass(clazz);\r\n    return remove(recordClass, query) == 1;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\protocol\\impl\\pb",
  "methodName" : "getProto",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "RefreshSuperUserGroupsConfigurationRequestProto getProto()\n{\r\n    this.translator.getBuilder();\r\n    return this.translator.build();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\protocol\\impl\\pb",
  "methodName" : "setProto",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setProto(Message proto)\n{\r\n    this.translator.setProto(proto);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\protocol\\impl\\pb",
  "methodName" : "readInstance",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void readInstance(String base64String) throws IOException\n{\r\n    this.translator.readInstance(base64String);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store",
  "methodName" : "checkCacheAvailable",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void checkCacheAvailable() throws StateStoreUnavailableException\n{\r\n    if (!this.initialized) {\r\n        throw new StateStoreUnavailableException(\"Cached State Store not initialized, \" + getRecordClass().getSimpleName() + \" records not valid\");\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store",
  "methodName" : "loadCache",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 14,
  "sourceCodeText" : "boolean loadCache(boolean force) throws IOException\n{\r\n    if (force || isUpdateTime()) {\r\n        List<R> newRecords = null;\r\n        long t = -1;\r\n        try {\r\n            QueryResult<R> result = getDriver().get(getRecordClass());\r\n            newRecords = result.getRecords();\r\n            t = result.getTimestamp();\r\n            if (this.override) {\r\n                overrideExpiredRecords(result);\r\n            }\r\n        } catch (IOException e) {\r\n            LOG.error(\"Cannot get \\\"{}\\\" records from the State Store\", getRecordClass().getSimpleName());\r\n            this.initialized = false;\r\n            return false;\r\n        }\r\n        writeLock.lock();\r\n        try {\r\n            this.records.clear();\r\n            this.records.addAll(newRecords);\r\n            this.timestamp = t;\r\n            this.initialized = true;\r\n        } finally {\r\n            writeLock.unlock();\r\n        }\r\n        StateStoreMetrics metrics = getDriver().getMetrics();\r\n        if (metrics != null) {\r\n            String recordName = getRecordClass().getSimpleName();\r\n            metrics.setCacheSize(recordName, this.records.size());\r\n        }\r\n        lastUpdate = Time.monotonicNow();\r\n    }\r\n    return true;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 2,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store",
  "methodName" : "isUpdateTime",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "boolean isUpdateTime()\n{\r\n    return Time.monotonicNow() - lastUpdate > MIN_UPDATE_MS;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store",
  "methodName" : "overrideExpiredRecords",
  "errType" : null,
  "containingMethodsNum" : 17,
  "sourceCodeText" : "void overrideExpiredRecords(QueryResult<R> query) throws IOException\n{\r\n    List<R> commitRecords = new ArrayList<>();\r\n    List<R> deleteRecords = new ArrayList<>();\r\n    List<R> newRecords = query.getRecords();\r\n    long currentDriverTime = query.getTimestamp();\r\n    if (newRecords == null || currentDriverTime <= 0) {\r\n        LOG.error(\"Cannot check overrides for record\");\r\n        return;\r\n    }\r\n    for (R record : newRecords) {\r\n        if (record.shouldBeDeleted(currentDriverTime)) {\r\n            String recordName = StateStoreUtils.getRecordName(record.getClass());\r\n            if (getDriver().remove(record)) {\r\n                deleteRecords.add(record);\r\n                LOG.info(\"Deleted State Store record {}: {}\", recordName, record);\r\n            } else {\r\n                LOG.warn(\"Couldn't delete State Store record {}: {}\", recordName, record);\r\n            }\r\n        } else if (record.checkExpired(currentDriverTime)) {\r\n            String recordName = StateStoreUtils.getRecordName(record.getClass());\r\n            LOG.info(\"Override State Store record {}: {}\", recordName, record);\r\n            commitRecords.add(record);\r\n        }\r\n    }\r\n    if (commitRecords.size() > 0) {\r\n        getDriver().putAll(commitRecords, true, false);\r\n    }\r\n    if (deleteRecords.size() > 0) {\r\n        newRecords.removeAll(deleteRecords);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store",
  "methodName" : "overrideExpiredRecord",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void overrideExpiredRecord(R record) throws IOException\n{\r\n    List<R> newRecords = new ArrayList<>();\r\n    newRecords.add(record);\r\n    long time = getDriver().getTime();\r\n    QueryResult<R> query = new QueryResult<>(newRecords, time);\r\n    overrideExpiredRecords(query);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store",
  "methodName" : "getCachedRecords",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "List<R> getCachedRecords() throws StateStoreUnavailableException\n{\r\n    checkCacheAvailable();\r\n    List<R> ret = new LinkedList<R>();\r\n    this.readLock.lock();\r\n    try {\r\n        ret.addAll(this.records);\r\n    } finally {\r\n        this.readLock.unlock();\r\n    }\r\n    return ret;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store",
  "methodName" : "getCachedRecordsAndTimeStamp",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "QueryResult<R> getCachedRecordsAndTimeStamp() throws StateStoreUnavailableException\n{\r\n    checkCacheAvailable();\r\n    this.readLock.lock();\r\n    try {\r\n        return new QueryResult<R>(this.records, this.timestamp);\r\n    } finally {\r\n        this.readLock.unlock();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\protocol",
  "methodName" : "newInstance",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "UpdateNamenodeRegistrationRequest newInstance() throws IOException\n{\r\n    return StateStoreSerializer.newRecord(UpdateNamenodeRegistrationRequest.class);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\protocol",
  "methodName" : "newInstance",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "UpdateNamenodeRegistrationRequest newInstance(String nameserviceId, String namenodeId, FederationNamenodeServiceState state) throws IOException\n{\r\n    UpdateNamenodeRegistrationRequest request = newInstance();\r\n    request.setNameserviceId(nameserviceId);\r\n    request.setNamenodeId(namenodeId);\r\n    request.setState(state);\r\n    return request;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\protocol",
  "methodName" : "getNameserviceId",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getNameserviceId()",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\protocol",
  "methodName" : "getNamenodeId",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getNamenodeId()",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\protocol",
  "methodName" : "getState",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "FederationNamenodeServiceState getState()",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\protocol",
  "methodName" : "setNameserviceId",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setNameserviceId(String nsId)",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\protocol",
  "methodName" : "setNamenodeId",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setNamenodeId(String nnId)",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\protocol",
  "methodName" : "setState",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setState(FederationNamenodeServiceState state)",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "main",
  "errType" : [ "Throwable" ],
  "containingMethodsNum" : 9,
  "sourceCodeText" : "void main(String[] argv)\n{\r\n    if (DFSUtil.parseHelpArgument(argv, USAGE, System.out, true)) {\r\n        System.exit(0);\r\n    }\r\n    try {\r\n        StringUtils.startupShutdownMessage(Router.class, argv, LOG);\r\n        Router router = new Router();\r\n        ShutdownHookManager.get().addShutdownHook(new CompositeServiceShutdownHook(router), SHUTDOWN_HOOK_PRIORITY);\r\n        Configuration conf = getConfiguration();\r\n        router.init(conf);\r\n        router.start();\r\n    } catch (Throwable e) {\r\n        LOG.error(\"Failed to start router\", e);\r\n        terminate(1, e);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "getConfiguration",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "Configuration getConfiguration()\n{\r\n    Configuration conf = new HdfsConfiguration();\r\n    conf.addResource(FedBalance.FED_BALANCE_DEFAULT_XML);\r\n    conf.addResource(FedBalance.FED_BALANCE_SITE_XML);\r\n    return conf;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "serviceInit",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void serviceInit(Configuration configuration) throws Exception\n{\r\n    this.routerMetrics = RouterMetrics.create(configuration);\r\n    this.routerClientMetrics = RouterClientMetrics.create(configuration);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "serviceStart",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void serviceStart() throws Exception\n{\r\n    this.nnMetrics = new NamenodeBeanMetrics(this.router);\r\n    this.rbfMetrics = new RBFMetrics(this.router);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "serviceStop",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void serviceStop() throws Exception\n{\r\n    if (this.rbfMetrics != null) {\r\n        this.rbfMetrics.close();\r\n    }\r\n    if (this.nnMetrics != null) {\r\n        this.nnMetrics.close();\r\n    }\r\n    if (this.routerMetrics != null) {\r\n        this.routerMetrics.shutdown();\r\n    }\r\n    if (this.routerClientMetrics != null) {\r\n        this.routerClientMetrics.shutdown();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "getRouterMetrics",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "RouterMetrics getRouterMetrics()\n{\r\n    return this.routerMetrics;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "getRouterClientMetrics",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "RouterClientMetrics getRouterClientMetrics()\n{\r\n    return this.routerClientMetrics;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "getRBFMetrics",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "RBFMetrics getRBFMetrics()\n{\r\n    return this.rbfMetrics;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "getNamenodeMetrics",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "NamenodeBeanMetrics getNamenodeMetrics()\n{\r\n    return this.nnMetrics;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "getJvmMetrics",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "JvmMetrics getJvmMetrics()\n{\r\n    if (this.routerMetrics == null) {\r\n        return null;\r\n    }\r\n    return this.routerMetrics.getJvmMetrics();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "refreshUserToGroupsMappings",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void refreshUserToGroupsMappings() throws IOException\n{\r\n    LOG.debug(\"Refresh user groups mapping in Router.\");\r\n    rpcServer.checkOperation(OperationCategory.UNCHECKED);\r\n    Set<FederationNamespaceInfo> nss = namenodeResolver.getNamespaces();\r\n    if (nss.isEmpty()) {\r\n        Groups.getUserToGroupsMappingService().refresh();\r\n    } else {\r\n        RemoteMethod method = new RemoteMethod(RefreshUserMappingsProtocol.class, \"refreshUserToGroupsMappings\");\r\n        rpcClient.invokeConcurrent(nss, method);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "refreshSuperUserGroupsConfiguration",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void refreshSuperUserGroupsConfiguration() throws IOException\n{\r\n    LOG.debug(\"Refresh superuser groups configuration in Router.\");\r\n    rpcServer.checkOperation(OperationCategory.UNCHECKED);\r\n    Set<FederationNamespaceInfo> nss = namenodeResolver.getNamespaces();\r\n    if (nss.isEmpty()) {\r\n        ProxyUsers.refreshSuperUserGroupsConfiguration();\r\n    } else {\r\n        RemoteMethod method = new RemoteMethod(RefreshUserMappingsProtocol.class, \"refreshSuperUserGroupsConfiguration\");\r\n        rpcClient.invokeConcurrent(nss, method);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "getGroupsForUser",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "String[] getGroupsForUser(String user) throws IOException\n{\r\n    LOG.debug(\"Getting groups for user {}\", user);\r\n    rpcServer.checkOperation(OperationCategory.UNCHECKED);\r\n    Set<FederationNamespaceInfo> nss = namenodeResolver.getNamespaces();\r\n    if (nss.isEmpty()) {\r\n        return UserGroupInformation.createRemoteUser(user).getGroupNames();\r\n    } else {\r\n        RemoteMethod method = new RemoteMethod(GetUserMappingsProtocol.class, \"getGroupsForUser\", new Class<?>[] { String.class }, user);\r\n        Map<FederationNamespaceInfo, String[]> results = rpcClient.invokeConcurrent(nss, method, String[].class);\r\n        return merge(results, String.class);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\rbfbalance",
  "methodName" : "disableWrite",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void disableWrite(FedBalanceContext context) throws IOException\n{\r\n    Configuration conf = context.getConf();\r\n    String mount = context.getMount();\r\n    MountTableProcedure.disableWrite(mount, conf);\r\n    updateStage(Stage.FINAL_DISTCP);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\rbfbalance",
  "methodName" : "enableWrite",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void enableWrite() throws IOException\n{\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\records",
  "methodName" : "newInstance",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "MembershipStats newInstance() throws IOException\n{\r\n    MembershipStats record = StateStoreSerializer.newRecord(MembershipStats.class);\r\n    record.init();\r\n    return record;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\records",
  "methodName" : "setTotalSpace",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setTotalSpace(long space)",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\records",
  "methodName" : "getTotalSpace",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getTotalSpace()",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\records",
  "methodName" : "setAvailableSpace",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setAvailableSpace(long space)",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\records",
  "methodName" : "getAvailableSpace",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getAvailableSpace()",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\records",
  "methodName" : "setProvidedSpace",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setProvidedSpace(long capacity)",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\records",
  "methodName" : "getProvidedSpace",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getProvidedSpace()",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\records",
  "methodName" : "setNumOfFiles",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setNumOfFiles(long files)",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\records",
  "methodName" : "getNumOfFiles",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getNumOfFiles()",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\records",
  "methodName" : "setNumOfBlocks",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setNumOfBlocks(long blocks)",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\records",
  "methodName" : "getNumOfBlocks",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getNumOfBlocks()",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\records",
  "methodName" : "setNumOfBlocksMissing",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setNumOfBlocksMissing(long blocks)",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\records",
  "methodName" : "getNumOfBlocksMissing",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getNumOfBlocksMissing()",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\records",
  "methodName" : "setNumOfBlocksPendingReplication",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setNumOfBlocksPendingReplication(long blocks)",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\records",
  "methodName" : "getNumOfBlocksPendingReplication",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getNumOfBlocksPendingReplication()",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\records",
  "methodName" : "setNumOfBlocksUnderReplicated",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setNumOfBlocksUnderReplicated(long blocks)",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\records",
  "methodName" : "getNumOfBlocksUnderReplicated",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getNumOfBlocksUnderReplicated()",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\records",
  "methodName" : "setNumOfBlocksPendingDeletion",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setNumOfBlocksPendingDeletion(long blocks)",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\records",
  "methodName" : "getNumOfBlocksPendingDeletion",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getNumOfBlocksPendingDeletion()",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\records",
  "methodName" : "setNumOfActiveDatanodes",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setNumOfActiveDatanodes(int nodes)",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\records",
  "methodName" : "getNumOfActiveDatanodes",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int getNumOfActiveDatanodes()",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\records",
  "methodName" : "setNumOfDeadDatanodes",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setNumOfDeadDatanodes(int nodes)",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\records",
  "methodName" : "getNumOfDeadDatanodes",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int getNumOfDeadDatanodes()",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\records",
  "methodName" : "setNumOfStaleDatanodes",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setNumOfStaleDatanodes(int nodes)",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\records",
  "methodName" : "getNumOfStaleDatanodes",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int getNumOfStaleDatanodes()",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\records",
  "methodName" : "setNumOfDecommissioningDatanodes",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setNumOfDecommissioningDatanodes(int nodes)",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\records",
  "methodName" : "getNumOfDecommissioningDatanodes",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int getNumOfDecommissioningDatanodes()",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\records",
  "methodName" : "setNumOfDecomActiveDatanodes",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setNumOfDecomActiveDatanodes(int nodes)",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\records",
  "methodName" : "getNumOfDecomActiveDatanodes",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int getNumOfDecomActiveDatanodes()",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\records",
  "methodName" : "setNumOfDecomDeadDatanodes",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setNumOfDecomDeadDatanodes(int nodes)",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\records",
  "methodName" : "getNumOfDecomDeadDatanodes",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int getNumOfDecomDeadDatanodes()",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\records",
  "methodName" : "setNumOfInMaintenanceLiveDataNodes",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setNumOfInMaintenanceLiveDataNodes(int nodes)",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\records",
  "methodName" : "getNumOfInMaintenanceLiveDataNodes",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int getNumOfInMaintenanceLiveDataNodes()",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\records",
  "methodName" : "setNumOfInMaintenanceDeadDataNodes",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setNumOfInMaintenanceDeadDataNodes(int nodes)",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\records",
  "methodName" : "getNumOfInMaintenanceDeadDataNodes",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int getNumOfInMaintenanceDeadDataNodes()",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\records",
  "methodName" : "setNumOfEnteringMaintenanceDataNodes",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setNumOfEnteringMaintenanceDataNodes(int nodes)",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\records",
  "methodName" : "getNumOfEnteringMaintenanceDataNodes",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int getNumOfEnteringMaintenanceDataNodes()",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\records",
  "methodName" : "setCorruptFilesCount",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setCorruptFilesCount(int num)",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\records",
  "methodName" : "getCorruptFilesCount",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int getCorruptFilesCount()",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\records",
  "methodName" : "setScheduledReplicationBlocks",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setScheduledReplicationBlocks(long blocks)",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\records",
  "methodName" : "getScheduledReplicationBlocks",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getScheduledReplicationBlocks()",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\records",
  "methodName" : "setNumberOfMissingBlocksWithReplicationFactorOne",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setNumberOfMissingBlocksWithReplicationFactorOne(long blocks)",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\records",
  "methodName" : "getNumberOfMissingBlocksWithReplicationFactorOne",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getNumberOfMissingBlocksWithReplicationFactorOne()",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\records",
  "methodName" : "setHighestPriorityLowRedundancyReplicatedBlocks",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setHighestPriorityLowRedundancyReplicatedBlocks(long blocks)",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\records",
  "methodName" : "getHighestPriorityLowRedundancyReplicatedBlocks",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getHighestPriorityLowRedundancyReplicatedBlocks()",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\records",
  "methodName" : "setHighestPriorityLowRedundancyECBlocks",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setHighestPriorityLowRedundancyECBlocks(long blocks)",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\records",
  "methodName" : "getHighestPriorityLowRedundancyECBlocks",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getHighestPriorityLowRedundancyECBlocks()",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\records",
  "methodName" : "setPendingSPSPaths",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setPendingSPSPaths(int pendingSPSPaths)",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\records",
  "methodName" : "getPendingSPSPaths",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int getPendingSPSPaths()",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\records",
  "methodName" : "getPrimaryKeys",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "SortedMap<String, String> getPrimaryKeys()\n{\r\n    SortedMap<String, String> map = new TreeMap<String, String>();\r\n    return map;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\records",
  "methodName" : "getExpirationMs",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getExpirationMs()\n{\r\n    return -1;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\records",
  "methodName" : "setDateModified",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setDateModified(long time)\n{\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\records",
  "methodName" : "getDateModified",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getDateModified()\n{\r\n    return 0;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\records",
  "methodName" : "setDateCreated",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setDateCreated(long time)\n{\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\records",
  "methodName" : "getDateCreated",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getDateCreated()\n{\r\n    return 0;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\resolver",
  "methodName" : "getNameserviceId",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getNameserviceId()\n{\r\n    String ret = this.nameserviceId;\r\n    if (this.namenodeId != null) {\r\n        ret += \"-\" + this.namenodeId;\r\n    }\r\n    return ret;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\resolver",
  "methodName" : "getDest",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getDest()\n{\r\n    return this.dstPath;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\resolver",
  "methodName" : "getSrc",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getSrc()\n{\r\n    return this.srcPath;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\resolver",
  "methodName" : "toString",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String toString()\n{\r\n    return getNameserviceId() + \"->\" + this.dstPath;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\protocol",
  "methodName" : "newInstance",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "LeaveSafeModeRequest newInstance() throws IOException\n{\r\n    return StateStoreSerializer.newRecord(LeaveSafeModeRequest.class);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\protocol\\impl\\pb",
  "methodName" : "getProto",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "DisableNameserviceResponseProto getProto()\n{\r\n    return translator.build();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\protocol\\impl\\pb",
  "methodName" : "setProto",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setProto(Message proto)\n{\r\n    this.translator.setProto(proto);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\protocol\\impl\\pb",
  "methodName" : "readInstance",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void readInstance(String base64String) throws IOException\n{\r\n    this.translator.readInstance(base64String);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\protocol\\impl\\pb",
  "methodName" : "getStatus",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "boolean getStatus()\n{\r\n    return this.translator.getProtoOrBuilder().getStatus();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\protocol\\impl\\pb",
  "methodName" : "setStatus",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setStatus(boolean status)\n{\r\n    this.translator.getBuilder().setStatus(status);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\resolver\\order",
  "methodName" : "getFirstNamespace",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "String getFirstNamespace(final String path, final PathLocation loc)\n{\r\n    final Set<String> namespaces = (loc == null) ? null : loc.getNamespaces();\r\n    if (CollectionUtils.isEmpty(namespaces)) {\r\n        LOG.error(\"Cannot get namespaces for {}\", loc);\r\n        return null;\r\n    }\r\n    final int index = ThreadLocalRandom.current().nextInt(namespaces.size());\r\n    return Iterables.get(namespaces, index);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\protocol",
  "methodName" : "newInstance",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "GetNamespaceInfoResponse newInstance()\n{\r\n    return StateStoreSerializer.newRecord(GetNamespaceInfoResponse.class);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\protocol",
  "methodName" : "newInstance",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "GetNamespaceInfoResponse newInstance(Set<FederationNamespaceInfo> namespaces) throws IOException\n{\r\n    GetNamespaceInfoResponse response = newInstance();\r\n    response.setNamespaceInfo(namespaces);\r\n    return response;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\protocol",
  "methodName" : "getNamespaceInfo",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Set<FederationNamespaceInfo> getNamespaceInfo()",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\protocol",
  "methodName" : "setNamespaceInfo",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setNamespaceInfo(Set<FederationNamespaceInfo> namespaceInfo)",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "getErasureCodingPolicies",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "ErasureCodingPolicyInfo[] getErasureCodingPolicies() throws IOException\n{\r\n    rpcServer.checkOperation(OperationCategory.READ);\r\n    RemoteMethod method = new RemoteMethod(\"getErasureCodingPolicies\");\r\n    Set<FederationNamespaceInfo> nss = namenodeResolver.getNamespaces();\r\n    Map<FederationNamespaceInfo, ErasureCodingPolicyInfo[]> ret = rpcClient.invokeConcurrent(nss, method, true, false, ErasureCodingPolicyInfo[].class);\r\n    return merge(ret, ErasureCodingPolicyInfo.class);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "getErasureCodingCodecs",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "Map<String, String> getErasureCodingCodecs() throws IOException\n{\r\n    rpcServer.checkOperation(OperationCategory.READ);\r\n    RemoteMethod method = new RemoteMethod(\"getErasureCodingCodecs\");\r\n    Set<FederationNamespaceInfo> nss = namenodeResolver.getNamespaces();\r\n    @SuppressWarnings(\"rawtypes\")\r\n    Map<FederationNamespaceInfo, Map> retCodecs = rpcClient.invokeConcurrent(nss, method, true, false, Map.class);\r\n    Map<String, String> ret = new HashMap<>();\r\n    Object obj = retCodecs;\r\n    @SuppressWarnings(\"unchecked\")\r\n    Map<FederationNamespaceInfo, Map<String, String>> results = (Map<FederationNamespaceInfo, Map<String, String>>) obj;\r\n    Collection<Map<String, String>> allCodecs = results.values();\r\n    for (Map<String, String> codecs : allCodecs) {\r\n        ret.putAll(codecs);\r\n    }\r\n    return ret;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "addErasureCodingPolicies",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "AddErasureCodingPolicyResponse[] addErasureCodingPolicies(ErasureCodingPolicy[] policies) throws IOException\n{\r\n    rpcServer.checkOperation(OperationCategory.WRITE);\r\n    RemoteMethod method = new RemoteMethod(\"addErasureCodingPolicies\", new Class<?>[] { ErasureCodingPolicy[].class }, new Object[] { policies });\r\n    Set<FederationNamespaceInfo> nss = namenodeResolver.getNamespaces();\r\n    Map<FederationNamespaceInfo, AddErasureCodingPolicyResponse[]> ret = rpcClient.invokeConcurrent(nss, method, true, false, AddErasureCodingPolicyResponse[].class);\r\n    return merge(ret, AddErasureCodingPolicyResponse.class);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "removeErasureCodingPolicy",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void removeErasureCodingPolicy(String ecPolicyName) throws IOException\n{\r\n    rpcServer.checkOperation(OperationCategory.WRITE);\r\n    RemoteMethod method = new RemoteMethod(\"removeErasureCodingPolicy\", new Class<?>[] { String.class }, ecPolicyName);\r\n    Set<FederationNamespaceInfo> nss = namenodeResolver.getNamespaces();\r\n    rpcClient.invokeConcurrent(nss, method, true, false);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "disableErasureCodingPolicy",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void disableErasureCodingPolicy(String ecPolicyName) throws IOException\n{\r\n    rpcServer.checkOperation(OperationCategory.WRITE);\r\n    RemoteMethod method = new RemoteMethod(\"disableErasureCodingPolicy\", new Class<?>[] { String.class }, ecPolicyName);\r\n    Set<FederationNamespaceInfo> nss = namenodeResolver.getNamespaces();\r\n    rpcClient.invokeConcurrent(nss, method, true, false);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "enableErasureCodingPolicy",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void enableErasureCodingPolicy(String ecPolicyName) throws IOException\n{\r\n    rpcServer.checkOperation(OperationCategory.WRITE);\r\n    RemoteMethod method = new RemoteMethod(\"enableErasureCodingPolicy\", new Class<?>[] { String.class }, ecPolicyName);\r\n    Set<FederationNamespaceInfo> nss = namenodeResolver.getNamespaces();\r\n    rpcClient.invokeConcurrent(nss, method, true, false);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "getErasureCodingPolicy",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "ErasureCodingPolicy getErasureCodingPolicy(String src) throws IOException\n{\r\n    rpcServer.checkOperation(OperationCategory.READ);\r\n    final List<RemoteLocation> locations = rpcServer.getLocationsForPath(src, false, false);\r\n    RemoteMethod remoteMethod = new RemoteMethod(\"getErasureCodingPolicy\", new Class<?>[] { String.class }, new RemoteParam());\r\n    ErasureCodingPolicy ret = rpcClient.invokeSequential(locations, remoteMethod, null, null);\r\n    return ret;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "setErasureCodingPolicy",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void setErasureCodingPolicy(String src, String ecPolicyName) throws IOException\n{\r\n    rpcServer.checkOperation(OperationCategory.WRITE);\r\n    final List<RemoteLocation> locations = rpcServer.getLocationsForPath(src, false, false);\r\n    RemoteMethod remoteMethod = new RemoteMethod(\"setErasureCodingPolicy\", new Class<?>[] { String.class, String.class }, new RemoteParam(), ecPolicyName);\r\n    if (rpcServer.isInvokeConcurrent(src)) {\r\n        rpcClient.invokeConcurrent(locations, remoteMethod);\r\n    } else {\r\n        rpcClient.invokeSequential(locations, remoteMethod);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "unsetErasureCodingPolicy",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void unsetErasureCodingPolicy(String src) throws IOException\n{\r\n    rpcServer.checkOperation(OperationCategory.WRITE);\r\n    final List<RemoteLocation> locations = rpcServer.getLocationsForPath(src, false, false);\r\n    RemoteMethod remoteMethod = new RemoteMethod(\"unsetErasureCodingPolicy\", new Class<?>[] { String.class }, new RemoteParam());\r\n    if (rpcServer.isInvokeConcurrent(src)) {\r\n        rpcClient.invokeConcurrent(locations, remoteMethod);\r\n    } else {\r\n        rpcClient.invokeSequential(locations, remoteMethod);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "getECTopologyResultForPolicies",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "ECTopologyVerifierResult getECTopologyResultForPolicies(String[] policyNames) throws IOException\n{\r\n    RemoteMethod method = new RemoteMethod(\"getECTopologyResultForPolicies\", new Class<?>[] { String[].class }, new Object[] { policyNames });\r\n    Set<FederationNamespaceInfo> nss = namenodeResolver.getNamespaces();\r\n    if (nss.isEmpty()) {\r\n        throw new IOException(\"No namespace availaible.\");\r\n    }\r\n    Map<FederationNamespaceInfo, ECTopologyVerifierResult> ret = rpcClient.invokeConcurrent(nss, method, true, false, ECTopologyVerifierResult.class);\r\n    for (Map.Entry<FederationNamespaceInfo, ECTopologyVerifierResult> entry : ret.entrySet()) {\r\n        if (!entry.getValue().isSupported()) {\r\n            return entry.getValue();\r\n        }\r\n    }\r\n    return ret.get(nss.iterator().next());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "getECBlockGroupStats",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "ECBlockGroupStats getECBlockGroupStats() throws IOException\n{\r\n    rpcServer.checkOperation(OperationCategory.READ);\r\n    RemoteMethod method = new RemoteMethod(\"getECBlockGroupStats\");\r\n    Set<FederationNamespaceInfo> nss = namenodeResolver.getNamespaces();\r\n    Map<FederationNamespaceInfo, ECBlockGroupStats> allStats = rpcClient.invokeConcurrent(nss, method, true, false, ECBlockGroupStats.class);\r\n    return ECBlockGroupStats.merge(allStats.values());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\protocol\\impl\\pb",
  "methodName" : "getProto",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "RemoveMountTableEntryResponseProto getProto()\n{\r\n    return this.translator.build();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\protocol\\impl\\pb",
  "methodName" : "setProto",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setProto(Message proto)\n{\r\n    this.translator.setProto(proto);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\protocol\\impl\\pb",
  "methodName" : "readInstance",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void readInstance(String base64String) throws IOException\n{\r\n    this.translator.readInstance(base64String);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\protocol\\impl\\pb",
  "methodName" : "getStatus",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "boolean getStatus()\n{\r\n    return this.translator.getProtoOrBuilder().getStatus();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\protocol\\impl\\pb",
  "methodName" : "setStatus",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setStatus(boolean result)\n{\r\n    this.translator.getBuilder().setStatus(result);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\protocol",
  "methodName" : "newInstance",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "RemoveMountTableEntryResponse newInstance() throws IOException\n{\r\n    return StateStoreSerializer.newRecord(RemoveMountTableEntryResponse.class);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\protocol",
  "methodName" : "getStatus",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean getStatus()",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\protocol",
  "methodName" : "setStatus",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setStatus(boolean result)",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "initializePermissionSettings",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void initializePermissionSettings(Configuration routerConf) throws IOException\n{\r\n    routerOwner = UserGroupInformation.getCurrentUser().getShortUserName();\r\n    superGroup = routerConf.get(DFSConfigKeys.DFS_PERMISSIONS_SUPERUSERGROUP_KEY, DFSConfigKeys.DFS_PERMISSIONS_SUPERUSERGROUP_DEFAULT);\r\n    isPermissionEnabled = routerConf.getBoolean(DFS_PERMISSIONS_ENABLED_KEY, DFS_PERMISSIONS_ENABLED_DEFAULT);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "getAdminServer",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Server getAdminServer()\n{\r\n    return this.adminServer;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "getMountTableStore",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "MountTableStore getMountTableStore() throws IOException\n{\r\n    if (this.mountTableStore == null) {\r\n        this.mountTableStore = router.getStateStore().getRegisteredRecordStore(MountTableStore.class);\r\n        if (this.mountTableStore == null) {\r\n            throw new IOException(\"Mount table state store is not available.\");\r\n        }\r\n    }\r\n    return this.mountTableStore;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "getDisabledNameserviceStore",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "DisabledNameserviceStore getDisabledNameserviceStore() throws IOException\n{\r\n    if (this.disabledStore == null) {\r\n        this.disabledStore = router.getStateStore().getRegisteredRecordStore(DisabledNameserviceStore.class);\r\n        if (this.disabledStore == null) {\r\n            throw new IOException(\"Disabled Nameservice state store is not available.\");\r\n        }\r\n    }\r\n    return this.disabledStore;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "getRpcAddress",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "InetSocketAddress getRpcAddress()\n{\r\n    return this.adminAddress;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "checkSuperuserPrivilege",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void checkSuperuserPrivilege() throws AccessControlException\n{\r\n    RouterPermissionChecker pc = RouterAdminServer.getPermissionChecker();\r\n    if (pc != null) {\r\n        pc.checkSuperuserPrivilege();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "verifyMaxComponentLength",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void verifyMaxComponentLength(String destPath) throws PathComponentTooLongException\n{\r\n    if (maxComponentLength <= 0) {\r\n        return;\r\n    }\r\n    if (destPath == null) {\r\n        return;\r\n    }\r\n    String[] components = destPath.split(Path.SEPARATOR);\r\n    for (String component : components) {\r\n        int length = component.length();\r\n        if (length > maxComponentLength) {\r\n            PathComponentTooLongException e = new PathComponentTooLongException(maxComponentLength, length, destPath, component);\r\n            throw e;\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "verifyMaxComponentLength",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void verifyMaxComponentLength(MountTable mountTable) throws PathComponentTooLongException\n{\r\n    if (mountTable != null) {\r\n        List<RemoteLocation> dests = mountTable.getDestinations();\r\n        if (dests != null && !dests.isEmpty()) {\r\n            for (RemoteLocation dest : dests) {\r\n                verifyMaxComponentLength(dest.getDest());\r\n            }\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "serviceInit",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void serviceInit(Configuration configuration) throws Exception\n{\r\n    this.conf = configuration;\r\n    super.serviceInit(conf);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "serviceStart",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void serviceStart() throws Exception\n{\r\n    this.adminServer.start();\r\n    super.serviceStart();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "serviceStop",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void serviceStop() throws Exception\n{\r\n    if (this.adminServer != null) {\r\n        this.adminServer.stop();\r\n    }\r\n    super.serviceStop();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "addMountTableEntry",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "AddMountTableEntryResponse addMountTableEntry(AddMountTableEntryRequest request) throws IOException\n{\r\n    MountTable mountTable = request.getEntry();\r\n    verifyMaxComponentLength(mountTable);\r\n    if (this.mountTableCheckDestination) {\r\n        List<String> nsIds = verifyFileInDestinations(mountTable);\r\n        if (!nsIds.isEmpty()) {\r\n            throw new IllegalArgumentException(\"File not found in downstream \" + \"nameservices: \" + StringUtils.join(\",\", nsIds));\r\n        }\r\n    }\r\n    return getMountTableStore().addMountTableEntry(request);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "updateMountTableEntry",
  "errType" : [ "Exception" ],
  "containingMethodsNum" : 19,
  "sourceCodeText" : "UpdateMountTableEntryResponse updateMountTableEntry(UpdateMountTableEntryRequest request) throws IOException\n{\r\n    MountTable updateEntry = request.getEntry();\r\n    MountTable oldEntry = null;\r\n    verifyMaxComponentLength(updateEntry);\r\n    if (this.mountTableCheckDestination) {\r\n        List<String> nsIds = verifyFileInDestinations(updateEntry);\r\n        if (!nsIds.isEmpty()) {\r\n            throw new IllegalArgumentException(\"File not found in downstream \" + \"nameservices: \" + StringUtils.join(\",\", nsIds));\r\n        }\r\n    }\r\n    if (this.router.getSubclusterResolver() instanceof MountTableResolver) {\r\n        MountTableResolver mResolver = (MountTableResolver) this.router.getSubclusterResolver();\r\n        oldEntry = mResolver.getMountPoint(updateEntry.getSourcePath());\r\n    }\r\n    UpdateMountTableEntryResponse response = getMountTableStore().updateMountTableEntry(request);\r\n    try {\r\n        if (updateEntry != null && router.isQuotaEnabled()) {\r\n            if (isQuotaUpdated(request, oldEntry)) {\r\n                synchronizeQuota(updateEntry.getSourcePath(), updateEntry.getQuota().getQuota(), updateEntry.getQuota().getSpaceQuota(), null);\r\n            }\r\n            RouterQuotaUsage newQuota = request.getEntry().getQuota();\r\n            boolean locationsChanged = oldEntry == null || !oldEntry.getDestinations().equals(updateEntry.getDestinations());\r\n            for (StorageType t : StorageType.values()) {\r\n                if (locationsChanged || oldEntry.getQuota().getTypeQuota(t) != newQuota.getTypeQuota(t)) {\r\n                    synchronizeQuota(updateEntry.getSourcePath(), HdfsConstants.QUOTA_DONT_SET, newQuota.getTypeQuota(t), t);\r\n                }\r\n            }\r\n        }\r\n    } catch (Exception e) {\r\n        LOG.warn(\"Unable to reset quota at the destinations for {}: {}\", request.getEntry(), e.getMessage());\r\n    }\r\n    return response;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "isQuotaUpdated",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "boolean isQuotaUpdated(UpdateMountTableEntryRequest request, MountTable oldEntry) throws IOException\n{\r\n    if (oldEntry != null) {\r\n        MountTable updateEntry = request.getEntry();\r\n        if (!oldEntry.getDestinations().equals(updateEntry.getDestinations())) {\r\n            return true;\r\n        }\r\n        RouterQuotaUsage preQuota = oldEntry.getQuota();\r\n        long nsQuota = preQuota.getQuota();\r\n        long ssQuota = preQuota.getSpaceQuota();\r\n        RouterQuotaUsage mountQuota = updateEntry.getQuota();\r\n        if (nsQuota != mountQuota.getQuota() || ssQuota != mountQuota.getSpaceQuota()) {\r\n            return true;\r\n        }\r\n        return false;\r\n    } else {\r\n        return true;\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "synchronizeQuota",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void synchronizeQuota(String path, long nsQuota, long ssQuota, StorageType type) throws IOException\n{\r\n    if (isQuotaSyncRequired(nsQuota, ssQuota)) {\r\n        if (iStateStoreCache) {\r\n            ((StateStoreCache) this.router.getSubclusterResolver()).loadCache(true);\r\n        }\r\n        Quota routerQuota = this.router.getRpcServer().getQuotaModule();\r\n        routerQuota.setQuota(path, nsQuota, ssQuota, type, false);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "isQuotaSyncRequired",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "boolean isQuotaSyncRequired(long nsQuota, long ssQuota)\n{\r\n    if (router.isQuotaEnabled()) {\r\n        if ((nsQuota != HdfsConstants.QUOTA_DONT_SET || ssQuota != HdfsConstants.QUOTA_DONT_SET)) {\r\n            return true;\r\n        }\r\n    }\r\n    return false;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "removeMountTableEntry",
  "errType" : [ "Exception" ],
  "containingMethodsNum" : 3,
  "sourceCodeText" : "RemoveMountTableEntryResponse removeMountTableEntry(RemoveMountTableEntryRequest request) throws IOException\n{\r\n    try {\r\n        synchronizeQuota(request.getSrcPath(), HdfsConstants.QUOTA_RESET, HdfsConstants.QUOTA_RESET, null);\r\n    } catch (Exception e) {\r\n        LOG.warn(\"Unable to clear quota at the destinations for {}: {}\", request.getSrcPath(), e.getMessage());\r\n    }\r\n    return getMountTableStore().removeMountTableEntry(request);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "getMountTableEntries",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "GetMountTableEntriesResponse getMountTableEntries(GetMountTableEntriesRequest request) throws IOException\n{\r\n    return getMountTableStore().getMountTableEntries(request);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "enterSafeMode",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "EnterSafeModeResponse enterSafeMode(EnterSafeModeRequest request) throws IOException\n{\r\n    checkSuperuserPrivilege();\r\n    boolean success = false;\r\n    RouterSafemodeService safeModeService = this.router.getSafemodeService();\r\n    if (safeModeService != null) {\r\n        this.router.updateRouterState(RouterServiceState.SAFEMODE);\r\n        safeModeService.setManualSafeMode(true);\r\n        success = verifySafeMode(true);\r\n        if (success) {\r\n            LOG.info(\"STATE* Safe mode is ON.\\n\" + \"It was turned on manually. \" + \"Use \\\"hdfs dfsrouteradmin -safemode leave\\\" to turn\" + \" safe mode off.\");\r\n        } else {\r\n            LOG.error(\"Unable to enter safemode.\");\r\n        }\r\n    }\r\n    return EnterSafeModeResponse.newInstance(success);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "leaveSafeMode",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "LeaveSafeModeResponse leaveSafeMode(LeaveSafeModeRequest request) throws IOException\n{\r\n    checkSuperuserPrivilege();\r\n    boolean success = false;\r\n    RouterSafemodeService safeModeService = this.router.getSafemodeService();\r\n    if (safeModeService != null) {\r\n        this.router.updateRouterState(RouterServiceState.RUNNING);\r\n        safeModeService.setManualSafeMode(false);\r\n        success = verifySafeMode(false);\r\n        if (success) {\r\n            LOG.info(\"STATE* Safe mode is OFF.\\n\" + \"It was turned off manually.\");\r\n        } else {\r\n            LOG.error(\"Unable to leave safemode.\");\r\n        }\r\n    }\r\n    return LeaveSafeModeResponse.newInstance(success);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "getSafeMode",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "GetSafeModeResponse getSafeMode(GetSafeModeRequest request) throws IOException\n{\r\n    boolean isInSafeMode = false;\r\n    RouterSafemodeService safeModeService = this.router.getSafemodeService();\r\n    if (safeModeService != null) {\r\n        isInSafeMode = safeModeService.isInSafeMode();\r\n        LOG.info(\"Safemode status retrieved successfully.\");\r\n    }\r\n    return GetSafeModeResponse.newInstance(isInSafeMode);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "refreshMountTableEntries",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "RefreshMountTableEntriesResponse refreshMountTableEntries(RefreshMountTableEntriesRequest request) throws IOException\n{\r\n    if (iStateStoreCache) {\r\n        boolean result = ((StateStoreCache) this.router.getSubclusterResolver()).loadCache(true);\r\n        RefreshMountTableEntriesResponse response = RefreshMountTableEntriesResponse.newInstance();\r\n        response.setResult(result);\r\n        return response;\r\n    } else {\r\n        return getMountTableStore().refreshMountTableEntries(request);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "getDestination",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "GetDestinationResponse getDestination(GetDestinationRequest request) throws IOException\n{\r\n    RouterRpcServer rpcServer = this.router.getRpcServer();\r\n    List<RemoteLocation> locations = rpcServer.getLocationsForPath(request.getSrcPath(), false);\r\n    List<String> nsIds = getDestinationNameServices(request, locations);\r\n    if (nsIds.isEmpty() && !locations.isEmpty()) {\r\n        String nsId = locations.get(0).getNameserviceId();\r\n        nsIds.add(nsId);\r\n    }\r\n    return GetDestinationResponse.newInstance(nsIds);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "getDestinationNameServices",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 7,
  "sourceCodeText" : "List<String> getDestinationNameServices(GetDestinationRequest request, List<RemoteLocation> locations) throws IOException\n{\r\n    final String src = request.getSrcPath();\r\n    final List<String> nsIds = new ArrayList<>();\r\n    RouterRpcServer rpcServer = this.router.getRpcServer();\r\n    RouterRpcClient rpcClient = rpcServer.getRPCClient();\r\n    RemoteMethod method = new RemoteMethod(\"getFileInfo\", new Class<?>[] { String.class }, new RemoteParam());\r\n    try {\r\n        Map<RemoteLocation, HdfsFileStatus> responses = rpcClient.invokeConcurrent(locations, method, false, false, HdfsFileStatus.class);\r\n        for (RemoteLocation location : locations) {\r\n            if (responses.get(location) != null) {\r\n                nsIds.add(location.getNameserviceId());\r\n            }\r\n        }\r\n    } catch (IOException ioe) {\r\n        LOG.error(\"Cannot get location for {}: {}\", src, ioe.getMessage());\r\n    }\r\n    return nsIds;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "verifyFileInDestinations",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "List<String> verifyFileInDestinations(MountTable entry) throws IOException\n{\r\n    GetDestinationRequest request = GetDestinationRequest.newInstance(entry.getSourcePath());\r\n    List<RemoteLocation> locations = entry.getDestinations();\r\n    List<String> nsId = getDestinationNameServices(request, locations);\r\n    Set<String> destNs = new HashSet<>(nsId);\r\n    List<String> nsWithoutFile = new ArrayList<>();\r\n    for (RemoteLocation location : locations) {\r\n        String ns = location.getNameserviceId();\r\n        if (!destNs.contains(ns)) {\r\n            nsWithoutFile.add(ns);\r\n        }\r\n    }\r\n    return nsWithoutFile;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "verifySafeMode",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "boolean verifySafeMode(boolean isInSafeMode)\n{\r\n    Preconditions.checkNotNull(this.router.getSafemodeService());\r\n    boolean serverInSafeMode = this.router.getSafemodeService().isInSafeMode();\r\n    RouterServiceState currentState = this.router.getRouterState();\r\n    return (isInSafeMode && currentState == RouterServiceState.SAFEMODE && serverInSafeMode) || (!isInSafeMode && currentState != RouterServiceState.SAFEMODE && !serverInSafeMode);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "disableNameservice",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "DisableNameserviceResponse disableNameservice(DisableNameserviceRequest request) throws IOException\n{\r\n    checkSuperuserPrivilege();\r\n    String nsId = request.getNameServiceId();\r\n    boolean success = false;\r\n    if (namespaceExists(nsId)) {\r\n        success = getDisabledNameserviceStore().disableNameservice(nsId);\r\n        if (success) {\r\n            LOG.info(\"Nameservice {} disabled successfully.\", nsId);\r\n        } else {\r\n            LOG.error(\"Unable to disable Nameservice {}\", nsId);\r\n        }\r\n    } else {\r\n        LOG.error(\"Cannot disable {}, it does not exists\", nsId);\r\n    }\r\n    return DisableNameserviceResponse.newInstance(success);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "namespaceExists",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "boolean namespaceExists(final String nsId) throws IOException\n{\r\n    boolean found = false;\r\n    ActiveNamenodeResolver resolver = router.getNamenodeResolver();\r\n    Set<FederationNamespaceInfo> nss = resolver.getNamespaces();\r\n    for (FederationNamespaceInfo ns : nss) {\r\n        if (nsId.equals(ns.getNameserviceId())) {\r\n            found = true;\r\n            break;\r\n        }\r\n    }\r\n    return found;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "enableNameservice",
  "errType" : null,
  "containingMethodsNum" : 10,
  "sourceCodeText" : "EnableNameserviceResponse enableNameservice(EnableNameserviceRequest request) throws IOException\n{\r\n    checkSuperuserPrivilege();\r\n    String nsId = request.getNameServiceId();\r\n    DisabledNameserviceStore store = getDisabledNameserviceStore();\r\n    Set<String> disabled = store.getDisabledNameservices();\r\n    boolean success = false;\r\n    if (disabled.contains(nsId)) {\r\n        success = store.enableNameservice(nsId);\r\n        if (success) {\r\n            LOG.info(\"Nameservice {} enabled successfully.\", nsId);\r\n        } else {\r\n            LOG.error(\"Unable to enable Nameservice {}\", nsId);\r\n        }\r\n    } else {\r\n        LOG.error(\"Cannot enable {}, it was not disabled\", nsId);\r\n    }\r\n    return EnableNameserviceResponse.newInstance(success);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "getDisabledNameservices",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "GetDisabledNameservicesResponse getDisabledNameservices(GetDisabledNameservicesRequest request) throws IOException\n{\r\n    Set<String> nsIds = getDisabledNameserviceStore().getDisabledNameservices();\r\n    return GetDisabledNameservicesResponse.newInstance(nsIds);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "getPermissionChecker",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 1,
  "sourceCodeText" : "RouterPermissionChecker getPermissionChecker() throws AccessControlException\n{\r\n    if (!isPermissionEnabled) {\r\n        return null;\r\n    }\r\n    try {\r\n        return new RouterPermissionChecker(routerOwner, superGroup, NameNode.getRemoteUser());\r\n    } catch (IOException e) {\r\n        throw new AccessControlException(e);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "getSuperUser",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getSuperUser()\n{\r\n    return routerOwner;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "getSuperGroup",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getSuperGroup()\n{\r\n    return superGroup;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "refresh",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Collection<RefreshResponse> refresh(String identifier, String[] args)\n{\r\n    return RefreshRegistry.defaultRegistry().dispatch(identifier, args);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "refreshSuperUserGroupsConfiguration",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "boolean refreshSuperUserGroupsConfiguration() throws IOException\n{\r\n    ProxyUsers.refreshSuperUserGroupsConfiguration();\r\n    return true;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "refreshCallQueue",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void refreshCallQueue() throws IOException\n{\r\n    LOG.info(\"Refreshing call queue.\");\r\n    Configuration configuration = new Configuration();\r\n    router.getRpcServer().getServer().refreshCallQueue(configuration);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\metrics",
  "methodName" : "addRead",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void addRead(long latency)\n{\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\metrics",
  "methodName" : "getReadOps",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getReadOps()\n{\r\n    return -1;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\metrics",
  "methodName" : "getReadAvg",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "double getReadAvg()\n{\r\n    return -1;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\metrics",
  "methodName" : "addWrite",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void addWrite(long latency)\n{\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\metrics",
  "methodName" : "getWriteOps",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getWriteOps()\n{\r\n    return -1;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\metrics",
  "methodName" : "getWriteAvg",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "double getWriteAvg()\n{\r\n    return -1;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\metrics",
  "methodName" : "addFailure",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void addFailure(long latency)\n{\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\metrics",
  "methodName" : "getFailureOps",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getFailureOps()\n{\r\n    return -1;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\metrics",
  "methodName" : "getFailureAvg",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "double getFailureAvg()\n{\r\n    return -1;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\metrics",
  "methodName" : "addRemove",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void addRemove(long latency)\n{\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\metrics",
  "methodName" : "getRemoveOps",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getRemoveOps()\n{\r\n    return -1;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\metrics",
  "methodName" : "getRemoveAvg",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "double getRemoveAvg()\n{\r\n    return -1;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\metrics",
  "methodName" : "setCacheSize",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setCacheSize(String name, int size)\n{\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\metrics",
  "methodName" : "reset",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void reset()\n{\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\metrics",
  "methodName" : "shutdown",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void shutdown()\n{\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\protocol",
  "methodName" : "newInstance",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "UpdateNamenodeRegistrationResponse newInstance()\n{\r\n    return StateStoreSerializer.newRecord(UpdateNamenodeRegistrationResponse.class);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\protocol",
  "methodName" : "newInstance",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "UpdateNamenodeRegistrationResponse newInstance(boolean status) throws IOException\n{\r\n    UpdateNamenodeRegistrationResponse response = newInstance();\r\n    response.setResult(status);\r\n    return response;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\protocol",
  "methodName" : "getResult",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean getResult()",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\protocol",
  "methodName" : "setResult",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setResult(boolean result)",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "createRouterProxy",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "RouterAdminProtocolTranslatorPB createRouterProxy(InetSocketAddress address, Configuration conf, UserGroupInformation ugi) throws IOException\n{\r\n    RPC.setProtocolEngine(conf, RouterAdminProtocolPB.class, ProtobufRpcEngine2.class);\r\n    AtomicBoolean fallbackToSimpleAuth = new AtomicBoolean(false);\r\n    final long version = RPC.getProtocolVersion(RouterAdminProtocolPB.class);\r\n    RouterAdminProtocolPB proxy = RPC.getProtocolProxy(RouterAdminProtocolPB.class, version, address, ugi, conf, NetUtils.getDefaultSocketFactory(conf), RPC.getRpcTimeout(conf), null, fallbackToSimpleAuth).getProxy();\r\n    return new RouterAdminProtocolTranslatorPB(proxy);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "getMountTableManager",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "MountTableManager getMountTableManager()\n{\r\n    return proxy;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "getRouterStateManager",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "RouterStateManager getRouterStateManager()\n{\r\n    return proxy;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "getNameserviceManager",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "NameserviceManager getNameserviceManager()\n{\r\n    return proxy;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "getRouterGenericManager",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "RouterGenericManager getRouterGenericManager()\n{\r\n    return proxy;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "close",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void close() throws IOException\n{\r\n    RPC.stopProxy(proxy);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "getDelegationToken",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "Token<DelegationTokenIdentifier> getDelegationToken(Text renewer) throws IOException\n{\r\n    rpcServer.checkOperation(NameNode.OperationCategory.WRITE, true);\r\n    return this.securityManager.getDelegationToken(renewer);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "getDelegationTokens",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Map<FederationNamespaceInfo, Token<DelegationTokenIdentifier>> getDelegationTokens(Text renewer) throws IOException\n{\r\n    rpcServer.checkOperation(NameNode.OperationCategory.WRITE, false);\r\n    return null;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "renewDelegationToken",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "long renewDelegationToken(Token<DelegationTokenIdentifier> token) throws IOException\n{\r\n    rpcServer.checkOperation(NameNode.OperationCategory.WRITE, true);\r\n    return this.securityManager.renewDelegationToken(token);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "cancelDelegationToken",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void cancelDelegationToken(Token<DelegationTokenIdentifier> token) throws IOException\n{\r\n    rpcServer.checkOperation(NameNode.OperationCategory.WRITE, true);\r\n    this.securityManager.cancelDelegationToken(token);\r\n    return;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "getBlockLocations",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "LocatedBlocks getBlockLocations(String src, final long offset, final long length) throws IOException\n{\r\n    rpcServer.checkOperation(NameNode.OperationCategory.READ);\r\n    List<RemoteLocation> locations = rpcServer.getLocationsForPath(src, false, false);\r\n    RemoteMethod remoteMethod = new RemoteMethod(\"getBlockLocations\", new Class<?>[] { String.class, long.class, long.class }, new RemoteParam(), offset, length);\r\n    return rpcClient.invokeSequential(locations, remoteMethod, LocatedBlocks.class, null);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "getServerDefaults",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "FsServerDefaults getServerDefaults() throws IOException\n{\r\n    rpcServer.checkOperation(NameNode.OperationCategory.READ);\r\n    long now = Time.monotonicNow();\r\n    if ((serverDefaults == null) || (now - serverDefaultsLastUpdate > serverDefaultsValidityPeriod)) {\r\n        RemoteMethod method = new RemoteMethod(\"getServerDefaults\");\r\n        serverDefaults = rpcServer.invokeAtAvailableNs(method, FsServerDefaults.class);\r\n        serverDefaultsLastUpdate = now;\r\n    }\r\n    return serverDefaults;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "create",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 13,
  "sourceCodeText" : "HdfsFileStatus create(String src, FsPermission masked, String clientName, EnumSetWritable<CreateFlag> flag, boolean createParent, short replication, long blockSize, CryptoProtocolVersion[] supportedVersions, String ecPolicyName, String storagePolicy) throws IOException\n{\r\n    rpcServer.checkOperation(NameNode.OperationCategory.WRITE);\r\n    if (createParent && rpcServer.isPathAll(src)) {\r\n        int index = src.lastIndexOf(Path.SEPARATOR);\r\n        String parent = src.substring(0, index);\r\n        LOG.debug(\"Creating {} requires creating parent {}\", src, parent);\r\n        FsPermission parentPermissions = getParentPermission(masked);\r\n        boolean success = mkdirs(parent, parentPermissions, createParent);\r\n        if (!success) {\r\n            LOG.error(\"Couldn't create parents for {}\", src);\r\n        }\r\n    }\r\n    RemoteMethod method = new RemoteMethod(\"create\", new Class<?>[] { String.class, FsPermission.class, String.class, EnumSetWritable.class, boolean.class, short.class, long.class, CryptoProtocolVersion[].class, String.class, String.class }, new RemoteParam(), masked, clientName, flag, createParent, replication, blockSize, supportedVersions, ecPolicyName, storagePolicy);\r\n    final List<RemoteLocation> locations = rpcServer.getLocationsForPath(src, true);\r\n    RemoteLocation createLocation = null;\r\n    try {\r\n        createLocation = rpcServer.getCreateLocation(src, locations);\r\n        return rpcClient.invokeSingle(createLocation, method, HdfsFileStatus.class);\r\n    } catch (IOException ioe) {\r\n        final List<RemoteLocation> newLocations = checkFaultTolerantRetry(method, src, ioe, createLocation, locations);\r\n        return rpcClient.invokeSequential(newLocations, method, HdfsFileStatus.class, null);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "isUnavailableSubclusterException",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "boolean isUnavailableSubclusterException(final IOException ioe)\n{\r\n    if (ioe instanceof ConnectException || ioe instanceof ConnectTimeoutException || ioe instanceof NoNamenodesAvailableException) {\r\n        return true;\r\n    }\r\n    if (ioe.getCause() instanceof IOException) {\r\n        IOException cause = (IOException) ioe.getCause();\r\n        return isUnavailableSubclusterException(cause);\r\n    }\r\n    return false;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "checkFaultTolerantRetry",
  "errType" : null,
  "containingMethodsNum" : 9,
  "sourceCodeText" : "List<RemoteLocation> checkFaultTolerantRetry(final RemoteMethod method, final String src, final IOException ioe, final RemoteLocation excludeLoc, final List<RemoteLocation> locations) throws IOException\n{\r\n    if (!isUnavailableSubclusterException(ioe)) {\r\n        LOG.debug(\"{} exception cannot be retried\", ioe.getClass().getSimpleName());\r\n        throw ioe;\r\n    }\r\n    if (!rpcServer.isPathFaultTolerant(src)) {\r\n        LOG.debug(\"{} does not allow retrying a failed subcluster\", src);\r\n        throw ioe;\r\n    }\r\n    final List<RemoteLocation> newLocations;\r\n    if (excludeLoc == null) {\r\n        LOG.error(\"Cannot invoke {} for {}: {}\", method, src, ioe.getMessage());\r\n        newLocations = locations;\r\n    } else {\r\n        LOG.error(\"Cannot invoke {} for {} in {}: {}\", method, src, excludeLoc, ioe.getMessage());\r\n        newLocations = new ArrayList<>();\r\n        for (final RemoteLocation loc : locations) {\r\n            if (!loc.equals(excludeLoc)) {\r\n                newLocations.add(loc);\r\n            }\r\n        }\r\n    }\r\n    LOG.info(\"{} allows retrying failed subclusters in {}\", src, newLocations);\r\n    return newLocations;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "append",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "LastBlockWithStatus append(String src, final String clientName, final EnumSetWritable<CreateFlag> flag) throws IOException\n{\r\n    rpcServer.checkOperation(NameNode.OperationCategory.WRITE);\r\n    List<RemoteLocation> locations = rpcServer.getLocationsForPath(src, true);\r\n    RemoteMethod method = new RemoteMethod(\"append\", new Class<?>[] { String.class, String.class, EnumSetWritable.class }, new RemoteParam(), clientName, flag);\r\n    return rpcClient.invokeSequential(locations, method, LastBlockWithStatus.class, null);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "recoverLease",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "boolean recoverLease(String src, String clientName) throws IOException\n{\r\n    rpcServer.checkOperation(NameNode.OperationCategory.WRITE);\r\n    final List<RemoteLocation> locations = rpcServer.getLocationsForPath(src, true, false);\r\n    RemoteMethod method = new RemoteMethod(\"recoverLease\", new Class<?>[] { String.class, String.class }, new RemoteParam(), clientName);\r\n    Object result = rpcClient.invokeSequential(locations, method, Boolean.class, null);\r\n    return (boolean) result;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "setReplication",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "boolean setReplication(String src, short replication) throws IOException\n{\r\n    rpcServer.checkOperation(NameNode.OperationCategory.WRITE);\r\n    List<RemoteLocation> locations = rpcServer.getLocationsForPath(src, true);\r\n    RemoteMethod method = new RemoteMethod(\"setReplication\", new Class<?>[] { String.class, short.class }, new RemoteParam(), replication);\r\n    if (rpcServer.isInvokeConcurrent(src)) {\r\n        return !rpcClient.invokeConcurrent(locations, method, Boolean.class).containsValue(false);\r\n    } else {\r\n        return rpcClient.invokeSequential(locations, method, Boolean.class, Boolean.TRUE);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "setStoragePolicy",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setStoragePolicy(String src, String policyName) throws IOException\n{\r\n    storagePolicy.setStoragePolicy(src, policyName);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "getStoragePolicies",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "BlockStoragePolicy[] getStoragePolicies() throws IOException\n{\r\n    return storagePolicy.getStoragePolicies();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "setPermission",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void setPermission(String src, FsPermission permissions) throws IOException\n{\r\n    rpcServer.checkOperation(NameNode.OperationCategory.WRITE);\r\n    final List<RemoteLocation> locations = rpcServer.getLocationsForPath(src, false, false);\r\n    RemoteMethod method = new RemoteMethod(\"setPermission\", new Class<?>[] { String.class, FsPermission.class }, new RemoteParam(), permissions);\r\n    if (rpcServer.isInvokeConcurrent(src)) {\r\n        rpcClient.invokeConcurrent(locations, method);\r\n    } else {\r\n        rpcClient.invokeSequential(locations, method);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "setOwner",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void setOwner(String src, String username, String groupname) throws IOException\n{\r\n    rpcServer.checkOperation(NameNode.OperationCategory.WRITE);\r\n    final List<RemoteLocation> locations = rpcServer.getLocationsForPath(src, false, false);\r\n    RemoteMethod method = new RemoteMethod(\"setOwner\", new Class<?>[] { String.class, String.class, String.class }, new RemoteParam(), username, groupname);\r\n    if (rpcServer.isInvokeConcurrent(src)) {\r\n        rpcClient.invokeConcurrent(locations, method);\r\n    } else {\r\n        rpcClient.invokeSequential(locations, method);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "addBlock",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "LocatedBlock addBlock(String src, String clientName, ExtendedBlock previous, DatanodeInfo[] excludedNodes, long fileId, String[] favoredNodes, EnumSet<AddBlockFlag> addBlockFlags) throws IOException\n{\r\n    rpcServer.checkOperation(NameNode.OperationCategory.WRITE);\r\n    RemoteMethod method = new RemoteMethod(\"addBlock\", new Class<?>[] { String.class, String.class, ExtendedBlock.class, DatanodeInfo[].class, long.class, String[].class, EnumSet.class }, new RemoteParam(), clientName, previous, excludedNodes, fileId, favoredNodes, addBlockFlags);\r\n    if (previous != null) {\r\n        return rpcClient.invokeSingle(previous, method, LocatedBlock.class);\r\n    }\r\n    final List<RemoteLocation> locations = rpcServer.getLocationsForPath(src, true);\r\n    return rpcClient.invokeSequential(locations, method, LocatedBlock.class, null);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "getAdditionalDatanode",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "LocatedBlock getAdditionalDatanode(final String src, final long fileId, final ExtendedBlock blk, final DatanodeInfo[] existings, final String[] existingStorageIDs, final DatanodeInfo[] excludes, final int numAdditionalNodes, final String clientName) throws IOException\n{\r\n    rpcServer.checkOperation(NameNode.OperationCategory.READ);\r\n    RemoteMethod method = new RemoteMethod(\"getAdditionalDatanode\", new Class<?>[] { String.class, long.class, ExtendedBlock.class, DatanodeInfo[].class, String[].class, DatanodeInfo[].class, int.class, String.class }, new RemoteParam(), fileId, blk, existings, existingStorageIDs, excludes, numAdditionalNodes, clientName);\r\n    if (blk != null) {\r\n        return rpcClient.invokeSingle(blk, method, LocatedBlock.class);\r\n    }\r\n    final List<RemoteLocation> locations = rpcServer.getLocationsForPath(src, false);\r\n    return rpcClient.invokeSequential(locations, method, LocatedBlock.class, null);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "abandonBlock",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void abandonBlock(ExtendedBlock b, long fileId, String src, String holder) throws IOException\n{\r\n    rpcServer.checkOperation(NameNode.OperationCategory.WRITE);\r\n    RemoteMethod method = new RemoteMethod(\"abandonBlock\", new Class<?>[] { ExtendedBlock.class, long.class, String.class, String.class }, b, fileId, new RemoteParam(), holder);\r\n    rpcClient.invokeSingle(b, method);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "complete",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "boolean complete(String src, String clientName, ExtendedBlock last, long fileId) throws IOException\n{\r\n    rpcServer.checkOperation(NameNode.OperationCategory.WRITE);\r\n    RemoteMethod method = new RemoteMethod(\"complete\", new Class<?>[] { String.class, String.class, ExtendedBlock.class, long.class }, new RemoteParam(), clientName, last, fileId);\r\n    if (last != null) {\r\n        return rpcClient.invokeSingle(last, method, Boolean.class);\r\n    }\r\n    final List<RemoteLocation> locations = rpcServer.getLocationsForPath(src, true);\r\n    return rpcClient.invokeSequential(locations, method, Boolean.class, null);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "updateBlockForPipeline",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "LocatedBlock updateBlockForPipeline(ExtendedBlock block, String clientName) throws IOException\n{\r\n    rpcServer.checkOperation(NameNode.OperationCategory.WRITE);\r\n    RemoteMethod method = new RemoteMethod(\"updateBlockForPipeline\", new Class<?>[] { ExtendedBlock.class, String.class }, block, clientName);\r\n    return rpcClient.invokeSingle(block, method, LocatedBlock.class);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "updatePipeline",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void updatePipeline(String clientName, ExtendedBlock oldBlock, ExtendedBlock newBlock, DatanodeID[] newNodes, String[] newStorageIDs) throws IOException\n{\r\n    rpcServer.checkOperation(NameNode.OperationCategory.WRITE);\r\n    RemoteMethod method = new RemoteMethod(\"updatePipeline\", new Class<?>[] { String.class, ExtendedBlock.class, ExtendedBlock.class, DatanodeID[].class, String[].class }, clientName, oldBlock, newBlock, newNodes, newStorageIDs);\r\n    rpcClient.invokeSingle(oldBlock, method);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "getPreferredBlockSize",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "long getPreferredBlockSize(String src) throws IOException\n{\r\n    rpcServer.checkOperation(NameNode.OperationCategory.READ);\r\n    final List<RemoteLocation> locations = rpcServer.getLocationsForPath(src, true, false);\r\n    RemoteMethod method = new RemoteMethod(\"getPreferredBlockSize\", new Class<?>[] { String.class }, new RemoteParam());\r\n    return rpcClient.invokeSequential(locations, method, Long.class, null);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "rename",
  "errType" : null,
  "containingMethodsNum" : 9,
  "sourceCodeText" : "boolean rename(final String src, final String dst) throws IOException\n{\r\n    rpcServer.checkOperation(NameNode.OperationCategory.WRITE);\r\n    final List<RemoteLocation> srcLocations = rpcServer.getLocationsForPath(src, true, false);\r\n    final List<RemoteLocation> dstLocations = rpcServer.getLocationsForPath(dst, false, false);\r\n    final List<RemoteLocation> locs = new LinkedList<>(srcLocations);\r\n    RemoteParam dstParam = getRenameDestinations(locs, dstLocations);\r\n    if (locs.isEmpty()) {\r\n        return rbfRename.routerFedRename(src, dst, srcLocations, dstLocations);\r\n    }\r\n    RemoteMethod method = new RemoteMethod(\"rename\", new Class<?>[] { String.class, String.class }, new RemoteParam(), dstParam);\r\n    if (isMultiDestDirectory(src)) {\r\n        return rpcClient.invokeAll(locs, method);\r\n    } else {\r\n        return rpcClient.invokeSequential(locs, method, Boolean.class, Boolean.TRUE);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "rename2",
  "errType" : null,
  "containingMethodsNum" : 10,
  "sourceCodeText" : "void rename2(final String src, final String dst, final Options.Rename... options) throws IOException\n{\r\n    rpcServer.checkOperation(NameNode.OperationCategory.WRITE);\r\n    final List<RemoteLocation> srcLocations = rpcServer.getLocationsForPath(src, true, false);\r\n    final List<RemoteLocation> dstLocations = rpcServer.getLocationsForPath(dst, false, false);\r\n    final List<RemoteLocation> locs = new LinkedList<>(srcLocations);\r\n    RemoteParam dstParam = getRenameDestinations(locs, dstLocations);\r\n    if (locs.isEmpty()) {\r\n        rbfRename.routerFedRename(src, dst, srcLocations, dstLocations);\r\n        return;\r\n    }\r\n    RemoteMethod method = new RemoteMethod(\"rename2\", new Class<?>[] { String.class, String.class, options.getClass() }, new RemoteParam(), dstParam, options);\r\n    if (isMultiDestDirectory(src)) {\r\n        rpcClient.invokeConcurrent(locs, method);\r\n    } else {\r\n        rpcClient.invokeSequential(locs, method, null, null);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "concat",
  "errType" : null,
  "containingMethodsNum" : 12,
  "sourceCodeText" : "void concat(String trg, String[] src) throws IOException\n{\r\n    rpcServer.checkOperation(NameNode.OperationCategory.WRITE);\r\n    LocatedBlocks targetBlocks = getBlockLocations(trg, 0, 1);\r\n    if (targetBlocks == null) {\r\n        throw new IOException(\"Cannot locate blocks for target file - \" + trg);\r\n    }\r\n    LocatedBlock lastLocatedBlock = targetBlocks.getLastLocatedBlock();\r\n    String targetBlockPoolId = lastLocatedBlock.getBlock().getBlockPoolId();\r\n    for (String source : src) {\r\n        LocatedBlocks sourceBlocks = getBlockLocations(source, 0, 1);\r\n        if (sourceBlocks == null) {\r\n            throw new IOException(\"Cannot located blocks for source file \" + source);\r\n        }\r\n        String sourceBlockPoolId = sourceBlocks.getLastLocatedBlock().getBlock().getBlockPoolId();\r\n        if (!sourceBlockPoolId.equals(targetBlockPoolId)) {\r\n            throw new IOException(\"Cannot concatenate source file \" + source + \" because it is located in a different namespace\" + \" with block pool id \" + sourceBlockPoolId + \" from the target file with block pool id \" + targetBlockPoolId);\r\n        }\r\n    }\r\n    final RemoteLocation targetDestination = rpcServer.getLocationForPath(trg, true, targetBlockPoolId);\r\n    String[] sourceDestinations = new String[src.length];\r\n    for (int i = 0; i < src.length; i++) {\r\n        String sourceFile = src[i];\r\n        RemoteLocation location = rpcServer.getLocationForPath(sourceFile, true, targetBlockPoolId);\r\n        sourceDestinations[i] = location.getDest();\r\n    }\r\n    RemoteMethod method = new RemoteMethod(\"concat\", new Class<?>[] { String.class, String[].class }, targetDestination.getDest(), sourceDestinations);\r\n    rpcClient.invokeSingle(targetDestination, method, Void.class);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "truncate",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "boolean truncate(String src, long newLength, String clientName) throws IOException\n{\r\n    rpcServer.checkOperation(NameNode.OperationCategory.WRITE);\r\n    final List<RemoteLocation> locations = rpcServer.getLocationsForPath(src, true);\r\n    RemoteMethod method = new RemoteMethod(\"truncate\", new Class<?>[] { String.class, long.class, String.class }, new RemoteParam(), newLength, clientName);\r\n    return rpcClient.invokeSequential(locations, method, Boolean.class, Boolean.TRUE);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "delete",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "boolean delete(String src, boolean recursive) throws IOException\n{\r\n    rpcServer.checkOperation(NameNode.OperationCategory.WRITE);\r\n    final List<RemoteLocation> locations = rpcServer.getLocationsForPath(src, true, false);\r\n    RemoteMethod method = new RemoteMethod(\"delete\", new Class<?>[] { String.class, boolean.class }, new RemoteParam(), recursive);\r\n    if (rpcServer.isPathAll(src)) {\r\n        return rpcClient.invokeAll(locations, method);\r\n    } else {\r\n        return rpcClient.invokeSequential(locations, method, Boolean.class, Boolean.TRUE);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "mkdirs",
  "errType" : [ "IOException", "IOException" ],
  "containingMethodsNum" : 11,
  "sourceCodeText" : "boolean mkdirs(String src, FsPermission masked, boolean createParent) throws IOException\n{\r\n    rpcServer.checkOperation(NameNode.OperationCategory.WRITE);\r\n    final List<RemoteLocation> locations = rpcServer.getLocationsForPath(src, false);\r\n    RemoteMethod method = new RemoteMethod(\"mkdirs\", new Class<?>[] { String.class, FsPermission.class, boolean.class }, new RemoteParam(), masked, createParent);\r\n    if (rpcServer.isPathAll(src)) {\r\n        return rpcClient.invokeAll(locations, method);\r\n    }\r\n    if (locations.size() > 1) {\r\n        try {\r\n            HdfsFileStatus fileStatus = getFileInfo(src);\r\n            if (fileStatus != null) {\r\n                return true;\r\n            }\r\n        } catch (IOException ioe) {\r\n            LOG.error(\"Error getting file info for {} while proxying mkdirs: {}\", src, ioe.getMessage());\r\n        }\r\n    }\r\n    final RemoteLocation firstLocation = locations.get(0);\r\n    try {\r\n        return rpcClient.invokeSingle(firstLocation, method, Boolean.class);\r\n    } catch (IOException ioe) {\r\n        final List<RemoteLocation> newLocations = checkFaultTolerantRetry(method, src, ioe, firstLocation, locations);\r\n        return rpcClient.invokeSequential(newLocations, method, Boolean.class, Boolean.TRUE);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 2,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "renewLease",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void renewLease(String clientName) throws IOException\n{\r\n    rpcServer.checkOperation(NameNode.OperationCategory.WRITE);\r\n    RemoteMethod method = new RemoteMethod(\"renewLease\", new Class<?>[] { String.class }, clientName);\r\n    Set<FederationNamespaceInfo> nss = namenodeResolver.getNamespaces();\r\n    rpcClient.invokeConcurrent(nss, method, false, false);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "getListing",
  "errType" : null,
  "containingMethodsNum" : 35,
  "sourceCodeText" : "DirectoryListing getListing(String src, byte[] startAfter, boolean needLocation) throws IOException\n{\r\n    rpcServer.checkOperation(NameNode.OperationCategory.READ);\r\n    List<RemoteResult<RemoteLocation, DirectoryListing>> listings = getListingInt(src, startAfter, needLocation);\r\n    TreeMap<String, HdfsFileStatus> nnListing = new TreeMap<>();\r\n    int totalRemainingEntries = 0;\r\n    int remainingEntries = 0;\r\n    boolean namenodeListingExists = false;\r\n    String lastName = null;\r\n    if (listings != null) {\r\n        for (RemoteResult<RemoteLocation, DirectoryListing> result : listings) {\r\n            if (result.hasException()) {\r\n                IOException ioe = result.getException();\r\n                if (ioe instanceof FileNotFoundException) {\r\n                    RemoteLocation location = result.getLocation();\r\n                    LOG.debug(\"Cannot get listing from {}\", location);\r\n                } else if (!allowPartialList) {\r\n                    throw ioe;\r\n                }\r\n            } else if (result.getResult() != null) {\r\n                DirectoryListing listing = result.getResult();\r\n                totalRemainingEntries += listing.getRemainingEntries();\r\n                HdfsFileStatus[] partialListing = listing.getPartialListing();\r\n                int length = partialListing.length;\r\n                if (length > 0) {\r\n                    HdfsFileStatus lastLocalEntry = partialListing[length - 1];\r\n                    String lastLocalName = lastLocalEntry.getLocalName();\r\n                    if (lastName == null || lastName.compareTo(lastLocalName) > 0) {\r\n                        lastName = lastLocalName;\r\n                    }\r\n                }\r\n            }\r\n        }\r\n        for (RemoteResult<RemoteLocation, DirectoryListing> result : listings) {\r\n            DirectoryListing listing = result.getResult();\r\n            if (listing != null) {\r\n                namenodeListingExists = true;\r\n                for (HdfsFileStatus file : listing.getPartialListing()) {\r\n                    String filename = file.getLocalName();\r\n                    if (totalRemainingEntries > 0 && filename.compareTo(lastName) > 0) {\r\n                        remainingEntries++;\r\n                    } else {\r\n                        nnListing.put(filename, file);\r\n                    }\r\n                }\r\n                remainingEntries += listing.getRemainingEntries();\r\n            }\r\n        }\r\n    }\r\n    final List<String> children = subclusterResolver.getMountPoints(src);\r\n    if (children != null) {\r\n        Collections.sort(children);\r\n    }\r\n    if (children != null) {\r\n        Map<String, Long> dates = getMountPointDates(src);\r\n        for (String child : children) {\r\n            long date = 0;\r\n            if (dates != null && dates.containsKey(child)) {\r\n                date = dates.get(child);\r\n            }\r\n            Path childPath = new Path(src, child);\r\n            HdfsFileStatus dirStatus = getMountPointStatus(childPath.toString(), 0, date);\r\n            if (lastName == null) {\r\n                nnListing.put(child, dirStatus);\r\n            } else {\r\n                if (shouldAddMountPoint(child, lastName, startAfter, remainingEntries)) {\r\n                    nnListing.put(child, dirStatus);\r\n                }\r\n            }\r\n        }\r\n        if (nnListing.size() > 0) {\r\n            String lastListing = nnListing.lastKey();\r\n            for (int i = 0; i < children.size(); i++) {\r\n                if (children.get(i).compareTo(lastListing) > 0) {\r\n                    remainingEntries += (children.size() - i);\r\n                    break;\r\n                }\r\n            }\r\n        }\r\n    }\r\n    if (!namenodeListingExists && nnListing.size() == 0) {\r\n        return null;\r\n    }\r\n    HdfsFileStatus[] combinedData = new HdfsFileStatus[nnListing.size()];\r\n    combinedData = nnListing.values().toArray(combinedData);\r\n    return new DirectoryListing(combinedData, remainingEntries);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "getBatchedListing",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "BatchedDirectoryListing getBatchedListing(String[] srcs, byte[] startAfter, boolean needLocation) throws IOException\n{\r\n    throw new UnsupportedOperationException(\"Not implemented\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "getFileInfo",
  "errType" : null,
  "containingMethodsNum" : 12,
  "sourceCodeText" : "HdfsFileStatus getFileInfo(String src) throws IOException\n{\r\n    rpcServer.checkOperation(NameNode.OperationCategory.READ);\r\n    final List<RemoteLocation> locations = rpcServer.getLocationsForPath(src, false, false);\r\n    RemoteMethod method = new RemoteMethod(\"getFileInfo\", new Class<?>[] { String.class }, new RemoteParam());\r\n    HdfsFileStatus ret = null;\r\n    if (rpcServer.isPathAll(src)) {\r\n        ret = getFileInfoAll(locations, method);\r\n    } else {\r\n        ret = rpcClient.invokeSequential(locations, method, HdfsFileStatus.class, null);\r\n    }\r\n    if (ret == null) {\r\n        List<String> children = subclusterResolver.getMountPoints(src);\r\n        if (children != null && !children.isEmpty()) {\r\n            Map<String, Long> dates = getMountPointDates(src);\r\n            long date = 0;\r\n            if (dates != null && dates.containsKey(src)) {\r\n                date = dates.get(src);\r\n            }\r\n            ret = getMountPointStatus(src, children.size(), date);\r\n        } else if (children != null) {\r\n            ret = getMountPointStatus(src, 0, 0);\r\n        }\r\n    }\r\n    return ret;\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "isFileClosed",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "boolean isFileClosed(String src) throws IOException\n{\r\n    rpcServer.checkOperation(NameNode.OperationCategory.READ);\r\n    final List<RemoteLocation> locations = rpcServer.getLocationsForPath(src, false, false);\r\n    RemoteMethod method = new RemoteMethod(\"isFileClosed\", new Class<?>[] { String.class }, new RemoteParam());\r\n    return rpcClient.invokeSequential(locations, method, Boolean.class, null);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "getFileLinkInfo",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "HdfsFileStatus getFileLinkInfo(String src) throws IOException\n{\r\n    rpcServer.checkOperation(NameNode.OperationCategory.READ);\r\n    final List<RemoteLocation> locations = rpcServer.getLocationsForPath(src, false, false);\r\n    RemoteMethod method = new RemoteMethod(\"getFileLinkInfo\", new Class<?>[] { String.class }, new RemoteParam());\r\n    return rpcClient.invokeSequential(locations, method, HdfsFileStatus.class, null);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "getLocatedFileInfo",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "HdfsLocatedFileStatus getLocatedFileInfo(String src, boolean needBlockToken) throws IOException\n{\r\n    rpcServer.checkOperation(NameNode.OperationCategory.READ);\r\n    final List<RemoteLocation> locations = rpcServer.getLocationsForPath(src, false, false);\r\n    RemoteMethod method = new RemoteMethod(\"getLocatedFileInfo\", new Class<?>[] { String.class, boolean.class }, new RemoteParam(), needBlockToken);\r\n    return (HdfsLocatedFileStatus) rpcClient.invokeSequential(locations, method, HdfsFileStatus.class, null);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "getStats",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "long[] getStats() throws IOException\n{\r\n    rpcServer.checkOperation(NameNode.OperationCategory.UNCHECKED);\r\n    RemoteMethod method = new RemoteMethod(\"getStats\");\r\n    Set<FederationNamespaceInfo> nss = namenodeResolver.getNamespaces();\r\n    Map<FederationNamespaceInfo, long[]> results = rpcClient.invokeConcurrent(nss, method, true, false, long[].class);\r\n    long[] combinedData = new long[STATS_ARRAY_LENGTH];\r\n    for (long[] data : results.values()) {\r\n        for (int i = 0; i < combinedData.length && i < data.length; i++) {\r\n            if (data[i] >= 0) {\r\n                combinedData[i] += data[i];\r\n            }\r\n        }\r\n    }\r\n    return combinedData;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "getDatanodeReport",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "DatanodeInfo[] getDatanodeReport(HdfsConstants.DatanodeReportType type) throws IOException\n{\r\n    rpcServer.checkOperation(NameNode.OperationCategory.UNCHECKED);\r\n    return rpcServer.getDatanodeReport(type, true, 0);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "getDatanodeStorageReport",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "DatanodeStorageReport[] getDatanodeStorageReport(HdfsConstants.DatanodeReportType type) throws IOException\n{\r\n    rpcServer.checkOperation(NameNode.OperationCategory.UNCHECKED);\r\n    Map<String, DatanodeStorageReport[]> dnSubcluster = rpcServer.getDatanodeStorageReportMap(type);\r\n    return mergeDtanodeStorageReport(dnSubcluster);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "getDatanodeStorageReport",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "DatanodeStorageReport[] getDatanodeStorageReport(HdfsConstants.DatanodeReportType type, boolean requireResponse, long timeOutMs) throws IOException\n{\r\n    rpcServer.checkOperation(NameNode.OperationCategory.UNCHECKED);\r\n    Map<String, DatanodeStorageReport[]> dnSubcluster = rpcServer.getDatanodeStorageReportMap(type, requireResponse, timeOutMs);\r\n    return mergeDtanodeStorageReport(dnSubcluster);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "mergeDtanodeStorageReport",
  "errType" : null,
  "containingMethodsNum" : 11,
  "sourceCodeText" : "DatanodeStorageReport[] mergeDtanodeStorageReport(Map<String, DatanodeStorageReport[]> dnSubcluster)\n{\r\n    Map<String, DatanodeStorageReport> datanodesMap = new LinkedHashMap<>();\r\n    for (DatanodeStorageReport[] dns : dnSubcluster.values()) {\r\n        for (DatanodeStorageReport dn : dns) {\r\n            DatanodeInfo dnInfo = dn.getDatanodeInfo();\r\n            String nodeId = dnInfo.getXferAddr();\r\n            DatanodeStorageReport oldDn = datanodesMap.get(nodeId);\r\n            if (oldDn == null || dnInfo.getLastUpdate() > oldDn.getDatanodeInfo().getLastUpdate()) {\r\n                datanodesMap.put(nodeId, dn);\r\n            } else {\r\n                LOG.debug(\"{} is in multiple subclusters\", nodeId);\r\n            }\r\n        }\r\n    }\r\n    Collection<DatanodeStorageReport> datanodes = datanodesMap.values();\r\n    DatanodeStorageReport[] combinedData = new DatanodeStorageReport[datanodes.size()];\r\n    combinedData = datanodes.toArray(combinedData);\r\n    return combinedData;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "setSafeMode",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "boolean setSafeMode(HdfsConstants.SafeModeAction action, boolean isChecked) throws IOException\n{\r\n    rpcServer.checkOperation(NameNode.OperationCategory.WRITE);\r\n    RemoteMethod method = new RemoteMethod(\"setSafeMode\", new Class<?>[] { HdfsConstants.SafeModeAction.class, boolean.class }, action, isChecked);\r\n    Set<FederationNamespaceInfo> nss = namenodeResolver.getNamespaces();\r\n    Map<FederationNamespaceInfo, Boolean> results = rpcClient.invokeConcurrent(nss, method, true, !isChecked, Boolean.class);\r\n    int numSafemode = 0;\r\n    for (boolean safemode : results.values()) {\r\n        if (safemode) {\r\n            numSafemode++;\r\n        }\r\n    }\r\n    return numSafemode == results.size();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "restoreFailedStorage",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "boolean restoreFailedStorage(String arg) throws IOException\n{\r\n    rpcServer.checkOperation(NameNode.OperationCategory.UNCHECKED);\r\n    RemoteMethod method = new RemoteMethod(\"restoreFailedStorage\", new Class<?>[] { String.class }, arg);\r\n    final Set<FederationNamespaceInfo> nss = namenodeResolver.getNamespaces();\r\n    Map<FederationNamespaceInfo, Boolean> ret = rpcClient.invokeConcurrent(nss, method, true, false, Boolean.class);\r\n    boolean success = true;\r\n    for (boolean s : ret.values()) {\r\n        if (!s) {\r\n            success = false;\r\n            break;\r\n        }\r\n    }\r\n    return success;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "saveNamespace",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "boolean saveNamespace(long timeWindow, long txGap) throws IOException\n{\r\n    rpcServer.checkOperation(NameNode.OperationCategory.UNCHECKED);\r\n    RemoteMethod method = new RemoteMethod(\"saveNamespace\", new Class<?>[] { long.class, long.class }, timeWindow, txGap);\r\n    final Set<FederationNamespaceInfo> nss = namenodeResolver.getNamespaces();\r\n    Map<FederationNamespaceInfo, Boolean> ret = rpcClient.invokeConcurrent(nss, method, true, false, boolean.class);\r\n    boolean success = true;\r\n    for (boolean s : ret.values()) {\r\n        if (!s) {\r\n            success = false;\r\n            break;\r\n        }\r\n    }\r\n    return success;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "rollEdits",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "long rollEdits() throws IOException\n{\r\n    rpcServer.checkOperation(NameNode.OperationCategory.WRITE);\r\n    RemoteMethod method = new RemoteMethod(\"rollEdits\", new Class<?>[] {});\r\n    final Set<FederationNamespaceInfo> nss = namenodeResolver.getNamespaces();\r\n    Map<FederationNamespaceInfo, Long> ret = rpcClient.invokeConcurrent(nss, method, true, false, long.class);\r\n    long txid = 0;\r\n    for (long t : ret.values()) {\r\n        if (t > txid) {\r\n            txid = t;\r\n        }\r\n    }\r\n    return txid;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "refreshNodes",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void refreshNodes() throws IOException\n{\r\n    rpcServer.checkOperation(NameNode.OperationCategory.UNCHECKED);\r\n    RemoteMethod method = new RemoteMethod(\"refreshNodes\", new Class<?>[] {});\r\n    final Set<FederationNamespaceInfo> nss = namenodeResolver.getNamespaces();\r\n    rpcClient.invokeConcurrent(nss, method, true, true);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "finalizeUpgrade",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void finalizeUpgrade() throws IOException\n{\r\n    rpcServer.checkOperation(NameNode.OperationCategory.UNCHECKED);\r\n    RemoteMethod method = new RemoteMethod(\"finalizeUpgrade\", new Class<?>[] {});\r\n    final Set<FederationNamespaceInfo> nss = namenodeResolver.getNamespaces();\r\n    rpcClient.invokeConcurrent(nss, method, true, false);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "upgradeStatus",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "boolean upgradeStatus() throws IOException\n{\r\n    String methodName = RouterRpcServer.getMethodName();\r\n    throw new UnsupportedOperationException(\"Operation \\\"\" + methodName + \"\\\" is not supported\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "rollingUpgrade",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "RollingUpgradeInfo rollingUpgrade(HdfsConstants.RollingUpgradeAction action) throws IOException\n{\r\n    rpcServer.checkOperation(NameNode.OperationCategory.READ);\r\n    RemoteMethod method = new RemoteMethod(\"rollingUpgrade\", new Class<?>[] { HdfsConstants.RollingUpgradeAction.class }, action);\r\n    final Set<FederationNamespaceInfo> nss = namenodeResolver.getNamespaces();\r\n    Map<FederationNamespaceInfo, RollingUpgradeInfo> ret = rpcClient.invokeConcurrent(nss, method, true, false, RollingUpgradeInfo.class);\r\n    RollingUpgradeInfo info = null;\r\n    for (RollingUpgradeInfo infoNs : ret.values()) {\r\n        if (info == null && infoNs != null) {\r\n            info = infoNs;\r\n        }\r\n    }\r\n    return info;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "metaSave",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void metaSave(String filename) throws IOException\n{\r\n    rpcServer.checkOperation(NameNode.OperationCategory.UNCHECKED);\r\n    RemoteMethod method = new RemoteMethod(\"metaSave\", new Class<?>[] { String.class }, filename);\r\n    final Set<FederationNamespaceInfo> nss = namenodeResolver.getNamespaces();\r\n    rpcClient.invokeConcurrent(nss, method, true, false);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "listCorruptFileBlocks",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "CorruptFileBlocks listCorruptFileBlocks(String path, String cookie) throws IOException\n{\r\n    rpcServer.checkOperation(NameNode.OperationCategory.READ);\r\n    final List<RemoteLocation> locations = rpcServer.getLocationsForPath(path, false, false);\r\n    RemoteMethod method = new RemoteMethod(\"listCorruptFileBlocks\", new Class<?>[] { String.class, String.class }, new RemoteParam(), cookie);\r\n    return rpcClient.invokeSequential(locations, method, CorruptFileBlocks.class, null);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "setBalancerBandwidth",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void setBalancerBandwidth(long bandwidth) throws IOException\n{\r\n    rpcServer.checkOperation(NameNode.OperationCategory.UNCHECKED);\r\n    RemoteMethod method = new RemoteMethod(\"setBalancerBandwidth\", new Class<?>[] { long.class }, bandwidth);\r\n    final Set<FederationNamespaceInfo> nss = namenodeResolver.getNamespaces();\r\n    rpcClient.invokeConcurrent(nss, method, true, false);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "getContentSummary",
  "errType" : [ "Exception" ],
  "containingMethodsNum" : 13,
  "sourceCodeText" : "ContentSummary getContentSummary(String path) throws IOException\n{\r\n    rpcServer.checkOperation(NameNode.OperationCategory.READ);\r\n    final Collection<ContentSummary> summaries = new ArrayList<>();\r\n    final List<RemoteLocation> locations = rpcServer.getLocationsForPath(path, false, false);\r\n    final RemoteMethod method = new RemoteMethod(\"getContentSummary\", new Class<?>[] { String.class }, new RemoteParam());\r\n    final List<RemoteResult<RemoteLocation, ContentSummary>> results = rpcClient.invokeConcurrent(locations, method, false, -1, ContentSummary.class);\r\n    FileNotFoundException notFoundException = null;\r\n    for (RemoteResult<RemoteLocation, ContentSummary> result : results) {\r\n        if (result.hasException()) {\r\n            IOException ioe = result.getException();\r\n            if (ioe instanceof FileNotFoundException) {\r\n                notFoundException = (FileNotFoundException) ioe;\r\n            } else if (!allowPartialList) {\r\n                throw ioe;\r\n            }\r\n        } else if (result.getResult() != null) {\r\n            summaries.add(result.getResult());\r\n        }\r\n    }\r\n    final List<String> children = subclusterResolver.getMountPoints(path);\r\n    if (children != null) {\r\n        for (String child : children) {\r\n            Path childPath = new Path(path, child);\r\n            try {\r\n                ContentSummary mountSummary = getContentSummary(childPath.toString());\r\n                if (mountSummary != null) {\r\n                    summaries.add(mountSummary);\r\n                }\r\n            } catch (Exception e) {\r\n                LOG.error(\"Cannot get content summary for mount {}: {}\", childPath, e.getMessage());\r\n            }\r\n        }\r\n    }\r\n    if (summaries.isEmpty() && notFoundException != null) {\r\n        throw notFoundException;\r\n    }\r\n    return aggregateContentSummary(summaries);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "fsync",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void fsync(String src, long fileId, String clientName, long lastBlockLength) throws IOException\n{\r\n    rpcServer.checkOperation(NameNode.OperationCategory.WRITE);\r\n    final List<RemoteLocation> locations = rpcServer.getLocationsForPath(src, true, false);\r\n    RemoteMethod method = new RemoteMethod(\"fsync\", new Class<?>[] { String.class, long.class, String.class, long.class }, new RemoteParam(), fileId, clientName, lastBlockLength);\r\n    rpcClient.invokeSequential(locations, method);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "setTimes",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void setTimes(String src, long mtime, long atime) throws IOException\n{\r\n    rpcServer.checkOperation(NameNode.OperationCategory.WRITE);\r\n    final List<RemoteLocation> locations = rpcServer.getLocationsForPath(src, false, false);\r\n    RemoteMethod method = new RemoteMethod(\"setTimes\", new Class<?>[] { String.class, long.class, long.class }, new RemoteParam(), mtime, atime);\r\n    rpcClient.invokeSequential(locations, method);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "createSymlink",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void createSymlink(String target, String link, FsPermission dirPerms, boolean createParent) throws IOException\n{\r\n    rpcServer.checkOperation(NameNode.OperationCategory.WRITE);\r\n    final List<RemoteLocation> targetLocations = rpcServer.getLocationsForPath(target, true);\r\n    final List<RemoteLocation> linkLocations = rpcServer.getLocationsForPath(link, true);\r\n    RemoteLocation linkLocation = linkLocations.get(0);\r\n    RemoteMethod method = new RemoteMethod(\"createSymlink\", new Class<?>[] { String.class, String.class, FsPermission.class, boolean.class }, new RemoteParam(), linkLocation.getDest(), dirPerms, createParent);\r\n    rpcClient.invokeSequential(targetLocations, method);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "getLinkTarget",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "String getLinkTarget(String path) throws IOException\n{\r\n    rpcServer.checkOperation(NameNode.OperationCategory.READ);\r\n    final List<RemoteLocation> locations = rpcServer.getLocationsForPath(path, false, false);\r\n    RemoteMethod method = new RemoteMethod(\"getLinkTarget\", new Class<?>[] { String.class }, new RemoteParam());\r\n    return rpcClient.invokeSequential(locations, method, String.class, null);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "allowSnapshot",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void allowSnapshot(String snapshotRoot) throws IOException\n{\r\n    snapshotProto.allowSnapshot(snapshotRoot);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "disallowSnapshot",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void disallowSnapshot(String snapshot) throws IOException\n{\r\n    snapshotProto.disallowSnapshot(snapshot);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "renameSnapshot",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void renameSnapshot(String snapshotRoot, String snapshotOldName, String snapshotNewName) throws IOException\n{\r\n    snapshotProto.renameSnapshot(snapshotRoot, snapshotOldName, snapshotNewName);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "getSnapshottableDirListing",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "SnapshottableDirectoryStatus[] getSnapshottableDirListing() throws IOException\n{\r\n    return snapshotProto.getSnapshottableDirListing();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "getSnapshotListing",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "SnapshotStatus[] getSnapshotListing(String snapshotRoot) throws IOException\n{\r\n    return snapshotProto.getSnapshotListing(snapshotRoot);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "getSnapshotDiffReport",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "SnapshotDiffReport getSnapshotDiffReport(String snapshotRoot, String earlierSnapshotName, String laterSnapshotName) throws IOException\n{\r\n    return snapshotProto.getSnapshotDiffReport(snapshotRoot, earlierSnapshotName, laterSnapshotName);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "getSnapshotDiffReportListing",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "SnapshotDiffReportListing getSnapshotDiffReportListing(String snapshotRoot, String earlierSnapshotName, String laterSnapshotName, byte[] startPath, int index) throws IOException\n{\r\n    return snapshotProto.getSnapshotDiffReportListing(snapshotRoot, earlierSnapshotName, laterSnapshotName, startPath, index);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "addCacheDirective",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "long addCacheDirective(CacheDirectiveInfo path, EnumSet<CacheFlag> flags) throws IOException\n{\r\n    return routerCacheAdmin.addCacheDirective(path, flags);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "modifyCacheDirective",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void modifyCacheDirective(CacheDirectiveInfo directive, EnumSet<CacheFlag> flags) throws IOException\n{\r\n    routerCacheAdmin.modifyCacheDirective(directive, flags);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "removeCacheDirective",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void removeCacheDirective(long id) throws IOException\n{\r\n    routerCacheAdmin.removeCacheDirective(id);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "listCacheDirectives",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "BatchedEntries<CacheDirectiveEntry> listCacheDirectives(long prevId, CacheDirectiveInfo filter) throws IOException\n{\r\n    return routerCacheAdmin.listCacheDirectives(prevId, filter);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "addCachePool",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void addCachePool(CachePoolInfo info) throws IOException\n{\r\n    routerCacheAdmin.addCachePool(info);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "modifyCachePool",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void modifyCachePool(CachePoolInfo info) throws IOException\n{\r\n    routerCacheAdmin.modifyCachePool(info);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "removeCachePool",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void removeCachePool(String cachePoolName) throws IOException\n{\r\n    routerCacheAdmin.removeCachePool(cachePoolName);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "listCachePools",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "BatchedEntries<CachePoolEntry> listCachePools(String prevKey) throws IOException\n{\r\n    return routerCacheAdmin.listCachePools(prevKey);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "modifyAclEntries",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void modifyAclEntries(String src, List<AclEntry> aclSpec) throws IOException\n{\r\n    rpcServer.checkOperation(NameNode.OperationCategory.WRITE);\r\n    final List<RemoteLocation> locations = rpcServer.getLocationsForPath(src, false, false);\r\n    RemoteMethod method = new RemoteMethod(\"modifyAclEntries\", new Class<?>[] { String.class, List.class }, new RemoteParam(), aclSpec);\r\n    if (rpcServer.isInvokeConcurrent(src)) {\r\n        rpcClient.invokeConcurrent(locations, method);\r\n    } else {\r\n        rpcClient.invokeSequential(locations, method);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "removeAclEntries",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void removeAclEntries(String src, List<AclEntry> aclSpec) throws IOException\n{\r\n    rpcServer.checkOperation(NameNode.OperationCategory.WRITE);\r\n    final List<RemoteLocation> locations = rpcServer.getLocationsForPath(src, false, false);\r\n    RemoteMethod method = new RemoteMethod(\"removeAclEntries\", new Class<?>[] { String.class, List.class }, new RemoteParam(), aclSpec);\r\n    if (rpcServer.isInvokeConcurrent(src)) {\r\n        rpcClient.invokeConcurrent(locations, method);\r\n    } else {\r\n        rpcClient.invokeSequential(locations, method);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "removeDefaultAcl",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void removeDefaultAcl(String src) throws IOException\n{\r\n    rpcServer.checkOperation(NameNode.OperationCategory.WRITE);\r\n    final List<RemoteLocation> locations = rpcServer.getLocationsForPath(src, false, false);\r\n    RemoteMethod method = new RemoteMethod(\"removeDefaultAcl\", new Class<?>[] { String.class }, new RemoteParam());\r\n    if (rpcServer.isInvokeConcurrent(src)) {\r\n        rpcClient.invokeConcurrent(locations, method);\r\n    } else {\r\n        rpcClient.invokeSequential(locations, method);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "removeAcl",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void removeAcl(String src) throws IOException\n{\r\n    rpcServer.checkOperation(NameNode.OperationCategory.WRITE);\r\n    final List<RemoteLocation> locations = rpcServer.getLocationsForPath(src, false, false);\r\n    RemoteMethod method = new RemoteMethod(\"removeAcl\", new Class<?>[] { String.class }, new RemoteParam());\r\n    if (rpcServer.isInvokeConcurrent(src)) {\r\n        rpcClient.invokeConcurrent(locations, method);\r\n    } else {\r\n        rpcClient.invokeSequential(locations, method);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "setAcl",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void setAcl(String src, List<AclEntry> aclSpec) throws IOException\n{\r\n    rpcServer.checkOperation(NameNode.OperationCategory.WRITE);\r\n    final List<RemoteLocation> locations = rpcServer.getLocationsForPath(src, false, false);\r\n    RemoteMethod method = new RemoteMethod(\"setAcl\", new Class<?>[] { String.class, List.class }, new RemoteParam(), aclSpec);\r\n    if (rpcServer.isInvokeConcurrent(src)) {\r\n        rpcClient.invokeConcurrent(locations, method);\r\n    } else {\r\n        rpcClient.invokeSequential(locations, method);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "getAclStatus",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "AclStatus getAclStatus(String src) throws IOException\n{\r\n    rpcServer.checkOperation(NameNode.OperationCategory.READ);\r\n    final List<RemoteLocation> locations = rpcServer.getLocationsForPath(src, false, false);\r\n    RemoteMethod method = new RemoteMethod(\"getAclStatus\", new Class<?>[] { String.class }, new RemoteParam());\r\n    return rpcClient.invokeSequential(locations, method, AclStatus.class, null);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "createEncryptionZone",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void createEncryptionZone(String src, String keyName) throws IOException\n{\r\n    rpcServer.checkOperation(NameNode.OperationCategory.WRITE);\r\n    final List<RemoteLocation> locations = rpcServer.getLocationsForPath(src, false, false);\r\n    RemoteMethod method = new RemoteMethod(\"createEncryptionZone\", new Class<?>[] { String.class, String.class }, new RemoteParam(), keyName);\r\n    rpcClient.invokeSequential(locations, method);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "getEZForPath",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "EncryptionZone getEZForPath(String src) throws IOException\n{\r\n    rpcServer.checkOperation(NameNode.OperationCategory.READ);\r\n    final List<RemoteLocation> locations = rpcServer.getLocationsForPath(src, false, false);\r\n    RemoteMethod method = new RemoteMethod(\"getEZForPath\", new Class<?>[] { String.class }, new RemoteParam());\r\n    return rpcClient.invokeSequential(locations, method, EncryptionZone.class, null);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "listEncryptionZones",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "BatchedEntries<EncryptionZone> listEncryptionZones(long prevId) throws IOException\n{\r\n    rpcServer.checkOperation(NameNode.OperationCategory.READ, false);\r\n    return null;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "reencryptEncryptionZone",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void reencryptEncryptionZone(String zone, HdfsConstants.ReencryptAction action) throws IOException\n{\r\n    rpcServer.checkOperation(NameNode.OperationCategory.WRITE, false);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "listReencryptionStatus",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "BatchedEntries<ZoneReencryptionStatus> listReencryptionStatus(long prevId) throws IOException\n{\r\n    rpcServer.checkOperation(NameNode.OperationCategory.READ, false);\r\n    return null;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "setXAttr",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void setXAttr(String src, XAttr xAttr, EnumSet<XAttrSetFlag> flag) throws IOException\n{\r\n    rpcServer.checkOperation(NameNode.OperationCategory.WRITE);\r\n    final List<RemoteLocation> locations = rpcServer.getLocationsForPath(src, false, false);\r\n    RemoteMethod method = new RemoteMethod(\"setXAttr\", new Class<?>[] { String.class, XAttr.class, EnumSet.class }, new RemoteParam(), xAttr, flag);\r\n    if (rpcServer.isInvokeConcurrent(src)) {\r\n        rpcClient.invokeConcurrent(locations, method);\r\n    } else {\r\n        rpcClient.invokeSequential(locations, method);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "getXAttrs",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "List<XAttr> getXAttrs(String src, List<XAttr> xAttrs) throws IOException\n{\r\n    rpcServer.checkOperation(NameNode.OperationCategory.READ);\r\n    final List<RemoteLocation> locations = rpcServer.getLocationsForPath(src, false, false);\r\n    RemoteMethod method = new RemoteMethod(\"getXAttrs\", new Class<?>[] { String.class, List.class }, new RemoteParam(), xAttrs);\r\n    return (List<XAttr>) rpcClient.invokeSequential(locations, method, List.class, null);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "listXAttrs",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "List<XAttr> listXAttrs(String src) throws IOException\n{\r\n    rpcServer.checkOperation(NameNode.OperationCategory.READ);\r\n    final List<RemoteLocation> locations = rpcServer.getLocationsForPath(src, false, false);\r\n    RemoteMethod method = new RemoteMethod(\"listXAttrs\", new Class<?>[] { String.class }, new RemoteParam());\r\n    return (List<XAttr>) rpcClient.invokeSequential(locations, method, List.class, null);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "removeXAttr",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void removeXAttr(String src, XAttr xAttr) throws IOException\n{\r\n    rpcServer.checkOperation(NameNode.OperationCategory.WRITE);\r\n    final List<RemoteLocation> locations = rpcServer.getLocationsForPath(src, false, false);\r\n    RemoteMethod method = new RemoteMethod(\"removeXAttr\", new Class<?>[] { String.class, XAttr.class }, new RemoteParam(), xAttr);\r\n    if (rpcServer.isInvokeConcurrent(src)) {\r\n        rpcClient.invokeConcurrent(locations, method);\r\n    } else {\r\n        rpcClient.invokeSequential(locations, method);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "checkAccess",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void checkAccess(String path, FsAction mode) throws IOException\n{\r\n    rpcServer.checkOperation(NameNode.OperationCategory.READ);\r\n    final List<RemoteLocation> locations = rpcServer.getLocationsForPath(path, false, false);\r\n    RemoteMethod method = new RemoteMethod(\"checkAccess\", new Class<?>[] { String.class, FsAction.class }, new RemoteParam(), mode);\r\n    rpcClient.invokeSequential(locations, method);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "getCurrentEditLogTxid",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "long getCurrentEditLogTxid() throws IOException\n{\r\n    rpcServer.checkOperation(NameNode.OperationCategory.READ);\r\n    RemoteMethod method = new RemoteMethod(\"getCurrentEditLogTxid\", new Class<?>[] {});\r\n    final Set<FederationNamespaceInfo> nss = namenodeResolver.getNamespaces();\r\n    Map<FederationNamespaceInfo, Long> ret = rpcClient.invokeConcurrent(nss, method, true, false, long.class);\r\n    long txid = 0;\r\n    for (long t : ret.values()) {\r\n        if (t > txid) {\r\n            txid = t;\r\n        }\r\n    }\r\n    return txid;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "getEditsFromTxid",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "EventBatchList getEditsFromTxid(long txid) throws IOException\n{\r\n    rpcServer.checkOperation(NameNode.OperationCategory.READ, false);\r\n    return null;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "getDataEncryptionKey",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "DataEncryptionKey getDataEncryptionKey() throws IOException\n{\r\n    rpcServer.checkOperation(NameNode.OperationCategory.READ, false);\r\n    return null;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "createSnapshot",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String createSnapshot(String snapshotRoot, String snapshotName) throws IOException\n{\r\n    return snapshotProto.createSnapshot(snapshotRoot, snapshotName);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "deleteSnapshot",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void deleteSnapshot(String snapshotRoot, String snapshotName) throws IOException\n{\r\n    snapshotProto.deleteSnapshot(snapshotRoot, snapshotName);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "setQuota",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setQuota(String path, long namespaceQuota, long storagespaceQuota, StorageType type) throws IOException\n{\r\n    rpcServer.getQuotaModule().setQuota(path, namespaceQuota, storagespaceQuota, type, true);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "getQuotaUsage",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "QuotaUsage getQuotaUsage(String path) throws IOException\n{\r\n    return rpcServer.getQuotaModule().getQuotaUsage(path);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "reportBadBlocks",
  "errType" : null,
  "containingMethodsNum" : 10,
  "sourceCodeText" : "void reportBadBlocks(LocatedBlock[] blocks) throws IOException\n{\r\n    rpcServer.checkOperation(NameNode.OperationCategory.WRITE);\r\n    Map<String, List<LocatedBlock>> blockLocations = new HashMap<>();\r\n    for (LocatedBlock block : blocks) {\r\n        String bpId = block.getBlock().getBlockPoolId();\r\n        List<LocatedBlock> bpBlocks = blockLocations.get(bpId);\r\n        if (bpBlocks == null) {\r\n            bpBlocks = new LinkedList<>();\r\n            blockLocations.put(bpId, bpBlocks);\r\n        }\r\n        bpBlocks.add(block);\r\n    }\r\n    for (Map.Entry<String, List<LocatedBlock>> entry : blockLocations.entrySet()) {\r\n        String bpId = entry.getKey();\r\n        List<LocatedBlock> bpBlocks = entry.getValue();\r\n        LocatedBlock[] bpBlocksArray = bpBlocks.toArray(new LocatedBlock[bpBlocks.size()]);\r\n        RemoteMethod method = new RemoteMethod(\"reportBadBlocks\", new Class<?>[] { LocatedBlock[].class }, new Object[] { bpBlocksArray });\r\n        rpcClient.invokeSingleBlockPool(bpId, method);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "unsetStoragePolicy",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void unsetStoragePolicy(String src) throws IOException\n{\r\n    storagePolicy.unsetStoragePolicy(src);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "getStoragePolicy",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "BlockStoragePolicy getStoragePolicy(String path) throws IOException\n{\r\n    return storagePolicy.getStoragePolicy(path);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "getErasureCodingPolicies",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "ErasureCodingPolicyInfo[] getErasureCodingPolicies() throws IOException\n{\r\n    return erasureCoding.getErasureCodingPolicies();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "getErasureCodingCodecs",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Map<String, String> getErasureCodingCodecs() throws IOException\n{\r\n    return erasureCoding.getErasureCodingCodecs();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "addErasureCodingPolicies",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "AddErasureCodingPolicyResponse[] addErasureCodingPolicies(ErasureCodingPolicy[] policies) throws IOException\n{\r\n    return erasureCoding.addErasureCodingPolicies(policies);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "removeErasureCodingPolicy",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void removeErasureCodingPolicy(String ecPolicyName) throws IOException\n{\r\n    erasureCoding.removeErasureCodingPolicy(ecPolicyName);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "disableErasureCodingPolicy",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void disableErasureCodingPolicy(String ecPolicyName) throws IOException\n{\r\n    erasureCoding.disableErasureCodingPolicy(ecPolicyName);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "enableErasureCodingPolicy",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void enableErasureCodingPolicy(String ecPolicyName) throws IOException\n{\r\n    erasureCoding.enableErasureCodingPolicy(ecPolicyName);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "getErasureCodingPolicy",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "ErasureCodingPolicy getErasureCodingPolicy(String src) throws IOException\n{\r\n    return erasureCoding.getErasureCodingPolicy(src);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "setErasureCodingPolicy",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setErasureCodingPolicy(String src, String ecPolicyName) throws IOException\n{\r\n    erasureCoding.setErasureCodingPolicy(src, ecPolicyName);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "unsetErasureCodingPolicy",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void unsetErasureCodingPolicy(String src) throws IOException\n{\r\n    erasureCoding.unsetErasureCodingPolicy(src);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "getECTopologyResultForPolicies",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "ECTopologyVerifierResult getECTopologyResultForPolicies(String... policyNames) throws IOException\n{\r\n    rpcServer.checkOperation(NameNode.OperationCategory.UNCHECKED, true);\r\n    return erasureCoding.getECTopologyResultForPolicies(policyNames);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "getECBlockGroupStats",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "ECBlockGroupStats getECBlockGroupStats() throws IOException\n{\r\n    return erasureCoding.getECBlockGroupStats();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "getReplicatedBlockStats",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "ReplicatedBlockStats getReplicatedBlockStats() throws IOException\n{\r\n    rpcServer.checkOperation(NameNode.OperationCategory.READ);\r\n    RemoteMethod method = new RemoteMethod(\"getReplicatedBlockStats\");\r\n    Set<FederationNamespaceInfo> nss = namenodeResolver.getNamespaces();\r\n    Map<FederationNamespaceInfo, ReplicatedBlockStats> ret = rpcClient.invokeConcurrent(nss, method, true, false, ReplicatedBlockStats.class);\r\n    return ReplicatedBlockStats.merge(ret.values());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "listOpenFiles",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "BatchedEntries<OpenFileEntry> listOpenFiles(long prevId) throws IOException\n{\r\n    return listOpenFiles(prevId, EnumSet.of(OpenFilesIterator.OpenFilesType.ALL_OPEN_FILES), OpenFilesIterator.FILTER_PATH_DEFAULT);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "listOpenFiles",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "BatchedEntries<OpenFileEntry> listOpenFiles(long prevId, EnumSet<OpenFilesIterator.OpenFilesType> openFilesTypes, String path) throws IOException\n{\r\n    rpcServer.checkOperation(NameNode.OperationCategory.READ, false);\r\n    return null;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "msync",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void msync() throws IOException\n{\r\n    rpcServer.checkOperation(NameNode.OperationCategory.READ, false);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "satisfyStoragePolicy",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void satisfyStoragePolicy(String path) throws IOException\n{\r\n    storagePolicy.satisfyStoragePolicy(path);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "getHAServiceState",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "HAServiceProtocol.HAServiceState getHAServiceState()\n{\r\n    if (rpcServer.isSafeMode()) {\r\n        return HAServiceProtocol.HAServiceState.STANDBY;\r\n    }\r\n    return HAServiceProtocol.HAServiceState.ACTIVE;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "getRenameDestinations",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "RemoteParam getRenameDestinations(final List<RemoteLocation> srcLocations, final List<RemoteLocation> dstLocations) throws IOException\n{\r\n    final Map<RemoteLocation, String> dstMap = new HashMap<>();\r\n    Iterator<RemoteLocation> iterator = srcLocations.iterator();\r\n    while (iterator.hasNext()) {\r\n        RemoteLocation srcLocation = iterator.next();\r\n        RemoteLocation eligibleDst = getFirstMatchingLocation(srcLocation, dstLocations);\r\n        if (eligibleDst != null) {\r\n            dstMap.put(srcLocation, eligibleDst.getDest());\r\n        } else {\r\n            iterator.remove();\r\n        }\r\n    }\r\n    return new RemoteParam(dstMap);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "getFirstMatchingLocation",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "RemoteLocation getFirstMatchingLocation(RemoteLocation location, List<RemoteLocation> locations)\n{\r\n    for (RemoteLocation loc : locations) {\r\n        if (loc.getNameserviceId().equals(location.getNameserviceId())) {\r\n            return loc;\r\n        }\r\n    }\r\n    return null;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "aggregateContentSummary",
  "errType" : null,
  "containingMethodsNum" : 11,
  "sourceCodeText" : "ContentSummary aggregateContentSummary(Collection<ContentSummary> summaries)\n{\r\n    if (summaries.size() == 1) {\r\n        return summaries.iterator().next();\r\n    }\r\n    long length = 0;\r\n    long fileCount = 0;\r\n    long directoryCount = 0;\r\n    long quota = 0;\r\n    long spaceConsumed = 0;\r\n    long spaceQuota = 0;\r\n    String ecPolicy = \"\";\r\n    for (ContentSummary summary : summaries) {\r\n        length += summary.getLength();\r\n        fileCount += summary.getFileCount();\r\n        directoryCount += summary.getDirectoryCount();\r\n        quota = summary.getQuota();\r\n        spaceConsumed += summary.getSpaceConsumed();\r\n        spaceQuota = summary.getSpaceQuota();\r\n        if (ecPolicy.isEmpty()) {\r\n            ecPolicy = summary.getErasureCodingPolicy();\r\n        }\r\n    }\r\n    ContentSummary ret = new ContentSummary.Builder().length(length).fileCount(fileCount).directoryCount(directoryCount).quota(quota).spaceConsumed(spaceConsumed).spaceQuota(spaceQuota).erasureCodingPolicy(ecPolicy).build();\r\n    return ret;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "getFileInfoAll",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "HdfsFileStatus getFileInfoAll(final List<RemoteLocation> locations, final RemoteMethod method) throws IOException\n{\r\n    return getFileInfoAll(locations, method, -1);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "getFileInfoAll",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "HdfsFileStatus getFileInfoAll(final List<RemoteLocation> locations, final RemoteMethod method, long timeOutMs) throws IOException\n{\r\n    Map<RemoteLocation, HdfsFileStatus> results = rpcClient.invokeConcurrent(locations, method, false, false, timeOutMs, HdfsFileStatus.class);\r\n    int children = 0;\r\n    HdfsFileStatus dirStatus = null;\r\n    for (RemoteLocation loc : locations) {\r\n        HdfsFileStatus fileStatus = results.get(loc);\r\n        if (fileStatus != null) {\r\n            children += fileStatus.getChildrenNum();\r\n            if (!fileStatus.isDirectory()) {\r\n                return fileStatus;\r\n            } else if (dirStatus == null) {\r\n                dirStatus = fileStatus;\r\n            }\r\n        }\r\n    }\r\n    if (dirStatus != null) {\r\n        return updateMountPointStatus(dirStatus, children);\r\n    }\r\n    return null;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "getParentPermission",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "FsPermission getParentPermission(final FsPermission mask)\n{\r\n    FsPermission ret = new FsPermission(mask.getUserAction().or(FsAction.WRITE_EXECUTE), mask.getGroupAction(), mask.getOtherAction());\r\n    return ret;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "getMountPointStatus",
  "errType" : [ "IOException", "IOException" ],
  "containingMethodsNum" : 23,
  "sourceCodeText" : "HdfsFileStatus getMountPointStatus(String name, int childrenNum, long date)\n{\r\n    long modTime = date;\r\n    long accessTime = date;\r\n    FsPermission permission = FsPermission.getDirDefault();\r\n    String owner = this.superUser;\r\n    String group = this.superGroup;\r\n    EnumSet<HdfsFileStatus.Flags> flags = EnumSet.noneOf(HdfsFileStatus.Flags.class);\r\n    if (subclusterResolver instanceof MountTableResolver) {\r\n        try {\r\n            String mName = name.startsWith(\"/\") ? name : \"/\" + name;\r\n            MountTableResolver mountTable = (MountTableResolver) subclusterResolver;\r\n            MountTable entry = mountTable.getMountPoint(mName);\r\n            if (entry != null) {\r\n                permission = entry.getMode();\r\n                owner = entry.getOwnerName();\r\n                group = entry.getGroupName();\r\n                RemoteMethod method = new RemoteMethod(\"getFileInfo\", new Class<?>[] { String.class }, new RemoteParam());\r\n                HdfsFileStatus fInfo = getFileInfoAll(entry.getDestinations(), method, mountStatusTimeOut);\r\n                if (fInfo != null) {\r\n                    permission = fInfo.getPermission();\r\n                    owner = fInfo.getOwner();\r\n                    group = fInfo.getGroup();\r\n                    childrenNum = fInfo.getChildrenNum();\r\n                    flags = DFSUtil.getFlags(fInfo.isEncrypted(), fInfo.isErasureCoded(), fInfo.isSnapshotEnabled(), fInfo.hasAcl());\r\n                }\r\n            }\r\n        } catch (IOException e) {\r\n            LOG.error(\"Cannot get mount point: {}\", e.getMessage());\r\n        }\r\n    } else {\r\n        try {\r\n            UserGroupInformation ugi = RouterRpcServer.getRemoteUser();\r\n            owner = ugi.getUserName();\r\n            group = ugi.getPrimaryGroupName();\r\n        } catch (IOException e) {\r\n            String msg = \"Cannot get remote user: \" + e.getMessage();\r\n            if (UserGroupInformation.isSecurityEnabled()) {\r\n                LOG.error(msg);\r\n            } else {\r\n                LOG.debug(msg);\r\n            }\r\n        }\r\n    }\r\n    long inodeId = 0;\r\n    Path path = new Path(name);\r\n    String nameStr = path.getName();\r\n    return new HdfsFileStatus.Builder().isdir(true).mtime(modTime).atime(accessTime).perm(permission).owner(owner).group(group).symlink(new byte[0]).path(DFSUtil.string2Bytes(nameStr)).fileId(inodeId).children(childrenNum).flags(flags).build();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 2,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "getMountPointDates",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 4,
  "sourceCodeText" : "Map<String, Long> getMountPointDates(String path)\n{\r\n    Map<String, Long> ret = new TreeMap<>();\r\n    if (subclusterResolver instanceof MountTableResolver) {\r\n        try {\r\n            final List<String> children = subclusterResolver.getMountPoints(path);\r\n            for (String child : children) {\r\n                Long modTime = getModifiedTime(ret, path, child);\r\n                ret.put(child, modTime);\r\n            }\r\n        } catch (IOException e) {\r\n            LOG.error(\"Cannot get mount point\", e);\r\n        }\r\n    }\r\n    return ret;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "getModifiedTime",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 9,
  "sourceCodeText" : "long getModifiedTime(Map<String, Long> ret, String path, String child)\n{\r\n    MountTableResolver mountTable = (MountTableResolver) subclusterResolver;\r\n    String srcPath;\r\n    if (path.equals(Path.SEPARATOR)) {\r\n        srcPath = Path.SEPARATOR + child;\r\n    } else {\r\n        srcPath = path + Path.SEPARATOR + child;\r\n    }\r\n    Long modTime = 0L;\r\n    try {\r\n        MountTable entry = mountTable.getMountPoint(srcPath);\r\n        if (entry == null) {\r\n            List<MountTable> entries = mountTable.getMounts(srcPath);\r\n            for (MountTable eachEntry : entries) {\r\n                if (ret.get(child) == null || ret.get(child) < eachEntry.getDateModified()) {\r\n                    modTime = eachEntry.getDateModified();\r\n                }\r\n            }\r\n        } else {\r\n            modTime = entry.getDateModified();\r\n        }\r\n    } catch (IOException e) {\r\n        LOG.error(\"Cannot get mount point\", e);\r\n    }\r\n    return modTime;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "getListingInt",
  "errType" : [ "RouterResolveException" ],
  "containingMethodsNum" : 5,
  "sourceCodeText" : "List<RemoteResult<RemoteLocation, DirectoryListing>> getListingInt(String src, byte[] startAfter, boolean needLocation) throws IOException\n{\r\n    try {\r\n        List<RemoteLocation> locations = rpcServer.getLocationsForPath(src, false, false);\r\n        if (locations.isEmpty()) {\r\n            return new ArrayList<>();\r\n        }\r\n        RemoteMethod method = new RemoteMethod(\"getListing\", new Class<?>[] { String.class, startAfter.getClass(), boolean.class }, new RemoteParam(), startAfter, needLocation);\r\n        List<RemoteResult<RemoteLocation, DirectoryListing>> listings = rpcClient.invokeConcurrent(locations, method, false, -1, DirectoryListing.class);\r\n        return listings;\r\n    } catch (RouterResolveException e) {\r\n        LOG.debug(\"Cannot get locations for {}, {}.\", src, e.getMessage());\r\n        return new ArrayList<>();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "shouldAddMountPoint",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "boolean shouldAddMountPoint(String mountPoint, String lastEntry, byte[] startAfter, int remainingEntries)\n{\r\n    if (mountPoint.compareTo(DFSUtil.bytes2String(startAfter)) > 0 && mountPoint.compareTo(lastEntry) <= 0) {\r\n        return true;\r\n    }\r\n    if (remainingEntries == 0 && mountPoint.compareTo(lastEntry) >= 0) {\r\n        return true;\r\n    }\r\n    return false;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "isMultiDestDirectory",
  "errType" : [ "UnresolvedPathException" ],
  "containingMethodsNum" : 6,
  "sourceCodeText" : "boolean isMultiDestDirectory(String src) throws IOException\n{\r\n    try {\r\n        if (rpcServer.isPathAll(src)) {\r\n            List<RemoteLocation> locations;\r\n            locations = rpcServer.getLocationsForPath(src, false, false);\r\n            RemoteMethod method = new RemoteMethod(\"getFileInfo\", new Class<?>[] { String.class }, new RemoteParam());\r\n            HdfsFileStatus fileStatus = rpcClient.invokeSequential(locations, method, HdfsFileStatus.class, null);\r\n            if (fileStatus != null) {\r\n                return fileStatus.isDirectory();\r\n            } else {\r\n                LOG.debug(\"The destination {} doesn't exist.\", src);\r\n            }\r\n        }\r\n    } catch (UnresolvedPathException e) {\r\n        LOG.debug(\"The destination {} is a symlink.\", src);\r\n    }\r\n    return false;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "getRouterFederationRenameCount",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "int getRouterFederationRenameCount()\n{\r\n    return rbfRename.getRouterFederationRenameCount();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\resolver",
  "methodName" : "registerCacheExternal",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void registerCacheExternal()\n{\r\n    if (this.stateStore != null) {\r\n        this.stateStore.registerCacheExternal(this);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\resolver",
  "methodName" : "initDefaultNameService",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void initDefaultNameService(Configuration conf)\n{\r\n    this.defaultNSEnable = conf.getBoolean(DFS_ROUTER_DEFAULT_NAMESERVICE_ENABLE, DFS_ROUTER_DEFAULT_NAMESERVICE_ENABLE_DEFAULT);\r\n    if (!this.defaultNSEnable) {\r\n        LOG.warn(\"Default name service is disabled.\");\r\n        return;\r\n    }\r\n    this.defaultNameService = conf.get(DFS_ROUTER_DEFAULT_NAMESERVICE, \"\");\r\n    if (this.defaultNameService.equals(\"\")) {\r\n        this.defaultNSEnable = false;\r\n        LOG.warn(\"Default name service is not set.\");\r\n    } else {\r\n        LOG.info(\"Default name service: {}, enabled to read or write\", this.defaultNameService);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\resolver",
  "methodName" : "getRouter",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Router getRouter()\n{\r\n    return this.router;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\resolver",
  "methodName" : "getMountTableStore",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "MountTableStore getMountTableStore() throws IOException\n{\r\n    if (this.mountTableStore == null) {\r\n        this.mountTableStore = this.stateStore.getRegisteredRecordStore(MountTableStore.class);\r\n        if (this.mountTableStore == null) {\r\n            throw new IOException(\"State Store does not have an interface for \" + MountTableStore.class);\r\n        }\r\n    }\r\n    return this.mountTableStore;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\resolver",
  "methodName" : "addEntry",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void addEntry(final MountTable entry)\n{\r\n    writeLock.lock();\r\n    try {\r\n        String srcPath = entry.getSourcePath();\r\n        this.tree.put(srcPath, entry);\r\n        invalidateLocationCache(srcPath);\r\n    } finally {\r\n        writeLock.unlock();\r\n    }\r\n    this.init = true;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\resolver",
  "methodName" : "removeEntry",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void removeEntry(final String srcPath)\n{\r\n    writeLock.lock();\r\n    try {\r\n        this.tree.remove(srcPath);\r\n        invalidateLocationCache(srcPath);\r\n    } finally {\r\n        writeLock.unlock();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\resolver",
  "methodName" : "invalidateLocationCache",
  "errType" : null,
  "containingMethodsNum" : 18,
  "sourceCodeText" : "void invalidateLocationCache(final String path)\n{\r\n    LOG.debug(\"Invalidating {} from {}\", path, locationCache);\r\n    if (locationCache == null || locationCache.size() == 0) {\r\n        return;\r\n    }\r\n    ConcurrentMap<String, PathLocation> map = locationCache.asMap();\r\n    Set<Entry<String, PathLocation>> entries = map.entrySet();\r\n    Iterator<Entry<String, PathLocation>> it = entries.iterator();\r\n    while (it.hasNext()) {\r\n        Entry<String, PathLocation> entry = it.next();\r\n        String key = entry.getKey();\r\n        PathLocation loc = entry.getValue();\r\n        String src = loc.getSourcePath();\r\n        if (src != null) {\r\n            if (isParentEntry(key, path)) {\r\n                LOG.debug(\"Removing {}\", src);\r\n                it.remove();\r\n            }\r\n        } else {\r\n            String dest = loc.getDefaultLocation().getDest();\r\n            if (dest.startsWith(path)) {\r\n                LOG.debug(\"Removing default cache {}\", dest);\r\n                it.remove();\r\n            }\r\n        }\r\n    }\r\n    LOG.debug(\"Location cache after invalidation: {}\", locationCache);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\resolver",
  "methodName" : "refreshEntries",
  "errType" : null,
  "containingMethodsNum" : 23,
  "sourceCodeText" : "void refreshEntries(final Collection<MountTable> entries)\n{\r\n    writeLock.lock();\r\n    try {\r\n        Map<String, MountTable> newEntries = new ConcurrentHashMap<>();\r\n        for (MountTable entry : entries) {\r\n            String srcPath = entry.getSourcePath();\r\n            newEntries.put(srcPath, entry);\r\n        }\r\n        Set<String> oldEntries = new TreeSet<>(Collections.reverseOrder());\r\n        for (MountTable entry : getTreeValues(\"/\")) {\r\n            String srcPath = entry.getSourcePath();\r\n            oldEntries.add(srcPath);\r\n        }\r\n        for (String srcPath : oldEntries) {\r\n            if (!newEntries.containsKey(srcPath)) {\r\n                this.tree.remove(srcPath);\r\n                invalidateLocationCache(srcPath);\r\n                LOG.info(\"Removed stale mount point {} from resolver\", srcPath);\r\n            }\r\n        }\r\n        for (MountTable entry : entries) {\r\n            String srcPath = entry.getSourcePath();\r\n            if (!oldEntries.contains(srcPath)) {\r\n                this.tree.put(srcPath, entry);\r\n                invalidateLocationCache(srcPath);\r\n                LOG.info(\"Added new mount point {} to resolver\", srcPath);\r\n            } else {\r\n                MountTable existingEntry = this.tree.get(srcPath);\r\n                if (existingEntry != null && !existingEntry.equals(entry)) {\r\n                    LOG.info(\"Entry has changed from \\\"{}\\\" to \\\"{}\\\"\", existingEntry, entry);\r\n                    this.tree.put(srcPath, entry);\r\n                    invalidateLocationCache(srcPath);\r\n                    LOG.info(\"Updated mount point {} in resolver\", srcPath);\r\n                }\r\n            }\r\n        }\r\n    } finally {\r\n        writeLock.unlock();\r\n    }\r\n    this.init = true;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\resolver",
  "methodName" : "isTrashPath",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "boolean isTrashPath(String path) throws IOException\n{\r\n    Pattern pattern = Pattern.compile(\"^\" + getTrashRoot() + TRASH_PATTERN + \"/\");\r\n    return pattern.matcher(path).find();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\resolver",
  "methodName" : "getTrashRoot",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String getTrashRoot() throws IOException\n{\r\n    return FileSystem.USER_HOME_PREFIX + \"/\" + RouterRpcServer.getRemoteUser().getUserName() + \"/\" + FileSystem.TRASH_PREFIX;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\resolver",
  "methodName" : "subtractTrashCurrentPath",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String subtractTrashCurrentPath(String path) throws IOException\n{\r\n    return path.replaceAll(\"^\" + getTrashRoot() + TRASH_PATTERN, \"\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\resolver",
  "methodName" : "processTrashPath",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "String processTrashPath(String path) throws IOException\n{\r\n    if (isTrashPath(path)) {\r\n        return subtractTrashCurrentPath(path);\r\n    } else {\r\n        return path;\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\resolver",
  "methodName" : "loadCache",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 7,
  "sourceCodeText" : "boolean loadCache(boolean force)\n{\r\n    try {\r\n        MountTableStore mountTable = this.getMountTableStore();\r\n        mountTable.loadCache(force);\r\n        GetMountTableEntriesRequest request = GetMountTableEntriesRequest.newInstance(\"/\");\r\n        GetMountTableEntriesResponse response = mountTable.getMountTableEntries(request);\r\n        List<MountTable> records = response.getEntries();\r\n        refreshEntries(records);\r\n    } catch (IOException e) {\r\n        LOG.error(\"Cannot fetch mount table entries from State Store\", e);\r\n        return false;\r\n    }\r\n    return true;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\resolver",
  "methodName" : "clear",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void clear()\n{\r\n    LOG.info(\"Clearing all mount location caches\");\r\n    writeLock.lock();\r\n    try {\r\n        if (this.locationCache != null) {\r\n            this.locationCache.invalidateAll();\r\n        }\r\n        this.tree.clear();\r\n    } finally {\r\n        writeLock.unlock();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\resolver",
  "methodName" : "getDestinationForPath",
  "errType" : [ "ExecutionException" ],
  "containingMethodsNum" : 11,
  "sourceCodeText" : "PathLocation getDestinationForPath(final String path) throws IOException\n{\r\n    verifyMountTable();\r\n    PathLocation res;\r\n    readLock.lock();\r\n    try {\r\n        if (this.locationCache == null) {\r\n            res = lookupLocation(processTrashPath(path));\r\n        } else {\r\n            Callable<? extends PathLocation> meh = (Callable<PathLocation>) () -> lookupLocation(processTrashPath(path));\r\n            res = this.locationCache.get(processTrashPath(path), meh);\r\n        }\r\n        if (isTrashPath(path)) {\r\n            List<RemoteLocation> remoteLocations = new ArrayList<>();\r\n            for (RemoteLocation remoteLocation : res.getDestinations()) {\r\n                remoteLocations.add(new RemoteLocation(remoteLocation, path));\r\n            }\r\n            return new PathLocation(path, remoteLocations, res.getDestinationOrder());\r\n        } else {\r\n            return res;\r\n        }\r\n    } catch (ExecutionException e) {\r\n        Throwable cause = e.getCause();\r\n        final IOException ioe;\r\n        if (cause instanceof IOException) {\r\n            ioe = (IOException) cause;\r\n        } else {\r\n            ioe = new IOException(cause);\r\n        }\r\n        throw ioe;\r\n    } finally {\r\n        readLock.unlock();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\resolver",
  "methodName" : "lookupLocation",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "PathLocation lookupLocation(final String str) throws IOException\n{\r\n    PathLocation ret = null;\r\n    final String path = RouterAdmin.normalizeFileSystemPath(str);\r\n    MountTable entry = findDeepest(path);\r\n    if (entry != null) {\r\n        ret = buildLocation(path, entry);\r\n    } else {\r\n        if (!defaultNSEnable) {\r\n            throw new RouterResolveException(\"Cannot find locations for \" + path + \", because the default nameservice is disabled to read or write\");\r\n        }\r\n        RemoteLocation remoteLocation = new RemoteLocation(defaultNameService, path, path);\r\n        List<RemoteLocation> locations = Collections.singletonList(remoteLocation);\r\n        ret = new PathLocation(null, locations);\r\n    }\r\n    return ret;\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\resolver",
  "methodName" : "getMountPoint",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "MountTable getMountPoint(final String path) throws IOException\n{\r\n    verifyMountTable();\r\n    return findDeepest(RouterAdmin.normalizeFileSystemPath(path));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\resolver",
  "methodName" : "getMountPoints",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "List<String> getMountPoints(final String str) throws IOException\n{\r\n    verifyMountTable();\r\n    String path = RouterAdmin.normalizeFileSystemPath(str);\r\n    if (isTrashPath(path)) {\r\n        path = subtractTrashCurrentPath(path);\r\n    }\r\n    readLock.lock();\r\n    try {\r\n        String from = path;\r\n        String to = path + Character.MAX_VALUE;\r\n        SortedMap<String, MountTable> subMap = this.tree.subMap(from, to);\r\n        return FileSubclusterResolver.getMountPoints(path, subMap.keySet());\r\n    } finally {\r\n        readLock.unlock();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\resolver",
  "methodName" : "getMounts",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "List<MountTable> getMounts(final String path) throws IOException\n{\r\n    verifyMountTable();\r\n    return getTreeValues(RouterAdmin.normalizeFileSystemPath(path), false);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\resolver",
  "methodName" : "verifyMountTable",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void verifyMountTable() throws StateStoreUnavailableException\n{\r\n    if (!this.init || disabled) {\r\n        throw new StateStoreUnavailableException(\"Mount Table not initialized\");\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\resolver",
  "methodName" : "toString",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "String toString()\n{\r\n    readLock.lock();\r\n    try {\r\n        return this.tree.toString();\r\n    } finally {\r\n        readLock.unlock();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\resolver",
  "methodName" : "buildLocation",
  "errType" : null,
  "containingMethodsNum" : 16,
  "sourceCodeText" : "PathLocation buildLocation(final String path, final MountTable entry) throws IOException\n{\r\n    String srcPath = entry.getSourcePath();\r\n    if (!path.startsWith(srcPath)) {\r\n        LOG.error(\"Cannot build location, {} not a child of {}\", path, srcPath);\r\n        return null;\r\n    }\r\n    List<RemoteLocation> dests = entry.getDestinations();\r\n    if (getClass() == MountTableResolver.class && dests.size() > 1) {\r\n        throw new IOException(\"Cannnot build location, \" + getClass().getSimpleName() + \" should not resolve multiple destinations for \" + path);\r\n    }\r\n    String remainingPath = path.substring(srcPath.length());\r\n    if (remainingPath.startsWith(Path.SEPARATOR)) {\r\n        remainingPath = remainingPath.substring(1);\r\n    }\r\n    List<RemoteLocation> locations = new LinkedList<>();\r\n    for (RemoteLocation oneDst : dests) {\r\n        String nsId = oneDst.getNameserviceId();\r\n        String dest = oneDst.getDest();\r\n        String newPath = dest;\r\n        if (!newPath.endsWith(Path.SEPARATOR) && !remainingPath.isEmpty()) {\r\n            newPath += Path.SEPARATOR;\r\n        }\r\n        newPath += remainingPath;\r\n        RemoteLocation remoteLocation = new RemoteLocation(nsId, newPath, path);\r\n        locations.add(remoteLocation);\r\n    }\r\n    DestinationOrder order = entry.getDestOrder();\r\n    return new PathLocation(srcPath, locations, order);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\resolver",
  "methodName" : "getDefaultNamespace",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getDefaultNamespace()\n{\r\n    return this.defaultNameService;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\resolver",
  "methodName" : "findDeepest",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "MountTable findDeepest(final String path)\n{\r\n    readLock.lock();\r\n    try {\r\n        Entry<String, MountTable> entry = this.tree.floorEntry(path);\r\n        while (entry != null && !isParentEntry(path, entry.getKey())) {\r\n            entry = this.tree.lowerEntry(entry.getKey());\r\n        }\r\n        if (entry == null) {\r\n            return null;\r\n        }\r\n        return entry.getValue();\r\n    } finally {\r\n        readLock.unlock();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\resolver",
  "methodName" : "getTreeValues",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "List<MountTable> getTreeValues(final String path)\n{\r\n    return getTreeValues(path, false);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\resolver",
  "methodName" : "getTreeValues",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "List<MountTable> getTreeValues(final String path, boolean reverse)\n{\r\n    LinkedList<MountTable> ret = new LinkedList<>();\r\n    readLock.lock();\r\n    try {\r\n        String from = path;\r\n        String to = path + Character.MAX_VALUE;\r\n        SortedMap<String, MountTable> subMap = this.tree.subMap(from, to);\r\n        for (MountTable entry : subMap.values()) {\r\n            if (!reverse) {\r\n                ret.add(entry);\r\n            } else {\r\n                ret.addFirst(entry);\r\n            }\r\n        }\r\n    } finally {\r\n        readLock.unlock();\r\n    }\r\n    return ret;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\resolver",
  "methodName" : "getCacheSize",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "long getCacheSize() throws IOException\n{\r\n    if (this.locationCache != null) {\r\n        return this.locationCache.size();\r\n    }\r\n    throw new IOException(\"localCache is null\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\resolver",
  "methodName" : "getDefaultNameService",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getDefaultNameService()\n{\r\n    return defaultNameService;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\resolver",
  "methodName" : "setDefaultNameService",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setDefaultNameService(String defaultNameService)\n{\r\n    this.defaultNameService = defaultNameService;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\resolver",
  "methodName" : "isDefaultNSEnable",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean isDefaultNSEnable()\n{\r\n    return defaultNSEnable;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\resolver",
  "methodName" : "setDefaultNSEnable",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setDefaultNSEnable(boolean defaultNSRWEnable)\n{\r\n    this.defaultNSEnable = defaultNSRWEnable;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\resolver",
  "methodName" : "setDisabled",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setDisabled(boolean disable)\n{\r\n    this.disabled = disable;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store",
  "methodName" : "serviceInit",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void serviceInit(Configuration conf) throws Exception\n{\r\n    this.setIntervalMs(conf.getTimeDuration(RBFConfigKeys.DFS_ROUTER_CACHE_TIME_TO_LIVE_MS, RBFConfigKeys.DFS_ROUTER_CACHE_TIME_TO_LIVE_MS_DEFAULT, TimeUnit.MILLISECONDS));\r\n    super.serviceInit(conf);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store",
  "methodName" : "periodicInvoke",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void periodicInvoke()\n{\r\n    LOG.debug(\"Updating State Store cache\");\r\n    stateStore.refreshCaches();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\driver\\impl",
  "methodName" : "exists",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 1,
  "sourceCodeText" : "boolean exists(String path)\n{\r\n    try {\r\n        return fs.exists(new Path(path));\r\n    } catch (IOException e) {\r\n        return false;\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\driver\\impl",
  "methodName" : "mkdir",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 1,
  "sourceCodeText" : "boolean mkdir(String path)\n{\r\n    try {\r\n        return fs.mkdirs(new Path(path));\r\n    } catch (IOException e) {\r\n        return false;\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\driver\\impl",
  "methodName" : "rename",
  "errType" : [ "Exception" ],
  "containingMethodsNum" : 5,
  "sourceCodeText" : "boolean rename(String src, String dst)\n{\r\n    try {\r\n        if (fs instanceof DistributedFileSystem) {\r\n            DistributedFileSystem dfs = (DistributedFileSystem) fs;\r\n            dfs.rename(new Path(src), new Path(dst), Options.Rename.OVERWRITE);\r\n            return true;\r\n        } else {\r\n            if (fs.exists(new Path(dst))) {\r\n                fs.delete(new Path(dst), true);\r\n            }\r\n            return fs.rename(new Path(src), new Path(dst));\r\n        }\r\n    } catch (Exception e) {\r\n        LOG.error(\"Cannot rename {} to {}\", src, dst, e);\r\n        return false;\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\driver\\impl",
  "methodName" : "remove",
  "errType" : [ "Exception" ],
  "containingMethodsNum" : 2,
  "sourceCodeText" : "boolean remove(String path)\n{\r\n    try {\r\n        return fs.delete(new Path(path), true);\r\n    } catch (Exception e) {\r\n        LOG.error(\"Cannot remove {}\", path, e);\r\n        return false;\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\driver\\impl",
  "methodName" : "getRootDir",
  "errType" : [ "Exception" ],
  "containingMethodsNum" : 2,
  "sourceCodeText" : "String getRootDir()\n{\r\n    if (this.workPath == null) {\r\n        String rootPath = getConf().get(FEDERATION_STORE_FS_PATH);\r\n        URI workUri;\r\n        try {\r\n            workUri = new URI(rootPath);\r\n            fs = FileSystem.get(workUri, getConf());\r\n        } catch (Exception ex) {\r\n            return null;\r\n        }\r\n        this.workPath = rootPath;\r\n    }\r\n    return this.workPath;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\driver\\impl",
  "methodName" : "close",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void close() throws Exception\n{\r\n    if (fs != null) {\r\n        fs.close();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\driver\\impl",
  "methodName" : "getReader",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 2,
  "sourceCodeText" : "BufferedReader getReader(String pathName)\n{\r\n    BufferedReader reader = null;\r\n    Path path = new Path(pathName);\r\n    try {\r\n        FSDataInputStream fdis = fs.open(path);\r\n        InputStreamReader isr = new InputStreamReader(fdis, StandardCharsets.UTF_8);\r\n        reader = new BufferedReader(isr);\r\n    } catch (IOException ex) {\r\n        LOG.error(\"Cannot open read stream for {}\", path, ex);\r\n    }\r\n    return reader;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\driver\\impl",
  "methodName" : "getWriter",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 2,
  "sourceCodeText" : "BufferedWriter getWriter(String pathName)\n{\r\n    BufferedWriter writer = null;\r\n    Path path = new Path(pathName);\r\n    try {\r\n        FSDataOutputStream fdos = fs.create(path, true);\r\n        OutputStreamWriter osw = new OutputStreamWriter(fdos, StandardCharsets.UTF_8);\r\n        writer = new BufferedWriter(osw);\r\n    } catch (IOException ex) {\r\n        LOG.error(\"Cannot open write stream for {}\", path, ex);\r\n    }\r\n    return writer;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\driver\\impl",
  "methodName" : "getChildren",
  "errType" : [ "Exception" ],
  "containingMethodsNum" : 6,
  "sourceCodeText" : "List<String> getChildren(String pathName)\n{\r\n    Path path = new Path(workPath, pathName);\r\n    try {\r\n        FileStatus[] files = fs.listStatus(path);\r\n        List<String> ret = new ArrayList<>(files.length);\r\n        for (FileStatus file : files) {\r\n            Path filePath = file.getPath();\r\n            String fileName = filePath.getName();\r\n            ret.add(fileName);\r\n        }\r\n        return ret;\r\n    } catch (Exception e) {\r\n        LOG.error(\"Cannot get children for {}\", pathName, e);\r\n        return Collections.emptyList();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "getAll",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "Set<String> getAll()\n{\r\n    readLock.lock();\r\n    try {\r\n        return this.cache.keySet();\r\n    } finally {\r\n        readLock.unlock();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "isMountEntry",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "boolean isMountEntry(String path)\n{\r\n    readLock.lock();\r\n    try {\r\n        return this.cache.containsKey(path);\r\n    } finally {\r\n        readLock.unlock();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "getQuotaUsage",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "RouterQuotaUsage getQuotaUsage(String path)\n{\r\n    readLock.lock();\r\n    try {\r\n        RouterQuotaUsage quotaUsage = this.cache.get(path);\r\n        if (quotaUsage != null && isQuotaSet(quotaUsage)) {\r\n            return quotaUsage;\r\n        }\r\n        int pos = path.lastIndexOf(Path.SEPARATOR);\r\n        if (pos != -1) {\r\n            String parentPath = path.substring(0, pos);\r\n            return getQuotaUsage(parentPath);\r\n        }\r\n    } finally {\r\n        readLock.unlock();\r\n    }\r\n    return null;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "getPaths",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "Set<String> getPaths(String parentPath)\n{\r\n    readLock.lock();\r\n    try {\r\n        String from = parentPath;\r\n        String to = parentPath + Character.MAX_VALUE;\r\n        SortedMap<String, RouterQuotaUsage> subMap = this.cache.subMap(from, to);\r\n        Set<String> validPaths = new HashSet<>();\r\n        if (subMap != null) {\r\n            for (String path : subMap.keySet()) {\r\n                if (isParentEntry(path, parentPath)) {\r\n                    validPaths.add(path);\r\n                }\r\n            }\r\n        }\r\n        return validPaths;\r\n    } finally {\r\n        readLock.unlock();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "getParentsContainingQuota",
  "errType" : null,
  "containingMethodsNum" : 9,
  "sourceCodeText" : "TreeMap<String, RouterQuotaUsage> getParentsContainingQuota(String childPath)\n{\r\n    TreeMap<String, RouterQuotaUsage> res = new TreeMap<>();\r\n    readLock.lock();\r\n    try {\r\n        Entry<String, RouterQuotaUsage> entry = this.cache.floorEntry(childPath);\r\n        while (entry != null) {\r\n            String mountPath = entry.getKey();\r\n            RouterQuotaUsage quota = entry.getValue();\r\n            if (isQuotaSet(quota) && isParentEntry(childPath, mountPath)) {\r\n                res.put(mountPath, quota);\r\n            }\r\n            entry = this.cache.lowerEntry(mountPath);\r\n        }\r\n        return res;\r\n    } finally {\r\n        readLock.unlock();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "put",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void put(String path, RouterQuotaUsage quotaUsage)\n{\r\n    writeLock.lock();\r\n    try {\r\n        this.cache.put(path, quotaUsage);\r\n    } finally {\r\n        writeLock.unlock();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "updateQuota",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void updateQuota(String path, RouterQuotaUsage quota)\n{\r\n    writeLock.lock();\r\n    try {\r\n        RouterQuotaUsage.Builder builder = new RouterQuotaUsage.Builder().quota(quota.getQuota()).spaceQuota(quota.getSpaceQuota());\r\n        RouterQuotaUsage current = this.cache.get(path);\r\n        if (current != null) {\r\n            builder.fileAndDirectoryCount(current.getFileAndDirectoryCount()).spaceConsumed(current.getSpaceConsumed());\r\n        }\r\n        this.cache.put(path, builder.build());\r\n    } finally {\r\n        writeLock.unlock();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "remove",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void remove(String path)\n{\r\n    writeLock.lock();\r\n    try {\r\n        this.cache.remove(path);\r\n    } finally {\r\n        writeLock.unlock();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "clear",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void clear()\n{\r\n    writeLock.lock();\r\n    try {\r\n        this.cache.clear();\r\n    } finally {\r\n        writeLock.unlock();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "isQuotaSet",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "boolean isQuotaSet(QuotaUsage quota)\n{\r\n    if (quota != null) {\r\n        long nsQuota = quota.getQuota();\r\n        long ssQuota = quota.getSpaceQuota();\r\n        if (nsQuota != HdfsConstants.QUOTA_RESET || ssQuota != HdfsConstants.QUOTA_RESET || Quota.orByStorageType(t -> quota.getTypeQuota(t) != HdfsConstants.QUOTA_RESET)) {\r\n            return true;\r\n        }\r\n    }\r\n    return false;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\protocol\\impl\\pb",
  "methodName" : "getProto",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "NamenodeHeartbeatResponseProto getProto()\n{\r\n    return this.translator.build();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\protocol\\impl\\pb",
  "methodName" : "setProto",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setProto(Message proto)\n{\r\n    this.translator.setProto(proto);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\protocol\\impl\\pb",
  "methodName" : "readInstance",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void readInstance(String base64String) throws IOException\n{\r\n    this.translator.readInstance(base64String);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\protocol\\impl\\pb",
  "methodName" : "getResult",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "boolean getResult()\n{\r\n    return this.translator.getProtoOrBuilder().getStatus();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\protocol\\impl\\pb",
  "methodName" : "setResult",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setResult(boolean result)\n{\r\n    this.translator.getBuilder().setStatus(result);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\protocol\\impl\\pb",
  "methodName" : "getProto",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "AddMountTableEntryRequestProto getProto()\n{\r\n    return this.translator.build();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\protocol\\impl\\pb",
  "methodName" : "setProto",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setProto(Message proto)\n{\r\n    this.translator.setProto(proto);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\protocol\\impl\\pb",
  "methodName" : "readInstance",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void readInstance(String base64String) throws IOException\n{\r\n    this.translator.readInstance(base64String);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\protocol\\impl\\pb",
  "methodName" : "getEntry",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "MountTable getEntry()\n{\r\n    MountTableRecordProto entryProto = this.translator.getProtoOrBuilder().getEntry();\r\n    return new MountTablePBImpl(entryProto);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\protocol\\impl\\pb",
  "methodName" : "setEntry",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void setEntry(MountTable mount)\n{\r\n    if (mount instanceof MountTablePBImpl) {\r\n        MountTablePBImpl mountPB = (MountTablePBImpl) mount;\r\n        MountTableRecordProto mountProto = mountPB.getProto();\r\n        translator.getBuilder().setEntry(mountProto);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\rbfbalance",
  "methodName" : "execute",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "boolean execute() throws RetryException, IOException\n{\r\n    updateMountTable();\r\n    return true;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\rbfbalance",
  "methodName" : "updateMountTable",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void updateMountTable() throws IOException\n{\r\n    updateMountTableDestination(mount, dstNs, dstPath, conf);\r\n    enableWrite(mount, conf);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\rbfbalance",
  "methodName" : "updateMountTableDestination",
  "errType" : null,
  "containingMethodsNum" : 10,
  "sourceCodeText" : "void updateMountTableDestination(String mount, String dstNs, String dstPath, Configuration conf) throws IOException\n{\r\n    String address = conf.getTrimmed(RBFConfigKeys.DFS_ROUTER_ADMIN_ADDRESS_KEY, RBFConfigKeys.DFS_ROUTER_ADMIN_ADDRESS_DEFAULT);\r\n    InetSocketAddress routerSocket = NetUtils.createSocketAddr(address);\r\n    RouterClient rClient = new RouterClient(routerSocket, conf);\r\n    try {\r\n        MountTableManager mountTable = rClient.getMountTableManager();\r\n        MountTable originalEntry = getMountEntry(mount, mountTable);\r\n        if (originalEntry == null) {\r\n            throw new IOException(\"Mount table \" + mount + \" doesn't exist\");\r\n        } else {\r\n            RemoteLocation remoteLocation = new RemoteLocation(dstNs, dstPath, mount);\r\n            originalEntry.setDestinations(Arrays.asList(remoteLocation));\r\n            UpdateMountTableEntryRequest updateRequest = UpdateMountTableEntryRequest.newInstance(originalEntry);\r\n            UpdateMountTableEntryResponse response = mountTable.updateMountTableEntry(updateRequest);\r\n            if (!response.getStatus()) {\r\n                throw new IOException(\"Failed update mount table \" + mount);\r\n            }\r\n            rClient.getMountTableManager().refreshMountTableEntries(RefreshMountTableEntriesRequest.newInstance());\r\n        }\r\n    } finally {\r\n        rClient.close();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\rbfbalance",
  "methodName" : "getMountEntry",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "MountTable getMountEntry(String mount, MountTableManager mountTable) throws IOException\n{\r\n    GetMountTableEntriesRequest getRequest = GetMountTableEntriesRequest.newInstance(mount);\r\n    GetMountTableEntriesResponse getResponse = mountTable.getMountTableEntries(getRequest);\r\n    List<MountTable> results = getResponse.getEntries();\r\n    MountTable existingEntry = null;\r\n    for (MountTable result : results) {\r\n        if (mount.equals(result.getSourcePath())) {\r\n            existingEntry = result;\r\n            break;\r\n        }\r\n    }\r\n    return existingEntry;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\rbfbalance",
  "methodName" : "disableWrite",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void disableWrite(String mount, Configuration conf) throws IOException\n{\r\n    setMountReadOnly(mount, true, conf);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\rbfbalance",
  "methodName" : "enableWrite",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void enableWrite(String mount, Configuration conf) throws IOException\n{\r\n    setMountReadOnly(mount, false, conf);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\rbfbalance",
  "methodName" : "setMountReadOnly",
  "errType" : null,
  "containingMethodsNum" : 10,
  "sourceCodeText" : "void setMountReadOnly(String mount, boolean readOnly, Configuration conf) throws IOException\n{\r\n    String address = conf.getTrimmed(RBFConfigKeys.DFS_ROUTER_ADMIN_ADDRESS_KEY, RBFConfigKeys.DFS_ROUTER_ADMIN_ADDRESS_DEFAULT);\r\n    InetSocketAddress routerSocket = NetUtils.createSocketAddr(address);\r\n    RouterClient rClient = new RouterClient(routerSocket, conf);\r\n    try {\r\n        MountTableManager mountTable = rClient.getMountTableManager();\r\n        MountTable originalEntry = getMountEntry(mount, mountTable);\r\n        if (originalEntry == null) {\r\n            throw new IOException(\"Mount table \" + mount + \" doesn't exist\");\r\n        } else {\r\n            originalEntry.setReadOnly(readOnly);\r\n            UpdateMountTableEntryRequest updateRequest = UpdateMountTableEntryRequest.newInstance(originalEntry);\r\n            UpdateMountTableEntryResponse response = mountTable.updateMountTableEntry(updateRequest);\r\n            if (!response.getStatus()) {\r\n                throw new IOException(\"Failed update mount table \" + mount + \" with readonly=\" + readOnly);\r\n            }\r\n            rClient.getMountTableManager().refreshMountTableEntries(RefreshMountTableEntriesRequest.newInstance());\r\n        }\r\n    } finally {\r\n        rClient.close();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\rbfbalance",
  "methodName" : "write",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void write(DataOutput out) throws IOException\n{\r\n    super.write(out);\r\n    Text.writeString(out, mount);\r\n    Text.writeString(out, dstPath);\r\n    Text.writeString(out, dstNs);\r\n    conf.write(out);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\rbfbalance",
  "methodName" : "readFields",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void readFields(DataInput in) throws IOException\n{\r\n    super.readFields(in);\r\n    mount = Text.readString(in);\r\n    dstPath = Text.readString(in);\r\n    dstNs = Text.readString(in);\r\n    conf = new Configuration(false);\r\n    conf.readFields(in);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\rbfbalance",
  "methodName" : "getMount",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getMount()\n{\r\n    return mount;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\rbfbalance",
  "methodName" : "getDstPath",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getDstPath()\n{\r\n    return dstPath;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\rbfbalance",
  "methodName" : "getDstNs",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getDstNs()\n{\r\n    return dstNs;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\protocol\\impl\\pb",
  "methodName" : "setProto",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void setProto(Message p)\n{\r\n    if (protoClass.isInstance(p)) {\r\n        if (this.builder != null) {\r\n            this.builder.mergeFrom((P) p);\r\n        } else {\r\n            this.proto = (P) p;\r\n        }\r\n    } else {\r\n        throw new IllegalArgumentException(\"Cannot decode proto type \" + p.getClass().getName());\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\protocol\\impl\\pb",
  "methodName" : "getBuilder",
  "errType" : [ "ReflectiveOperationException" ],
  "containingMethodsNum" : 3,
  "sourceCodeText" : "B getBuilder()\n{\r\n    if (this.builder == null) {\r\n        try {\r\n            Method method = protoClass.getMethod(\"newBuilder\");\r\n            this.builder = (B) method.invoke(null);\r\n            if (this.proto != null) {\r\n                this.builder.mergeFrom(this.proto);\r\n            }\r\n        } catch (ReflectiveOperationException e) {\r\n            this.builder = null;\r\n        }\r\n    }\r\n    return this.builder;\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\protocol\\impl\\pb",
  "methodName" : "build",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "P build()\n{\r\n    if (this.builder != null) {\r\n        Message m = this.builder.build();\r\n        return (P) m;\r\n    } else if (this.proto != null) {\r\n        return this.proto;\r\n    }\r\n    return null;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\protocol\\impl\\pb",
  "methodName" : "getProtoOrBuilder",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "T getProtoOrBuilder()\n{\r\n    if (this.builder != null) {\r\n        return (T) this.builder;\r\n    } else if (this.proto != null) {\r\n        return (T) this.proto;\r\n    } else {\r\n        return (T) this.getBuilder();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\protocol\\impl\\pb",
  "methodName" : "readInstance",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void readInstance(String base64String) throws IOException\n{\r\n    byte[] bytes = Base64.decodeBase64(base64String);\r\n    Message msg = getBuilder().mergeFrom(bytes).build();\r\n    this.proto = (P) msg;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\records\\impl\\pb",
  "methodName" : "getProto",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "MountTableRecordProto getProto()\n{\r\n    return this.translator.build();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\records\\impl\\pb",
  "methodName" : "setProto",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setProto(Message proto)\n{\r\n    this.translator.setProto(proto);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\records\\impl\\pb",
  "methodName" : "readInstance",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void readInstance(String base64String) throws IOException\n{\r\n    this.translator.readInstance(base64String);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\records\\impl\\pb",
  "methodName" : "getSourcePath",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "String getSourcePath()\n{\r\n    MountTableRecordProtoOrBuilder proto = this.translator.getProtoOrBuilder();\r\n    if (!proto.hasSrcPath()) {\r\n        return null;\r\n    }\r\n    return proto.getSrcPath();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\records\\impl\\pb",
  "methodName" : "setSourcePath",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void setSourcePath(String path)\n{\r\n    Builder builder = this.translator.getBuilder();\r\n    if (path == null) {\r\n        builder.clearSrcPath();\r\n    } else {\r\n        builder.setSrcPath(path);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\records\\impl\\pb",
  "methodName" : "getDestinations",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "List<RemoteLocation> getDestinations()\n{\r\n    MountTableRecordProtoOrBuilder proto = this.translator.getProtoOrBuilder();\r\n    if (proto.getDestinationsCount() == 0) {\r\n        return null;\r\n    }\r\n    final List<RemoteLocation> ret = new LinkedList<>();\r\n    final List<RemoteLocationProto> destList = proto.getDestinationsList();\r\n    for (RemoteLocationProto dest : destList) {\r\n        String nsId = dest.getNameserviceId();\r\n        String path = dest.getPath();\r\n        RemoteLocation loc = new RemoteLocation(nsId, path, getSourcePath());\r\n        ret.add(loc);\r\n    }\r\n    return ret;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\records\\impl\\pb",
  "methodName" : "setDestinations",
  "errType" : null,
  "containingMethodsNum" : 9,
  "sourceCodeText" : "void setDestinations(final List<RemoteLocation> dests)\n{\r\n    Builder builder = this.translator.getBuilder();\r\n    builder.clearDestinations();\r\n    for (RemoteLocation dest : dests) {\r\n        RemoteLocationProto.Builder itemBuilder = RemoteLocationProto.newBuilder();\r\n        String nsId = dest.getNameserviceId();\r\n        String path = dest.getDest();\r\n        itemBuilder.setNameserviceId(nsId);\r\n        itemBuilder.setPath(path);\r\n        RemoteLocationProto item = itemBuilder.build();\r\n        builder.addDestinations(item);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\records\\impl\\pb",
  "methodName" : "addDestination",
  "errType" : null,
  "containingMethodsNum" : 9,
  "sourceCodeText" : "boolean addDestination(String nsId, String path)\n{\r\n    List<RemoteLocation> dests = getDestinations();\r\n    for (RemoteLocation dest : dests) {\r\n        if (dest.getNameserviceId().equals(nsId) && dest.getDest().equals(path)) {\r\n            return false;\r\n        }\r\n    }\r\n    Builder builder = this.translator.getBuilder();\r\n    RemoteLocationProto.Builder itemBuilder = RemoteLocationProto.newBuilder();\r\n    itemBuilder.setNameserviceId(nsId);\r\n    itemBuilder.setPath(path);\r\n    RemoteLocationProto item = itemBuilder.build();\r\n    builder.addDestinations(item);\r\n    return true;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\records\\impl\\pb",
  "methodName" : "setDateModified",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setDateModified(long time)\n{\r\n    this.translator.getBuilder().setDateModified(time);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\records\\impl\\pb",
  "methodName" : "getDateModified",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "long getDateModified()\n{\r\n    return this.translator.getProtoOrBuilder().getDateModified();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\records\\impl\\pb",
  "methodName" : "setDateCreated",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setDateCreated(long time)\n{\r\n    this.translator.getBuilder().setDateCreated(time);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\records\\impl\\pb",
  "methodName" : "getDateCreated",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "long getDateCreated()\n{\r\n    return this.translator.getProtoOrBuilder().getDateCreated();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\records\\impl\\pb",
  "methodName" : "isReadOnly",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "boolean isReadOnly()\n{\r\n    MountTableRecordProtoOrBuilder proto = this.translator.getProtoOrBuilder();\r\n    if (!proto.hasReadOnly()) {\r\n        return false;\r\n    }\r\n    return proto.getReadOnly();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\records\\impl\\pb",
  "methodName" : "setReadOnly",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setReadOnly(boolean ro)\n{\r\n    this.translator.getBuilder().setReadOnly(ro);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\records\\impl\\pb",
  "methodName" : "getDestOrder",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "DestinationOrder getDestOrder()\n{\r\n    MountTableRecordProtoOrBuilder proto = this.translator.getProtoOrBuilder();\r\n    return convert(proto.getDestOrder());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\records\\impl\\pb",
  "methodName" : "setDestOrder",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void setDestOrder(DestinationOrder order)\n{\r\n    Builder builder = this.translator.getBuilder();\r\n    if (order == null) {\r\n        builder.clearDestOrder();\r\n    } else {\r\n        builder.setDestOrder(convert(order));\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\records\\impl\\pb",
  "methodName" : "isFaultTolerant",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "boolean isFaultTolerant()\n{\r\n    MountTableRecordProtoOrBuilder proto = this.translator.getProtoOrBuilder();\r\n    if (!proto.hasFaultTolerant()) {\r\n        return false;\r\n    }\r\n    return proto.getFaultTolerant();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\records\\impl\\pb",
  "methodName" : "setFaultTolerant",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setFaultTolerant(boolean faultTolerant)\n{\r\n    this.translator.getBuilder().setFaultTolerant(faultTolerant);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\records\\impl\\pb",
  "methodName" : "getOwnerName",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "String getOwnerName()\n{\r\n    MountTableRecordProtoOrBuilder proto = this.translator.getProtoOrBuilder();\r\n    if (!proto.hasOwnerName()) {\r\n        return RouterAdminServer.getSuperUser();\r\n    }\r\n    return proto.getOwnerName();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\records\\impl\\pb",
  "methodName" : "setOwnerName",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void setOwnerName(String owner)\n{\r\n    Builder builder = this.translator.getBuilder();\r\n    if (owner == null) {\r\n        builder.clearOwnerName();\r\n    } else {\r\n        builder.setOwnerName(owner);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\records\\impl\\pb",
  "methodName" : "getGroupName",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "String getGroupName()\n{\r\n    MountTableRecordProtoOrBuilder proto = this.translator.getProtoOrBuilder();\r\n    if (!proto.hasGroupName()) {\r\n        return RouterAdminServer.getSuperGroup();\r\n    }\r\n    return proto.getGroupName();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\records\\impl\\pb",
  "methodName" : "setGroupName",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void setGroupName(String group)\n{\r\n    Builder builder = this.translator.getBuilder();\r\n    if (group == null) {\r\n        builder.clearGroupName();\r\n    } else {\r\n        builder.setGroupName(group);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\records\\impl\\pb",
  "methodName" : "getMode",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "FsPermission getMode()\n{\r\n    MountTableRecordProtoOrBuilder proto = this.translator.getProtoOrBuilder();\r\n    short mode = RouterPermissionChecker.MOUNT_TABLE_PERMISSION_DEFAULT;\r\n    if (proto.hasMode()) {\r\n        mode = (short) proto.getMode();\r\n    }\r\n    return new FsPermission(mode);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\records\\impl\\pb",
  "methodName" : "setMode",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void setMode(FsPermission mode)\n{\r\n    Builder builder = this.translator.getBuilder();\r\n    if (mode == null) {\r\n        builder.clearMode();\r\n    } else {\r\n        builder.setMode(mode.toShort());\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\records\\impl\\pb",
  "methodName" : "getQuota",
  "errType" : null,
  "containingMethodsNum" : 20,
  "sourceCodeText" : "RouterQuotaUsage getQuota()\n{\r\n    MountTableRecordProtoOrBuilder proto = this.translator.getProtoOrBuilder();\r\n    long nsQuota = HdfsConstants.QUOTA_RESET;\r\n    long nsCount = RouterQuotaUsage.QUOTA_USAGE_COUNT_DEFAULT;\r\n    long ssQuota = HdfsConstants.QUOTA_RESET;\r\n    long ssCount = RouterQuotaUsage.QUOTA_USAGE_COUNT_DEFAULT;\r\n    long[] typeQuota = new long[StorageType.values().length];\r\n    long[] typeConsume = new long[StorageType.values().length];\r\n    eachByStorageType(t -> typeQuota[t.ordinal()] = HdfsConstants.QUOTA_RESET);\r\n    eachByStorageType(t -> typeConsume[t.ordinal()] = RouterQuotaUsage.QUOTA_USAGE_COUNT_DEFAULT);\r\n    if (proto.hasQuota()) {\r\n        QuotaUsageProto quotaProto = proto.getQuota();\r\n        nsQuota = quotaProto.getQuota();\r\n        nsCount = quotaProto.getFileAndDirectoryCount();\r\n        ssQuota = quotaProto.getSpaceQuota();\r\n        ssCount = quotaProto.getSpaceConsumed();\r\n        if (quotaProto.hasTypeQuotaInfos()) {\r\n            StorageTypeQuotaInfosProto typeInfo = quotaProto.getTypeQuotaInfos();\r\n            for (StorageTypeQuotaInfoProto tp : typeInfo.getTypeQuotaInfoList()) {\r\n                typeQuota[StorageType.parseStorageType(tp.getType().name()).ordinal()] = tp.getQuota();\r\n                typeConsume[StorageType.parseStorageType(tp.getType().name()).ordinal()] = tp.getConsumed();\r\n            }\r\n        }\r\n    }\r\n    RouterQuotaUsage.Builder builder = new RouterQuotaUsage.Builder().quota(nsQuota).fileAndDirectoryCount(nsCount).spaceQuota(ssQuota).spaceConsumed(ssCount).typeQuota(typeQuota).typeConsumed(typeConsume);\r\n    return builder.build();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\records\\impl\\pb",
  "methodName" : "setQuota",
  "errType" : null,
  "containingMethodsNum" : 10,
  "sourceCodeText" : "void setQuota(RouterQuotaUsage quota)\n{\r\n    Builder builder = this.translator.getBuilder();\r\n    if (quota == null) {\r\n        builder.clearQuota();\r\n    } else {\r\n        QuotaUsageProto.Builder quotaBuilder = QuotaUsageProto.newBuilder();\r\n        quotaBuilder.setFileAndDirectoryCount(quota.getFileAndDirectoryCount()).setQuota(quota.getQuota()).setSpaceConsumed(quota.getSpaceConsumed()).setSpaceQuota(quota.getSpaceQuota());\r\n        if (quota.isTypeQuotaSet()) {\r\n            StorageTypeQuotaInfosProto.Builder infoBuilder = StorageTypeQuotaInfosProto.newBuilder();\r\n            eachByStorageType(t -> infoBuilder.addTypeQuotaInfo(StorageTypeQuotaInfoProto.newBuilder().setType(HdfsProtos.StorageTypeProto.valueOf(t.name())).setQuota(quota.getTypeQuota(t)).setConsumed(quota.getTypeConsumed(t)).build()));\r\n            quotaBuilder.setTypeQuotaInfos(infoBuilder.build());\r\n        }\r\n        QuotaUsageProto quotaUsage = quotaBuilder.build();\r\n        builder.setQuota(quotaUsage);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\records\\impl\\pb",
  "methodName" : "convert",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "DestinationOrder convert(DestOrder order)\n{\r\n    switch(order) {\r\n        case LOCAL:\r\n            return DestinationOrder.LOCAL;\r\n        case RANDOM:\r\n            return DestinationOrder.RANDOM;\r\n        case HASH_ALL:\r\n            return DestinationOrder.HASH_ALL;\r\n        case SPACE:\r\n            return DestinationOrder.SPACE;\r\n        default:\r\n            return DestinationOrder.HASH;\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\records\\impl\\pb",
  "methodName" : "convert",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "DestOrder convert(DestinationOrder order)\n{\r\n    switch(order) {\r\n        case LOCAL:\r\n            return DestOrder.LOCAL;\r\n        case RANDOM:\r\n            return DestOrder.RANDOM;\r\n        case HASH_ALL:\r\n            return DestOrder.HASH_ALL;\r\n        case SPACE:\r\n            return DestOrder.SPACE;\r\n        default:\r\n            return DestOrder.HASH;\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "setIntervalMs",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setIntervalMs(long interval)\n{\r\n    if (getServiceState() == STATE.STARTED) {\r\n        throw new ServiceStateException(\"Periodic service already started\");\r\n    } else {\r\n        this.intervalMs = interval;\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "getIntervalMs",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getIntervalMs()\n{\r\n    return this.intervalMs;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "getErrorCount",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getErrorCount()\n{\r\n    return this.errorCount;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "getRunCount",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getRunCount()\n{\r\n    return this.runCount;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "getLastUpdate",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getLastUpdate()\n{\r\n    return this.lastRun;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "serviceStart",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void serviceStart() throws Exception\n{\r\n    super.serviceStart();\r\n    LOG.info(\"Starting periodic service {}\", this.serviceName);\r\n    startPeriodic();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "serviceStop",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void serviceStop() throws Exception\n{\r\n    stopPeriodic();\r\n    LOG.info(\"Stopping periodic service {}\", this.serviceName);\r\n    super.serviceStop();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "stopPeriodic",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void stopPeriodic()\n{\r\n    if (this.isRunning) {\r\n        LOG.info(\"{} is shutting down\", this.serviceName);\r\n        this.isRunning = false;\r\n        this.scheduler.shutdownNow();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "startPeriodic",
  "errType" : [ "Exception" ],
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void startPeriodic()\n{\r\n    stopPeriodic();\r\n    Runnable updateRunnable = new Runnable() {\r\n\r\n        @Override\r\n        public void run() {\r\n            LOG.debug(\"Running {} update task\", serviceName);\r\n            try {\r\n                if (!isRunning) {\r\n                    return;\r\n                }\r\n                periodicInvoke();\r\n                runCount++;\r\n                lastRun = Time.now();\r\n            } catch (Exception ex) {\r\n                errorCount++;\r\n                LOG.warn(serviceName + \" service threw an exception\", ex);\r\n            }\r\n        }\r\n    };\r\n    this.isRunning = true;\r\n    this.scheduler.scheduleWithFixedDelay(updateRunnable, 0, this.intervalMs, TimeUnit.MILLISECONDS);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\router",
  "methodName" : "periodicInvoke",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void periodicInvoke()",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\driver",
  "methodName" : "init",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "boolean init(final Configuration config, final String id, final Collection<Class<? extends BaseRecord>> records, final StateStoreMetrics stateStoreMetrics)\n{\r\n    this.conf = config;\r\n    this.identifier = id;\r\n    this.metrics = stateStoreMetrics;\r\n    if (this.identifier == null) {\r\n        LOG.warn(\"The identifier for the State Store connection is not set\");\r\n    }\r\n    boolean success = initDriver();\r\n    if (!success) {\r\n        LOG.error(\"Cannot initialize driver for {}\", getDriverName());\r\n        return false;\r\n    }\r\n    for (Class<? extends BaseRecord> cls : records) {\r\n        String recordString = StateStoreUtils.getRecordName(cls);\r\n        if (!initRecordStorage(recordString, cls)) {\r\n            LOG.error(\"Cannot initialize record store for {}\", cls.getSimpleName());\r\n            return false;\r\n        }\r\n    }\r\n    return true;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\driver",
  "methodName" : "getConf",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Configuration getConf()\n{\r\n    return this.conf;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\driver",
  "methodName" : "getIdentifier",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getIdentifier()\n{\r\n    return this.identifier;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\driver",
  "methodName" : "getMetrics",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "StateStoreMetrics getMetrics()\n{\r\n    return this.metrics;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\driver",
  "methodName" : "initDriver",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean initDriver()",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\driver",
  "methodName" : "initRecordStorage",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean initRecordStorage(String className, Class<T> clazz)",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\driver",
  "methodName" : "isDriverReady",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean isDriverReady()",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\driver",
  "methodName" : "verifyDriverReady",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void verifyDriverReady() throws StateStoreUnavailableException\n{\r\n    if (!isDriverReady()) {\r\n        String driverName = getDriverName();\r\n        String hostname = getHostname();\r\n        throw new StateStoreUnavailableException(\"State Store driver \" + driverName + \" in \" + hostname + \" is not ready.\");\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\driver",
  "methodName" : "close",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void close() throws Exception",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\driver",
  "methodName" : "getTime",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "long getTime()\n{\r\n    return Time.now();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\driver",
  "methodName" : "getDriverName",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String getDriverName()\n{\r\n    return this.getClass().getSimpleName();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-rbf\\src\\main\\java\\org\\apache\\hadoop\\hdfs\\server\\federation\\store\\driver",
  "methodName" : "getHostname",
  "errType" : [ "Exception" ],
  "containingMethodsNum" : 2,
  "sourceCodeText" : "String getHostname()\n{\r\n    String hostname = \"Unknown\";\r\n    try {\r\n        hostname = InetAddress.getLocalHost().getHostName();\r\n    } catch (Exception e) {\r\n        LOG.error(\"Cannot get local address\", e);\r\n    }\r\n    return hostname;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
} ]
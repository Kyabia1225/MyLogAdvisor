[ {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app",
  "methodName" : "initJobCredentialsAndUGI",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void initJobCredentialsAndUGI(Configuration conf)\n{\r\n    String shuffleSecret = \"fake-shuffle-secret\";\r\n    TokenCache.setShuffleSecretKey(shuffleSecret.getBytes(), getCredentials());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app",
  "methodName" : "getApplicationAttemptId",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "ApplicationAttemptId getApplicationAttemptId(ApplicationId applicationId, int startCount)\n{\r\n    ApplicationAttemptId applicationAttemptId = ApplicationAttemptId.newInstance(applicationId, startCount);\r\n    return applicationAttemptId;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app",
  "methodName" : "getContainerId",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "ContainerId getContainerId(ApplicationId applicationId, int startCount)\n{\r\n    ApplicationAttemptId appAttemptId = getApplicationAttemptId(applicationId, startCount);\r\n    ContainerId containerId = ContainerId.newContainerId(appAttemptId, startCount);\r\n    return containerId;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app",
  "methodName" : "serviceInit",
  "errType" : [ "Exception" ],
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void serviceInit(Configuration conf) throws Exception\n{\r\n    try {\r\n        String user = UserGroupInformation.getCurrentUser().getShortUserName();\r\n        Path stagingDir = MRApps.getStagingAreaDir(conf, user);\r\n        FileSystem fs = getFileSystem(conf);\r\n        fs.mkdirs(stagingDir);\r\n    } catch (Exception e) {\r\n        throw new YarnRuntimeException(\"Error creating staging dir\", e);\r\n    }\r\n    super.serviceInit(conf);\r\n    if (this.clusterInfo != null) {\r\n        getContext().getClusterInfo().setMaxContainerCapability(this.clusterInfo.getMaxContainerCapability());\r\n    } else {\r\n        getContext().getClusterInfo().setMaxContainerCapability(Resource.newInstance(10240, 1));\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app",
  "methodName" : "submit",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Job submit(Configuration conf) throws Exception\n{\r\n    return submit(conf, false, false);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app",
  "methodName" : "submit",
  "errType" : null,
  "containingMethodsNum" : 15,
  "sourceCodeText" : "Job submit(Configuration conf, boolean mapSpeculative, boolean reduceSpeculative) throws Exception\n{\r\n    String user = conf.get(MRJobConfig.USER_NAME, UserGroupInformation.getCurrentUser().getShortUserName());\r\n    conf.set(MRJobConfig.USER_NAME, user);\r\n    conf.set(MRJobConfig.MR_AM_STAGING_DIR, testAbsPath.toString());\r\n    conf.setBoolean(MRJobConfig.MR_AM_CREATE_JH_INTERMEDIATE_BASE_DIR, true);\r\n    conf.setBoolean(MRJobConfig.MAP_SPECULATIVE, mapSpeculative);\r\n    conf.setBoolean(MRJobConfig.REDUCE_SPECULATIVE, reduceSpeculative);\r\n    init(conf);\r\n    start();\r\n    DefaultMetricsSystem.shutdown();\r\n    Job job = getContext().getAllJobs().values().iterator().next();\r\n    if (assignedQueue != null) {\r\n        job.setQueueName(assignedQueue);\r\n    }\r\n    String jobFile = MRApps.getJobFile(conf, user, TypeConverter.fromYarn(job.getID()));\r\n    LOG.info(\"Writing job conf to \" + jobFile);\r\n    new File(jobFile).getParentFile().mkdirs();\r\n    conf.writeXml(new FileOutputStream(jobFile));\r\n    return job;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app",
  "methodName" : "waitForInternalState",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void waitForInternalState(JobImpl job, JobStateInternal finalState) throws Exception\n{\r\n    int timeoutSecs = 0;\r\n    JobStateInternal iState = job.getInternalState();\r\n    while (!finalState.equals(iState) && timeoutSecs++ < WAIT_FOR_STATE_CNT) {\r\n        Thread.sleep(WAIT_FOR_STATE_INTERVAL);\r\n        iState = job.getInternalState();\r\n    }\r\n    LOG.info(\"Job {} Internal State is : {}\", job.getID(), iState);\r\n    Assert.assertEquals(\"Task Internal state is not correct (timedout)\", finalState, iState);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app",
  "methodName" : "waitForInternalState",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void waitForInternalState(TaskImpl task, TaskStateInternal finalState) throws Exception\n{\r\n    int timeoutSecs = 0;\r\n    TaskStateInternal iState = task.getInternalState();\r\n    while (!finalState.equals(iState) && timeoutSecs++ < WAIT_FOR_STATE_CNT) {\r\n        Thread.sleep(WAIT_FOR_STATE_INTERVAL);\r\n        iState = task.getInternalState();\r\n    }\r\n    LOG.info(\"Task {} Internal State is : {}\", task.getID(), iState);\r\n    Assert.assertEquals(\"Task Internal state is not correct (timedout)\", finalState, iState);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app",
  "methodName" : "waitForInternalState",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void waitForInternalState(TaskAttemptImpl attempt, TaskAttemptStateInternal finalState) throws Exception\n{\r\n    int timeoutSecs = 0;\r\n    TaskAttemptStateInternal iState = attempt.getInternalState();\r\n    while (!finalState.equals(iState) && timeoutSecs++ < WAIT_FOR_STATE_CNT) {\r\n        Thread.sleep(WAIT_FOR_STATE_INTERVAL);\r\n        iState = attempt.getInternalState();\r\n    }\r\n    LOG.info(\"TaskAttempt {} Internal State is : {}\", attempt.getID(), iState);\r\n    Assert.assertEquals(\"TaskAttempt Internal state is not correct (timedout)\", finalState, iState);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app",
  "methodName" : "waitForState",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void waitForState(TaskAttempt attempt, TaskAttemptState finalState) throws Exception\n{\r\n    int timeoutSecs = 0;\r\n    TaskAttemptReport report = attempt.getReport();\r\n    while (!finalState.equals(report.getTaskAttemptState()) && timeoutSecs++ < WAIT_FOR_STATE_CNT) {\r\n        Thread.sleep(WAIT_FOR_STATE_INTERVAL);\r\n        report = attempt.getReport();\r\n    }\r\n    LOG.info(\"TaskAttempt {} State is : {}\", attempt.getID(), report.getTaskAttemptState());\r\n    Assert.assertEquals(\"TaskAttempt state is not correct (timedout)\", finalState, report.getTaskAttemptState());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app",
  "methodName" : "waitForState",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void waitForState(Task task, TaskState finalState) throws Exception\n{\r\n    int timeoutSecs = 0;\r\n    TaskReport report = task.getReport();\r\n    while (!finalState.equals(report.getTaskState()) && timeoutSecs++ < WAIT_FOR_STATE_CNT) {\r\n        Thread.sleep(WAIT_FOR_STATE_INTERVAL);\r\n        report = task.getReport();\r\n    }\r\n    LOG.info(\"Task {} State is : {}\", task.getID(), report.getTaskState());\r\n    Assert.assertEquals(\"Task state is not correct (timedout)\", finalState, report.getTaskState());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app",
  "methodName" : "waitForState",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void waitForState(Job job, JobState finalState) throws Exception\n{\r\n    int timeoutSecs = 0;\r\n    JobReport report = job.getReport();\r\n    while (!finalState.equals(report.getJobState()) && timeoutSecs++ < WAIT_FOR_STATE_CNT) {\r\n        report = job.getReport();\r\n        Thread.sleep(WAIT_FOR_STATE_INTERVAL);\r\n    }\r\n    LOG.info(\"Job {} State is : {}\", job.getID(), report.getJobState());\r\n    Assert.assertEquals(\"Job state is not correct (timedout)\", finalState, job.getState());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app",
  "methodName" : "waitForState",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void waitForState(Service.STATE finalState) throws Exception\n{\r\n    if (finalState == Service.STATE.STOPPED) {\r\n        Assert.assertTrue(\"Timeout while waiting for MRApp to stop\", waitForServiceToStop(20 * 1000));\r\n    } else {\r\n        int timeoutSecs = 0;\r\n        while (!finalState.equals(getServiceState()) && timeoutSecs++ < WAIT_FOR_STATE_CNT) {\r\n            Thread.sleep(WAIT_FOR_STATE_INTERVAL);\r\n        }\r\n        LOG.info(\"MRApp State is : {}\", getServiceState());\r\n        Assert.assertEquals(\"MRApp state is not correct (timedout)\", finalState, getServiceState());\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app",
  "methodName" : "verifyCompleted",
  "errType" : null,
  "containingMethodsNum" : 14,
  "sourceCodeText" : "void verifyCompleted()\n{\r\n    for (Job job : getContext().getAllJobs().values()) {\r\n        JobReport jobReport = job.getReport();\r\n        LOG.info(\"Job start time :{}\", jobReport.getStartTime());\r\n        LOG.info(\"Job finish time :\", jobReport.getFinishTime());\r\n        Assert.assertTrue(\"Job start time is not less than finish time\", jobReport.getStartTime() <= jobReport.getFinishTime());\r\n        Assert.assertTrue(\"Job finish time is in future\", jobReport.getFinishTime() <= System.currentTimeMillis());\r\n        for (Task task : job.getTasks().values()) {\r\n            TaskReport taskReport = task.getReport();\r\n            LOG.info(\"Task {} start time : {}\", task.getID(), taskReport.getStartTime());\r\n            LOG.info(\"Task {} finish time : {}\", task.getID(), taskReport.getFinishTime());\r\n            Assert.assertTrue(\"Task start time is not less than finish time\", taskReport.getStartTime() <= taskReport.getFinishTime());\r\n            for (TaskAttempt attempt : task.getAttempts().values()) {\r\n                TaskAttemptReport attemptReport = attempt.getReport();\r\n                Assert.assertTrue(\"Attempt start time is not less than finish time\", attemptReport.getStartTime() <= attemptReport.getFinishTime());\r\n            }\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app",
  "methodName" : "createJob",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 12,
  "sourceCodeText" : "Job createJob(Configuration conf, JobStateInternal forcedState, String diagnostic)\n{\r\n    UserGroupInformation currentUser = null;\r\n    try {\r\n        currentUser = UserGroupInformation.getCurrentUser();\r\n    } catch (IOException e) {\r\n        throw new YarnRuntimeException(e);\r\n    }\r\n    Job newJob = new TestJob(getJobId(), getAttemptID(), conf, getDispatcher().getEventHandler(), getTaskAttemptListener(), getContext().getClock(), getCommitter(), isNewApiCommitter(), currentUser.getUserName(), getContext(), forcedState, diagnostic);\r\n    ((AppContext) getContext()).getAllJobs().put(newJob.getID(), newJob);\r\n    getDispatcher().register(JobFinishEvent.Type.class, new EventHandler<JobFinishEvent>() {\r\n\r\n        @Override\r\n        public void handle(JobFinishEvent event) {\r\n            stop();\r\n        }\r\n    });\r\n    return newJob;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app",
  "methodName" : "createTaskAttemptFinishingMonitor",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "TaskAttemptFinishingMonitor createTaskAttemptFinishingMonitor(EventHandler eventHandler)\n{\r\n    return new TaskAttemptFinishingMonitor(eventHandler) {\r\n\r\n        @Override\r\n        public synchronized void register(TaskAttemptId attemptID) {\r\n            getContext().getEventHandler().handle(new TaskAttemptEvent(attemptID, TaskAttemptEventType.TA_CONTAINER_COMPLETED));\r\n        }\r\n    };\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app",
  "methodName" : "createTaskAttemptListener",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "TaskAttemptListener createTaskAttemptListener(AppContext context, AMPreemptionPolicy policy)\n{\r\n    return new TaskAttemptListener() {\r\n\r\n        @Override\r\n        public InetSocketAddress getAddress() {\r\n            return NetUtils.createSocketAddr(\"localhost:54321\");\r\n        }\r\n\r\n        @Override\r\n        public void registerLaunchedTask(TaskAttemptId attemptID, WrappedJvmID jvmID) {\r\n        }\r\n\r\n        @Override\r\n        public void unregister(TaskAttemptId attemptID, WrappedJvmID jvmID) {\r\n        }\r\n\r\n        @Override\r\n        public void registerPendingTask(org.apache.hadoop.mapred.Task task, WrappedJvmID jvmID) {\r\n        }\r\n    };\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app",
  "methodName" : "createJobHistoryHandler",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "EventHandler<JobHistoryEvent> createJobHistoryHandler(AppContext context)\n{\r\n    return new EventHandler<JobHistoryEvent>() {\r\n\r\n        @Override\r\n        public void handle(JobHistoryEvent event) {\r\n        }\r\n    };\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app",
  "methodName" : "createContainerLauncher",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "ContainerLauncher createContainerLauncher(AppContext context)\n{\r\n    return new MockContainerLauncher();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app",
  "methodName" : "containerLaunched",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void containerLaunched(TaskAttemptId attemptID, int shufflePort)\n{\r\n    getContext().getEventHandler().handle(new TaskAttemptContainerLaunchedEvent(attemptID, shufflePort));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app",
  "methodName" : "attemptLaunched",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void attemptLaunched(TaskAttemptId attemptID)\n{\r\n    if (autoComplete) {\r\n        getContext().getEventHandler().handle(new TaskAttemptEvent(attemptID, TaskAttemptEventType.TA_DONE));\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app",
  "methodName" : "createContainerAllocator",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "ContainerAllocator createContainerAllocator(ClientService clientService, final AppContext context)\n{\r\n    return new MRAppContainerAllocator();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app",
  "methodName" : "createCommitterEventHandler",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "EventHandler<CommitterEvent> createCommitterEventHandler(AppContext context, final OutputCommitter committer)\n{\r\n    OutputCommitter stubbedCommitter = new OutputCommitter() {\r\n\r\n        @Override\r\n        public void setupJob(JobContext jobContext) throws IOException {\r\n            committer.setupJob(jobContext);\r\n        }\r\n\r\n        @SuppressWarnings(\"deprecation\")\r\n        @Override\r\n        public void cleanupJob(JobContext jobContext) throws IOException {\r\n            committer.cleanupJob(jobContext);\r\n        }\r\n\r\n        @Override\r\n        public void commitJob(JobContext jobContext) throws IOException {\r\n            committer.commitJob(jobContext);\r\n        }\r\n\r\n        @Override\r\n        public void abortJob(JobContext jobContext, State state) throws IOException {\r\n            committer.abortJob(jobContext, state);\r\n        }\r\n\r\n        @Override\r\n        public boolean isRecoverySupported(JobContext jobContext) throws IOException {\r\n            return committer.isRecoverySupported(jobContext);\r\n        }\r\n\r\n        @SuppressWarnings(\"deprecation\")\r\n        @Override\r\n        public boolean isRecoverySupported() {\r\n            return committer.isRecoverySupported();\r\n        }\r\n\r\n        @Override\r\n        public void setupTask(TaskAttemptContext taskContext) throws IOException {\r\n        }\r\n\r\n        @Override\r\n        public boolean needsTaskCommit(TaskAttemptContext taskContext) throws IOException {\r\n            return false;\r\n        }\r\n\r\n        @Override\r\n        public void commitTask(TaskAttemptContext taskContext) throws IOException {\r\n        }\r\n\r\n        @Override\r\n        public void abortTask(TaskAttemptContext taskContext) throws IOException {\r\n        }\r\n\r\n        @Override\r\n        public void recoverTask(TaskAttemptContext taskContext) throws IOException {\r\n        }\r\n    };\r\n    return new CommitterEventHandler(context, stubbedCommitter, getRMHeartbeatHandler());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app",
  "methodName" : "createClientService",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "ClientService createClientService(AppContext context)\n{\r\n    return new MRClientService(context) {\r\n\r\n        @Override\r\n        public InetSocketAddress getBindAddress() {\r\n            return NetUtils.createSocketAddr(\"localhost:9876\");\r\n        }\r\n\r\n        @Override\r\n        public int getHttpPort() {\r\n            return -1;\r\n        }\r\n    };\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app",
  "methodName" : "setClusterInfo",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void setClusterInfo(ClusterInfo clusterInfo)\n{\r\n    if (getServiceState() == Service.STATE.NOTINITED || getServiceState() == Service.STATE.INITED) {\r\n        this.clusterInfo = clusterInfo;\r\n    } else {\r\n        throw new IllegalStateException(\"ClusterInfo can only be set before the App is STARTED\");\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app",
  "methodName" : "setAllocatedContainerResource",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setAllocatedContainerResource(Resource resource)\n{\r\n    this.resource = resource;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app",
  "methodName" : "newContainerToken",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "Token newContainerToken(NodeId nodeId, byte[] password, ContainerTokenIdentifier tokenIdentifier)\n{\r\n    InetSocketAddress addr = NetUtils.createSocketAddrForHost(nodeId.getHost(), nodeId.getPort());\r\n    Token containerToken = Token.newInstance(tokenIdentifier.getBytes(), ContainerTokenIdentifier.KIND.toString(), password, SecurityUtil.buildTokenService(addr).toString());\r\n    return containerToken;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app",
  "methodName" : "newContainerId",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "ContainerId newContainerId(int appId, int appAttemptId, long timestamp, int containerId)\n{\r\n    ApplicationId applicationId = ApplicationId.newInstance(timestamp, appId);\r\n    ApplicationAttemptId applicationAttemptId = ApplicationAttemptId.newInstance(applicationId, appAttemptId);\r\n    return ContainerId.newContainerId(applicationAttemptId, containerId);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app",
  "methodName" : "newContainerTokenIdentifier",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "ContainerTokenIdentifier newContainerTokenIdentifier(Token containerToken) throws IOException\n{\r\n    org.apache.hadoop.security.token.Token<ContainerTokenIdentifier> token = new org.apache.hadoop.security.token.Token<ContainerTokenIdentifier>(containerToken.getIdentifier().array(), containerToken.getPassword().array(), new Text(containerToken.getKind()), new Text(containerToken.getService()));\r\n    return token.decodeIdentifier();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\rm",
  "methodName" : "setup",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void setup()\n{\r\n    MyContainerAllocator.getJobUpdatedNodeEvents().clear();\r\n    MyContainerAllocator.getTaskAttemptKillEvents().clear();\r\n    UserGroupInformation.setLoginUser(null);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\rm",
  "methodName" : "tearDown",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void tearDown()\n{\r\n    DefaultMetricsSystem.shutdown();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\rm",
  "methodName" : "testSimple",
  "errType" : null,
  "containingMethodsNum" : 42,
  "sourceCodeText" : "void testSimple() throws Exception\n{\r\n    LOG.info(\"Running testSimple\");\r\n    Configuration conf = new Configuration();\r\n    MyResourceManager rm = new MyResourceManager(conf);\r\n    rm.start();\r\n    RMApp app = MockRMAppSubmitter.submitWithMemory(1024, rm);\r\n    rm.drainEvents();\r\n    MockNM amNodeManager = rm.registerNode(\"amNM:1234\", 2048);\r\n    amNodeManager.nodeHeartbeat(true);\r\n    rm.drainEvents();\r\n    ApplicationAttemptId appAttemptId = app.getCurrentAppAttempt().getAppAttemptId();\r\n    rm.sendAMLaunched(appAttemptId);\r\n    rm.drainEvents();\r\n    JobId jobId = MRBuilderUtils.newJobId(appAttemptId.getApplicationId(), 0);\r\n    Job mockJob = mock(Job.class);\r\n    when(mockJob.getReport()).thenReturn(MRBuilderUtils.newJobReport(jobId, \"job\", \"user\", JobState.RUNNING, 0, 0, 0, 0, 0, 0, 0, \"jobfile\", null, false, \"\"));\r\n    MyContainerAllocator allocator = new MyContainerAllocator(rm, conf, appAttemptId, mockJob);\r\n    MockNM nodeManager1 = rm.registerNode(\"h1:1234\", 10240);\r\n    MockNM nodeManager2 = rm.registerNode(\"h2:1234\", 10240);\r\n    MockNM nodeManager3 = rm.registerNode(\"h3:1234\", 10240);\r\n    rm.drainEvents();\r\n    ContainerRequestEvent event1 = ContainerRequestCreator.createRequest(jobId, 1, Resource.newInstance(1024, 1), new String[] { \"h1\" });\r\n    allocator.sendRequest(event1);\r\n    ContainerRequestEvent event2 = ContainerRequestCreator.createRequest(jobId, 2, Resource.newInstance(1024, 1), new String[] { \"h2\" });\r\n    allocator.sendRequest(event2);\r\n    List<TaskAttemptContainerAssignedEvent> assigned = allocator.schedule();\r\n    rm.drainEvents();\r\n    Assert.assertEquals(\"No of assignments must be 0\", 0, assigned.size());\r\n    Assert.assertEquals(4, rm.getMyFifoScheduler().lastAsk.size());\r\n    ContainerRequestEvent event3 = ContainerRequestCreator.createRequest(jobId, 3, Resource.newInstance(1024, 1), new String[] { \"h3\" });\r\n    allocator.sendRequest(event3);\r\n    assigned = allocator.schedule();\r\n    rm.drainEvents();\r\n    Assert.assertEquals(\"No of assignments must be 0\", 0, assigned.size());\r\n    Assert.assertEquals(3, rm.getMyFifoScheduler().lastAsk.size());\r\n    nodeManager1.nodeHeartbeat(true);\r\n    nodeManager2.nodeHeartbeat(true);\r\n    nodeManager3.nodeHeartbeat(true);\r\n    rm.drainEvents();\r\n    assigned = allocator.schedule();\r\n    rm.drainEvents();\r\n    Assert.assertEquals(0, rm.getMyFifoScheduler().lastAsk.size());\r\n    checkAssignments(new ContainerRequestEvent[] { event1, event2, event3 }, assigned, false);\r\n    allocator.schedule();\r\n    rm.drainEvents();\r\n    Assert.assertEquals(5, rm.getMyFifoScheduler().lastAsk.size());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\rm",
  "methodName" : "testMapNodeLocality",
  "errType" : null,
  "containingMethodsNum" : 36,
  "sourceCodeText" : "void testMapNodeLocality() throws Exception\n{\r\n    LOG.info(\"Running testMapNodeLocality\");\r\n    Configuration conf = new Configuration();\r\n    MyResourceManager rm = new MyResourceManager(conf);\r\n    rm.start();\r\n    RMApp app = MockRMAppSubmitter.submitWithMemory(1024, rm);\r\n    rm.drainEvents();\r\n    MockNM amNodeManager = rm.registerNode(\"amNM:1234\", 2048);\r\n    amNodeManager.nodeHeartbeat(true);\r\n    rm.drainEvents();\r\n    ApplicationAttemptId appAttemptId = app.getCurrentAppAttempt().getAppAttemptId();\r\n    rm.sendAMLaunched(appAttemptId);\r\n    rm.drainEvents();\r\n    JobId jobId = MRBuilderUtils.newJobId(appAttemptId.getApplicationId(), 0);\r\n    Job mockJob = mock(Job.class);\r\n    when(mockJob.getReport()).thenReturn(MRBuilderUtils.newJobReport(jobId, \"job\", \"user\", JobState.RUNNING, 0, 0, 0, 0, 0, 0, 0, \"jobfile\", null, false, \"\"));\r\n    MyContainerAllocator allocator = new MyContainerAllocator(rm, conf, appAttemptId, mockJob);\r\n    MockNM nodeManager1 = rm.registerNode(\"h1:1234\", 3072);\r\n    rm.registerNode(\"h2:1234\", 10240);\r\n    MockNM nodeManager3 = rm.registerNode(\"h3:1234\", 1536);\r\n    rm.drainEvents();\r\n    ContainerRequestEvent event1 = ContainerRequestCreator.createRequest(jobId, 1, Resource.newInstance(1024, 1), new String[] { \"h1\" });\r\n    allocator.sendRequest(event1);\r\n    ContainerRequestEvent event2 = ContainerRequestCreator.createRequest(jobId, 2, Resource.newInstance(1024, 1), new String[] { \"h1\" });\r\n    allocator.sendRequest(event2);\r\n    ContainerRequestEvent event3 = ContainerRequestCreator.createRequest(jobId, 3, Resource.newInstance(1024, 1), new String[] { \"h2\" });\r\n    allocator.sendRequest(event3);\r\n    List<TaskAttemptContainerAssignedEvent> assigned = allocator.schedule();\r\n    rm.drainEvents();\r\n    Assert.assertEquals(\"No of assignments must be 0\", 0, assigned.size());\r\n    nodeManager3.nodeHeartbeat(true);\r\n    nodeManager1.nodeHeartbeat(true);\r\n    rm.drainEvents();\r\n    assigned = allocator.schedule();\r\n    rm.drainEvents();\r\n    checkAssignments(new ContainerRequestEvent[] { event1, event2, event3 }, assigned, false);\r\n    for (TaskAttemptContainerAssignedEvent event : assigned) {\r\n        if (event.getTaskAttemptID().equals(event3.getAttemptID())) {\r\n            assigned.remove(event);\r\n            Assert.assertEquals(\"h3\", event.getContainer().getNodeId().getHost());\r\n            break;\r\n        }\r\n    }\r\n    checkAssignments(new ContainerRequestEvent[] { event1, event2 }, assigned, true);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\rm",
  "methodName" : "testResource",
  "errType" : null,
  "containingMethodsNum" : 31,
  "sourceCodeText" : "void testResource() throws Exception\n{\r\n    LOG.info(\"Running testResource\");\r\n    Configuration conf = new Configuration();\r\n    MyResourceManager rm = new MyResourceManager(conf);\r\n    rm.start();\r\n    RMApp app = MockRMAppSubmitter.submitWithMemory(1024, rm);\r\n    rm.drainEvents();\r\n    MockNM amNodeManager = rm.registerNode(\"amNM:1234\", 2048);\r\n    amNodeManager.nodeHeartbeat(true);\r\n    rm.drainEvents();\r\n    ApplicationAttemptId appAttemptId = app.getCurrentAppAttempt().getAppAttemptId();\r\n    rm.sendAMLaunched(appAttemptId);\r\n    rm.drainEvents();\r\n    JobId jobId = MRBuilderUtils.newJobId(appAttemptId.getApplicationId(), 0);\r\n    Job mockJob = mock(Job.class);\r\n    when(mockJob.getReport()).thenReturn(MRBuilderUtils.newJobReport(jobId, \"job\", \"user\", JobState.RUNNING, 0, 0, 0, 0, 0, 0, 0, \"jobfile\", null, false, \"\"));\r\n    MyContainerAllocator allocator = new MyContainerAllocator(rm, conf, appAttemptId, mockJob);\r\n    MockNM nodeManager1 = rm.registerNode(\"h1:1234\", 10240);\r\n    MockNM nodeManager2 = rm.registerNode(\"h2:1234\", 10240);\r\n    MockNM nodeManager3 = rm.registerNode(\"h3:1234\", 10240);\r\n    rm.drainEvents();\r\n    ContainerRequestEvent event1 = ContainerRequestCreator.createRequest(jobId, 1, Resource.newInstance(1024, 1), new String[] { \"h1\" });\r\n    allocator.sendRequest(event1);\r\n    ContainerRequestEvent event2 = ContainerRequestCreator.createRequest(jobId, 2, Resource.newInstance(1024, 1), new String[] { \"h2\" });\r\n    allocator.sendRequest(event2);\r\n    List<TaskAttemptContainerAssignedEvent> assigned = allocator.schedule();\r\n    rm.drainEvents();\r\n    Assert.assertEquals(\"No of assignments must be 0\", 0, assigned.size());\r\n    nodeManager1.nodeHeartbeat(true);\r\n    nodeManager2.nodeHeartbeat(true);\r\n    nodeManager3.nodeHeartbeat(true);\r\n    rm.drainEvents();\r\n    assigned = allocator.schedule();\r\n    rm.drainEvents();\r\n    checkAssignments(new ContainerRequestEvent[] { event1, event2 }, assigned, false);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\rm",
  "methodName" : "testReducerRampdownDiagnostics",
  "errType" : null,
  "containingMethodsNum" : 27,
  "sourceCodeText" : "void testReducerRampdownDiagnostics() throws Exception\n{\r\n    LOG.info(\"Running tesReducerRampdownDiagnostics\");\r\n    final Configuration conf = new Configuration();\r\n    conf.setFloat(MRJobConfig.COMPLETED_MAPS_FOR_REDUCE_SLOWSTART, 0.0f);\r\n    final MyResourceManager rm = new MyResourceManager(conf);\r\n    rm.start();\r\n    final RMApp app = MockRMAppSubmitter.submitWithMemory(1024, rm);\r\n    rm.drainEvents();\r\n    final String host = \"host1\";\r\n    final MockNM nm = rm.registerNode(String.format(\"%s:1234\", host), 2048);\r\n    nm.nodeHeartbeat(true);\r\n    rm.drainEvents();\r\n    final ApplicationAttemptId appAttemptId = app.getCurrentAppAttempt().getAppAttemptId();\r\n    rm.sendAMLaunched(appAttemptId);\r\n    rm.drainEvents();\r\n    final JobId jobId = MRBuilderUtils.newJobId(appAttemptId.getApplicationId(), 0);\r\n    final Job mockJob = mock(Job.class);\r\n    when(mockJob.getReport()).thenReturn(MRBuilderUtils.newJobReport(jobId, \"job\", \"user\", JobState.RUNNING, 0, 0, 0, 0, 0, 0, 0, \"jobfile\", null, false, \"\"));\r\n    final MyContainerAllocator allocator = new MyContainerAllocator(rm, conf, appAttemptId, mockJob, SystemClock.getInstance());\r\n    rm.drainEvents();\r\n    final String[] locations = new String[] { host };\r\n    allocator.sendRequest(createRequest(jobId, 0, Resource.newInstance(1024, 1), locations, false, true));\r\n    for (int i = 0; i < 1; ) {\r\n        rm.drainEvents();\r\n        i += allocator.schedule().size();\r\n        nm.nodeHeartbeat(true);\r\n    }\r\n    allocator.sendRequest(createRequest(jobId, 0, Resource.newInstance(1024, 1), locations, true, false));\r\n    while (allocator.getTaskAttemptKillEvents().size() == 0) {\r\n        rm.drainEvents();\r\n        allocator.schedule().size();\r\n        nm.nodeHeartbeat(true);\r\n    }\r\n    final String killEventMessage = allocator.getTaskAttemptKillEvents().get(0).getMessage();\r\n    Assert.assertTrue(\"No reducer rampDown preemption message\", killEventMessage.contains(RMContainerAllocator.RAMPDOWN_DIAGNOSTIC));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\rm",
  "methodName" : "testPreemptReducers",
  "errType" : null,
  "containingMethodsNum" : 23,
  "sourceCodeText" : "void testPreemptReducers() throws Exception\n{\r\n    LOG.info(\"Running testPreemptReducers\");\r\n    Configuration conf = new Configuration();\r\n    MyResourceManager rm = new MyResourceManager(conf);\r\n    rm.start();\r\n    RMApp app = MockRMAppSubmitter.submitWithMemory(1024, rm);\r\n    rm.drainEvents();\r\n    MockNM amNodeManager = rm.registerNode(\"amNM:1234\", 2048);\r\n    amNodeManager.nodeHeartbeat(true);\r\n    rm.drainEvents();\r\n    ApplicationAttemptId appAttemptId = app.getCurrentAppAttempt().getAppAttemptId();\r\n    rm.sendAMLaunched(appAttemptId);\r\n    rm.drainEvents();\r\n    JobId jobId = MRBuilderUtils.newJobId(appAttemptId.getApplicationId(), 0);\r\n    Job mockJob = mock(Job.class);\r\n    when(mockJob.getReport()).thenReturn(MRBuilderUtils.newJobReport(jobId, \"job\", \"user\", JobState.RUNNING, 0, 0, 0, 0, 0, 0, 0, \"jobfile\", null, false, \"\"));\r\n    MyContainerAllocator allocator = new MyContainerAllocator(rm, conf, appAttemptId, mockJob, SystemClock.getInstance());\r\n    allocator.setMapResourceRequest(BuilderUtils.newResource(1024, 1));\r\n    allocator.setReduceResourceRequest(BuilderUtils.newResource(1024, 1));\r\n    RMContainerAllocator.AssignedRequests assignedRequests = allocator.getAssignedRequests();\r\n    RMContainerAllocator.ScheduledRequests scheduledRequests = allocator.getScheduledRequests();\r\n    ContainerRequestEvent event1 = createRequest(jobId, 1, Resource.newInstance(2048, 1), new String[] { \"h1\" }, false, false);\r\n    scheduledRequests.maps.put(mock(TaskAttemptId.class), new RMContainerRequestor.ContainerRequest(event1, null, null));\r\n    assignedRequests.reduces.put(mock(TaskAttemptId.class), mock(Container.class));\r\n    allocator.preemptReducesIfNeeded();\r\n    Assert.assertEquals(\"The reducer is not preempted\", 1, assignedRequests.preemptionWaitingReduces.size());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\rm",
  "methodName" : "testNonAggressivelyPreemptReducers",
  "errType" : null,
  "containingMethodsNum" : 28,
  "sourceCodeText" : "void testNonAggressivelyPreemptReducers() throws Exception\n{\r\n    LOG.info(\"Running testNonAggressivelyPreemptReducers\");\r\n    final int preemptThreshold = 2;\r\n    Configuration conf = new Configuration();\r\n    conf.setInt(MRJobConfig.MR_JOB_REDUCER_PREEMPT_DELAY_SEC, preemptThreshold);\r\n    MyResourceManager rm = new MyResourceManager(conf);\r\n    rm.start();\r\n    RMApp app = MockRMAppSubmitter.submitWithMemory(1024, rm);\r\n    rm.drainEvents();\r\n    MockNM amNodeManager = rm.registerNode(\"amNM:1234\", 2048);\r\n    amNodeManager.nodeHeartbeat(true);\r\n    rm.drainEvents();\r\n    ApplicationAttemptId appAttemptId = app.getCurrentAppAttempt().getAppAttemptId();\r\n    rm.sendAMLaunched(appAttemptId);\r\n    rm.drainEvents();\r\n    JobId jobId = MRBuilderUtils.newJobId(appAttemptId.getApplicationId(), 0);\r\n    Job mockJob = mock(Job.class);\r\n    when(mockJob.getReport()).thenReturn(MRBuilderUtils.newJobReport(jobId, \"job\", \"user\", JobState.RUNNING, 0, 0, 0, 0, 0, 0, 0, \"jobfile\", null, false, \"\"));\r\n    ControlledClock clock = new ControlledClock(null);\r\n    clock.setTime(1);\r\n    MyContainerAllocator allocator = new MyContainerAllocator(rm, conf, appAttemptId, mockJob, clock);\r\n    allocator.setMapResourceRequest(BuilderUtils.newResource(1024, 1));\r\n    allocator.setReduceResourceRequest(BuilderUtils.newResource(1024, 1));\r\n    RMContainerAllocator.AssignedRequests assignedRequests = allocator.getAssignedRequests();\r\n    RMContainerAllocator.ScheduledRequests scheduledRequests = allocator.getScheduledRequests();\r\n    ContainerRequestEvent event1 = createRequest(jobId, 1, Resource.newInstance(2048, 1), new String[] { \"h1\" }, false, false);\r\n    scheduledRequests.maps.put(mock(TaskAttemptId.class), new RMContainerRequestor.ContainerRequest(event1, null, clock.getTime()));\r\n    assignedRequests.reduces.put(mock(TaskAttemptId.class), mock(Container.class));\r\n    clock.setTime(clock.getTime() + 1);\r\n    allocator.preemptReducesIfNeeded();\r\n    Assert.assertEquals(\"The reducer is aggressively preeempted\", 0, assignedRequests.preemptionWaitingReduces.size());\r\n    clock.setTime(clock.getTime() + (preemptThreshold) * 1000);\r\n    allocator.preemptReducesIfNeeded();\r\n    Assert.assertEquals(\"The reducer is not preeempted\", 1, assignedRequests.preemptionWaitingReduces.size());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\rm",
  "methodName" : "testUnconditionalPreemptReducers",
  "errType" : null,
  "containingMethodsNum" : 30,
  "sourceCodeText" : "void testUnconditionalPreemptReducers() throws Exception\n{\r\n    LOG.info(\"Running testForcePreemptReducers\");\r\n    int forcePreemptThresholdSecs = 2;\r\n    Configuration conf = new Configuration();\r\n    conf.setInt(MRJobConfig.MR_JOB_REDUCER_PREEMPT_DELAY_SEC, 2 * forcePreemptThresholdSecs);\r\n    conf.setInt(MRJobConfig.MR_JOB_REDUCER_UNCONDITIONAL_PREEMPT_DELAY_SEC, forcePreemptThresholdSecs);\r\n    MyResourceManager rm = new MyResourceManager(conf);\r\n    rm.start();\r\n    rm.getMyFifoScheduler().forceResourceLimit(Resource.newInstance(8192, 8));\r\n    RMApp app = MockRMAppSubmitter.submitWithMemory(1024, rm);\r\n    rm.drainEvents();\r\n    MockNM amNodeManager = rm.registerNode(\"amNM:1234\", 2048);\r\n    amNodeManager.nodeHeartbeat(true);\r\n    rm.drainEvents();\r\n    ApplicationAttemptId appAttemptId = app.getCurrentAppAttempt().getAppAttemptId();\r\n    rm.sendAMLaunched(appAttemptId);\r\n    rm.drainEvents();\r\n    JobId jobId = MRBuilderUtils.newJobId(appAttemptId.getApplicationId(), 0);\r\n    Job mockJob = mock(Job.class);\r\n    when(mockJob.getReport()).thenReturn(MRBuilderUtils.newJobReport(jobId, \"job\", \"user\", JobState.RUNNING, 0, 0, 0, 0, 0, 0, 0, \"jobfile\", null, false, \"\"));\r\n    ControlledClock clock = new ControlledClock(null);\r\n    clock.setTime(1);\r\n    MyContainerAllocator allocator = new MyContainerAllocator(rm, conf, appAttemptId, mockJob, clock);\r\n    allocator.setMapResourceRequest(BuilderUtils.newResource(1024, 1));\r\n    allocator.setReduceResourceRequest(BuilderUtils.newResource(1024, 1));\r\n    RMContainerAllocator.AssignedRequests assignedRequests = allocator.getAssignedRequests();\r\n    RMContainerAllocator.ScheduledRequests scheduledRequests = allocator.getScheduledRequests();\r\n    ContainerRequestEvent event1 = createRequest(jobId, 1, Resource.newInstance(2048, 1), new String[] { \"h1\" }, false, false);\r\n    scheduledRequests.maps.put(mock(TaskAttemptId.class), new RMContainerRequestor.ContainerRequest(event1, null, clock.getTime()));\r\n    assignedRequests.reduces.put(mock(TaskAttemptId.class), mock(Container.class));\r\n    clock.setTime(clock.getTime() + 1);\r\n    allocator.preemptReducesIfNeeded();\r\n    Assert.assertEquals(\"The reducer is preeempted too soon\", 0, assignedRequests.preemptionWaitingReduces.size());\r\n    clock.setTime(clock.getTime() + 1000 * forcePreemptThresholdSecs);\r\n    allocator.preemptReducesIfNeeded();\r\n    Assert.assertEquals(\"The reducer is not preeempted\", 1, assignedRequests.preemptionWaitingReduces.size());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\rm",
  "methodName" : "testExcessReduceContainerAssign",
  "errType" : null,
  "containingMethodsNum" : 24,
  "sourceCodeText" : "void testExcessReduceContainerAssign() throws Exception\n{\r\n    final Configuration conf = new Configuration();\r\n    conf.setFloat(MRJobConfig.COMPLETED_MAPS_FOR_REDUCE_SLOWSTART, 0.0f);\r\n    final MyResourceManager2 rm = new MyResourceManager2(conf);\r\n    rm.start();\r\n    final RMApp app = MockRMAppSubmitter.submitWithMemory(2048, rm);\r\n    rm.drainEvents();\r\n    final String host = \"host1\";\r\n    final MockNM nm = rm.registerNode(String.format(\"%s:1234\", host), 4096);\r\n    nm.nodeHeartbeat(true);\r\n    rm.drainEvents();\r\n    final ApplicationAttemptId appAttemptId = app.getCurrentAppAttempt().getAppAttemptId();\r\n    rm.sendAMLaunched(appAttemptId);\r\n    rm.drainEvents();\r\n    final JobId jobId = MRBuilderUtils.newJobId(appAttemptId.getApplicationId(), 0);\r\n    final Job mockJob = mock(Job.class);\r\n    when(mockJob.getReport()).thenReturn(MRBuilderUtils.newJobReport(jobId, \"job\", \"user\", JobState.RUNNING, 0, 0, 0, 0, 0, 0, 0, \"jobfile\", null, false, \"\"));\r\n    final MyContainerAllocator allocator = new MyContainerAllocator(rm, conf, appAttemptId, mockJob, SystemClock.getInstance());\r\n    final String[] locations = new String[] { host };\r\n    allocator.sendRequest(createRequest(jobId, 0, Resource.newInstance(1024, 1), locations, false, true));\r\n    allocator.scheduleAllReduces();\r\n    allocator.makeRemoteRequest();\r\n    nm.nodeHeartbeat(true);\r\n    rm.drainEvents();\r\n    allocator.sendRequest(createRequest(jobId, 1, Resource.newInstance(1024, 1), locations, false, false));\r\n    int assignedContainer;\r\n    for (assignedContainer = 0; assignedContainer < 1; ) {\r\n        assignedContainer += allocator.schedule().size();\r\n        nm.nodeHeartbeat(true);\r\n        rm.drainEvents();\r\n    }\r\n    assertThat(assignedContainer).isEqualTo(1);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\rm",
  "methodName" : "testMapReduceAllocationWithNodeLabelExpression",
  "errType" : null,
  "containingMethodsNum" : 26,
  "sourceCodeText" : "void testMapReduceAllocationWithNodeLabelExpression() throws Exception\n{\r\n    LOG.info(\"Running testMapReduceAllocationWithNodeLabelExpression\");\r\n    Configuration conf = new Configuration();\r\n    conf.setFloat(MRJobConfig.COMPLETED_MAPS_FOR_REDUCE_SLOWSTART, 1.0f);\r\n    conf.set(MRJobConfig.MAP_NODE_LABEL_EXP, \"MapNodes\");\r\n    conf.set(MRJobConfig.REDUCE_NODE_LABEL_EXP, \"ReduceNodes\");\r\n    ApplicationId appId = ApplicationId.newInstance(1, 1);\r\n    ApplicationAttemptId appAttemptId = ApplicationAttemptId.newInstance(appId, 1);\r\n    JobId jobId = MRBuilderUtils.newJobId(appAttemptId.getApplicationId(), 0);\r\n    Job mockJob = mock(Job.class);\r\n    when(mockJob.getReport()).thenReturn(MRBuilderUtils.newJobReport(jobId, \"job\", \"user\", JobState.RUNNING, 0, 0, 0, 0, 0, 0, 0, \"jobfile\", null, false, \"\"));\r\n    final MockScheduler mockScheduler = new MockScheduler(appAttemptId);\r\n    MyContainerAllocator allocator = new MyContainerAllocator(null, conf, appAttemptId, mockJob, SystemClock.getInstance()) {\r\n\r\n        @Override\r\n        protected void register() {\r\n        }\r\n\r\n        @Override\r\n        protected ApplicationMasterProtocol createSchedulerProxy() {\r\n            return mockScheduler;\r\n        }\r\n    };\r\n    ContainerRequestEvent reqMapEvents;\r\n    reqMapEvents = ContainerRequestCreator.createRequest(jobId, 0, Resource.newInstance(1024, 1), new String[] { \"map\" });\r\n    allocator.sendRequests(Arrays.asList(reqMapEvents));\r\n    ContainerRequestEvent reqReduceEvents;\r\n    reqReduceEvents = createRequest(jobId, 0, Resource.newInstance(2048, 1), new String[] { \"reduce\" }, false, true);\r\n    allocator.sendRequests(Arrays.asList(reqReduceEvents));\r\n    allocator.schedule();\r\n    Assert.assertEquals(3, mockScheduler.lastAsk.size());\r\n    validateLabelsRequests(mockScheduler.lastAsk.get(0), false);\r\n    validateLabelsRequests(mockScheduler.lastAsk.get(1), false);\r\n    validateLabelsRequests(mockScheduler.lastAsk.get(2), false);\r\n    ContainerId cid0 = mockScheduler.assignContainer(\"map\", false);\r\n    allocator.schedule();\r\n    Assert.assertEquals(3, mockScheduler.lastAsk.size());\r\n    validateLabelsRequests(mockScheduler.lastAsk.get(0), true);\r\n    validateLabelsRequests(mockScheduler.lastAsk.get(1), true);\r\n    validateLabelsRequests(mockScheduler.lastAsk.get(2), true);\r\n    allocator.close();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\rm",
  "methodName" : "validateLabelsRequests",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void validateLabelsRequests(ResourceRequest resourceRequest, boolean isReduce)\n{\r\n    switch(resourceRequest.getResourceName()) {\r\n        case \"map\":\r\n        case \"reduce\":\r\n        case NetworkTopology.DEFAULT_RACK:\r\n            Assert.assertNull(resourceRequest.getNodeLabelExpression());\r\n            break;\r\n        case \"*\":\r\n            Assert.assertEquals(isReduce ? \"ReduceNodes\" : \"MapNodes\", resourceRequest.getNodeLabelExpression());\r\n            break;\r\n        default:\r\n            Assert.fail(\"Invalid resource location \" + resourceRequest.getResourceName());\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\rm",
  "methodName" : "testUpdateCollectorInfo",
  "errType" : null,
  "containingMethodsNum" : 38,
  "sourceCodeText" : "void testUpdateCollectorInfo() throws Exception\n{\r\n    LOG.info(\"Running testUpdateCollectorInfo\");\r\n    Configuration conf = new Configuration();\r\n    conf.setBoolean(YarnConfiguration.TIMELINE_SERVICE_ENABLED, true);\r\n    conf.setFloat(YarnConfiguration.TIMELINE_SERVICE_VERSION, 2.0f);\r\n    ApplicationId appId = ApplicationId.newInstance(1, 1);\r\n    ApplicationAttemptId attemptId = ApplicationAttemptId.newInstance(appId, 1);\r\n    JobId jobId = MRBuilderUtils.newJobId(appId, 0);\r\n    Job mockJob = mock(Job.class);\r\n    when(mockJob.getReport()).thenReturn(MRBuilderUtils.newJobReport(jobId, \"job\", \"user\", JobState.RUNNING, 0, 0, 0, 0, 0, 0, 0, \"jobfile\", null, false, \"\"));\r\n    String localAddr = \"localhost:1234\";\r\n    UserGroupInformation ugi = UserGroupInformation.getCurrentUser();\r\n    TimelineDelegationTokenIdentifier ident = new TimelineDelegationTokenIdentifier(new Text(ugi.getUserName()), new Text(\"renewer\"), null);\r\n    ident.setSequenceNumber(1);\r\n    Token<TimelineDelegationTokenIdentifier> collectorToken = new Token<TimelineDelegationTokenIdentifier>(ident.getBytes(), new byte[0], TimelineDelegationTokenIdentifier.KIND_NAME, new Text(localAddr));\r\n    org.apache.hadoop.yarn.api.records.Token token = org.apache.hadoop.yarn.api.records.Token.newInstance(collectorToken.getIdentifier(), collectorToken.getKind().toString(), collectorToken.getPassword(), collectorToken.getService().toString());\r\n    CollectorInfo collectorInfo = CollectorInfo.newInstance(localAddr, token);\r\n    final MockSchedulerForTimelineCollector mockScheduler = new MockSchedulerForTimelineCollector(collectorInfo);\r\n    MyContainerAllocator allocator = new MyContainerAllocator(null, conf, attemptId, mockJob, SystemClock.getInstance()) {\r\n\r\n        @Override\r\n        protected void register() {\r\n        }\r\n\r\n        @Override\r\n        protected ApplicationMasterProtocol createSchedulerProxy() {\r\n            return mockScheduler;\r\n        }\r\n    };\r\n    ArrayList<Token<? extends TokenIdentifier>> tokens = new ArrayList<>(ugi.getTokens());\r\n    assertEquals(0, tokens.size());\r\n    TimelineV2Client client = spy(TimelineV2Client.createTimelineClient(appId));\r\n    client.init(conf);\r\n    when(((RunningAppContext) allocator.getContext()).getTimelineV2Client()).thenReturn(client);\r\n    allocator.schedule();\r\n    verify(client).setTimelineCollectorInfo(collectorInfo);\r\n    tokens = new ArrayList<>(ugi.getTokens());\r\n    assertEquals(1, tokens.size());\r\n    assertEquals(TimelineDelegationTokenIdentifier.KIND_NAME, tokens.get(0).getKind());\r\n    assertEquals(collectorToken.decodeIdentifier(), tokens.get(0).decodeIdentifier());\r\n    ident.setSequenceNumber(100);\r\n    Token<TimelineDelegationTokenIdentifier> collectorToken1 = new Token<TimelineDelegationTokenIdentifier>(ident.getBytes(), new byte[0], TimelineDelegationTokenIdentifier.KIND_NAME, new Text(localAddr));\r\n    token = org.apache.hadoop.yarn.api.records.Token.newInstance(collectorToken1.getIdentifier(), collectorToken1.getKind().toString(), collectorToken1.getPassword(), collectorToken1.getService().toString());\r\n    collectorInfo = CollectorInfo.newInstance(localAddr, token);\r\n    mockScheduler.updateCollectorInfo(collectorInfo);\r\n    allocator.schedule();\r\n    verify(client).setTimelineCollectorInfo(collectorInfo);\r\n    tokens = new ArrayList<>(ugi.getTokens());\r\n    assertEquals(1, tokens.size());\r\n    assertEquals(TimelineDelegationTokenIdentifier.KIND_NAME, tokens.get(0).getKind());\r\n    assertEquals(collectorToken1.decodeIdentifier(), tokens.get(0).decodeIdentifier());\r\n    allocator.close();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\rm",
  "methodName" : "testMapReduceScheduling",
  "errType" : null,
  "containingMethodsNum" : 35,
  "sourceCodeText" : "void testMapReduceScheduling() throws Exception\n{\r\n    LOG.info(\"Running testMapReduceScheduling\");\r\n    Configuration conf = new Configuration();\r\n    MyResourceManager rm = new MyResourceManager(conf);\r\n    rm.start();\r\n    RMApp app = MockRMAppSubmitter.submitWithMemory(1024, rm);\r\n    rm.drainEvents();\r\n    MockNM amNodeManager = rm.registerNode(\"amNM:1234\", 2048);\r\n    amNodeManager.nodeHeartbeat(true);\r\n    rm.drainEvents();\r\n    ApplicationAttemptId appAttemptId = app.getCurrentAppAttempt().getAppAttemptId();\r\n    rm.sendAMLaunched(appAttemptId);\r\n    rm.drainEvents();\r\n    JobId jobId = MRBuilderUtils.newJobId(appAttemptId.getApplicationId(), 0);\r\n    Job mockJob = mock(Job.class);\r\n    when(mockJob.getReport()).thenReturn(MRBuilderUtils.newJobReport(jobId, \"job\", \"user\", JobState.RUNNING, 0, 0, 0, 0, 0, 0, 0, \"jobfile\", null, false, \"\"));\r\n    MyContainerAllocator allocator = new MyContainerAllocator(rm, conf, appAttemptId, mockJob, SystemClock.getInstance());\r\n    MockNM nodeManager1 = rm.registerNode(\"h1:1234\", 1024);\r\n    MockNM nodeManager2 = rm.registerNode(\"h2:1234\", 10240);\r\n    MockNM nodeManager3 = rm.registerNode(\"h3:1234\", 10240);\r\n    rm.drainEvents();\r\n    ContainerRequestEvent event1 = createRequest(jobId, 1, Resource.newInstance(2048, 1), new String[] { \"h1\", \"h2\" }, true, false);\r\n    allocator.sendRequest(event1);\r\n    ContainerRequestEvent event2 = createRequest(jobId, 2, Resource.newInstance(3000, 1), new String[] { \"h1\" }, false, true);\r\n    allocator.sendRequest(event2);\r\n    ContainerRequestEvent event3 = createRequest(jobId, 3, Resource.newInstance(2048, 1), new String[] { \"h3\" }, false, false);\r\n    allocator.sendRequest(event3);\r\n    List<TaskAttemptContainerAssignedEvent> assigned = allocator.schedule();\r\n    rm.drainEvents();\r\n    Assert.assertEquals(\"No of assignments must be 0\", 0, assigned.size());\r\n    nodeManager1.nodeHeartbeat(true);\r\n    nodeManager2.nodeHeartbeat(true);\r\n    nodeManager3.nodeHeartbeat(true);\r\n    rm.drainEvents();\r\n    assigned = allocator.schedule();\r\n    rm.drainEvents();\r\n    checkAssignments(new ContainerRequestEvent[] { event1, event3 }, assigned, false);\r\n    for (TaskAttemptContainerAssignedEvent assig : assigned) {\r\n        Assert.assertFalse(\"Assigned count not correct\", \"h1\".equals(assig.getContainer().getNodeId().getHost()));\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\rm",
  "methodName" : "testReportedAppProgress",
  "errType" : null,
  "containingMethodsNum" : 68,
  "sourceCodeText" : "void testReportedAppProgress() throws Exception\n{\r\n    LOG.info(\"Running testReportedAppProgress\");\r\n    Configuration conf = new Configuration();\r\n    final MyResourceManager rm = new MyResourceManager(conf);\r\n    rm.start();\r\n    DrainDispatcher rmDispatcher = (DrainDispatcher) rm.getRMContext().getDispatcher();\r\n    RMApp rmApp = MockRMAppSubmitter.submitWithMemory(1024, rm);\r\n    rm.drainEvents();\r\n    MockNM amNodeManager = rm.registerNode(\"amNM:1234\", 21504);\r\n    amNodeManager.nodeHeartbeat(true);\r\n    rm.drainEvents();\r\n    final ApplicationAttemptId appAttemptId = rmApp.getCurrentAppAttempt().getAppAttemptId();\r\n    rm.sendAMLaunched(appAttemptId);\r\n    rm.drainEvents();\r\n    MRApp mrApp = new MRApp(appAttemptId, ContainerId.newContainerId(appAttemptId, 0), 10, 10, false, this.getClass().getName(), true, 1) {\r\n\r\n        @Override\r\n        protected Dispatcher createDispatcher() {\r\n            return new DrainDispatcher();\r\n        }\r\n\r\n        protected ContainerAllocator createContainerAllocator(ClientService clientService, AppContext context) {\r\n            return new MyContainerAllocator(rm, appAttemptId, context);\r\n        }\r\n    };\r\n    Assert.assertEquals(0.0, rmApp.getProgress(), 0.0);\r\n    mrApp.submit(conf);\r\n    Job job = mrApp.getContext().getAllJobs().entrySet().iterator().next().getValue();\r\n    DrainDispatcher amDispatcher = (DrainDispatcher) mrApp.getDispatcher();\r\n    MyContainerAllocator allocator = (MyContainerAllocator) mrApp.getContainerAllocator();\r\n    mrApp.waitForInternalState((JobImpl) job, JobStateInternal.RUNNING);\r\n    amDispatcher.await();\r\n    for (Task t : job.getTasks().values()) {\r\n        if (t.getType() == TaskType.MAP) {\r\n            mrApp.waitForInternalState((TaskAttemptImpl) t.getAttempts().values().iterator().next(), TaskAttemptStateInternal.UNASSIGNED);\r\n        }\r\n    }\r\n    amDispatcher.await();\r\n    allocator.schedule();\r\n    rm.drainEvents();\r\n    amNodeManager.nodeHeartbeat(true);\r\n    rm.drainEvents();\r\n    allocator.schedule();\r\n    rm.drainEvents();\r\n    for (Task t : job.getTasks().values()) {\r\n        if (t.getType() == TaskType.MAP) {\r\n            mrApp.waitForState(t, TaskState.RUNNING);\r\n        }\r\n    }\r\n    allocator.schedule();\r\n    rm.drainEvents();\r\n    Assert.assertEquals(0.05f, job.getProgress(), 0.001f);\r\n    Assert.assertEquals(0.05f, rmApp.getProgress(), 0.001f);\r\n    Iterator<Task> it = job.getTasks().values().iterator();\r\n    finishNextNTasks(rmDispatcher, amNodeManager, mrApp, it, 1);\r\n    allocator.schedule();\r\n    rm.drainEvents();\r\n    Assert.assertEquals(0.095f, job.getProgress(), 0.001f);\r\n    Assert.assertEquals(0.095f, rmApp.getProgress(), 0.001f);\r\n    finishNextNTasks(rmDispatcher, amNodeManager, mrApp, it, 7);\r\n    allocator.schedule();\r\n    rm.drainEvents();\r\n    Assert.assertEquals(0.41f, job.getProgress(), 0.001f);\r\n    Assert.assertEquals(0.41f, rmApp.getProgress(), 0.001f);\r\n    finishNextNTasks(rmDispatcher, amNodeManager, mrApp, it, 2);\r\n    allocator.schedule();\r\n    rm.drainEvents();\r\n    amNodeManager.nodeHeartbeat(true);\r\n    rm.drainEvents();\r\n    allocator.schedule();\r\n    rm.drainEvents();\r\n    for (Task t : job.getTasks().values()) {\r\n        if (t.getType() == TaskType.REDUCE) {\r\n            mrApp.waitForState(t, TaskState.RUNNING);\r\n        }\r\n    }\r\n    finishNextNTasks(rmDispatcher, amNodeManager, mrApp, it, 2);\r\n    allocator.schedule();\r\n    rm.drainEvents();\r\n    Assert.assertEquals(0.59f, job.getProgress(), 0.001f);\r\n    Assert.assertEquals(0.59f, rmApp.getProgress(), 0.001f);\r\n    finishNextNTasks(rmDispatcher, amNodeManager, mrApp, it, 8);\r\n    allocator.schedule();\r\n    rm.drainEvents();\r\n    Assert.assertEquals(0.95f, job.getProgress(), 0.001f);\r\n    Assert.assertEquals(0.95f, rmApp.getProgress(), 0.001f);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\rm",
  "methodName" : "finishNextNTasks",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void finishNextNTasks(DrainDispatcher rmDispatcher, MockNM node, MRApp mrApp, Iterator<Task> it, int nextN) throws Exception\n{\r\n    Task task;\r\n    for (int i = 0; i < nextN; i++) {\r\n        task = it.next();\r\n        finishTask(rmDispatcher, node, mrApp, task);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\rm",
  "methodName" : "finishTask",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void finishTask(DrainDispatcher rmDispatcher, MockNM node, MRApp mrApp, Task task) throws Exception\n{\r\n    TaskAttempt attempt = task.getAttempts().values().iterator().next();\r\n    List<ContainerStatus> contStatus = new ArrayList<ContainerStatus>(1);\r\n    contStatus.add(ContainerStatus.newInstance(attempt.getAssignedContainerID(), ContainerState.COMPLETE, \"\", 0));\r\n    Map<ApplicationId, List<ContainerStatus>> statusUpdate = new HashMap<ApplicationId, List<ContainerStatus>>(1);\r\n    statusUpdate.put(mrApp.getAppID(), contStatus);\r\n    node.nodeHeartbeat(statusUpdate, true);\r\n    rmDispatcher.await();\r\n    mrApp.getContext().getEventHandler().handle(new TaskAttemptEvent(attempt.getID(), TaskAttemptEventType.TA_DONE));\r\n    mrApp.waitForState(task, TaskState.SUCCEEDED);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\rm",
  "methodName" : "testReportedAppProgressWithOnlyMaps",
  "errType" : null,
  "containingMethodsNum" : 51,
  "sourceCodeText" : "void testReportedAppProgressWithOnlyMaps() throws Exception\n{\r\n    LOG.info(\"Running testReportedAppProgressWithOnlyMaps\");\r\n    Configuration conf = new Configuration();\r\n    final MyResourceManager rm = new MyResourceManager(conf);\r\n    rm.start();\r\n    DrainDispatcher rmDispatcher = (DrainDispatcher) rm.getRMContext().getDispatcher();\r\n    RMApp rmApp = MockRMAppSubmitter.submitWithMemory(1024, rm);\r\n    rm.drainEvents();\r\n    MockNM amNodeManager = rm.registerNode(\"amNM:1234\", 11264);\r\n    amNodeManager.nodeHeartbeat(true);\r\n    rm.drainEvents();\r\n    final ApplicationAttemptId appAttemptId = rmApp.getCurrentAppAttempt().getAppAttemptId();\r\n    rm.sendAMLaunched(appAttemptId);\r\n    rm.drainEvents();\r\n    MRApp mrApp = new MRApp(appAttemptId, ContainerId.newContainerId(appAttemptId, 0), 10, 0, false, this.getClass().getName(), true, 1) {\r\n\r\n        @Override\r\n        protected Dispatcher createDispatcher() {\r\n            return new DrainDispatcher();\r\n        }\r\n\r\n        protected ContainerAllocator createContainerAllocator(ClientService clientService, AppContext context) {\r\n            return new MyContainerAllocator(rm, appAttemptId, context);\r\n        }\r\n    };\r\n    Assert.assertEquals(0.0, rmApp.getProgress(), 0.0);\r\n    mrApp.submit(conf);\r\n    Job job = mrApp.getContext().getAllJobs().entrySet().iterator().next().getValue();\r\n    DrainDispatcher amDispatcher = (DrainDispatcher) mrApp.getDispatcher();\r\n    MyContainerAllocator allocator = (MyContainerAllocator) mrApp.getContainerAllocator();\r\n    mrApp.waitForInternalState((JobImpl) job, JobStateInternal.RUNNING);\r\n    amDispatcher.await();\r\n    for (Task t : job.getTasks().values()) {\r\n        mrApp.waitForInternalState((TaskAttemptImpl) t.getAttempts().values().iterator().next(), TaskAttemptStateInternal.UNASSIGNED);\r\n    }\r\n    amDispatcher.await();\r\n    allocator.schedule();\r\n    rm.drainEvents();\r\n    amNodeManager.nodeHeartbeat(true);\r\n    rm.drainEvents();\r\n    allocator.schedule();\r\n    rm.drainEvents();\r\n    for (Task t : job.getTasks().values()) {\r\n        mrApp.waitForState(t, TaskState.RUNNING);\r\n    }\r\n    allocator.schedule();\r\n    rm.drainEvents();\r\n    Assert.assertEquals(0.05f, job.getProgress(), 0.001f);\r\n    Assert.assertEquals(0.05f, rmApp.getProgress(), 0.001f);\r\n    Iterator<Task> it = job.getTasks().values().iterator();\r\n    finishNextNTasks(rmDispatcher, amNodeManager, mrApp, it, 1);\r\n    allocator.schedule();\r\n    rm.drainEvents();\r\n    Assert.assertEquals(0.14f, job.getProgress(), 0.001f);\r\n    Assert.assertEquals(0.14f, rmApp.getProgress(), 0.001f);\r\n    finishNextNTasks(rmDispatcher, amNodeManager, mrApp, it, 5);\r\n    allocator.schedule();\r\n    rm.drainEvents();\r\n    Assert.assertEquals(0.59f, job.getProgress(), 0.001f);\r\n    Assert.assertEquals(0.59f, rmApp.getProgress(), 0.001f);\r\n    finishNextNTasks(rmDispatcher, amNodeManager, mrApp, it, 4);\r\n    allocator.schedule();\r\n    rm.drainEvents();\r\n    Assert.assertEquals(0.95f, job.getProgress(), 0.001f);\r\n    Assert.assertEquals(0.95f, rmApp.getProgress(), 0.001f);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\rm",
  "methodName" : "testUpdatedNodes",
  "errType" : null,
  "containingMethodsNum" : 52,
  "sourceCodeText" : "void testUpdatedNodes() throws Exception\n{\r\n    Configuration conf = new Configuration();\r\n    MyResourceManager rm = new MyResourceManager(conf);\r\n    rm.start();\r\n    RMApp app = MockRMAppSubmitter.submitWithMemory(1024, rm);\r\n    rm.drainEvents();\r\n    MockNM amNodeManager = rm.registerNode(\"amNM:1234\", 2048);\r\n    amNodeManager.nodeHeartbeat(true);\r\n    rm.drainEvents();\r\n    ApplicationAttemptId appAttemptId = app.getCurrentAppAttempt().getAppAttemptId();\r\n    rm.sendAMLaunched(appAttemptId);\r\n    rm.drainEvents();\r\n    JobId jobId = MRBuilderUtils.newJobId(appAttemptId.getApplicationId(), 0);\r\n    Job mockJob = mock(Job.class);\r\n    MyContainerAllocator allocator = new MyContainerAllocator(rm, conf, appAttemptId, mockJob);\r\n    MockNM nm1 = rm.registerNode(\"h1:1234\", 10240);\r\n    MockNM nm2 = rm.registerNode(\"h2:1234\", 10240);\r\n    rm.drainEvents();\r\n    ContainerRequestEvent event = ContainerRequestCreator.createRequest(jobId, 1, Resource.newInstance(1024, 1), new String[] { \"h1\" });\r\n    allocator.sendRequest(event);\r\n    TaskAttemptId attemptId = event.getAttemptID();\r\n    TaskAttempt mockTaskAttempt = mock(TaskAttempt.class);\r\n    when(mockTaskAttempt.getNodeId()).thenReturn(nm1.getNodeId());\r\n    Task mockTask = mock(Task.class);\r\n    when(mockTask.getAttempt(attemptId)).thenReturn(mockTaskAttempt);\r\n    when(mockJob.getTask(attemptId.getTaskId())).thenReturn(mockTask);\r\n    List<TaskAttemptContainerAssignedEvent> assigned = allocator.schedule();\r\n    rm.drainEvents();\r\n    nm1.nodeHeartbeat(true);\r\n    rm.drainEvents();\r\n    Assert.assertEquals(1, allocator.getJobUpdatedNodeEvents().size());\r\n    Assert.assertEquals(3, allocator.getJobUpdatedNodeEvents().get(0).getUpdatedNodes().size());\r\n    allocator.getJobUpdatedNodeEvents().clear();\r\n    assigned = allocator.schedule();\r\n    rm.drainEvents();\r\n    Assert.assertEquals(1, assigned.size());\r\n    Assert.assertEquals(nm1.getNodeId(), assigned.get(0).getContainer().getNodeId());\r\n    Assert.assertTrue(allocator.getJobUpdatedNodeEvents().isEmpty());\r\n    Assert.assertTrue(allocator.getTaskAttemptKillEvents().isEmpty());\r\n    nm1.nodeHeartbeat(false);\r\n    nm2.nodeHeartbeat(false);\r\n    rm.drainEvents();\r\n    assigned = allocator.schedule();\r\n    rm.drainEvents();\r\n    Assert.assertEquals(0, assigned.size());\r\n    Assert.assertEquals(1, allocator.getJobUpdatedNodeEvents().size());\r\n    Assert.assertEquals(1, allocator.getTaskAttemptKillEvents().size());\r\n    Assert.assertEquals(2, allocator.getJobUpdatedNodeEvents().get(0).getUpdatedNodes().size());\r\n    Assert.assertEquals(attemptId, allocator.getTaskAttemptKillEvents().get(0).getTaskAttemptID());\r\n    allocator.getJobUpdatedNodeEvents().clear();\r\n    allocator.getTaskAttemptKillEvents().clear();\r\n    assigned = allocator.schedule();\r\n    rm.drainEvents();\r\n    Assert.assertEquals(0, assigned.size());\r\n    Assert.assertTrue(allocator.getJobUpdatedNodeEvents().isEmpty());\r\n    Assert.assertTrue(allocator.getTaskAttemptKillEvents().isEmpty());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\rm",
  "methodName" : "testBlackListedNodes",
  "errType" : null,
  "containingMethodsNum" : 55,
  "sourceCodeText" : "void testBlackListedNodes() throws Exception\n{\r\n    LOG.info(\"Running testBlackListedNodes\");\r\n    Configuration conf = new Configuration();\r\n    conf.setBoolean(MRJobConfig.MR_AM_JOB_NODE_BLACKLISTING_ENABLE, true);\r\n    conf.setInt(MRJobConfig.MAX_TASK_FAILURES_PER_TRACKER, 1);\r\n    conf.setInt(MRJobConfig.MR_AM_IGNORE_BLACKLISTING_BLACKLISTED_NODE_PERECENT, -1);\r\n    MyResourceManager rm = new MyResourceManager(conf);\r\n    rm.start();\r\n    RMApp app = MockRMAppSubmitter.submitWithMemory(1024, rm);\r\n    rm.drainEvents();\r\n    MockNM amNodeManager = rm.registerNode(\"amNM:1234\", 2048);\r\n    amNodeManager.nodeHeartbeat(true);\r\n    rm.drainEvents();\r\n    ApplicationAttemptId appAttemptId = app.getCurrentAppAttempt().getAppAttemptId();\r\n    rm.sendAMLaunched(appAttemptId);\r\n    rm.drainEvents();\r\n    JobId jobId = MRBuilderUtils.newJobId(appAttemptId.getApplicationId(), 0);\r\n    Job mockJob = mock(Job.class);\r\n    when(mockJob.getReport()).thenReturn(MRBuilderUtils.newJobReport(jobId, \"job\", \"user\", JobState.RUNNING, 0, 0, 0, 0, 0, 0, 0, \"jobfile\", null, false, \"\"));\r\n    MyContainerAllocator allocator = new MyContainerAllocator(rm, conf, appAttemptId, mockJob);\r\n    MockNM nodeManager1 = rm.registerNode(\"h1:1234\", 10240);\r\n    MockNM nodeManager2 = rm.registerNode(\"h2:1234\", 10240);\r\n    MockNM nodeManager3 = rm.registerNode(\"h3:1234\", 10240);\r\n    rm.drainEvents();\r\n    ContainerRequestEvent event1 = ContainerRequestCreator.createRequest(jobId, 1, Resource.newInstance(1024, 1), new String[] { \"h1\" });\r\n    allocator.sendRequest(event1);\r\n    ContainerRequestEvent event2 = ContainerRequestCreator.createRequest(jobId, 2, Resource.newInstance(1024, 1), new String[] { \"h2\" });\r\n    allocator.sendRequest(event2);\r\n    ContainerRequestEvent event3 = ContainerRequestCreator.createRequest(jobId, 3, Resource.newInstance(1024, 1), new String[] { \"h3\" });\r\n    allocator.sendRequest(event3);\r\n    List<TaskAttemptContainerAssignedEvent> assigned = allocator.schedule();\r\n    rm.drainEvents();\r\n    Assert.assertEquals(\"No of assignments must be 0\", 0, assigned.size());\r\n    ContainerFailedEvent f1 = createFailEvent(jobId, 1, \"h1\", false);\r\n    allocator.sendFailure(f1);\r\n    ContainerFailedEvent f2 = createFailEvent(jobId, 1, \"h2\", false);\r\n    allocator.sendFailure(f2);\r\n    nodeManager1.nodeHeartbeat(true);\r\n    nodeManager2.nodeHeartbeat(true);\r\n    rm.drainEvents();\r\n    assigned = allocator.schedule();\r\n    Assert.assertEquals(\"No of assignments must be 0\", 0, assigned.size());\r\n    rm.drainEvents();\r\n    Assert.assertEquals(\"No of assignments must be 0\", 0, assigned.size());\r\n    assertBlacklistAdditionsAndRemovals(2, 0, rm);\r\n    nodeManager1.nodeHeartbeat(false);\r\n    nodeManager2.nodeHeartbeat(false);\r\n    rm.drainEvents();\r\n    assigned = allocator.schedule();\r\n    rm.drainEvents();\r\n    assertBlacklistAdditionsAndRemovals(0, 0, rm);\r\n    Assert.assertEquals(\"No of assignments must be 0\", 0, assigned.size());\r\n    nodeManager3.nodeHeartbeat(true);\r\n    rm.drainEvents();\r\n    assigned = allocator.schedule();\r\n    rm.drainEvents();\r\n    assertBlacklistAdditionsAndRemovals(0, 0, rm);\r\n    Assert.assertTrue(\"No of assignments must be 3\", assigned.size() == 3);\r\n    for (TaskAttemptContainerAssignedEvent assig : assigned) {\r\n        Assert.assertTrue(\"Assigned container host not correct\", \"h3\".equals(assig.getContainer().getNodeId().getHost()));\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\rm",
  "methodName" : "testIgnoreBlacklisting",
  "errType" : null,
  "containingMethodsNum" : 56,
  "sourceCodeText" : "void testIgnoreBlacklisting() throws Exception\n{\r\n    LOG.info(\"Running testIgnoreBlacklisting\");\r\n    Configuration conf = new Configuration();\r\n    conf.setBoolean(MRJobConfig.MR_AM_JOB_NODE_BLACKLISTING_ENABLE, true);\r\n    conf.setInt(MRJobConfig.MAX_TASK_FAILURES_PER_TRACKER, 1);\r\n    conf.setInt(MRJobConfig.MR_AM_IGNORE_BLACKLISTING_BLACKLISTED_NODE_PERECENT, 33);\r\n    MyResourceManager rm = new MyResourceManager(conf);\r\n    rm.start();\r\n    RMApp app = MockRMAppSubmitter.submitWithMemory(1024, rm);\r\n    rm.drainEvents();\r\n    MockNM[] nodeManagers = new MockNM[10];\r\n    int nmNum = 0;\r\n    List<TaskAttemptContainerAssignedEvent> assigned = null;\r\n    nodeManagers[nmNum] = registerNodeManager(nmNum++, rm);\r\n    nodeManagers[0].nodeHeartbeat(true);\r\n    rm.drainEvents();\r\n    ApplicationAttemptId appAttemptId = app.getCurrentAppAttempt().getAppAttemptId();\r\n    rm.sendAMLaunched(appAttemptId);\r\n    rm.drainEvents();\r\n    JobId jobId = MRBuilderUtils.newJobId(appAttemptId.getApplicationId(), 0);\r\n    Job mockJob = mock(Job.class);\r\n    when(mockJob.getReport()).thenReturn(MRBuilderUtils.newJobReport(jobId, \"job\", \"user\", JobState.RUNNING, 0, 0, 0, 0, 0, 0, 0, \"jobfile\", null, false, \"\"));\r\n    MyContainerAllocator allocator = new MyContainerAllocator(rm, conf, appAttemptId, mockJob);\r\n    assigned = getContainerOnHost(jobId, 1, 1024, new String[] { \"h1\" }, nodeManagers[0], allocator, 0, 0, 0, 0, rm);\r\n    Assert.assertEquals(\"No of assignments must be 1\", 1, assigned.size());\r\n    LOG.info(\"Failing container _1 on H1 (Node should be blacklisted and\" + \" ignore blacklisting enabled\");\r\n    ContainerFailedEvent f1 = createFailEvent(jobId, 1, \"h1\", false);\r\n    allocator.sendFailure(f1);\r\n    assigned = getContainerOnHost(jobId, 2, 1024, new String[] { \"h1\" }, nodeManagers[0], allocator, 1, 0, 0, 1, rm);\r\n    Assert.assertEquals(\"No of assignments must be 0\", 0, assigned.size());\r\n    assigned = getContainerOnHost(jobId, 2, 1024, new String[] { \"h1\" }, nodeManagers[0], allocator, 0, 0, 0, 0, rm);\r\n    Assert.assertEquals(\"No of assignments must be 1\", 1, assigned.size());\r\n    nodeManagers[nmNum] = registerNodeManager(nmNum++, rm);\r\n    assigned = getContainerOnHost(jobId, 3, 1024, new String[] { \"h2\" }, nodeManagers[1], allocator, 0, 0, 0, 0, rm);\r\n    Assert.assertEquals(\"No of assignments must be 1\", 1, assigned.size());\r\n    nodeManagers[nmNum] = registerNodeManager(nmNum++, rm);\r\n    assigned = getContainerOnHost(jobId, 4, 1024, new String[] { \"h3\" }, nodeManagers[2], allocator, 0, 0, 0, 0, rm);\r\n    Assert.assertEquals(\"No of assignments must be 1\", 1, assigned.size());\r\n    assigned = getContainerOnHost(jobId, 5, 1024, new String[] { \"h1\" }, nodeManagers[0], allocator, 0, 0, 0, 0, rm);\r\n    Assert.assertEquals(\"No of assignments must be 1\", 1, assigned.size());\r\n    nodeManagers[nmNum] = registerNodeManager(nmNum++, rm);\r\n    assigned = getContainerOnHost(jobId, 6, 1024, new String[] { \"h4\" }, nodeManagers[3], allocator, 0, 0, 1, 0, rm);\r\n    Assert.assertEquals(\"No of assignments must be 1\", 1, assigned.size());\r\n    assigned = getContainerOnHost(jobId, 7, 1024, new String[] { \"h1\" }, nodeManagers[0], allocator, 0, 0, 0, 0, rm);\r\n    Assert.assertEquals(\"No of assignments must be 0\", 0, assigned.size());\r\n    ContainerFailedEvent f2 = createFailEvent(jobId, 3, \"h2\", false);\r\n    allocator.sendFailure(f2);\r\n    assigned = getContainerOnHost(jobId, 8, 1024, new String[] { \"h1\" }, nodeManagers[0], allocator, 1, 0, 0, 2, rm);\r\n    Assert.assertEquals(\"No of assignments must be 0\", 0, assigned.size());\r\n    assigned = getContainerOnHost(jobId, 8, 1024, new String[] { \"h1\" }, nodeManagers[0], allocator, 0, 0, 0, 0, rm);\r\n    Assert.assertEquals(\"No of assignments must be 2\", 2, assigned.size());\r\n    assigned = getContainerOnHost(jobId, 9, 1024, new String[] { \"h2\" }, nodeManagers[1], allocator, 0, 0, 0, 0, rm);\r\n    Assert.assertEquals(\"No of assignments must be 1\", 1, assigned.size());\r\n    ContainerFailedEvent f3 = createFailEvent(jobId, 4, \"h3\", false);\r\n    allocator.sendFailure(f3);\r\n    nodeManagers[nmNum] = registerNodeManager(nmNum++, rm);\r\n    assigned = getContainerOnHost(jobId, 10, 1024, new String[] { \"h3\" }, nodeManagers[2], allocator, 0, 0, 0, 0, rm);\r\n    Assert.assertEquals(\"No of assignments must be 1\", 1, assigned.size());\r\n    for (int i = 0; i < 5; i++) {\r\n        nodeManagers[nmNum] = registerNodeManager(nmNum++, rm);\r\n        assigned = getContainerOnHost(jobId, 11 + i, 1024, new String[] { String.valueOf(5 + i) }, nodeManagers[4 + i], allocator, 0, 0, (i == 4 ? 3 : 0), 0, rm);\r\n        Assert.assertEquals(\"No of assignments must be 1\", 1, assigned.size());\r\n    }\r\n    assigned = getContainerOnHost(jobId, 20, 1024, new String[] { \"h3\" }, nodeManagers[2], allocator, 0, 0, 0, 0, rm);\r\n    Assert.assertEquals(\"No of assignments must be 0\", 0, assigned.size());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\rm",
  "methodName" : "registerNodeManager",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "MockNM registerNodeManager(int i, MyResourceManager rm) throws Exception\n{\r\n    MockNM nm = rm.registerNode(\"h\" + (i + 1) + \":1234\", 10240);\r\n    rm.drainEvents();\r\n    return nm;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\rm",
  "methodName" : "getContainerOnHost",
  "errType" : null,
  "containingMethodsNum" : 11,
  "sourceCodeText" : "List<TaskAttemptContainerAssignedEvent> getContainerOnHost(JobId jobId, int taskAttemptId, int memory, String[] hosts, MockNM mockNM, MyContainerAllocator allocator, int expectedAdditions1, int expectedRemovals1, int expectedAdditions2, int expectedRemovals2, MyResourceManager rm) throws Exception\n{\r\n    ContainerRequestEvent reqEvent = ContainerRequestCreator.createRequest(jobId, taskAttemptId, Resource.newInstance(memory, 1), hosts);\r\n    allocator.sendRequest(reqEvent);\r\n    List<TaskAttemptContainerAssignedEvent> assigned = allocator.schedule();\r\n    rm.drainEvents();\r\n    assertBlacklistAdditionsAndRemovals(expectedAdditions1, expectedRemovals1, rm);\r\n    Assert.assertEquals(\"No of assignments must be 0\", 0, assigned.size());\r\n    mockNM.nodeHeartbeat(true);\r\n    rm.drainEvents();\r\n    assigned = allocator.schedule();\r\n    rm.drainEvents();\r\n    assertBlacklistAdditionsAndRemovals(expectedAdditions2, expectedRemovals2, rm);\r\n    return assigned;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\rm",
  "methodName" : "testBlackListedNodesWithSchedulingToThatNode",
  "errType" : null,
  "containingMethodsNum" : 68,
  "sourceCodeText" : "void testBlackListedNodesWithSchedulingToThatNode() throws Exception\n{\r\n    LOG.info(\"Running testBlackListedNodesWithSchedulingToThatNode\");\r\n    Configuration conf = new Configuration();\r\n    conf.setBoolean(MRJobConfig.MR_AM_JOB_NODE_BLACKLISTING_ENABLE, true);\r\n    conf.setInt(MRJobConfig.MAX_TASK_FAILURES_PER_TRACKER, 1);\r\n    conf.setInt(MRJobConfig.MR_AM_IGNORE_BLACKLISTING_BLACKLISTED_NODE_PERECENT, -1);\r\n    MyResourceManager rm = new MyResourceManager(conf);\r\n    rm.start();\r\n    RMApp app = MockRMAppSubmitter.submitWithMemory(1024, rm);\r\n    rm.drainEvents();\r\n    MockNM amNodeManager = rm.registerNode(\"amNM:1234\", 2048);\r\n    amNodeManager.nodeHeartbeat(true);\r\n    rm.drainEvents();\r\n    ApplicationAttemptId appAttemptId = app.getCurrentAppAttempt().getAppAttemptId();\r\n    rm.sendAMLaunched(appAttemptId);\r\n    rm.drainEvents();\r\n    JobId jobId = MRBuilderUtils.newJobId(appAttemptId.getApplicationId(), 0);\r\n    Job mockJob = mock(Job.class);\r\n    when(mockJob.getReport()).thenReturn(MRBuilderUtils.newJobReport(jobId, \"job\", \"user\", JobState.RUNNING, 0, 0, 0, 0, 0, 0, 0, \"jobfile\", null, false, \"\"));\r\n    MyContainerAllocator allocator = new MyContainerAllocator(rm, conf, appAttemptId, mockJob);\r\n    MockNM nodeManager1 = rm.registerNode(\"h1:1234\", 10240);\r\n    MockNM nodeManager3 = rm.registerNode(\"h3:1234\", 10240);\r\n    rm.drainEvents();\r\n    LOG.info(\"Requesting 1 Containers _1 on H1\");\r\n    ContainerRequestEvent event1 = ContainerRequestCreator.createRequest(jobId, 1, Resource.newInstance(1024, 1), new String[] { \"h1\" });\r\n    allocator.sendRequest(event1);\r\n    LOG.info(\"RM Heartbeat (to send the container requests)\");\r\n    List<TaskAttemptContainerAssignedEvent> assigned = allocator.schedule();\r\n    rm.drainEvents();\r\n    Assert.assertEquals(\"No of assignments must be 0\", 0, assigned.size());\r\n    LOG.info(\"h1 Heartbeat (To actually schedule the containers)\");\r\n    nodeManager1.nodeHeartbeat(true);\r\n    rm.drainEvents();\r\n    LOG.info(\"RM Heartbeat (To process the scheduled containers)\");\r\n    assigned = allocator.schedule();\r\n    rm.drainEvents();\r\n    assertBlacklistAdditionsAndRemovals(0, 0, rm);\r\n    Assert.assertEquals(\"No of assignments must be 1\", 1, assigned.size());\r\n    LOG.info(\"Failing container _1 on H1 (should blacklist the node)\");\r\n    ContainerFailedEvent f1 = createFailEvent(jobId, 1, \"h1\", false);\r\n    allocator.sendFailure(f1);\r\n    ContainerRequestEvent event1f = createRequest(jobId, 1, Resource.newInstance(1024, 1), new String[] { \"h1\" }, true, false);\r\n    allocator.sendRequest(event1f);\r\n    assigned = allocator.schedule();\r\n    rm.drainEvents();\r\n    assertBlacklistAdditionsAndRemovals(1, 0, rm);\r\n    Assert.assertEquals(\"No of assignments must be 0\", 0, assigned.size());\r\n    ContainerRequestEvent event3 = ContainerRequestCreator.createRequest(jobId, 3, Resource.newInstance(1024, 1), new String[] { \"h1\", \"h3\" });\r\n    allocator.sendRequest(event3);\r\n    LOG.info(\"h1 Heartbeat (To actually schedule the containers)\");\r\n    nodeManager1.nodeHeartbeat(true);\r\n    rm.drainEvents();\r\n    LOG.info(\"RM Heartbeat (To process the scheduled containers)\");\r\n    assigned = allocator.schedule();\r\n    rm.drainEvents();\r\n    assertBlacklistAdditionsAndRemovals(0, 0, rm);\r\n    Assert.assertEquals(\"No of assignments must be 0\", 0, assigned.size());\r\n    LOG.info(\"RM Heartbeat (To process the re-scheduled containers)\");\r\n    assigned = allocator.schedule();\r\n    rm.drainEvents();\r\n    assertBlacklistAdditionsAndRemovals(0, 0, rm);\r\n    Assert.assertEquals(\"No of assignments must be 0\", 0, assigned.size());\r\n    LOG.info(\"h3 Heartbeat (To re-schedule the containers)\");\r\n    nodeManager3.nodeHeartbeat(true);\r\n    rm.drainEvents();\r\n    LOG.info(\"RM Heartbeat (To process the re-scheduled containers for H3)\");\r\n    assigned = allocator.schedule();\r\n    assertBlacklistAdditionsAndRemovals(0, 0, rm);\r\n    rm.drainEvents();\r\n    for (TaskAttemptContainerAssignedEvent assig : assigned) {\r\n        LOG.info(assig.getTaskAttemptID() + \" assgined to \" + assig.getContainer().getId() + \" with priority \" + assig.getContainer().getPriority());\r\n    }\r\n    Assert.assertEquals(\"No of assignments must be 2\", 2, assigned.size());\r\n    for (TaskAttemptContainerAssignedEvent assig : assigned) {\r\n        Assert.assertEquals(\"Assigned container \" + assig.getContainer().getId() + \" host not correct\", \"h3\", assig.getContainer().getNodeId().getHost());\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\rm",
  "methodName" : "assertBlacklistAdditionsAndRemovals",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void assertBlacklistAdditionsAndRemovals(int expectedAdditions, int expectedRemovals, MyResourceManager rm)\n{\r\n    Assert.assertEquals(expectedAdditions, rm.getMyFifoScheduler().lastBlacklistAdditions.size());\r\n    Assert.assertEquals(expectedRemovals, rm.getMyFifoScheduler().lastBlacklistRemovals.size());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\rm",
  "methodName" : "assertAsksAndReleases",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void assertAsksAndReleases(int expectedAsk, int expectedRelease, MyResourceManager rm)\n{\r\n    Assert.assertEquals(expectedAsk, rm.getMyFifoScheduler().lastAsk.size());\r\n    Assert.assertEquals(expectedRelease, rm.getMyFifoScheduler().lastRelease.size());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\rm",
  "methodName" : "createFailEvent",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "ContainerFailedEvent createFailEvent(JobId jobId, int taskAttemptId, String host, boolean reduce)\n{\r\n    TaskId taskId;\r\n    if (reduce) {\r\n        taskId = MRBuilderUtils.newTaskId(jobId, 0, TaskType.REDUCE);\r\n    } else {\r\n        taskId = MRBuilderUtils.newTaskId(jobId, 0, TaskType.MAP);\r\n    }\r\n    TaskAttemptId attemptId = MRBuilderUtils.newTaskAttemptId(taskId, taskAttemptId);\r\n    return new ContainerFailedEvent(attemptId, host);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\rm",
  "methodName" : "createDeallocateEvent",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "ContainerAllocatorEvent createDeallocateEvent(JobId jobId, int taskAttemptId, boolean reduce)\n{\r\n    TaskId taskId;\r\n    if (reduce) {\r\n        taskId = MRBuilderUtils.newTaskId(jobId, 0, TaskType.REDUCE);\r\n    } else {\r\n        taskId = MRBuilderUtils.newTaskId(jobId, 0, TaskType.MAP);\r\n    }\r\n    TaskAttemptId attemptId = MRBuilderUtils.newTaskAttemptId(taskId, taskAttemptId);\r\n    return new ContainerAllocatorEvent(attemptId, ContainerAllocator.EventType.CONTAINER_DEALLOCATE);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\rm",
  "methodName" : "checkAssignments",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void checkAssignments(ContainerRequestEvent[] requests, List<TaskAttemptContainerAssignedEvent> assignments, boolean checkHostMatch)\n{\r\n    Assert.assertNotNull(\"Container not assigned\", assignments);\r\n    Assert.assertEquals(\"Assigned count not correct\", requests.length, assignments.size());\r\n    Set<ContainerId> containerIds = new HashSet<ContainerId>();\r\n    for (TaskAttemptContainerAssignedEvent assigned : assignments) {\r\n        containerIds.add(assigned.getContainer().getId());\r\n    }\r\n    Assert.assertEquals(\"Assigned containers must be different\", assignments.size(), containerIds.size());\r\n    for (ContainerRequestEvent req : requests) {\r\n        TaskAttemptContainerAssignedEvent assigned = null;\r\n        for (TaskAttemptContainerAssignedEvent ass : assignments) {\r\n            if (ass.getTaskAttemptID().equals(req.getAttemptID())) {\r\n                assigned = ass;\r\n                break;\r\n            }\r\n        }\r\n        checkAssignment(req, assigned, checkHostMatch);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\rm",
  "methodName" : "checkAssignment",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void checkAssignment(ContainerRequestEvent request, TaskAttemptContainerAssignedEvent assigned, boolean checkHostMatch)\n{\r\n    Assert.assertNotNull(\"Nothing assigned to attempt \" + request.getAttemptID(), assigned);\r\n    Assert.assertEquals(\"assigned to wrong attempt\", request.getAttemptID(), assigned.getTaskAttemptID());\r\n    if (checkHostMatch) {\r\n        Assert.assertTrue(\"Not assigned to requested host\", Arrays.asList(request.getHosts()).contains(assigned.getContainer().getNodeId().getHost()));\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\rm",
  "methodName" : "testIfApplicationPriorityIsNotSet",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void testIfApplicationPriorityIsNotSet()\n{\r\n    Job mockJob = mock(Job.class);\r\n    RMCommunicator communicator = mock(RMCommunicator.class);\r\n    ClientService service = mock(ClientService.class);\r\n    AppContext context = mock(AppContext.class);\r\n    AMPreemptionPolicy policy = mock(AMPreemptionPolicy.class);\r\n    when(communicator.getJob()).thenReturn(mockJob);\r\n    RMContainerAllocator allocator = new RMContainerAllocator(service, context, policy);\r\n    AllocateResponse response = Records.newRecord(AllocateResponse.class);\r\n    allocator.handleJobPriorityChange(response);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\rm",
  "methodName" : "testReduceScheduling",
  "errType" : null,
  "containingMethodsNum" : 30,
  "sourceCodeText" : "void testReduceScheduling() throws Exception\n{\r\n    int totalMaps = 10;\r\n    int succeededMaps = 1;\r\n    int scheduledMaps = 10;\r\n    int scheduledReduces = 0;\r\n    int assignedMaps = 2;\r\n    int assignedReduces = 0;\r\n    Resource mapResourceReqt = BuilderUtils.newResource(1024, 1);\r\n    Resource reduceResourceReqt = BuilderUtils.newResource(2 * 1024, 1);\r\n    int numPendingReduces = 4;\r\n    float maxReduceRampupLimit = 0.5f;\r\n    float reduceSlowStart = 0.2f;\r\n    RMContainerAllocator allocator = mock(RMContainerAllocator.class);\r\n    doCallRealMethod().when(allocator).scheduleReduces(anyInt(), anyInt(), anyInt(), anyInt(), anyInt(), anyInt(), any(Resource.class), any(Resource.class), anyInt(), anyFloat(), anyFloat());\r\n    doReturn(EnumSet.of(SchedulerResourceTypes.MEMORY)).when(allocator).getSchedulerResourceTypes();\r\n    allocator.scheduleReduces(totalMaps, succeededMaps, scheduledMaps, scheduledReduces, assignedMaps, assignedReduces, mapResourceReqt, reduceResourceReqt, numPendingReduces, maxReduceRampupLimit, reduceSlowStart);\r\n    verify(allocator, never()).setIsReduceStarted(true);\r\n    allocator.scheduleReduces(totalMaps, succeededMaps, 0, scheduledReduces, totalMaps - succeededMaps, assignedReduces, mapResourceReqt, reduceResourceReqt, numPendingReduces, maxReduceRampupLimit, reduceSlowStart);\r\n    verify(allocator, never()).setIsReduceStarted(true);\r\n    verify(allocator, never()).scheduleAllReduces();\r\n    succeededMaps = 3;\r\n    doReturn(BuilderUtils.newResource(0, 0)).when(allocator).getResourceLimit();\r\n    allocator.scheduleReduces(totalMaps, succeededMaps, scheduledMaps, scheduledReduces, assignedMaps, assignedReduces, mapResourceReqt, reduceResourceReqt, numPendingReduces, maxReduceRampupLimit, reduceSlowStart);\r\n    verify(allocator, times(1)).setIsReduceStarted(true);\r\n    doReturn(BuilderUtils.newResource(100 * 1024, 100 * 1)).when(allocator).getResourceLimit();\r\n    allocator.scheduleReduces(totalMaps, succeededMaps, scheduledMaps, scheduledReduces, assignedMaps, assignedReduces, mapResourceReqt, reduceResourceReqt, numPendingReduces, maxReduceRampupLimit, reduceSlowStart);\r\n    verify(allocator).rampUpReduces(anyInt());\r\n    verify(allocator, never()).rampDownReduces(anyInt());\r\n    scheduledReduces = 3;\r\n    doReturn(BuilderUtils.newResource(10 * 1024, 10 * 1)).when(allocator).getResourceLimit();\r\n    allocator.scheduleReduces(totalMaps, succeededMaps, scheduledMaps, scheduledReduces, assignedMaps, assignedReduces, mapResourceReqt, reduceResourceReqt, numPendingReduces, maxReduceRampupLimit, reduceSlowStart);\r\n    verify(allocator).rampDownReduces(anyInt());\r\n    scheduledMaps = 2;\r\n    assignedReduces = 2;\r\n    doReturn(BuilderUtils.newResource(10 * 1024, 10 * 1)).when(allocator).getResourceLimit();\r\n    allocator.scheduleReduces(totalMaps, succeededMaps, scheduledMaps, scheduledReduces, assignedMaps, assignedReduces, mapResourceReqt, reduceResourceReqt, numPendingReduces, maxReduceRampupLimit, reduceSlowStart);\r\n    verify(allocator, times(2)).rampDownReduces(anyInt());\r\n    doReturn(EnumSet.of(SchedulerResourceTypes.MEMORY, SchedulerResourceTypes.CPU)).when(allocator).getSchedulerResourceTypes();\r\n    scheduledMaps = 10;\r\n    assignedReduces = 0;\r\n    doReturn(BuilderUtils.newResource(100 * 1024, 5 * 1)).when(allocator).getResourceLimit();\r\n    allocator.scheduleReduces(totalMaps, succeededMaps, scheduledMaps, scheduledReduces, assignedMaps, assignedReduces, mapResourceReqt, reduceResourceReqt, numPendingReduces, maxReduceRampupLimit, reduceSlowStart);\r\n    verify(allocator, times(3)).rampDownReduces(anyInt());\r\n    doReturn(BuilderUtils.newResource(10 * 1024, 100 * 1)).when(allocator).getResourceLimit();\r\n    allocator.scheduleReduces(totalMaps, succeededMaps, scheduledMaps, scheduledReduces, assignedMaps, assignedReduces, mapResourceReqt, reduceResourceReqt, numPendingReduces, maxReduceRampupLimit, reduceSlowStart);\r\n    verify(allocator, times(4)).rampDownReduces(anyInt());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\rm",
  "methodName" : "testCompletedTasksRecalculateSchedule",
  "errType" : null,
  "containingMethodsNum" : 22,
  "sourceCodeText" : "void testCompletedTasksRecalculateSchedule() throws Exception\n{\r\n    LOG.info(\"Running testCompletedTasksRecalculateSchedule\");\r\n    Configuration conf = new Configuration();\r\n    final MyResourceManager rm = new MyResourceManager(conf);\r\n    rm.start();\r\n    RMApp app = MockRMAppSubmitter.submitWithMemory(1024, rm);\r\n    rm.drainEvents();\r\n    MockNM amNodeManager = rm.registerNode(\"amNM:1234\", 2048);\r\n    amNodeManager.nodeHeartbeat(true);\r\n    rm.drainEvents();\r\n    ApplicationAttemptId appAttemptId = app.getCurrentAppAttempt().getAppAttemptId();\r\n    rm.sendAMLaunched(appAttemptId);\r\n    rm.drainEvents();\r\n    JobId jobId = MRBuilderUtils.newJobId(appAttemptId.getApplicationId(), 0);\r\n    Job job = mock(Job.class);\r\n    when(job.getReport()).thenReturn(MRBuilderUtils.newJobReport(jobId, \"job\", \"user\", JobState.RUNNING, 0, 0, 0, 0, 0, 0, 0, \"jobfile\", null, false, \"\"));\r\n    doReturn(10).when(job).getTotalMaps();\r\n    doReturn(10).when(job).getTotalReduces();\r\n    doReturn(0).when(job).getCompletedMaps();\r\n    RecalculateContainerAllocator allocator = new RecalculateContainerAllocator(rm, conf, appAttemptId, job);\r\n    allocator.schedule();\r\n    allocator.recalculatedReduceSchedule = false;\r\n    allocator.schedule();\r\n    Assert.assertFalse(\"Unexpected recalculate of reduce schedule\", allocator.recalculatedReduceSchedule);\r\n    doReturn(1).when(job).getCompletedMaps();\r\n    allocator.schedule();\r\n    Assert.assertTrue(\"Expected recalculate of reduce schedule\", allocator.recalculatedReduceSchedule);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\rm",
  "methodName" : "testHeartbeatHandler",
  "errType" : null,
  "containingMethodsNum" : 23,
  "sourceCodeText" : "void testHeartbeatHandler() throws Exception\n{\r\n    LOG.info(\"Running testHeartbeatHandler\");\r\n    Configuration conf = new Configuration();\r\n    conf.setInt(MRJobConfig.MR_AM_TO_RM_HEARTBEAT_INTERVAL_MS, 1);\r\n    ControlledClock clock = new ControlledClock();\r\n    AppContext appContext = mock(AppContext.class);\r\n    when(appContext.getClock()).thenReturn(clock);\r\n    when(appContext.getApplicationID()).thenReturn(ApplicationId.newInstance(1, 1));\r\n    RMContainerAllocator allocator = new RMContainerAllocator(mock(ClientService.class), appContext, new NoopAMPreemptionPolicy()) {\r\n\r\n        @Override\r\n        protected void register() {\r\n        }\r\n\r\n        @Override\r\n        protected ApplicationMasterProtocol createSchedulerProxy() {\r\n            return mock(ApplicationMasterProtocol.class);\r\n        }\r\n\r\n        @Override\r\n        protected synchronized void heartbeat() throws Exception {\r\n        }\r\n    };\r\n    allocator.init(conf);\r\n    allocator.start();\r\n    clock.setTime(5);\r\n    int timeToWaitMs = 5000;\r\n    while (allocator.getLastHeartbeatTime() != 5 && timeToWaitMs > 0) {\r\n        Thread.sleep(10);\r\n        timeToWaitMs -= 10;\r\n    }\r\n    Assert.assertEquals(5, allocator.getLastHeartbeatTime());\r\n    clock.setTime(7);\r\n    timeToWaitMs = 5000;\r\n    while (allocator.getLastHeartbeatTime() != 7 && timeToWaitMs > 0) {\r\n        Thread.sleep(10);\r\n        timeToWaitMs -= 10;\r\n    }\r\n    Assert.assertEquals(7, allocator.getLastHeartbeatTime());\r\n    final AtomicBoolean callbackCalled = new AtomicBoolean(false);\r\n    allocator.runOnNextHeartbeat(new Runnable() {\r\n\r\n        @Override\r\n        public void run() {\r\n            callbackCalled.set(true);\r\n        }\r\n    });\r\n    clock.setTime(8);\r\n    timeToWaitMs = 5000;\r\n    while (allocator.getLastHeartbeatTime() != 8 && timeToWaitMs > 0) {\r\n        Thread.sleep(10);\r\n        timeToWaitMs -= 10;\r\n    }\r\n    Assert.assertEquals(8, allocator.getLastHeartbeatTime());\r\n    Assert.assertTrue(callbackCalled.get());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\rm",
  "methodName" : "testCompletedContainerEvent",
  "errType" : null,
  "containingMethodsNum" : 26,
  "sourceCodeText" : "void testCompletedContainerEvent()\n{\r\n    RMContainerAllocator allocator = new RMContainerAllocator(mock(ClientService.class), mock(AppContext.class), new NoopAMPreemptionPolicy());\r\n    TaskAttemptId attemptId = MRBuilderUtils.newTaskAttemptId(MRBuilderUtils.newTaskId(MRBuilderUtils.newJobId(1, 1, 1), 1, TaskType.MAP), 1);\r\n    ApplicationId applicationId = ApplicationId.newInstance(1, 1);\r\n    ApplicationAttemptId applicationAttemptId = ApplicationAttemptId.newInstance(applicationId, 1);\r\n    ContainerId containerId = ContainerId.newContainerId(applicationAttemptId, 1);\r\n    ContainerStatus status = ContainerStatus.newInstance(containerId, ContainerState.RUNNING, \"\", 0);\r\n    ContainerStatus abortedStatus = ContainerStatus.newInstance(containerId, ContainerState.RUNNING, \"\", ContainerExitStatus.ABORTED);\r\n    TaskAttemptEvent event = allocator.createContainerFinishedEvent(status, attemptId);\r\n    Assert.assertEquals(TaskAttemptEventType.TA_CONTAINER_COMPLETED, event.getType());\r\n    TaskAttemptEvent abortedEvent = allocator.createContainerFinishedEvent(abortedStatus, attemptId);\r\n    Assert.assertEquals(TaskAttemptEventType.TA_KILL, abortedEvent.getType());\r\n    ContainerId containerId2 = ContainerId.newContainerId(applicationAttemptId, 2);\r\n    ContainerStatus status2 = ContainerStatus.newInstance(containerId2, ContainerState.RUNNING, \"\", 0);\r\n    ContainerStatus preemptedStatus = ContainerStatus.newInstance(containerId2, ContainerState.RUNNING, \"\", ContainerExitStatus.PREEMPTED);\r\n    TaskAttemptEvent event2 = allocator.createContainerFinishedEvent(status2, attemptId);\r\n    Assert.assertEquals(TaskAttemptEventType.TA_CONTAINER_COMPLETED, event2.getType());\r\n    TaskAttemptEvent abortedEvent2 = allocator.createContainerFinishedEvent(preemptedStatus, attemptId);\r\n    Assert.assertEquals(TaskAttemptEventType.TA_KILL, abortedEvent2.getType());\r\n    ContainerId containerId3 = ContainerId.newContainerId(applicationAttemptId, 3);\r\n    ContainerStatus status3 = ContainerStatus.newInstance(containerId3, ContainerState.RUNNING, \"\", 0);\r\n    ContainerStatus killedByContainerSchedulerStatus = ContainerStatus.newInstance(containerId3, ContainerState.RUNNING, \"\", ContainerExitStatus.KILLED_BY_CONTAINER_SCHEDULER);\r\n    TaskAttemptEvent event3 = allocator.createContainerFinishedEvent(status3, attemptId);\r\n    Assert.assertEquals(TaskAttemptEventType.TA_CONTAINER_COMPLETED, event3.getType());\r\n    TaskAttemptEvent abortedEvent3 = allocator.createContainerFinishedEvent(killedByContainerSchedulerStatus, attemptId);\r\n    Assert.assertEquals(TaskAttemptEventType.TA_KILL, abortedEvent3.getType());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\rm",
  "methodName" : "testUnregistrationOnlyIfRegistered",
  "errType" : null,
  "containingMethodsNum" : 18,
  "sourceCodeText" : "void testUnregistrationOnlyIfRegistered() throws Exception\n{\r\n    Configuration conf = new Configuration();\r\n    final MyResourceManager rm = new MyResourceManager(conf);\r\n    rm.start();\r\n    RMApp rmApp = MockRMAppSubmitter.submitWithMemory(1024, rm);\r\n    rm.drainEvents();\r\n    MockNM amNodeManager = rm.registerNode(\"127.0.0.1:1234\", 11264);\r\n    amNodeManager.nodeHeartbeat(true);\r\n    rm.drainEvents();\r\n    final ApplicationAttemptId appAttemptId = rmApp.getCurrentAppAttempt().getAppAttemptId();\r\n    rm.sendAMLaunched(appAttemptId);\r\n    rm.drainEvents();\r\n    MRApp mrApp = new MRApp(appAttemptId, ContainerId.newContainerId(appAttemptId, 0), 10, 0, false, this.getClass().getName(), true, 1) {\r\n\r\n        @Override\r\n        protected Dispatcher createDispatcher() {\r\n            return new DrainDispatcher();\r\n        }\r\n\r\n        protected ContainerAllocator createContainerAllocator(ClientService clientService, AppContext context) {\r\n            return new MyContainerAllocator(rm, appAttemptId, context);\r\n        }\r\n    };\r\n    mrApp.submit(conf);\r\n    DrainDispatcher amDispatcher = (DrainDispatcher) mrApp.getDispatcher();\r\n    MyContainerAllocator allocator = (MyContainerAllocator) mrApp.getContainerAllocator();\r\n    amDispatcher.await();\r\n    Assert.assertTrue(allocator.isApplicationMasterRegistered());\r\n    mrApp.stop();\r\n    Assert.assertTrue(allocator.isUnregistered());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\rm",
  "methodName" : "testRMContainerAllocatorResendsRequestsOnRMRestart",
  "errType" : null,
  "containingMethodsNum" : 83,
  "sourceCodeText" : "void testRMContainerAllocatorResendsRequestsOnRMRestart() throws Exception\n{\r\n    Configuration conf = new Configuration();\r\n    conf.set(YarnConfiguration.RECOVERY_ENABLED, \"true\");\r\n    conf.set(YarnConfiguration.RM_STORE, MemoryRMStateStore.class.getName());\r\n    conf.setInt(YarnConfiguration.RM_AM_MAX_ATTEMPTS, YarnConfiguration.DEFAULT_RM_AM_MAX_ATTEMPTS);\r\n    conf.setBoolean(YarnConfiguration.RM_WORK_PRESERVING_RECOVERY_ENABLED, true);\r\n    conf.setLong(YarnConfiguration.RM_WORK_PRESERVING_RECOVERY_SCHEDULING_WAIT_MS, 0);\r\n    conf.setBoolean(MRJobConfig.MR_AM_JOB_NODE_BLACKLISTING_ENABLE, true);\r\n    conf.setInt(MRJobConfig.MAX_TASK_FAILURES_PER_TRACKER, 1);\r\n    conf.setInt(MRJobConfig.MR_AM_IGNORE_BLACKLISTING_BLACKLISTED_NODE_PERECENT, -1);\r\n    MyResourceManager rm1 = new MyResourceManager(conf);\r\n    rm1.start();\r\n    RMApp app = MockRMAppSubmitter.submitWithMemory(1024, rm1);\r\n    rm1.drainEvents();\r\n    MockNM nm1 = new MockNM(\"h1:1234\", 15120, rm1.getResourceTrackerService());\r\n    nm1.registerNode();\r\n    nm1.nodeHeartbeat(true);\r\n    rm1.drainEvents();\r\n    ApplicationAttemptId appAttemptId = app.getCurrentAppAttempt().getAppAttemptId();\r\n    rm1.sendAMLaunched(appAttemptId);\r\n    rm1.drainEvents();\r\n    JobId jobId = MRBuilderUtils.newJobId(appAttemptId.getApplicationId(), 0);\r\n    Job mockJob = mock(Job.class);\r\n    when(mockJob.getReport()).thenReturn(MRBuilderUtils.newJobReport(jobId, \"job\", \"user\", JobState.RUNNING, 0, 0, 0, 0, 0, 0, 0, \"jobfile\", null, false, \"\"));\r\n    MyContainerAllocator allocator = new MyContainerAllocator(rm1, conf, appAttemptId, mockJob);\r\n    ContainerRequestEvent event1 = ContainerRequestCreator.createRequest(jobId, 1, Resource.newInstance(1024, 1), new String[] { \"h1\" });\r\n    allocator.sendRequest(event1);\r\n    ContainerRequestEvent event2 = ContainerRequestCreator.createRequest(jobId, 2, Resource.newInstance(2048, 1), new String[] { \"h1\", \"h2\" });\r\n    allocator.sendRequest(event2);\r\n    ContainerFailedEvent f1 = createFailEvent(jobId, 1, \"h2\", false);\r\n    allocator.sendFailure(f1);\r\n    List<TaskAttemptContainerAssignedEvent> assignedContainers = allocator.schedule();\r\n    rm1.drainEvents();\r\n    Assert.assertEquals(\"No of assignments must be 0\", 0, assignedContainers.size());\r\n    assertAsksAndReleases(3, 0, rm1);\r\n    assertBlacklistAdditionsAndRemovals(1, 0, rm1);\r\n    nm1.nodeHeartbeat(true);\r\n    rm1.drainEvents();\r\n    assignedContainers = allocator.schedule();\r\n    rm1.drainEvents();\r\n    Assert.assertEquals(\"No of assignments must be 2\", 2, assignedContainers.size());\r\n    assertAsksAndReleases(0, 0, rm1);\r\n    assertBlacklistAdditionsAndRemovals(0, 0, rm1);\r\n    assignedContainers = allocator.schedule();\r\n    Assert.assertEquals(\"No of assignments must be 0\", 0, assignedContainers.size());\r\n    assertAsksAndReleases(3, 0, rm1);\r\n    assertBlacklistAdditionsAndRemovals(0, 0, rm1);\r\n    ContainerRequestEvent event3 = ContainerRequestCreator.createRequest(jobId, 3, Resource.newInstance(1000, 1), new String[] { \"h1\" });\r\n    allocator.sendRequest(event3);\r\n    ContainerAllocatorEvent deallocate1 = createDeallocateEvent(jobId, 1, false);\r\n    allocator.sendDeallocate(deallocate1);\r\n    assignedContainers = allocator.schedule();\r\n    Assert.assertEquals(\"No of assignments must be 0\", 0, assignedContainers.size());\r\n    assertAsksAndReleases(3, 1, rm1);\r\n    assertBlacklistAdditionsAndRemovals(0, 0, rm1);\r\n    MyResourceManager rm2 = new MyResourceManager(conf, rm1.getRMStateStore());\r\n    rm2.start();\r\n    nm1.setResourceTrackerService(rm2.getResourceTrackerService());\r\n    allocator.updateSchedulerProxy(rm2);\r\n    NodeHeartbeatResponse hbResponse = nm1.nodeHeartbeat(true);\r\n    Assert.assertEquals(NodeAction.RESYNC, hbResponse.getNodeAction());\r\n    nm1 = new MockNM(\"h1:1234\", 10240, rm2.getResourceTrackerService());\r\n    nm1.registerNode();\r\n    nm1.nodeHeartbeat(true);\r\n    rm2.drainEvents();\r\n    ContainerAllocatorEvent deallocate2 = createDeallocateEvent(jobId, 2, false);\r\n    allocator.sendDeallocate(deallocate2);\r\n    ContainerFailedEvent f2 = createFailEvent(jobId, 1, \"h3\", false);\r\n    allocator.sendFailure(f2);\r\n    ContainerRequestEvent event4 = ContainerRequestCreator.createRequest(jobId, 4, Resource.newInstance(2000, 1), new String[] { \"h1\", \"h2\" });\r\n    allocator.sendRequest(event4);\r\n    allocator.schedule();\r\n    rm2.drainEvents();\r\n    ContainerRequestEvent event5 = ContainerRequestCreator.createRequest(jobId, 5, Resource.newInstance(3000, 1), new String[] { \"h1\", \"h2\", \"h3\" });\r\n    allocator.sendRequest(event5);\r\n    assignedContainers = allocator.schedule();\r\n    rm2.drainEvents();\r\n    assertAsksAndReleases(3, 2, rm2);\r\n    assertBlacklistAdditionsAndRemovals(2, 0, rm2);\r\n    nm1.nodeHeartbeat(true);\r\n    rm2.drainEvents();\r\n    assignedContainers = allocator.schedule();\r\n    rm2.drainEvents();\r\n    Assert.assertEquals(\"Number of container should be 3\", 3, assignedContainers.size());\r\n    for (TaskAttemptContainerAssignedEvent assig : assignedContainers) {\r\n        Assert.assertTrue(\"Assigned count not correct\", \"h1\".equals(assig.getContainer().getNodeId().getHost()));\r\n    }\r\n    rm1.stop();\r\n    rm2.stop();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\rm",
  "methodName" : "testUnsupportedMapContainerRequirement",
  "errType" : null,
  "containingMethodsNum" : 11,
  "sourceCodeText" : "void testUnsupportedMapContainerRequirement() throws Exception\n{\r\n    final Resource maxContainerSupported = Resource.newInstance(1, 1);\r\n    final ApplicationId appId = ApplicationId.newInstance(1, 1);\r\n    final ApplicationAttemptId appAttemptId = ApplicationAttemptId.newInstance(appId, 1);\r\n    final JobId jobId = MRBuilderUtils.newJobId(appAttemptId.getApplicationId(), 0);\r\n    final MockScheduler mockScheduler = new MockScheduler(appAttemptId);\r\n    final Configuration conf = new Configuration();\r\n    final MyContainerAllocator allocator = new MyContainerAllocator(null, conf, appAttemptId, mock(Job.class), SystemClock.getInstance()) {\r\n\r\n        @Override\r\n        protected void register() {\r\n        }\r\n\r\n        @Override\r\n        protected ApplicationMasterProtocol createSchedulerProxy() {\r\n            return mockScheduler;\r\n        }\r\n\r\n        @Override\r\n        protected Resource getMaxContainerCapability() {\r\n            return maxContainerSupported;\r\n        }\r\n    };\r\n    final int memory = (int) (maxContainerSupported.getMemorySize() + 10);\r\n    ContainerRequestEvent mapRequestEvt = createRequest(jobId, 0, Resource.newInstance(memory, maxContainerSupported.getVirtualCores()), new String[0], false, false);\r\n    allocator.sendRequests(Arrays.asList(mapRequestEvt));\r\n    allocator.schedule();\r\n    Assert.assertEquals(0, mockScheduler.lastAnyAskMap);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\rm",
  "methodName" : "testUnsupportedReduceContainerRequirement",
  "errType" : null,
  "containingMethodsNum" : 12,
  "sourceCodeText" : "void testUnsupportedReduceContainerRequirement() throws Exception\n{\r\n    final Resource maxContainerSupported = Resource.newInstance(1, 1);\r\n    final ApplicationId appId = ApplicationId.newInstance(1, 1);\r\n    final ApplicationAttemptId appAttemptId = ApplicationAttemptId.newInstance(appId, 1);\r\n    final JobId jobId = MRBuilderUtils.newJobId(appAttemptId.getApplicationId(), 0);\r\n    final MockScheduler mockScheduler = new MockScheduler(appAttemptId);\r\n    final Configuration conf = new Configuration();\r\n    final MyContainerAllocator allocator = new MyContainerAllocator(null, conf, appAttemptId, mock(Job.class), SystemClock.getInstance()) {\r\n\r\n        @Override\r\n        protected void register() {\r\n        }\r\n\r\n        @Override\r\n        protected ApplicationMasterProtocol createSchedulerProxy() {\r\n            return mockScheduler;\r\n        }\r\n\r\n        @Override\r\n        protected Resource getMaxContainerCapability() {\r\n            return maxContainerSupported;\r\n        }\r\n    };\r\n    final int memory = (int) (maxContainerSupported.getMemorySize() + 10);\r\n    ContainerRequestEvent reduceRequestEvt = createRequest(jobId, 0, Resource.newInstance(memory, maxContainerSupported.getVirtualCores()), new String[0], false, true);\r\n    allocator.sendRequests(Arrays.asList(reduceRequestEvt));\r\n    allocator.scheduleAllReduces();\r\n    allocator.schedule();\r\n    Assert.assertEquals(0, mockScheduler.lastAnyAskReduce);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\rm",
  "methodName" : "testRMUnavailable",
  "errType" : [ "RMContainerAllocationException" ],
  "containingMethodsNum" : 22,
  "sourceCodeText" : "void testRMUnavailable() throws Exception\n{\r\n    Configuration conf = new Configuration();\r\n    conf.setInt(MRJobConfig.MR_AM_TO_RM_WAIT_INTERVAL_MS, 0);\r\n    MyResourceManager rm1 = new MyResourceManager(conf);\r\n    rm1.start();\r\n    RMApp app = MockRMAppSubmitter.submitWithMemory(1024, rm1);\r\n    rm1.drainEvents();\r\n    MockNM nm1 = new MockNM(\"h1:1234\", 15120, rm1.getResourceTrackerService());\r\n    nm1.registerNode();\r\n    nm1.nodeHeartbeat(true);\r\n    rm1.drainEvents();\r\n    ApplicationAttemptId appAttemptId = app.getCurrentAppAttempt().getAppAttemptId();\r\n    rm1.sendAMLaunched(appAttemptId);\r\n    rm1.drainEvents();\r\n    JobId jobId = MRBuilderUtils.newJobId(appAttemptId.getApplicationId(), 0);\r\n    Job mockJob = mock(Job.class);\r\n    when(mockJob.getReport()).thenReturn(MRBuilderUtils.newJobReport(jobId, \"job\", \"user\", JobState.RUNNING, 0, 0, 0, 0, 0, 0, 0, \"jobfile\", null, false, \"\"));\r\n    MyContainerAllocator2 allocator = new MyContainerAllocator2(rm1, conf, appAttemptId, mockJob);\r\n    allocator.jobEvents.clear();\r\n    try {\r\n        allocator.schedule();\r\n        Assert.fail(\"Should Have Exception\");\r\n    } catch (RMContainerAllocationException e) {\r\n        Assert.assertTrue(e.getMessage().contains(\"Could not contact RM after\"));\r\n    }\r\n    rm1.drainEvents();\r\n    Assert.assertEquals(\"Should Have 1 Job Event\", 1, allocator.jobEvents.size());\r\n    JobEvent event = allocator.jobEvents.get(0);\r\n    Assert.assertTrue(\"Should Reboot\", event.getType().equals(JobEventType.JOB_AM_REBOOT));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\rm",
  "methodName" : "testAMRMTokenUpdate",
  "errType" : null,
  "containingMethodsNum" : 28,
  "sourceCodeText" : "void testAMRMTokenUpdate() throws Exception\n{\r\n    LOG.info(\"Running testAMRMTokenUpdate\");\r\n    final String rmAddr = \"somermaddress:1234\";\r\n    final Configuration conf = new YarnConfiguration();\r\n    conf.setLong(YarnConfiguration.RM_AMRM_TOKEN_MASTER_KEY_ROLLING_INTERVAL_SECS, 8);\r\n    conf.setLong(YarnConfiguration.RM_AM_EXPIRY_INTERVAL_MS, 2000);\r\n    conf.set(YarnConfiguration.RM_SCHEDULER_ADDRESS, rmAddr);\r\n    final MyResourceManager rm = new MyResourceManager(conf);\r\n    rm.start();\r\n    AMRMTokenSecretManager secretMgr = rm.getRMContext().getAMRMTokenSecretManager();\r\n    RMApp app = MockRMAppSubmitter.submitWithMemory(1024, rm);\r\n    rm.drainEvents();\r\n    MockNM amNodeManager = rm.registerNode(\"amNM:1234\", 2048);\r\n    amNodeManager.nodeHeartbeat(true);\r\n    rm.drainEvents();\r\n    final ApplicationAttemptId appAttemptId = app.getCurrentAppAttempt().getAppAttemptId();\r\n    final ApplicationId appId = app.getApplicationId();\r\n    rm.sendAMLaunched(appAttemptId);\r\n    rm.drainEvents();\r\n    JobId jobId = MRBuilderUtils.newJobId(appAttemptId.getApplicationId(), 0);\r\n    final Job mockJob = mock(Job.class);\r\n    when(mockJob.getReport()).thenReturn(MRBuilderUtils.newJobReport(jobId, \"job\", \"user\", JobState.RUNNING, 0, 0, 0, 0, 0, 0, 0, \"jobfile\", null, false, \"\"));\r\n    final Token<AMRMTokenIdentifier> oldToken = rm.getRMContext().getRMApps().get(appId).getRMAppAttempt(appAttemptId).getAMRMToken();\r\n    Assert.assertNotNull(\"app should have a token\", oldToken);\r\n    UserGroupInformation testUgi = UserGroupInformation.createUserForTesting(\"someuser\", new String[0]);\r\n    Token<AMRMTokenIdentifier> newToken = testUgi.doAs(new PrivilegedExceptionAction<Token<AMRMTokenIdentifier>>() {\r\n\r\n        @Override\r\n        public Token<AMRMTokenIdentifier> run() throws Exception {\r\n            MyContainerAllocator allocator = new MyContainerAllocator(rm, conf, appAttemptId, mockJob);\r\n            Token<AMRMTokenIdentifier> currentToken = oldToken;\r\n            long startTime = Time.monotonicNow();\r\n            while (currentToken == oldToken) {\r\n                if (Time.monotonicNow() - startTime > 20000) {\r\n                    Assert.fail(\"Took to long to see AMRM token change\");\r\n                }\r\n                Thread.sleep(100);\r\n                allocator.schedule();\r\n                currentToken = rm.getRMContext().getRMApps().get(appId).getRMAppAttempt(appAttemptId).getAMRMToken();\r\n            }\r\n            return currentToken;\r\n        }\r\n    });\r\n    int tokenCount = 0;\r\n    Token<? extends TokenIdentifier> ugiToken = null;\r\n    for (Token<? extends TokenIdentifier> token : testUgi.getTokens()) {\r\n        if (AMRMTokenIdentifier.KIND_NAME.equals(token.getKind())) {\r\n            ugiToken = token;\r\n            ++tokenCount;\r\n        }\r\n    }\r\n    Assert.assertEquals(\"too many AMRM tokens\", 1, tokenCount);\r\n    Assert.assertArrayEquals(\"token identifier not updated\", newToken.getIdentifier(), ugiToken.getIdentifier());\r\n    Assert.assertArrayEquals(\"token password not updated\", newToken.getPassword(), ugiToken.getPassword());\r\n    Assert.assertEquals(\"AMRM token service not updated\", new Text(rmAddr), ugiToken.getService());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\rm",
  "methodName" : "testConcurrentTaskLimitsDisabledIfSmaller",
  "errType" : null,
  "containingMethodsNum" : 20,
  "sourceCodeText" : "void testConcurrentTaskLimitsDisabledIfSmaller() throws Exception\n{\r\n    final int MAP_COUNT = 1;\r\n    final int REDUCE_COUNT = 1;\r\n    final int MAP_LIMIT = 1;\r\n    final int REDUCE_LIMIT = 1;\r\n    Configuration conf = new Configuration();\r\n    conf.setInt(MRJobConfig.JOB_RUNNING_MAP_LIMIT, MAP_LIMIT);\r\n    conf.setInt(MRJobConfig.JOB_RUNNING_REDUCE_LIMIT, REDUCE_LIMIT);\r\n    conf.setFloat(MRJobConfig.COMPLETED_MAPS_FOR_REDUCE_SLOWSTART, 0.0f);\r\n    ApplicationId appId = ApplicationId.newInstance(1, 1);\r\n    ApplicationAttemptId appAttemptId = ApplicationAttemptId.newInstance(appId, 1);\r\n    JobId jobId = MRBuilderUtils.newJobId(appAttemptId.getApplicationId(), 0);\r\n    Job mockJob = mock(Job.class);\r\n    when(mockJob.getReport()).thenReturn(MRBuilderUtils.newJobReport(jobId, \"job\", \"user\", JobState.RUNNING, 0, 0, 0, 0, 0, 0, 0, \"jobfile\", null, false, \"\"));\r\n    when(mockJob.getTotalMaps()).thenReturn(MAP_COUNT);\r\n    when(mockJob.getTotalReduces()).thenReturn(REDUCE_COUNT);\r\n    final MockScheduler mockScheduler = new MockScheduler(appAttemptId);\r\n    MyContainerAllocator allocator = new MyContainerAllocator(null, conf, appAttemptId, mockJob, SystemClock.getInstance()) {\r\n\r\n        @Override\r\n        protected void register() {\r\n        }\r\n\r\n        @Override\r\n        protected ApplicationMasterProtocol createSchedulerProxy() {\r\n            return mockScheduler;\r\n        }\r\n\r\n        @Override\r\n        protected void setRequestLimit(Priority priority, Resource capability, int limit) {\r\n            Assert.fail(\"setRequestLimit() should not be invoked\");\r\n        }\r\n    };\r\n    ContainerRequestEvent[] reqMapEvents = new ContainerRequestEvent[MAP_COUNT];\r\n    for (int i = 0; i < reqMapEvents.length; ++i) {\r\n        reqMapEvents[i] = ContainerRequestCreator.createRequest(jobId, i, Resource.newInstance(1024, 1), new String[] { \"h\" + i });\r\n    }\r\n    allocator.sendRequests(Arrays.asList(reqMapEvents));\r\n    ContainerRequestEvent[] reqReduceEvents = new ContainerRequestEvent[REDUCE_COUNT];\r\n    for (int i = 0; i < reqReduceEvents.length; ++i) {\r\n        reqReduceEvents[i] = createRequest(jobId, i, Resource.newInstance(1024, 1), new String[] {}, false, true);\r\n    }\r\n    allocator.sendRequests(Arrays.asList(reqReduceEvents));\r\n    allocator.schedule();\r\n    allocator.schedule();\r\n    allocator.schedule();\r\n    allocator.close();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\rm",
  "methodName" : "testConcurrentTaskLimits",
  "errType" : null,
  "containingMethodsNum" : 65,
  "sourceCodeText" : "void testConcurrentTaskLimits() throws Exception\n{\r\n    final int MAP_COUNT = 5;\r\n    final int REDUCE_COUNT = 2;\r\n    final int MAP_LIMIT = 3;\r\n    final int REDUCE_LIMIT = 1;\r\n    LOG.info(\"Running testConcurrentTaskLimits\");\r\n    Configuration conf = new Configuration();\r\n    conf.setInt(MRJobConfig.JOB_RUNNING_MAP_LIMIT, MAP_LIMIT);\r\n    conf.setInt(MRJobConfig.JOB_RUNNING_REDUCE_LIMIT, REDUCE_LIMIT);\r\n    conf.setFloat(MRJobConfig.COMPLETED_MAPS_FOR_REDUCE_SLOWSTART, 0.0f);\r\n    ApplicationId appId = ApplicationId.newInstance(1, 1);\r\n    ApplicationAttemptId appAttemptId = ApplicationAttemptId.newInstance(appId, 1);\r\n    JobId jobId = MRBuilderUtils.newJobId(appAttemptId.getApplicationId(), 0);\r\n    Job mockJob = mock(Job.class);\r\n    when(mockJob.getReport()).thenReturn(MRBuilderUtils.newJobReport(jobId, \"job\", \"user\", JobState.RUNNING, 0, 0, 0, 0, 0, 0, 0, \"jobfile\", null, false, \"\"));\r\n    when(mockJob.getTotalMaps()).thenReturn(MAP_COUNT);\r\n    when(mockJob.getTotalReduces()).thenReturn(REDUCE_COUNT);\r\n    final MockScheduler mockScheduler = new MockScheduler(appAttemptId);\r\n    MyContainerAllocator allocator = new MyContainerAllocator(null, conf, appAttemptId, mockJob, SystemClock.getInstance()) {\r\n\r\n        @Override\r\n        protected void register() {\r\n        }\r\n\r\n        @Override\r\n        protected ApplicationMasterProtocol createSchedulerProxy() {\r\n            return mockScheduler;\r\n        }\r\n    };\r\n    ContainerRequestEvent[] reqMapEvents = new ContainerRequestEvent[MAP_COUNT];\r\n    for (int i = 0; i < reqMapEvents.length; ++i) {\r\n        reqMapEvents[i] = ContainerRequestCreator.createRequest(jobId, i, Resource.newInstance(1024, 1), new String[] { \"h\" + i });\r\n    }\r\n    allocator.sendRequests(Arrays.asList(reqMapEvents));\r\n    ContainerRequestEvent[] reqReduceEvents = new ContainerRequestEvent[REDUCE_COUNT];\r\n    for (int i = 0; i < reqReduceEvents.length; ++i) {\r\n        reqReduceEvents[i] = createRequest(jobId, i, Resource.newInstance(1024, 1), new String[] {}, false, true);\r\n    }\r\n    allocator.sendRequests(Arrays.asList(reqReduceEvents));\r\n    allocator.schedule();\r\n    Assert.assertEquals(reqMapEvents.length + 2, mockScheduler.lastAsk.size());\r\n    Assert.assertEquals(MAP_LIMIT, mockScheduler.lastAnyAskMap);\r\n    ContainerId cid0 = mockScheduler.assignContainer(\"h0\", false);\r\n    allocator.schedule();\r\n    allocator.schedule();\r\n    Assert.assertEquals(2, mockScheduler.lastAnyAskMap);\r\n    mockScheduler.completeContainer(cid0);\r\n    allocator.schedule();\r\n    allocator.schedule();\r\n    Assert.assertEquals(3, mockScheduler.lastAnyAskMap);\r\n    ContainerId cid1 = mockScheduler.assignContainer(\"h1\", false);\r\n    ContainerId cid2 = mockScheduler.assignContainer(\"h2\", false);\r\n    ContainerId cid3 = mockScheduler.assignContainer(\"h3\", false);\r\n    allocator.schedule();\r\n    allocator.schedule();\r\n    Assert.assertEquals(0, mockScheduler.lastAnyAskMap);\r\n    mockScheduler.completeContainer(cid2);\r\n    mockScheduler.completeContainer(cid3);\r\n    allocator.schedule();\r\n    allocator.schedule();\r\n    Assert.assertEquals(1, mockScheduler.lastAnyAskMap);\r\n    mockScheduler.completeContainer(cid1);\r\n    ContainerId cid4 = mockScheduler.assignContainer(\"h4\", false);\r\n    allocator.schedule();\r\n    allocator.schedule();\r\n    Assert.assertEquals(0, mockScheduler.lastAnyAskMap);\r\n    mockScheduler.completeContainer(cid4);\r\n    allocator.schedule();\r\n    allocator.schedule();\r\n    Assert.assertEquals(0, mockScheduler.lastAnyAskMap);\r\n    Assert.assertEquals(REDUCE_LIMIT, mockScheduler.lastAnyAskReduce);\r\n    cid0 = mockScheduler.assignContainer(\"h0\", true);\r\n    allocator.schedule();\r\n    allocator.schedule();\r\n    Assert.assertEquals(0, mockScheduler.lastAnyAskReduce);\r\n    mockScheduler.completeContainer(cid0);\r\n    allocator.schedule();\r\n    allocator.schedule();\r\n    Assert.assertEquals(1, mockScheduler.lastAnyAskReduce);\r\n    cid0 = mockScheduler.assignContainer(\"h0\", true);\r\n    allocator.schedule();\r\n    allocator.schedule();\r\n    Assert.assertEquals(0, mockScheduler.lastAnyAskReduce);\r\n    mockScheduler.completeContainer(cid0);\r\n    allocator.schedule();\r\n    allocator.schedule();\r\n    Assert.assertEquals(0, mockScheduler.lastAnyAskReduce);\r\n    allocator.close();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\rm",
  "methodName" : "testAttemptNotFoundCausesRMCommunicatorException",
  "errType" : null,
  "containingMethodsNum" : 15,
  "sourceCodeText" : "void testAttemptNotFoundCausesRMCommunicatorException() throws Exception\n{\r\n    Configuration conf = new Configuration();\r\n    MyResourceManager rm = new MyResourceManager(conf);\r\n    rm.start();\r\n    RMApp app = MockRMAppSubmitter.submitWithMemory(1024, rm);\r\n    rm.drainEvents();\r\n    MockNM amNodeManager = rm.registerNode(\"amNM:1234\", 2048);\r\n    amNodeManager.nodeHeartbeat(true);\r\n    rm.drainEvents();\r\n    ApplicationAttemptId appAttemptId = app.getCurrentAppAttempt().getAppAttemptId();\r\n    rm.sendAMLaunched(appAttemptId);\r\n    rm.drainEvents();\r\n    JobId jobId = MRBuilderUtils.newJobId(appAttemptId.getApplicationId(), 0);\r\n    Job mockJob = mock(Job.class);\r\n    when(mockJob.getReport()).thenReturn(MRBuilderUtils.newJobReport(jobId, \"job\", \"user\", JobState.RUNNING, 0, 0, 0, 0, 0, 0, 0, \"jobfile\", null, false, \"\"));\r\n    MyContainerAllocator allocator = new MyContainerAllocator(rm, conf, appAttemptId, mockJob);\r\n    rm.killApp(app.getApplicationId());\r\n    rm.waitForState(app.getApplicationId(), RMAppState.KILLED);\r\n    allocator.schedule();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\rm",
  "methodName" : "testUpdateAskOnRampDownAllReduces",
  "errType" : null,
  "containingMethodsNum" : 58,
  "sourceCodeText" : "void testUpdateAskOnRampDownAllReduces() throws Exception\n{\r\n    LOG.info(\"Running testUpdateAskOnRampDownAllReduces\");\r\n    Configuration conf = new Configuration();\r\n    MyResourceManager rm = new MyResourceManager(conf);\r\n    rm.start();\r\n    RMApp app = MockRMAppSubmitter.submitWithMemory(1024, rm);\r\n    rm.drainEvents();\r\n    MockNM amNodeManager = rm.registerNode(\"amNM:1234\", 1260);\r\n    amNodeManager.nodeHeartbeat(true);\r\n    rm.drainEvents();\r\n    ApplicationAttemptId appAttemptId = app.getCurrentAppAttempt().getAppAttemptId();\r\n    rm.sendAMLaunched(appAttemptId);\r\n    rm.drainEvents();\r\n    JobId jobId = MRBuilderUtils.newJobId(appAttemptId.getApplicationId(), 0);\r\n    Job mockJob = mock(Job.class);\r\n    when(mockJob.getReport()).thenReturn(MRBuilderUtils.newJobReport(jobId, \"job\", \"user\", JobState.RUNNING, 0, 0, 0, 0, 0, 0, 0, \"jobfile\", null, false, \"\"));\r\n    MyContainerAllocator allocator = new MyContainerAllocator(rm, conf, appAttemptId, mockJob);\r\n    ControlledClock clock = (ControlledClock) allocator.getContext().getClock();\r\n    clock.setTime(System.currentTimeMillis());\r\n    MockNM nodeManager = rm.registerNode(\"h1:1234\", 1024);\r\n    rm.drainEvents();\r\n    ContainerRequestEvent event1 = ContainerRequestCreator.createRequest(jobId, 1, Resource.newInstance(1024, 1), new String[] { \"h1\" });\r\n    allocator.sendRequest(event1);\r\n    ContainerRequestEvent event2 = ContainerRequestCreator.createRequest(jobId, 2, Resource.newInstance(1024, 1), new String[] { \"h2\" });\r\n    allocator.sendRequest(event2);\r\n    ContainerRequestEvent event3 = createRequest(jobId, 3, Resource.newInstance(1024, 1), new String[] { \"h2\" }, false, true);\r\n    allocator.sendRequest(event3);\r\n    allocator.schedule();\r\n    rm.drainEvents();\r\n    clock.setTime(System.currentTimeMillis() + 500000L);\r\n    ContainerRequestEvent event4 = createRequest(jobId, 4, Resource.newInstance(1024, 1), new String[] { \"h3\" }, false, true);\r\n    allocator.sendRequest(event4);\r\n    allocator.schedule();\r\n    rm.drainEvents();\r\n    nodeManager.nodeHeartbeat(true);\r\n    rm.drainEvents();\r\n    rm.getMyFifoScheduler().forceResourceLimit(Resource.newInstance(1024, 1));\r\n    allocator.schedule();\r\n    rm.drainEvents();\r\n    Assert.assertEquals(1, allocator.getAssignedRequests().maps.size());\r\n    ContainerAllocatorEvent deallocate = createDeallocateEvent(jobId, 1, false);\r\n    allocator.sendDeallocate(deallocate);\r\n    Assert.assertEquals(1, allocator.getScheduledRequests().reduces.size());\r\n    Assert.assertEquals(1, allocator.getNumOfPendingReduces());\r\n    Assert.assertEquals(1, allocator.getScheduledRequests().maps.size());\r\n    Assert.assertEquals(0, allocator.getAssignedRequests().maps.size());\r\n    Assert.assertEquals(6, allocator.getAsk().size());\r\n    for (ResourceRequest req : allocator.getAsk()) {\r\n        boolean isReduce = req.getPriority().equals(RMContainerAllocator.PRIORITY_REDUCE);\r\n        if (isReduce) {\r\n            Assert.assertTrue((req.getResourceName().equals(\"*\") || req.getResourceName().equals(\"/default-rack\") || req.getResourceName().equals(\"h2\")) && req.getNumContainers() == 1);\r\n        } else {\r\n            Assert.assertTrue(((req.getResourceName().equals(\"*\") || req.getResourceName().equals(\"/default-rack\")) && req.getNumContainers() == 1) || (req.getResourceName().equals(\"h1\") && req.getNumContainers() == 0));\r\n        }\r\n    }\r\n    rm.getMyFifoScheduler().forceResourceLimit(Resource.newInstance(0, 0));\r\n    allocator.schedule();\r\n    rm.drainEvents();\r\n    Assert.assertEquals(0, allocator.getScheduledRequests().reduces.size());\r\n    Assert.assertEquals(2, allocator.getNumOfPendingReduces());\r\n    Assert.assertEquals(3, allocator.getAsk().size());\r\n    for (ResourceRequest req : allocator.getAsk()) {\r\n        Assert.assertEquals(RMContainerAllocator.PRIORITY_REDUCE, req.getPriority());\r\n        Assert.assertTrue(req.getResourceName().equals(\"*\") || req.getResourceName().equals(\"/default-rack\") || req.getResourceName().equals(\"h2\"));\r\n        Assert.assertEquals(Resource.newInstance(1024, 1), req.getCapability());\r\n        Assert.assertEquals(0, req.getNumContainers());\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\rm",
  "methodName" : "testHandlingFinishedContainers",
  "errType" : null,
  "containingMethodsNum" : 12,
  "sourceCodeText" : "void testHandlingFinishedContainers()\n{\r\n    EventHandler eventHandler = mock(EventHandler.class);\r\n    AppContext context = mock(RunningAppContext.class);\r\n    when(context.getClock()).thenReturn(new ControlledClock());\r\n    when(context.getClusterInfo()).thenReturn(new ClusterInfo(Resource.newInstance(10240, 1)));\r\n    when(context.getEventHandler()).thenReturn(eventHandler);\r\n    RMContainerAllocator containerAllocator = new RMContainerAllocatorForFinishedContainer(null, context, mock(AMPreemptionPolicy.class));\r\n    ContainerStatus finishedContainer = ContainerStatus.newInstance(mock(ContainerId.class), ContainerState.COMPLETE, \"\", 0);\r\n    containerAllocator.processFinishedContainer(finishedContainer);\r\n    InOrder inOrder = inOrder(eventHandler);\r\n    inOrder.verify(eventHandler).handle(isA(TaskAttemptDiagnosticsUpdateEvent.class));\r\n    inOrder.verify(eventHandler).handle(isA(TaskAttemptEvent.class));\r\n    inOrder.verifyNoMoreInteractions();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\rm",
  "methodName" : "testAvoidAskMoreReducersWhenReducerPreemptionIsRequired",
  "errType" : null,
  "containingMethodsNum" : 59,
  "sourceCodeText" : "void testAvoidAskMoreReducersWhenReducerPreemptionIsRequired() throws Exception\n{\r\n    LOG.info(\"Running testAvoidAskMoreReducersWhenReducerPreemptionIsRequired\");\r\n    Configuration conf = new Configuration();\r\n    MyResourceManager rm = new MyResourceManager(conf);\r\n    rm.start();\r\n    RMApp app = MockRMAppSubmitter.submitWithMemory(1024, rm);\r\n    rm.drainEvents();\r\n    MockNM amNodeManager = rm.registerNode(\"amNM:1234\", 1260);\r\n    amNodeManager.nodeHeartbeat(true);\r\n    rm.drainEvents();\r\n    ApplicationAttemptId appAttemptId = app.getCurrentAppAttempt().getAppAttemptId();\r\n    rm.sendAMLaunched(appAttemptId);\r\n    rm.drainEvents();\r\n    JobId jobId = MRBuilderUtils.newJobId(appAttemptId.getApplicationId(), 0);\r\n    Job mockJob = mock(Job.class);\r\n    when(mockJob.getReport()).thenReturn(MRBuilderUtils.newJobReport(jobId, \"job\", \"user\", JobState.RUNNING, 0, 0, 0, 0, 0, 0, 0, \"jobfile\", null, false, \"\"));\r\n    MyContainerAllocator allocator = new MyContainerAllocator(rm, conf, appAttemptId, mockJob);\r\n    ControlledClock clock = (ControlledClock) allocator.getContext().getClock();\r\n    clock.setTime(System.currentTimeMillis());\r\n    MockNM nodeManager = rm.registerNode(\"h1:1234\", 1024);\r\n    rm.drainEvents();\r\n    ContainerRequestEvent event1 = ContainerRequestCreator.createRequest(jobId, 1, Resource.newInstance(1024, 1), new String[] { \"h1\" });\r\n    allocator.sendRequest(event1);\r\n    ContainerRequestEvent event2 = ContainerRequestCreator.createRequest(jobId, 2, Resource.newInstance(1024, 1), new String[] { \"h2\" });\r\n    allocator.sendRequest(event2);\r\n    ContainerRequestEvent event3 = createRequest(jobId, 3, Resource.newInstance(1024, 1), new String[] { \"h2\" }, false, true);\r\n    allocator.sendRequest(event3);\r\n    allocator.schedule();\r\n    rm.drainEvents();\r\n    clock.setTime(System.currentTimeMillis() + 500000L);\r\n    ContainerRequestEvent event4 = createRequest(jobId, 4, Resource.newInstance(1024, 1), new String[] { \"h3\" }, false, true);\r\n    allocator.sendRequest(event4);\r\n    allocator.schedule();\r\n    rm.drainEvents();\r\n    nodeManager.nodeHeartbeat(true);\r\n    rm.drainEvents();\r\n    rm.getMyFifoScheduler().forceResourceLimit(Resource.newInstance(1024, 1));\r\n    allocator.schedule();\r\n    rm.drainEvents();\r\n    Assert.assertEquals(1, allocator.getAssignedRequests().maps.size());\r\n    ContainerAllocatorEvent deallocate = createDeallocateEvent(jobId, 1, false);\r\n    allocator.sendDeallocate(deallocate);\r\n    Assert.assertEquals(1, allocator.getScheduledRequests().reduces.size());\r\n    Assert.assertEquals(1, allocator.getNumOfPendingReduces());\r\n    Assert.assertEquals(1, allocator.getScheduledRequests().maps.size());\r\n    Assert.assertEquals(0, allocator.getAssignedRequests().maps.size());\r\n    Assert.assertEquals(6, allocator.getAsk().size());\r\n    for (ResourceRequest req : allocator.getAsk()) {\r\n        boolean isReduce = req.getPriority().equals(RMContainerAllocator.PRIORITY_REDUCE);\r\n        if (isReduce) {\r\n            Assert.assertTrue((req.getResourceName().equals(\"*\") || req.getResourceName().equals(\"/default-rack\") || req.getResourceName().equals(\"h2\")) && req.getNumContainers() == 1);\r\n        } else {\r\n            Assert.assertTrue(((req.getResourceName().equals(\"*\") || req.getResourceName().equals(\"/default-rack\")) && req.getNumContainers() == 1) || (req.getResourceName().equals(\"h1\") && req.getNumContainers() == 0));\r\n        }\r\n    }\r\n    clock.setTime(System.currentTimeMillis() + 500000L + 10 * 60 * 1000);\r\n    rm.getMyFifoScheduler().forceResourceLimit(Resource.newInstance(2048, 0));\r\n    allocator.schedule();\r\n    rm.drainEvents();\r\n    Assert.assertEquals(0, allocator.getScheduledRequests().reduces.size());\r\n    Assert.assertEquals(2, allocator.getNumOfPendingReduces());\r\n    Assert.assertEquals(3, allocator.getAsk().size());\r\n    for (ResourceRequest req : allocator.getAsk()) {\r\n        Assert.assertEquals(RMContainerAllocator.PRIORITY_REDUCE, req.getPriority());\r\n        Assert.assertTrue(req.getResourceName().equals(\"*\") || req.getResourceName().equals(\"/default-rack\") || req.getResourceName().equals(\"h2\"));\r\n        Assert.assertEquals(Resource.newInstance(1024, 1), req.getCapability());\r\n        Assert.assertEquals(0, req.getNumContainers());\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\rm",
  "methodName" : "testExcludeSchedReducesFromHeadroom",
  "errType" : null,
  "containingMethodsNum" : 66,
  "sourceCodeText" : "void testExcludeSchedReducesFromHeadroom() throws Exception\n{\r\n    LOG.info(\"Running testExcludeSchedReducesFromHeadroom\");\r\n    Configuration conf = new Configuration();\r\n    conf.setInt(MRJobConfig.MR_JOB_REDUCER_UNCONDITIONAL_PREEMPT_DELAY_SEC, -1);\r\n    MyResourceManager rm = new MyResourceManager(conf);\r\n    rm.start();\r\n    RMApp app = MockRMAppSubmitter.submitWithMemory(1024, rm);\r\n    rm.drainEvents();\r\n    MockNM amNodeManager = rm.registerNode(\"amNM:1234\", 1260);\r\n    amNodeManager.nodeHeartbeat(true);\r\n    rm.drainEvents();\r\n    ApplicationAttemptId appAttemptId = app.getCurrentAppAttempt().getAppAttemptId();\r\n    rm.sendAMLaunched(appAttemptId);\r\n    rm.drainEvents();\r\n    JobId jobId = MRBuilderUtils.newJobId(appAttemptId.getApplicationId(), 0);\r\n    Job mockJob = mock(Job.class);\r\n    when(mockJob.getReport()).thenReturn(MRBuilderUtils.newJobReport(jobId, \"job\", \"user\", JobState.RUNNING, 0, 0, 0, 0, 0, 0, 0, \"jobfile\", null, false, \"\"));\r\n    Task mockTask = mock(Task.class);\r\n    TaskAttempt mockTaskAttempt = mock(TaskAttempt.class);\r\n    when(mockJob.getTask((TaskId) any())).thenReturn(mockTask);\r\n    when(mockTask.getAttempt((TaskAttemptId) any())).thenReturn(mockTaskAttempt);\r\n    when(mockTaskAttempt.getProgress()).thenReturn(0.01f);\r\n    MyContainerAllocator allocator = new MyContainerAllocator(rm, conf, appAttemptId, mockJob);\r\n    MockNM nodeManager = rm.registerNode(\"h1:1234\", 4096);\r\n    rm.drainEvents();\r\n    MockNM nodeManager2 = rm.registerNode(\"h2:1234\", 1024);\r\n    rm.drainEvents();\r\n    ContainerRequestEvent event1 = ContainerRequestCreator.createRequest(jobId, 1, Resource.newInstance(1024, 1), new String[] { \"h1\" });\r\n    allocator.sendRequest(event1);\r\n    ContainerRequestEvent event2 = ContainerRequestCreator.createRequest(jobId, 2, Resource.newInstance(1024, 1), new String[] { \"h2\" });\r\n    allocator.sendRequest(event2);\r\n    ContainerRequestEvent event3 = createRequest(jobId, 3, Resource.newInstance(1024, 1), new String[] { \"h1\" }, false, true);\r\n    allocator.sendRequest(event3);\r\n    allocator.schedule();\r\n    rm.drainEvents();\r\n    ContainerRequestEvent event4 = createRequest(jobId, 4, Resource.newInstance(1024, 1), new String[] { \"h3\" }, false, true);\r\n    allocator.sendRequest(event4);\r\n    allocator.schedule();\r\n    rm.drainEvents();\r\n    nodeManager.nodeHeartbeat(true);\r\n    rm.drainEvents();\r\n    rm.getMyFifoScheduler().forceResourceLimit(Resource.newInstance(3072, 3));\r\n    allocator.schedule();\r\n    rm.drainEvents();\r\n    Assert.assertEquals(2, allocator.getAssignedRequests().maps.size());\r\n    ContainerAllocatorEvent deallocate1 = createDeallocateEvent(jobId, 1, false);\r\n    allocator.sendDeallocate(deallocate1);\r\n    ContainerAllocatorEvent deallocate2 = createDeallocateEvent(jobId, 2, false);\r\n    allocator.sendDeallocate(deallocate2);\r\n    Assert.assertEquals(0, allocator.getAssignedRequests().maps.size());\r\n    nodeManager.nodeHeartbeat(true);\r\n    rm.drainEvents();\r\n    rm.getMyFifoScheduler().forceResourceLimit(Resource.newInstance(1024, 1));\r\n    allocator.schedule();\r\n    rm.drainEvents();\r\n    nodeManager2.nodeHeartbeat(true);\r\n    rm.drainEvents();\r\n    ContainerRequestEvent event5 = ContainerRequestCreator.createRequest(jobId, 5, Resource.newInstance(1024, 1), new String[] { \"h1\" });\r\n    allocator.sendRequest(event5);\r\n    rm.getMyFifoScheduler().forceResourceLimit(Resource.newInstance(2048, 2));\r\n    allocator.schedule();\r\n    rm.drainEvents();\r\n    Assert.assertEquals(1, allocator.getScheduledRequests().maps.size());\r\n    Assert.assertEquals(1, allocator.getAssignedRequests().reduces.size());\r\n    rm.getMyFifoScheduler().forceResourceLimit(Resource.newInstance(1260, 2));\r\n    allocator.schedule();\r\n    rm.drainEvents();\r\n    Assert.assertEquals(1, MyContainerAllocator.getTaskAttemptKillEvents().size());\r\n    Assert.assertEquals(RMContainerAllocator.RAMPDOWN_DIAGNOSTIC, MyContainerAllocator.getTaskAttemptKillEvents().get(0).getMessage());\r\n    Assert.assertEquals(1, allocator.getNumOfPendingReduces());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\rm",
  "methodName" : "main",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void main(String[] args) throws Exception\n{\r\n    TestRMContainerAllocator t = new TestRMContainerAllocator();\r\n    t.testSimple();\r\n    t.testResource();\r\n    t.testMapReduceScheduling();\r\n    t.testReportedAppProgress();\r\n    t.testReportedAppProgressWithOnlyMaps();\r\n    t.testBlackListedNodes();\r\n    t.testCompletedTasksRecalculateSchedule();\r\n    t.testAMRMTokenUpdate();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\rm",
  "methodName" : "testComputeAvailableContainers",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void testComputeAvailableContainers() throws Exception\n{\r\n    Resource clusterAvailableResources = Resource.newInstance(81920, 40);\r\n    Resource nonZeroResource = Resource.newInstance(1024, 2);\r\n    int expectedNumberOfContainersForMemory = 80;\r\n    int expectedNumberOfContainersForCPU = 20;\r\n    verifyDifferentResourceTypes(clusterAvailableResources, nonZeroResource, expectedNumberOfContainersForMemory, expectedNumberOfContainersForCPU);\r\n    Resource zeroMemoryResource = Resource.newInstance(0, nonZeroResource.getVirtualCores());\r\n    verifyDifferentResourceTypes(clusterAvailableResources, zeroMemoryResource, Integer.MAX_VALUE, expectedNumberOfContainersForCPU);\r\n    Resource zeroCpuResource = Resource.newInstance(nonZeroResource.getMemorySize(), 0);\r\n    verifyDifferentResourceTypes(clusterAvailableResources, zeroCpuResource, expectedNumberOfContainersForMemory, expectedNumberOfContainersForMemory);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\rm",
  "methodName" : "verifyDifferentResourceTypes",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void verifyDifferentResourceTypes(Resource clusterAvailableResources, Resource nonZeroResource, int expectedNumberOfContainersForMemoryOnly, int expectedNumberOfContainersOverall)\n{\r\n    Assert.assertEquals(\"Incorrect number of available containers for Memory\", expectedNumberOfContainersForMemoryOnly, ResourceCalculatorUtils.computeAvailableContainers(clusterAvailableResources, nonZeroResource, EnumSet.of(SchedulerResourceTypes.MEMORY)));\r\n    Assert.assertEquals(\"Incorrect number of available containers overall\", expectedNumberOfContainersOverall, ResourceCalculatorUtils.computeAvailableContainers(clusterAvailableResources, nonZeroResource, EnumSet.of(SchedulerResourceTypes.CPU, SchedulerResourceTypes.MEMORY)));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "before",
  "errType" : null,
  "containingMethodsNum" : 20,
  "sourceCodeText" : "void before()\n{\r\n    JobId mockJobId = mock(JobId.class);\r\n    when(mockJobId.toString()).thenReturn(\"testJobId\");\r\n    summary.setJobId(mockJobId);\r\n    summary.setJobSubmitTime(2L);\r\n    summary.setJobLaunchTime(3L);\r\n    summary.setFirstMapTaskLaunchTime(4L);\r\n    summary.setFirstReduceTaskLaunchTime(5L);\r\n    summary.setJobFinishTime(6L);\r\n    summary.setNumSucceededMaps(1);\r\n    summary.setNumFailedMaps(0);\r\n    summary.setNumSucceededReduces(1);\r\n    summary.setNumFailedReduces(0);\r\n    summary.setNumKilledMaps(0);\r\n    summary.setNumKilledReduces(0);\r\n    summary.setUser(\"testUser\");\r\n    summary.setQueue(\"testQueue\");\r\n    summary.setJobStatus(\"testJobStatus\");\r\n    summary.setMapSlotSeconds(7);\r\n    summary.setReduceSlotSeconds(8);\r\n    summary.setJobName(\"testName\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "testEscapeJobSummary",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void testEscapeJobSummary()\n{\r\n    summary.setJobName(\"aa\\rbb\\ncc\\r\\ndd\");\r\n    String out = summary.getJobSummaryString();\r\n    LOG.info(\"summary: \" + out);\r\n    Assert.assertFalse(out.contains(\"\\r\"));\r\n    Assert.assertFalse(out.contains(\"\\n\"));\r\n    Assert.assertTrue(out.contains(\"aa\\\\rbb\\\\ncc\\\\r\\\\ndd\"));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\job\\impl",
  "methodName" : "setup",
  "errType" : null,
  "containingMethodsNum" : 13,
  "sourceCodeText" : "void setup()\n{\r\n    dispatcher = new InlineDispatcher();\r\n    ++startCount;\r\n    conf = new JobConf();\r\n    taskAttemptListener = mock(TaskAttemptListener.class);\r\n    jobToken = (Token<JobTokenIdentifier>) mock(Token.class);\r\n    remoteJobConfFile = mock(Path.class);\r\n    credentials = null;\r\n    clock = SystemClock.getInstance();\r\n    metrics = mock(MRAppMetrics.class);\r\n    dataLocations = new String[1];\r\n    appId = ApplicationId.newInstance(System.currentTimeMillis(), 1);\r\n    jobId = Records.newRecord(JobId.class);\r\n    jobId.setId(1);\r\n    jobId.setAppId(appId);\r\n    appContext = mock(AppContext.class);\r\n    taskSplitMetaInfo = mock(TaskSplitMetaInfo.class);\r\n    when(taskSplitMetaInfo.getLocations()).thenReturn(dataLocations);\r\n    taskAttempts = new ArrayList<MockTaskAttemptImpl>();\r\n    taskAttemptEventHandler = new MockTaskAttemptEventHandler();\r\n    dispatcher.register(TaskAttemptEventType.class, taskAttemptEventHandler);\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\job\\impl",
  "methodName" : "createMockTask",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "MockTaskImpl createMockTask(TaskType taskType)\n{\r\n    return new MockTaskImpl(jobId, partition, dispatcher.getEventHandler(), remoteJobConfFile, conf, taskAttemptListener, jobToken, credentials, clock, startCount, metrics, appContext, taskType);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\job\\impl",
  "methodName" : "teardown",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void teardown()\n{\r\n    taskAttempts.clear();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\job\\impl",
  "methodName" : "getNewTaskID",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "TaskId getNewTaskID()\n{\r\n    TaskId taskId = Records.newRecord(TaskId.class);\r\n    taskId.setId(++taskCounter);\r\n    taskId.setJobId(jobId);\r\n    taskId.setTaskType(mockTask.getType());\r\n    return taskId;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\job\\impl",
  "methodName" : "scheduleTaskAttempt",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void scheduleTaskAttempt(TaskId taskId)\n{\r\n    mockTask.handle(new TaskEvent(taskId, TaskEventType.T_SCHEDULE));\r\n    assertTaskScheduledState();\r\n    assertTaskAttemptAvataar(Avataar.VIRGIN);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\job\\impl",
  "methodName" : "killTask",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void killTask(TaskId taskId)\n{\r\n    mockTask.handle(new TaskEvent(taskId, TaskEventType.T_KILL));\r\n    assertTaskKillWaitState();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\job\\impl",
  "methodName" : "killScheduledTaskAttempt",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void killScheduledTaskAttempt(TaskAttemptId attemptId)\n{\r\n    killScheduledTaskAttempt(attemptId, false);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\job\\impl",
  "methodName" : "killScheduledTaskAttempt",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void killScheduledTaskAttempt(TaskAttemptId attemptId, boolean reschedule)\n{\r\n    mockTask.handle(new TaskTAttemptKilledEvent(attemptId, reschedule));\r\n    assertTaskScheduledState();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\job\\impl",
  "methodName" : "launchTaskAttempt",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void launchTaskAttempt(TaskAttemptId attemptId)\n{\r\n    mockTask.handle(new TaskTAttemptEvent(attemptId, TaskEventType.T_ATTEMPT_LAUNCHED));\r\n    ((MockTaskAttemptImpl) (mockTask.getAttempt(attemptId))).assignContainer();\r\n    assertTaskRunningState();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\job\\impl",
  "methodName" : "commitTaskAttempt",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void commitTaskAttempt(TaskAttemptId attemptId)\n{\r\n    mockTask.handle(new TaskTAttemptEvent(attemptId, TaskEventType.T_ATTEMPT_COMMIT_PENDING));\r\n    assertTaskRunningState();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\job\\impl",
  "methodName" : "getLastAttempt",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "MockTaskAttemptImpl getLastAttempt()\n{\r\n    return taskAttempts.get(taskAttempts.size() - 1);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\job\\impl",
  "methodName" : "updateLastAttemptProgress",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void updateLastAttemptProgress(float p)\n{\r\n    getLastAttempt().setProgress(p);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\job\\impl",
  "methodName" : "updateLastAttemptState",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void updateLastAttemptState(TaskAttemptState s)\n{\r\n    getLastAttempt().setState(s);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\job\\impl",
  "methodName" : "killRunningTaskAttempt",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void killRunningTaskAttempt(TaskAttemptId attemptId)\n{\r\n    killRunningTaskAttempt(attemptId, false);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\job\\impl",
  "methodName" : "killRunningTaskAttempt",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void killRunningTaskAttempt(TaskAttemptId attemptId, boolean reschedule)\n{\r\n    mockTask.handle(new TaskTAttemptKilledEvent(attemptId, reschedule));\r\n    assertTaskRunningState();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\job\\impl",
  "methodName" : "failRunningTaskAttempt",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void failRunningTaskAttempt(TaskAttemptId attemptId)\n{\r\n    mockTask.handle(new TaskTAttemptFailedEvent(attemptId));\r\n    assertTaskRunningState();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\job\\impl",
  "methodName" : "assertTaskNewState",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void assertTaskNewState()\n{\r\n    assertEquals(TaskState.NEW, mockTask.getState());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\job\\impl",
  "methodName" : "assertTaskScheduledState",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void assertTaskScheduledState()\n{\r\n    assertEquals(TaskState.SCHEDULED, mockTask.getState());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\job\\impl",
  "methodName" : "assertTaskRunningState",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void assertTaskRunningState()\n{\r\n    assertEquals(TaskState.RUNNING, mockTask.getState());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\job\\impl",
  "methodName" : "assertTaskKillWaitState",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void assertTaskKillWaitState()\n{\r\n    assertEquals(TaskStateInternal.KILL_WAIT, mockTask.getInternalState());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\job\\impl",
  "methodName" : "assertTaskSucceededState",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void assertTaskSucceededState()\n{\r\n    assertEquals(TaskState.SUCCEEDED, mockTask.getState());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\job\\impl",
  "methodName" : "assertTaskAttemptAvataar",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void assertTaskAttemptAvataar(Avataar avataar)\n{\r\n    for (TaskAttempt taskAttempt : mockTask.getAttempts().values()) {\r\n        if (((TaskAttemptImpl) taskAttempt).getAvataar() == avataar) {\r\n            return;\r\n        }\r\n    }\r\n    fail(\"There is no \" + (avataar == Avataar.VIRGIN ? \"virgin\" : \"speculative\") + \"task attempt\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\job\\impl",
  "methodName" : "testInit",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void testInit()\n{\r\n    LOG.info(\"--- START: testInit ---\");\r\n    mockTask = createMockTask(TaskType.MAP);\r\n    assertTaskNewState();\r\n    assert (taskAttempts.size() == 0);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\job\\impl",
  "methodName" : "testScheduleTask",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void testScheduleTask()\n{\r\n    LOG.info(\"--- START: testScheduleTask ---\");\r\n    mockTask = createMockTask(TaskType.MAP);\r\n    TaskId taskId = getNewTaskID();\r\n    scheduleTaskAttempt(taskId);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\job\\impl",
  "methodName" : "testKillScheduledTask",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void testKillScheduledTask()\n{\r\n    LOG.info(\"--- START: testKillScheduledTask ---\");\r\n    mockTask = createMockTask(TaskType.MAP);\r\n    TaskId taskId = getNewTaskID();\r\n    scheduleTaskAttempt(taskId);\r\n    killTask(taskId);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\job\\impl",
  "methodName" : "testKillScheduledTaskAttempt",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void testKillScheduledTaskAttempt()\n{\r\n    LOG.info(\"--- START: testKillScheduledTaskAttempt ---\");\r\n    mockTask = createMockTask(TaskType.MAP);\r\n    TaskId taskId = getNewTaskID();\r\n    scheduleTaskAttempt(taskId);\r\n    killScheduledTaskAttempt(getLastAttempt().getAttemptId(), true);\r\n    assertEquals(TaskAttemptEventType.TA_RESCHEDULE, taskAttemptEventHandler.lastTaskAttemptEvent.getType());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\job\\impl",
  "methodName" : "testLaunchTaskAttempt",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void testLaunchTaskAttempt()\n{\r\n    LOG.info(\"--- START: testLaunchTaskAttempt ---\");\r\n    mockTask = createMockTask(TaskType.MAP);\r\n    TaskId taskId = getNewTaskID();\r\n    scheduleTaskAttempt(taskId);\r\n    launchTaskAttempt(getLastAttempt().getAttemptId());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\job\\impl",
  "methodName" : "testKillRunningTaskAttempt",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void testKillRunningTaskAttempt()\n{\r\n    LOG.info(\"--- START: testKillRunningTaskAttempt ---\");\r\n    mockTask = createMockTask(TaskType.MAP);\r\n    TaskId taskId = getNewTaskID();\r\n    scheduleTaskAttempt(taskId);\r\n    launchTaskAttempt(getLastAttempt().getAttemptId());\r\n    killRunningTaskAttempt(getLastAttempt().getAttemptId(), true);\r\n    assertEquals(TaskAttemptEventType.TA_RESCHEDULE, taskAttemptEventHandler.lastTaskAttemptEvent.getType());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\job\\impl",
  "methodName" : "testKillSuccessfulTask",
  "errType" : null,
  "containingMethodsNum" : 10,
  "sourceCodeText" : "void testKillSuccessfulTask()\n{\r\n    LOG.info(\"--- START: testKillSuccesfulTask ---\");\r\n    mockTask = createMockTask(TaskType.MAP);\r\n    TaskId taskId = getNewTaskID();\r\n    scheduleTaskAttempt(taskId);\r\n    launchTaskAttempt(getLastAttempt().getAttemptId());\r\n    commitTaskAttempt(getLastAttempt().getAttemptId());\r\n    mockTask.handle(new TaskTAttemptEvent(getLastAttempt().getAttemptId(), TaskEventType.T_ATTEMPT_SUCCEEDED));\r\n    assertTaskSucceededState();\r\n    mockTask.handle(new TaskEvent(taskId, TaskEventType.T_KILL));\r\n    assertTaskSucceededState();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\job\\impl",
  "methodName" : "testKillAttemptForSuccessfulTask",
  "errType" : null,
  "containingMethodsNum" : 11,
  "sourceCodeText" : "void testKillAttemptForSuccessfulTask()\n{\r\n    LOG.info(\"--- START: testKillAttemptForSuccessfulTask ---\");\r\n    mockTask = createMockTask(TaskType.MAP);\r\n    TaskId taskId = getNewTaskID();\r\n    scheduleTaskAttempt(taskId);\r\n    launchTaskAttempt(getLastAttempt().getAttemptId());\r\n    commitTaskAttempt(getLastAttempt().getAttemptId());\r\n    mockTask.handle(new TaskTAttemptEvent(getLastAttempt().getAttemptId(), TaskEventType.T_ATTEMPT_SUCCEEDED));\r\n    assertTaskSucceededState();\r\n    mockTask.handle(new TaskTAttemptKilledEvent(getLastAttempt().getAttemptId(), true));\r\n    assertEquals(TaskAttemptEventType.TA_RESCHEDULE, taskAttemptEventHandler.lastTaskAttemptEvent.getType());\r\n    assertTaskScheduledState();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\job\\impl",
  "methodName" : "testTaskProgress",
  "errType" : null,
  "containingMethodsNum" : 18,
  "sourceCodeText" : "void testTaskProgress()\n{\r\n    LOG.info(\"--- START: testTaskProgress ---\");\r\n    mockTask = createMockTask(TaskType.MAP);\r\n    TaskId taskId = getNewTaskID();\r\n    scheduleTaskAttempt(taskId);\r\n    float progress = 0f;\r\n    assert (mockTask.getProgress() == progress);\r\n    launchTaskAttempt(getLastAttempt().getAttemptId());\r\n    progress = 50f;\r\n    updateLastAttemptProgress(progress);\r\n    assert (mockTask.getProgress() == progress);\r\n    progress = 100f;\r\n    updateLastAttemptProgress(progress);\r\n    assert (mockTask.getProgress() == progress);\r\n    progress = 0f;\r\n    updateLastAttemptState(TaskAttemptState.KILLED);\r\n    assert (mockTask.getProgress() == progress);\r\n    killRunningTaskAttempt(getLastAttempt().getAttemptId());\r\n    assert (taskAttempts.size() == 2);\r\n    assert (mockTask.getProgress() == 0f);\r\n    launchTaskAttempt(getLastAttempt().getAttemptId());\r\n    progress = 50f;\r\n    updateLastAttemptProgress(progress);\r\n    assert (mockTask.getProgress() == progress);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\job\\impl",
  "methodName" : "testKillDuringTaskAttemptCommit",
  "errType" : null,
  "containingMethodsNum" : 10,
  "sourceCodeText" : "void testKillDuringTaskAttemptCommit()\n{\r\n    mockTask = createMockTask(TaskType.REDUCE);\r\n    TaskId taskId = getNewTaskID();\r\n    scheduleTaskAttempt(taskId);\r\n    launchTaskAttempt(getLastAttempt().getAttemptId());\r\n    updateLastAttemptState(TaskAttemptState.COMMIT_PENDING);\r\n    commitTaskAttempt(getLastAttempt().getAttemptId());\r\n    TaskAttemptId commitAttempt = getLastAttempt().getAttemptId();\r\n    updateLastAttemptState(TaskAttemptState.KILLED);\r\n    killRunningTaskAttempt(commitAttempt);\r\n    assertFalse(mockTask.canCommit(commitAttempt));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\job\\impl",
  "methodName" : "testFailureDuringTaskAttemptCommit",
  "errType" : null,
  "containingMethodsNum" : 15,
  "sourceCodeText" : "void testFailureDuringTaskAttemptCommit()\n{\r\n    mockTask = createMockTask(TaskType.MAP);\r\n    TaskId taskId = getNewTaskID();\r\n    scheduleTaskAttempt(taskId);\r\n    launchTaskAttempt(getLastAttempt().getAttemptId());\r\n    updateLastAttemptState(TaskAttemptState.COMMIT_PENDING);\r\n    commitTaskAttempt(getLastAttempt().getAttemptId());\r\n    updateLastAttemptState(TaskAttemptState.FAILED);\r\n    failRunningTaskAttempt(getLastAttempt().getAttemptId());\r\n    assertEquals(2, taskAttempts.size());\r\n    updateLastAttemptState(TaskAttemptState.SUCCEEDED);\r\n    commitTaskAttempt(getLastAttempt().getAttemptId());\r\n    mockTask.handle(new TaskTAttemptEvent(getLastAttempt().getAttemptId(), TaskEventType.T_ATTEMPT_SUCCEEDED));\r\n    assertFalse(\"First attempt should not commit\", mockTask.canCommit(taskAttempts.get(0).getAttemptId()));\r\n    assertTrue(\"Second attempt should commit\", mockTask.canCommit(getLastAttempt().getAttemptId()));\r\n    assertTaskSucceededState();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\job\\impl",
  "methodName" : "runSpeculativeTaskAttemptSucceeds",
  "errType" : null,
  "containingMethodsNum" : 14,
  "sourceCodeText" : "void runSpeculativeTaskAttemptSucceeds(TaskEventType firstAttemptFinishEvent)\n{\r\n    TaskId taskId = getNewTaskID();\r\n    scheduleTaskAttempt(taskId);\r\n    launchTaskAttempt(getLastAttempt().getAttemptId());\r\n    updateLastAttemptState(TaskAttemptState.RUNNING);\r\n    mockTask.handle(new TaskTAttemptEvent(getLastAttempt().getAttemptId(), TaskEventType.T_ADD_SPEC_ATTEMPT));\r\n    launchTaskAttempt(getLastAttempt().getAttemptId());\r\n    commitTaskAttempt(getLastAttempt().getAttemptId());\r\n    mockTask.handle(new TaskTAttemptEvent(getLastAttempt().getAttemptId(), TaskEventType.T_ATTEMPT_SUCCEEDED));\r\n    assertTaskSucceededState();\r\n    if (firstAttemptFinishEvent.equals(TaskEventType.T_ATTEMPT_FAILED)) {\r\n        mockTask.handle(new TaskTAttemptFailedEvent(taskAttempts.get(0).getAttemptId()));\r\n    } else {\r\n        mockTask.handle(new TaskTAttemptEvent(taskAttempts.get(0).getAttemptId(), firstAttemptFinishEvent));\r\n    }\r\n    assertTaskSucceededState();\r\n    assertTaskAttemptAvataar(Avataar.SPECULATIVE);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\job\\impl",
  "methodName" : "testMapSpeculativeTaskAttemptSucceedsEvenIfFirstFails",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void testMapSpeculativeTaskAttemptSucceedsEvenIfFirstFails()\n{\r\n    mockTask = createMockTask(TaskType.MAP);\r\n    runSpeculativeTaskAttemptSucceeds(TaskEventType.T_ATTEMPT_FAILED);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\job\\impl",
  "methodName" : "testReduceSpeculativeTaskAttemptSucceedsEvenIfFirstFails",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void testReduceSpeculativeTaskAttemptSucceedsEvenIfFirstFails()\n{\r\n    mockTask = createMockTask(TaskType.REDUCE);\r\n    runSpeculativeTaskAttemptSucceeds(TaskEventType.T_ATTEMPT_FAILED);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\job\\impl",
  "methodName" : "testMapSpeculativeTaskAttemptSucceedsEvenIfFirstIsKilled",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void testMapSpeculativeTaskAttemptSucceedsEvenIfFirstIsKilled()\n{\r\n    mockTask = createMockTask(TaskType.MAP);\r\n    runSpeculativeTaskAttemptSucceeds(TaskEventType.T_ATTEMPT_KILLED);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\job\\impl",
  "methodName" : "testReduceSpeculativeTaskAttemptSucceedsEvenIfFirstIsKilled",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void testReduceSpeculativeTaskAttemptSucceedsEvenIfFirstIsKilled()\n{\r\n    mockTask = createMockTask(TaskType.REDUCE);\r\n    runSpeculativeTaskAttemptSucceeds(TaskEventType.T_ATTEMPT_KILLED);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\job\\impl",
  "methodName" : "testMultipleTaskAttemptsSucceed",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void testMultipleTaskAttemptsSucceed()\n{\r\n    mockTask = createMockTask(TaskType.MAP);\r\n    runSpeculativeTaskAttemptSucceeds(TaskEventType.T_ATTEMPT_SUCCEEDED);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\job\\impl",
  "methodName" : "testCommitAfterSucceeds",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void testCommitAfterSucceeds()\n{\r\n    mockTask = createMockTask(TaskType.REDUCE);\r\n    runSpeculativeTaskAttemptSucceeds(TaskEventType.T_ATTEMPT_COMMIT_PENDING);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\job\\impl",
  "methodName" : "testSpeculativeMapFetchFailure",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void testSpeculativeMapFetchFailure()\n{\r\n    mockTask = createMockTask(TaskType.MAP);\r\n    runSpeculativeTaskAttemptSucceeds(TaskEventType.T_ATTEMPT_KILLED);\r\n    assertEquals(2, taskAttempts.size());\r\n    mockTask.handle(new TaskTAttemptFailedEvent(taskAttempts.get(1).getAttemptId()));\r\n    assertTaskScheduledState();\r\n    assertEquals(3, taskAttempts.size());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\job\\impl",
  "methodName" : "testSpeculativeMapMultipleSucceedFetchFailure",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void testSpeculativeMapMultipleSucceedFetchFailure()\n{\r\n    mockTask = createMockTask(TaskType.MAP);\r\n    runSpeculativeTaskAttemptSucceeds(TaskEventType.T_ATTEMPT_SUCCEEDED);\r\n    assertEquals(2, taskAttempts.size());\r\n    mockTask.handle(new TaskTAttemptFailedEvent(taskAttempts.get(1).getAttemptId()));\r\n    assertTaskScheduledState();\r\n    assertEquals(3, taskAttempts.size());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\job\\impl",
  "methodName" : "testSpeculativeMapFailedFetchFailure",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void testSpeculativeMapFailedFetchFailure()\n{\r\n    mockTask = createMockTask(TaskType.MAP);\r\n    runSpeculativeTaskAttemptSucceeds(TaskEventType.T_ATTEMPT_FAILED);\r\n    assertEquals(2, taskAttempts.size());\r\n    mockTask.handle(new TaskTAttemptFailedEvent(taskAttempts.get(1).getAttemptId()));\r\n    assertTaskScheduledState();\r\n    assertEquals(3, taskAttempts.size());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\job\\impl",
  "methodName" : "testFailedTransitions",
  "errType" : null,
  "containingMethodsNum" : 36,
  "sourceCodeText" : "void testFailedTransitions()\n{\r\n    mockTask = new MockTaskImpl(jobId, partition, dispatcher.getEventHandler(), remoteJobConfFile, conf, taskAttemptListener, jobToken, credentials, clock, startCount, metrics, appContext, TaskType.MAP) {\r\n\r\n        @Override\r\n        protected int getMaxAttempts() {\r\n            return 1;\r\n        }\r\n    };\r\n    TaskId taskId = getNewTaskID();\r\n    scheduleTaskAttempt(taskId);\r\n    launchTaskAttempt(getLastAttempt().getAttemptId());\r\n    mockTask.handle(new TaskTAttemptEvent(getLastAttempt().getAttemptId(), TaskEventType.T_ADD_SPEC_ATTEMPT));\r\n    launchTaskAttempt(getLastAttempt().getAttemptId());\r\n    mockTask.handle(new TaskTAttemptEvent(getLastAttempt().getAttemptId(), TaskEventType.T_ADD_SPEC_ATTEMPT));\r\n    launchTaskAttempt(getLastAttempt().getAttemptId());\r\n    mockTask.handle(new TaskTAttemptEvent(getLastAttempt().getAttemptId(), TaskEventType.T_ADD_SPEC_ATTEMPT));\r\n    launchTaskAttempt(getLastAttempt().getAttemptId());\r\n    assertEquals(4, taskAttempts.size());\r\n    MockTaskAttemptImpl taskAttempt = taskAttempts.get(0);\r\n    taskAttempt.setState(TaskAttemptState.FAILED);\r\n    mockTask.handle(new TaskTAttemptFailedEvent(taskAttempt.getAttemptId()));\r\n    assertEquals(TaskState.FAILED, mockTask.getState());\r\n    mockTask.handle(new TaskEvent(taskId, TaskEventType.T_KILL));\r\n    assertEquals(TaskState.FAILED, mockTask.getState());\r\n    mockTask.handle(new TaskTAttemptEvent(getLastAttempt().getAttemptId(), TaskEventType.T_ADD_SPEC_ATTEMPT));\r\n    mockTask.handle(new TaskTAttemptEvent(getLastAttempt().getAttemptId(), TaskEventType.T_ATTEMPT_LAUNCHED));\r\n    assertEquals(TaskState.FAILED, mockTask.getState());\r\n    assertEquals(4, taskAttempts.size());\r\n    taskAttempt = taskAttempts.get(1);\r\n    taskAttempt.setState(TaskAttemptState.COMMIT_PENDING);\r\n    mockTask.handle(new TaskTAttemptEvent(taskAttempt.getAttemptId(), TaskEventType.T_ATTEMPT_COMMIT_PENDING));\r\n    assertEquals(TaskState.FAILED, mockTask.getState());\r\n    taskAttempt.setState(TaskAttemptState.FAILED);\r\n    mockTask.handle(new TaskTAttemptFailedEvent(taskAttempt.getAttemptId()));\r\n    assertEquals(TaskState.FAILED, mockTask.getState());\r\n    taskAttempt = taskAttempts.get(2);\r\n    taskAttempt.setState(TaskAttemptState.SUCCEEDED);\r\n    mockTask.handle(new TaskTAttemptEvent(taskAttempt.getAttemptId(), TaskEventType.T_ATTEMPT_SUCCEEDED));\r\n    assertEquals(TaskState.FAILED, mockTask.getState());\r\n    taskAttempt = taskAttempts.get(3);\r\n    taskAttempt.setState(TaskAttemptState.KILLED);\r\n    mockTask.handle(new TaskTAttemptKilledEvent(taskAttempt.getAttemptId(), false));\r\n    assertEquals(TaskState.FAILED, mockTask.getState());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\job\\impl",
  "methodName" : "testFailedTransitionWithHangingSpeculativeMap",
  "errType" : null,
  "containingMethodsNum" : 17,
  "sourceCodeText" : "void testFailedTransitionWithHangingSpeculativeMap()\n{\r\n    mockTask = new MockTaskImpl(jobId, partition, new PartialAttemptEventHandler(), remoteJobConfFile, conf, taskAttemptListener, jobToken, credentials, clock, startCount, metrics, appContext, TaskType.MAP) {\r\n\r\n        @Override\r\n        protected int getMaxAttempts() {\r\n            return 4;\r\n        }\r\n    };\r\n    TaskId taskId = getNewTaskID();\r\n    scheduleTaskAttempt(taskId);\r\n    launchTaskAttempt(getLastAttempt().getAttemptId());\r\n    mockTask.handle(new TaskTAttemptEvent(getLastAttempt().getAttemptId(), TaskEventType.T_ADD_SPEC_ATTEMPT));\r\n    MockTaskAttemptImpl taskAttempt = taskAttempts.get(0);\r\n    taskAttempt.setState(TaskAttemptState.FAILED);\r\n    mockTask.handle(new TaskTAttemptFailedEvent(taskAttempt.getAttemptId()));\r\n    assertEquals(TaskState.RUNNING, mockTask.getState());\r\n    assertEquals(3, taskAttempts.size());\r\n    assertEquals(false, taskAttempts.get(1).getRescheduled());\r\n    assertEquals(true, taskAttempts.get(2).getRescheduled());\r\n    launchTaskAttempt(getLastAttempt().getAttemptId());\r\n    MockTaskAttemptImpl taskAttempt1 = taskAttempts.get(1);\r\n    taskAttempt1.setState(TaskAttemptState.FAILED);\r\n    mockTask.handle(new TaskTAttemptFailedEvent(taskAttempt1.getAttemptId()));\r\n    assertEquals(TaskState.RUNNING, mockTask.getState());\r\n    assertEquals(3, taskAttempts.size());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\job\\impl",
  "methodName" : "testCountersWithSpeculation",
  "errType" : null,
  "containingMethodsNum" : 22,
  "sourceCodeText" : "void testCountersWithSpeculation()\n{\r\n    mockTask = new MockTaskImpl(jobId, partition, dispatcher.getEventHandler(), remoteJobConfFile, conf, taskAttemptListener, jobToken, credentials, clock, startCount, metrics, appContext, TaskType.MAP) {\r\n\r\n        @Override\r\n        protected int getMaxAttempts() {\r\n            return 1;\r\n        }\r\n    };\r\n    TaskId taskId = getNewTaskID();\r\n    scheduleTaskAttempt(taskId);\r\n    launchTaskAttempt(getLastAttempt().getAttemptId());\r\n    updateLastAttemptState(TaskAttemptState.RUNNING);\r\n    MockTaskAttemptImpl baseAttempt = getLastAttempt();\r\n    mockTask.handle(new TaskTAttemptEvent(getLastAttempt().getAttemptId(), TaskEventType.T_ADD_SPEC_ATTEMPT));\r\n    launchTaskAttempt(getLastAttempt().getAttemptId());\r\n    updateLastAttemptState(TaskAttemptState.RUNNING);\r\n    MockTaskAttemptImpl specAttempt = getLastAttempt();\r\n    assertEquals(2, taskAttempts.size());\r\n    Counters specAttemptCounters = new Counters();\r\n    Counter cpuCounter = specAttemptCounters.findCounter(TaskCounter.CPU_MILLISECONDS);\r\n    cpuCounter.setValue(1000);\r\n    specAttempt.setCounters(specAttemptCounters);\r\n    commitTaskAttempt(specAttempt.getAttemptId());\r\n    specAttempt.setProgress(1.0f);\r\n    specAttempt.setState(TaskAttemptState.SUCCEEDED);\r\n    mockTask.handle(new TaskTAttemptEvent(specAttempt.getAttemptId(), TaskEventType.T_ATTEMPT_SUCCEEDED));\r\n    assertEquals(TaskState.SUCCEEDED, mockTask.getState());\r\n    baseAttempt.setProgress(1.0f);\r\n    Counters taskCounters = mockTask.getCounters();\r\n    assertEquals(\"wrong counters for task\", specAttemptCounters, taskCounters);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\webapp",
  "methodName" : "setUp",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void setUp() throws Exception\n{\r\n    super.setUp();\r\n    testConfDir.mkdir();\r\n    GuiceServletConfig.setInjector(Guice.createInjector(new WebServletModule()));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\webapp",
  "methodName" : "stop",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void stop()\n{\r\n    FileUtil.fullyDelete(testConfDir);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\webapp",
  "methodName" : "testJobConf",
  "errType" : null,
  "containingMethodsNum" : 10,
  "sourceCodeText" : "void testJobConf() throws JSONException, Exception\n{\r\n    WebResource r = resource();\r\n    Map<JobId, Job> jobsMap = appContext.getAllJobs();\r\n    for (JobId id : jobsMap.keySet()) {\r\n        String jobId = MRApps.toString(id);\r\n        ClientResponse response = r.path(\"ws\").path(\"v1\").path(\"mapreduce\").path(\"jobs\").path(jobId).path(\"conf\").accept(MediaType.APPLICATION_JSON).get(ClientResponse.class);\r\n        assertEquals(MediaType.APPLICATION_JSON_TYPE + \"; \" + JettyUtils.UTF_8, response.getType().toString());\r\n        JSONObject json = response.getEntity(JSONObject.class);\r\n        assertEquals(\"incorrect number of elements\", 1, json.length());\r\n        JSONObject info = json.getJSONObject(\"conf\");\r\n        verifyAMJobConf(info, jobsMap.get(id));\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\webapp",
  "methodName" : "testJobConfSlash",
  "errType" : null,
  "containingMethodsNum" : 10,
  "sourceCodeText" : "void testJobConfSlash() throws JSONException, Exception\n{\r\n    WebResource r = resource();\r\n    Map<JobId, Job> jobsMap = appContext.getAllJobs();\r\n    for (JobId id : jobsMap.keySet()) {\r\n        String jobId = MRApps.toString(id);\r\n        ClientResponse response = r.path(\"ws\").path(\"v1\").path(\"mapreduce\").path(\"jobs\").path(jobId).path(\"conf/\").accept(MediaType.APPLICATION_JSON).get(ClientResponse.class);\r\n        assertEquals(MediaType.APPLICATION_JSON_TYPE + \"; \" + JettyUtils.UTF_8, response.getType().toString());\r\n        JSONObject json = response.getEntity(JSONObject.class);\r\n        assertEquals(\"incorrect number of elements\", 1, json.length());\r\n        JSONObject info = json.getJSONObject(\"conf\");\r\n        verifyAMJobConf(info, jobsMap.get(id));\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\webapp",
  "methodName" : "testJobConfDefault",
  "errType" : null,
  "containingMethodsNum" : 10,
  "sourceCodeText" : "void testJobConfDefault() throws JSONException, Exception\n{\r\n    WebResource r = resource();\r\n    Map<JobId, Job> jobsMap = appContext.getAllJobs();\r\n    for (JobId id : jobsMap.keySet()) {\r\n        String jobId = MRApps.toString(id);\r\n        ClientResponse response = r.path(\"ws\").path(\"v1\").path(\"mapreduce\").path(\"jobs\").path(jobId).path(\"conf\").get(ClientResponse.class);\r\n        assertEquals(MediaType.APPLICATION_JSON_TYPE + \"; \" + JettyUtils.UTF_8, response.getType().toString());\r\n        JSONObject json = response.getEntity(JSONObject.class);\r\n        assertEquals(\"incorrect number of elements\", 1, json.length());\r\n        JSONObject info = json.getJSONObject(\"conf\");\r\n        verifyAMJobConf(info, jobsMap.get(id));\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\webapp",
  "methodName" : "testJobConfXML",
  "errType" : null,
  "containingMethodsNum" : 13,
  "sourceCodeText" : "void testJobConfXML() throws JSONException, Exception\n{\r\n    WebResource r = resource();\r\n    Map<JobId, Job> jobsMap = appContext.getAllJobs();\r\n    for (JobId id : jobsMap.keySet()) {\r\n        String jobId = MRApps.toString(id);\r\n        ClientResponse response = r.path(\"ws\").path(\"v1\").path(\"mapreduce\").path(\"jobs\").path(jobId).path(\"conf\").accept(MediaType.APPLICATION_XML).get(ClientResponse.class);\r\n        assertEquals(MediaType.APPLICATION_XML_TYPE + \"; \" + JettyUtils.UTF_8, response.getType().toString());\r\n        String xml = response.getEntity(String.class);\r\n        DocumentBuilderFactory dbf = DocumentBuilderFactory.newInstance();\r\n        DocumentBuilder db = dbf.newDocumentBuilder();\r\n        InputSource is = new InputSource();\r\n        is.setCharacterStream(new StringReader(xml));\r\n        Document dom = db.parse(is);\r\n        NodeList info = dom.getElementsByTagName(\"conf\");\r\n        verifyAMJobConfXML(info, jobsMap.get(id));\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\webapp",
  "methodName" : "verifyAMJobConf",
  "errType" : null,
  "containingMethodsNum" : 9,
  "sourceCodeText" : "void verifyAMJobConf(JSONObject info, Job job) throws JSONException\n{\r\n    assertEquals(\"incorrect number of elements\", 2, info.length());\r\n    WebServicesTestUtils.checkStringMatch(\"path\", job.getConfFile().toString(), info.getString(\"path\"));\r\n    JSONArray properties = info.getJSONArray(\"property\");\r\n    for (int i = 0; i < properties.length(); i++) {\r\n        JSONObject prop = properties.getJSONObject(i);\r\n        String name = prop.getString(\"name\");\r\n        String value = prop.getString(\"value\");\r\n        assertTrue(\"name not set\", (name != null && !name.isEmpty()));\r\n        assertTrue(\"value not set\", (value != null && !value.isEmpty()));\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\webapp",
  "methodName" : "verifyAMJobConfXML",
  "errType" : null,
  "containingMethodsNum" : 12,
  "sourceCodeText" : "void verifyAMJobConfXML(NodeList nodes, Job job)\n{\r\n    assertEquals(\"incorrect number of elements\", 1, nodes.getLength());\r\n    for (int i = 0; i < nodes.getLength(); i++) {\r\n        Element element = (Element) nodes.item(i);\r\n        WebServicesTestUtils.checkStringMatch(\"path\", job.getConfFile().toString(), WebServicesTestUtils.getXmlString(element, \"path\"));\r\n        NodeList properties = element.getElementsByTagName(\"property\");\r\n        for (int j = 0; j < properties.getLength(); j++) {\r\n            Element property = (Element) properties.item(j);\r\n            assertNotNull(\"should have counters in the web service info\", property);\r\n            String name = WebServicesTestUtils.getXmlString(property, \"name\");\r\n            String value = WebServicesTestUtils.getXmlString(property, \"value\");\r\n            assertTrue(\"name not set\", (name != null && !name.isEmpty()));\r\n            assertTrue(\"name not set\", (value != null && !value.isEmpty()));\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\webapp",
  "methodName" : "setUp",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void setUp() throws Exception\n{\r\n    super.setUp();\r\n    GuiceServletConfig.setInjector(Guice.createInjector(new WebServletModule()));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\webapp",
  "methodName" : "testJobs",
  "errType" : null,
  "containingMethodsNum" : 10,
  "sourceCodeText" : "void testJobs() throws JSONException, Exception\n{\r\n    WebResource r = resource();\r\n    ClientResponse response = r.path(\"ws\").path(\"v1\").path(\"mapreduce\").path(\"jobs\").accept(MediaType.APPLICATION_JSON).get(ClientResponse.class);\r\n    assertEquals(MediaType.APPLICATION_JSON_TYPE + \"; \" + JettyUtils.UTF_8, response.getType().toString());\r\n    JSONObject json = response.getEntity(JSONObject.class);\r\n    assertEquals(\"incorrect number of elements\", 1, json.length());\r\n    JSONObject jobs = json.getJSONObject(\"jobs\");\r\n    JSONArray arr = jobs.getJSONArray(\"job\");\r\n    JSONObject info = arr.getJSONObject(0);\r\n    Job job = appContext.getJob(MRApps.toJobID(info.getString(\"id\")));\r\n    verifyAMJob(info, job);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\webapp",
  "methodName" : "testJobsSlash",
  "errType" : null,
  "containingMethodsNum" : 10,
  "sourceCodeText" : "void testJobsSlash() throws JSONException, Exception\n{\r\n    WebResource r = resource();\r\n    ClientResponse response = r.path(\"ws\").path(\"v1\").path(\"mapreduce\").path(\"jobs/\").accept(MediaType.APPLICATION_JSON).get(ClientResponse.class);\r\n    assertEquals(MediaType.APPLICATION_JSON_TYPE + \"; \" + JettyUtils.UTF_8, response.getType().toString());\r\n    JSONObject json = response.getEntity(JSONObject.class);\r\n    assertEquals(\"incorrect number of elements\", 1, json.length());\r\n    JSONObject jobs = json.getJSONObject(\"jobs\");\r\n    JSONArray arr = jobs.getJSONArray(\"job\");\r\n    JSONObject info = arr.getJSONObject(0);\r\n    Job job = appContext.getJob(MRApps.toJobID(info.getString(\"id\")));\r\n    verifyAMJob(info, job);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\webapp",
  "methodName" : "testJobsDefault",
  "errType" : null,
  "containingMethodsNum" : 10,
  "sourceCodeText" : "void testJobsDefault() throws JSONException, Exception\n{\r\n    WebResource r = resource();\r\n    ClientResponse response = r.path(\"ws\").path(\"v1\").path(\"mapreduce\").path(\"jobs\").get(ClientResponse.class);\r\n    assertEquals(MediaType.APPLICATION_JSON_TYPE + \"; \" + JettyUtils.UTF_8, response.getType().toString());\r\n    JSONObject json = response.getEntity(JSONObject.class);\r\n    assertEquals(\"incorrect number of elements\", 1, json.length());\r\n    JSONObject jobs = json.getJSONObject(\"jobs\");\r\n    JSONArray arr = jobs.getJSONArray(\"job\");\r\n    JSONObject info = arr.getJSONObject(0);\r\n    Job job = appContext.getJob(MRApps.toJobID(info.getString(\"id\")));\r\n    verifyAMJob(info, job);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\webapp",
  "methodName" : "testJobsXML",
  "errType" : null,
  "containingMethodsNum" : 13,
  "sourceCodeText" : "void testJobsXML() throws Exception\n{\r\n    WebResource r = resource();\r\n    ClientResponse response = r.path(\"ws\").path(\"v1\").path(\"mapreduce\").path(\"jobs\").accept(MediaType.APPLICATION_XML).get(ClientResponse.class);\r\n    assertEquals(MediaType.APPLICATION_XML_TYPE + \"; \" + JettyUtils.UTF_8, response.getType().toString());\r\n    String xml = response.getEntity(String.class);\r\n    DocumentBuilderFactory dbf = DocumentBuilderFactory.newInstance();\r\n    DocumentBuilder db = dbf.newDocumentBuilder();\r\n    InputSource is = new InputSource();\r\n    is.setCharacterStream(new StringReader(xml));\r\n    Document dom = db.parse(is);\r\n    NodeList jobs = dom.getElementsByTagName(\"jobs\");\r\n    assertEquals(\"incorrect number of elements\", 1, jobs.getLength());\r\n    NodeList job = dom.getElementsByTagName(\"job\");\r\n    assertEquals(\"incorrect number of elements\", 1, job.getLength());\r\n    verifyAMJobXML(job, appContext);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\webapp",
  "methodName" : "testJobId",
  "errType" : null,
  "containingMethodsNum" : 10,
  "sourceCodeText" : "void testJobId() throws JSONException, Exception\n{\r\n    WebResource r = resource();\r\n    Map<JobId, Job> jobsMap = appContext.getAllJobs();\r\n    for (JobId id : jobsMap.keySet()) {\r\n        String jobId = MRApps.toString(id);\r\n        ClientResponse response = r.path(\"ws\").path(\"v1\").path(\"mapreduce\").path(\"jobs\").path(jobId).accept(MediaType.APPLICATION_JSON).get(ClientResponse.class);\r\n        assertEquals(MediaType.APPLICATION_JSON_TYPE + \"; \" + JettyUtils.UTF_8, response.getType().toString());\r\n        JSONObject json = response.getEntity(JSONObject.class);\r\n        assertEquals(\"incorrect number of elements\", 1, json.length());\r\n        JSONObject info = json.getJSONObject(\"job\");\r\n        verifyAMJob(info, jobsMap.get(id));\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\webapp",
  "methodName" : "testJobIdSlash",
  "errType" : null,
  "containingMethodsNum" : 10,
  "sourceCodeText" : "void testJobIdSlash() throws JSONException, Exception\n{\r\n    WebResource r = resource();\r\n    Map<JobId, Job> jobsMap = appContext.getAllJobs();\r\n    for (JobId id : jobsMap.keySet()) {\r\n        String jobId = MRApps.toString(id);\r\n        ClientResponse response = r.path(\"ws\").path(\"v1\").path(\"mapreduce\").path(\"jobs\").path(jobId + \"/\").accept(MediaType.APPLICATION_JSON).get(ClientResponse.class);\r\n        assertEquals(MediaType.APPLICATION_JSON_TYPE + \"; \" + JettyUtils.UTF_8, response.getType().toString());\r\n        JSONObject json = response.getEntity(JSONObject.class);\r\n        assertEquals(\"incorrect number of elements\", 1, json.length());\r\n        JSONObject info = json.getJSONObject(\"job\");\r\n        verifyAMJob(info, jobsMap.get(id));\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\webapp",
  "methodName" : "testJobIdDefault",
  "errType" : null,
  "containingMethodsNum" : 10,
  "sourceCodeText" : "void testJobIdDefault() throws JSONException, Exception\n{\r\n    WebResource r = resource();\r\n    Map<JobId, Job> jobsMap = appContext.getAllJobs();\r\n    for (JobId id : jobsMap.keySet()) {\r\n        String jobId = MRApps.toString(id);\r\n        ClientResponse response = r.path(\"ws\").path(\"v1\").path(\"mapreduce\").path(\"jobs\").path(jobId).get(ClientResponse.class);\r\n        assertEquals(MediaType.APPLICATION_JSON_TYPE + \"; \" + JettyUtils.UTF_8, response.getType().toString());\r\n        JSONObject json = response.getEntity(JSONObject.class);\r\n        assertEquals(\"incorrect number of elements\", 1, json.length());\r\n        JSONObject info = json.getJSONObject(\"job\");\r\n        verifyAMJob(info, jobsMap.get(id));\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\webapp",
  "methodName" : "testJobIdNonExist",
  "errType" : [ "UniformInterfaceException" ],
  "containingMethodsNum" : 15,
  "sourceCodeText" : "void testJobIdNonExist() throws JSONException, Exception\n{\r\n    WebResource r = resource();\r\n    try {\r\n        r.path(\"ws\").path(\"v1\").path(\"mapreduce\").path(\"jobs\").path(\"job_0_1234\").get(JSONObject.class);\r\n        fail(\"should have thrown exception on invalid uri\");\r\n    } catch (UniformInterfaceException ue) {\r\n        ClientResponse response = ue.getResponse();\r\n        assertResponseStatusCode(Status.NOT_FOUND, response.getStatusInfo());\r\n        assertEquals(MediaType.APPLICATION_JSON_TYPE + \"; \" + JettyUtils.UTF_8, response.getType().toString());\r\n        JSONObject msg = response.getEntity(JSONObject.class);\r\n        JSONObject exception = msg.getJSONObject(\"RemoteException\");\r\n        assertEquals(\"incorrect number of elements\", 3, exception.length());\r\n        String message = exception.getString(\"message\");\r\n        String type = exception.getString(\"exception\");\r\n        String classname = exception.getString(\"javaClassName\");\r\n        WebServicesTestUtils.checkStringMatch(\"exception message\", \"java.lang.Exception: job, job_0_1234, is not found\", message);\r\n        WebServicesTestUtils.checkStringMatch(\"exception type\", \"NotFoundException\", type);\r\n        WebServicesTestUtils.checkStringMatch(\"exception classname\", \"org.apache.hadoop.yarn.webapp.NotFoundException\", classname);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\webapp",
  "methodName" : "testJobIdInvalid",
  "errType" : [ "UniformInterfaceException" ],
  "containingMethodsNum" : 13,
  "sourceCodeText" : "void testJobIdInvalid() throws JSONException, Exception\n{\r\n    WebResource r = resource();\r\n    try {\r\n        r.path(\"ws\").path(\"v1\").path(\"mapreduce\").path(\"jobs\").path(\"job_foo\").accept(MediaType.APPLICATION_JSON).get(JSONObject.class);\r\n        fail(\"should have thrown exception on invalid uri\");\r\n    } catch (UniformInterfaceException ue) {\r\n        ClientResponse response = ue.getResponse();\r\n        assertResponseStatusCode(Status.NOT_FOUND, response.getStatusInfo());\r\n        assertEquals(MediaType.APPLICATION_JSON_TYPE + \"; \" + JettyUtils.UTF_8, response.getType().toString());\r\n        JSONObject msg = response.getEntity(JSONObject.class);\r\n        JSONObject exception = msg.getJSONObject(\"RemoteException\");\r\n        assertEquals(\"incorrect number of elements\", 3, exception.length());\r\n        String message = exception.getString(\"message\");\r\n        String type = exception.getString(\"exception\");\r\n        String classname = exception.getString(\"javaClassName\");\r\n        verifyJobIdInvalid(message, type, classname);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\webapp",
  "methodName" : "testJobIdInvalidDefault",
  "errType" : [ "UniformInterfaceException" ],
  "containingMethodsNum" : 13,
  "sourceCodeText" : "void testJobIdInvalidDefault() throws JSONException, Exception\n{\r\n    WebResource r = resource();\r\n    try {\r\n        r.path(\"ws\").path(\"v1\").path(\"mapreduce\").path(\"jobs\").path(\"job_foo\").get(JSONObject.class);\r\n        fail(\"should have thrown exception on invalid uri\");\r\n    } catch (UniformInterfaceException ue) {\r\n        ClientResponse response = ue.getResponse();\r\n        assertResponseStatusCode(Status.NOT_FOUND, response.getStatusInfo());\r\n        assertEquals(MediaType.APPLICATION_JSON_TYPE + \"; \" + JettyUtils.UTF_8, response.getType().toString());\r\n        JSONObject msg = response.getEntity(JSONObject.class);\r\n        JSONObject exception = msg.getJSONObject(\"RemoteException\");\r\n        assertEquals(\"incorrect number of elements\", 3, exception.length());\r\n        String message = exception.getString(\"message\");\r\n        String type = exception.getString(\"exception\");\r\n        String classname = exception.getString(\"javaClassName\");\r\n        verifyJobIdInvalid(message, type, classname);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\webapp",
  "methodName" : "testJobIdInvalidXML",
  "errType" : [ "UniformInterfaceException" ],
  "containingMethodsNum" : 18,
  "sourceCodeText" : "void testJobIdInvalidXML() throws JSONException, Exception\n{\r\n    WebResource r = resource();\r\n    try {\r\n        r.path(\"ws\").path(\"v1\").path(\"mapreduce\").path(\"jobs\").path(\"job_foo\").accept(MediaType.APPLICATION_XML).get(JSONObject.class);\r\n        fail(\"should have thrown exception on invalid uri\");\r\n    } catch (UniformInterfaceException ue) {\r\n        ClientResponse response = ue.getResponse();\r\n        assertResponseStatusCode(Status.NOT_FOUND, response.getStatusInfo());\r\n        assertEquals(MediaType.APPLICATION_XML_TYPE + \"; \" + JettyUtils.UTF_8, response.getType().toString());\r\n        String msg = response.getEntity(String.class);\r\n        System.out.println(msg);\r\n        DocumentBuilderFactory dbf = DocumentBuilderFactory.newInstance();\r\n        DocumentBuilder db = dbf.newDocumentBuilder();\r\n        InputSource is = new InputSource();\r\n        is.setCharacterStream(new StringReader(msg));\r\n        Document dom = db.parse(is);\r\n        NodeList nodes = dom.getElementsByTagName(\"RemoteException\");\r\n        Element element = (Element) nodes.item(0);\r\n        String message = WebServicesTestUtils.getXmlString(element, \"message\");\r\n        String type = WebServicesTestUtils.getXmlString(element, \"exception\");\r\n        String classname = WebServicesTestUtils.getXmlString(element, \"javaClassName\");\r\n        verifyJobIdInvalid(message, type, classname);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\webapp",
  "methodName" : "verifyJobIdInvalid",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void verifyJobIdInvalid(String message, String type, String classname)\n{\r\n    WebServicesTestUtils.checkStringMatch(\"exception message\", \"java.lang.Exception: JobId string : job_foo is not properly formed\", message);\r\n    WebServicesTestUtils.checkStringMatch(\"exception type\", \"NotFoundException\", type);\r\n    WebServicesTestUtils.checkStringMatch(\"exception classname\", \"org.apache.hadoop.yarn.webapp.NotFoundException\", classname);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\webapp",
  "methodName" : "testJobIdInvalidBogus",
  "errType" : [ "UniformInterfaceException" ],
  "containingMethodsNum" : 15,
  "sourceCodeText" : "void testJobIdInvalidBogus() throws JSONException, Exception\n{\r\n    WebResource r = resource();\r\n    try {\r\n        r.path(\"ws\").path(\"v1\").path(\"mapreduce\").path(\"jobs\").path(\"bogusfoo\").get(JSONObject.class);\r\n        fail(\"should have thrown exception on invalid uri\");\r\n    } catch (UniformInterfaceException ue) {\r\n        ClientResponse response = ue.getResponse();\r\n        assertResponseStatusCode(Status.NOT_FOUND, response.getStatusInfo());\r\n        assertEquals(MediaType.APPLICATION_JSON_TYPE + \"; \" + JettyUtils.UTF_8, response.getType().toString());\r\n        JSONObject msg = response.getEntity(JSONObject.class);\r\n        JSONObject exception = msg.getJSONObject(\"RemoteException\");\r\n        assertEquals(\"incorrect number of elements\", 3, exception.length());\r\n        String message = exception.getString(\"message\");\r\n        String type = exception.getString(\"exception\");\r\n        String classname = exception.getString(\"javaClassName\");\r\n        WebServicesTestUtils.checkStringMatch(\"exception message\", \"java.lang.Exception: JobId string : bogusfoo is not properly formed\", message);\r\n        WebServicesTestUtils.checkStringMatch(\"exception type\", \"NotFoundException\", type);\r\n        WebServicesTestUtils.checkStringMatch(\"exception classname\", \"org.apache.hadoop.yarn.webapp.NotFoundException\", classname);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\webapp",
  "methodName" : "testJobIdXML",
  "errType" : null,
  "containingMethodsNum" : 13,
  "sourceCodeText" : "void testJobIdXML() throws Exception\n{\r\n    WebResource r = resource();\r\n    Map<JobId, Job> jobsMap = appContext.getAllJobs();\r\n    for (JobId id : jobsMap.keySet()) {\r\n        String jobId = MRApps.toString(id);\r\n        ClientResponse response = r.path(\"ws\").path(\"v1\").path(\"mapreduce\").path(\"jobs\").path(jobId).accept(MediaType.APPLICATION_XML).get(ClientResponse.class);\r\n        assertEquals(MediaType.APPLICATION_XML_TYPE + \"; \" + JettyUtils.UTF_8, response.getType().toString());\r\n        String xml = response.getEntity(String.class);\r\n        DocumentBuilderFactory dbf = DocumentBuilderFactory.newInstance();\r\n        DocumentBuilder db = dbf.newDocumentBuilder();\r\n        InputSource is = new InputSource();\r\n        is.setCharacterStream(new StringReader(xml));\r\n        Document dom = db.parse(is);\r\n        NodeList job = dom.getElementsByTagName(\"job\");\r\n        verifyAMJobXML(job, appContext);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\webapp",
  "methodName" : "verifyAMJob",
  "errType" : null,
  "containingMethodsNum" : 17,
  "sourceCodeText" : "void verifyAMJob(JSONObject info, Job job) throws JSONException\n{\r\n    assertEquals(\"incorrect number of elements\", 31, info.length());\r\n    verifyAMJobGeneric(job, info.getString(\"id\"), info.getString(\"user\"), info.getString(\"name\"), info.getString(\"state\"), info.getLong(\"startTime\"), info.getLong(\"finishTime\"), info.getLong(\"elapsedTime\"), info.getInt(\"mapsTotal\"), info.getInt(\"mapsCompleted\"), info.getInt(\"reducesTotal\"), info.getInt(\"reducesCompleted\"), (float) info.getDouble(\"reduceProgress\"), (float) info.getDouble(\"mapProgress\"));\r\n    String diagnostics = \"\";\r\n    if (info.has(\"diagnostics\")) {\r\n        diagnostics = info.getString(\"diagnostics\");\r\n    }\r\n    verifyAMJobGenericSecure(job, info.getInt(\"mapsPending\"), info.getInt(\"mapsRunning\"), info.getInt(\"reducesPending\"), info.getInt(\"reducesRunning\"), info.getBoolean(\"uberized\"), diagnostics, info.getInt(\"newReduceAttempts\"), info.getInt(\"runningReduceAttempts\"), info.getInt(\"failedReduceAttempts\"), info.getInt(\"killedReduceAttempts\"), info.getInt(\"successfulReduceAttempts\"), info.getInt(\"newMapAttempts\"), info.getInt(\"runningMapAttempts\"), info.getInt(\"failedMapAttempts\"), info.getInt(\"killedMapAttempts\"), info.getInt(\"successfulMapAttempts\"));\r\n    Map<JobACL, AccessControlList> allacls = job.getJobACLs();\r\n    if (allacls != null) {\r\n        for (Map.Entry<JobACL, AccessControlList> entry : allacls.entrySet()) {\r\n            String expectName = entry.getKey().getAclName();\r\n            String expectValue = entry.getValue().getAclString();\r\n            Boolean found = false;\r\n            if (info.has(\"acls\")) {\r\n                JSONArray arr = info.getJSONArray(\"acls\");\r\n                for (int i = 0; i < arr.length(); i++) {\r\n                    JSONObject aclInfo = arr.getJSONObject(i);\r\n                    if (expectName.matches(aclInfo.getString(\"name\"))) {\r\n                        found = true;\r\n                        WebServicesTestUtils.checkStringMatch(\"value\", expectValue, aclInfo.getString(\"value\"));\r\n                    }\r\n                }\r\n            } else {\r\n                fail(\"should have acls in the web service info\");\r\n            }\r\n            assertTrue(\"acl: \" + expectName + \" not found in webservice output\", found);\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\webapp",
  "methodName" : "verifyAMJobXML",
  "errType" : null,
  "containingMethodsNum" : 19,
  "sourceCodeText" : "void verifyAMJobXML(NodeList nodes, AppContext appContext)\n{\r\n    assertEquals(\"incorrect number of elements\", 1, nodes.getLength());\r\n    for (int i = 0; i < nodes.getLength(); i++) {\r\n        Element element = (Element) nodes.item(i);\r\n        Job job = appContext.getJob(MRApps.toJobID(WebServicesTestUtils.getXmlString(element, \"id\")));\r\n        assertNotNull(\"Job not found - output incorrect\", job);\r\n        verifyAMJobGeneric(job, WebServicesTestUtils.getXmlString(element, \"id\"), WebServicesTestUtils.getXmlString(element, \"user\"), WebServicesTestUtils.getXmlString(element, \"name\"), WebServicesTestUtils.getXmlString(element, \"state\"), WebServicesTestUtils.getXmlLong(element, \"startTime\"), WebServicesTestUtils.getXmlLong(element, \"finishTime\"), WebServicesTestUtils.getXmlLong(element, \"elapsedTime\"), WebServicesTestUtils.getXmlInt(element, \"mapsTotal\"), WebServicesTestUtils.getXmlInt(element, \"mapsCompleted\"), WebServicesTestUtils.getXmlInt(element, \"reducesTotal\"), WebServicesTestUtils.getXmlInt(element, \"reducesCompleted\"), WebServicesTestUtils.getXmlFloat(element, \"reduceProgress\"), WebServicesTestUtils.getXmlFloat(element, \"mapProgress\"));\r\n        verifyAMJobGenericSecure(job, WebServicesTestUtils.getXmlInt(element, \"mapsPending\"), WebServicesTestUtils.getXmlInt(element, \"mapsRunning\"), WebServicesTestUtils.getXmlInt(element, \"reducesPending\"), WebServicesTestUtils.getXmlInt(element, \"reducesRunning\"), WebServicesTestUtils.getXmlBoolean(element, \"uberized\"), WebServicesTestUtils.getXmlString(element, \"diagnostics\"), WebServicesTestUtils.getXmlInt(element, \"newReduceAttempts\"), WebServicesTestUtils.getXmlInt(element, \"runningReduceAttempts\"), WebServicesTestUtils.getXmlInt(element, \"failedReduceAttempts\"), WebServicesTestUtils.getXmlInt(element, \"killedReduceAttempts\"), WebServicesTestUtils.getXmlInt(element, \"successfulReduceAttempts\"), WebServicesTestUtils.getXmlInt(element, \"newMapAttempts\"), WebServicesTestUtils.getXmlInt(element, \"runningMapAttempts\"), WebServicesTestUtils.getXmlInt(element, \"failedMapAttempts\"), WebServicesTestUtils.getXmlInt(element, \"killedMapAttempts\"), WebServicesTestUtils.getXmlInt(element, \"successfulMapAttempts\"));\r\n        Map<JobACL, AccessControlList> allacls = job.getJobACLs();\r\n        if (allacls != null) {\r\n            for (Map.Entry<JobACL, AccessControlList> entry : allacls.entrySet()) {\r\n                String expectName = entry.getKey().getAclName();\r\n                String expectValue = entry.getValue().getAclString();\r\n                Boolean found = false;\r\n                NodeList id = element.getElementsByTagName(\"acls\");\r\n                if (id != null) {\r\n                    for (int j = 0; j < id.getLength(); j++) {\r\n                        Element aclElem = (Element) id.item(j);\r\n                        if (aclElem == null) {\r\n                            fail(\"should have acls in the web service info\");\r\n                        }\r\n                        if (expectName.matches(WebServicesTestUtils.getXmlString(aclElem, \"name\"))) {\r\n                            found = true;\r\n                            WebServicesTestUtils.checkStringMatch(\"value\", expectValue, WebServicesTestUtils.getXmlString(aclElem, \"value\"));\r\n                        }\r\n                    }\r\n                } else {\r\n                    fail(\"should have acls in the web service info\");\r\n                }\r\n                assertTrue(\"acl: \" + expectName + \" not found in webservice output\", found);\r\n            }\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\webapp",
  "methodName" : "verifyAMJobGeneric",
  "errType" : null,
  "containingMethodsNum" : 14,
  "sourceCodeText" : "void verifyAMJobGeneric(Job job, String id, String user, String name, String state, long startTime, long finishTime, long elapsedTime, int mapsTotal, int mapsCompleted, int reducesTotal, int reducesCompleted, float reduceProgress, float mapProgress)\n{\r\n    JobReport report = job.getReport();\r\n    WebServicesTestUtils.checkStringMatch(\"id\", MRApps.toString(job.getID()), id);\r\n    WebServicesTestUtils.checkStringMatch(\"user\", job.getUserName().toString(), user);\r\n    WebServicesTestUtils.checkStringMatch(\"name\", job.getName(), name);\r\n    WebServicesTestUtils.checkStringMatch(\"state\", job.getState().toString(), state);\r\n    assertEquals(\"startTime incorrect\", report.getStartTime(), startTime);\r\n    assertEquals(\"finishTime incorrect\", report.getFinishTime(), finishTime);\r\n    assertEquals(\"elapsedTime incorrect\", Times.elapsed(report.getStartTime(), report.getFinishTime()), elapsedTime);\r\n    assertEquals(\"mapsTotal incorrect\", job.getTotalMaps(), mapsTotal);\r\n    assertEquals(\"mapsCompleted incorrect\", job.getCompletedMaps(), mapsCompleted);\r\n    assertEquals(\"reducesTotal incorrect\", job.getTotalReduces(), reducesTotal);\r\n    assertEquals(\"reducesCompleted incorrect\", job.getCompletedReduces(), reducesCompleted);\r\n    assertEquals(\"mapProgress incorrect\", report.getMapProgress() * 100, mapProgress, 0);\r\n    assertEquals(\"reduceProgress incorrect\", report.getReduceProgress() * 100, reduceProgress, 0);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\webapp",
  "methodName" : "verifyAMJobGenericSecure",
  "errType" : null,
  "containingMethodsNum" : 20,
  "sourceCodeText" : "void verifyAMJobGenericSecure(Job job, int mapsPending, int mapsRunning, int reducesPending, int reducesRunning, Boolean uberized, String diagnostics, int newReduceAttempts, int runningReduceAttempts, int failedReduceAttempts, int killedReduceAttempts, int successfulReduceAttempts, int newMapAttempts, int runningMapAttempts, int failedMapAttempts, int killedMapAttempts, int successfulMapAttempts)\n{\r\n    String diagString = \"\";\r\n    List<String> diagList = job.getDiagnostics();\r\n    if (diagList != null && !diagList.isEmpty()) {\r\n        StringBuffer b = new StringBuffer();\r\n        for (String diag : diagList) {\r\n            b.append(diag);\r\n        }\r\n        diagString = b.toString();\r\n    }\r\n    WebServicesTestUtils.checkStringMatch(\"diagnostics\", diagString, diagnostics);\r\n    assertEquals(\"isUber incorrect\", job.isUber(), uberized);\r\n    assertTrue(\"mapsPending not >= 0\", mapsPending >= 0);\r\n    assertTrue(\"mapsRunning not >= 0\", mapsRunning >= 0);\r\n    assertTrue(\"reducesPending not >= 0\", reducesPending >= 0);\r\n    assertTrue(\"reducesRunning not >= 0\", reducesRunning >= 0);\r\n    assertTrue(\"newReduceAttempts not >= 0\", newReduceAttempts >= 0);\r\n    assertTrue(\"runningReduceAttempts not >= 0\", runningReduceAttempts >= 0);\r\n    assertTrue(\"failedReduceAttempts not >= 0\", failedReduceAttempts >= 0);\r\n    assertTrue(\"killedReduceAttempts not >= 0\", killedReduceAttempts >= 0);\r\n    assertTrue(\"successfulReduceAttempts not >= 0\", successfulReduceAttempts >= 0);\r\n    assertTrue(\"newMapAttempts not >= 0\", newMapAttempts >= 0);\r\n    assertTrue(\"runningMapAttempts not >= 0\", runningMapAttempts >= 0);\r\n    assertTrue(\"failedMapAttempts not >= 0\", failedMapAttempts >= 0);\r\n    assertTrue(\"killedMapAttempts not >= 0\", killedMapAttempts >= 0);\r\n    assertTrue(\"successfulMapAttempts not >= 0\", successfulMapAttempts >= 0);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\webapp",
  "methodName" : "testJobCounters",
  "errType" : null,
  "containingMethodsNum" : 10,
  "sourceCodeText" : "void testJobCounters() throws JSONException, Exception\n{\r\n    WebResource r = resource();\r\n    Map<JobId, Job> jobsMap = appContext.getAllJobs();\r\n    for (JobId id : jobsMap.keySet()) {\r\n        String jobId = MRApps.toString(id);\r\n        ClientResponse response = r.path(\"ws\").path(\"v1\").path(\"mapreduce\").path(\"jobs\").path(jobId).path(\"counters\").accept(MediaType.APPLICATION_JSON).get(ClientResponse.class);\r\n        assertEquals(MediaType.APPLICATION_JSON_TYPE + \"; \" + JettyUtils.UTF_8, response.getType().toString());\r\n        JSONObject json = response.getEntity(JSONObject.class);\r\n        assertEquals(\"incorrect number of elements\", 1, json.length());\r\n        JSONObject info = json.getJSONObject(\"jobCounters\");\r\n        verifyAMJobCounters(info, jobsMap.get(id));\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\webapp",
  "methodName" : "testJobCountersSlash",
  "errType" : null,
  "containingMethodsNum" : 10,
  "sourceCodeText" : "void testJobCountersSlash() throws JSONException, Exception\n{\r\n    WebResource r = resource();\r\n    Map<JobId, Job> jobsMap = appContext.getAllJobs();\r\n    for (JobId id : jobsMap.keySet()) {\r\n        String jobId = MRApps.toString(id);\r\n        ClientResponse response = r.path(\"ws\").path(\"v1\").path(\"mapreduce\").path(\"jobs\").path(jobId).path(\"counters/\").accept(MediaType.APPLICATION_JSON).get(ClientResponse.class);\r\n        assertEquals(MediaType.APPLICATION_JSON_TYPE + \"; \" + JettyUtils.UTF_8, response.getType().toString());\r\n        JSONObject json = response.getEntity(JSONObject.class);\r\n        assertEquals(\"incorrect number of elements\", 1, json.length());\r\n        JSONObject info = json.getJSONObject(\"jobCounters\");\r\n        verifyAMJobCounters(info, jobsMap.get(id));\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\webapp",
  "methodName" : "testJobCountersDefault",
  "errType" : null,
  "containingMethodsNum" : 10,
  "sourceCodeText" : "void testJobCountersDefault() throws JSONException, Exception\n{\r\n    WebResource r = resource();\r\n    Map<JobId, Job> jobsMap = appContext.getAllJobs();\r\n    for (JobId id : jobsMap.keySet()) {\r\n        String jobId = MRApps.toString(id);\r\n        ClientResponse response = r.path(\"ws\").path(\"v1\").path(\"mapreduce\").path(\"jobs\").path(jobId).path(\"counters/\").get(ClientResponse.class);\r\n        assertEquals(MediaType.APPLICATION_JSON_TYPE + \"; \" + JettyUtils.UTF_8, response.getType().toString());\r\n        JSONObject json = response.getEntity(JSONObject.class);\r\n        assertEquals(\"incorrect number of elements\", 1, json.length());\r\n        JSONObject info = json.getJSONObject(\"jobCounters\");\r\n        verifyAMJobCounters(info, jobsMap.get(id));\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\webapp",
  "methodName" : "testJobCountersXML",
  "errType" : null,
  "containingMethodsNum" : 13,
  "sourceCodeText" : "void testJobCountersXML() throws Exception\n{\r\n    WebResource r = resource();\r\n    Map<JobId, Job> jobsMap = appContext.getAllJobs();\r\n    for (JobId id : jobsMap.keySet()) {\r\n        String jobId = MRApps.toString(id);\r\n        ClientResponse response = r.path(\"ws\").path(\"v1\").path(\"mapreduce\").path(\"jobs\").path(jobId).path(\"counters\").accept(MediaType.APPLICATION_XML).get(ClientResponse.class);\r\n        assertEquals(MediaType.APPLICATION_XML_TYPE + \"; \" + JettyUtils.UTF_8, response.getType().toString());\r\n        String xml = response.getEntity(String.class);\r\n        DocumentBuilderFactory dbf = DocumentBuilderFactory.newInstance();\r\n        DocumentBuilder db = dbf.newDocumentBuilder();\r\n        InputSource is = new InputSource();\r\n        is.setCharacterStream(new StringReader(xml));\r\n        Document dom = db.parse(is);\r\n        NodeList info = dom.getElementsByTagName(\"jobCounters\");\r\n        verifyAMJobCountersXML(info, jobsMap.get(id));\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\webapp",
  "methodName" : "verifyAMJobCounters",
  "errType" : null,
  "containingMethodsNum" : 18,
  "sourceCodeText" : "void verifyAMJobCounters(JSONObject info, Job job) throws JSONException\n{\r\n    assertEquals(\"incorrect number of elements\", 2, info.length());\r\n    WebServicesTestUtils.checkStringMatch(\"id\", MRApps.toString(job.getID()), info.getString(\"id\"));\r\n    JSONArray counterGroups = info.getJSONArray(\"counterGroup\");\r\n    for (int i = 0; i < counterGroups.length(); i++) {\r\n        JSONObject counterGroup = counterGroups.getJSONObject(i);\r\n        String name = counterGroup.getString(\"counterGroupName\");\r\n        assertTrue(\"name not set\", (name != null && !name.isEmpty()));\r\n        JSONArray counters = counterGroup.getJSONArray(\"counter\");\r\n        for (int j = 0; j < counters.length(); j++) {\r\n            JSONObject counter = counters.getJSONObject(j);\r\n            String counterName = counter.getString(\"name\");\r\n            assertTrue(\"counter name not set\", (counterName != null && !counterName.isEmpty()));\r\n            long mapValue = counter.getLong(\"mapCounterValue\");\r\n            assertTrue(\"mapCounterValue  >= 0\", mapValue >= 0);\r\n            long reduceValue = counter.getLong(\"reduceCounterValue\");\r\n            assertTrue(\"reduceCounterValue  >= 0\", reduceValue >= 0);\r\n            long totalValue = counter.getLong(\"totalCounterValue\");\r\n            assertTrue(\"totalCounterValue  >= 0\", totalValue >= 0);\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\webapp",
  "methodName" : "verifyAMJobCountersXML",
  "errType" : null,
  "containingMethodsNum" : 21,
  "sourceCodeText" : "void verifyAMJobCountersXML(NodeList nodes, Job job)\n{\r\n    for (int i = 0; i < nodes.getLength(); i++) {\r\n        Element element = (Element) nodes.item(i);\r\n        assertNotNull(\"Job not found - output incorrect\", job);\r\n        WebServicesTestUtils.checkStringMatch(\"id\", MRApps.toString(job.getID()), WebServicesTestUtils.getXmlString(element, \"id\"));\r\n        NodeList groups = element.getElementsByTagName(\"counterGroup\");\r\n        for (int j = 0; j < groups.getLength(); j++) {\r\n            Element counters = (Element) groups.item(j);\r\n            assertNotNull(\"should have counters in the web service info\", counters);\r\n            String name = WebServicesTestUtils.getXmlString(counters, \"counterGroupName\");\r\n            assertTrue(\"name not set\", (name != null && !name.isEmpty()));\r\n            NodeList counterArr = counters.getElementsByTagName(\"counter\");\r\n            for (int z = 0; z < counterArr.getLength(); z++) {\r\n                Element counter = (Element) counterArr.item(z);\r\n                String counterName = WebServicesTestUtils.getXmlString(counter, \"name\");\r\n                assertTrue(\"counter name not set\", (counterName != null && !counterName.isEmpty()));\r\n                long mapValue = WebServicesTestUtils.getXmlLong(counter, \"mapCounterValue\");\r\n                assertTrue(\"mapCounterValue not >= 0\", mapValue >= 0);\r\n                long reduceValue = WebServicesTestUtils.getXmlLong(counter, \"reduceCounterValue\");\r\n                assertTrue(\"reduceCounterValue  >= 0\", reduceValue >= 0);\r\n                long totalValue = WebServicesTestUtils.getXmlLong(counter, \"totalCounterValue\");\r\n                assertTrue(\"totalCounterValue  >= 0\", totalValue >= 0);\r\n            }\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\webapp",
  "methodName" : "testJobAttempts",
  "errType" : null,
  "containingMethodsNum" : 10,
  "sourceCodeText" : "void testJobAttempts() throws JSONException, Exception\n{\r\n    WebResource r = resource();\r\n    Map<JobId, Job> jobsMap = appContext.getAllJobs();\r\n    for (JobId id : jobsMap.keySet()) {\r\n        String jobId = MRApps.toString(id);\r\n        ClientResponse response = r.path(\"ws\").path(\"v1\").path(\"mapreduce\").path(\"jobs\").path(jobId).path(\"jobattempts\").accept(MediaType.APPLICATION_JSON).get(ClientResponse.class);\r\n        assertEquals(MediaType.APPLICATION_JSON_TYPE + \"; \" + JettyUtils.UTF_8, response.getType().toString());\r\n        JSONObject json = response.getEntity(JSONObject.class);\r\n        assertEquals(\"incorrect number of elements\", 1, json.length());\r\n        JSONObject info = json.getJSONObject(\"jobAttempts\");\r\n        verifyJobAttempts(info, jobsMap.get(id));\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\webapp",
  "methodName" : "testJobAttemptsSlash",
  "errType" : null,
  "containingMethodsNum" : 10,
  "sourceCodeText" : "void testJobAttemptsSlash() throws JSONException, Exception\n{\r\n    WebResource r = resource();\r\n    Map<JobId, Job> jobsMap = appContext.getAllJobs();\r\n    for (JobId id : jobsMap.keySet()) {\r\n        String jobId = MRApps.toString(id);\r\n        ClientResponse response = r.path(\"ws\").path(\"v1\").path(\"mapreduce\").path(\"jobs\").path(jobId).path(\"jobattempts/\").accept(MediaType.APPLICATION_JSON).get(ClientResponse.class);\r\n        assertEquals(MediaType.APPLICATION_JSON_TYPE + \"; \" + JettyUtils.UTF_8, response.getType().toString());\r\n        JSONObject json = response.getEntity(JSONObject.class);\r\n        assertEquals(\"incorrect number of elements\", 1, json.length());\r\n        JSONObject info = json.getJSONObject(\"jobAttempts\");\r\n        verifyJobAttempts(info, jobsMap.get(id));\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\webapp",
  "methodName" : "testJobAttemptsDefault",
  "errType" : null,
  "containingMethodsNum" : 10,
  "sourceCodeText" : "void testJobAttemptsDefault() throws JSONException, Exception\n{\r\n    WebResource r = resource();\r\n    Map<JobId, Job> jobsMap = appContext.getAllJobs();\r\n    for (JobId id : jobsMap.keySet()) {\r\n        String jobId = MRApps.toString(id);\r\n        ClientResponse response = r.path(\"ws\").path(\"v1\").path(\"mapreduce\").path(\"jobs\").path(jobId).path(\"jobattempts\").get(ClientResponse.class);\r\n        assertEquals(MediaType.APPLICATION_JSON_TYPE + \"; \" + JettyUtils.UTF_8, response.getType().toString());\r\n        JSONObject json = response.getEntity(JSONObject.class);\r\n        assertEquals(\"incorrect number of elements\", 1, json.length());\r\n        JSONObject info = json.getJSONObject(\"jobAttempts\");\r\n        verifyJobAttempts(info, jobsMap.get(id));\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\webapp",
  "methodName" : "testJobAttemptsXML",
  "errType" : null,
  "containingMethodsNum" : 15,
  "sourceCodeText" : "void testJobAttemptsXML() throws Exception\n{\r\n    WebResource r = resource();\r\n    Map<JobId, Job> jobsMap = appContext.getAllJobs();\r\n    for (JobId id : jobsMap.keySet()) {\r\n        String jobId = MRApps.toString(id);\r\n        ClientResponse response = r.path(\"ws\").path(\"v1\").path(\"mapreduce\").path(\"jobs\").path(jobId).path(\"jobattempts\").accept(MediaType.APPLICATION_XML).get(ClientResponse.class);\r\n        assertEquals(MediaType.APPLICATION_XML_TYPE + \"; \" + JettyUtils.UTF_8, response.getType().toString());\r\n        String xml = response.getEntity(String.class);\r\n        DocumentBuilderFactory dbf = DocumentBuilderFactory.newInstance();\r\n        DocumentBuilder db = dbf.newDocumentBuilder();\r\n        InputSource is = new InputSource();\r\n        is.setCharacterStream(new StringReader(xml));\r\n        Document dom = db.parse(is);\r\n        NodeList attempts = dom.getElementsByTagName(\"jobAttempts\");\r\n        assertEquals(\"incorrect number of elements\", 1, attempts.getLength());\r\n        NodeList info = dom.getElementsByTagName(\"jobAttempt\");\r\n        verifyJobAttemptsXML(info, jobsMap.get(id));\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\webapp",
  "methodName" : "verifyJobAttempts",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void verifyJobAttempts(JSONObject info, Job job) throws JSONException\n{\r\n    JSONArray attempts = info.getJSONArray(\"jobAttempt\");\r\n    assertEquals(\"incorrect number of elements\", 2, attempts.length());\r\n    for (int i = 0; i < attempts.length(); i++) {\r\n        JSONObject attempt = attempts.getJSONObject(i);\r\n        verifyJobAttemptsGeneric(job, attempt.getString(\"nodeHttpAddress\"), attempt.getString(\"nodeId\"), attempt.getInt(\"id\"), attempt.getLong(\"startTime\"), attempt.getString(\"containerId\"), attempt.getString(\"logsLink\"));\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\webapp",
  "methodName" : "verifyJobAttemptsXML",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void verifyJobAttemptsXML(NodeList nodes, Job job)\n{\r\n    assertEquals(\"incorrect number of elements\", 2, nodes.getLength());\r\n    for (int i = 0; i < nodes.getLength(); i++) {\r\n        Element element = (Element) nodes.item(i);\r\n        verifyJobAttemptsGeneric(job, WebServicesTestUtils.getXmlString(element, \"nodeHttpAddress\"), WebServicesTestUtils.getXmlString(element, \"nodeId\"), WebServicesTestUtils.getXmlInt(element, \"id\"), WebServicesTestUtils.getXmlLong(element, \"startTime\"), WebServicesTestUtils.getXmlString(element, \"containerId\"), WebServicesTestUtils.getXmlString(element, \"logsLink\"));\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\webapp",
  "methodName" : "verifyJobAttemptsGeneric",
  "errType" : null,
  "containingMethodsNum" : 12,
  "sourceCodeText" : "void verifyJobAttemptsGeneric(Job job, String nodeHttpAddress, String nodeId, int id, long startTime, String containerId, String logsLink)\n{\r\n    boolean attemptFound = false;\r\n    for (AMInfo amInfo : job.getAMInfos()) {\r\n        if (amInfo.getAppAttemptId().getAttemptId() == id) {\r\n            attemptFound = true;\r\n            String nmHost = amInfo.getNodeManagerHost();\r\n            int nmHttpPort = amInfo.getNodeManagerHttpPort();\r\n            int nmPort = amInfo.getNodeManagerPort();\r\n            WebServicesTestUtils.checkStringMatch(\"nodeHttpAddress\", nmHost + \":\" + nmHttpPort, nodeHttpAddress);\r\n            WebServicesTestUtils.checkStringMatch(\"nodeId\", NodeId.newInstance(nmHost, nmPort).toString(), nodeId);\r\n            assertTrue(\"startime not greater than 0\", startTime > 0);\r\n            WebServicesTestUtils.checkStringMatch(\"containerId\", amInfo.getContainerId().toString(), containerId);\r\n            String localLogsLink = ujoin(\"node\", \"containerlogs\", containerId, job.getUserName());\r\n            assertTrue(\"logsLink\", logsLink.contains(localLogsLink));\r\n        }\r\n    }\r\n    assertTrue(\"attempt: \" + id + \" was not found\", attemptFound);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\webapp",
  "methodName" : "testConfigurationBlock",
  "errType" : null,
  "containingMethodsNum" : 18,
  "sourceCodeText" : "void testConfigurationBlock() throws Exception\n{\r\n    AppContext ctx = mock(AppContext.class);\r\n    Job job = mock(Job.class);\r\n    Path path = new Path(\"conf\");\r\n    Configuration configuration = new Configuration();\r\n    configuration.set(\"Key for test\", \"Value for test\");\r\n    final String redactedProp = \"Key for redaction\";\r\n    configuration.set(MRJobConfig.MR_JOB_REDACTED_PROPERTIES, redactedProp);\r\n    when(job.getConfFile()).thenReturn(path);\r\n    when(job.loadConfFile()).thenReturn(configuration);\r\n    when(ctx.getJob(any(JobId.class))).thenReturn(job);\r\n    ConfBlockForTest configurationBlock = new ConfBlockForTest(ctx);\r\n    PrintWriter pWriter = new PrintWriter(data);\r\n    Block html = new BlockForTest(new HtmlBlockForTest(), pWriter, 0, false);\r\n    configurationBlock.render(html);\r\n    pWriter.flush();\r\n    assertTrue(data.toString().contains(\"Sorry, can't do anything without a JobID\"));\r\n    configurationBlock.addParameter(AMParams.JOB_ID, \"job_01_01\");\r\n    data.reset();\r\n    configurationBlock.render(html);\r\n    pWriter.flush();\r\n    assertTrue(data.toString().contains(\"Key for test\"));\r\n    assertTrue(data.toString().contains(\"Value for test\"));\r\n    assertTrue(data.toString().contains(redactedProp));\r\n    assertTrue(data.toString().contains(MRJobConfUtil.REDACTION_REPLACEMENT_VAL));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\webapp",
  "methodName" : "testTasksBlock",
  "errType" : null,
  "containingMethodsNum" : 31,
  "sourceCodeText" : "void testTasksBlock() throws Exception\n{\r\n    ApplicationId appId = ApplicationIdPBImpl.newInstance(0, 1);\r\n    JobId jobId = new JobIdPBImpl();\r\n    jobId.setId(0);\r\n    jobId.setAppId(appId);\r\n    TaskId taskId = new TaskIdPBImpl();\r\n    taskId.setId(0);\r\n    taskId.setTaskType(TaskType.MAP);\r\n    taskId.setJobId(jobId);\r\n    Task task = mock(Task.class);\r\n    when(task.getID()).thenReturn(taskId);\r\n    TaskReport report = mock(TaskReport.class);\r\n    when(report.getProgress()).thenReturn(0.7f);\r\n    when(report.getTaskState()).thenReturn(TaskState.SUCCEEDED);\r\n    when(report.getStartTime()).thenReturn(100001L);\r\n    when(report.getFinishTime()).thenReturn(100011L);\r\n    when(report.getStatus()).thenReturn(\"Dummy Status \\n*\");\r\n    when(task.getReport()).thenReturn(report);\r\n    when(task.getType()).thenReturn(TaskType.MAP);\r\n    Map<TaskId, Task> tasks = new HashMap<TaskId, Task>();\r\n    tasks.put(taskId, task);\r\n    AppContext ctx = mock(AppContext.class);\r\n    Job job = mock(Job.class);\r\n    when(job.getTasks()).thenReturn(tasks);\r\n    App app = new App(ctx);\r\n    app.setJob(job);\r\n    TasksBlockForTest taskBlock = new TasksBlockForTest(app);\r\n    taskBlock.addParameter(AMParams.TASK_TYPE, \"m\");\r\n    PrintWriter pWriter = new PrintWriter(data);\r\n    Block html = new BlockForTest(new HtmlBlockForTest(), pWriter, 0, false);\r\n    taskBlock.render(html);\r\n    pWriter.flush();\r\n    assertTrue(data.toString().contains(\"task_0_0001_m_000000\"));\r\n    assertTrue(data.toString().contains(\"70.00\"));\r\n    assertTrue(data.toString().contains(\"SUCCEEDED\"));\r\n    assertTrue(data.toString().contains(\"100001\"));\r\n    assertTrue(data.toString().contains(\"100011\"));\r\n    assertFalse(data.toString().contains(\"Dummy Status \\n*\"));\r\n    assertTrue(data.toString().contains(\"Dummy Status \\\\n*\"));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\webapp",
  "methodName" : "testAttemptsBlock",
  "errType" : null,
  "containingMethodsNum" : 31,
  "sourceCodeText" : "void testAttemptsBlock()\n{\r\n    AppContext ctx = mock(AppContext.class);\r\n    AppForTest app = new AppForTest(ctx);\r\n    JobId jobId = new JobIdPBImpl();\r\n    jobId.setId(0);\r\n    jobId.setAppId(ApplicationIdPBImpl.newInstance(0, 1));\r\n    TaskId taskId = new TaskIdPBImpl();\r\n    taskId.setId(0);\r\n    taskId.setTaskType(TaskType.REDUCE);\r\n    taskId.setJobId(jobId);\r\n    Task task = mock(Task.class);\r\n    when(task.getID()).thenReturn(taskId);\r\n    TaskReport report = mock(TaskReport.class);\r\n    when(task.getReport()).thenReturn(report);\r\n    when(task.getType()).thenReturn(TaskType.REDUCE);\r\n    Map<TaskId, Task> tasks = new HashMap<TaskId, Task>();\r\n    Map<TaskAttemptId, TaskAttempt> attempts = new HashMap<TaskAttemptId, TaskAttempt>();\r\n    TaskAttempt attempt = mock(TaskAttempt.class);\r\n    TaskAttemptId taId = new TaskAttemptIdPBImpl();\r\n    taId.setId(0);\r\n    taId.setTaskId(task.getID());\r\n    when(attempt.getID()).thenReturn(taId);\r\n    final TaskAttemptState taState = TaskAttemptState.SUCCEEDED;\r\n    when(attempt.getState()).thenReturn(taState);\r\n    TaskAttemptReport taReport = mock(TaskAttemptReport.class);\r\n    when(taReport.getTaskAttemptState()).thenReturn(taState);\r\n    when(attempt.getReport()).thenReturn(taReport);\r\n    attempts.put(taId, attempt);\r\n    tasks.put(taskId, task);\r\n    when(task.getAttempts()).thenReturn(attempts);\r\n    app.setTask(task);\r\n    Job job = mock(Job.class);\r\n    when(job.getTasks(TaskType.REDUCE)).thenReturn(tasks);\r\n    app.setJob(job);\r\n    AttemptsBlockForTest block = new AttemptsBlockForTest(app, new Configuration());\r\n    block.addParameter(AMParams.TASK_TYPE, \"r\");\r\n    block.addParameter(AMParams.ATTEMPT_STATE, \"SUCCESSFUL\");\r\n    PrintWriter pWriter = new PrintWriter(data);\r\n    Block html = new BlockForTest(new HtmlBlockForTest(), pWriter, 0, false);\r\n    block.render(html);\r\n    pWriter.flush();\r\n    assertTrue(data.toString().contains(\"<a href='\" + block.url(\"task\", task.getID().toString()) + \"'>\" + \"attempt_0_0001_r_000000_0</a>\"));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\webapp",
  "methodName" : "testSingleCounterBlock",
  "errType" : null,
  "containingMethodsNum" : 37,
  "sourceCodeText" : "void testSingleCounterBlock()\n{\r\n    AppContext appCtx = mock(AppContext.class);\r\n    View.ViewContext ctx = mock(View.ViewContext.class);\r\n    JobId jobId = new JobIdPBImpl();\r\n    jobId.setId(0);\r\n    jobId.setAppId(ApplicationIdPBImpl.newInstance(0, 1));\r\n    TaskId mapTaskId = new TaskIdPBImpl();\r\n    mapTaskId.setId(0);\r\n    mapTaskId.setTaskType(TaskType.MAP);\r\n    mapTaskId.setJobId(jobId);\r\n    Task mapTask = mock(Task.class);\r\n    when(mapTask.getID()).thenReturn(mapTaskId);\r\n    TaskReport mapReport = mock(TaskReport.class);\r\n    when(mapTask.getReport()).thenReturn(mapReport);\r\n    when(mapTask.getType()).thenReturn(TaskType.MAP);\r\n    TaskId reduceTaskId = new TaskIdPBImpl();\r\n    reduceTaskId.setId(0);\r\n    reduceTaskId.setTaskType(TaskType.REDUCE);\r\n    reduceTaskId.setJobId(jobId);\r\n    Task reduceTask = mock(Task.class);\r\n    when(reduceTask.getID()).thenReturn(reduceTaskId);\r\n    TaskReport reduceReport = mock(TaskReport.class);\r\n    when(reduceTask.getReport()).thenReturn(reduceReport);\r\n    when(reduceTask.getType()).thenReturn(TaskType.REDUCE);\r\n    Map<TaskId, Task> tasks = new HashMap<TaskId, Task>();\r\n    tasks.put(mapTaskId, mapTask);\r\n    tasks.put(reduceTaskId, reduceTask);\r\n    Job job = mock(Job.class);\r\n    when(job.getTasks()).thenReturn(tasks);\r\n    when(appCtx.getJob(any(JobId.class))).thenReturn(job);\r\n    SingleCounterBlockForMapTest blockForMapTest = spy(new SingleCounterBlockForMapTest(appCtx, ctx));\r\n    PrintWriter pWriterForMapTest = new PrintWriter(data);\r\n    Block htmlForMapTest = new BlockForTest(new HtmlBlockForTest(), pWriterForMapTest, 0, false);\r\n    blockForMapTest.render(htmlForMapTest);\r\n    pWriterForMapTest.flush();\r\n    assertTrue(data.toString().contains(\"task_0_0001_m_000000\"));\r\n    assertFalse(data.toString().contains(\"task_0_0001_r_000000\"));\r\n    data.reset();\r\n    SingleCounterBlockForReduceTest blockForReduceTest = spy(new SingleCounterBlockForReduceTest(appCtx, ctx));\r\n    PrintWriter pWriterForReduceTest = new PrintWriter(data);\r\n    Block htmlForReduceTest = new BlockForTest(new HtmlBlockForTest(), pWriterForReduceTest, 0, false);\r\n    blockForReduceTest.render(htmlForReduceTest);\r\n    pWriterForReduceTest.flush();\r\n    System.out.println(data.toString());\r\n    assertFalse(data.toString().contains(\"task_0_0001_m_000000\"));\r\n    assertTrue(data.toString().contains(\"task_0_0001_r_000000\"));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "setUpClass",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setUpClass() throws Exception\n{\r\n    coreSitePath = \".\" + File.separator + \"target\" + File.separator + \"test-classes\" + File.separator + \"core-site.xml\";\r\n    Configuration conf = new HdfsConfiguration();\r\n    dfsCluster = new MiniDFSCluster.Builder(conf).build();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "cleanUpClass",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void cleanUpClass() throws Exception\n{\r\n    dfsCluster.shutdown();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "cleanTest",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void cleanTest() throws Exception\n{\r\n    new File(coreSitePath).delete();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "testFirstFlushOnCompletionEvent",
  "errType" : null,
  "containingMethodsNum" : 18,
  "sourceCodeText" : "void testFirstFlushOnCompletionEvent() throws Exception\n{\r\n    TestParams t = new TestParams();\r\n    Configuration conf = new Configuration();\r\n    conf.set(MRJobConfig.MR_AM_STAGING_DIR, t.workDir);\r\n    conf.setLong(MRJobConfig.MR_AM_HISTORY_COMPLETE_EVENT_FLUSH_TIMEOUT_MS, 60 * 1000l);\r\n    conf.setInt(MRJobConfig.MR_AM_HISTORY_JOB_COMPLETE_UNFLUSHED_MULTIPLIER, 10);\r\n    conf.setInt(MRJobConfig.MR_AM_HISTORY_MAX_UNFLUSHED_COMPLETE_EVENTS, 10);\r\n    conf.setInt(MRJobConfig.MR_AM_HISTORY_USE_BATCHED_FLUSH_QUEUE_SIZE_THRESHOLD, 200);\r\n    JHEvenHandlerForTest realJheh = new JHEvenHandlerForTest(t.mockAppContext, 0);\r\n    JHEvenHandlerForTest jheh = spy(realJheh);\r\n    jheh.init(conf);\r\n    EventWriter mockWriter = null;\r\n    try {\r\n        jheh.start();\r\n        handleEvent(jheh, new JobHistoryEvent(t.jobId, new AMStartedEvent(t.appAttemptId, 200, t.containerId, \"nmhost\", 3000, 4000, -1)));\r\n        mockWriter = jheh.getEventWriter();\r\n        verify(mockWriter).write(any(HistoryEvent.class));\r\n        for (int i = 0; i < 100; i++) {\r\n            queueEvent(jheh, new JobHistoryEvent(t.jobId, new TaskStartedEvent(t.taskID, 0, TaskType.MAP, \"\")));\r\n        }\r\n        handleNextNEvents(jheh, 100);\r\n        verify(mockWriter, times(0)).flush();\r\n        handleEvent(jheh, new JobHistoryEvent(t.jobId, new TaskFinishedEvent(t.taskID, t.taskAttemptID, 0, TaskType.MAP, \"\", null, 0)));\r\n        verify(mockWriter).flush();\r\n    } finally {\r\n        jheh.stop();\r\n        verify(mockWriter).close();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "testMaxUnflushedCompletionEvents",
  "errType" : null,
  "containingMethodsNum" : 20,
  "sourceCodeText" : "void testMaxUnflushedCompletionEvents() throws Exception\n{\r\n    TestParams t = new TestParams();\r\n    Configuration conf = new Configuration();\r\n    conf.set(MRJobConfig.MR_AM_STAGING_DIR, t.workDir);\r\n    conf.setLong(MRJobConfig.MR_AM_HISTORY_COMPLETE_EVENT_FLUSH_TIMEOUT_MS, 60 * 1000l);\r\n    conf.setInt(MRJobConfig.MR_AM_HISTORY_JOB_COMPLETE_UNFLUSHED_MULTIPLIER, 10);\r\n    conf.setInt(MRJobConfig.MR_AM_HISTORY_MAX_UNFLUSHED_COMPLETE_EVENTS, 10);\r\n    conf.setInt(MRJobConfig.MR_AM_HISTORY_USE_BATCHED_FLUSH_QUEUE_SIZE_THRESHOLD, 5);\r\n    JHEvenHandlerForTest realJheh = new JHEvenHandlerForTest(t.mockAppContext, 0);\r\n    JHEvenHandlerForTest jheh = spy(realJheh);\r\n    jheh.init(conf);\r\n    EventWriter mockWriter = null;\r\n    try {\r\n        jheh.start();\r\n        handleEvent(jheh, new JobHistoryEvent(t.jobId, new AMStartedEvent(t.appAttemptId, 200, t.containerId, \"nmhost\", 3000, 4000, -1)));\r\n        mockWriter = jheh.getEventWriter();\r\n        verify(mockWriter).write(any(HistoryEvent.class));\r\n        for (int i = 0; i < 100; i++) {\r\n            queueEvent(jheh, new JobHistoryEvent(t.jobId, new TaskFinishedEvent(t.taskID, t.taskAttemptID, 0, TaskType.MAP, \"\", null, 0)));\r\n        }\r\n        handleNextNEvents(jheh, 9);\r\n        verify(mockWriter, times(0)).flush();\r\n        handleNextNEvents(jheh, 1);\r\n        verify(mockWriter).flush();\r\n        handleNextNEvents(jheh, 50);\r\n        verify(mockWriter, times(6)).flush();\r\n    } finally {\r\n        jheh.stop();\r\n        verify(mockWriter).close();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "testUnflushedTimer",
  "errType" : null,
  "containingMethodsNum" : 20,
  "sourceCodeText" : "void testUnflushedTimer() throws Exception\n{\r\n    TestParams t = new TestParams();\r\n    Configuration conf = new Configuration();\r\n    conf.set(MRJobConfig.MR_AM_STAGING_DIR, t.workDir);\r\n    conf.setLong(MRJobConfig.MR_AM_HISTORY_COMPLETE_EVENT_FLUSH_TIMEOUT_MS, 2 * 1000l);\r\n    conf.setInt(MRJobConfig.MR_AM_HISTORY_JOB_COMPLETE_UNFLUSHED_MULTIPLIER, 10);\r\n    conf.setInt(MRJobConfig.MR_AM_HISTORY_MAX_UNFLUSHED_COMPLETE_EVENTS, 100);\r\n    conf.setInt(MRJobConfig.MR_AM_HISTORY_USE_BATCHED_FLUSH_QUEUE_SIZE_THRESHOLD, 5);\r\n    JHEvenHandlerForTest realJheh = new JHEvenHandlerForTest(t.mockAppContext, 0);\r\n    JHEvenHandlerForTest jheh = spy(realJheh);\r\n    jheh.init(conf);\r\n    EventWriter mockWriter = null;\r\n    try {\r\n        jheh.start();\r\n        handleEvent(jheh, new JobHistoryEvent(t.jobId, new AMStartedEvent(t.appAttemptId, 200, t.containerId, \"nmhost\", 3000, 4000, -1)));\r\n        mockWriter = jheh.getEventWriter();\r\n        verify(mockWriter).write(any(HistoryEvent.class));\r\n        for (int i = 0; i < 100; i++) {\r\n            queueEvent(jheh, new JobHistoryEvent(t.jobId, new TaskFinishedEvent(t.taskID, t.taskAttemptID, 0, TaskType.MAP, \"\", null, 0)));\r\n        }\r\n        handleNextNEvents(jheh, 9);\r\n        Assert.assertTrue(jheh.getFlushTimerStatus());\r\n        verify(mockWriter, times(0)).flush();\r\n        Thread.sleep(2 * 4 * 1000l);\r\n        verify(mockWriter).flush();\r\n        Assert.assertFalse(jheh.getFlushTimerStatus());\r\n    } finally {\r\n        jheh.stop();\r\n        verify(mockWriter).close();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "testBatchedFlushJobEndMultiplier",
  "errType" : null,
  "containingMethodsNum" : 19,
  "sourceCodeText" : "void testBatchedFlushJobEndMultiplier() throws Exception\n{\r\n    TestParams t = new TestParams();\r\n    Configuration conf = new Configuration();\r\n    conf.set(MRJobConfig.MR_AM_STAGING_DIR, t.workDir);\r\n    conf.setLong(MRJobConfig.MR_AM_HISTORY_COMPLETE_EVENT_FLUSH_TIMEOUT_MS, 60 * 1000l);\r\n    conf.setInt(MRJobConfig.MR_AM_HISTORY_JOB_COMPLETE_UNFLUSHED_MULTIPLIER, 3);\r\n    conf.setInt(MRJobConfig.MR_AM_HISTORY_MAX_UNFLUSHED_COMPLETE_EVENTS, 10);\r\n    conf.setInt(MRJobConfig.MR_AM_HISTORY_USE_BATCHED_FLUSH_QUEUE_SIZE_THRESHOLD, 0);\r\n    JHEvenHandlerForTest realJheh = new JHEvenHandlerForTest(t.mockAppContext, 0);\r\n    JHEvenHandlerForTest jheh = spy(realJheh);\r\n    jheh.init(conf);\r\n    EventWriter mockWriter = null;\r\n    try {\r\n        jheh.start();\r\n        handleEvent(jheh, new JobHistoryEvent(t.jobId, new AMStartedEvent(t.appAttemptId, 200, t.containerId, \"nmhost\", 3000, 4000, -1)));\r\n        mockWriter = jheh.getEventWriter();\r\n        verify(mockWriter).write(any(HistoryEvent.class));\r\n        for (int i = 0; i < 100; i++) {\r\n            queueEvent(jheh, new JobHistoryEvent(t.jobId, new TaskFinishedEvent(t.taskID, t.taskAttemptID, 0, TaskType.MAP, \"\", null, 0)));\r\n        }\r\n        queueEvent(jheh, new JobHistoryEvent(t.jobId, new JobFinishedEvent(TypeConverter.fromYarn(t.jobId), 0, 10, 10, 0, 0, 0, 0, null, null, new Counters())));\r\n        handleNextNEvents(jheh, 29);\r\n        verify(mockWriter, times(0)).flush();\r\n        handleNextNEvents(jheh, 72);\r\n        verify(mockWriter, times(4)).flush();\r\n    } finally {\r\n        jheh.stop();\r\n        verify(mockWriter).close();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "testProcessDoneFilesOnLastAMRetry",
  "errType" : null,
  "containingMethodsNum" : 17,
  "sourceCodeText" : "void testProcessDoneFilesOnLastAMRetry() throws Exception\n{\r\n    TestParams t = new TestParams(true);\r\n    Configuration conf = new Configuration();\r\n    JHEvenHandlerForTest realJheh = new JHEvenHandlerForTest(t.mockAppContext, 0);\r\n    JHEvenHandlerForTest jheh = spy(realJheh);\r\n    jheh.init(conf);\r\n    EventWriter mockWriter = null;\r\n    try {\r\n        jheh.start();\r\n        handleEvent(jheh, new JobHistoryEvent(t.jobId, new AMStartedEvent(t.appAttemptId, 200, t.containerId, \"nmhost\", 3000, 4000, -1)));\r\n        verify(jheh, times(0)).processDoneFiles(any(JobId.class));\r\n        handleEvent(jheh, new JobHistoryEvent(t.jobId, new JobUnsuccessfulCompletionEvent(TypeConverter.fromYarn(t.jobId), 0, 0, 0, 0, 0, 0, 0, JobStateInternal.ERROR.toString())));\r\n        verify(jheh, times(1)).processDoneFiles(any(JobId.class));\r\n        handleEvent(jheh, new JobHistoryEvent(t.jobId, new JobFinishedEvent(TypeConverter.fromYarn(t.jobId), 0, 0, 0, 0, 0, 0, 0, new Counters(), new Counters(), new Counters())));\r\n        verify(jheh, times(2)).processDoneFiles(any(JobId.class));\r\n        handleEvent(jheh, new JobHistoryEvent(t.jobId, new JobUnsuccessfulCompletionEvent(TypeConverter.fromYarn(t.jobId), 0, 0, 0, 0, 0, 0, 0, JobStateInternal.FAILED.toString())));\r\n        verify(jheh, times(3)).processDoneFiles(any(JobId.class));\r\n        handleEvent(jheh, new JobHistoryEvent(t.jobId, new JobUnsuccessfulCompletionEvent(TypeConverter.fromYarn(t.jobId), 0, 0, 0, 0, 0, 0, 0, JobStateInternal.KILLED.toString())));\r\n        verify(jheh, times(4)).processDoneFiles(any(JobId.class));\r\n        mockWriter = jheh.getEventWriter();\r\n        verify(mockWriter, times(5)).write(any(HistoryEvent.class));\r\n    } finally {\r\n        jheh.stop();\r\n        verify(mockWriter).close();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "testProcessDoneFilesNotLastAMRetry",
  "errType" : null,
  "containingMethodsNum" : 17,
  "sourceCodeText" : "void testProcessDoneFilesNotLastAMRetry() throws Exception\n{\r\n    TestParams t = new TestParams(false);\r\n    Configuration conf = new Configuration();\r\n    JHEvenHandlerForTest realJheh = new JHEvenHandlerForTest(t.mockAppContext, 0);\r\n    JHEvenHandlerForTest jheh = spy(realJheh);\r\n    jheh.init(conf);\r\n    EventWriter mockWriter = null;\r\n    try {\r\n        jheh.start();\r\n        handleEvent(jheh, new JobHistoryEvent(t.jobId, new AMStartedEvent(t.appAttemptId, 200, t.containerId, \"nmhost\", 3000, 4000, -1)));\r\n        verify(jheh, times(0)).processDoneFiles(t.jobId);\r\n        handleEvent(jheh, new JobHistoryEvent(t.jobId, new JobUnsuccessfulCompletionEvent(TypeConverter.fromYarn(t.jobId), 0, 0, 0, 0, 0, 0, 0, JobStateInternal.ERROR.toString())));\r\n        verify(jheh, times(0)).processDoneFiles(t.jobId);\r\n        handleEvent(jheh, new JobHistoryEvent(t.jobId, new JobFinishedEvent(TypeConverter.fromYarn(t.jobId), 0, 0, 0, 0, 0, 0, 0, new Counters(), new Counters(), new Counters())));\r\n        verify(jheh, times(1)).processDoneFiles(t.jobId);\r\n        handleEvent(jheh, new JobHistoryEvent(t.jobId, new JobUnsuccessfulCompletionEvent(TypeConverter.fromYarn(t.jobId), 0, 0, 0, 0, 0, 0, 0, JobStateInternal.FAILED.toString())));\r\n        verify(jheh, times(2)).processDoneFiles(t.jobId);\r\n        handleEvent(jheh, new JobHistoryEvent(t.jobId, new JobUnsuccessfulCompletionEvent(TypeConverter.fromYarn(t.jobId), 0, 0, 0, 0, 0, 0, 0, JobStateInternal.KILLED.toString())));\r\n        verify(jheh, times(3)).processDoneFiles(t.jobId);\r\n        mockWriter = jheh.getEventWriter();\r\n        verify(mockWriter, times(5)).write(any(HistoryEvent.class));\r\n    } finally {\r\n        jheh.stop();\r\n        verify(mockWriter).close();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "testPropertyRedactionForJHS",
  "errType" : null,
  "containingMethodsNum" : 16,
  "sourceCodeText" : "void testPropertyRedactionForJHS() throws Exception\n{\r\n    final Configuration conf = new Configuration();\r\n    String sensitivePropertyName = \"aws.fake.credentials.name\";\r\n    String sensitivePropertyValue = \"aws.fake.credentials.val\";\r\n    conf.set(sensitivePropertyName, sensitivePropertyValue);\r\n    conf.set(MRJobConfig.MR_JOB_REDACTED_PROPERTIES, sensitivePropertyName);\r\n    conf.set(CommonConfigurationKeysPublic.FS_DEFAULT_NAME_KEY, dfsCluster.getURI().toString());\r\n    final TestParams params = new TestParams();\r\n    conf.set(MRJobConfig.MR_AM_STAGING_DIR, params.dfsWorkDir);\r\n    final JHEvenHandlerForTest jheh = new JHEvenHandlerForTest(params.mockAppContext, 0, false);\r\n    try {\r\n        jheh.init(conf);\r\n        jheh.start();\r\n        handleEvent(jheh, new JobHistoryEvent(params.jobId, new AMStartedEvent(params.appAttemptId, 200, params.containerId, \"nmhost\", 3000, 4000, -1)));\r\n        handleEvent(jheh, new JobHistoryEvent(params.jobId, new JobUnsuccessfulCompletionEvent(TypeConverter.fromYarn(params.jobId), 0, 0, 0, 0, 0, 0, 0, JobStateInternal.FAILED.toString())));\r\n        assertThat(conf.get(sensitivePropertyName)).isEqualTo(sensitivePropertyValue).withFailMessage(sensitivePropertyName + \" is modified.\");\r\n        Path jhsJobConfFile = getJobConfInIntermediateDoneDir(conf, params.jobId);\r\n        Assert.assertTrue(\"The job_conf.xml file is not in the JHS directory\", FileContext.getFileContext(conf).util().exists(jhsJobConfFile));\r\n        Configuration jhsJobConf = new Configuration();\r\n        try (InputStream input = FileSystem.get(conf).open(jhsJobConfFile)) {\r\n            jhsJobConf.addResource(input);\r\n            Assert.assertEquals(sensitivePropertyName + \" is not redacted in HDFS.\", MRJobConfUtil.REDACTION_REPLACEMENT_VAL, jhsJobConf.get(sensitivePropertyName));\r\n        }\r\n    } finally {\r\n        jheh.stop();\r\n        purgeHdfsHistoryIntermediateDoneDirectory(conf);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 2,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "getJobConfInIntermediateDoneDir",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "Path getJobConfInIntermediateDoneDir(Configuration conf, JobId jobId) throws IOException\n{\r\n    Path userDoneDir = new Path(JobHistoryUtils.getHistoryIntermediateDoneDirForUser(conf));\r\n    Path doneDirPrefix = FileContext.getFileContext(conf).makeQualified(userDoneDir);\r\n    return new Path(doneDirPrefix, JobHistoryUtils.getIntermediateConfFileName(jobId));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "purgeHdfsHistoryIntermediateDoneDirectory",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void purgeHdfsHistoryIntermediateDoneDirectory(Configuration conf) throws IOException\n{\r\n    FileSystem fs = FileSystem.get(dfsCluster.getConfiguration(0));\r\n    String intermDoneDirPrefix = JobHistoryUtils.getConfiguredHistoryIntermediateDoneDirPrefix(conf);\r\n    fs.delete(new Path(intermDoneDirPrefix), true);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "testDefaultFsIsUsedForHistory",
  "errType" : null,
  "containingMethodsNum" : 16,
  "sourceCodeText" : "void testDefaultFsIsUsedForHistory() throws Exception\n{\r\n    Configuration conf = new Configuration();\r\n    conf.set(CommonConfigurationKeysPublic.FS_DEFAULT_NAME_KEY, dfsCluster.getURI().toString());\r\n    FileOutputStream os = new FileOutputStream(coreSitePath);\r\n    conf.writeXml(os);\r\n    os.close();\r\n    conf.set(CommonConfigurationKeysPublic.FS_DEFAULT_NAME_KEY, \"file:///\");\r\n    TestParams t = new TestParams();\r\n    conf.set(MRJobConfig.MR_AM_STAGING_DIR, t.dfsWorkDir);\r\n    JHEvenHandlerForTest realJheh = new JHEvenHandlerForTest(t.mockAppContext, 0, false);\r\n    JHEvenHandlerForTest jheh = spy(realJheh);\r\n    jheh.init(conf);\r\n    try {\r\n        jheh.start();\r\n        handleEvent(jheh, new JobHistoryEvent(t.jobId, new AMStartedEvent(t.appAttemptId, 200, t.containerId, \"nmhost\", 3000, 4000, -1)));\r\n        handleEvent(jheh, new JobHistoryEvent(t.jobId, new JobFinishedEvent(TypeConverter.fromYarn(t.jobId), 0, 0, 0, 0, 0, 0, 0, new Counters(), new Counters(), new Counters())));\r\n        FileSystem dfsFileSystem = dfsCluster.getFileSystem();\r\n        assertTrue(\"Minicluster contains some history files\", dfsFileSystem.globStatus(new Path(t.dfsWorkDir + \"/*\")).length != 0);\r\n        FileSystem localFileSystem = LocalFileSystem.get(conf);\r\n        assertFalse(\"No history directory on non-default file system\", localFileSystem.exists(new Path(t.dfsWorkDir)));\r\n    } finally {\r\n        jheh.stop();\r\n        purgeHdfsHistoryIntermediateDoneDirectory(conf);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "testGetHistoryIntermediateDoneDirForUser",
  "errType" : null,
  "containingMethodsNum" : 10,
  "sourceCodeText" : "void testGetHistoryIntermediateDoneDirForUser() throws IOException\n{\r\n    Configuration conf = new Configuration();\r\n    conf.set(JHAdminConfig.MR_HISTORY_INTERMEDIATE_DONE_DIR, \"/mapred/history/done_intermediate\");\r\n    conf.set(MRJobConfig.USER_NAME, System.getProperty(\"user.name\"));\r\n    String pathStr = JobHistoryUtils.getHistoryIntermediateDoneDirForUser(conf);\r\n    Assert.assertEquals(\"/mapred/history/done_intermediate/\" + System.getProperty(\"user.name\"), pathStr);\r\n    conf.set(CommonConfigurationKeysPublic.FS_DEFAULT_NAME_KEY, dfsCluster.getURI().toString());\r\n    FileOutputStream os = new FileOutputStream(coreSitePath);\r\n    conf.writeXml(os);\r\n    os.close();\r\n    conf.set(CommonConfigurationKeysPublic.FS_DEFAULT_NAME_KEY, \"file:///\");\r\n    pathStr = JobHistoryUtils.getHistoryIntermediateDoneDirForUser(conf);\r\n    Assert.assertEquals(dfsCluster.getURI().toString() + \"/mapred/history/done_intermediate/\" + System.getProperty(\"user.name\"), pathStr);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "testAMStartedEvent",
  "errType" : null,
  "containingMethodsNum" : 18,
  "sourceCodeText" : "void testAMStartedEvent() throws Exception\n{\r\n    TestParams t = new TestParams();\r\n    Configuration conf = new Configuration();\r\n    JHEvenHandlerForTest realJheh = new JHEvenHandlerForTest(t.mockAppContext, 0);\r\n    JHEvenHandlerForTest jheh = spy(realJheh);\r\n    jheh.init(conf);\r\n    EventWriter mockWriter = null;\r\n    try {\r\n        jheh.start();\r\n        handleEvent(jheh, new JobHistoryEvent(t.jobId, new AMStartedEvent(t.appAttemptId, 200, t.containerId, \"nmhost\", 3000, 4000, 100)));\r\n        JobHistoryEventHandler.MetaInfo mi = JobHistoryEventHandler.fileMap.get(t.jobId);\r\n        assertThat(mi.getJobIndexInfo().getSubmitTime()).isEqualTo(100);\r\n        assertThat(mi.getJobIndexInfo().getJobStartTime()).isEqualTo(200);\r\n        assertThat(mi.getJobSummary().getJobSubmitTime()).isEqualTo(100);\r\n        assertThat(mi.getJobSummary().getJobLaunchTime()).isEqualTo(200);\r\n        handleEvent(jheh, new JobHistoryEvent(t.jobId, new JobUnsuccessfulCompletionEvent(TypeConverter.fromYarn(t.jobId), 0, 0, 0, 0, 0, 0, 0, JobStateInternal.FAILED.toString())));\r\n        assertThat(mi.getJobIndexInfo().getSubmitTime()).isEqualTo(100);\r\n        assertThat(mi.getJobIndexInfo().getJobStartTime()).isEqualTo(200);\r\n        assertThat(mi.getJobSummary().getJobSubmitTime()).isEqualTo(100);\r\n        assertThat(mi.getJobSummary().getJobLaunchTime()).isEqualTo(200);\r\n        verify(jheh, times(1)).processDoneFiles(t.jobId);\r\n        mockWriter = jheh.getEventWriter();\r\n        verify(mockWriter, times(2)).write(any(HistoryEvent.class));\r\n    } finally {\r\n        jheh.stop();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "testTimelineEventHandling",
  "errType" : null,
  "containingMethodsNum" : 94,
  "sourceCodeText" : "void testTimelineEventHandling() throws Exception\n{\r\n    TestParams t = new TestParams(RunningAppContext.class, false);\r\n    Configuration conf = new YarnConfiguration();\r\n    conf.setBoolean(YarnConfiguration.TIMELINE_SERVICE_ENABLED, true);\r\n    long currentTime = System.currentTimeMillis();\r\n    try (MiniYARNCluster yarnCluster = new MiniYARNCluster(TestJobHistoryEventHandler.class.getSimpleName(), 1, 1, 1, 1)) {\r\n        yarnCluster.init(conf);\r\n        yarnCluster.start();\r\n        Configuration confJHEH = new YarnConfiguration(conf);\r\n        confJHEH.setBoolean(MRJobConfig.MAPREDUCE_JOB_EMIT_TIMELINE_DATA, true);\r\n        confJHEH.set(YarnConfiguration.TIMELINE_SERVICE_WEBAPP_ADDRESS, MiniYARNCluster.getHostname() + \":\" + yarnCluster.getApplicationHistoryServer().getPort());\r\n        JHEvenHandlerForTest jheh = new JHEvenHandlerForTest(t.mockAppContext, 0);\r\n        jheh.init(confJHEH);\r\n        jheh.start();\r\n        TimelineStore ts = yarnCluster.getApplicationHistoryServer().getTimelineStore();\r\n        handleEvent(jheh, new JobHistoryEvent(t.jobId, new AMStartedEvent(t.appAttemptId, 200, t.containerId, \"nmhost\", 3000, 4000, -1), currentTime - 10));\r\n        jheh.getDispatcher().await();\r\n        TimelineEntities entities = ts.getEntities(\"MAPREDUCE_JOB\", null, null, null, null, null, null, null, null, null);\r\n        Assert.assertEquals(1, entities.getEntities().size());\r\n        TimelineEntity tEntity = entities.getEntities().get(0);\r\n        Assert.assertEquals(t.jobId.toString(), tEntity.getEntityId());\r\n        Assert.assertEquals(1, tEntity.getEvents().size());\r\n        Assert.assertEquals(EventType.AM_STARTED.toString(), tEntity.getEvents().get(0).getEventType());\r\n        Assert.assertEquals(currentTime - 10, tEntity.getEvents().get(0).getTimestamp());\r\n        handleEvent(jheh, new JobHistoryEvent(t.jobId, new JobSubmittedEvent(TypeConverter.fromYarn(t.jobId), \"name\", \"user\", 200, \"/foo/job.xml\", new HashMap<JobACL, AccessControlList>(), \"default\"), currentTime + 10));\r\n        jheh.getDispatcher().await();\r\n        entities = ts.getEntities(\"MAPREDUCE_JOB\", null, null, null, null, null, null, null, null, null);\r\n        Assert.assertEquals(1, entities.getEntities().size());\r\n        tEntity = entities.getEntities().get(0);\r\n        Assert.assertEquals(t.jobId.toString(), tEntity.getEntityId());\r\n        Assert.assertEquals(2, tEntity.getEvents().size());\r\n        Assert.assertEquals(EventType.JOB_SUBMITTED.toString(), tEntity.getEvents().get(0).getEventType());\r\n        Assert.assertEquals(EventType.AM_STARTED.toString(), tEntity.getEvents().get(1).getEventType());\r\n        Assert.assertEquals(currentTime + 10, tEntity.getEvents().get(0).getTimestamp());\r\n        Assert.assertEquals(currentTime - 10, tEntity.getEvents().get(1).getTimestamp());\r\n        handleEvent(jheh, new JobHistoryEvent(t.jobId, new JobQueueChangeEvent(TypeConverter.fromYarn(t.jobId), \"q2\"), currentTime - 20));\r\n        jheh.getDispatcher().await();\r\n        entities = ts.getEntities(\"MAPREDUCE_JOB\", null, null, null, null, null, null, null, null, null);\r\n        Assert.assertEquals(1, entities.getEntities().size());\r\n        tEntity = entities.getEntities().get(0);\r\n        Assert.assertEquals(t.jobId.toString(), tEntity.getEntityId());\r\n        Assert.assertEquals(3, tEntity.getEvents().size());\r\n        Assert.assertEquals(EventType.JOB_SUBMITTED.toString(), tEntity.getEvents().get(0).getEventType());\r\n        Assert.assertEquals(EventType.AM_STARTED.toString(), tEntity.getEvents().get(1).getEventType());\r\n        Assert.assertEquals(EventType.JOB_QUEUE_CHANGED.toString(), tEntity.getEvents().get(2).getEventType());\r\n        Assert.assertEquals(currentTime + 10, tEntity.getEvents().get(0).getTimestamp());\r\n        Assert.assertEquals(currentTime - 10, tEntity.getEvents().get(1).getTimestamp());\r\n        Assert.assertEquals(currentTime - 20, tEntity.getEvents().get(2).getTimestamp());\r\n        handleEvent(jheh, new JobHistoryEvent(t.jobId, new JobFinishedEvent(TypeConverter.fromYarn(t.jobId), 0, 0, 0, 0, 0, 0, 0, new Counters(), new Counters(), new Counters()), currentTime));\r\n        jheh.getDispatcher().await();\r\n        entities = ts.getEntities(\"MAPREDUCE_JOB\", null, null, null, null, null, null, null, null, null);\r\n        Assert.assertEquals(1, entities.getEntities().size());\r\n        tEntity = entities.getEntities().get(0);\r\n        Assert.assertEquals(t.jobId.toString(), tEntity.getEntityId());\r\n        Assert.assertEquals(4, tEntity.getEvents().size());\r\n        Assert.assertEquals(EventType.JOB_SUBMITTED.toString(), tEntity.getEvents().get(0).getEventType());\r\n        Assert.assertEquals(EventType.JOB_FINISHED.toString(), tEntity.getEvents().get(1).getEventType());\r\n        Assert.assertEquals(EventType.AM_STARTED.toString(), tEntity.getEvents().get(2).getEventType());\r\n        Assert.assertEquals(EventType.JOB_QUEUE_CHANGED.toString(), tEntity.getEvents().get(3).getEventType());\r\n        Assert.assertEquals(currentTime + 10, tEntity.getEvents().get(0).getTimestamp());\r\n        Assert.assertEquals(currentTime, tEntity.getEvents().get(1).getTimestamp());\r\n        Assert.assertEquals(currentTime - 10, tEntity.getEvents().get(2).getTimestamp());\r\n        Assert.assertEquals(currentTime - 20, tEntity.getEvents().get(3).getTimestamp());\r\n        handleEvent(jheh, new JobHistoryEvent(t.jobId, new JobUnsuccessfulCompletionEvent(TypeConverter.fromYarn(t.jobId), 0, 0, 0, 0, 0, 0, 0, JobStateInternal.KILLED.toString()), currentTime + 20));\r\n        jheh.getDispatcher().await();\r\n        entities = ts.getEntities(\"MAPREDUCE_JOB\", null, null, null, null, null, null, null, null, null);\r\n        Assert.assertEquals(1, entities.getEntities().size());\r\n        tEntity = entities.getEntities().get(0);\r\n        Assert.assertEquals(t.jobId.toString(), tEntity.getEntityId());\r\n        Assert.assertEquals(5, tEntity.getEvents().size());\r\n        Assert.assertEquals(EventType.JOB_KILLED.toString(), tEntity.getEvents().get(0).getEventType());\r\n        Assert.assertEquals(EventType.JOB_SUBMITTED.toString(), tEntity.getEvents().get(1).getEventType());\r\n        Assert.assertEquals(EventType.JOB_FINISHED.toString(), tEntity.getEvents().get(2).getEventType());\r\n        Assert.assertEquals(EventType.AM_STARTED.toString(), tEntity.getEvents().get(3).getEventType());\r\n        Assert.assertEquals(EventType.JOB_QUEUE_CHANGED.toString(), tEntity.getEvents().get(4).getEventType());\r\n        Assert.assertEquals(currentTime + 20, tEntity.getEvents().get(0).getTimestamp());\r\n        Assert.assertEquals(currentTime + 10, tEntity.getEvents().get(1).getTimestamp());\r\n        Assert.assertEquals(currentTime, tEntity.getEvents().get(2).getTimestamp());\r\n        Assert.assertEquals(currentTime - 10, tEntity.getEvents().get(3).getTimestamp());\r\n        Assert.assertEquals(currentTime - 20, tEntity.getEvents().get(4).getTimestamp());\r\n        handleEvent(jheh, new JobHistoryEvent(t.jobId, new TaskStartedEvent(t.taskID, 0, TaskType.MAP, \"\")));\r\n        jheh.getDispatcher().await();\r\n        entities = ts.getEntities(\"MAPREDUCE_TASK\", null, null, null, null, null, null, null, null, null);\r\n        Assert.assertEquals(1, entities.getEntities().size());\r\n        tEntity = entities.getEntities().get(0);\r\n        Assert.assertEquals(t.taskID.toString(), tEntity.getEntityId());\r\n        Assert.assertEquals(1, tEntity.getEvents().size());\r\n        Assert.assertEquals(EventType.TASK_STARTED.toString(), tEntity.getEvents().get(0).getEventType());\r\n        Assert.assertEquals(TaskType.MAP.toString(), tEntity.getEvents().get(0).getEventInfo().get(\"TASK_TYPE\"));\r\n        handleEvent(jheh, new JobHistoryEvent(t.jobId, new TaskStartedEvent(t.taskID, 0, TaskType.REDUCE, \"\")));\r\n        jheh.getDispatcher().await();\r\n        entities = ts.getEntities(\"MAPREDUCE_TASK\", null, null, null, null, null, null, null, null, null);\r\n        Assert.assertEquals(1, entities.getEntities().size());\r\n        tEntity = entities.getEntities().get(0);\r\n        Assert.assertEquals(t.taskID.toString(), tEntity.getEntityId());\r\n        Assert.assertEquals(2, tEntity.getEvents().size());\r\n        Assert.assertEquals(EventType.TASK_STARTED.toString(), tEntity.getEvents().get(1).getEventType());\r\n        Assert.assertEquals(TaskType.REDUCE.toString(), tEntity.getEvents().get(0).getEventInfo().get(\"TASK_TYPE\"));\r\n        Assert.assertEquals(TaskType.MAP.toString(), tEntity.getEvents().get(1).getEventInfo().get(\"TASK_TYPE\"));\r\n    }\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "testCountersToJSON",
  "errType" : null,
  "containingMethodsNum" : 14,
  "sourceCodeText" : "void testCountersToJSON() throws Exception\n{\r\n    JobHistoryEventHandler jheh = new JobHistoryEventHandler(null, 0);\r\n    Counters counters = new Counters();\r\n    CounterGroup group1 = counters.addGroup(\"DOCTORS\", \"Incarnations of the Doctor\");\r\n    group1.addCounter(\"PETER_CAPALDI\", \"Peter Capaldi\", 12);\r\n    group1.addCounter(\"MATT_SMITH\", \"Matt Smith\", 11);\r\n    group1.addCounter(\"DAVID_TENNANT\", \"David Tennant\", 10);\r\n    CounterGroup group2 = counters.addGroup(\"COMPANIONS\", \"Companions of the Doctor\");\r\n    group2.addCounter(\"CLARA_OSWALD\", \"Clara Oswald\", 6);\r\n    group2.addCounter(\"RORY_WILLIAMS\", \"Rory Williams\", 5);\r\n    group2.addCounter(\"AMY_POND\", \"Amy Pond\", 4);\r\n    group2.addCounter(\"MARTHA_JONES\", \"Martha Jones\", 3);\r\n    group2.addCounter(\"DONNA_NOBLE\", \"Donna Noble\", 2);\r\n    group2.addCounter(\"ROSE_TYLER\", \"Rose Tyler\", 1);\r\n    JsonNode jsonNode = JobHistoryEventUtils.countersToJSON(counters);\r\n    String jsonStr = new ObjectMapper().writeValueAsString(jsonNode);\r\n    String expected = \"[{\\\"NAME\\\":\\\"COMPANIONS\\\",\\\"DISPLAY_NAME\\\":\\\"Companions \" + \"of the Doctor\\\",\\\"COUNTERS\\\":[{\\\"NAME\\\":\\\"AMY_POND\\\",\\\"DISPLAY_NAME\\\"\" + \":\\\"Amy Pond\\\",\\\"VALUE\\\":4},{\\\"NAME\\\":\\\"CLARA_OSWALD\\\",\" + \"\\\"DISPLAY_NAME\\\":\\\"Clara Oswald\\\",\\\"VALUE\\\":6},{\\\"NAME\\\":\" + \"\\\"DONNA_NOBLE\\\",\\\"DISPLAY_NAME\\\":\\\"Donna Noble\\\",\\\"VALUE\\\":2},\" + \"{\\\"NAME\\\":\\\"MARTHA_JONES\\\",\\\"DISPLAY_NAME\\\":\\\"Martha Jones\\\",\" + \"\\\"VALUE\\\":3},{\\\"NAME\\\":\\\"RORY_WILLIAMS\\\",\\\"DISPLAY_NAME\\\":\\\"Rory \" + \"Williams\\\",\\\"VALUE\\\":5},{\\\"NAME\\\":\\\"ROSE_TYLER\\\",\\\"DISPLAY_NAME\\\":\" + \"\\\"Rose Tyler\\\",\\\"VALUE\\\":1}]},{\\\"NAME\\\":\\\"DOCTORS\\\",\\\"DISPLAY_NAME\\\"\" + \":\\\"Incarnations of the Doctor\\\",\\\"COUNTERS\\\":[{\\\"NAME\\\":\" + \"\\\"DAVID_TENNANT\\\",\\\"DISPLAY_NAME\\\":\\\"David Tennant\\\",\\\"VALUE\\\":10},\" + \"{\\\"NAME\\\":\\\"MATT_SMITH\\\",\\\"DISPLAY_NAME\\\":\\\"Matt Smith\\\",\\\"VALUE\\\":\" + \"11},{\\\"NAME\\\":\\\"PETER_CAPALDI\\\",\\\"DISPLAY_NAME\\\":\\\"Peter Capaldi\\\",\" + \"\\\"VALUE\\\":12}]}]\";\r\n    Assert.assertEquals(expected, jsonStr);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "testCountersToJSONEmpty",
  "errType" : null,
  "containingMethodsNum" : 10,
  "sourceCodeText" : "void testCountersToJSONEmpty() throws Exception\n{\r\n    JobHistoryEventHandler jheh = new JobHistoryEventHandler(null, 0);\r\n    Counters counters = null;\r\n    JsonNode jsonNode = JobHistoryEventUtils.countersToJSON(counters);\r\n    String jsonStr = new ObjectMapper().writeValueAsString(jsonNode);\r\n    String expected = \"[]\";\r\n    Assert.assertEquals(expected, jsonStr);\r\n    counters = new Counters();\r\n    jsonNode = JobHistoryEventUtils.countersToJSON(counters);\r\n    jsonStr = new ObjectMapper().writeValueAsString(jsonNode);\r\n    expected = \"[]\";\r\n    Assert.assertEquals(expected, jsonStr);\r\n    counters.addGroup(\"DOCTORS\", \"Incarnations of the Doctor\");\r\n    jsonNode = JobHistoryEventUtils.countersToJSON(counters);\r\n    jsonStr = new ObjectMapper().writeValueAsString(jsonNode);\r\n    expected = \"[{\\\"NAME\\\":\\\"DOCTORS\\\",\\\"DISPLAY_NAME\\\":\\\"Incarnations of the \" + \"Doctor\\\",\\\"COUNTERS\\\":[]}]\";\r\n    Assert.assertEquals(expected, jsonStr);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "queueEvent",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void queueEvent(JHEvenHandlerForTest jheh, JobHistoryEvent event)\n{\r\n    jheh.handle(event);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "handleEvent",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void handleEvent(JHEvenHandlerForTest jheh, JobHistoryEvent event) throws InterruptedException\n{\r\n    jheh.handle(event);\r\n    jheh.handleEvent(jheh.eventQueue.take());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "handleNextNEvents",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void handleNextNEvents(JHEvenHandlerForTest jheh, int numEvents) throws InterruptedException\n{\r\n    for (int i = 0; i < numEvents; i++) {\r\n        jheh.handleEvent(jheh.eventQueue.take());\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "setupTestWorkDir",
  "errType" : [ "Exception" ],
  "containingMethodsNum" : 4,
  "sourceCodeText" : "String setupTestWorkDir()\n{\r\n    File testWorkDir = new File(\"target\", this.getClass().getCanonicalName());\r\n    try {\r\n        FileContext.getLocalFSFileContext().delete(new Path(testWorkDir.getAbsolutePath()), true);\r\n        return testWorkDir.getAbsolutePath();\r\n    } catch (Exception e) {\r\n        LOG.warn(\"Could not cleanup\", e);\r\n        throw new YarnRuntimeException(\"could not cleanup test dir\", e);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "mockJob",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "Job mockJob()\n{\r\n    Job mockJob = mock(Job.class);\r\n    when(mockJob.getAllCounters()).thenReturn(new Counters());\r\n    when(mockJob.getTotalMaps()).thenReturn(10);\r\n    when(mockJob.getTotalReduces()).thenReturn(10);\r\n    when(mockJob.getName()).thenReturn(\"mockjob\");\r\n    return mockJob;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "mockAppContext",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "AppContext mockAppContext(Class<? extends AppContext> contextClass, ApplicationId appId, boolean isLastAMRetry)\n{\r\n    JobId jobId = TypeConverter.toYarn(TypeConverter.fromYarn(appId));\r\n    AppContext mockContext = mock(contextClass);\r\n    Job mockJob = mockJob();\r\n    when(mockContext.getJob(jobId)).thenReturn(mockJob);\r\n    when(mockContext.getApplicationID()).thenReturn(appId);\r\n    when(mockContext.isLastAMRetry()).thenReturn(isLastAMRetry);\r\n    if (mockContext instanceof RunningAppContext) {\r\n        when(((RunningAppContext) mockContext).getTimelineClient()).thenReturn(TimelineClient.createTimelineClient());\r\n        when(((RunningAppContext) mockContext).getTimelineV2Client()).thenReturn(TimelineV2Client.createTimelineClient(ApplicationId.newInstance(0, 1)));\r\n    }\r\n    return mockContext;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "getEventToEnqueue",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "JobHistoryEvent getEventToEnqueue(JobId jobId)\n{\r\n    HistoryEvent toReturn = new JobStatusChangedEvent(new JobID(Integer.toString(jobId.getId()), jobId.getId()), \"change status\");\r\n    return new JobHistoryEvent(jobId, toReturn);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "testSigTermedFunctionality",
  "errType" : null,
  "containingMethodsNum" : 19,
  "sourceCodeText" : "void testSigTermedFunctionality() throws IOException\n{\r\n    AppContext mockedContext = Mockito.mock(AppContext.class);\r\n    JHEventHandlerForSigtermTest jheh = new JHEventHandlerForSigtermTest(mockedContext, 0);\r\n    JobId jobId = Mockito.mock(JobId.class);\r\n    jheh.addToFileMap(jobId);\r\n    final int numEvents = 4;\r\n    JobHistoryEvent[] events = new JobHistoryEvent[numEvents];\r\n    for (int i = 0; i < numEvents; ++i) {\r\n        events[i] = getEventToEnqueue(jobId);\r\n        jheh.handle(events[i]);\r\n    }\r\n    jheh.stop();\r\n    assertTrue(\"handleEvent should've been called only 4 times but was \" + jheh.eventsHandled, jheh.eventsHandled == 4);\r\n    jheh = new JHEventHandlerForSigtermTest(mockedContext, 0);\r\n    Job job = Mockito.mock(Job.class);\r\n    Mockito.when(mockedContext.getJob(jobId)).thenReturn(job);\r\n    ApplicationId mockAppId = Mockito.mock(ApplicationId.class);\r\n    Mockito.when(mockAppId.getClusterTimestamp()).thenReturn(1000l);\r\n    Mockito.when(jobId.getAppId()).thenReturn(mockAppId);\r\n    jheh.addToFileMap(jobId);\r\n    jheh.setForcejobCompletion(true);\r\n    for (int i = 0; i < numEvents; ++i) {\r\n        events[i] = getEventToEnqueue(jobId);\r\n        jheh.handle(events[i]);\r\n    }\r\n    jheh.stop();\r\n    assertTrue(\"handleEvent should've been called only 5 times but was \" + jheh.eventsHandled, jheh.eventsHandled == 5);\r\n    assertTrue(\"Last event handled wasn't JobUnsuccessfulCompletionEvent\", jheh.lastEventHandled.getHistoryEvent() instanceof JobUnsuccessfulCompletionEvent);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "testSetTrackingURLAfterHistoryIsWritten",
  "errType" : null,
  "containingMethodsNum" : 11,
  "sourceCodeText" : "void testSetTrackingURLAfterHistoryIsWritten() throws Exception\n{\r\n    TestParams t = new TestParams(true);\r\n    Configuration conf = new Configuration();\r\n    JHEvenHandlerForTest realJheh = new JHEvenHandlerForTest(t.mockAppContext, 0, false);\r\n    JHEvenHandlerForTest jheh = spy(realJheh);\r\n    jheh.init(conf);\r\n    try {\r\n        jheh.start();\r\n        handleEvent(jheh, new JobHistoryEvent(t.jobId, new AMStartedEvent(t.appAttemptId, 200, t.containerId, \"nmhost\", 3000, 4000, -1)));\r\n        verify(jheh, times(0)).processDoneFiles(any(JobId.class));\r\n        verify(t.mockAppContext, times(0)).setHistoryUrl(any(String.class));\r\n        handleEvent(jheh, new JobHistoryEvent(t.jobId, new JobFinishedEvent(TypeConverter.fromYarn(t.jobId), 0, 0, 0, 0, 0, 0, 0, new Counters(), new Counters(), new Counters())));\r\n        verify(jheh, times(1)).processDoneFiles(any(JobId.class));\r\n        String historyUrl = MRWebAppUtil.getApplicationWebURLOnJHSWithScheme(conf, t.mockAppContext.getApplicationID());\r\n        verify(t.mockAppContext, times(1)).setHistoryUrl(historyUrl);\r\n    } finally {\r\n        jheh.stop();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "testDontSetTrackingURLIfHistoryWriteFailed",
  "errType" : null,
  "containingMethodsNum" : 12,
  "sourceCodeText" : "void testDontSetTrackingURLIfHistoryWriteFailed() throws Exception\n{\r\n    TestParams t = new TestParams(true);\r\n    Configuration conf = new Configuration();\r\n    JHEvenHandlerForTest realJheh = new JHEvenHandlerForTest(t.mockAppContext, 0, false);\r\n    JHEvenHandlerForTest jheh = spy(realJheh);\r\n    jheh.init(conf);\r\n    try {\r\n        jheh.start();\r\n        doReturn(false).when(jheh).moveToDoneNow(any(Path.class), any(Path.class));\r\n        doNothing().when(jheh).moveTmpToDone(any(Path.class));\r\n        handleEvent(jheh, new JobHistoryEvent(t.jobId, new AMStartedEvent(t.appAttemptId, 200, t.containerId, \"nmhost\", 3000, 4000, -1)));\r\n        verify(jheh, times(0)).processDoneFiles(any(JobId.class));\r\n        verify(t.mockAppContext, times(0)).setHistoryUrl(any(String.class));\r\n        handleEvent(jheh, new JobHistoryEvent(t.jobId, new JobFinishedEvent(TypeConverter.fromYarn(t.jobId), 0, 0, 0, 0, 0, 0, 0, new Counters(), new Counters(), new Counters())));\r\n        verify(jheh, times(1)).processDoneFiles(any(JobId.class));\r\n        verify(t.mockAppContext, times(0)).setHistoryUrl(any(String.class));\r\n    } finally {\r\n        jheh.stop();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "testDontSetTrackingURLIfHistoryWriteThrows",
  "errType" : [ "YarnRuntimeException" ],
  "containingMethodsNum" : 11,
  "sourceCodeText" : "void testDontSetTrackingURLIfHistoryWriteThrows() throws Exception\n{\r\n    TestParams t = new TestParams(true);\r\n    Configuration conf = new Configuration();\r\n    JHEvenHandlerForTest realJheh = new JHEvenHandlerForTest(t.mockAppContext, 0, false);\r\n    JHEvenHandlerForTest jheh = spy(realJheh);\r\n    jheh.init(conf);\r\n    try {\r\n        jheh.start();\r\n        doThrow(new YarnRuntimeException(new IOException())).when(jheh).processDoneFiles(any(JobId.class));\r\n        handleEvent(jheh, new JobHistoryEvent(t.jobId, new AMStartedEvent(t.appAttemptId, 200, t.containerId, \"nmhost\", 3000, 4000, -1)));\r\n        verify(jheh, times(0)).processDoneFiles(any(JobId.class));\r\n        verify(t.mockAppContext, times(0)).setHistoryUrl(any(String.class));\r\n        try {\r\n            handleEvent(jheh, new JobHistoryEvent(t.jobId, new JobFinishedEvent(TypeConverter.fromYarn(t.jobId), 0, 0, 0, 0, 0, 0, 0, new Counters(), new Counters(), new Counters())));\r\n            throw new RuntimeException(\"processDoneFiles didn't throw, but should have\");\r\n        } catch (YarnRuntimeException yre) {\r\n        }\r\n        verify(jheh, times(1)).processDoneFiles(any(JobId.class));\r\n        verify(t.mockAppContext, times(0)).setHistoryUrl(any(String.class));\r\n    } finally {\r\n        jheh.stop();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 2,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "serviceInit",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void serviceInit(Configuration conf) throws Exception\n{\r\n    super.serviceInit(conf);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "serviceStart",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void serviceStart()\n{\r\n    if (timelineClient != null) {\r\n        timelineClient.start();\r\n    } else if (timelineV2Client != null) {\r\n        timelineV2Client.start();\r\n    }\r\n    if (handleTimelineEvent) {\r\n        atsEventDispatcher.start();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "createDispatcher",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "AsyncDispatcher createDispatcher()\n{\r\n    dispatcher = new DrainDispatcher();\r\n    return dispatcher;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "getDispatcher",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "DrainDispatcher getDispatcher()\n{\r\n    return dispatcher;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "createEventWriter",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "EventWriter createEventWriter(Path historyFilePath) throws IOException\n{\r\n    if (mockHistoryProcessing) {\r\n        this.eventWriter = mock(EventWriter.class);\r\n    } else {\r\n        this.eventWriter = super.createEventWriter(historyFilePath);\r\n    }\r\n    return this.eventWriter;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "closeEventWriter",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void closeEventWriter(JobId jobId)\n{\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "getEventWriter",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "EventWriter getEventWriter()\n{\r\n    return this.eventWriter;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "processDoneFiles",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void processDoneFiles(JobId jobId) throws IOException\n{\r\n    if (!mockHistoryProcessing) {\r\n        super.processDoneFiles(jobId);\r\n    } else {\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "addToFileMap",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void addToFileMap(JobId jobId)\n{\r\n    MetaInfo metaInfo = Mockito.mock(MetaInfo.class);\r\n    Mockito.when(metaInfo.isWriterActive()).thenReturn(true);\r\n    fileMap.put(jobId, metaInfo);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "handleEvent",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void handleEvent(JobHistoryEvent event)\n{\r\n    this.lastEventHandled = event;\r\n    this.eventsHandled++;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\job\\impl",
  "methodName" : "setupBeforeClass",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setupBeforeClass()\n{\r\n    ResourceUtils.resetResourceTypes(new Configuration());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\job\\impl",
  "methodName" : "before",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void before()\n{\r\n    TaskAttemptImpl.RESOURCE_REQUEST_CACHE.clear();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\job\\impl",
  "methodName" : "tearDown",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void tearDown()\n{\r\n    ResourceUtils.resetResourceTypes(new Configuration());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\job\\impl",
  "methodName" : "testMRAppHistoryForMap",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void testMRAppHistoryForMap() throws Exception\n{\r\n    MRApp app = null;\r\n    try {\r\n        app = new FailingAttemptsMRApp(1, 0);\r\n        testMRAppHistory(app);\r\n    } finally {\r\n        app.close();\r\n    }\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\job\\impl",
  "methodName" : "testMRAppHistoryForReduce",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void testMRAppHistoryForReduce() throws Exception\n{\r\n    MRApp app = null;\r\n    try {\r\n        app = new FailingAttemptsMRApp(0, 1);\r\n        testMRAppHistory(app);\r\n    } finally {\r\n        app.close();\r\n    }\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\job\\impl",
  "methodName" : "testMRAppHistoryForTAFailedInAssigned",
  "errType" : null,
  "containingMethodsNum" : 21,
  "sourceCodeText" : "void testMRAppHistoryForTAFailedInAssigned() throws Exception\n{\r\n    FailingAttemptsDuringAssignedMRApp app = null;\r\n    try {\r\n        app = new FailingAttemptsDuringAssignedMRApp(1, 0, TaskAttemptEventType.TA_CONTAINER_LAUNCH_FAILED);\r\n        testTaskAttemptAssignedFailHistory(app);\r\n        app.close();\r\n        app = new FailingAttemptsDuringAssignedMRApp(0, 1, TaskAttemptEventType.TA_CONTAINER_LAUNCH_FAILED);\r\n        testTaskAttemptAssignedFailHistory(app);\r\n        app.close();\r\n        app = new FailingAttemptsDuringAssignedMRApp(1, 0, TaskAttemptEventType.TA_CONTAINER_COMPLETED);\r\n        testTaskAttemptAssignedFailHistory(app);\r\n        app.close();\r\n        app = new FailingAttemptsDuringAssignedMRApp(0, 1, TaskAttemptEventType.TA_CONTAINER_COMPLETED);\r\n        testTaskAttemptAssignedFailHistory(app);\r\n        app.close();\r\n        app = new FailingAttemptsDuringAssignedMRApp(1, 0, TaskAttemptEventType.TA_FAILMSG);\r\n        testTaskAttemptAssignedFailHistory(app);\r\n        app.close();\r\n        app = new FailingAttemptsDuringAssignedMRApp(0, 1, TaskAttemptEventType.TA_FAILMSG);\r\n        testTaskAttemptAssignedFailHistory(app);\r\n        app.close();\r\n        app = new FailingAttemptsDuringAssignedMRApp(1, 0, TaskAttemptEventType.TA_FAILMSG_BY_CLIENT);\r\n        testTaskAttemptAssignedFailHistory(app);\r\n        app.close();\r\n        app = new FailingAttemptsDuringAssignedMRApp(0, 1, TaskAttemptEventType.TA_FAILMSG_BY_CLIENT);\r\n        testTaskAttemptAssignedFailHistory(app);\r\n        app.close();\r\n        app = new FailingAttemptsDuringAssignedMRApp(1, 0, TaskAttemptEventType.TA_KILL);\r\n        testTaskAttemptAssignedKilledHistory(app);\r\n        app.close();\r\n        app = new FailingAttemptsDuringAssignedMRApp(0, 1, TaskAttemptEventType.TA_KILL);\r\n        testTaskAttemptAssignedKilledHistory(app);\r\n        app.close();\r\n    } finally {\r\n        app.close();\r\n    }\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\job\\impl",
  "methodName" : "testSingleRackRequest",
  "errType" : null,
  "containingMethodsNum" : 11,
  "sourceCodeText" : "void testSingleRackRequest() throws Exception\n{\r\n    TaskAttemptImpl.RequestContainerTransition rct = new TaskAttemptImpl.RequestContainerTransition(false);\r\n    EventHandler eventHandler = mock(EventHandler.class);\r\n    String[] hosts = new String[3];\r\n    hosts[0] = \"host1\";\r\n    hosts[1] = \"host2\";\r\n    hosts[2] = \"host3\";\r\n    TaskSplitMetaInfo splitInfo = new TaskSplitMetaInfo(hosts, 0, 128 * 1024 * 1024l);\r\n    TaskAttemptImpl mockTaskAttempt = createMapTaskAttemptImplForTest(eventHandler, splitInfo);\r\n    TaskAttemptEvent mockTAEvent = mock(TaskAttemptEvent.class);\r\n    rct.transition(mockTaskAttempt, mockTAEvent);\r\n    ArgumentCaptor<Event> arg = ArgumentCaptor.forClass(Event.class);\r\n    verify(eventHandler, times(2)).handle(arg.capture());\r\n    if (!(arg.getAllValues().get(1) instanceof ContainerRequestEvent)) {\r\n        Assert.fail(\"Second Event not of type ContainerRequestEvent\");\r\n    }\r\n    ContainerRequestEvent cre = (ContainerRequestEvent) arg.getAllValues().get(1);\r\n    String[] requestedRacks = cre.getRacks();\r\n    assertEquals(1, requestedRacks.length);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\job\\impl",
  "methodName" : "testHostResolveAttempt",
  "errType" : null,
  "containingMethodsNum" : 19,
  "sourceCodeText" : "void testHostResolveAttempt() throws Exception\n{\r\n    TaskAttemptImpl.RequestContainerTransition rct = new TaskAttemptImpl.RequestContainerTransition(false);\r\n    EventHandler eventHandler = mock(EventHandler.class);\r\n    String[] hosts = new String[3];\r\n    hosts[0] = \"192.168.1.1\";\r\n    hosts[1] = \"host2\";\r\n    hosts[2] = \"host3\";\r\n    TaskSplitMetaInfo splitInfo = new TaskSplitMetaInfo(hosts, 0, 128 * 1024 * 1024l);\r\n    TaskAttemptImpl mockTaskAttempt = createMapTaskAttemptImplForTest(eventHandler, splitInfo);\r\n    TaskAttemptImpl spyTa = spy(mockTaskAttempt);\r\n    when(spyTa.resolveHost(hosts[0])).thenReturn(\"host1\");\r\n    spyTa.dataLocalHosts = spyTa.resolveHosts(splitInfo.getLocations());\r\n    TaskAttemptEvent mockTAEvent = mock(TaskAttemptEvent.class);\r\n    rct.transition(spyTa, mockTAEvent);\r\n    verify(spyTa).resolveHost(hosts[0]);\r\n    ArgumentCaptor<Event> arg = ArgumentCaptor.forClass(Event.class);\r\n    verify(eventHandler, times(2)).handle(arg.capture());\r\n    if (!(arg.getAllValues().get(1) instanceof ContainerRequestEvent)) {\r\n        Assert.fail(\"Second Event not of type ContainerRequestEvent\");\r\n    }\r\n    Map<String, Boolean> expected = new HashMap<String, Boolean>();\r\n    expected.put(\"host1\", true);\r\n    expected.put(\"host2\", true);\r\n    expected.put(\"host3\", true);\r\n    ContainerRequestEvent cre = (ContainerRequestEvent) arg.getAllValues().get(1);\r\n    String[] requestedHosts = cre.getHosts();\r\n    for (String h : requestedHosts) {\r\n        expected.remove(h);\r\n    }\r\n    assertEquals(0, expected.size());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\job\\impl",
  "methodName" : "testMillisCountersUpdate",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void testMillisCountersUpdate() throws Exception\n{\r\n    verifyMillisCounters(Resource.newInstance(1024, 1), 512);\r\n    verifyMillisCounters(Resource.newInstance(2048, 4), 1024);\r\n    verifyMillisCounters(Resource.newInstance(10240, 8), 2048);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\job\\impl",
  "methodName" : "verifyMillisCounters",
  "errType" : null,
  "containingMethodsNum" : 41,
  "sourceCodeText" : "void verifyMillisCounters(Resource containerResource, int minContainerSize) throws Exception\n{\r\n    Clock actualClock = SystemClock.getInstance();\r\n    ControlledClock clock = new ControlledClock(actualClock);\r\n    clock.setTime(10);\r\n    MRApp app = new MRApp(1, 1, false, \"testSlotMillisCounterUpdate\", true, clock);\r\n    app.setAllocatedContainerResource(containerResource);\r\n    Configuration conf = new Configuration();\r\n    conf.setInt(YarnConfiguration.RM_SCHEDULER_MINIMUM_ALLOCATION_MB, minContainerSize);\r\n    app.setClusterInfo(new ClusterInfo(Resource.newInstance(10240, 1)));\r\n    Job job = app.submit(conf);\r\n    app.waitForState(job, JobState.RUNNING);\r\n    Map<TaskId, Task> tasks = job.getTasks();\r\n    Assert.assertEquals(\"Num tasks is not correct\", 2, tasks.size());\r\n    Iterator<Task> taskIter = tasks.values().iterator();\r\n    Task mTask = taskIter.next();\r\n    app.waitForState(mTask, TaskState.RUNNING);\r\n    Task rTask = taskIter.next();\r\n    app.waitForState(rTask, TaskState.RUNNING);\r\n    Map<TaskAttemptId, TaskAttempt> mAttempts = mTask.getAttempts();\r\n    Assert.assertEquals(\"Num attempts is not correct\", 1, mAttempts.size());\r\n    Map<TaskAttemptId, TaskAttempt> rAttempts = rTask.getAttempts();\r\n    Assert.assertEquals(\"Num attempts is not correct\", 1, rAttempts.size());\r\n    TaskAttempt mta = mAttempts.values().iterator().next();\r\n    TaskAttempt rta = rAttempts.values().iterator().next();\r\n    app.waitForState(mta, TaskAttemptState.RUNNING);\r\n    app.waitForState(rta, TaskAttemptState.RUNNING);\r\n    clock.setTime(11);\r\n    app.getContext().getEventHandler().handle(new TaskAttemptEvent(mta.getID(), TaskAttemptEventType.TA_DONE));\r\n    app.getContext().getEventHandler().handle(new TaskAttemptEvent(rta.getID(), TaskAttemptEventType.TA_DONE));\r\n    app.waitForState(job, JobState.SUCCEEDED);\r\n    assertThat(mta.getFinishTime()).isEqualTo(11);\r\n    assertThat(mta.getLaunchTime()).isEqualTo(10);\r\n    assertThat(rta.getFinishTime()).isEqualTo(11);\r\n    assertThat(rta.getLaunchTime()).isEqualTo(10);\r\n    Counters counters = job.getAllCounters();\r\n    int memoryMb = (int) containerResource.getMemorySize();\r\n    int vcores = containerResource.getVirtualCores();\r\n    Assert.assertEquals((int) Math.ceil((float) memoryMb / minContainerSize), counters.findCounter(JobCounter.SLOTS_MILLIS_MAPS).getValue());\r\n    Assert.assertEquals((int) Math.ceil((float) memoryMb / minContainerSize), counters.findCounter(JobCounter.SLOTS_MILLIS_REDUCES).getValue());\r\n    Assert.assertEquals(1, counters.findCounter(JobCounter.MILLIS_MAPS).getValue());\r\n    Assert.assertEquals(1, counters.findCounter(JobCounter.MILLIS_REDUCES).getValue());\r\n    Assert.assertEquals(memoryMb, counters.findCounter(JobCounter.MB_MILLIS_MAPS).getValue());\r\n    Assert.assertEquals(memoryMb, counters.findCounter(JobCounter.MB_MILLIS_REDUCES).getValue());\r\n    Assert.assertEquals(vcores, counters.findCounter(JobCounter.VCORES_MILLIS_MAPS).getValue());\r\n    Assert.assertEquals(vcores, counters.findCounter(JobCounter.VCORES_MILLIS_REDUCES).getValue());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\job\\impl",
  "methodName" : "createMapTaskAttemptImplForTest",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "TaskAttemptImpl createMapTaskAttemptImplForTest(EventHandler eventHandler, TaskSplitMetaInfo taskSplitMetaInfo)\n{\r\n    Clock clock = SystemClock.getInstance();\r\n    return createMapTaskAttemptImplForTest(eventHandler, taskSplitMetaInfo, clock, new JobConf());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\job\\impl",
  "methodName" : "createMapTaskAttemptImplForTest",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "TaskAttemptImpl createMapTaskAttemptImplForTest(EventHandler eventHandler, TaskSplitMetaInfo taskSplitMetaInfo, Clock clock, JobConf jobConf)\n{\r\n    ApplicationId appId = ApplicationId.newInstance(1, 1);\r\n    JobId jobId = MRBuilderUtils.newJobId(appId, 1);\r\n    TaskId taskId = MRBuilderUtils.newTaskId(jobId, 1, TaskType.MAP);\r\n    TaskAttemptListener taListener = mock(TaskAttemptListener.class);\r\n    Path jobFile = mock(Path.class);\r\n    TaskAttemptImpl taImpl = new MapTaskAttemptImpl(taskId, 1, eventHandler, jobFile, 1, taskSplitMetaInfo, jobConf, taListener, null, null, clock, null);\r\n    return taImpl;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\job\\impl",
  "methodName" : "createReduceTaskAttemptImplForTest",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "TaskAttemptImpl createReduceTaskAttemptImplForTest(EventHandler eventHandler, Clock clock, JobConf jobConf)\n{\r\n    ApplicationId appId = ApplicationId.newInstance(1, 1);\r\n    JobId jobId = MRBuilderUtils.newJobId(appId, 1);\r\n    TaskId taskId = MRBuilderUtils.newTaskId(jobId, 1, TaskType.REDUCE);\r\n    TaskAttemptListener taListener = mock(TaskAttemptListener.class);\r\n    Path jobFile = mock(Path.class);\r\n    TaskAttemptImpl taImpl = new ReduceTaskAttemptImpl(taskId, 1, eventHandler, jobFile, 1, 1, jobConf, taListener, null, null, clock, null);\r\n    return taImpl;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\job\\impl",
  "methodName" : "testMRAppHistory",
  "errType" : null,
  "containingMethodsNum" : 14,
  "sourceCodeText" : "void testMRAppHistory(MRApp app) throws Exception\n{\r\n    Configuration conf = new Configuration();\r\n    Job job = app.submit(conf);\r\n    app.waitForState(job, JobState.FAILED);\r\n    Map<TaskId, Task> tasks = job.getTasks();\r\n    Assert.assertEquals(\"Num tasks is not correct\", 1, tasks.size());\r\n    Task task = tasks.values().iterator().next();\r\n    Assert.assertEquals(\"Task state not correct\", TaskState.FAILED, task.getReport().getTaskState());\r\n    Map<TaskAttemptId, TaskAttempt> attempts = tasks.values().iterator().next().getAttempts();\r\n    Assert.assertEquals(\"Num attempts is not correct\", 4, attempts.size());\r\n    Iterator<TaskAttempt> it = attempts.values().iterator();\r\n    TaskAttemptReport report = it.next().getReport();\r\n    Assert.assertEquals(\"Attempt state not correct\", TaskAttemptState.FAILED, report.getTaskAttemptState());\r\n    Assert.assertEquals(\"Diagnostic Information is not Correct\", \"Test Diagnostic Event\", report.getDiagnosticInfo());\r\n    report = it.next().getReport();\r\n    Assert.assertEquals(\"Attempt state not correct\", TaskAttemptState.FAILED, report.getTaskAttemptState());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\job\\impl",
  "methodName" : "testTaskAttemptAssignedFailHistory",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void testTaskAttemptAssignedFailHistory(FailingAttemptsDuringAssignedMRApp app) throws Exception\n{\r\n    Configuration conf = new Configuration();\r\n    Job job = app.submit(conf);\r\n    app.waitForState(job, JobState.FAILED);\r\n    Map<TaskId, Task> tasks = job.getTasks();\r\n    Assert.assertTrue(\"No Ta Started JH Event\", app.getTaStartJHEvent());\r\n    Assert.assertTrue(\"No Ta Failed JH Event\", app.getTaFailedJHEvent());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\job\\impl",
  "methodName" : "testTaskAttemptAssignedKilledHistory",
  "errType" : null,
  "containingMethodsNum" : 10,
  "sourceCodeText" : "void testTaskAttemptAssignedKilledHistory(FailingAttemptsDuringAssignedMRApp app) throws Exception\n{\r\n    Configuration conf = new Configuration();\r\n    Job job = app.submit(conf);\r\n    app.waitForState(job, JobState.RUNNING);\r\n    Map<TaskId, Task> tasks = job.getTasks();\r\n    Task task = tasks.values().iterator().next();\r\n    app.waitForState(task, TaskState.SCHEDULED);\r\n    Map<TaskAttemptId, TaskAttempt> attempts = task.getAttempts();\r\n    TaskAttempt attempt = attempts.values().iterator().next();\r\n    app.waitForState(attempt, TaskAttemptState.KILLED);\r\n    waitFor(app::getTaStartJHEvent, 100, 800);\r\n    waitFor(app::getTaKilledJHEvent, 100, 800);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\job\\impl",
  "methodName" : "testLaunchFailedWhileKilling",
  "errType" : null,
  "containingMethodsNum" : 27,
  "sourceCodeText" : "void testLaunchFailedWhileKilling() throws Exception\n{\r\n    ApplicationId appId = ApplicationId.newInstance(1, 2);\r\n    ApplicationAttemptId appAttemptId = ApplicationAttemptId.newInstance(appId, 0);\r\n    JobId jobId = MRBuilderUtils.newJobId(appId, 1);\r\n    TaskId taskId = MRBuilderUtils.newTaskId(jobId, 1, TaskType.MAP);\r\n    TaskAttemptId attemptId = MRBuilderUtils.newTaskAttemptId(taskId, 0);\r\n    Path jobFile = mock(Path.class);\r\n    MockEventHandler eventHandler = new MockEventHandler();\r\n    TaskAttemptListener taListener = mock(TaskAttemptListener.class);\r\n    when(taListener.getAddress()).thenReturn(new InetSocketAddress(\"localhost\", 0));\r\n    JobConf jobConf = new JobConf();\r\n    jobConf.setClass(\"fs.file.impl\", StubbedFS.class, FileSystem.class);\r\n    jobConf.setBoolean(\"fs.file.impl.disable.cache\", true);\r\n    jobConf.set(JobConf.MAPRED_MAP_TASK_ENV, \"\");\r\n    jobConf.set(MRJobConfig.APPLICATION_ATTEMPT_ID, \"10\");\r\n    TaskSplitMetaInfo splits = mock(TaskSplitMetaInfo.class);\r\n    when(splits.getLocations()).thenReturn(new String[] { \"127.0.0.1\" });\r\n    TaskAttemptImpl taImpl = new MapTaskAttemptImpl(taskId, 1, eventHandler, jobFile, 1, splits, jobConf, taListener, new Token(), new Credentials(), SystemClock.getInstance(), null);\r\n    NodeId nid = NodeId.newInstance(\"127.0.0.1\", 0);\r\n    ContainerId contId = ContainerId.newContainerId(appAttemptId, 3);\r\n    Container container = mock(Container.class);\r\n    when(container.getId()).thenReturn(contId);\r\n    when(container.getNodeId()).thenReturn(nid);\r\n    taImpl.handle(new TaskAttemptEvent(attemptId, TaskAttemptEventType.TA_SCHEDULE));\r\n    taImpl.handle(new TaskAttemptContainerAssignedEvent(attemptId, container, mock(Map.class)));\r\n    taImpl.handle(new TaskAttemptEvent(attemptId, TaskAttemptEventType.TA_KILL));\r\n    taImpl.handle(new TaskAttemptEvent(attemptId, TaskAttemptEventType.TA_CONTAINER_CLEANED));\r\n    taImpl.handle(new TaskAttemptEvent(attemptId, TaskAttemptEventType.TA_CONTAINER_LAUNCH_FAILED));\r\n    assertFalse(eventHandler.internalError);\r\n    assertEquals(\"Task attempt is not assigned on the local node\", Locality.NODE_LOCAL, taImpl.getLocality());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\job\\impl",
  "methodName" : "testContainerCleanedWhileRunning",
  "errType" : null,
  "containingMethodsNum" : 34,
  "sourceCodeText" : "void testContainerCleanedWhileRunning() throws Exception\n{\r\n    ApplicationId appId = ApplicationId.newInstance(1, 2);\r\n    ApplicationAttemptId appAttemptId = ApplicationAttemptId.newInstance(appId, 0);\r\n    JobId jobId = MRBuilderUtils.newJobId(appId, 1);\r\n    TaskId taskId = MRBuilderUtils.newTaskId(jobId, 1, TaskType.MAP);\r\n    TaskAttemptId attemptId = MRBuilderUtils.newTaskAttemptId(taskId, 0);\r\n    Path jobFile = mock(Path.class);\r\n    MockEventHandler eventHandler = new MockEventHandler();\r\n    TaskAttemptListener taListener = mock(TaskAttemptListener.class);\r\n    when(taListener.getAddress()).thenReturn(new InetSocketAddress(\"localhost\", 0));\r\n    JobConf jobConf = new JobConf();\r\n    jobConf.setClass(\"fs.file.impl\", StubbedFS.class, FileSystem.class);\r\n    jobConf.setBoolean(\"fs.file.impl.disable.cache\", true);\r\n    jobConf.set(JobConf.MAPRED_MAP_TASK_ENV, \"\");\r\n    jobConf.set(MRJobConfig.APPLICATION_ATTEMPT_ID, \"10\");\r\n    TaskSplitMetaInfo splits = mock(TaskSplitMetaInfo.class);\r\n    when(splits.getLocations()).thenReturn(new String[] { \"127.0.0.1\" });\r\n    AppContext appCtx = mock(AppContext.class);\r\n    ClusterInfo clusterInfo = mock(ClusterInfo.class);\r\n    Resource resource = mock(Resource.class);\r\n    when(appCtx.getClusterInfo()).thenReturn(clusterInfo);\r\n    when(resource.getMemorySize()).thenReturn(1024L);\r\n    setupTaskAttemptFinishingMonitor(eventHandler, jobConf, appCtx);\r\n    TaskAttemptImpl taImpl = new MapTaskAttemptImpl(taskId, 1, eventHandler, jobFile, 1, splits, jobConf, taListener, new Token(), new Credentials(), SystemClock.getInstance(), appCtx);\r\n    NodeId nid = NodeId.newInstance(\"127.0.0.2\", 0);\r\n    ContainerId contId = ContainerId.newContainerId(appAttemptId, 3);\r\n    Container container = mock(Container.class);\r\n    when(container.getId()).thenReturn(contId);\r\n    when(container.getNodeId()).thenReturn(nid);\r\n    when(container.getNodeHttpAddress()).thenReturn(\"localhost:0\");\r\n    taImpl.handle(new TaskAttemptEvent(attemptId, TaskAttemptEventType.TA_SCHEDULE));\r\n    taImpl.handle(new TaskAttemptContainerAssignedEvent(attemptId, container, mock(Map.class)));\r\n    taImpl.handle(new TaskAttemptContainerLaunchedEvent(attemptId, 0));\r\n    assertThat(taImpl.getState()).withFailMessage(\"Task attempt is not in RUNNING state\").isEqualTo(TaskAttemptState.RUNNING);\r\n    taImpl.handle(new TaskAttemptEvent(attemptId, TaskAttemptEventType.TA_CONTAINER_CLEANED));\r\n    assertFalse(\"InternalError occurred trying to handle TA_CONTAINER_CLEANED\", eventHandler.internalError);\r\n    assertEquals(\"Task attempt is not assigned on the local rack\", Locality.RACK_LOCAL, taImpl.getLocality());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\job\\impl",
  "methodName" : "testContainerCleanedWhileCommitting",
  "errType" : null,
  "containingMethodsNum" : 35,
  "sourceCodeText" : "void testContainerCleanedWhileCommitting() throws Exception\n{\r\n    ApplicationId appId = ApplicationId.newInstance(1, 2);\r\n    ApplicationAttemptId appAttemptId = ApplicationAttemptId.newInstance(appId, 0);\r\n    JobId jobId = MRBuilderUtils.newJobId(appId, 1);\r\n    TaskId taskId = MRBuilderUtils.newTaskId(jobId, 1, TaskType.MAP);\r\n    TaskAttemptId attemptId = MRBuilderUtils.newTaskAttemptId(taskId, 0);\r\n    Path jobFile = mock(Path.class);\r\n    MockEventHandler eventHandler = new MockEventHandler();\r\n    TaskAttemptListener taListener = mock(TaskAttemptListener.class);\r\n    when(taListener.getAddress()).thenReturn(new InetSocketAddress(\"localhost\", 0));\r\n    JobConf jobConf = new JobConf();\r\n    jobConf.setClass(\"fs.file.impl\", StubbedFS.class, FileSystem.class);\r\n    jobConf.setBoolean(\"fs.file.impl.disable.cache\", true);\r\n    jobConf.set(JobConf.MAPRED_MAP_TASK_ENV, \"\");\r\n    jobConf.set(MRJobConfig.APPLICATION_ATTEMPT_ID, \"10\");\r\n    TaskSplitMetaInfo splits = mock(TaskSplitMetaInfo.class);\r\n    when(splits.getLocations()).thenReturn(new String[] {});\r\n    AppContext appCtx = mock(AppContext.class);\r\n    ClusterInfo clusterInfo = mock(ClusterInfo.class);\r\n    Resource resource = mock(Resource.class);\r\n    when(appCtx.getClusterInfo()).thenReturn(clusterInfo);\r\n    when(resource.getMemorySize()).thenReturn(1024L);\r\n    setupTaskAttemptFinishingMonitor(eventHandler, jobConf, appCtx);\r\n    TaskAttemptImpl taImpl = new MapTaskAttemptImpl(taskId, 1, eventHandler, jobFile, 1, splits, jobConf, taListener, new Token(), new Credentials(), SystemClock.getInstance(), appCtx);\r\n    NodeId nid = NodeId.newInstance(\"127.0.0.1\", 0);\r\n    ContainerId contId = ContainerId.newContainerId(appAttemptId, 3);\r\n    Container container = mock(Container.class);\r\n    when(container.getId()).thenReturn(contId);\r\n    when(container.getNodeId()).thenReturn(nid);\r\n    when(container.getNodeHttpAddress()).thenReturn(\"localhost:0\");\r\n    taImpl.handle(new TaskAttemptEvent(attemptId, TaskAttemptEventType.TA_SCHEDULE));\r\n    taImpl.handle(new TaskAttemptContainerAssignedEvent(attemptId, container, mock(Map.class)));\r\n    taImpl.handle(new TaskAttemptContainerLaunchedEvent(attemptId, 0));\r\n    taImpl.handle(new TaskAttemptEvent(attemptId, TaskAttemptEventType.TA_COMMIT_PENDING));\r\n    assertThat(taImpl.getState()).withFailMessage(\"Task attempt is not in COMMIT_PENDING state\").isEqualTo(TaskAttemptState.COMMIT_PENDING);\r\n    taImpl.handle(new TaskAttemptEvent(attemptId, TaskAttemptEventType.TA_CONTAINER_CLEANED));\r\n    assertFalse(\"InternalError occurred trying to handle TA_CONTAINER_CLEANED\", eventHandler.internalError);\r\n    assertEquals(\"Task attempt is assigned locally\", Locality.OFF_SWITCH, taImpl.getLocality());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\job\\impl",
  "methodName" : "testDoubleTooManyFetchFailure",
  "errType" : null,
  "containingMethodsNum" : 40,
  "sourceCodeText" : "void testDoubleTooManyFetchFailure() throws Exception\n{\r\n    ApplicationId appId = ApplicationId.newInstance(1, 2);\r\n    ApplicationAttemptId appAttemptId = ApplicationAttemptId.newInstance(appId, 0);\r\n    JobId jobId = MRBuilderUtils.newJobId(appId, 1);\r\n    TaskId taskId = MRBuilderUtils.newTaskId(jobId, 1, TaskType.MAP);\r\n    TaskAttemptId attemptId = MRBuilderUtils.newTaskAttemptId(taskId, 0);\r\n    TaskId reduceTaskId = MRBuilderUtils.newTaskId(jobId, 1, TaskType.REDUCE);\r\n    TaskAttemptId reduceTAId = MRBuilderUtils.newTaskAttemptId(reduceTaskId, 0);\r\n    Path jobFile = mock(Path.class);\r\n    MockEventHandler eventHandler = new MockEventHandler();\r\n    TaskAttemptListener taListener = mock(TaskAttemptListener.class);\r\n    when(taListener.getAddress()).thenReturn(new InetSocketAddress(\"localhost\", 0));\r\n    JobConf jobConf = new JobConf();\r\n    jobConf.setClass(\"fs.file.impl\", StubbedFS.class, FileSystem.class);\r\n    jobConf.setBoolean(\"fs.file.impl.disable.cache\", true);\r\n    jobConf.set(JobConf.MAPRED_MAP_TASK_ENV, \"\");\r\n    jobConf.set(MRJobConfig.APPLICATION_ATTEMPT_ID, \"10\");\r\n    TaskSplitMetaInfo splits = mock(TaskSplitMetaInfo.class);\r\n    when(splits.getLocations()).thenReturn(new String[] { \"127.0.0.1\" });\r\n    AppContext appCtx = mock(AppContext.class);\r\n    ClusterInfo clusterInfo = mock(ClusterInfo.class);\r\n    Resource resource = mock(Resource.class);\r\n    when(appCtx.getClusterInfo()).thenReturn(clusterInfo);\r\n    when(resource.getMemorySize()).thenReturn(1024L);\r\n    setupTaskAttemptFinishingMonitor(eventHandler, jobConf, appCtx);\r\n    TaskAttemptImpl taImpl = new MapTaskAttemptImpl(taskId, 1, eventHandler, jobFile, 1, splits, jobConf, taListener, new Token(), new Credentials(), SystemClock.getInstance(), appCtx);\r\n    NodeId nid = NodeId.newInstance(\"127.0.0.1\", 0);\r\n    ContainerId contId = ContainerId.newContainerId(appAttemptId, 3);\r\n    Container container = mock(Container.class);\r\n    when(container.getId()).thenReturn(contId);\r\n    when(container.getNodeId()).thenReturn(nid);\r\n    when(container.getNodeHttpAddress()).thenReturn(\"localhost:0\");\r\n    taImpl.handle(new TaskAttemptEvent(attemptId, TaskAttemptEventType.TA_SCHEDULE));\r\n    taImpl.handle(new TaskAttemptContainerAssignedEvent(attemptId, container, mock(Map.class)));\r\n    taImpl.handle(new TaskAttemptContainerLaunchedEvent(attemptId, 0));\r\n    taImpl.handle(new TaskAttemptEvent(attemptId, TaskAttemptEventType.TA_DONE));\r\n    taImpl.handle(new TaskAttemptEvent(attemptId, TaskAttemptEventType.TA_CONTAINER_COMPLETED));\r\n    assertThat(taImpl.getState()).withFailMessage(\"Task attempt is not in SUCCEEDED state\").isEqualTo(TaskAttemptState.SUCCEEDED);\r\n    taImpl.handle(new TaskAttemptTooManyFetchFailureEvent(attemptId, reduceTAId, \"Host\"));\r\n    assertThat(taImpl.getState()).withFailMessage(\"Task attempt is not in FAILED state\").isEqualTo(TaskAttemptState.FAILED);\r\n    taImpl.handle(new TaskAttemptEvent(attemptId, TaskAttemptEventType.TA_TOO_MANY_FETCH_FAILURE));\r\n    assertThat(taImpl.getState()).withFailMessage(\"Task attempt is not in FAILED state, still\").isEqualTo(TaskAttemptState.FAILED);\r\n    assertFalse(\"InternalError occurred trying to handle TA_CONTAINER_CLEANED\", eventHandler.internalError);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\job\\impl",
  "methodName" : "testAppDiagnosticEventOnUnassignedTask",
  "errType" : [ "Exception" ],
  "containingMethodsNum" : 33,
  "sourceCodeText" : "void testAppDiagnosticEventOnUnassignedTask()\n{\r\n    ApplicationId appId = ApplicationId.newInstance(1, 2);\r\n    ApplicationAttemptId appAttemptId = ApplicationAttemptId.newInstance(appId, 0);\r\n    JobId jobId = MRBuilderUtils.newJobId(appId, 1);\r\n    TaskId taskId = MRBuilderUtils.newTaskId(jobId, 1, TaskType.MAP);\r\n    TaskAttemptId attemptId = MRBuilderUtils.newTaskAttemptId(taskId, 0);\r\n    Path jobFile = mock(Path.class);\r\n    MockEventHandler eventHandler = new MockEventHandler();\r\n    TaskAttemptListener taListener = mock(TaskAttemptListener.class);\r\n    when(taListener.getAddress()).thenReturn(new InetSocketAddress(\"localhost\", 0));\r\n    JobConf jobConf = new JobConf();\r\n    jobConf.setClass(\"fs.file.impl\", StubbedFS.class, FileSystem.class);\r\n    jobConf.setBoolean(\"fs.file.impl.disable.cache\", true);\r\n    jobConf.set(JobConf.MAPRED_MAP_TASK_ENV, \"\");\r\n    jobConf.set(MRJobConfig.APPLICATION_ATTEMPT_ID, \"10\");\r\n    TaskSplitMetaInfo splits = mock(TaskSplitMetaInfo.class);\r\n    when(splits.getLocations()).thenReturn(new String[] { \"127.0.0.1\" });\r\n    AppContext appCtx = mock(AppContext.class);\r\n    ClusterInfo clusterInfo = mock(ClusterInfo.class);\r\n    Resource resource = mock(Resource.class);\r\n    when(appCtx.getClusterInfo()).thenReturn(clusterInfo);\r\n    when(resource.getMemorySize()).thenReturn(1024L);\r\n    setupTaskAttemptFinishingMonitor(eventHandler, jobConf, appCtx);\r\n    TaskAttemptImpl taImpl = new MapTaskAttemptImpl(taskId, 1, eventHandler, jobFile, 1, splits, jobConf, taListener, new Token(), new Credentials(), SystemClock.getInstance(), appCtx);\r\n    NodeId nid = NodeId.newInstance(\"127.0.0.1\", 0);\r\n    ContainerId contId = ContainerId.newContainerId(appAttemptId, 3);\r\n    Container container = mock(Container.class);\r\n    when(container.getId()).thenReturn(contId);\r\n    when(container.getNodeId()).thenReturn(nid);\r\n    when(container.getNodeHttpAddress()).thenReturn(\"localhost:0\");\r\n    taImpl.handle(new TaskAttemptEvent(attemptId, TaskAttemptEventType.TA_SCHEDULE));\r\n    taImpl.handle(new TaskAttemptDiagnosticsUpdateEvent(attemptId, \"Task got killed\"));\r\n    assertFalse(\"InternalError occurred trying to handle TA_DIAGNOSTICS_UPDATE on assigned task\", eventHandler.internalError);\r\n    try {\r\n        taImpl.handle(new TaskAttemptEvent(attemptId, TaskAttemptEventType.TA_KILL));\r\n        Assert.assertTrue(\"No exception on UNASSIGNED STATE KILL event\", true);\r\n    } catch (Exception e) {\r\n        Assert.assertFalse(\"Exception not expected for UNASSIGNED STATE KILL event\", true);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\job\\impl",
  "methodName" : "testTooManyFetchFailureAfterKill",
  "errType" : null,
  "containingMethodsNum" : 39,
  "sourceCodeText" : "void testTooManyFetchFailureAfterKill() throws Exception\n{\r\n    ApplicationId appId = ApplicationId.newInstance(1, 2);\r\n    ApplicationAttemptId appAttemptId = ApplicationAttemptId.newInstance(appId, 0);\r\n    JobId jobId = MRBuilderUtils.newJobId(appId, 1);\r\n    TaskId taskId = MRBuilderUtils.newTaskId(jobId, 1, TaskType.MAP);\r\n    TaskAttemptId attemptId = MRBuilderUtils.newTaskAttemptId(taskId, 0);\r\n    Path jobFile = mock(Path.class);\r\n    MockEventHandler eventHandler = new MockEventHandler();\r\n    TaskAttemptListener taListener = mock(TaskAttemptListener.class);\r\n    when(taListener.getAddress()).thenReturn(new InetSocketAddress(\"localhost\", 0));\r\n    JobConf jobConf = new JobConf();\r\n    jobConf.setClass(\"fs.file.impl\", StubbedFS.class, FileSystem.class);\r\n    jobConf.setBoolean(\"fs.file.impl.disable.cache\", true);\r\n    jobConf.set(JobConf.MAPRED_MAP_TASK_ENV, \"\");\r\n    jobConf.set(MRJobConfig.APPLICATION_ATTEMPT_ID, \"10\");\r\n    TaskSplitMetaInfo splits = mock(TaskSplitMetaInfo.class);\r\n    when(splits.getLocations()).thenReturn(new String[] { \"127.0.0.1\" });\r\n    AppContext appCtx = mock(AppContext.class);\r\n    ClusterInfo clusterInfo = mock(ClusterInfo.class);\r\n    Resource resource = mock(Resource.class);\r\n    when(appCtx.getClusterInfo()).thenReturn(clusterInfo);\r\n    when(resource.getMemorySize()).thenReturn(1024L);\r\n    setupTaskAttemptFinishingMonitor(eventHandler, jobConf, appCtx);\r\n    TaskAttemptImpl taImpl = new MapTaskAttemptImpl(taskId, 1, eventHandler, jobFile, 1, splits, jobConf, taListener, mock(Token.class), new Credentials(), SystemClock.getInstance(), appCtx);\r\n    NodeId nid = NodeId.newInstance(\"127.0.0.1\", 0);\r\n    ContainerId contId = ContainerId.newContainerId(appAttemptId, 3);\r\n    Container container = mock(Container.class);\r\n    when(container.getId()).thenReturn(contId);\r\n    when(container.getNodeId()).thenReturn(nid);\r\n    when(container.getNodeHttpAddress()).thenReturn(\"localhost:0\");\r\n    taImpl.handle(new TaskAttemptEvent(attemptId, TaskAttemptEventType.TA_SCHEDULE));\r\n    taImpl.handle(new TaskAttemptContainerAssignedEvent(attemptId, container, mock(Map.class)));\r\n    taImpl.handle(new TaskAttemptContainerLaunchedEvent(attemptId, 0));\r\n    taImpl.handle(new TaskAttemptEvent(attemptId, TaskAttemptEventType.TA_DONE));\r\n    taImpl.handle(new TaskAttemptEvent(attemptId, TaskAttemptEventType.TA_CONTAINER_COMPLETED));\r\n    assertThat(taImpl.getState()).withFailMessage(\"Task attempt is not in SUCCEEDED state\").isEqualTo(TaskAttemptState.SUCCEEDED);\r\n    taImpl.handle(new TaskAttemptEvent(attemptId, TaskAttemptEventType.TA_KILL));\r\n    assertThat(taImpl.getState()).withFailMessage(\"Task attempt is not in KILLED state\").isEqualTo(TaskAttemptState.KILLED);\r\n    taImpl.handle(new TaskAttemptEvent(attemptId, TaskAttemptEventType.TA_TOO_MANY_FETCH_FAILURE));\r\n    assertThat(taImpl.getState()).withFailMessage(\"Task attempt is not in KILLED state, still\").isEqualTo(TaskAttemptState.KILLED);\r\n    assertFalse(\"InternalError occurred trying to handle TA_CONTAINER_CLEANED\", eventHandler.internalError);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\job\\impl",
  "methodName" : "testAppDiagnosticEventOnNewTask",
  "errType" : null,
  "containingMethodsNum" : 29,
  "sourceCodeText" : "void testAppDiagnosticEventOnNewTask()\n{\r\n    ApplicationId appId = ApplicationId.newInstance(1, 2);\r\n    ApplicationAttemptId appAttemptId = ApplicationAttemptId.newInstance(appId, 0);\r\n    JobId jobId = MRBuilderUtils.newJobId(appId, 1);\r\n    TaskId taskId = MRBuilderUtils.newTaskId(jobId, 1, TaskType.MAP);\r\n    TaskAttemptId attemptId = MRBuilderUtils.newTaskAttemptId(taskId, 0);\r\n    Path jobFile = mock(Path.class);\r\n    MockEventHandler eventHandler = new MockEventHandler();\r\n    TaskAttemptListener taListener = mock(TaskAttemptListener.class);\r\n    when(taListener.getAddress()).thenReturn(new InetSocketAddress(\"localhost\", 0));\r\n    JobConf jobConf = new JobConf();\r\n    jobConf.setClass(\"fs.file.impl\", StubbedFS.class, FileSystem.class);\r\n    jobConf.setBoolean(\"fs.file.impl.disable.cache\", true);\r\n    jobConf.set(JobConf.MAPRED_MAP_TASK_ENV, \"\");\r\n    jobConf.set(MRJobConfig.APPLICATION_ATTEMPT_ID, \"10\");\r\n    TaskSplitMetaInfo splits = mock(TaskSplitMetaInfo.class);\r\n    when(splits.getLocations()).thenReturn(new String[] { \"127.0.0.1\" });\r\n    AppContext appCtx = mock(AppContext.class);\r\n    ClusterInfo clusterInfo = mock(ClusterInfo.class);\r\n    Resource resource = mock(Resource.class);\r\n    when(appCtx.getClusterInfo()).thenReturn(clusterInfo);\r\n    when(resource.getMemorySize()).thenReturn(1024L);\r\n    setupTaskAttemptFinishingMonitor(eventHandler, jobConf, appCtx);\r\n    TaskAttemptImpl taImpl = new MapTaskAttemptImpl(taskId, 1, eventHandler, jobFile, 1, splits, jobConf, taListener, new Token(), new Credentials(), SystemClock.getInstance(), appCtx);\r\n    NodeId nid = NodeId.newInstance(\"127.0.0.1\", 0);\r\n    ContainerId contId = ContainerId.newContainerId(appAttemptId, 3);\r\n    Container container = mock(Container.class);\r\n    when(container.getId()).thenReturn(contId);\r\n    when(container.getNodeId()).thenReturn(nid);\r\n    when(container.getNodeHttpAddress()).thenReturn(\"localhost:0\");\r\n    taImpl.handle(new TaskAttemptDiagnosticsUpdateEvent(attemptId, \"Task got killed\"));\r\n    assertFalse(\"InternalError occurred trying to handle TA_DIAGNOSTICS_UPDATE on assigned task\", eventHandler.internalError);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\job\\impl",
  "methodName" : "testFetchFailureAttemptFinishTime",
  "errType" : null,
  "containingMethodsNum" : 40,
  "sourceCodeText" : "void testFetchFailureAttemptFinishTime() throws Exception\n{\r\n    ApplicationId appId = ApplicationId.newInstance(1, 2);\r\n    ApplicationAttemptId appAttemptId = ApplicationAttemptId.newInstance(appId, 0);\r\n    JobId jobId = MRBuilderUtils.newJobId(appId, 1);\r\n    TaskId taskId = MRBuilderUtils.newTaskId(jobId, 1, TaskType.MAP);\r\n    TaskAttemptId attemptId = MRBuilderUtils.newTaskAttemptId(taskId, 0);\r\n    TaskId reducetaskId = MRBuilderUtils.newTaskId(jobId, 1, TaskType.REDUCE);\r\n    TaskAttemptId reduceTAId = MRBuilderUtils.newTaskAttemptId(reducetaskId, 0);\r\n    Path jobFile = mock(Path.class);\r\n    MockEventHandler eventHandler = new MockEventHandler();\r\n    TaskAttemptListener taListener = mock(TaskAttemptListener.class);\r\n    when(taListener.getAddress()).thenReturn(new InetSocketAddress(\"localhost\", 0));\r\n    JobConf jobConf = new JobConf();\r\n    jobConf.setClass(\"fs.file.impl\", StubbedFS.class, FileSystem.class);\r\n    jobConf.setBoolean(\"fs.file.impl.disable.cache\", true);\r\n    jobConf.set(JobConf.MAPRED_MAP_TASK_ENV, \"\");\r\n    jobConf.set(MRJobConfig.APPLICATION_ATTEMPT_ID, \"10\");\r\n    TaskSplitMetaInfo splits = mock(TaskSplitMetaInfo.class);\r\n    when(splits.getLocations()).thenReturn(new String[] { \"127.0.0.1\" });\r\n    AppContext appCtx = mock(AppContext.class);\r\n    ClusterInfo clusterInfo = mock(ClusterInfo.class);\r\n    when(appCtx.getClusterInfo()).thenReturn(clusterInfo);\r\n    setupTaskAttemptFinishingMonitor(eventHandler, jobConf, appCtx);\r\n    TaskAttemptImpl taImpl = new MapTaskAttemptImpl(taskId, 1, eventHandler, jobFile, 1, splits, jobConf, taListener, mock(Token.class), new Credentials(), SystemClock.getInstance(), appCtx);\r\n    NodeId nid = NodeId.newInstance(\"127.0.0.1\", 0);\r\n    ContainerId contId = ContainerId.newContainerId(appAttemptId, 3);\r\n    Container container = mock(Container.class);\r\n    when(container.getId()).thenReturn(contId);\r\n    when(container.getNodeId()).thenReturn(nid);\r\n    when(container.getNodeHttpAddress()).thenReturn(\"localhost:0\");\r\n    taImpl.handle(new TaskAttemptEvent(attemptId, TaskAttemptEventType.TA_SCHEDULE));\r\n    taImpl.handle(new TaskAttemptContainerAssignedEvent(attemptId, container, mock(Map.class)));\r\n    taImpl.handle(new TaskAttemptContainerLaunchedEvent(attemptId, 0));\r\n    taImpl.handle(new TaskAttemptEvent(attemptId, TaskAttemptEventType.TA_DONE));\r\n    taImpl.handle(new TaskAttemptEvent(attemptId, TaskAttemptEventType.TA_CONTAINER_COMPLETED));\r\n    assertThat(taImpl.getState()).withFailMessage(\"Task attempt is not in SUCCEEDED state\").isEqualTo(TaskAttemptState.SUCCEEDED);\r\n    assertTrue(\"Task Attempt finish time is not greater than 0\", taImpl.getFinishTime() > 0);\r\n    Long finishTime = taImpl.getFinishTime();\r\n    Thread.sleep(5);\r\n    taImpl.handle(new TaskAttemptTooManyFetchFailureEvent(attemptId, reduceTAId, \"Host\"));\r\n    assertThat(taImpl.getState()).withFailMessage(\"Task attempt is not in FAILED state\").isEqualTo(TaskAttemptState.FAILED);\r\n    assertEquals(\"After TA_TOO_MANY_FETCH_FAILURE,\" + \" Task attempt finish time is not the same \", finishTime, Long.valueOf(taImpl.getFinishTime()));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\job\\impl",
  "methodName" : "containerKillBeforeAssignment",
  "errType" : null,
  "containingMethodsNum" : 16,
  "sourceCodeText" : "void containerKillBeforeAssignment(boolean scheduleAttempt) throws Exception\n{\r\n    MockEventHandler eventHandler = new MockEventHandler();\r\n    ApplicationId appId = ApplicationId.newInstance(1, 2);\r\n    JobId jobId = MRBuilderUtils.newJobId(appId, 1);\r\n    TaskId taskId = MRBuilderUtils.newTaskId(jobId, 1, TaskType.MAP);\r\n    TaskAttemptImpl taImpl = new MapTaskAttemptImpl(taskId, 1, eventHandler, mock(Path.class), 1, mock(TaskSplitMetaInfo.class), new JobConf(), mock(TaskAttemptListener.class), mock(Token.class), new Credentials(), SystemClock.getInstance(), mock(AppContext.class));\r\n    if (scheduleAttempt) {\r\n        taImpl.handle(new TaskAttemptEvent(taImpl.getID(), TaskAttemptEventType.TA_SCHEDULE));\r\n    }\r\n    taImpl.handle(new TaskAttemptKillEvent(taImpl.getID(), \"\", true));\r\n    assertThat(taImpl.getState()).withFailMessage(\"Task attempt is not in KILLED state\").isEqualTo(TaskAttemptState.KILLED);\r\n    assertThat(taImpl.getInternalState()).withFailMessage(\"Task attempt's internal state is not KILLED\").isEqualTo(TaskAttemptStateInternal.KILLED);\r\n    assertFalse(\"InternalError occurred\", eventHandler.internalError);\r\n    TaskEvent event = eventHandler.lastTaskEvent;\r\n    assertEquals(TaskEventType.T_ATTEMPT_KILLED, event.getType());\r\n    assertFalse(((TaskTAttemptKilledEvent) event).getRescheduleAttempt());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\job\\impl",
  "methodName" : "testContainerKillOnNew",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testContainerKillOnNew() throws Exception\n{\r\n    containerKillBeforeAssignment(false);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\job\\impl",
  "methodName" : "testContainerKillOnUnassigned",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testContainerKillOnUnassigned() throws Exception\n{\r\n    containerKillBeforeAssignment(true);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\job\\impl",
  "methodName" : "testContainerKillAfterAssigned",
  "errType" : null,
  "containingMethodsNum" : 31,
  "sourceCodeText" : "void testContainerKillAfterAssigned() throws Exception\n{\r\n    ApplicationId appId = ApplicationId.newInstance(1, 2);\r\n    ApplicationAttemptId appAttemptId = ApplicationAttemptId.newInstance(appId, 0);\r\n    JobId jobId = MRBuilderUtils.newJobId(appId, 1);\r\n    TaskId taskId = MRBuilderUtils.newTaskId(jobId, 1, TaskType.MAP);\r\n    TaskAttemptId attemptId = MRBuilderUtils.newTaskAttemptId(taskId, 0);\r\n    Path jobFile = mock(Path.class);\r\n    MockEventHandler eventHandler = new MockEventHandler();\r\n    TaskAttemptListener taListener = mock(TaskAttemptListener.class);\r\n    when(taListener.getAddress()).thenReturn(new InetSocketAddress(\"localhost\", 0));\r\n    JobConf jobConf = new JobConf();\r\n    jobConf.setClass(\"fs.file.impl\", StubbedFS.class, FileSystem.class);\r\n    jobConf.setBoolean(\"fs.file.impl.disable.cache\", true);\r\n    jobConf.set(JobConf.MAPRED_MAP_TASK_ENV, \"\");\r\n    jobConf.set(MRJobConfig.APPLICATION_ATTEMPT_ID, \"10\");\r\n    TaskSplitMetaInfo splits = mock(TaskSplitMetaInfo.class);\r\n    when(splits.getLocations()).thenReturn(new String[] { \"127.0.0.1\" });\r\n    AppContext appCtx = mock(AppContext.class);\r\n    ClusterInfo clusterInfo = mock(ClusterInfo.class);\r\n    Resource resource = mock(Resource.class);\r\n    when(appCtx.getClusterInfo()).thenReturn(clusterInfo);\r\n    when(resource.getMemorySize()).thenReturn(1024L);\r\n    TaskAttemptImpl taImpl = new MapTaskAttemptImpl(taskId, 1, eventHandler, jobFile, 1, splits, jobConf, taListener, new Token(), new Credentials(), SystemClock.getInstance(), appCtx);\r\n    NodeId nid = NodeId.newInstance(\"127.0.0.2\", 0);\r\n    ContainerId contId = ContainerId.newContainerId(appAttemptId, 3);\r\n    Container container = mock(Container.class);\r\n    when(container.getId()).thenReturn(contId);\r\n    when(container.getNodeId()).thenReturn(nid);\r\n    when(container.getNodeHttpAddress()).thenReturn(\"localhost:0\");\r\n    taImpl.handle(new TaskAttemptEvent(attemptId, TaskAttemptEventType.TA_SCHEDULE));\r\n    taImpl.handle(new TaskAttemptContainerAssignedEvent(attemptId, container, mock(Map.class)));\r\n    assertThat(taImpl.getInternalState()).withFailMessage(\"Task attempt is not in ASSIGNED state\").isEqualTo(TaskAttemptStateInternal.ASSIGNED);\r\n    taImpl.handle(new TaskAttemptEvent(attemptId, TaskAttemptEventType.TA_KILL));\r\n    assertThat(taImpl.getInternalState()).withFailMessage(\"Task should be in KILL_CONTAINER_CLEANUP state\").isEqualTo(TaskAttemptStateInternal.KILL_CONTAINER_CLEANUP);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\job\\impl",
  "methodName" : "testContainerKillWhileRunning",
  "errType" : null,
  "containingMethodsNum" : 33,
  "sourceCodeText" : "void testContainerKillWhileRunning() throws Exception\n{\r\n    ApplicationId appId = ApplicationId.newInstance(1, 2);\r\n    ApplicationAttemptId appAttemptId = ApplicationAttemptId.newInstance(appId, 0);\r\n    JobId jobId = MRBuilderUtils.newJobId(appId, 1);\r\n    TaskId taskId = MRBuilderUtils.newTaskId(jobId, 1, TaskType.MAP);\r\n    TaskAttemptId attemptId = MRBuilderUtils.newTaskAttemptId(taskId, 0);\r\n    Path jobFile = mock(Path.class);\r\n    MockEventHandler eventHandler = new MockEventHandler();\r\n    TaskAttemptListener taListener = mock(TaskAttemptListener.class);\r\n    when(taListener.getAddress()).thenReturn(new InetSocketAddress(\"localhost\", 0));\r\n    JobConf jobConf = new JobConf();\r\n    jobConf.setClass(\"fs.file.impl\", StubbedFS.class, FileSystem.class);\r\n    jobConf.setBoolean(\"fs.file.impl.disable.cache\", true);\r\n    jobConf.set(JobConf.MAPRED_MAP_TASK_ENV, \"\");\r\n    jobConf.set(MRJobConfig.APPLICATION_ATTEMPT_ID, \"10\");\r\n    TaskSplitMetaInfo splits = mock(TaskSplitMetaInfo.class);\r\n    when(splits.getLocations()).thenReturn(new String[] { \"127.0.0.1\" });\r\n    AppContext appCtx = mock(AppContext.class);\r\n    ClusterInfo clusterInfo = mock(ClusterInfo.class);\r\n    Resource resource = mock(Resource.class);\r\n    when(appCtx.getClusterInfo()).thenReturn(clusterInfo);\r\n    when(resource.getMemorySize()).thenReturn(1024L);\r\n    TaskAttemptImpl taImpl = new MapTaskAttemptImpl(taskId, 1, eventHandler, jobFile, 1, splits, jobConf, taListener, new Token(), new Credentials(), SystemClock.getInstance(), appCtx);\r\n    NodeId nid = NodeId.newInstance(\"127.0.0.2\", 0);\r\n    ContainerId contId = ContainerId.newContainerId(appAttemptId, 3);\r\n    Container container = mock(Container.class);\r\n    when(container.getId()).thenReturn(contId);\r\n    when(container.getNodeId()).thenReturn(nid);\r\n    when(container.getNodeHttpAddress()).thenReturn(\"localhost:0\");\r\n    taImpl.handle(new TaskAttemptEvent(attemptId, TaskAttemptEventType.TA_SCHEDULE));\r\n    taImpl.handle(new TaskAttemptContainerAssignedEvent(attemptId, container, mock(Map.class)));\r\n    taImpl.handle(new TaskAttemptContainerLaunchedEvent(attemptId, 0));\r\n    assertThat(taImpl.getState()).withFailMessage(\"Task attempt is not in RUNNING state\").isEqualTo(TaskAttemptState.RUNNING);\r\n    taImpl.handle(new TaskAttemptEvent(attemptId, TaskAttemptEventType.TA_KILL));\r\n    assertFalse(\"InternalError occurred trying to handle TA_KILL\", eventHandler.internalError);\r\n    assertThat(taImpl.getInternalState()).withFailMessage(\"Task should be in KILL_CONTAINER_CLEANUP state\").isEqualTo(TaskAttemptStateInternal.KILL_CONTAINER_CLEANUP);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\job\\impl",
  "methodName" : "testContainerKillWhileCommitPending",
  "errType" : null,
  "containingMethodsNum" : 35,
  "sourceCodeText" : "void testContainerKillWhileCommitPending() throws Exception\n{\r\n    ApplicationId appId = ApplicationId.newInstance(1, 2);\r\n    ApplicationAttemptId appAttemptId = ApplicationAttemptId.newInstance(appId, 0);\r\n    JobId jobId = MRBuilderUtils.newJobId(appId, 1);\r\n    TaskId taskId = MRBuilderUtils.newTaskId(jobId, 1, TaskType.MAP);\r\n    TaskAttemptId attemptId = MRBuilderUtils.newTaskAttemptId(taskId, 0);\r\n    Path jobFile = mock(Path.class);\r\n    MockEventHandler eventHandler = new MockEventHandler();\r\n    TaskAttemptListener taListener = mock(TaskAttemptListener.class);\r\n    when(taListener.getAddress()).thenReturn(new InetSocketAddress(\"localhost\", 0));\r\n    JobConf jobConf = new JobConf();\r\n    jobConf.setClass(\"fs.file.impl\", StubbedFS.class, FileSystem.class);\r\n    jobConf.setBoolean(\"fs.file.impl.disable.cache\", true);\r\n    jobConf.set(JobConf.MAPRED_MAP_TASK_ENV, \"\");\r\n    jobConf.set(MRJobConfig.APPLICATION_ATTEMPT_ID, \"10\");\r\n    TaskSplitMetaInfo splits = mock(TaskSplitMetaInfo.class);\r\n    when(splits.getLocations()).thenReturn(new String[] { \"127.0.0.1\" });\r\n    AppContext appCtx = mock(AppContext.class);\r\n    ClusterInfo clusterInfo = mock(ClusterInfo.class);\r\n    Resource resource = mock(Resource.class);\r\n    when(appCtx.getClusterInfo()).thenReturn(clusterInfo);\r\n    when(resource.getMemorySize()).thenReturn(1024L);\r\n    TaskAttemptImpl taImpl = new MapTaskAttemptImpl(taskId, 1, eventHandler, jobFile, 1, splits, jobConf, taListener, new Token(), new Credentials(), SystemClock.getInstance(), appCtx);\r\n    NodeId nid = NodeId.newInstance(\"127.0.0.2\", 0);\r\n    ContainerId contId = ContainerId.newContainerId(appAttemptId, 3);\r\n    Container container = mock(Container.class);\r\n    when(container.getId()).thenReturn(contId);\r\n    when(container.getNodeId()).thenReturn(nid);\r\n    when(container.getNodeHttpAddress()).thenReturn(\"localhost:0\");\r\n    taImpl.handle(new TaskAttemptEvent(attemptId, TaskAttemptEventType.TA_SCHEDULE));\r\n    taImpl.handle(new TaskAttemptContainerAssignedEvent(attemptId, container, mock(Map.class)));\r\n    taImpl.handle(new TaskAttemptContainerLaunchedEvent(attemptId, 0));\r\n    assertThat(taImpl.getState()).withFailMessage(\"Task attempt is not in RUNNING state\").isEqualTo(TaskAttemptState.RUNNING);\r\n    taImpl.handle(new TaskAttemptEvent(attemptId, TaskAttemptEventType.TA_COMMIT_PENDING));\r\n    assertThat(taImpl.getInternalState()).withFailMessage(\"Task should be in COMMIT_PENDING state\").isEqualTo(TaskAttemptStateInternal.COMMIT_PENDING);\r\n    taImpl.handle(new TaskAttemptEvent(attemptId, TaskAttemptEventType.TA_KILL));\r\n    assertFalse(\"InternalError occurred trying to handle TA_KILL\", eventHandler.internalError);\r\n    assertThat(taImpl.getInternalState()).withFailMessage(\"Task should be in KILL_CONTAINER_CLEANUP state\").isEqualTo(TaskAttemptStateInternal.KILL_CONTAINER_CLEANUP);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\job\\impl",
  "methodName" : "testKillMapTaskWhileSuccessFinishing",
  "errType" : null,
  "containingMethodsNum" : 12,
  "sourceCodeText" : "void testKillMapTaskWhileSuccessFinishing() throws Exception\n{\r\n    MockEventHandler eventHandler = new MockEventHandler();\r\n    TaskAttemptImpl taImpl = createTaskAttemptImpl(eventHandler);\r\n    taImpl.handle(new TaskAttemptEvent(taImpl.getID(), TaskAttemptEventType.TA_DONE));\r\n    assertThat(taImpl.getState()).withFailMessage(\"Task attempt is not in SUCCEEDED state\").isEqualTo(TaskAttemptState.SUCCEEDED);\r\n    assertThat(taImpl.getInternalState()).withFailMessage(\"Task attempt's internal state is not SUCCESS_FINISHING_CONTAINER\").isEqualTo(TaskAttemptStateInternal.SUCCESS_FINISHING_CONTAINER);\r\n    taImpl.handle(new TaskAttemptEvent(taImpl.getID(), TaskAttemptEventType.TA_KILL));\r\n    assertThat(taImpl.getState()).withFailMessage(\"Task attempt is not in KILLED state\").isEqualTo(TaskAttemptState.KILLED);\r\n    assertThat(taImpl.getInternalState()).withFailMessage(\"Task attempt's internal state is not KILL_CONTAINER_CLEANUP\").isEqualTo(TaskAttemptStateInternal.KILL_CONTAINER_CLEANUP);\r\n    taImpl.handle(new TaskAttemptEvent(taImpl.getID(), TaskAttemptEventType.TA_CONTAINER_CLEANED));\r\n    assertThat(taImpl.getInternalState()).withFailMessage(\"Task attempt's internal state is not KILL_TASK_CLEANUP\").isEqualTo(TaskAttemptStateInternal.KILL_TASK_CLEANUP);\r\n    taImpl.handle(new TaskAttemptEvent(taImpl.getID(), TaskAttemptEventType.TA_CLEANUP_DONE));\r\n    assertThat(taImpl.getState()).withFailMessage(\"Task attempt is not in KILLED state\").isEqualTo(TaskAttemptState.KILLED);\r\n    assertFalse(\"InternalError occurred\", eventHandler.internalError);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\job\\impl",
  "methodName" : "testKillMapOnlyTaskWhileSuccessFinishing",
  "errType" : null,
  "containingMethodsNum" : 11,
  "sourceCodeText" : "void testKillMapOnlyTaskWhileSuccessFinishing() throws Exception\n{\r\n    MockEventHandler eventHandler = new MockEventHandler();\r\n    TaskAttemptImpl taImpl = createMapOnlyTaskAttemptImpl(eventHandler);\r\n    taImpl.handle(new TaskAttemptEvent(taImpl.getID(), TaskAttemptEventType.TA_DONE));\r\n    assertEquals(\"Task attempt is not in SUCCEEDED state\", TaskAttemptState.SUCCEEDED, taImpl.getState());\r\n    assertEquals(\"Task attempt's internal state is not \" + \"SUCCESS_FINISHING_CONTAINER\", TaskAttemptStateInternal.SUCCESS_FINISHING_CONTAINER, taImpl.getInternalState());\r\n    taImpl.handle(new TaskAttemptEvent(taImpl.getID(), TaskAttemptEventType.TA_KILL));\r\n    assertEquals(\"Task attempt is not in SUCCEEDED state\", TaskAttemptState.SUCCEEDED, taImpl.getState());\r\n    assertEquals(\"Task attempt's internal state is not \" + \"SUCCESS_CONTAINER_CLEANUP\", TaskAttemptStateInternal.SUCCESS_CONTAINER_CLEANUP, taImpl.getInternalState());\r\n    taImpl.handle(new TaskAttemptEvent(taImpl.getID(), TaskAttemptEventType.TA_CONTAINER_CLEANED));\r\n    assertEquals(\"Task attempt is not in SUCCEEDED state\", TaskAttemptState.SUCCEEDED, taImpl.getState());\r\n    assertEquals(\"Task attempt's internal state is not SUCCEEDED state\", TaskAttemptStateInternal.SUCCEEDED, taImpl.getInternalState());\r\n    assertFalse(\"InternalError occurred\", eventHandler.internalError);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\job\\impl",
  "methodName" : "testKillMapTaskAfterSuccess",
  "errType" : null,
  "containingMethodsNum" : 11,
  "sourceCodeText" : "void testKillMapTaskAfterSuccess() throws Exception\n{\r\n    MockEventHandler eventHandler = new MockEventHandler();\r\n    TaskAttemptImpl taImpl = createTaskAttemptImpl(eventHandler);\r\n    taImpl.handle(new TaskAttemptEvent(taImpl.getID(), TaskAttemptEventType.TA_DONE));\r\n    assertThat(taImpl.getState()).withFailMessage(\"Task attempt is not in SUCCEEDED state\").isEqualTo(TaskAttemptState.SUCCEEDED);\r\n    assertThat(taImpl.getInternalState()).withFailMessage(\"Task attempt's internal state is not \" + \"SUCCESS_FINISHING_CONTAINER\").isEqualTo(TaskAttemptStateInternal.SUCCESS_FINISHING_CONTAINER);\r\n    taImpl.handle(new TaskAttemptEvent(taImpl.getID(), TaskAttemptEventType.TA_CONTAINER_CLEANED));\r\n    taImpl.handle(new TaskAttemptKillEvent(taImpl.getID(), \"\", true));\r\n    assertThat(taImpl.getState()).withFailMessage(\"Task attempt is not in KILLED state\").isEqualTo(TaskAttemptState.KILLED);\r\n    assertThat(taImpl.getInternalState()).withFailMessage(\"Task attempt's internal state is not KILLED\").isEqualTo(TaskAttemptStateInternal.KILLED);\r\n    assertFalse(\"InternalError occurred\", eventHandler.internalError);\r\n    TaskEvent event = eventHandler.lastTaskEvent;\r\n    assertEquals(TaskEventType.T_ATTEMPT_KILLED, event.getType());\r\n    assertTrue(((TaskTAttemptKilledEvent) event).getRescheduleAttempt());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\job\\impl",
  "methodName" : "testKillMapOnlyTaskAfterSuccess",
  "errType" : null,
  "containingMethodsNum" : 10,
  "sourceCodeText" : "void testKillMapOnlyTaskAfterSuccess() throws Exception\n{\r\n    MockEventHandler eventHandler = new MockEventHandler();\r\n    TaskAttemptImpl taImpl = createMapOnlyTaskAttemptImpl(eventHandler);\r\n    taImpl.handle(new TaskAttemptEvent(taImpl.getID(), TaskAttemptEventType.TA_DONE));\r\n    assertEquals(\"Task attempt is not in SUCCEEDED state\", TaskAttemptState.SUCCEEDED, taImpl.getState());\r\n    assertEquals(\"Task attempt's internal state is not \" + \"SUCCESS_FINISHING_CONTAINER\", TaskAttemptStateInternal.SUCCESS_FINISHING_CONTAINER, taImpl.getInternalState());\r\n    taImpl.handle(new TaskAttemptEvent(taImpl.getID(), TaskAttemptEventType.TA_CONTAINER_CLEANED));\r\n    taImpl.handle(new TaskAttemptKillEvent(taImpl.getID(), \"\", true));\r\n    assertEquals(\"Task attempt is not in SUCCEEDED state\", TaskAttemptState.SUCCEEDED, taImpl.getState());\r\n    assertEquals(\"Task attempt's internal state is not SUCCEEDED\", TaskAttemptStateInternal.SUCCEEDED, taImpl.getInternalState());\r\n    assertFalse(\"InternalError occurred\", eventHandler.internalError);\r\n    TaskEvent event = eventHandler.lastTaskEvent;\r\n    assertEquals(TaskEventType.T_ATTEMPT_SUCCEEDED, event.getType());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\job\\impl",
  "methodName" : "testKillMapTaskWhileFailFinishing",
  "errType" : null,
  "containingMethodsNum" : 14,
  "sourceCodeText" : "void testKillMapTaskWhileFailFinishing() throws Exception\n{\r\n    MockEventHandler eventHandler = new MockEventHandler();\r\n    TaskAttemptImpl taImpl = createTaskAttemptImpl(eventHandler);\r\n    taImpl.handle(new TaskAttemptFailEvent(taImpl.getID()));\r\n    assertThat(taImpl.getState()).withFailMessage(\"Task attempt is not in FAILED state\").isEqualTo(TaskAttemptState.FAILED);\r\n    assertThat(taImpl.getInternalState()).withFailMessage(\"Task attempt's internal state is not \" + \"FAIL_FINISHING_CONTAINER\").isEqualTo(TaskAttemptStateInternal.FAIL_FINISHING_CONTAINER);\r\n    taImpl.handle(new TaskAttemptEvent(taImpl.getID(), TaskAttemptEventType.TA_KILL));\r\n    assertThat(taImpl.getState()).withFailMessage(\"Task attempt is not in FAILED state\").isEqualTo(TaskAttemptState.FAILED);\r\n    assertThat(taImpl.getInternalState()).withFailMessage(\"Task attempt's internal state is not \" + \"FAIL_FINISHING_CONTAINER\").isEqualTo(TaskAttemptStateInternal.FAIL_FINISHING_CONTAINER);\r\n    taImpl.handle(new TaskAttemptEvent(taImpl.getID(), TaskAttemptEventType.TA_TIMED_OUT));\r\n    assertThat(taImpl.getInternalState()).withFailMessage(\"Task attempt's internal state is not \" + \"FAIL_CONTAINER_CLEANUP\").isEqualTo(TaskAttemptStateInternal.FAIL_CONTAINER_CLEANUP);\r\n    taImpl.handle(new TaskAttemptEvent(taImpl.getID(), TaskAttemptEventType.TA_CONTAINER_CLEANED));\r\n    assertThat(taImpl.getInternalState()).withFailMessage(\"Task attempt's internal state is not \" + \"FAIL_TASK_CLEANUP\").isEqualTo(TaskAttemptStateInternal.FAIL_TASK_CLEANUP);\r\n    taImpl.handle(new TaskAttemptEvent(taImpl.getID(), TaskAttemptEventType.TA_CLEANUP_DONE));\r\n    assertThat(taImpl.getState()).withFailMessage(\"Task attempt is not in FAILED state\").isEqualTo(TaskAttemptState.FAILED);\r\n    assertFalse(\"InternalError occurred\", eventHandler.internalError);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\job\\impl",
  "methodName" : "testFailMapTaskByClient",
  "errType" : null,
  "containingMethodsNum" : 9,
  "sourceCodeText" : "void testFailMapTaskByClient() throws Exception\n{\r\n    MockEventHandler eventHandler = new MockEventHandler();\r\n    TaskAttemptImpl taImpl = createTaskAttemptImpl(eventHandler);\r\n    taImpl.handle(new TaskAttemptEvent(taImpl.getID(), TaskAttemptEventType.TA_FAILMSG_BY_CLIENT));\r\n    assertThat(taImpl.getState()).withFailMessage(\"Task attempt is not in FAILED state\").isEqualTo(TaskAttemptState.FAILED);\r\n    assertThat(taImpl.getInternalState()).withFailMessage(\"Task attempt's internal state is not \" + \"FAIL_CONTAINER_CLEANUP\").isEqualTo(TaskAttemptStateInternal.FAIL_CONTAINER_CLEANUP);\r\n    taImpl.handle(new TaskAttemptEvent(taImpl.getID(), TaskAttemptEventType.TA_CONTAINER_CLEANED));\r\n    assertThat(taImpl.getInternalState()).withFailMessage(\"Task attempt's internal state is not \" + \"FAIL_TASK_CLEANUP\").isEqualTo(TaskAttemptStateInternal.FAIL_TASK_CLEANUP);\r\n    taImpl.handle(new TaskAttemptEvent(taImpl.getID(), TaskAttemptEventType.TA_CLEANUP_DONE));\r\n    assertThat(taImpl.getState()).withFailMessage(\"Task attempt is not in FAILED state\").isEqualTo(TaskAttemptState.FAILED);\r\n    assertFalse(\"InternalError occurred\", eventHandler.internalError);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\job\\impl",
  "methodName" : "testTaskAttemptDiagnosticEventOnFinishing",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void testTaskAttemptDiagnosticEventOnFinishing() throws Exception\n{\r\n    MockEventHandler eventHandler = new MockEventHandler();\r\n    TaskAttemptImpl taImpl = createTaskAttemptImpl(eventHandler);\r\n    taImpl.handle(new TaskAttemptEvent(taImpl.getID(), TaskAttemptEventType.TA_DONE));\r\n    assertThat(taImpl.getState()).withFailMessage(\"Task attempt is not in SUCCEEDED state\").isEqualTo(TaskAttemptState.SUCCEEDED);\r\n    assertThat(taImpl.getInternalState()).withFailMessage(\"Task attempt's internal state is not \" + \"SUCCESS_FINISHING_CONTAINER\").isEqualTo(TaskAttemptStateInternal.SUCCESS_FINISHING_CONTAINER);\r\n    taImpl.handle(new TaskAttemptDiagnosticsUpdateEvent(taImpl.getID(), \"Task got updated\"));\r\n    assertThat(taImpl.getState()).withFailMessage(\"Task attempt is not in SUCCEEDED state\").isEqualTo(TaskAttemptState.SUCCEEDED);\r\n    assertThat(taImpl.getInternalState()).withFailMessage(\"Task attempt's internal state is not \" + \"SUCCESS_FINISHING_CONTAINER\").isEqualTo(TaskAttemptStateInternal.SUCCESS_FINISHING_CONTAINER);\r\n    assertFalse(\"InternalError occurred\", eventHandler.internalError);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\job\\impl",
  "methodName" : "testTimeoutWhileSuccessFinishing",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void testTimeoutWhileSuccessFinishing() throws Exception\n{\r\n    MockEventHandler eventHandler = new MockEventHandler();\r\n    TaskAttemptImpl taImpl = createTaskAttemptImpl(eventHandler);\r\n    taImpl.handle(new TaskAttemptEvent(taImpl.getID(), TaskAttemptEventType.TA_DONE));\r\n    assertThat(taImpl.getState()).withFailMessage(\"Task attempt is not in SUCCEEDED state\").isEqualTo(TaskAttemptState.SUCCEEDED);\r\n    assertThat(taImpl.getInternalState()).withFailMessage(\"Task attempt's internal state is not \" + \"SUCCESS_FINISHING_CONTAINER\").isEqualTo(TaskAttemptStateInternal.SUCCESS_FINISHING_CONTAINER);\r\n    taImpl.handle(new TaskAttemptEvent(taImpl.getID(), TaskAttemptEventType.TA_TIMED_OUT));\r\n    assertThat(taImpl.getState()).withFailMessage(\"Task attempt is not in SUCCEEDED state\").isEqualTo(TaskAttemptState.SUCCEEDED);\r\n    assertThat(taImpl.getInternalState()).withFailMessage(\"Task attempt's internal state is not \" + \"SUCCESS_CONTAINER_CLEANUP\").isEqualTo(TaskAttemptStateInternal.SUCCESS_CONTAINER_CLEANUP);\r\n    assertFalse(\"InternalError occurred\", eventHandler.internalError);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\job\\impl",
  "methodName" : "testTimeoutWhileFailFinishing",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void testTimeoutWhileFailFinishing() throws Exception\n{\r\n    MockEventHandler eventHandler = new MockEventHandler();\r\n    TaskAttemptImpl taImpl = createTaskAttemptImpl(eventHandler);\r\n    taImpl.handle(new TaskAttemptFailEvent(taImpl.getID()));\r\n    assertThat(taImpl.getState()).withFailMessage(\"Task attempt is not in FAILED state\").isEqualTo(TaskAttemptState.FAILED);\r\n    assertThat(taImpl.getInternalState()).withFailMessage(\"Task attempt's internal state is not \" + \"FAIL_FINISHING_CONTAINER\").isEqualTo(TaskAttemptStateInternal.FAIL_FINISHING_CONTAINER);\r\n    taImpl.handle(new TaskAttemptEvent(taImpl.getID(), TaskAttemptEventType.TA_TIMED_OUT));\r\n    assertThat(taImpl.getInternalState()).withFailMessage(\"Task attempt's internal state is not \" + \"FAIL_CONTAINER_CLEANUP\").isEqualTo(TaskAttemptStateInternal.FAIL_CONTAINER_CLEANUP);\r\n    assertFalse(\"InternalError occurred\", eventHandler.internalError);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\job\\impl",
  "methodName" : "testMapperCustomResourceTypes",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void testMapperCustomResourceTypes()\n{\r\n    initResourceTypes();\r\n    EventHandler eventHandler = mock(EventHandler.class);\r\n    TaskSplitMetaInfo taskSplitMetaInfo = new TaskSplitMetaInfo();\r\n    Clock clock = SystemClock.getInstance();\r\n    JobConf jobConf = new JobConf();\r\n    jobConf.setLong(MRJobConfig.MAP_RESOURCE_TYPE_PREFIX + CUSTOM_RESOURCE_NAME, 7L);\r\n    TaskAttemptImpl taImpl = createMapTaskAttemptImplForTest(eventHandler, taskSplitMetaInfo, clock, jobConf);\r\n    ResourceInformation resourceInfo = getResourceInfoFromContainerRequest(taImpl, eventHandler).getResourceInformation(CUSTOM_RESOURCE_NAME);\r\n    assertEquals(\"Expecting the default unit (G)\", \"G\", resourceInfo.getUnits());\r\n    assertEquals(7L, resourceInfo.getValue());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\job\\impl",
  "methodName" : "testReducerCustomResourceTypes",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void testReducerCustomResourceTypes()\n{\r\n    initResourceTypes();\r\n    EventHandler eventHandler = mock(EventHandler.class);\r\n    Clock clock = SystemClock.getInstance();\r\n    JobConf jobConf = new JobConf();\r\n    jobConf.set(MRJobConfig.REDUCE_RESOURCE_TYPE_PREFIX + CUSTOM_RESOURCE_NAME, \"3m\");\r\n    TaskAttemptImpl taImpl = createReduceTaskAttemptImplForTest(eventHandler, clock, jobConf);\r\n    ResourceInformation resourceInfo = getResourceInfoFromContainerRequest(taImpl, eventHandler).getResourceInformation(CUSTOM_RESOURCE_NAME);\r\n    assertEquals(\"Expecting the specified unit (m)\", \"m\", resourceInfo.getUnits());\r\n    assertEquals(3L, resourceInfo.getValue());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\job\\impl",
  "methodName" : "testReducerMemoryRequestViaMapreduceReduceMemoryMb",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void testReducerMemoryRequestViaMapreduceReduceMemoryMb()\n{\r\n    EventHandler eventHandler = mock(EventHandler.class);\r\n    Clock clock = SystemClock.getInstance();\r\n    JobConf jobConf = new JobConf();\r\n    jobConf.setInt(MRJobConfig.REDUCE_MEMORY_MB, 2048);\r\n    TaskAttemptImpl taImpl = createReduceTaskAttemptImplForTest(eventHandler, clock, jobConf);\r\n    long memorySize = getResourceInfoFromContainerRequest(taImpl, eventHandler).getMemorySize();\r\n    assertEquals(2048, memorySize);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\job\\impl",
  "methodName" : "testReducerMemoryRequestViaMapreduceReduceResourceMemory",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void testReducerMemoryRequestViaMapreduceReduceResourceMemory()\n{\r\n    EventHandler eventHandler = mock(EventHandler.class);\r\n    Clock clock = SystemClock.getInstance();\r\n    JobConf jobConf = new JobConf();\r\n    jobConf.set(MRJobConfig.REDUCE_RESOURCE_TYPE_PREFIX + MRJobConfig.RESOURCE_TYPE_NAME_MEMORY, \"2 Gi\");\r\n    TaskAttemptImpl taImpl = createReduceTaskAttemptImplForTest(eventHandler, clock, jobConf);\r\n    long memorySize = getResourceInfoFromContainerRequest(taImpl, eventHandler).getMemorySize();\r\n    assertEquals(2048, memorySize);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\job\\impl",
  "methodName" : "testReducerMemoryRequestDefaultMemory",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void testReducerMemoryRequestDefaultMemory()\n{\r\n    EventHandler eventHandler = mock(EventHandler.class);\r\n    Clock clock = SystemClock.getInstance();\r\n    TaskAttemptImpl taImpl = createReduceTaskAttemptImplForTest(eventHandler, clock, new JobConf());\r\n    long memorySize = getResourceInfoFromContainerRequest(taImpl, eventHandler).getMemorySize();\r\n    assertEquals(MRJobConfig.DEFAULT_REDUCE_MEMORY_MB, memorySize);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\job\\impl",
  "methodName" : "testReducerMemoryRequestWithoutUnits",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void testReducerMemoryRequestWithoutUnits()\n{\r\n    Clock clock = SystemClock.getInstance();\r\n    for (String memoryResourceName : ImmutableList.of(MRJobConfig.RESOURCE_TYPE_NAME_MEMORY, MRJobConfig.RESOURCE_TYPE_ALTERNATIVE_NAME_MEMORY)) {\r\n        EventHandler eventHandler = mock(EventHandler.class);\r\n        JobConf jobConf = new JobConf();\r\n        jobConf.setInt(MRJobConfig.REDUCE_RESOURCE_TYPE_PREFIX + memoryResourceName, 2048);\r\n        TaskAttemptImpl taImpl = createReduceTaskAttemptImplForTest(eventHandler, clock, jobConf);\r\n        long memorySize = getResourceInfoFromContainerRequest(taImpl, eventHandler).getMemorySize();\r\n        assertEquals(2048, memorySize);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\job\\impl",
  "methodName" : "testReducerMemoryRequestOverriding",
  "errType" : null,
  "containingMethodsNum" : 13,
  "sourceCodeText" : "void testReducerMemoryRequestOverriding()\n{\r\n    for (String memoryName : ImmutableList.of(MRJobConfig.RESOURCE_TYPE_NAME_MEMORY, MRJobConfig.RESOURCE_TYPE_ALTERNATIVE_NAME_MEMORY)) {\r\n        TestAppender testAppender = new TestAppender();\r\n        final Logger logger = Logger.getLogger(TaskAttemptImpl.class);\r\n        try {\r\n            TaskAttemptImpl.RESOURCE_REQUEST_CACHE.clear();\r\n            logger.addAppender(testAppender);\r\n            EventHandler eventHandler = mock(EventHandler.class);\r\n            Clock clock = SystemClock.getInstance();\r\n            JobConf jobConf = new JobConf();\r\n            jobConf.set(MRJobConfig.REDUCE_RESOURCE_TYPE_PREFIX + memoryName, \"3Gi\");\r\n            jobConf.setInt(MRJobConfig.REDUCE_MEMORY_MB, 2048);\r\n            TaskAttemptImpl taImpl = createReduceTaskAttemptImplForTest(eventHandler, clock, jobConf);\r\n            long memorySize = getResourceInfoFromContainerRequest(taImpl, eventHandler).getMemorySize();\r\n            assertEquals(3072, memorySize);\r\n            assertTrue(testAppender.getLogEvents().stream().anyMatch(e -> e.getLevel() == Level.WARN && (\"Configuration \" + \"mapreduce.reduce.resource.\" + memoryName + \"=3Gi is \" + \"overriding the mapreduce.reduce.memory.mb=2048 configuration\").equals(e.getMessage())));\r\n        } finally {\r\n            logger.removeAppender(testAppender);\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\job\\impl",
  "methodName" : "testReducerMemoryRequestMultipleName",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void testReducerMemoryRequestMultipleName()\n{\r\n    EventHandler eventHandler = mock(EventHandler.class);\r\n    Clock clock = SystemClock.getInstance();\r\n    JobConf jobConf = new JobConf();\r\n    for (String memoryName : ImmutableList.of(MRJobConfig.RESOURCE_TYPE_NAME_MEMORY, MRJobConfig.RESOURCE_TYPE_ALTERNATIVE_NAME_MEMORY)) {\r\n        jobConf.set(MRJobConfig.REDUCE_RESOURCE_TYPE_PREFIX + memoryName, \"3Gi\");\r\n    }\r\n    createReduceTaskAttemptImplForTest(eventHandler, clock, jobConf);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\job\\impl",
  "methodName" : "testReducerCpuRequestViaMapreduceReduceCpuVcores",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void testReducerCpuRequestViaMapreduceReduceCpuVcores()\n{\r\n    EventHandler eventHandler = mock(EventHandler.class);\r\n    Clock clock = SystemClock.getInstance();\r\n    JobConf jobConf = new JobConf();\r\n    jobConf.setInt(MRJobConfig.REDUCE_CPU_VCORES, 3);\r\n    TaskAttemptImpl taImpl = createReduceTaskAttemptImplForTest(eventHandler, clock, jobConf);\r\n    int vCores = getResourceInfoFromContainerRequest(taImpl, eventHandler).getVirtualCores();\r\n    assertEquals(3, vCores);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\job\\impl",
  "methodName" : "testReducerCpuRequestViaMapreduceReduceResourceVcores",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void testReducerCpuRequestViaMapreduceReduceResourceVcores()\n{\r\n    EventHandler eventHandler = mock(EventHandler.class);\r\n    Clock clock = SystemClock.getInstance();\r\n    JobConf jobConf = new JobConf();\r\n    jobConf.set(MRJobConfig.REDUCE_RESOURCE_TYPE_PREFIX + MRJobConfig.RESOURCE_TYPE_NAME_VCORE, \"5\");\r\n    TaskAttemptImpl taImpl = createReduceTaskAttemptImplForTest(eventHandler, clock, jobConf);\r\n    int vCores = getResourceInfoFromContainerRequest(taImpl, eventHandler).getVirtualCores();\r\n    assertEquals(5, vCores);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\job\\impl",
  "methodName" : "testReducerCpuRequestDefaultMemory",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void testReducerCpuRequestDefaultMemory()\n{\r\n    EventHandler eventHandler = mock(EventHandler.class);\r\n    Clock clock = SystemClock.getInstance();\r\n    TaskAttemptImpl taImpl = createReduceTaskAttemptImplForTest(eventHandler, clock, new JobConf());\r\n    int vCores = getResourceInfoFromContainerRequest(taImpl, eventHandler).getVirtualCores();\r\n    assertEquals(MRJobConfig.DEFAULT_REDUCE_CPU_VCORES, vCores);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\job\\impl",
  "methodName" : "testReducerCpuRequestOverriding",
  "errType" : null,
  "containingMethodsNum" : 11,
  "sourceCodeText" : "void testReducerCpuRequestOverriding()\n{\r\n    TestAppender testAppender = new TestAppender();\r\n    final Logger logger = Logger.getLogger(TaskAttemptImpl.class);\r\n    try {\r\n        logger.addAppender(testAppender);\r\n        EventHandler eventHandler = mock(EventHandler.class);\r\n        Clock clock = SystemClock.getInstance();\r\n        JobConf jobConf = new JobConf();\r\n        jobConf.set(MRJobConfig.REDUCE_RESOURCE_TYPE_PREFIX + MRJobConfig.RESOURCE_TYPE_NAME_VCORE, \"7\");\r\n        jobConf.setInt(MRJobConfig.REDUCE_CPU_VCORES, 9);\r\n        TaskAttemptImpl taImpl = createReduceTaskAttemptImplForTest(eventHandler, clock, jobConf);\r\n        long vCores = getResourceInfoFromContainerRequest(taImpl, eventHandler).getVirtualCores();\r\n        assertEquals(7, vCores);\r\n        assertTrue(testAppender.getLogEvents().stream().anyMatch(e -> e.getLevel() == Level.WARN && (\"Configuration \" + \"mapreduce.reduce.resource.vcores=7 is overriding the \" + \"mapreduce.reduce.cpu.vcores=9 configuration\").equals(e.getMessage())));\r\n    } finally {\r\n        logger.removeAppender(testAppender);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\job\\impl",
  "methodName" : "getResourceInfoFromContainerRequest",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "Resource getResourceInfoFromContainerRequest(TaskAttemptImpl taImpl, EventHandler eventHandler)\n{\r\n    taImpl.handle(new TaskAttemptEvent(taImpl.getID(), TaskAttemptEventType.TA_SCHEDULE));\r\n    assertThat(taImpl.getState()).withFailMessage(\"Task attempt is not in STARTING state\").isEqualTo(TaskAttemptState.STARTING);\r\n    ArgumentCaptor<Event> captor = ArgumentCaptor.forClass(Event.class);\r\n    verify(eventHandler, times(2)).handle(captor.capture());\r\n    List<ContainerRequestEvent> containerRequestEvents = new ArrayList<>();\r\n    for (Event e : captor.getAllValues()) {\r\n        if (e instanceof ContainerRequestEvent) {\r\n            containerRequestEvents.add((ContainerRequestEvent) e);\r\n        }\r\n    }\r\n    assertEquals(\"Expected one ContainerRequestEvent after scheduling \" + \"task attempt\", 1, containerRequestEvents.size());\r\n    return containerRequestEvents.get(0).getCapability();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\job\\impl",
  "methodName" : "testReducerCustomResourceTypeWithInvalidUnit",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void testReducerCustomResourceTypeWithInvalidUnit()\n{\r\n    initResourceTypes();\r\n    EventHandler eventHandler = mock(EventHandler.class);\r\n    Clock clock = SystemClock.getInstance();\r\n    JobConf jobConf = new JobConf();\r\n    jobConf.set(MRJobConfig.REDUCE_RESOURCE_TYPE_PREFIX + CUSTOM_RESOURCE_NAME, \"3z\");\r\n    createReduceTaskAttemptImplForTest(eventHandler, clock, jobConf);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\job\\impl",
  "methodName" : "testKillingTaskWhenContainerCleanup",
  "errType" : null,
  "containingMethodsNum" : 9,
  "sourceCodeText" : "void testKillingTaskWhenContainerCleanup()\n{\r\n    MockEventHandler eventHandler = new MockEventHandler();\r\n    TaskAttemptImpl taImpl = createTaskAttemptImpl(eventHandler);\r\n    TaskId maptaskId = MRBuilderUtils.newTaskId(taImpl.getID().getTaskId().getJobId(), 1, TaskType.MAP);\r\n    TaskAttemptId mapTAId = MRBuilderUtils.newTaskAttemptId(maptaskId, 0);\r\n    taImpl.handle(new TaskAttemptEvent(taImpl.getID(), TaskAttemptEventType.TA_DONE));\r\n    assertEquals(\"Task attempt's internal state is not \" + \"SUCCESS_FINISHING_CONTAINER\", TaskAttemptStateInternal.SUCCESS_FINISHING_CONTAINER, taImpl.getInternalState());\r\n    taImpl.handle(new TaskAttemptEvent(taImpl.getID(), TaskAttemptEventType.TA_TIMED_OUT));\r\n    assertEquals(\"Task attempt's internal state is not \" + \"SUCCESS_CONTAINER_CLEANUP\", TaskAttemptStateInternal.SUCCESS_CONTAINER_CLEANUP, taImpl.getInternalState());\r\n    taImpl.handle(new TaskAttemptKillEvent(mapTAId, \"\", true));\r\n    assertEquals(\"Task attempt is not in KILLED state\", TaskAttemptState.KILLED, taImpl.getState());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\job\\impl",
  "methodName" : "testTooManyFetchFailureWhileContainerCleanup",
  "errType" : null,
  "containingMethodsNum" : 10,
  "sourceCodeText" : "void testTooManyFetchFailureWhileContainerCleanup()\n{\r\n    MockEventHandler eventHandler = new MockEventHandler();\r\n    TaskAttemptImpl taImpl = createTaskAttemptImpl(eventHandler);\r\n    TaskId reducetaskId = MRBuilderUtils.newTaskId(taImpl.getID().getTaskId().getJobId(), 1, TaskType.REDUCE);\r\n    TaskAttemptId reduceTAId = MRBuilderUtils.newTaskAttemptId(reducetaskId, 0);\r\n    taImpl.handle(new TaskAttemptEvent(taImpl.getID(), TaskAttemptEventType.TA_DONE));\r\n    assertEquals(\"Task attempt's internal state is not \" + \"SUCCESS_FINISHING_CONTAINER\", TaskAttemptStateInternal.SUCCESS_FINISHING_CONTAINER, taImpl.getInternalState());\r\n    taImpl.handle(new TaskAttemptEvent(taImpl.getID(), TaskAttemptEventType.TA_TIMED_OUT));\r\n    assertEquals(\"Task attempt's internal state is not \" + \"SUCCESS_CONTAINER_CLEANUP\", TaskAttemptStateInternal.SUCCESS_CONTAINER_CLEANUP, taImpl.getInternalState());\r\n    taImpl.handle(new TaskAttemptTooManyFetchFailureEvent(taImpl.getID(), reduceTAId, \"Host\"));\r\n    assertEquals(\"Task attempt is not in FAILED state\", TaskAttemptState.FAILED, taImpl.getState());\r\n    assertFalse(\"InternalError occurred\", eventHandler.internalError);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\job\\impl",
  "methodName" : "initResourceTypes",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void initResourceTypes()\n{\r\n    CustomResourceTypesConfigurationProvider.initResourceTypes(ImmutableMap.<String, String>builder().put(CUSTOM_RESOURCE_NAME, \"G\").build());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\job\\impl",
  "methodName" : "testTooManyFetchFailureWhileSuccessFinishing",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void testTooManyFetchFailureWhileSuccessFinishing() throws Exception\n{\r\n    MockEventHandler eventHandler = new MockEventHandler();\r\n    TaskAttemptImpl taImpl = createTaskAttemptImpl(eventHandler);\r\n    TaskId reducetaskId = MRBuilderUtils.newTaskId(taImpl.getID().getTaskId().getJobId(), 1, TaskType.REDUCE);\r\n    TaskAttemptId reduceTAId = MRBuilderUtils.newTaskAttemptId(reducetaskId, 0);\r\n    taImpl.handle(new TaskAttemptEvent(taImpl.getID(), TaskAttemptEventType.TA_DONE));\r\n    assertEquals(\"Task attempt's internal state is not \" + \"SUCCESS_FINISHING_CONTAINER\", TaskAttemptStateInternal.SUCCESS_FINISHING_CONTAINER, taImpl.getInternalState());\r\n    taImpl.handle(new TaskAttemptTooManyFetchFailureEvent(taImpl.getID(), reduceTAId, \"Host\"));\r\n    assertEquals(\"Task attempt is not in FAILED state\", TaskAttemptState.FAILED, taImpl.getState());\r\n    assertFalse(\"InternalError occurred\", eventHandler.internalError);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\job\\impl",
  "methodName" : "setupTaskAttemptFinishingMonitor",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void setupTaskAttemptFinishingMonitor(EventHandler eventHandler, JobConf jobConf, AppContext appCtx)\n{\r\n    TaskAttemptFinishingMonitor taskAttemptFinishingMonitor = new TaskAttemptFinishingMonitor(eventHandler);\r\n    taskAttemptFinishingMonitor.init(jobConf);\r\n    when(appCtx.getTaskAttemptFinishingMonitor()).thenReturn(taskAttemptFinishingMonitor);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\job\\impl",
  "methodName" : "createCommonTaskAttemptImpl",
  "errType" : null,
  "containingMethodsNum" : 29,
  "sourceCodeText" : "TaskAttemptImpl createCommonTaskAttemptImpl(MockEventHandler eventHandler, JobConf jobConf)\n{\r\n    ApplicationId appId = ApplicationId.newInstance(1, 2);\r\n    ApplicationAttemptId appAttemptId = ApplicationAttemptId.newInstance(appId, 0);\r\n    JobId jobId = MRBuilderUtils.newJobId(appId, 1);\r\n    TaskId taskId = MRBuilderUtils.newTaskId(jobId, 1, TaskType.MAP);\r\n    TaskAttemptId attemptId = MRBuilderUtils.newTaskAttemptId(taskId, 0);\r\n    Path jobFile = mock(Path.class);\r\n    TaskAttemptListener taListener = mock(TaskAttemptListener.class);\r\n    when(taListener.getAddress()).thenReturn(new InetSocketAddress(\"localhost\", 0));\r\n    jobConf.setClass(\"fs.file.impl\", StubbedFS.class, FileSystem.class);\r\n    jobConf.setBoolean(\"fs.file.impl.disable.cache\", true);\r\n    jobConf.set(JobConf.MAPRED_MAP_TASK_ENV, \"\");\r\n    jobConf.set(MRJobConfig.APPLICATION_ATTEMPT_ID, \"10\");\r\n    TaskSplitMetaInfo splits = mock(TaskSplitMetaInfo.class);\r\n    when(splits.getLocations()).thenReturn(new String[] { \"127.0.0.1\" });\r\n    AppContext appCtx = mock(AppContext.class);\r\n    ClusterInfo clusterInfo = mock(ClusterInfo.class);\r\n    when(appCtx.getClusterInfo()).thenReturn(clusterInfo);\r\n    setupTaskAttemptFinishingMonitor(eventHandler, jobConf, appCtx);\r\n    TaskAttemptImpl taImpl = new MapTaskAttemptImpl(taskId, 1, eventHandler, jobFile, 1, splits, jobConf, taListener, mock(Token.class), new Credentials(), SystemClock.getInstance(), appCtx);\r\n    NodeId nid = NodeId.newInstance(\"127.0.0.1\", 0);\r\n    ContainerId contId = ContainerId.newInstance(appAttemptId, 3);\r\n    Container container = mock(Container.class);\r\n    when(container.getId()).thenReturn(contId);\r\n    when(container.getNodeId()).thenReturn(nid);\r\n    when(container.getNodeHttpAddress()).thenReturn(\"localhost:0\");\r\n    taImpl.handle(new TaskAttemptEvent(attemptId, TaskAttemptEventType.TA_SCHEDULE));\r\n    taImpl.handle(new TaskAttemptContainerAssignedEvent(attemptId, container, mock(Map.class)));\r\n    taImpl.handle(new TaskAttemptContainerLaunchedEvent(attemptId, 0));\r\n    return taImpl;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\job\\impl",
  "methodName" : "createTaskAttemptImpl",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "TaskAttemptImpl createTaskAttemptImpl(MockEventHandler eventHandler)\n{\r\n    JobConf jobConf = new JobConf();\r\n    return createCommonTaskAttemptImpl(eventHandler, jobConf);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\job\\impl",
  "methodName" : "createMapOnlyTaskAttemptImpl",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "TaskAttemptImpl createMapOnlyTaskAttemptImpl(MockEventHandler eventHandler)\n{\r\n    JobConf jobConf = new JobConf();\r\n    jobConf.setInt(MRJobConfig.NUM_REDUCES, 0);\r\n    return createCommonTaskAttemptImpl(eventHandler, jobConf);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app",
  "methodName" : "run",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void run(MRApp app) throws Exception\n{\r\n    GenericTestUtils.setRootLogLevel(Level.WARN);\r\n    long startTime = System.currentTimeMillis();\r\n    Job job = app.submit(new Configuration());\r\n    while (!job.getReport().getJobState().equals(JobState.SUCCEEDED)) {\r\n        printStat(job, startTime);\r\n        Thread.sleep(2000);\r\n    }\r\n    printStat(job, startTime);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app",
  "methodName" : "printStat",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void printStat(Job job, long startTime) throws Exception\n{\r\n    long currentTime = System.currentTimeMillis();\r\n    Runtime.getRuntime().gc();\r\n    long mem = Runtime.getRuntime().totalMemory() - Runtime.getRuntime().freeMemory();\r\n    System.out.println(\"JobState:\" + job.getState() + \" CompletedMaps:\" + job.getCompletedMaps() + \" CompletedReduces:\" + job.getCompletedReduces() + \" Memory(total-free)(KB):\" + mem / 1024 + \" ElapsedTime(ms):\" + (currentTime - startTime));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app",
  "methodName" : "benchmark1",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void benchmark1() throws Exception\n{\r\n    int maps = 100;\r\n    int reduces = 0;\r\n    System.out.println(\"Running benchmark with maps:\" + maps + \" reduces:\" + reduces);\r\n    run(new MRApp(maps, reduces, true, this.getClass().getName(), true) {\r\n\r\n        @Override\r\n        protected ContainerAllocator createContainerAllocator(ClientService clientService, AppContext context) {\r\n            AMPreemptionPolicy policy = new NoopAMPreemptionPolicy();\r\n            return new RMContainerAllocator(clientService, context, policy) {\r\n\r\n                @Override\r\n                protected ApplicationMasterProtocol createSchedulerProxy() {\r\n                    return new ApplicationMasterProtocol() {\r\n\r\n                        @Override\r\n                        public RegisterApplicationMasterResponse registerApplicationMaster(RegisterApplicationMasterRequest request) throws IOException {\r\n                            RegisterApplicationMasterResponse response = Records.newRecord(RegisterApplicationMasterResponse.class);\r\n                            response.setMaximumResourceCapability(Resource.newInstance(10240, 1));\r\n                            response.setQueue(\"queue1\");\r\n                            return response;\r\n                        }\r\n\r\n                        @Override\r\n                        public FinishApplicationMasterResponse finishApplicationMaster(FinishApplicationMasterRequest request) throws IOException {\r\n                            FinishApplicationMasterResponse response = Records.newRecord(FinishApplicationMasterResponse.class);\r\n                            return response;\r\n                        }\r\n\r\n                        @Override\r\n                        public AllocateResponse allocate(AllocateRequest request) throws IOException {\r\n                            AllocateResponse response = Records.newRecord(AllocateResponse.class);\r\n                            List<ResourceRequest> askList = request.getAskList();\r\n                            List<Container> containers = new ArrayList<Container>();\r\n                            for (ResourceRequest req : askList) {\r\n                                if (!ResourceRequest.isAnyLocation(req.getResourceName())) {\r\n                                    continue;\r\n                                }\r\n                                int numContainers = req.getNumContainers();\r\n                                for (int i = 0; i < numContainers; i++) {\r\n                                    ContainerId containerId = ContainerId.newContainerId(getContext().getApplicationAttemptId(), request.getResponseId() + i);\r\n                                    containers.add(Container.newInstance(containerId, NodeId.newInstance(\"host\" + containerId.getContainerId(), 2345), \"host\" + containerId.getContainerId() + \":5678\", req.getCapability(), req.getPriority(), null));\r\n                                }\r\n                            }\r\n                            response.setAllocatedContainers(containers);\r\n                            response.setResponseId(request.getResponseId() + 1);\r\n                            response.setNumClusterNodes(350);\r\n                            response.setApplicationPriority(Priority.newInstance(100));\r\n                            return response;\r\n                        }\r\n                    };\r\n                }\r\n            };\r\n        }\r\n    });\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app",
  "methodName" : "benchmark2",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void benchmark2() throws Exception\n{\r\n    int maps = 100;\r\n    int reduces = 50;\r\n    int maxConcurrentRunningTasks = 500;\r\n    System.out.println(\"Running benchmark with throttled running tasks with \" + \"maxConcurrentRunningTasks:\" + maxConcurrentRunningTasks + \" maps:\" + maps + \" reduces:\" + reduces);\r\n    run(new ThrottledMRApp(maps, reduces, maxConcurrentRunningTasks));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app",
  "methodName" : "main",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void main(String[] args) throws Exception\n{\r\n    MRAppBenchmark benchmark = new MRAppBenchmark();\r\n    benchmark.benchmark1();\r\n    benchmark.benchmark2();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\speculate\\forecast",
  "methodName" : "incTestSimpleExponentialForecast",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "int incTestSimpleExponentialForecast()\n{\r\n    clock = new ControlledClock();\r\n    clock.tickMsec(clockTicks);\r\n    SimpleExponentialSmoothing forecaster = new SimpleExponentialSmoothing(10000, 12, 10000, clock.getTime());\r\n    double progress = 0.0;\r\n    while (progress <= 1.0) {\r\n        clock.tickMsec(clockTicks);\r\n        forecaster.incorporateReading(clock.getTime(), progress);\r\n        LOG.info(\"progress: \" + progress + \" --> \" + forecaster.toString());\r\n        progress += 0.005;\r\n    }\r\n    return forecaster.getSSE() < Math.pow(10.0, -6) ? 0 : 1;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\speculate\\forecast",
  "methodName" : "decTestSimpleExponentialForecast",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "int decTestSimpleExponentialForecast()\n{\r\n    clock = new ControlledClock();\r\n    clock.tickMsec(clockTicks);\r\n    SimpleExponentialSmoothing forecaster = new SimpleExponentialSmoothing(800, 12, 10000, clock.getTime());\r\n    double progress = 0.0;\r\n    double[] progressRates = new double[] { 0.005, 0.004, 0.002, 0.001 };\r\n    while (progress <= 1.0) {\r\n        clock.tickMsec(clockTicks);\r\n        forecaster.incorporateReading(clock.getTime(), progress);\r\n        LOG.info(\"progress: \" + progress + \" --> \" + forecaster.toString());\r\n        progress += progressRates[(int) (progress / 0.25)];\r\n    }\r\n    return forecaster.getSSE() < Math.pow(10.0, -6) ? 0 : 1;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\speculate\\forecast",
  "methodName" : "zeroTestSimpleExponentialForecast",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "int zeroTestSimpleExponentialForecast()\n{\r\n    clock = new ControlledClock();\r\n    clock.tickMsec(clockTicks);\r\n    SimpleExponentialSmoothing forecaster = new SimpleExponentialSmoothing(800, 12, 10000, clock.getTime());\r\n    double progress = 0.0;\r\n    double[] progressRates = new double[] { 0.005, 0.004, 0.002, 0.0, 0.003 };\r\n    int progressInd = 0;\r\n    while (progress <= 1.0) {\r\n        clock.tickMsec(clockTicks);\r\n        forecaster.incorporateReading(clock.getTime(), progress);\r\n        LOG.info(\"progress: \" + progress + \" --> \" + forecaster.toString());\r\n        int currInd = progressInd++ > 1000 ? 4 : (int) (progress / 0.25);\r\n        progress += progressRates[currInd];\r\n    }\r\n    return forecaster.getSSE() < Math.pow(10.0, -6) ? 0 : 1;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\speculate\\forecast",
  "methodName" : "testSimpleExponentialForecastLinearInc",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void testSimpleExponentialForecastLinearInc() throws Exception\n{\r\n    int res = incTestSimpleExponentialForecast();\r\n    Assert.assertEquals(\"We got the wrong estimate from simple exponential.\", res, 0);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\speculate\\forecast",
  "methodName" : "testSimpleExponentialForecastLinearDec",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void testSimpleExponentialForecastLinearDec() throws Exception\n{\r\n    int res = decTestSimpleExponentialForecast();\r\n    Assert.assertEquals(\"We got the wrong estimate from simple exponential.\", res, 0);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\speculate\\forecast",
  "methodName" : "testSimpleExponentialForecastZeros",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void testSimpleExponentialForecastZeros() throws Exception\n{\r\n    int res = zeroTestSimpleExponentialForecast();\r\n    Assert.assertEquals(\"We got the wrong estimate from simple exponential.\", res, 0);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app",
  "methodName" : "testFailTask",
  "errType" : null,
  "containingMethodsNum" : 12,
  "sourceCodeText" : "void testFailTask() throws Exception\n{\r\n    MRApp app = new MockFirstFailingAttemptMRApp(1, 0);\r\n    Configuration conf = new Configuration();\r\n    conf.setBoolean(MRJobConfig.JOB_UBERTASK_ENABLE, false);\r\n    Job job = app.submit(conf);\r\n    app.waitForState(job, JobState.SUCCEEDED);\r\n    Map<TaskId, Task> tasks = job.getTasks();\r\n    Assert.assertEquals(\"Num tasks is not correct\", 1, tasks.size());\r\n    Task task = tasks.values().iterator().next();\r\n    Assert.assertEquals(\"Task state not correct\", TaskState.SUCCEEDED, task.getReport().getTaskState());\r\n    Map<TaskAttemptId, TaskAttempt> attempts = tasks.values().iterator().next().getAttempts();\r\n    Assert.assertEquals(\"Num attempts is not correct\", 2, attempts.size());\r\n    Iterator<TaskAttempt> it = attempts.values().iterator();\r\n    Assert.assertEquals(\"Attempt state not correct\", TaskAttemptState.FAILED, it.next().getReport().getTaskAttemptState());\r\n    Assert.assertEquals(\"Attempt state not correct\", TaskAttemptState.SUCCEEDED, it.next().getReport().getTaskAttemptState());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app",
  "methodName" : "testMapFailureMaxPercent",
  "errType" : null,
  "containingMethodsNum" : 12,
  "sourceCodeText" : "void testMapFailureMaxPercent() throws Exception\n{\r\n    MRApp app = new MockFirstFailingTaskMRApp(4, 0);\r\n    Configuration conf = new Configuration();\r\n    conf.setInt(MRJobConfig.MAP_MAX_ATTEMPTS, 2);\r\n    conf.setInt(MRJobConfig.REDUCE_MAX_ATTEMPTS, 1);\r\n    conf.setInt(MRJobConfig.MAP_FAILURES_MAX_PERCENT, 20);\r\n    conf.setInt(MRJobConfig.MAP_MAX_ATTEMPTS, 1);\r\n    Job job = app.submit(conf);\r\n    app.waitForState(job, JobState.FAILED);\r\n    app = new MockFirstFailingTaskMRApp(4, 0);\r\n    conf = new Configuration();\r\n    conf.setInt(MRJobConfig.MAP_MAX_ATTEMPTS, 2);\r\n    conf.setInt(MRJobConfig.REDUCE_MAX_ATTEMPTS, 1);\r\n    conf.setInt(MRJobConfig.MAP_FAILURES_MAX_PERCENT, 25);\r\n    conf.setInt(MRJobConfig.MAP_MAX_ATTEMPTS, 1);\r\n    job = app.submit(conf);\r\n    app.waitForState(job, JobState.SUCCEEDED);\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app",
  "methodName" : "testReduceFailureMaxPercent",
  "errType" : null,
  "containingMethodsNum" : 16,
  "sourceCodeText" : "void testReduceFailureMaxPercent() throws Exception\n{\r\n    MRApp app = new MockFirstFailingTaskMRApp(2, 4);\r\n    Configuration conf = new Configuration();\r\n    conf.setInt(MRJobConfig.MAP_MAX_ATTEMPTS, 1);\r\n    conf.setInt(MRJobConfig.REDUCE_MAX_ATTEMPTS, 2);\r\n    conf.setInt(MRJobConfig.MAP_FAILURES_MAX_PERCENT, 50);\r\n    conf.setInt(MRJobConfig.MAP_MAX_ATTEMPTS, 1);\r\n    conf.setInt(MRJobConfig.REDUCE_FAILURES_MAXPERCENT, 20);\r\n    conf.setInt(MRJobConfig.REDUCE_MAX_ATTEMPTS, 1);\r\n    Job job = app.submit(conf);\r\n    app.waitForState(job, JobState.FAILED);\r\n    app = new MockFirstFailingTaskMRApp(2, 4);\r\n    conf = new Configuration();\r\n    conf.setInt(MRJobConfig.MAP_MAX_ATTEMPTS, 1);\r\n    conf.setInt(MRJobConfig.REDUCE_MAX_ATTEMPTS, 2);\r\n    conf.setInt(MRJobConfig.MAP_FAILURES_MAX_PERCENT, 50);\r\n    conf.setInt(MRJobConfig.MAP_MAX_ATTEMPTS, 1);\r\n    conf.setInt(MRJobConfig.REDUCE_FAILURES_MAXPERCENT, 25);\r\n    conf.setInt(MRJobConfig.REDUCE_MAX_ATTEMPTS, 1);\r\n    job = app.submit(conf);\r\n    app.waitForState(job, JobState.SUCCEEDED);\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app",
  "methodName" : "testTimedOutTask",
  "errType" : null,
  "containingMethodsNum" : 12,
  "sourceCodeText" : "void testTimedOutTask() throws Exception\n{\r\n    MRApp app = new TimeOutTaskMRApp(1, 0);\r\n    Configuration conf = new Configuration();\r\n    int maxAttempts = 2;\r\n    conf.setInt(MRJobConfig.MAP_MAX_ATTEMPTS, maxAttempts);\r\n    conf.setBoolean(MRJobConfig.JOB_UBERTASK_ENABLE, false);\r\n    Job job = app.submit(conf);\r\n    app.waitForState(job, JobState.FAILED);\r\n    Map<TaskId, Task> tasks = job.getTasks();\r\n    Assert.assertEquals(\"Num tasks is not correct\", 1, tasks.size());\r\n    Task task = tasks.values().iterator().next();\r\n    Assert.assertEquals(\"Task state not correct\", TaskState.FAILED, task.getReport().getTaskState());\r\n    Map<TaskAttemptId, TaskAttempt> attempts = tasks.values().iterator().next().getAttempts();\r\n    Assert.assertEquals(\"Num attempts is not correct\", maxAttempts, attempts.size());\r\n    for (TaskAttempt attempt : attempts.values()) {\r\n        Assert.assertEquals(\"Attempt state not correct\", TaskAttemptState.FAILED, attempt.getReport().getTaskAttemptState());\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app",
  "methodName" : "testTaskFailWithUnusedContainer",
  "errType" : null,
  "containingMethodsNum" : 14,
  "sourceCodeText" : "void testTaskFailWithUnusedContainer() throws Exception\n{\r\n    MRApp app = new MRAppWithFailingTaskAndUnusedContainer();\r\n    Configuration conf = new Configuration();\r\n    int maxAttempts = 1;\r\n    conf.setInt(MRJobConfig.MAP_MAX_ATTEMPTS, maxAttempts);\r\n    conf.setBoolean(MRJobConfig.JOB_UBERTASK_ENABLE, false);\r\n    Job job = app.submit(conf);\r\n    app.waitForState(job, JobState.RUNNING);\r\n    Map<TaskId, Task> tasks = job.getTasks();\r\n    Assert.assertEquals(\"Num tasks is not correct\", 1, tasks.size());\r\n    Task task = tasks.values().iterator().next();\r\n    app.waitForState(task, TaskState.SCHEDULED);\r\n    Map<TaskAttemptId, TaskAttempt> attempts = tasks.values().iterator().next().getAttempts();\r\n    Assert.assertEquals(\"Num attempts is not correct\", maxAttempts, attempts.size());\r\n    TaskAttempt attempt = attempts.values().iterator().next();\r\n    app.waitForInternalState((TaskAttemptImpl) attempt, TaskAttemptStateInternal.ASSIGNED);\r\n    app.getDispatcher().getEventHandler().handle(new TaskAttemptEvent(attempt.getID(), TaskAttemptEventType.TA_CONTAINER_COMPLETED));\r\n    app.waitForState(job, JobState.FAILED);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app",
  "methodName" : "main",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void main(String[] args) throws Exception\n{\r\n    TestFail t = new TestFail();\r\n    t.testFailTask();\r\n    t.testTimedOutTask();\r\n    t.testMapFailureMaxPercent();\r\n    t.testReduceFailureMaxPercent();\r\n    t.testTaskFailWithUnusedContainer();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "testTaskAttemptFinishedEvent",
  "errType" : null,
  "containingMethodsNum" : 10,
  "sourceCodeText" : "void testTaskAttemptFinishedEvent() throws Exception\n{\r\n    JobID jid = new JobID(\"001\", 1);\r\n    TaskID tid = new TaskID(jid, TaskType.REDUCE, 2);\r\n    TaskAttemptID taskAttemptId = new TaskAttemptID(tid, 3);\r\n    Counters counters = new Counters();\r\n    TaskAttemptFinishedEvent test = new TaskAttemptFinishedEvent(taskAttemptId, TaskType.REDUCE, \"TEST\", 123L, \"RAKNAME\", \"HOSTNAME\", \"STATUS\", counters, 234);\r\n    assertThat(test.getAttemptId().toString()).isEqualTo(taskAttemptId.toString());\r\n    assertThat(test.getCounters()).isEqualTo(counters);\r\n    assertThat(test.getFinishTime()).isEqualTo(123L);\r\n    assertThat(test.getHostname()).isEqualTo(\"HOSTNAME\");\r\n    assertThat(test.getRackName()).isEqualTo(\"RAKNAME\");\r\n    assertThat(test.getState()).isEqualTo(\"STATUS\");\r\n    assertThat(test.getTaskId()).isEqualTo(tid);\r\n    assertThat(test.getTaskStatus()).isEqualTo(\"TEST\");\r\n    assertThat(test.getTaskType()).isEqualTo(TaskType.REDUCE);\r\n    assertEquals(234, test.getStartTime());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "testJobPriorityChange",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void testJobPriorityChange() throws Exception\n{\r\n    org.apache.hadoop.mapreduce.JobID jid = new JobID(\"001\", 1);\r\n    JobPriorityChangeEvent test = new JobPriorityChangeEvent(jid, JobPriority.LOW);\r\n    assertThat(test.getJobId().toString()).isEqualTo(jid.toString());\r\n    assertThat(test.getPriority()).isEqualTo(JobPriority.LOW);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "testJobQueueChange",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void testJobQueueChange() throws Exception\n{\r\n    org.apache.hadoop.mapreduce.JobID jid = new JobID(\"001\", 1);\r\n    JobQueueChangeEvent test = new JobQueueChangeEvent(jid, \"newqueue\");\r\n    assertThat(test.getJobId().toString()).isEqualTo(jid.toString());\r\n    assertThat(test.getJobQueueName()).isEqualTo(\"newqueue\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "testTaskUpdated",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void testTaskUpdated() throws Exception\n{\r\n    JobID jid = new JobID(\"001\", 1);\r\n    TaskID tid = new TaskID(jid, TaskType.REDUCE, 2);\r\n    TaskUpdatedEvent test = new TaskUpdatedEvent(tid, 1234L);\r\n    assertThat(test.getTaskId().toString()).isEqualTo(tid.toString());\r\n    assertThat(test.getFinishTime()).isEqualTo(1234L);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "testEvents",
  "errType" : null,
  "containingMethodsNum" : 41,
  "sourceCodeText" : "void testEvents() throws Exception\n{\r\n    EventReader reader = new EventReader(new DataInputStream(new ByteArrayInputStream(getEvents())));\r\n    HistoryEvent e = reader.getNextEvent();\r\n    assertTrue(e.getEventType().equals(EventType.JOB_PRIORITY_CHANGED));\r\n    assertEquals(\"ID\", ((JobPriorityChange) e.getDatum()).getJobid().toString());\r\n    e = reader.getNextEvent();\r\n    assertTrue(e.getEventType().equals(EventType.JOB_STATUS_CHANGED));\r\n    assertEquals(\"ID\", ((JobStatusChanged) e.getDatum()).getJobid().toString());\r\n    e = reader.getNextEvent();\r\n    assertTrue(e.getEventType().equals(EventType.TASK_UPDATED));\r\n    assertEquals(\"ID\", ((TaskUpdated) e.getDatum()).getTaskid().toString());\r\n    e = reader.getNextEvent();\r\n    assertTrue(e.getEventType().equals(EventType.REDUCE_ATTEMPT_KILLED));\r\n    assertEquals(taskId, ((TaskAttemptUnsuccessfulCompletion) e.getDatum()).getTaskid().toString());\r\n    e = reader.getNextEvent();\r\n    assertTrue(e.getEventType().equals(EventType.JOB_KILLED));\r\n    assertEquals(\"ID\", ((JobUnsuccessfulCompletion) e.getDatum()).getJobid().toString());\r\n    e = reader.getNextEvent();\r\n    assertTrue(e.getEventType().equals(EventType.REDUCE_ATTEMPT_STARTED));\r\n    assertEquals(taskId, ((TaskAttemptStarted) e.getDatum()).getTaskid().toString());\r\n    e = reader.getNextEvent();\r\n    assertTrue(e.getEventType().equals(EventType.REDUCE_ATTEMPT_FINISHED));\r\n    assertEquals(taskId, ((TaskAttemptFinished) e.getDatum()).getTaskid().toString());\r\n    e = reader.getNextEvent();\r\n    assertTrue(e.getEventType().equals(EventType.REDUCE_ATTEMPT_KILLED));\r\n    assertEquals(taskId, ((TaskAttemptUnsuccessfulCompletion) e.getDatum()).getTaskid().toString());\r\n    e = reader.getNextEvent();\r\n    assertTrue(e.getEventType().equals(EventType.REDUCE_ATTEMPT_KILLED));\r\n    assertEquals(taskId, ((TaskAttemptUnsuccessfulCompletion) e.getDatum()).getTaskid().toString());\r\n    e = reader.getNextEvent();\r\n    assertTrue(e.getEventType().equals(EventType.REDUCE_ATTEMPT_STARTED));\r\n    assertEquals(taskId, ((TaskAttemptStarted) e.getDatum()).getTaskid().toString());\r\n    e = reader.getNextEvent();\r\n    assertTrue(e.getEventType().equals(EventType.REDUCE_ATTEMPT_FINISHED));\r\n    assertEquals(taskId, ((TaskAttemptFinished) e.getDatum()).getTaskid().toString());\r\n    e = reader.getNextEvent();\r\n    assertTrue(e.getEventType().equals(EventType.REDUCE_ATTEMPT_KILLED));\r\n    assertEquals(taskId, ((TaskAttemptUnsuccessfulCompletion) e.getDatum()).getTaskid().toString());\r\n    e = reader.getNextEvent();\r\n    assertTrue(e.getEventType().equals(EventType.REDUCE_ATTEMPT_KILLED));\r\n    assertEquals(taskId, ((TaskAttemptUnsuccessfulCompletion) e.getDatum()).getTaskid().toString());\r\n    reader.close();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "getEvents",
  "errType" : null,
  "containingMethodsNum" : 16,
  "sourceCodeText" : "byte[] getEvents() throws Exception\n{\r\n    ByteArrayOutputStream output = new ByteArrayOutputStream();\r\n    FSDataOutputStream fsOutput = new FSDataOutputStream(output, new FileSystem.Statistics(\"scheme\"));\r\n    EventWriter writer = new EventWriter(fsOutput, EventWriter.WriteMode.JSON);\r\n    writer.write(getJobPriorityChangedEvent());\r\n    writer.write(getJobStatusChangedEvent());\r\n    writer.write(getTaskUpdatedEvent());\r\n    writer.write(getReduceAttemptKilledEvent());\r\n    writer.write(getJobKilledEvent());\r\n    writer.write(getSetupAttemptStartedEvent());\r\n    writer.write(getTaskAttemptFinishedEvent());\r\n    writer.write(getSetupAttemptFieledEvent());\r\n    writer.write(getSetupAttemptKilledEvent());\r\n    writer.write(getCleanupAttemptStartedEvent());\r\n    writer.write(getCleanupAttemptFinishedEvent());\r\n    writer.write(getCleanupAttemptFiledEvent());\r\n    writer.write(getCleanupAttemptKilledEvent());\r\n    writer.flush();\r\n    writer.close();\r\n    return output.toByteArray();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "getCleanupAttemptKilledEvent",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "FakeEvent getCleanupAttemptKilledEvent()\n{\r\n    FakeEvent result = new FakeEvent(EventType.CLEANUP_ATTEMPT_KILLED);\r\n    result.setDatum(getTaskAttemptUnsuccessfulCompletion());\r\n    return result;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "getCleanupAttemptFiledEvent",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "FakeEvent getCleanupAttemptFiledEvent()\n{\r\n    FakeEvent result = new FakeEvent(EventType.CLEANUP_ATTEMPT_FAILED);\r\n    result.setDatum(getTaskAttemptUnsuccessfulCompletion());\r\n    return result;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "getTaskAttemptUnsuccessfulCompletion",
  "errType" : null,
  "containingMethodsNum" : 14,
  "sourceCodeText" : "TaskAttemptUnsuccessfulCompletion getTaskAttemptUnsuccessfulCompletion()\n{\r\n    TaskAttemptUnsuccessfulCompletion datum = new TaskAttemptUnsuccessfulCompletion();\r\n    datum.setAttemptId(\"attempt_1_2_r3_4_5\");\r\n    datum.setClockSplits(Arrays.asList(1, 2, 3));\r\n    datum.setCpuUsages(Arrays.asList(100, 200, 300));\r\n    datum.setError(\"Error\");\r\n    datum.setFinishTime(2L);\r\n    datum.setHostname(\"hostname\");\r\n    datum.setRackname(\"rackname\");\r\n    datum.setPhysMemKbytes(Arrays.asList(1000, 2000, 3000));\r\n    datum.setTaskid(taskId);\r\n    datum.setPort(1000);\r\n    datum.setTaskType(\"REDUCE\");\r\n    datum.setStatus(\"STATUS\");\r\n    datum.setCounters(getCounters());\r\n    datum.setVMemKbytes(Arrays.asList(1000, 2000, 3000));\r\n    return datum;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "getCounters",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "JhCounters getCounters()\n{\r\n    JhCounters counters = new JhCounters();\r\n    counters.setGroups(new ArrayList<JhCounterGroup>(0));\r\n    counters.setName(\"name\");\r\n    return counters;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "getCleanupAttemptFinishedEvent",
  "errType" : null,
  "containingMethodsNum" : 10,
  "sourceCodeText" : "FakeEvent getCleanupAttemptFinishedEvent()\n{\r\n    FakeEvent result = new FakeEvent(EventType.CLEANUP_ATTEMPT_FINISHED);\r\n    TaskAttemptFinished datum = new TaskAttemptFinished();\r\n    datum.setAttemptId(\"attempt_1_2_r3_4_5\");\r\n    datum.setCounters(getCounters());\r\n    datum.setFinishTime(2L);\r\n    datum.setHostname(\"hostname\");\r\n    datum.setRackname(\"rackName\");\r\n    datum.setState(\"state\");\r\n    datum.setTaskid(taskId);\r\n    datum.setTaskStatus(\"taskStatus\");\r\n    datum.setTaskType(\"REDUCE\");\r\n    result.setDatum(datum);\r\n    return result;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "getCleanupAttemptStartedEvent",
  "errType" : null,
  "containingMethodsNum" : 11,
  "sourceCodeText" : "FakeEvent getCleanupAttemptStartedEvent()\n{\r\n    FakeEvent result = new FakeEvent(EventType.CLEANUP_ATTEMPT_STARTED);\r\n    TaskAttemptStarted datum = new TaskAttemptStarted();\r\n    datum.setAttemptId(\"attempt_1_2_r3_4_5\");\r\n    datum.setAvataar(\"avatar\");\r\n    datum.setContainerId(\"containerId\");\r\n    datum.setHttpPort(10000);\r\n    datum.setLocality(\"locality\");\r\n    datum.setShufflePort(10001);\r\n    datum.setStartTime(1L);\r\n    datum.setTaskid(taskId);\r\n    datum.setTaskType(\"taskType\");\r\n    datum.setTrackerName(\"trackerName\");\r\n    result.setDatum(datum);\r\n    return result;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "getSetupAttemptKilledEvent",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "FakeEvent getSetupAttemptKilledEvent()\n{\r\n    FakeEvent result = new FakeEvent(EventType.SETUP_ATTEMPT_KILLED);\r\n    result.setDatum(getTaskAttemptUnsuccessfulCompletion());\r\n    return result;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "getSetupAttemptFieledEvent",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "FakeEvent getSetupAttemptFieledEvent()\n{\r\n    FakeEvent result = new FakeEvent(EventType.SETUP_ATTEMPT_FAILED);\r\n    result.setDatum(getTaskAttemptUnsuccessfulCompletion());\r\n    return result;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "getTaskAttemptFinishedEvent",
  "errType" : null,
  "containingMethodsNum" : 10,
  "sourceCodeText" : "FakeEvent getTaskAttemptFinishedEvent()\n{\r\n    FakeEvent result = new FakeEvent(EventType.SETUP_ATTEMPT_FINISHED);\r\n    TaskAttemptFinished datum = new TaskAttemptFinished();\r\n    datum.setAttemptId(\"attempt_1_2_r3_4_5\");\r\n    datum.setCounters(getCounters());\r\n    datum.setFinishTime(2L);\r\n    datum.setHostname(\"hostname\");\r\n    datum.setRackname(\"rackname\");\r\n    datum.setState(\"state\");\r\n    datum.setTaskid(taskId);\r\n    datum.setTaskStatus(\"taskStatus\");\r\n    datum.setTaskType(\"REDUCE\");\r\n    result.setDatum(datum);\r\n    return result;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "getSetupAttemptStartedEvent",
  "errType" : null,
  "containingMethodsNum" : 11,
  "sourceCodeText" : "FakeEvent getSetupAttemptStartedEvent()\n{\r\n    FakeEvent result = new FakeEvent(EventType.SETUP_ATTEMPT_STARTED);\r\n    TaskAttemptStarted datum = new TaskAttemptStarted();\r\n    datum.setAttemptId(\"ID\");\r\n    datum.setAvataar(\"avataar\");\r\n    datum.setContainerId(\"containerId\");\r\n    datum.setHttpPort(10000);\r\n    datum.setLocality(\"locality\");\r\n    datum.setShufflePort(10001);\r\n    datum.setStartTime(1L);\r\n    datum.setTaskid(taskId);\r\n    datum.setTaskType(\"taskType\");\r\n    datum.setTrackerName(\"trackerName\");\r\n    result.setDatum(datum);\r\n    return result;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "getJobKilledEvent",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "FakeEvent getJobKilledEvent()\n{\r\n    FakeEvent result = new FakeEvent(EventType.JOB_KILLED);\r\n    JobUnsuccessfulCompletion datum = new JobUnsuccessfulCompletion();\r\n    datum.setFinishedMaps(1);\r\n    datum.setFinishedReduces(2);\r\n    datum.setFinishTime(3L);\r\n    datum.setJobid(\"ID\");\r\n    datum.setJobStatus(\"STATUS\");\r\n    datum.setDiagnostics(JobImpl.JOB_KILLED_DIAG);\r\n    result.setDatum(datum);\r\n    return result;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "getReduceAttemptKilledEvent",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "FakeEvent getReduceAttemptKilledEvent()\n{\r\n    FakeEvent result = new FakeEvent(EventType.REDUCE_ATTEMPT_KILLED);\r\n    result.setDatum(getTaskAttemptUnsuccessfulCompletion());\r\n    return result;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "getJobPriorityChangedEvent",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "FakeEvent getJobPriorityChangedEvent()\n{\r\n    FakeEvent result = new FakeEvent(EventType.JOB_PRIORITY_CHANGED);\r\n    JobPriorityChange datum = new JobPriorityChange();\r\n    datum.setJobid(\"ID\");\r\n    datum.setPriority(\"priority\");\r\n    result.setDatum(datum);\r\n    return result;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "getJobStatusChangedEvent",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "FakeEvent getJobStatusChangedEvent()\n{\r\n    FakeEvent result = new FakeEvent(EventType.JOB_STATUS_CHANGED);\r\n    JobStatusChanged datum = new JobStatusChanged();\r\n    datum.setJobid(\"ID\");\r\n    datum.setJobStatus(\"newStatus\");\r\n    result.setDatum(datum);\r\n    return result;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\jobhistory",
  "methodName" : "getTaskUpdatedEvent",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "FakeEvent getTaskUpdatedEvent()\n{\r\n    FakeEvent result = new FakeEvent(EventType.TASK_UPDATED);\r\n    TaskUpdated datum = new TaskUpdated();\r\n    datum.setFinishTime(2L);\r\n    datum.setTaskid(\"ID\");\r\n    result.setDatum(datum);\r\n    return result;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "testFinshingAttemptTimeout",
  "errType" : null,
  "containingMethodsNum" : 23,
  "sourceCodeText" : "void testFinshingAttemptTimeout() throws IOException, InterruptedException\n{\r\n    SystemClock clock = SystemClock.getInstance();\r\n    Configuration conf = new Configuration();\r\n    conf.setInt(MRJobConfig.TASK_EXIT_TIMEOUT, 100);\r\n    conf.setInt(MRJobConfig.TASK_EXIT_TIMEOUT_CHECK_INTERVAL_MS, 10);\r\n    conf.setDouble(MRJobConfig.TASK_LOG_PROGRESS_DELTA_THRESHOLD, 0.01);\r\n    AppContext appCtx = mock(AppContext.class);\r\n    JobTokenSecretManager secret = mock(JobTokenSecretManager.class);\r\n    RMHeartbeatHandler rmHeartbeatHandler = mock(RMHeartbeatHandler.class);\r\n    MockEventHandler eventHandler = new MockEventHandler();\r\n    TaskAttemptFinishingMonitor taskAttemptFinishingMonitor = new TaskAttemptFinishingMonitor(eventHandler);\r\n    taskAttemptFinishingMonitor.init(conf);\r\n    taskAttemptFinishingMonitor.start();\r\n    when(appCtx.getEventHandler()).thenReturn(eventHandler);\r\n    when(appCtx.getNMHostname()).thenReturn(\"0.0.0.0\");\r\n    when(appCtx.getTaskAttemptFinishingMonitor()).thenReturn(taskAttemptFinishingMonitor);\r\n    when(appCtx.getClock()).thenReturn(clock);\r\n    CheckpointAMPreemptionPolicy policy = new CheckpointAMPreemptionPolicy();\r\n    policy.init(appCtx);\r\n    TaskAttemptListenerImpl listener = new TaskAttemptListenerImpl(appCtx, secret, rmHeartbeatHandler, policy);\r\n    listener.init(conf);\r\n    listener.start();\r\n    JobId jid = MRBuilderUtils.newJobId(12345, 1, 1);\r\n    TaskId tid = MRBuilderUtils.newTaskId(jid, 0, org.apache.hadoop.mapreduce.v2.api.records.TaskType.MAP);\r\n    TaskAttemptId attemptId = MRBuilderUtils.newTaskAttemptId(tid, 0);\r\n    appCtx.getTaskAttemptFinishingMonitor().register(attemptId);\r\n    int check = 0;\r\n    while (!eventHandler.timedOut && check++ < 10) {\r\n        Thread.sleep(100);\r\n    }\r\n    taskAttemptFinishingMonitor.stop();\r\n    assertTrue(\"Finishing attempt didn't time out.\", eventHandler.timedOut);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce",
  "methodName" : "initializeMemberVariables",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void initializeMemberVariables()\n{\r\n    xmlFilename = \"mapred-default.xml\";\r\n    configurationClasses = new Class[] { MRJobConfig.class, MRConfig.class, JHAdminConfig.class, ShuffleHandler.class, FileOutputFormat.class, FileInputFormat.class, Job.class, NLineInputFormat.class, JobConf.class, FileOutputCommitter.class, PathOutputCommitterFactory.class };\r\n    configurationPropsToSkipCompare = new HashSet<>();\r\n    errorIfMissingConfigProps = true;\r\n    errorIfMissingXmlProps = false;\r\n    configurationPropsToSkipCompare.add(JobConf.MAPRED_JOB_MAP_MEMORY_MB_PROPERTY);\r\n    configurationPropsToSkipCompare.add(JobConf.MAPRED_JOB_REDUCE_MEMORY_MB_PROPERTY);\r\n    configurationPropsToSkipCompare.add(MRJobConfig.MR_AM_RESOURCE_PREFIX);\r\n    configurationPropsToSkipCompare.add(MRJobConfig.MAP_RESOURCE_TYPE_PREFIX);\r\n    configurationPropsToSkipCompare.add(MRJobConfig.REDUCE_RESOURCE_TYPE_PREFIX);\r\n    xmlPrefixToSkipCompare = new HashSet<>();\r\n    xmlPrefixToSkipCompare.add(PathOutputCommitterFactory.COMMITTER_FACTORY_SCHEME);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app",
  "methodName" : "testKillAMPreemptPolicy",
  "errType" : null,
  "containingMethodsNum" : 30,
  "sourceCodeText" : "void testKillAMPreemptPolicy()\n{\r\n    ApplicationId appId = ApplicationId.newInstance(123456789, 1);\r\n    ContainerId container = ContainerId.newContainerId(ApplicationAttemptId.newInstance(appId, 1), 1);\r\n    AMPreemptionPolicy.Context mPctxt = mock(AMPreemptionPolicy.Context.class);\r\n    when(mPctxt.getTaskAttempt(any(ContainerId.class))).thenReturn(MRBuilderUtils.newTaskAttemptId(MRBuilderUtils.newTaskId(MRBuilderUtils.newJobId(appId, 1), 1, TaskType.MAP), 0));\r\n    List<Container> p = new ArrayList<Container>();\r\n    p.add(Container.newInstance(container, null, null, null, null, null));\r\n    when(mPctxt.getContainers(any(TaskType.class))).thenReturn(p);\r\n    KillAMPreemptionPolicy policy = new KillAMPreemptionPolicy();\r\n    RunningAppContext mActxt = getRunningAppContext();\r\n    policy.init(mActxt);\r\n    PreemptionMessage pM = getPreemptionMessage(false, false, container);\r\n    policy.preempt(mPctxt, pM);\r\n    verify(mActxt.getEventHandler(), times(0)).handle(any(TaskAttemptEvent.class));\r\n    verify(mActxt.getEventHandler(), times(0)).handle(any(JobCounterUpdateEvent.class));\r\n    mActxt = getRunningAppContext();\r\n    policy.init(mActxt);\r\n    pM = getPreemptionMessage(true, false, container);\r\n    policy.preempt(mPctxt, pM);\r\n    verify(mActxt.getEventHandler(), times(1)).handle(any(TaskAttemptEvent.class));\r\n    verify(mActxt.getEventHandler(), times(1)).handle(any(JobCounterUpdateEvent.class));\r\n    mActxt = getRunningAppContext();\r\n    policy.init(mActxt);\r\n    pM = getPreemptionMessage(false, true, container);\r\n    policy.preempt(mPctxt, pM);\r\n    verify(mActxt.getEventHandler(), times(1)).handle(any(TaskAttemptEvent.class));\r\n    verify(mActxt.getEventHandler(), times(1)).handle(any(JobCounterUpdateEvent.class));\r\n    mActxt = getRunningAppContext();\r\n    policy.init(mActxt);\r\n    pM = getPreemptionMessage(true, true, container);\r\n    policy.preempt(mPctxt, pM);\r\n    verify(mActxt.getEventHandler(), times(2)).handle(any(TaskAttemptEvent.class));\r\n    verify(mActxt.getEventHandler(), times(2)).handle(any(JobCounterUpdateEvent.class));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app",
  "methodName" : "getRunningAppContext",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "RunningAppContext getRunningAppContext()\n{\r\n    RunningAppContext mActxt = mock(RunningAppContext.class);\r\n    @SuppressWarnings(\"unchecked\")\r\n    EventHandler<Event> eventHandler = mock(EventHandler.class);\r\n    when(mActxt.getEventHandler()).thenReturn(eventHandler);\r\n    return mActxt;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app",
  "methodName" : "getPreemptionMessage",
  "errType" : null,
  "containingMethodsNum" : 10,
  "sourceCodeText" : "PreemptionMessage getPreemptionMessage(boolean strictContract, boolean contract, final ContainerId container)\n{\r\n    PreemptionMessage preemptionMessage = recordFactory.newRecordInstance(PreemptionMessage.class);\r\n    Set<PreemptionContainer> cntrs = new HashSet<PreemptionContainer>();\r\n    PreemptionContainer preemptContainer = recordFactory.newRecordInstance(PreemptionContainer.class);\r\n    preemptContainer.setId(container);\r\n    cntrs.add(preemptContainer);\r\n    if (strictContract) {\r\n        StrictPreemptionContract set = recordFactory.newRecordInstance(StrictPreemptionContract.class);\r\n        set.setContainers(cntrs);\r\n        preemptionMessage.setStrictContract(set);\r\n    }\r\n    if (contract) {\r\n        PreemptionContract preemptContract = recordFactory.newRecordInstance(PreemptionContract.class);\r\n        preemptContract.setContainers(cntrs);\r\n        preemptionMessage.setContract(preemptContract);\r\n    }\r\n    return preemptionMessage;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app",
  "methodName" : "handle",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void handle(Event event)\n{\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "after",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void after() throws IOException\n{\r\n    if (listener != null) {\r\n        listener.close();\r\n        listener = null;\r\n    }\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "testGetTask",
  "errType" : [ "IllegalArgumentException" ],
  "containingMethodsNum" : 27,
  "sourceCodeText" : "void testGetTask() throws IOException\n{\r\n    configureMocks();\r\n    startListener(false);\r\n    JvmContext context = new JvmContext();\r\n    context.jvmId = id;\r\n    JvmTask result = listener.getTask(context);\r\n    assertNotNull(result);\r\n    assertTrue(result.shouldDie);\r\n    listener.registerPendingTask(task, wid);\r\n    result = listener.getTask(context);\r\n    assertNull(result);\r\n    listener.unregister(attemptId, wid);\r\n    listener.registerPendingTask(task, wid);\r\n    listener.registerLaunchedTask(attemptId, wid);\r\n    verify(hbHandler).register(attemptId);\r\n    result = listener.getTask(context);\r\n    assertNotNull(result);\r\n    assertFalse(result.shouldDie);\r\n    result = listener.getTask(context);\r\n    assertNotNull(result);\r\n    assertTrue(result.shouldDie);\r\n    listener.unregister(attemptId, wid);\r\n    result = listener.getTask(context);\r\n    assertNotNull(result);\r\n    assertTrue(result.shouldDie);\r\n    JVMId jvmid = JVMId.forName(\"jvm_001_002_m_004\");\r\n    assertNotNull(jvmid);\r\n    try {\r\n        JVMId.forName(\"jvm_001_002_m_004_006\");\r\n        fail();\r\n    } catch (IllegalArgumentException e) {\r\n        assertThat(e.getMessage()).isEqualTo(\"TaskId string : jvm_001_002_m_004_006 is not properly formed\");\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "testJVMId",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void testJVMId()\n{\r\n    JVMId jvmid = new JVMId(\"test\", 1, true, 2);\r\n    JVMId jvmid1 = JVMId.forName(\"jvm_test_0001_m_000002\");\r\n    assertEquals(0, jvmid.compareTo(jvmid1));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "testGetMapCompletionEvents",
  "errType" : null,
  "containingMethodsNum" : 21,
  "sourceCodeText" : "void testGetMapCompletionEvents() throws IOException\n{\r\n    TaskAttemptCompletionEvent[] empty = {};\r\n    TaskAttemptCompletionEvent[] taskEvents = { createTce(0, true, TaskAttemptCompletionEventStatus.OBSOLETE), createTce(1, false, TaskAttemptCompletionEventStatus.FAILED), createTce(2, true, TaskAttemptCompletionEventStatus.SUCCEEDED), createTce(3, false, TaskAttemptCompletionEventStatus.FAILED) };\r\n    TaskAttemptCompletionEvent[] mapEvents = { taskEvents[0], taskEvents[2] };\r\n    Job mockJob = mock(Job.class);\r\n    when(mockJob.getTaskAttemptCompletionEvents(0, 100)).thenReturn(taskEvents);\r\n    when(mockJob.getTaskAttemptCompletionEvents(0, 2)).thenReturn(Arrays.copyOfRange(taskEvents, 0, 2));\r\n    when(mockJob.getTaskAttemptCompletionEvents(2, 100)).thenReturn(Arrays.copyOfRange(taskEvents, 2, 4));\r\n    when(mockJob.getMapAttemptCompletionEvents(0, 100)).thenReturn(TypeConverter.fromYarn(mapEvents));\r\n    when(mockJob.getMapAttemptCompletionEvents(0, 2)).thenReturn(TypeConverter.fromYarn(mapEvents));\r\n    when(mockJob.getMapAttemptCompletionEvents(2, 100)).thenReturn(TypeConverter.fromYarn(empty));\r\n    configureMocks();\r\n    when(appCtx.getJob(any(JobId.class))).thenReturn(mockJob);\r\n    listener = new MockTaskAttemptListenerImpl(appCtx, secret, rmHeartbeatHandler, policy) {\r\n\r\n        @Override\r\n        protected void registerHeartbeatHandler(Configuration conf) {\r\n            taskHeartbeatHandler = hbHandler;\r\n        }\r\n    };\r\n    Configuration conf = new Configuration();\r\n    listener.init(conf);\r\n    listener.start();\r\n    JobID jid = new JobID(\"12345\", 1);\r\n    TaskAttemptID tid = new TaskAttemptID(\"12345\", 1, TaskType.REDUCE, 1, 0);\r\n    MapTaskCompletionEventsUpdate update = listener.getMapCompletionEvents(jid, 0, 100, tid);\r\n    assertEquals(2, update.events.length);\r\n    update = listener.getMapCompletionEvents(jid, 0, 2, tid);\r\n    assertEquals(2, update.events.length);\r\n    update = listener.getMapCompletionEvents(jid, 2, 100, tid);\r\n    assertEquals(0, update.events.length);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "createTce",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "TaskAttemptCompletionEvent createTce(int eventId, boolean isMap, TaskAttemptCompletionEventStatus status)\n{\r\n    JobId jid = MRBuilderUtils.newJobId(12345, 1, 1);\r\n    TaskId tid = MRBuilderUtils.newTaskId(jid, 0, isMap ? org.apache.hadoop.mapreduce.v2.api.records.TaskType.MAP : org.apache.hadoop.mapreduce.v2.api.records.TaskType.REDUCE);\r\n    TaskAttemptId attemptId = MRBuilderUtils.newTaskAttemptId(tid, 0);\r\n    RecordFactory recordFactory = RecordFactoryProvider.getRecordFactory(null);\r\n    TaskAttemptCompletionEvent tce = recordFactory.newRecordInstance(TaskAttemptCompletionEvent.class);\r\n    tce.setEventId(eventId);\r\n    tce.setAttemptId(attemptId);\r\n    tce.setStatus(status);\r\n    return tce;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "testCommitWindow",
  "errType" : null,
  "containingMethodsNum" : 17,
  "sourceCodeText" : "void testCommitWindow() throws IOException\n{\r\n    SystemClock clock = SystemClock.getInstance();\r\n    configureMocks();\r\n    org.apache.hadoop.mapreduce.v2.app.job.Task mockTask = mock(org.apache.hadoop.mapreduce.v2.app.job.Task.class);\r\n    when(mockTask.canCommit(any(TaskAttemptId.class))).thenReturn(true);\r\n    Job mockJob = mock(Job.class);\r\n    when(mockJob.getTask(any(TaskId.class))).thenReturn(mockTask);\r\n    when(appCtx.getJob(any(JobId.class))).thenReturn(mockJob);\r\n    when(appCtx.getClock()).thenReturn(clock);\r\n    listener = new MockTaskAttemptListenerImpl(appCtx, secret, rmHeartbeatHandler, policy) {\r\n\r\n        @Override\r\n        protected void registerHeartbeatHandler(Configuration conf) {\r\n            taskHeartbeatHandler = hbHandler;\r\n        }\r\n    };\r\n    Configuration conf = new Configuration();\r\n    listener.init(conf);\r\n    listener.start();\r\n    TaskAttemptID tid = new TaskAttemptID(\"12345\", 1, TaskType.REDUCE, 1, 0);\r\n    boolean canCommit = listener.canCommit(tid);\r\n    assertFalse(canCommit);\r\n    verify(mockTask, never()).canCommit(any(TaskAttemptId.class));\r\n    when(rmHeartbeatHandler.getLastHeartbeatTime()).thenReturn(clock.getTime());\r\n    canCommit = listener.canCommit(tid);\r\n    assertTrue(canCommit);\r\n    verify(mockTask, times(1)).canCommit(any(TaskAttemptId.class));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "testCheckpointIDTracking",
  "errType" : null,
  "containingMethodsNum" : 19,
  "sourceCodeText" : "void testCheckpointIDTracking() throws IOException, InterruptedException\n{\r\n    configureMocks();\r\n    listener = new MockTaskAttemptListenerImpl(appCtx, secret, rmHeartbeatHandler, policy) {\r\n\r\n        @Override\r\n        protected void registerHeartbeatHandler(Configuration conf) {\r\n            taskHeartbeatHandler = hbHandler;\r\n        }\r\n    };\r\n    Configuration conf = new Configuration();\r\n    conf.setBoolean(MRJobConfig.TASK_PREEMPTION, true);\r\n    listener.init(conf);\r\n    listener.start();\r\n    TaskAttemptID tid = new TaskAttemptID(\"12345\", 1, TaskType.REDUCE, 1, 0);\r\n    List<Path> partialOut = new ArrayList<Path>();\r\n    partialOut.add(new Path(\"/prev1\"));\r\n    partialOut.add(new Path(\"/prev2\"));\r\n    Counters counters = mock(Counters.class);\r\n    final long CBYTES = 64L * 1024 * 1024;\r\n    final long CTIME = 4344L;\r\n    final Path CLOC = new Path(\"/test/1\");\r\n    Counter cbytes = mock(Counter.class);\r\n    when(cbytes.getValue()).thenReturn(CBYTES);\r\n    Counter ctime = mock(Counter.class);\r\n    when(ctime.getValue()).thenReturn(CTIME);\r\n    when(counters.findCounter(eq(EnumCounter.CHECKPOINT_BYTES))).thenReturn(cbytes);\r\n    when(counters.findCounter(eq(EnumCounter.CHECKPOINT_MS))).thenReturn(ctime);\r\n    TaskCheckpointID incid = new TaskCheckpointID(new FSCheckpointID(CLOC), partialOut, counters);\r\n    listener.setCheckpointID(org.apache.hadoop.mapred.TaskID.downgrade(tid.getTaskID()), incid);\r\n    CheckpointID outcid = listener.getCheckpointID(tid.getTaskID());\r\n    TaskCheckpointID tcid = (TaskCheckpointID) outcid;\r\n    assertEquals(CBYTES, tcid.getCheckpointBytes());\r\n    assertEquals(CTIME, tcid.getCheckpointTime());\r\n    assertTrue(partialOut.containsAll(tcid.getPartialCommittedOutput()));\r\n    assertTrue(tcid.getPartialCommittedOutput().containsAll(partialOut));\r\n    assert outcid == incid;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "testStatusUpdateProgress",
  "errType" : null,
  "containingMethodsNum" : 9,
  "sourceCodeText" : "void testStatusUpdateProgress() throws IOException, InterruptedException\n{\r\n    configureMocks();\r\n    startListener(true);\r\n    verify(hbHandler).register(attemptId);\r\n    AMFeedback feedback = listener.statusUpdate(attemptID, null);\r\n    assertTrue(feedback.getTaskFound());\r\n    verify(hbHandler, never()).progressing(eq(attemptId));\r\n    MapTaskStatus mockStatus = new MapTaskStatus(attemptID, 0.0f, 1, TaskStatus.State.RUNNING, \"\", \"RUNNING\", \"\", TaskStatus.Phase.MAP, new Counters());\r\n    feedback = listener.statusUpdate(attemptID, mockStatus);\r\n    assertTrue(feedback.getTaskFound());\r\n    verify(hbHandler).progressing(eq(attemptId));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "testSingleStatusUpdate",
  "errType" : null,
  "containingMethodsNum" : 9,
  "sourceCodeText" : "void testSingleStatusUpdate() throws IOException, InterruptedException\n{\r\n    configureMocks();\r\n    startListener(true);\r\n    listener.statusUpdate(attemptID, firstReduceStatus);\r\n    verify(ea).handle(eventCaptor.capture());\r\n    TaskAttemptStatusUpdateEvent updateEvent = (TaskAttemptStatusUpdateEvent) eventCaptor.getValue();\r\n    TaskAttemptStatus status = updateEvent.getTaskAttemptStatusRef().get();\r\n    assertTrue(status.fetchFailedMaps.contains(TASKATTEMPTID1));\r\n    assertEquals(1, status.fetchFailedMaps.size());\r\n    assertEquals(Phase.SHUFFLE, status.phase);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "testStatusUpdateEventCoalescing",
  "errType" : null,
  "containingMethodsNum" : 11,
  "sourceCodeText" : "void testStatusUpdateEventCoalescing() throws IOException, InterruptedException\n{\r\n    configureMocks();\r\n    startListener(true);\r\n    listener.statusUpdate(attemptID, firstReduceStatus);\r\n    listener.statusUpdate(attemptID, secondReduceStatus);\r\n    verify(ea).handle(any(Event.class));\r\n    ConcurrentMap<TaskAttemptId, AtomicReference<TaskAttemptStatus>> attemptIdToStatus = listener.getAttemptIdToStatus();\r\n    TaskAttemptStatus status = attemptIdToStatus.get(attemptId).get();\r\n    assertTrue(status.fetchFailedMaps.contains(TASKATTEMPTID1));\r\n    assertTrue(status.fetchFailedMaps.contains(TASKATTEMPTID2));\r\n    assertEquals(2, status.fetchFailedMaps.size());\r\n    assertEquals(Phase.SORT, status.phase);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "testCoalescedStatusUpdatesCleared",
  "errType" : null,
  "containingMethodsNum" : 12,
  "sourceCodeText" : "void testCoalescedStatusUpdatesCleared() throws IOException, InterruptedException\n{\r\n    configureMocks();\r\n    startListener(true);\r\n    listener.statusUpdate(attemptID, firstReduceStatus);\r\n    listener.statusUpdate(attemptID, secondReduceStatus);\r\n    ConcurrentMap<TaskAttemptId, AtomicReference<TaskAttemptStatus>> attemptIdToStatus = listener.getAttemptIdToStatus();\r\n    attemptIdToStatus.get(attemptId).set(null);\r\n    listener.statusUpdate(attemptID, thirdReduceStatus);\r\n    verify(ea, times(2)).handle(eventCaptor.capture());\r\n    TaskAttemptStatusUpdateEvent updateEvent = (TaskAttemptStatusUpdateEvent) eventCaptor.getValue();\r\n    TaskAttemptStatus status = updateEvent.getTaskAttemptStatusRef().get();\r\n    assertNull(status.fetchFailedMaps);\r\n    assertEquals(Phase.REDUCE, status.phase);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "testStatusUpdateFromUnregisteredTask",
  "errType" : [ "Exception" ],
  "containingMethodsNum" : 20,
  "sourceCodeText" : "void testStatusUpdateFromUnregisteredTask() throws Exception\n{\r\n    configureMocks();\r\n    ControlledClock clock = new ControlledClock();\r\n    clock.setTime(0);\r\n    doReturn(clock).when(appCtx).getClock();\r\n    final TaskAttemptListenerImpl tal = new TaskAttemptListenerImpl(appCtx, secret, rmHeartbeatHandler, policy) {\r\n\r\n        @Override\r\n        protected void startRpcServer() {\r\n        }\r\n\r\n        @Override\r\n        protected void stopRpcServer() {\r\n        }\r\n    };\r\n    Configuration conf = new Configuration();\r\n    conf.setLong(MRJobConfig.TASK_TIMEOUT_CHECK_INTERVAL_MS, 1);\r\n    conf.setDouble(MRJobConfig.TASK_LOG_PROGRESS_DELTA_THRESHOLD, 0.01);\r\n    conf.setDouble(MRJobConfig.TASK_LOG_PROGRESS_WAIT_INTERVAL_SECONDS, 1);\r\n    tal.init(conf);\r\n    tal.start();\r\n    AMFeedback feedback = tal.statusUpdate(attemptID, firstReduceStatus);\r\n    assertFalse(feedback.getTaskFound());\r\n    tal.registerPendingTask(task, wid);\r\n    tal.registerLaunchedTask(attemptId, wid);\r\n    feedback = tal.statusUpdate(attemptID, firstReduceStatus);\r\n    assertTrue(feedback.getTaskFound());\r\n    tal.unregister(attemptId, wid);\r\n    feedback = tal.statusUpdate(attemptID, firstReduceStatus);\r\n    assertTrue(feedback.getTaskFound());\r\n    long unregisterTimeout = conf.getLong(MRJobConfig.TASK_EXIT_TIMEOUT, MRJobConfig.TASK_EXIT_TIMEOUT_DEFAULT);\r\n    clock.setTime(unregisterTimeout + 1);\r\n    GenericTestUtils.waitFor(new Supplier<Boolean>() {\r\n\r\n        @Override\r\n        public Boolean get() {\r\n            try {\r\n                AMFeedback response = tal.statusUpdate(attemptID, firstReduceStatus);\r\n                return !response.getTaskFound();\r\n            } catch (Exception e) {\r\n                throw new RuntimeException(\"status update failed\", e);\r\n            }\r\n        }\r\n    }, 10, 10000);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "configureMocks",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void configureMocks()\n{\r\n    firstReduceStatus = new ReduceTaskStatus(attemptID, 0.0f, 1, TaskStatus.State.RUNNING, \"\", \"RUNNING\", \"\", TaskStatus.Phase.SHUFFLE, new Counters());\r\n    firstReduceStatus.addFetchFailedMap(TaskAttemptID.forName(ATTEMPT1_ID));\r\n    secondReduceStatus = new ReduceTaskStatus(attemptID, 0.0f, 1, TaskStatus.State.RUNNING, \"\", \"RUNNING\", \"\", TaskStatus.Phase.SORT, new Counters());\r\n    secondReduceStatus.addFetchFailedMap(TaskAttemptID.forName(ATTEMPT2_ID));\r\n    thirdReduceStatus = new ReduceTaskStatus(attemptID, 0.0f, 1, TaskStatus.State.RUNNING, \"\", \"RUNNING\", \"\", TaskStatus.Phase.REDUCE, new Counters());\r\n    when(appCtx.getEventHandler()).thenReturn(ea);\r\n    policy = new CheckpointAMPreemptionPolicy();\r\n    policy.init(appCtx);\r\n    listener = new MockTaskAttemptListenerImpl(appCtx, secret, rmHeartbeatHandler, hbHandler, policy);\r\n    id = new JVMId(\"foo\", 1, true, 1);\r\n    wid = new WrappedJvmID(id.getJobId(), id.isMap, id.getId());\r\n    attemptID = new TaskAttemptID(\"1\", 1, TaskType.MAP, 1, 1);\r\n    attemptId = TypeConverter.toYarn(attemptID);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "startListener",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void startListener(boolean registerTask)\n{\r\n    Configuration conf = new Configuration();\r\n    listener.init(conf);\r\n    listener.start();\r\n    if (registerTask) {\r\n        listener.registerPendingTask(task, wid);\r\n        listener.registerLaunchedTask(attemptId, wid);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\launcher",
  "methodName" : "testPoolSize",
  "errType" : null,
  "containingMethodsNum" : 38,
  "sourceCodeText" : "void testPoolSize() throws InterruptedException\n{\r\n    ApplicationId appId = ApplicationId.newInstance(12345, 67);\r\n    ApplicationAttemptId appAttemptId = ApplicationAttemptId.newInstance(appId, 3);\r\n    JobId jobId = MRBuilderUtils.newJobId(appId, 8);\r\n    TaskId taskId = MRBuilderUtils.newTaskId(jobId, 9, TaskType.MAP);\r\n    AppContext context = mock(AppContext.class);\r\n    CustomContainerLauncher containerLauncher = new CustomContainerLauncher(context);\r\n    containerLauncher.init(new Configuration());\r\n    containerLauncher.start();\r\n    ThreadPoolExecutor threadPool = containerLauncher.getThreadPool();\r\n    assertThat(containerLauncher.initialPoolSize).isEqualTo(MRJobConfig.DEFAULT_MR_AM_CONTAINERLAUNCHER_THREADPOOL_INITIAL_SIZE);\r\n    Assert.assertEquals(0, threadPool.getPoolSize());\r\n    Assert.assertEquals(containerLauncher.initialPoolSize, threadPool.getCorePoolSize());\r\n    Assert.assertNull(containerLauncher.foundErrors);\r\n    containerLauncher.expectedCorePoolSize = containerLauncher.initialPoolSize;\r\n    for (int i = 0; i < 10; i++) {\r\n        ContainerId containerId = ContainerId.newContainerId(appAttemptId, i);\r\n        TaskAttemptId taskAttemptId = MRBuilderUtils.newTaskAttemptId(taskId, i);\r\n        containerLauncher.handle(new ContainerLauncherEvent(taskAttemptId, containerId, \"host\" + i + \":1234\", null, ContainerLauncher.EventType.CONTAINER_REMOTE_LAUNCH));\r\n    }\r\n    waitForEvents(containerLauncher, 10);\r\n    Assert.assertEquals(10, threadPool.getPoolSize());\r\n    Assert.assertNull(containerLauncher.foundErrors);\r\n    containerLauncher.finishEventHandling = true;\r\n    int timeOut = 0;\r\n    while (containerLauncher.numEventsProcessed.get() < 10 && timeOut++ < 200) {\r\n        LOG.info(\"Waiting for number of events processed to become \" + 10 + \". It is now \" + containerLauncher.numEventsProcessed.get() + \". Timeout is \" + timeOut);\r\n        Thread.sleep(1000);\r\n    }\r\n    Assert.assertEquals(10, containerLauncher.numEventsProcessed.get());\r\n    containerLauncher.finishEventHandling = false;\r\n    for (int i = 0; i < 10; i++) {\r\n        ContainerId containerId = ContainerId.newContainerId(appAttemptId, i + 10);\r\n        TaskAttemptId taskAttemptId = MRBuilderUtils.newTaskAttemptId(taskId, i + 10);\r\n        containerLauncher.handle(new ContainerLauncherEvent(taskAttemptId, containerId, \"host\" + i + \":1234\", null, ContainerLauncher.EventType.CONTAINER_REMOTE_LAUNCH));\r\n    }\r\n    waitForEvents(containerLauncher, 20);\r\n    Assert.assertEquals(10, threadPool.getPoolSize());\r\n    Assert.assertNull(containerLauncher.foundErrors);\r\n    containerLauncher.expectedCorePoolSize = 11 + containerLauncher.initialPoolSize;\r\n    containerLauncher.finishEventHandling = false;\r\n    ContainerId containerId = ContainerId.newContainerId(appAttemptId, 21);\r\n    TaskAttemptId taskAttemptId = MRBuilderUtils.newTaskAttemptId(taskId, 21);\r\n    containerLauncher.handle(new ContainerLauncherEvent(taskAttemptId, containerId, \"host11:1234\", null, ContainerLauncher.EventType.CONTAINER_REMOTE_LAUNCH));\r\n    waitForEvents(containerLauncher, 21);\r\n    Assert.assertEquals(11, threadPool.getPoolSize());\r\n    Assert.assertNull(containerLauncher.foundErrors);\r\n    containerLauncher.stop();\r\n    Configuration conf = new Configuration();\r\n    conf.setInt(MRJobConfig.MR_AM_CONTAINERLAUNCHER_THREADPOOL_INITIAL_SIZE, 20);\r\n    containerLauncher = new CustomContainerLauncher(context);\r\n    containerLauncher.init(conf);\r\n    assertThat(containerLauncher.initialPoolSize).isEqualTo(20);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\launcher",
  "methodName" : "testPoolLimits",
  "errType" : null,
  "containingMethodsNum" : 23,
  "sourceCodeText" : "void testPoolLimits() throws InterruptedException\n{\r\n    ApplicationId appId = ApplicationId.newInstance(12345, 67);\r\n    ApplicationAttemptId appAttemptId = ApplicationAttemptId.newInstance(appId, 3);\r\n    JobId jobId = MRBuilderUtils.newJobId(appId, 8);\r\n    TaskId taskId = MRBuilderUtils.newTaskId(jobId, 9, TaskType.MAP);\r\n    TaskAttemptId taskAttemptId = MRBuilderUtils.newTaskAttemptId(taskId, 0);\r\n    ContainerId containerId = ContainerId.newContainerId(appAttemptId, 10);\r\n    AppContext context = mock(AppContext.class);\r\n    CustomContainerLauncher containerLauncher = new CustomContainerLauncher(context);\r\n    Configuration conf = new Configuration();\r\n    conf.setInt(MRJobConfig.MR_AM_CONTAINERLAUNCHER_THREAD_COUNT_LIMIT, 12);\r\n    containerLauncher.init(conf);\r\n    containerLauncher.start();\r\n    ThreadPoolExecutor threadPool = containerLauncher.getThreadPool();\r\n    containerLauncher.expectedCorePoolSize = containerLauncher.initialPoolSize;\r\n    for (int i = 0; i < 10; i++) {\r\n        containerLauncher.handle(new ContainerLauncherEvent(taskAttemptId, containerId, \"host\" + i + \":1234\", null, ContainerLauncher.EventType.CONTAINER_REMOTE_LAUNCH));\r\n    }\r\n    waitForEvents(containerLauncher, 10);\r\n    Assert.assertEquals(10, threadPool.getPoolSize());\r\n    Assert.assertNull(containerLauncher.foundErrors);\r\n    containerLauncher.expectedCorePoolSize = 12;\r\n    for (int i = 1; i <= 4; i++) {\r\n        containerLauncher.handle(new ContainerLauncherEvent(taskAttemptId, containerId, \"host1\" + i + \":1234\", null, ContainerLauncher.EventType.CONTAINER_REMOTE_LAUNCH));\r\n    }\r\n    waitForEvents(containerLauncher, 12);\r\n    Assert.assertEquals(12, threadPool.getPoolSize());\r\n    Assert.assertNull(containerLauncher.foundErrors);\r\n    containerLauncher.finishEventHandling = true;\r\n    waitForEvents(containerLauncher, 14);\r\n    Assert.assertEquals(12, threadPool.getPoolSize());\r\n    Assert.assertNull(containerLauncher.foundErrors);\r\n    containerLauncher.stop();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\launcher",
  "methodName" : "waitForEvents",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void waitForEvents(CustomContainerLauncher containerLauncher, int expectedNumEvents) throws InterruptedException\n{\r\n    int timeOut = 0;\r\n    while (containerLauncher.numEventsProcessing.get() < expectedNumEvents && timeOut++ < 20) {\r\n        LOG.info(\"Waiting for number of events to become \" + expectedNumEvents + \". It is now \" + containerLauncher.numEventsProcessing.get());\r\n        Thread.sleep(1000);\r\n    }\r\n    Assert.assertEquals(expectedNumEvents, containerLauncher.numEventsProcessing.get());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\launcher",
  "methodName" : "testSlowNM",
  "errType" : null,
  "containingMethodsNum" : 29,
  "sourceCodeText" : "void testSlowNM() throws Exception\n{\r\n    conf = new Configuration();\r\n    int maxAttempts = 1;\r\n    conf.setInt(MRJobConfig.MAP_MAX_ATTEMPTS, maxAttempts);\r\n    conf.setBoolean(MRJobConfig.JOB_UBERTASK_ENABLE, false);\r\n    conf.setInt(\"yarn.rpc.nm-command-timeout\", 3000);\r\n    conf.set(YarnConfiguration.IPC_RPC_IMPL, HadoopYarnProtoRPC.class.getName());\r\n    YarnRPC rpc = YarnRPC.create(conf);\r\n    String bindAddr = \"localhost:0\";\r\n    InetSocketAddress addr = NetUtils.createSocketAddr(bindAddr);\r\n    NMTokenSecretManagerInNM tokenSecretManager = new NMTokenSecretManagerInNM();\r\n    MasterKey masterKey = Records.newRecord(MasterKey.class);\r\n    masterKey.setBytes(ByteBuffer.wrap(\"key\".getBytes()));\r\n    tokenSecretManager.setMasterKey(masterKey);\r\n    conf.set(CommonConfigurationKeysPublic.HADOOP_SECURITY_AUTHENTICATION, \"token\");\r\n    server = rpc.getServer(ContainerManagementProtocol.class, new DummyContainerManager(), addr, conf, tokenSecretManager, 1);\r\n    server.start();\r\n    MRApp app = new MRAppWithSlowNM(tokenSecretManager);\r\n    try {\r\n        Job job = app.submit(conf);\r\n        app.waitForState(job, JobState.RUNNING);\r\n        Map<TaskId, Task> tasks = job.getTasks();\r\n        Assert.assertEquals(\"Num tasks is not correct\", 1, tasks.size());\r\n        Task task = tasks.values().iterator().next();\r\n        app.waitForState(task, TaskState.SCHEDULED);\r\n        Map<TaskAttemptId, TaskAttempt> attempts = tasks.values().iterator().next().getAttempts();\r\n        Assert.assertEquals(\"Num attempts is not correct\", maxAttempts, attempts.size());\r\n        TaskAttempt attempt = attempts.values().iterator().next();\r\n        app.waitForInternalState((TaskAttemptImpl) attempt, TaskAttemptStateInternal.ASSIGNED);\r\n        app.waitForState(job, JobState.FAILED);\r\n        String diagnostics = attempt.getDiagnostics().toString();\r\n        LOG.info(\"attempt.getDiagnostics: \" + diagnostics);\r\n        Assert.assertTrue(diagnostics.contains(\"Container launch failed for \" + \"container_0_0000_01_000000 : \"));\r\n        Assert.assertTrue(diagnostics.contains(\"java.net.SocketTimeoutException: 3000 millis timeout while waiting for channel\"));\r\n    } finally {\r\n        server.stop();\r\n        app.stop();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\job\\impl",
  "methodName" : "setup",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setup()\n{\r\n    File dir = new File(stagingDir);\r\n    stagingDir = dir.getAbsolutePath();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\job\\impl",
  "methodName" : "cleanup",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void cleanup() throws IOException\n{\r\n    File dir = new File(stagingDir);\r\n    if (dir.exists()) {\r\n        FileUtils.deleteDirectory(dir);\r\n    }\r\n    dir.mkdirs();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\job\\impl",
  "methodName" : "testJobNoTasks",
  "errType" : [ "InterruptedException" ],
  "containingMethodsNum" : 24,
  "sourceCodeText" : "void testJobNoTasks()\n{\r\n    Configuration conf = new Configuration();\r\n    conf.setInt(MRJobConfig.NUM_REDUCES, 0);\r\n    conf.set(MRJobConfig.MR_AM_STAGING_DIR, stagingDir);\r\n    conf.set(MRJobConfig.WORKFLOW_ID, \"testId\");\r\n    conf.set(MRJobConfig.WORKFLOW_NAME, \"testName\");\r\n    conf.set(MRJobConfig.WORKFLOW_NODE_NAME, \"testNodeName\");\r\n    conf.set(MRJobConfig.WORKFLOW_ADJACENCY_PREFIX_STRING + \"key1\", \"value1\");\r\n    conf.set(MRJobConfig.WORKFLOW_ADJACENCY_PREFIX_STRING + \"key2\", \"value2\");\r\n    conf.set(MRJobConfig.WORKFLOW_TAGS, \"tag1,tag2\");\r\n    AsyncDispatcher dispatcher = new AsyncDispatcher();\r\n    dispatcher.init(conf);\r\n    dispatcher.start();\r\n    OutputCommitter committer = mock(OutputCommitter.class);\r\n    CommitterEventHandler commitHandler = createCommitterEventHandler(dispatcher, committer);\r\n    commitHandler.init(conf);\r\n    commitHandler.start();\r\n    JobSubmittedEventHandler jseHandler = new JobSubmittedEventHandler(\"testId\", \"testName\", \"testNodeName\", \"\\\"key2\\\"=\\\"value2\\\" \\\"key1\\\"=\\\"value1\\\" \", \"tag1,tag2\");\r\n    dispatcher.register(EventType.class, jseHandler);\r\n    JobImpl job = createStubbedJob(conf, dispatcher, 0, null);\r\n    job.handle(new JobEvent(job.getID(), JobEventType.JOB_INIT));\r\n    assertJobState(job, JobStateInternal.INITED);\r\n    job.handle(new JobStartEvent(job.getID()));\r\n    assertJobState(job, JobStateInternal.SUCCEEDED);\r\n    dispatcher.stop();\r\n    commitHandler.stop();\r\n    try {\r\n        Assert.assertTrue(jseHandler.getAssertValue());\r\n    } catch (InterruptedException e) {\r\n        Assert.fail(\"Workflow related attributes are not tested properly\");\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\job\\impl",
  "methodName" : "testCommitJobFailsJob",
  "errType" : null,
  "containingMethodsNum" : 13,
  "sourceCodeText" : "void testCommitJobFailsJob() throws Exception\n{\r\n    Configuration conf = new Configuration();\r\n    conf.set(MRJobConfig.MR_AM_STAGING_DIR, stagingDir);\r\n    AsyncDispatcher dispatcher = new AsyncDispatcher();\r\n    dispatcher.init(conf);\r\n    dispatcher.start();\r\n    CyclicBarrier syncBarrier = new CyclicBarrier(2);\r\n    OutputCommitter committer = new TestingOutputCommitter(syncBarrier, false);\r\n    CommitterEventHandler commitHandler = createCommitterEventHandler(dispatcher, committer);\r\n    commitHandler.init(conf);\r\n    commitHandler.start();\r\n    JobImpl job = createRunningStubbedJob(conf, dispatcher, 2, null);\r\n    completeJobTasks(job);\r\n    assertJobState(job, JobStateInternal.COMMITTING);\r\n    syncBarrier.await();\r\n    assertJobState(job, JobStateInternal.FAILED);\r\n    dispatcher.stop();\r\n    commitHandler.stop();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\job\\impl",
  "methodName" : "testCheckJobCompleteSuccess",
  "errType" : null,
  "containingMethodsNum" : 27,
  "sourceCodeText" : "void testCheckJobCompleteSuccess() throws Exception\n{\r\n    Configuration conf = new Configuration();\r\n    conf.set(MRJobConfig.MR_AM_STAGING_DIR, stagingDir);\r\n    DrainDispatcher dispatcher = new DrainDispatcher();\r\n    dispatcher.init(conf);\r\n    dispatcher.start();\r\n    CyclicBarrier syncBarrier = new CyclicBarrier(2);\r\n    OutputCommitter committer = new TestingOutputCommitter(syncBarrier, true);\r\n    CommitterEventHandler commitHandler = createCommitterEventHandler(dispatcher, committer);\r\n    commitHandler.init(conf);\r\n    commitHandler.start();\r\n    JobImpl job = createRunningStubbedJob(conf, dispatcher, 2, null);\r\n    completeJobTasks(job);\r\n    assertJobState(job, JobStateInternal.COMMITTING);\r\n    job.handle(new JobEvent(job.getID(), JobEventType.JOB_TASK_ATTEMPT_COMPLETED));\r\n    assertJobState(job, JobStateInternal.COMMITTING);\r\n    job.handle(new JobEvent(job.getID(), JobEventType.JOB_MAP_TASK_RESCHEDULED));\r\n    assertJobState(job, JobStateInternal.COMMITTING);\r\n    job.handle(new JobEvent(job.getID(), JobEventType.JOB_TASK_COMPLETED));\r\n    dispatcher.await();\r\n    assertJobState(job, JobStateInternal.COMMITTING);\r\n    syncBarrier.await();\r\n    assertJobState(job, JobStateInternal.SUCCEEDED);\r\n    job.handle(new JobEvent(job.getID(), JobEventType.JOB_TASK_ATTEMPT_COMPLETED));\r\n    assertJobState(job, JobStateInternal.SUCCEEDED);\r\n    job.handle(new JobEvent(job.getID(), JobEventType.JOB_MAP_TASK_RESCHEDULED));\r\n    assertJobState(job, JobStateInternal.SUCCEEDED);\r\n    job.handle(new JobEvent(job.getID(), JobEventType.JOB_TASK_COMPLETED));\r\n    dispatcher.await();\r\n    assertJobState(job, JobStateInternal.SUCCEEDED);\r\n    dispatcher.stop();\r\n    commitHandler.stop();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\job\\impl",
  "methodName" : "testRebootedDuringSetup",
  "errType" : [ "InterruptedException" ],
  "containingMethodsNum" : 21,
  "sourceCodeText" : "void testRebootedDuringSetup() throws Exception\n{\r\n    Configuration conf = new Configuration();\r\n    conf.set(MRJobConfig.MR_AM_STAGING_DIR, stagingDir);\r\n    AsyncDispatcher dispatcher = new AsyncDispatcher();\r\n    dispatcher.init(conf);\r\n    dispatcher.start();\r\n    OutputCommitter committer = new StubbedOutputCommitter() {\r\n\r\n        @Override\r\n        public synchronized void setupJob(JobContext jobContext) throws IOException {\r\n            while (!Thread.interrupted()) {\r\n                try {\r\n                    wait();\r\n                } catch (InterruptedException e) {\r\n                }\r\n            }\r\n        }\r\n    };\r\n    CommitterEventHandler commitHandler = createCommitterEventHandler(dispatcher, committer);\r\n    commitHandler.init(conf);\r\n    commitHandler.start();\r\n    AppContext mockContext = mock(AppContext.class);\r\n    when(mockContext.isLastAMRetry()).thenReturn(false);\r\n    JobImpl job = createStubbedJob(conf, dispatcher, 2, mockContext);\r\n    JobId jobId = job.getID();\r\n    job.handle(new JobEvent(jobId, JobEventType.JOB_INIT));\r\n    assertJobState(job, JobStateInternal.INITED);\r\n    job.handle(new JobStartEvent(jobId));\r\n    assertJobState(job, JobStateInternal.SETUP);\r\n    job.handle(new JobEvent(job.getID(), JobEventType.JOB_AM_REBOOT));\r\n    assertJobState(job, JobStateInternal.REBOOT);\r\n    Assert.assertEquals(JobState.RUNNING, job.getState());\r\n    dispatcher.stop();\r\n    commitHandler.stop();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\job\\impl",
  "methodName" : "testRebootedDuringCommit",
  "errType" : null,
  "containingMethodsNum" : 21,
  "sourceCodeText" : "void testRebootedDuringCommit() throws Exception\n{\r\n    Configuration conf = new Configuration();\r\n    conf.set(MRJobConfig.MR_AM_STAGING_DIR, stagingDir);\r\n    conf.setInt(MRJobConfig.MR_AM_MAX_ATTEMPTS, 2);\r\n    AsyncDispatcher dispatcher = new AsyncDispatcher();\r\n    dispatcher.init(conf);\r\n    dispatcher.start();\r\n    CyclicBarrier syncBarrier = new CyclicBarrier(2);\r\n    OutputCommitter committer = new WaitingOutputCommitter(syncBarrier, true);\r\n    CommitterEventHandler commitHandler = createCommitterEventHandler(dispatcher, committer);\r\n    commitHandler.init(conf);\r\n    commitHandler.start();\r\n    AppContext mockContext = mock(AppContext.class);\r\n    when(mockContext.isLastAMRetry()).thenReturn(true);\r\n    when(mockContext.hasSuccessfullyUnregistered()).thenReturn(false);\r\n    JobImpl job = createRunningStubbedJob(conf, dispatcher, 2, mockContext);\r\n    completeJobTasks(job);\r\n    assertJobState(job, JobStateInternal.COMMITTING);\r\n    syncBarrier.await();\r\n    job.handle(new JobEvent(job.getID(), JobEventType.JOB_AM_REBOOT));\r\n    assertJobState(job, JobStateInternal.REBOOT);\r\n    Assert.assertEquals(JobState.RUNNING, job.getState());\r\n    when(mockContext.hasSuccessfullyUnregistered()).thenReturn(true);\r\n    Assert.assertEquals(JobState.ERROR, job.getState());\r\n    dispatcher.stop();\r\n    commitHandler.stop();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\job\\impl",
  "methodName" : "testKilledDuringSetup",
  "errType" : [ "InterruptedException" ],
  "containingMethodsNum" : 18,
  "sourceCodeText" : "void testKilledDuringSetup() throws Exception\n{\r\n    Configuration conf = new Configuration();\r\n    conf.set(MRJobConfig.MR_AM_STAGING_DIR, stagingDir);\r\n    AsyncDispatcher dispatcher = new AsyncDispatcher();\r\n    dispatcher.init(conf);\r\n    dispatcher.start();\r\n    OutputCommitter committer = new StubbedOutputCommitter() {\r\n\r\n        @Override\r\n        public synchronized void setupJob(JobContext jobContext) throws IOException {\r\n            while (!Thread.interrupted()) {\r\n                try {\r\n                    wait();\r\n                } catch (InterruptedException e) {\r\n                }\r\n            }\r\n        }\r\n    };\r\n    CommitterEventHandler commitHandler = createCommitterEventHandler(dispatcher, committer);\r\n    commitHandler.init(conf);\r\n    commitHandler.start();\r\n    JobImpl job = createStubbedJob(conf, dispatcher, 2, null);\r\n    JobId jobId = job.getID();\r\n    job.handle(new JobEvent(jobId, JobEventType.JOB_INIT));\r\n    assertJobState(job, JobStateInternal.INITED);\r\n    job.handle(new JobStartEvent(jobId));\r\n    assertJobState(job, JobStateInternal.SETUP);\r\n    job.handle(new JobEvent(job.getID(), JobEventType.JOB_KILL));\r\n    assertJobState(job, JobStateInternal.KILLED);\r\n    dispatcher.stop();\r\n    commitHandler.stop();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\job\\impl",
  "methodName" : "testKilledDuringCommit",
  "errType" : null,
  "containingMethodsNum" : 14,
  "sourceCodeText" : "void testKilledDuringCommit() throws Exception\n{\r\n    Configuration conf = new Configuration();\r\n    conf.set(MRJobConfig.MR_AM_STAGING_DIR, stagingDir);\r\n    AsyncDispatcher dispatcher = new AsyncDispatcher();\r\n    dispatcher.init(conf);\r\n    dispatcher.start();\r\n    CyclicBarrier syncBarrier = new CyclicBarrier(2);\r\n    OutputCommitter committer = new WaitingOutputCommitter(syncBarrier, true);\r\n    CommitterEventHandler commitHandler = createCommitterEventHandler(dispatcher, committer);\r\n    commitHandler.init(conf);\r\n    commitHandler.start();\r\n    JobImpl job = createRunningStubbedJob(conf, dispatcher, 2, null);\r\n    completeJobTasks(job);\r\n    assertJobState(job, JobStateInternal.COMMITTING);\r\n    syncBarrier.await();\r\n    job.handle(new JobEvent(job.getID(), JobEventType.JOB_KILL));\r\n    assertJobState(job, JobStateInternal.KILLED);\r\n    dispatcher.stop();\r\n    commitHandler.stop();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\job\\impl",
  "methodName" : "testAbortJobCalledAfterKillingTasks",
  "errType" : null,
  "containingMethodsNum" : 15,
  "sourceCodeText" : "void testAbortJobCalledAfterKillingTasks() throws IOException\n{\r\n    Configuration conf = new Configuration();\r\n    conf.set(MRJobConfig.MR_AM_STAGING_DIR, stagingDir);\r\n    conf.set(MRJobConfig.MR_AM_COMMITTER_CANCEL_TIMEOUT_MS, \"1000\");\r\n    InlineDispatcher dispatcher = new InlineDispatcher();\r\n    dispatcher.init(conf);\r\n    dispatcher.start();\r\n    OutputCommitter committer = Mockito.mock(OutputCommitter.class);\r\n    CommitterEventHandler commitHandler = createCommitterEventHandler(dispatcher, committer);\r\n    commitHandler.init(conf);\r\n    commitHandler.start();\r\n    JobImpl job = createRunningStubbedJob(conf, dispatcher, 2, null);\r\n    job.handle(new JobTaskEvent(MRBuilderUtils.newTaskId(job.getID(), 1, TaskType.MAP), TaskState.FAILED));\r\n    Mockito.verify(committer, Mockito.never()).abortJob((JobContext) Mockito.any(), (State) Mockito.any());\r\n    assertJobState(job, JobStateInternal.FAIL_WAIT);\r\n    Mockito.verify(committer, Mockito.timeout(2000).times(1)).abortJob((JobContext) Mockito.any(), (State) Mockito.any());\r\n    assertJobState(job, JobStateInternal.FAILED);\r\n    dispatcher.stop();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\job\\impl",
  "methodName" : "testFailAbortDoesntHang",
  "errType" : null,
  "containingMethodsNum" : 19,
  "sourceCodeText" : "void testFailAbortDoesntHang() throws IOException\n{\r\n    Configuration conf = new Configuration();\r\n    conf.set(MRJobConfig.MR_AM_STAGING_DIR, stagingDir);\r\n    conf.set(MRJobConfig.MR_AM_COMMITTER_CANCEL_TIMEOUT_MS, \"1000\");\r\n    DrainDispatcher dispatcher = new DrainDispatcher();\r\n    dispatcher.init(conf);\r\n    dispatcher.start();\r\n    OutputCommitter committer = Mockito.mock(OutputCommitter.class);\r\n    CommitterEventHandler commitHandler = createCommitterEventHandler(dispatcher, committer);\r\n    commitHandler.init(conf);\r\n    commitHandler.start();\r\n    conf.setInt(MRJobConfig.NUM_REDUCES, 0);\r\n    conf.setInt(MRJobConfig.MAP_MAX_ATTEMPTS, 1);\r\n    JobImpl job = createRunningStubbedJob(conf, dispatcher, 1, null);\r\n    for (Task t : job.tasks.values()) {\r\n        TaskImpl task = (TaskImpl) t;\r\n        task.handle(new TaskEvent(task.getID(), TaskEventType.T_SCHEDULE));\r\n        for (TaskAttempt ta : task.getAttempts().values()) {\r\n            task.handle(new TaskTAttemptFailedEvent(ta.getID()));\r\n        }\r\n    }\r\n    dispatcher.await();\r\n    Mockito.verify(committer, Mockito.timeout(2000).times(1)).abortJob((JobContext) Mockito.any(), (State) Mockito.any());\r\n    assertJobState(job, JobStateInternal.FAILED);\r\n    dispatcher.stop();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\job\\impl",
  "methodName" : "testKilledDuringFailAbort",
  "errType" : [ "InterruptedException" ],
  "containingMethodsNum" : 18,
  "sourceCodeText" : "void testKilledDuringFailAbort() throws Exception\n{\r\n    Configuration conf = new Configuration();\r\n    conf.set(MRJobConfig.MR_AM_STAGING_DIR, stagingDir);\r\n    AsyncDispatcher dispatcher = new AsyncDispatcher();\r\n    dispatcher.init(conf);\r\n    dispatcher.start();\r\n    OutputCommitter committer = new StubbedOutputCommitter() {\r\n\r\n        @Override\r\n        public void setupJob(JobContext jobContext) throws IOException {\r\n            throw new IOException(\"forced failure\");\r\n        }\r\n\r\n        @Override\r\n        public synchronized void abortJob(JobContext jobContext, State state) throws IOException {\r\n            while (!Thread.interrupted()) {\r\n                try {\r\n                    wait();\r\n                } catch (InterruptedException e) {\r\n                }\r\n            }\r\n        }\r\n    };\r\n    CommitterEventHandler commitHandler = createCommitterEventHandler(dispatcher, committer);\r\n    commitHandler.init(conf);\r\n    commitHandler.start();\r\n    JobImpl job = createStubbedJob(conf, dispatcher, 2, null);\r\n    JobId jobId = job.getID();\r\n    job.handle(new JobEvent(jobId, JobEventType.JOB_INIT));\r\n    assertJobState(job, JobStateInternal.INITED);\r\n    job.handle(new JobStartEvent(jobId));\r\n    assertJobState(job, JobStateInternal.FAIL_ABORT);\r\n    job.handle(new JobEvent(jobId, JobEventType.JOB_KILL));\r\n    assertJobState(job, JobStateInternal.KILLED);\r\n    dispatcher.stop();\r\n    commitHandler.stop();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\job\\impl",
  "methodName" : "testKilledDuringKillAbort",
  "errType" : [ "InterruptedException" ],
  "containingMethodsNum" : 19,
  "sourceCodeText" : "void testKilledDuringKillAbort() throws Exception\n{\r\n    Configuration conf = new Configuration();\r\n    conf.set(MRJobConfig.MR_AM_STAGING_DIR, stagingDir);\r\n    AsyncDispatcher dispatcher = new AsyncDispatcher();\r\n    dispatcher.init(conf);\r\n    OutputCommitter committer = new StubbedOutputCommitter() {\r\n\r\n        @Override\r\n        public synchronized void abortJob(JobContext jobContext, State state) throws IOException {\r\n            while (!Thread.interrupted()) {\r\n                try {\r\n                    wait();\r\n                } catch (InterruptedException e) {\r\n                }\r\n            }\r\n        }\r\n    };\r\n    CommitterEventHandler commitHandler = createCommitterEventHandler(dispatcher, committer);\r\n    commitHandler.init(conf);\r\n    commitHandler.start();\r\n    JobImpl job = createStubbedJob(conf, dispatcher, 2, null);\r\n    JobId jobId = job.getID();\r\n    job.handle(new JobEvent(jobId, JobEventType.JOB_INIT));\r\n    assertJobState(job, JobStateInternal.INITED);\r\n    job.handle(new JobStartEvent(jobId));\r\n    assertJobState(job, JobStateInternal.SETUP);\r\n    job.handle(new JobEvent(jobId, JobEventType.JOB_KILL));\r\n    assertJobState(job, JobStateInternal.KILL_ABORT);\r\n    job.handle(new JobEvent(jobId, JobEventType.JOB_KILL));\r\n    assertJobState(job, JobStateInternal.KILLED);\r\n    dispatcher.stop();\r\n    commitHandler.stop();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\job\\impl",
  "methodName" : "testUnusableNodeTransition",
  "errType" : null,
  "containingMethodsNum" : 37,
  "sourceCodeText" : "void testUnusableNodeTransition() throws Exception\n{\r\n    Configuration conf = new Configuration();\r\n    conf.set(MRJobConfig.MR_AM_STAGING_DIR, stagingDir);\r\n    conf.setInt(MRJobConfig.NUM_REDUCES, 1);\r\n    DrainDispatcher dispatcher = new DrainDispatcher();\r\n    dispatcher.init(conf);\r\n    dispatcher.start();\r\n    CyclicBarrier syncBarrier = new CyclicBarrier(2);\r\n    OutputCommitter committer = new TestingOutputCommitter(syncBarrier, true);\r\n    CommitterEventHandler commitHandler = createCommitterEventHandler(dispatcher, committer);\r\n    commitHandler.init(conf);\r\n    commitHandler.start();\r\n    final JobImpl job = createRunningStubbedJob(conf, dispatcher, 2, null);\r\n    EventHandler<TaskAttemptEvent> taskAttemptEventHandler = new EventHandler<TaskAttemptEvent>() {\r\n\r\n        @Override\r\n        public void handle(TaskAttemptEvent event) {\r\n            if (event.getType() == TaskAttemptEventType.TA_KILL) {\r\n                job.decrementSucceededMapperCount();\r\n            }\r\n        }\r\n    };\r\n    dispatcher.register(TaskAttemptEventType.class, taskAttemptEventHandler);\r\n    Map<TaskId, Task> spiedTasks = new HashMap<>();\r\n    List<NodeReport> nodeReports = new ArrayList<>();\r\n    Map<NodeReport, TaskId> nodeReportsToTaskIds = new HashMap<>();\r\n    createSpiedMapTasks(nodeReportsToTaskIds, spiedTasks, job, NodeState.UNHEALTHY, nodeReports);\r\n    job.tasks.putAll(spiedTasks);\r\n    for (TaskId taskId : job.tasks.keySet()) {\r\n        if (taskId.getTaskType() == TaskType.MAP) {\r\n            TaskAttemptCompletionEvent tce = Records.newRecord(TaskAttemptCompletionEvent.class);\r\n            TaskAttemptId attemptId = MRBuilderUtils.newTaskAttemptId(taskId, 0);\r\n            tce.setAttemptId(attemptId);\r\n            tce.setStatus(TaskAttemptCompletionEventStatus.SUCCEEDED);\r\n            job.handle(new JobTaskAttemptCompletedEvent(tce));\r\n            job.handle(new JobTaskEvent(taskId, TaskState.SUCCEEDED));\r\n            Assert.assertEquals(JobState.RUNNING, job.getState());\r\n        }\r\n    }\r\n    NodeReport firstMapperNodeReport = nodeReports.get(0);\r\n    NodeReport secondMapperNodeReport = nodeReports.get(1);\r\n    job.handle(new JobUpdatedNodesEvent(job.getID(), Collections.singletonList(firstMapperNodeReport)));\r\n    dispatcher.await();\r\n    for (TaskId taskId : job.tasks.keySet()) {\r\n        if (taskId.getTaskType() == TaskType.REDUCE) {\r\n            job.handle(new JobTaskEvent(taskId, TaskState.SUCCEEDED));\r\n        }\r\n    }\r\n    job.handle(new JobUpdatedNodesEvent(job.getID(), Collections.singletonList(secondMapperNodeReport)));\r\n    TaskId firstMapper = nodeReportsToTaskIds.get(firstMapperNodeReport);\r\n    job.handle(new JobTaskEvent(firstMapper, TaskState.SUCCEEDED));\r\n    assertJobState(job, JobStateInternal.COMMITTING);\r\n    syncBarrier.await();\r\n    assertJobState(job, JobStateInternal.SUCCEEDED);\r\n    dispatcher.stop();\r\n    commitHandler.stop();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\job\\impl",
  "methodName" : "testJobNCompletedWhenAllReducersAreFinished",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testJobNCompletedWhenAllReducersAreFinished() throws Exception\n{\r\n    testJobCompletionWhenReducersAreFinished(true);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\job\\impl",
  "methodName" : "testJobNotCompletedWhenAllReducersAreFinished",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testJobNotCompletedWhenAllReducersAreFinished() throws Exception\n{\r\n    testJobCompletionWhenReducersAreFinished(false);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\job\\impl",
  "methodName" : "testJobCompletionWhenReducersAreFinished",
  "errType" : null,
  "containingMethodsNum" : 21,
  "sourceCodeText" : "void testJobCompletionWhenReducersAreFinished(boolean killMappers) throws InterruptedException, BrokenBarrierException\n{\r\n    Configuration conf = new Configuration();\r\n    conf.setBoolean(MRJobConfig.FINISH_JOB_WHEN_REDUCERS_DONE, killMappers);\r\n    conf.set(MRJobConfig.MR_AM_STAGING_DIR, stagingDir);\r\n    conf.setInt(MRJobConfig.NUM_REDUCES, 1);\r\n    DrainDispatcher dispatcher = new DrainDispatcher();\r\n    dispatcher.init(conf);\r\n    final List<TaskEvent> killedEvents = Collections.synchronizedList(new ArrayList<TaskEvent>());\r\n    dispatcher.register(TaskEventType.class, new EventHandler<TaskEvent>() {\r\n\r\n        @Override\r\n        public void handle(TaskEvent event) {\r\n            if (event.getType() == TaskEventType.T_KILL) {\r\n                killedEvents.add(event);\r\n            }\r\n        }\r\n    });\r\n    dispatcher.start();\r\n    CyclicBarrier syncBarrier = new CyclicBarrier(2);\r\n    OutputCommitter committer = new TestingOutputCommitter(syncBarrier, true);\r\n    CommitterEventHandler commitHandler = createCommitterEventHandler(dispatcher, committer);\r\n    commitHandler.init(conf);\r\n    commitHandler.start();\r\n    final JobImpl job = createRunningStubbedJob(conf, dispatcher, 2, null);\r\n    Map<TaskId, Task> spiedTasks = new HashMap<>();\r\n    List<NodeReport> nodeReports = new ArrayList<>();\r\n    Map<NodeReport, TaskId> nodeReportsToTaskIds = new HashMap<>();\r\n    createSpiedMapTasks(nodeReportsToTaskIds, spiedTasks, job, NodeState.RUNNING, nodeReports);\r\n    job.tasks.putAll(spiedTasks);\r\n    for (TaskId taskId : job.tasks.keySet()) {\r\n        if (taskId.getTaskType() == TaskType.REDUCE) {\r\n            job.handle(new JobTaskEvent(taskId, TaskState.SUCCEEDED));\r\n        }\r\n    }\r\n    dispatcher.await();\r\n    if (killMappers) {\r\n        Assert.assertEquals(\"Number of killed events\", 2, killedEvents.size());\r\n        Assert.assertEquals(\"AttemptID\", \"task_1234567890000_0001_m_000000\", killedEvents.get(0).getTaskID().toString());\r\n        Assert.assertEquals(\"AttemptID\", \"task_1234567890000_0001_m_000001\", killedEvents.get(1).getTaskID().toString());\r\n    } else {\r\n        Assert.assertEquals(\"Number of killed events\", 0, killedEvents.size());\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\job\\impl",
  "methodName" : "main",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void main(String[] args) throws Exception\n{\r\n    TestJobImpl t = new TestJobImpl();\r\n    t.testJobNoTasks();\r\n    t.testCheckJobCompleteSuccess();\r\n    t.testCheckAccess();\r\n    t.testReportDiagnostics();\r\n    t.testUberDecision();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\job\\impl",
  "methodName" : "testCheckAccess",
  "errType" : null,
  "containingMethodsNum" : 25,
  "sourceCodeText" : "void testCheckAccess()\n{\r\n    String user1 = System.getProperty(\"user.name\");\r\n    String user2 = user1 + \"1234\";\r\n    UserGroupInformation ugi1 = UserGroupInformation.createRemoteUser(user1);\r\n    UserGroupInformation ugi2 = UserGroupInformation.createRemoteUser(user2);\r\n    JobID jobID = JobID.forName(\"job_1234567890000_0001\");\r\n    JobId jobId = TypeConverter.toYarn(jobID);\r\n    Configuration conf1 = new Configuration();\r\n    conf1.setBoolean(MRConfig.MR_ACLS_ENABLED, true);\r\n    conf1.set(MRJobConfig.JOB_ACL_VIEW_JOB, \"\");\r\n    JobImpl job1 = new JobImpl(jobId, null, conf1, null, null, null, null, null, null, null, null, true, user1, 0, null, null, null, null);\r\n    Assert.assertTrue(job1.checkAccess(ugi1, JobACL.VIEW_JOB));\r\n    Assert.assertFalse(job1.checkAccess(ugi2, JobACL.VIEW_JOB));\r\n    Configuration conf2 = new Configuration();\r\n    conf2.setBoolean(MRConfig.MR_ACLS_ENABLED, true);\r\n    conf2.set(MRJobConfig.JOB_ACL_VIEW_JOB, user2);\r\n    JobImpl job2 = new JobImpl(jobId, null, conf2, null, null, null, null, null, null, null, null, true, user1, 0, null, null, null, null);\r\n    Assert.assertTrue(job2.checkAccess(ugi1, JobACL.VIEW_JOB));\r\n    Assert.assertTrue(job2.checkAccess(ugi2, JobACL.VIEW_JOB));\r\n    Configuration conf3 = new Configuration();\r\n    conf3.setBoolean(MRConfig.MR_ACLS_ENABLED, true);\r\n    conf3.set(MRJobConfig.JOB_ACL_VIEW_JOB, \"*\");\r\n    JobImpl job3 = new JobImpl(jobId, null, conf3, null, null, null, null, null, null, null, null, true, user1, 0, null, null, null, null);\r\n    Assert.assertTrue(job3.checkAccess(ugi1, JobACL.VIEW_JOB));\r\n    Assert.assertTrue(job3.checkAccess(ugi2, JobACL.VIEW_JOB));\r\n    Configuration conf4 = new Configuration();\r\n    conf4.setBoolean(MRConfig.MR_ACLS_ENABLED, false);\r\n    conf4.set(MRJobConfig.JOB_ACL_VIEW_JOB, \"\");\r\n    JobImpl job4 = new JobImpl(jobId, null, conf4, null, null, null, null, null, null, null, null, true, user1, 0, null, null, null, null);\r\n    Assert.assertTrue(job4.checkAccess(ugi1, JobACL.VIEW_JOB));\r\n    Assert.assertTrue(job4.checkAccess(ugi2, JobACL.VIEW_JOB));\r\n    Configuration conf5 = new Configuration();\r\n    conf5.setBoolean(MRConfig.MR_ACLS_ENABLED, true);\r\n    conf5.set(MRJobConfig.JOB_ACL_VIEW_JOB, \"\");\r\n    JobImpl job5 = new JobImpl(jobId, null, conf5, null, null, null, null, null, null, null, null, true, user1, 0, null, null, null, null);\r\n    Assert.assertTrue(job5.checkAccess(ugi1, null));\r\n    Assert.assertTrue(job5.checkAccess(ugi2, null));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\job\\impl",
  "methodName" : "testReportDiagnostics",
  "errType" : null,
  "containingMethodsNum" : 22,
  "sourceCodeText" : "void testReportDiagnostics() throws Exception\n{\r\n    JobID jobID = JobID.forName(\"job_1234567890000_0001\");\r\n    JobId jobId = TypeConverter.toYarn(jobID);\r\n    final String diagMsg = \"some diagnostic message\";\r\n    final JobDiagnosticsUpdateEvent diagUpdateEvent = new JobDiagnosticsUpdateEvent(jobId, diagMsg);\r\n    MRAppMetrics mrAppMetrics = MRAppMetrics.create();\r\n    AppContext mockContext = mock(AppContext.class);\r\n    when(mockContext.hasSuccessfullyUnregistered()).thenReturn(true);\r\n    JobImpl job = new JobImpl(jobId, Records.newRecord(ApplicationAttemptId.class), new Configuration(), mock(EventHandler.class), null, mock(JobTokenSecretManager.class), null, SystemClock.getInstance(), null, mrAppMetrics, null, true, null, 0, null, mockContext, null, null);\r\n    job.handle(diagUpdateEvent);\r\n    String diagnostics = job.getReport().getDiagnostics();\r\n    Assert.assertNotNull(diagnostics);\r\n    Assert.assertTrue(diagnostics.contains(diagMsg));\r\n    job = new JobImpl(jobId, Records.newRecord(ApplicationAttemptId.class), new Configuration(), mock(EventHandler.class), null, mock(JobTokenSecretManager.class), null, SystemClock.getInstance(), null, mrAppMetrics, null, true, null, 0, null, mockContext, null, null);\r\n    job.handle(new JobEvent(jobId, JobEventType.JOB_KILL));\r\n    job.handle(diagUpdateEvent);\r\n    diagnostics = job.getReport().getDiagnostics();\r\n    Assert.assertNotNull(diagnostics);\r\n    Assert.assertTrue(diagnostics.contains(diagMsg));\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\job\\impl",
  "methodName" : "testUberDecision",
  "errType" : null,
  "containingMethodsNum" : 25,
  "sourceCodeText" : "void testUberDecision() throws Exception\n{\r\n    Configuration conf = new Configuration();\r\n    boolean isUber = testUberDecision(conf);\r\n    Assert.assertFalse(isUber);\r\n    conf = new Configuration();\r\n    conf.setBoolean(MRJobConfig.JOB_UBERTASK_ENABLE, true);\r\n    isUber = testUberDecision(conf);\r\n    Assert.assertTrue(isUber);\r\n    conf = new Configuration();\r\n    conf.setBoolean(MRJobConfig.JOB_UBERTASK_ENABLE, true);\r\n    conf.setInt(MRJobConfig.JOB_UBERTASK_MAXREDUCES, 0);\r\n    conf.setInt(MRJobConfig.NUM_REDUCES, 1);\r\n    isUber = testUberDecision(conf);\r\n    Assert.assertFalse(isUber);\r\n    conf = new Configuration();\r\n    conf.setBoolean(MRJobConfig.JOB_UBERTASK_ENABLE, true);\r\n    conf.setInt(MRJobConfig.JOB_UBERTASK_MAXREDUCES, 1);\r\n    conf.setInt(MRJobConfig.NUM_REDUCES, 1);\r\n    isUber = testUberDecision(conf);\r\n    Assert.assertTrue(isUber);\r\n    conf = new Configuration();\r\n    conf.setBoolean(MRJobConfig.JOB_UBERTASK_ENABLE, true);\r\n    conf.setInt(MRJobConfig.JOB_UBERTASK_MAXMAPS, 1);\r\n    isUber = testUberDecision(conf);\r\n    Assert.assertFalse(isUber);\r\n    conf = new Configuration();\r\n    conf.setBoolean(MRJobConfig.JOB_UBERTASK_ENABLE, true);\r\n    conf.setInt(MRJobConfig.NUM_REDUCES, 0);\r\n    conf.setInt(MRJobConfig.REDUCE_MEMORY_MB, 2048);\r\n    conf.setInt(MRJobConfig.REDUCE_CPU_VCORES, 10);\r\n    isUber = testUberDecision(conf);\r\n    Assert.assertTrue(isUber);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\job\\impl",
  "methodName" : "testUberDecision",
  "errType" : null,
  "containingMethodsNum" : 9,
  "sourceCodeText" : "boolean testUberDecision(Configuration conf)\n{\r\n    JobID jobID = JobID.forName(\"job_1234567890000_0001\");\r\n    JobId jobId = TypeConverter.toYarn(jobID);\r\n    MRAppMetrics mrAppMetrics = MRAppMetrics.create();\r\n    JobImpl job = new JobImpl(jobId, ApplicationAttemptId.newInstance(ApplicationId.newInstance(0, 0), 0), conf, mock(EventHandler.class), null, new JobTokenSecretManager(), new Credentials(), null, null, mrAppMetrics, null, true, null, 0, null, null, null, null);\r\n    InitTransition initTransition = getInitTransition(2);\r\n    JobEvent mockJobEvent = mock(JobEvent.class);\r\n    initTransition.transition(job, mockJobEvent);\r\n    boolean isUber = job.isUber();\r\n    return isUber;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\job\\impl",
  "methodName" : "getInitTransition",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "InitTransition getInitTransition(final int numSplits)\n{\r\n    InitTransition initTransition = new InitTransition() {\r\n\r\n        @Override\r\n        protected TaskSplitMetaInfo[] createSplits(JobImpl job, JobId jobId) {\r\n            TaskSplitMetaInfo[] splits = new TaskSplitMetaInfo[numSplits];\r\n            for (int i = 0; i < numSplits; ++i) {\r\n                splits[i] = new TaskSplitMetaInfo();\r\n            }\r\n            return splits;\r\n        }\r\n    };\r\n    return initTransition;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\job\\impl",
  "methodName" : "testTransitionsAtFailed",
  "errType" : null,
  "containingMethodsNum" : 28,
  "sourceCodeText" : "void testTransitionsAtFailed() throws IOException\n{\r\n    Configuration conf = new Configuration();\r\n    AsyncDispatcher dispatcher = new AsyncDispatcher();\r\n    dispatcher.init(conf);\r\n    dispatcher.start();\r\n    OutputCommitter committer = mock(OutputCommitter.class);\r\n    doThrow(new IOException(\"forcefail\")).when(committer).setupJob(any(JobContext.class));\r\n    CommitterEventHandler commitHandler = createCommitterEventHandler(dispatcher, committer);\r\n    commitHandler.init(conf);\r\n    commitHandler.start();\r\n    AppContext mockContext = mock(AppContext.class);\r\n    when(mockContext.hasSuccessfullyUnregistered()).thenReturn(false);\r\n    JobImpl job = createStubbedJob(conf, dispatcher, 2, mockContext);\r\n    JobId jobId = job.getID();\r\n    job.handle(new JobEvent(jobId, JobEventType.JOB_INIT));\r\n    assertJobState(job, JobStateInternal.INITED);\r\n    job.handle(new JobStartEvent(jobId));\r\n    assertJobState(job, JobStateInternal.FAILED);\r\n    job.handle(new JobEvent(jobId, JobEventType.JOB_TASK_COMPLETED));\r\n    assertJobState(job, JobStateInternal.FAILED);\r\n    job.handle(new JobEvent(jobId, JobEventType.JOB_TASK_ATTEMPT_COMPLETED));\r\n    assertJobState(job, JobStateInternal.FAILED);\r\n    job.handle(new JobEvent(jobId, JobEventType.JOB_MAP_TASK_RESCHEDULED));\r\n    assertJobState(job, JobStateInternal.FAILED);\r\n    job.handle(new JobEvent(jobId, JobEventType.JOB_TASK_ATTEMPT_FETCH_FAILURE));\r\n    assertJobState(job, JobStateInternal.FAILED);\r\n    Assert.assertEquals(JobState.RUNNING, job.getState());\r\n    when(mockContext.hasSuccessfullyUnregistered()).thenReturn(true);\r\n    Assert.assertEquals(JobState.FAILED, job.getState());\r\n    dispatcher.stop();\r\n    commitHandler.stop();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\job\\impl",
  "methodName" : "testMetaInfoSizeOverMax",
  "errType" : null,
  "containingMethodsNum" : 10,
  "sourceCodeText" : "void testMetaInfoSizeOverMax() throws Exception\n{\r\n    Configuration conf = new Configuration();\r\n    JobID jobID = JobID.forName(\"job_1234567890000_0001\");\r\n    JobId jobId = TypeConverter.toYarn(jobID);\r\n    MRAppMetrics mrAppMetrics = MRAppMetrics.create();\r\n    JobImpl job = new JobImpl(jobId, ApplicationAttemptId.newInstance(ApplicationId.newInstance(0, 0), 0), conf, mock(EventHandler.class), null, new JobTokenSecretManager(), new Credentials(), null, null, mrAppMetrics, null, true, null, 0, null, null, null, null);\r\n    InitTransition initTransition = new InitTransition() {\r\n\r\n        @Override\r\n        protected TaskSplitMetaInfo[] createSplits(JobImpl job, JobId jobId) {\r\n            throw new YarnRuntimeException(EXCEPTIONMSG);\r\n        }\r\n    };\r\n    JobEvent mockJobEvent = mock(JobEvent.class);\r\n    JobStateInternal jobSI = initTransition.transition(job, mockJobEvent);\r\n    Assert.assertTrue(\"When init fails, return value from InitTransition.transition should equal NEW.\", jobSI.equals(JobStateInternal.NEW));\r\n    Assert.assertTrue(\"Job diagnostics should contain YarnRuntimeException\", job.getDiagnostics().toString().contains(\"YarnRuntimeException\"));\r\n    Assert.assertTrue(\"Job diagnostics should contain \" + EXCEPTIONMSG, job.getDiagnostics().toString().contains(EXCEPTIONMSG));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\job\\impl",
  "methodName" : "testJobPriorityUpdate",
  "errType" : null,
  "containingMethodsNum" : 20,
  "sourceCodeText" : "void testJobPriorityUpdate() throws Exception\n{\r\n    Configuration conf = new Configuration();\r\n    AsyncDispatcher dispatcher = new AsyncDispatcher();\r\n    dispatcher.init(conf);\r\n    Priority submittedPriority = Priority.newInstance(5);\r\n    AppContext mockContext = mock(AppContext.class);\r\n    when(mockContext.hasSuccessfullyUnregistered()).thenReturn(false);\r\n    JobImpl job = createStubbedJob(conf, dispatcher, 2, mockContext);\r\n    JobId jobId = job.getID();\r\n    job.handle(new JobEvent(jobId, JobEventType.JOB_INIT));\r\n    assertJobState(job, JobStateInternal.INITED);\r\n    job.handle(new JobStartEvent(jobId));\r\n    assertJobState(job, JobStateInternal.SETUP);\r\n    job.setJobPriority(submittedPriority);\r\n    Assert.assertEquals(submittedPriority, job.getReport().getJobPriority());\r\n    job.handle(new JobSetupCompletedEvent(jobId));\r\n    assertJobState(job, JobStateInternal.RUNNING);\r\n    Priority updatedPriority = Priority.newInstance(8);\r\n    job.setJobPriority(updatedPriority);\r\n    assertJobState(job, JobStateInternal.RUNNING);\r\n    Priority jobPriority = job.getReport().getJobPriority();\r\n    Assert.assertNotNull(jobPriority);\r\n    Assert.assertEquals(updatedPriority, jobPriority);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\job\\impl",
  "methodName" : "testCleanupSharedCacheUploadPolicies",
  "errType" : null,
  "containingMethodsNum" : 11,
  "sourceCodeText" : "void testCleanupSharedCacheUploadPolicies()\n{\r\n    Configuration config = new Configuration();\r\n    Map<String, Boolean> archivePolicies = new HashMap<>();\r\n    archivePolicies.put(\"archive1\", true);\r\n    archivePolicies.put(\"archive2\", true);\r\n    Job.setArchiveSharedCacheUploadPolicies(config, archivePolicies);\r\n    Map<String, Boolean> filePolicies = new HashMap<>();\r\n    filePolicies.put(\"file1\", true);\r\n    filePolicies.put(\"jar1\", true);\r\n    Job.setFileSharedCacheUploadPolicies(config, filePolicies);\r\n    Assert.assertEquals(2, Job.getArchiveSharedCacheUploadPolicies(config).size());\r\n    Assert.assertEquals(2, Job.getFileSharedCacheUploadPolicies(config).size());\r\n    JobImpl.cleanupSharedCacheUploadPolicies(config);\r\n    Assert.assertEquals(0, Job.getArchiveSharedCacheUploadPolicies(config).size());\r\n    Assert.assertEquals(0, Job.getFileSharedCacheUploadPolicies(config).size());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\job\\impl",
  "methodName" : "createCommitterEventHandler",
  "errType" : null,
  "containingMethodsNum" : 10,
  "sourceCodeText" : "CommitterEventHandler createCommitterEventHandler(Dispatcher dispatcher, OutputCommitter committer)\n{\r\n    final SystemClock clock = SystemClock.getInstance();\r\n    AppContext appContext = mock(AppContext.class);\r\n    when(appContext.getEventHandler()).thenReturn(dispatcher.getEventHandler());\r\n    when(appContext.getClock()).thenReturn(clock);\r\n    RMHeartbeatHandler heartbeatHandler = new RMHeartbeatHandler() {\r\n\r\n        @Override\r\n        public long getLastHeartbeatTime() {\r\n            return clock.getTime();\r\n        }\r\n\r\n        @Override\r\n        public void runOnNextHeartbeat(Runnable callback) {\r\n            callback.run();\r\n        }\r\n    };\r\n    ApplicationAttemptId id = ApplicationAttemptId.fromString(\"appattempt_1234567890000_0001_0\");\r\n    when(appContext.getApplicationID()).thenReturn(id.getApplicationId());\r\n    when(appContext.getApplicationAttemptId()).thenReturn(id);\r\n    CommitterEventHandler handler = new CommitterEventHandler(appContext, committer, heartbeatHandler);\r\n    dispatcher.register(CommitterEventType.class, handler);\r\n    return handler;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\job\\impl",
  "methodName" : "createStubbedJob",
  "errType" : null,
  "containingMethodsNum" : 11,
  "sourceCodeText" : "StubbedJob createStubbedJob(Configuration conf, Dispatcher dispatcher, int numSplits, AppContext appContext)\n{\r\n    JobID jobID = JobID.forName(\"job_1234567890000_0001\");\r\n    JobId jobId = TypeConverter.toYarn(jobID);\r\n    if (appContext == null) {\r\n        appContext = mock(AppContext.class);\r\n        when(appContext.hasSuccessfullyUnregistered()).thenReturn(true);\r\n    }\r\n    StubbedJob job = new StubbedJob(jobId, ApplicationAttemptId.newInstance(ApplicationId.newInstance(0, 0), 0), conf, dispatcher.getEventHandler(), true, \"somebody\", numSplits, appContext);\r\n    dispatcher.register(JobEventType.class, job);\r\n    EventHandler mockHandler = mock(EventHandler.class);\r\n    dispatcher.register(TaskEventType.class, mockHandler);\r\n    dispatcher.register(org.apache.hadoop.mapreduce.jobhistory.EventType.class, mockHandler);\r\n    dispatcher.register(JobFinishEvent.Type.class, mockHandler);\r\n    return job;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\job\\impl",
  "methodName" : "createRunningStubbedJob",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "StubbedJob createRunningStubbedJob(Configuration conf, Dispatcher dispatcher, int numSplits, AppContext appContext)\n{\r\n    StubbedJob job = createStubbedJob(conf, dispatcher, numSplits, appContext);\r\n    job.handle(new JobEvent(job.getID(), JobEventType.JOB_INIT));\r\n    assertJobState(job, JobStateInternal.INITED);\r\n    job.handle(new JobStartEvent(job.getID()));\r\n    assertJobState(job, JobStateInternal.RUNNING);\r\n    return job;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\job\\impl",
  "methodName" : "completeJobTasks",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void completeJobTasks(JobImpl job)\n{\r\n    int numMaps = job.getTotalMaps();\r\n    for (int i = 0; i < numMaps; ++i) {\r\n        job.handle(new JobTaskEvent(MRBuilderUtils.newTaskId(job.getID(), 1, TaskType.MAP), TaskState.SUCCEEDED));\r\n        Assert.assertEquals(JobState.RUNNING, job.getState());\r\n    }\r\n    int numReduces = job.getTotalReduces();\r\n    for (int i = 0; i < numReduces; ++i) {\r\n        job.handle(new JobTaskEvent(MRBuilderUtils.newTaskId(job.getID(), 1, TaskType.MAP), TaskState.SUCCEEDED));\r\n        Assert.assertEquals(JobState.RUNNING, job.getState());\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\job\\impl",
  "methodName" : "assertJobState",
  "errType" : [ "InterruptedException" ],
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void assertJobState(JobImpl job, JobStateInternal state)\n{\r\n    int timeToWaitMsec = 5 * 1000;\r\n    while (timeToWaitMsec > 0 && job.getInternalState() != state) {\r\n        try {\r\n            Thread.sleep(10);\r\n            timeToWaitMsec -= 10;\r\n        } catch (InterruptedException e) {\r\n            break;\r\n        }\r\n    }\r\n    Assert.assertEquals(state, job.getInternalState());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\job\\impl",
  "methodName" : "createSpiedMapTasks",
  "errType" : null,
  "containingMethodsNum" : 19,
  "sourceCodeText" : "void createSpiedMapTasks(Map<NodeReport, TaskId> nodeReportsToTaskIds, Map<TaskId, Task> spiedTasks, JobImpl job, NodeState nodeState, List<NodeReport> nodeReports)\n{\r\n    for (Map.Entry<TaskId, Task> e : job.tasks.entrySet()) {\r\n        TaskId taskId = e.getKey();\r\n        Task task = e.getValue();\r\n        if (taskId.getTaskType() == TaskType.MAP) {\r\n            NodeId nodeId = mock(NodeId.class);\r\n            TaskAttempt attempt = mock(TaskAttempt.class);\r\n            when(attempt.getNodeId()).thenReturn(nodeId);\r\n            TaskAttemptId attemptId = MRBuilderUtils.newTaskAttemptId(taskId, 0);\r\n            when(attempt.getID()).thenReturn(attemptId);\r\n            Task spied = spy(task);\r\n            Map<TaskAttemptId, TaskAttempt> attemptMap = new HashMap<>();\r\n            attemptMap.put(attemptId, attempt);\r\n            when(spied.getAttempts()).thenReturn(attemptMap);\r\n            doReturn(attempt).when(spied).getAttempt(any(TaskAttemptId.class));\r\n            spiedTasks.put(taskId, spied);\r\n            NodeReport report = mock(NodeReport.class);\r\n            when(report.getNodeState()).thenReturn(nodeState);\r\n            when(report.getNodeId()).thenReturn(nodeId);\r\n            nodeReports.add(report);\r\n            nodeReportsToTaskIds.put(report, taskId);\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app",
  "methodName" : "getApplicationAttemptId",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "ApplicationAttemptId getApplicationAttemptId()\n{\r\n    return appAttemptID;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app",
  "methodName" : "getApplicationID",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "ApplicationId getApplicationID()\n{\r\n    return appID;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app",
  "methodName" : "getUser",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "CharSequence getUser()\n{\r\n    return user;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app",
  "methodName" : "getJob",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Job getJob(JobId jobID)\n{\r\n    return jobs.get(jobID);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app",
  "methodName" : "getAllJobs",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Map<JobId, Job> getAllJobs()\n{\r\n    return jobs;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app",
  "methodName" : "getEventHandler",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "EventHandler<Event> getEventHandler()\n{\r\n    return new MockEventHandler();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app",
  "methodName" : "getClock",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Clock getClock()\n{\r\n    return null;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app",
  "methodName" : "getApplicationName",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getApplicationName()\n{\r\n    return \"TestApp\";\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app",
  "methodName" : "getStartTime",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "long getStartTime()\n{\r\n    return startTime;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app",
  "methodName" : "getClusterInfo",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "ClusterInfo getClusterInfo()\n{\r\n    return null;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app",
  "methodName" : "getBlacklistedNodes",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Set<String> getBlacklistedNodes()\n{\r\n    return blacklistedNodes;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app",
  "methodName" : "setBlacklistedNodes",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setBlacklistedNodes(Set<String> blacklistedNodes)\n{\r\n    this.blacklistedNodes = blacklistedNodes;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app",
  "methodName" : "getClientToAMTokenSecretManager",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "ClientToAMTokenSecretManager getClientToAMTokenSecretManager()\n{\r\n    return null;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app",
  "methodName" : "isLastAMRetry",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean isLastAMRetry()\n{\r\n    return false;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app",
  "methodName" : "hasSuccessfullyUnregistered",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "boolean hasSuccessfullyUnregistered()\n{\r\n    return true;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app",
  "methodName" : "getNMHostname",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getNMHostname()\n{\r\n    return null;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app",
  "methodName" : "getTaskAttemptFinishingMonitor",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "TaskAttemptFinishingMonitor getTaskAttemptFinishingMonitor()\n{\r\n    return null;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app",
  "methodName" : "getHistoryUrl",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getHistoryUrl()\n{\r\n    return null;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app",
  "methodName" : "setHistoryUrl",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setHistoryUrl(String historyUrl)\n{\r\n    return;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\api\\records",
  "methodName" : "testSetRawCounters",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void testSetRawCounters()\n{\r\n    TaskReport report = Records.newRecord(TaskReport.class);\r\n    org.apache.hadoop.mapreduce.Counters rCounters = MockJobs.newCounters();\r\n    report.setRawCounters(rCounters);\r\n    Counters counters = report.getCounters();\r\n    assertNotEquals(null, counters);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\api\\records",
  "methodName" : "testBuildImplicitRawCounters",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void testBuildImplicitRawCounters()\n{\r\n    TaskReportPBImpl report = new TaskReportPBImpl();\r\n    org.apache.hadoop.mapreduce.Counters rCounters = MockJobs.newCounters();\r\n    report.setRawCounters(rCounters);\r\n    MRProtos.TaskReportProto protoVal = report.getProto();\r\n    assertTrue(protoVal.hasCounters());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\api\\records",
  "methodName" : "testCountersOverRawCounters",
  "errType" : null,
  "containingMethodsNum" : 9,
  "sourceCodeText" : "void testCountersOverRawCounters()\n{\r\n    TaskReport report = Records.newRecord(TaskReport.class);\r\n    org.apache.hadoop.mapreduce.Counters rCounters = MockJobs.newCounters();\r\n    Counters altCounters = TypeConverter.toYarn(rCounters);\r\n    report.setRawCounters(rCounters);\r\n    report.setCounters(altCounters);\r\n    Counters counters = report.getCounters();\r\n    assertThat(counters).isNotNull();\r\n    assertNotEquals(rCounters, altCounters);\r\n    assertEquals(counters, altCounters);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\api\\records",
  "methodName" : "testUninitializedCounters",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void testUninitializedCounters()\n{\r\n    TaskReport report = Records.newRecord(TaskReport.class);\r\n    assertThat(report.getCounters()).isNull();\r\n    assertThat(report.getRawCounters()).isNull();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\api\\records",
  "methodName" : "testSetRawCountersToNull",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void testSetRawCountersToNull()\n{\r\n    TaskReport report = Records.newRecord(TaskReport.class);\r\n    report.setRawCounters(null);\r\n    assertThat(report.getCounters()).isNull();\r\n    assertThat(report.getRawCounters()).isNull();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\api\\records",
  "methodName" : "testSetCountersToNull",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void testSetCountersToNull()\n{\r\n    TaskReport report = Records.newRecord(TaskReport.class);\r\n    report.setCounters(null);\r\n    assertThat(report.getCounters()).isNull();\r\n    assertThat(report.getRawCounters()).isNull();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\api\\records",
  "methodName" : "testSetNonNullCountersToNull",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void testSetNonNullCountersToNull()\n{\r\n    TaskReport report = Records.newRecord(TaskReport.class);\r\n    org.apache.hadoop.mapreduce.Counters rCounters = MockJobs.newCounters();\r\n    report.setRawCounters(rCounters);\r\n    Counters counters = report.getCounters();\r\n    assertNotEquals(null, counters);\r\n    report.setCounters(null);\r\n    assertThat(report.getCounters()).isNull();\r\n    assertThat(report.getRawCounters()).isNull();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\api\\records",
  "methodName" : "testSetNonNullRawCountersToNull",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void testSetNonNullRawCountersToNull()\n{\r\n    TaskReport report = Records.newRecord(TaskReport.class);\r\n    org.apache.hadoop.mapreduce.Counters rCounters = MockJobs.newCounters();\r\n    report.setRawCounters(rCounters);\r\n    Counters counters = report.getCounters();\r\n    assertNotEquals(null, counters);\r\n    report.setRawCounters(null);\r\n    assertThat(report.getCounters()).isNull();\r\n    assertThat(report.getRawCounters()).isNull();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app",
  "methodName" : "testKillJob",
  "errType" : null,
  "containingMethodsNum" : 14,
  "sourceCodeText" : "void testKillJob() throws Exception\n{\r\n    final CountDownLatch latch = new CountDownLatch(1);\r\n    MRApp app = new BlockingMRApp(1, 0, latch);\r\n    Job job = app.submit(new Configuration());\r\n    app.waitForInternalState((JobImpl) job, JobStateInternal.RUNNING);\r\n    app.getContext().getEventHandler().handle(new JobEvent(job.getID(), JobEventType.JOB_KILL));\r\n    latch.countDown();\r\n    app.waitForState(job, JobState.KILLED);\r\n    app.waitForState(Service.STATE.STOPPED);\r\n    Map<TaskId, Task> tasks = job.getTasks();\r\n    Assert.assertEquals(\"No of tasks is not correct\", 1, tasks.size());\r\n    Task task = tasks.values().iterator().next();\r\n    Assert.assertEquals(\"Task state not correct\", TaskState.KILLED, task.getReport().getTaskState());\r\n    Map<TaskAttemptId, TaskAttempt> attempts = tasks.values().iterator().next().getAttempts();\r\n    Assert.assertEquals(\"No of attempts is not correct\", 1, attempts.size());\r\n    Iterator<TaskAttempt> it = attempts.values().iterator();\r\n    Assert.assertEquals(\"Attempt state not correct\", TaskAttemptState.KILLED, it.next().getReport().getTaskAttemptState());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app",
  "methodName" : "testKillTask",
  "errType" : null,
  "containingMethodsNum" : 20,
  "sourceCodeText" : "void testKillTask() throws Exception\n{\r\n    final CountDownLatch latch = new CountDownLatch(1);\r\n    MRApp app = new BlockingMRApp(2, 0, latch);\r\n    Job job = app.submit(new Configuration());\r\n    app.waitForInternalState((JobImpl) job, JobStateInternal.RUNNING);\r\n    Map<TaskId, Task> tasks = job.getTasks();\r\n    Assert.assertEquals(\"No of tasks is not correct\", 2, tasks.size());\r\n    Iterator<Task> it = tasks.values().iterator();\r\n    Task task1 = it.next();\r\n    Task task2 = it.next();\r\n    app.getContext().getEventHandler().handle(new TaskEvent(task1.getID(), TaskEventType.T_KILL));\r\n    latch.countDown();\r\n    app.waitForState(job, JobState.SUCCEEDED);\r\n    Assert.assertEquals(\"Task state not correct\", TaskState.KILLED, task1.getReport().getTaskState());\r\n    Assert.assertEquals(\"Task state not correct\", TaskState.SUCCEEDED, task2.getReport().getTaskState());\r\n    Map<TaskAttemptId, TaskAttempt> attempts = task1.getAttempts();\r\n    Assert.assertEquals(\"No of attempts is not correct\", 1, attempts.size());\r\n    Iterator<TaskAttempt> iter = attempts.values().iterator();\r\n    Assert.assertEquals(\"Attempt state not correct\", TaskAttemptState.KILLED, iter.next().getReport().getTaskAttemptState());\r\n    attempts = task2.getAttempts();\r\n    Assert.assertEquals(\"No of attempts is not correct\", 1, attempts.size());\r\n    iter = attempts.values().iterator();\r\n    Assert.assertEquals(\"Attempt state not correct\", TaskAttemptState.SUCCEEDED, iter.next().getReport().getTaskAttemptState());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app",
  "methodName" : "testKillTaskWait",
  "errType" : null,
  "containingMethodsNum" : 29,
  "sourceCodeText" : "void testKillTaskWait() throws Exception\n{\r\n    final Dispatcher dispatcher = new AsyncDispatcher() {\r\n\r\n        private TaskAttemptEvent cachedKillEvent;\r\n\r\n        @Override\r\n        protected void dispatch(Event event) {\r\n            if (event instanceof TaskAttemptEvent) {\r\n                TaskAttemptEvent killEvent = (TaskAttemptEvent) event;\r\n                if (killEvent.getType() == TaskAttemptEventType.TA_KILL) {\r\n                    TaskAttemptId taID = killEvent.getTaskAttemptID();\r\n                    if (taID.getTaskId().getTaskType() == TaskType.REDUCE && taID.getTaskId().getId() == 0 && taID.getId() == 0) {\r\n                        super.dispatch(new TaskAttemptEvent(taID, TaskAttemptEventType.TA_DONE));\r\n                        super.dispatch(new TaskAttemptEvent(taID, TaskAttemptEventType.TA_CONTAINER_COMPLETED));\r\n                        super.dispatch(new TaskTAttemptEvent(taID, TaskEventType.T_ATTEMPT_SUCCEEDED));\r\n                        this.cachedKillEvent = killEvent;\r\n                        return;\r\n                    }\r\n                }\r\n            } else if (event instanceof TaskEvent) {\r\n                TaskEvent taskEvent = (TaskEvent) event;\r\n                if (taskEvent.getType() == TaskEventType.T_ATTEMPT_SUCCEEDED && this.cachedKillEvent != null) {\r\n                    super.dispatch(this.cachedKillEvent);\r\n                    return;\r\n                }\r\n            }\r\n            super.dispatch(event);\r\n        }\r\n    };\r\n    MRApp app = new MRApp(1, 1, false, this.getClass().getName(), true) {\r\n\r\n        @Override\r\n        public Dispatcher createDispatcher() {\r\n            return dispatcher;\r\n        }\r\n    };\r\n    Job job = app.submit(new Configuration());\r\n    JobId jobId = app.getJobId();\r\n    app.waitForState(job, JobState.RUNNING);\r\n    Assert.assertEquals(\"Num tasks not correct\", 2, job.getTasks().size());\r\n    Iterator<Task> it = job.getTasks().values().iterator();\r\n    Task mapTask = it.next();\r\n    Task reduceTask = it.next();\r\n    app.waitForState(mapTask, TaskState.RUNNING);\r\n    app.waitForState(reduceTask, TaskState.RUNNING);\r\n    TaskAttempt mapAttempt = mapTask.getAttempts().values().iterator().next();\r\n    app.waitForState(mapAttempt, TaskAttemptState.RUNNING);\r\n    TaskAttempt reduceAttempt = reduceTask.getAttempts().values().iterator().next();\r\n    app.waitForState(reduceAttempt, TaskAttemptState.RUNNING);\r\n    app.getContext().getEventHandler().handle(new TaskAttemptEvent(mapAttempt.getID(), TaskAttemptEventType.TA_DONE));\r\n    app.waitForState(mapTask, TaskState.SUCCEEDED);\r\n    app.getContext().getEventHandler().handle(new JobEvent(jobId, JobEventType.JOB_KILL));\r\n    app.waitForInternalState((JobImpl) job, JobStateInternal.KILLED);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app",
  "methodName" : "testKillTaskWaitKillJobAfterTA_DONE",
  "errType" : null,
  "containingMethodsNum" : 18,
  "sourceCodeText" : "void testKillTaskWaitKillJobAfterTA_DONE() throws Exception\n{\r\n    CountDownLatch latch = new CountDownLatch(1);\r\n    final Dispatcher dispatcher = new MyAsyncDispatch(latch, TaskAttemptEventType.TA_DONE);\r\n    MRApp app = new MRApp(1, 1, false, this.getClass().getName(), true) {\r\n\r\n        @Override\r\n        public Dispatcher createDispatcher() {\r\n            return dispatcher;\r\n        }\r\n    };\r\n    Job job = app.submit(new Configuration());\r\n    JobId jobId = app.getJobId();\r\n    app.waitForState(job, JobState.RUNNING);\r\n    Assert.assertEquals(\"Num tasks not correct\", 2, job.getTasks().size());\r\n    Iterator<Task> it = job.getTasks().values().iterator();\r\n    Task mapTask = it.next();\r\n    Task reduceTask = it.next();\r\n    app.waitForState(mapTask, TaskState.RUNNING);\r\n    app.waitForState(reduceTask, TaskState.RUNNING);\r\n    TaskAttempt mapAttempt = mapTask.getAttempts().values().iterator().next();\r\n    app.waitForState(mapAttempt, TaskAttemptState.RUNNING);\r\n    TaskAttempt reduceAttempt = reduceTask.getAttempts().values().iterator().next();\r\n    app.waitForState(reduceAttempt, TaskAttemptState.RUNNING);\r\n    app.getContext().getEventHandler().handle(new TaskAttemptEvent(mapAttempt.getID(), TaskAttemptEventType.TA_DONE));\r\n    app.getContext().getEventHandler().handle(new JobEvent(jobId, JobEventType.JOB_KILL));\r\n    latch.countDown();\r\n    app.waitForInternalState((JobImpl) job, JobStateInternal.KILLED);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app",
  "methodName" : "testKillTaskWaitKillJobBeforeTA_DONE",
  "errType" : null,
  "containingMethodsNum" : 18,
  "sourceCodeText" : "void testKillTaskWaitKillJobBeforeTA_DONE() throws Exception\n{\r\n    CountDownLatch latch = new CountDownLatch(1);\r\n    final Dispatcher dispatcher = new MyAsyncDispatch(latch, JobEventType.JOB_KILL);\r\n    MRApp app = new MRApp(1, 1, false, this.getClass().getName(), true) {\r\n\r\n        @Override\r\n        public Dispatcher createDispatcher() {\r\n            return dispatcher;\r\n        }\r\n    };\r\n    Job job = app.submit(new Configuration());\r\n    JobId jobId = app.getJobId();\r\n    app.waitForState(job, JobState.RUNNING);\r\n    Assert.assertEquals(\"Num tasks not correct\", 2, job.getTasks().size());\r\n    Iterator<Task> it = job.getTasks().values().iterator();\r\n    Task mapTask = it.next();\r\n    Task reduceTask = it.next();\r\n    app.waitForState(mapTask, TaskState.RUNNING);\r\n    app.waitForState(reduceTask, TaskState.RUNNING);\r\n    TaskAttempt mapAttempt = mapTask.getAttempts().values().iterator().next();\r\n    app.waitForState(mapAttempt, TaskAttemptState.RUNNING);\r\n    TaskAttempt reduceAttempt = reduceTask.getAttempts().values().iterator().next();\r\n    app.waitForState(reduceAttempt, TaskAttemptState.RUNNING);\r\n    app.getContext().getEventHandler().handle(new JobEvent(jobId, JobEventType.JOB_KILL));\r\n    app.getContext().getEventHandler().handle(new TaskAttemptEvent(mapAttempt.getID(), TaskAttemptEventType.TA_DONE));\r\n    latch.countDown();\r\n    app.waitForInternalState((JobImpl) job, JobStateInternal.KILLED);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app",
  "methodName" : "testKillTaskAttempt",
  "errType" : null,
  "containingMethodsNum" : 24,
  "sourceCodeText" : "void testKillTaskAttempt() throws Exception\n{\r\n    final CountDownLatch latch = new CountDownLatch(1);\r\n    MRApp app = new BlockingMRApp(2, 0, latch);\r\n    Job job = app.submit(new Configuration());\r\n    app.waitForState(job, JobState.RUNNING);\r\n    Map<TaskId, Task> tasks = job.getTasks();\r\n    Assert.assertEquals(\"No of tasks is not correct\", 2, tasks.size());\r\n    Iterator<Task> it = tasks.values().iterator();\r\n    Task task1 = it.next();\r\n    Task task2 = it.next();\r\n    app.waitForState(task1, TaskState.SCHEDULED);\r\n    app.waitForState(task2, TaskState.SCHEDULED);\r\n    TaskAttempt attempt = task1.getAttempts().values().iterator().next();\r\n    app.getContext().getEventHandler().handle(new TaskAttemptEvent(attempt.getID(), TaskAttemptEventType.TA_KILL));\r\n    latch.countDown();\r\n    app.waitForState(job, JobState.SUCCEEDED);\r\n    Assert.assertEquals(\"Task state not correct\", TaskState.SUCCEEDED, task1.getReport().getTaskState());\r\n    Assert.assertEquals(\"Task state not correct\", TaskState.SUCCEEDED, task2.getReport().getTaskState());\r\n    Map<TaskAttemptId, TaskAttempt> attempts = task1.getAttempts();\r\n    Assert.assertEquals(\"No of attempts is not correct\", 2, attempts.size());\r\n    Iterator<TaskAttempt> iter = attempts.values().iterator();\r\n    Assert.assertEquals(\"Attempt state not correct\", TaskAttemptState.KILLED, iter.next().getReport().getTaskAttemptState());\r\n    Assert.assertEquals(\"Attempt state not correct\", TaskAttemptState.SUCCEEDED, iter.next().getReport().getTaskAttemptState());\r\n    attempts = task2.getAttempts();\r\n    Assert.assertEquals(\"No of attempts is not correct\", 1, attempts.size());\r\n    iter = attempts.values().iterator();\r\n    Assert.assertEquals(\"Attempt state not correct\", TaskAttemptState.SUCCEEDED, iter.next().getReport().getTaskAttemptState());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app",
  "methodName" : "main",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void main(String[] args) throws Exception\n{\r\n    TestKill t = new TestKill();\r\n    t.testKillJob();\r\n    t.testKillTask();\r\n    t.testKillTaskAttempt();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\speculate",
  "methodName" : "testEmptyDataStatistics",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void testEmptyDataStatistics() throws Exception\n{\r\n    DataStatistics statistics = new DataStatistics();\r\n    Assert.assertEquals(0, statistics.count(), TOL);\r\n    Assert.assertEquals(0, statistics.mean(), TOL);\r\n    Assert.assertEquals(0, statistics.var(), TOL);\r\n    Assert.assertEquals(0, statistics.std(), TOL);\r\n    Assert.assertEquals(0, statistics.outlier(1.0f), TOL);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\speculate",
  "methodName" : "testSingleEntryDataStatistics",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void testSingleEntryDataStatistics() throws Exception\n{\r\n    DataStatistics statistics = new DataStatistics(17.29);\r\n    Assert.assertEquals(1, statistics.count(), TOL);\r\n    Assert.assertEquals(17.29, statistics.mean(), TOL);\r\n    Assert.assertEquals(0, statistics.var(), TOL);\r\n    Assert.assertEquals(0, statistics.std(), TOL);\r\n    Assert.assertEquals(17.29, statistics.outlier(1.0f), TOL);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\speculate",
  "methodName" : "testMutiEntryDataStatistics",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void testMutiEntryDataStatistics() throws Exception\n{\r\n    DataStatistics statistics = new DataStatistics();\r\n    statistics.add(17);\r\n    statistics.add(29);\r\n    Assert.assertEquals(2, statistics.count(), TOL);\r\n    Assert.assertEquals(23.0, statistics.mean(), TOL);\r\n    Assert.assertEquals(36.0, statistics.var(), TOL);\r\n    Assert.assertEquals(6.0, statistics.std(), TOL);\r\n    Assert.assertEquals(29.0, statistics.outlier(1.0f), TOL);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\speculate",
  "methodName" : "testUpdateStatistics",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void testUpdateStatistics() throws Exception\n{\r\n    DataStatistics statistics = new DataStatistics(17);\r\n    statistics.add(29);\r\n    Assert.assertEquals(2, statistics.count(), TOL);\r\n    Assert.assertEquals(23.0, statistics.mean(), TOL);\r\n    Assert.assertEquals(36.0, statistics.var(), TOL);\r\n    statistics.updateStatistics(17, 29);\r\n    Assert.assertEquals(2, statistics.count(), TOL);\r\n    Assert.assertEquals(29.0, statistics.mean(), TOL);\r\n    Assert.assertEquals(0.0, statistics.var(), TOL);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\webapp",
  "methodName" : "setUp",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void setUp() throws Exception\n{\r\n    super.setUp();\r\n    GuiceServletConfig.setInjector(Guice.createInjector(new WebServletModule()));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\webapp",
  "methodName" : "testGetTaskAttemptIdState",
  "errType" : null,
  "containingMethodsNum" : 14,
  "sourceCodeText" : "void testGetTaskAttemptIdState() throws Exception\n{\r\n    WebResource r = resource();\r\n    Map<JobId, Job> jobsMap = appContext.getAllJobs();\r\n    for (JobId id : jobsMap.keySet()) {\r\n        String jobId = MRApps.toString(id);\r\n        for (Task task : jobsMap.get(id).getTasks().values()) {\r\n            String tid = MRApps.toString(task.getID());\r\n            for (TaskAttempt att : task.getAttempts().values()) {\r\n                TaskAttemptId attemptid = att.getID();\r\n                String attid = MRApps.toString(attemptid);\r\n                ClientResponse response = r.path(\"ws\").path(\"v1\").path(\"mapreduce\").path(\"jobs\").path(jobId).path(\"tasks\").path(tid).path(\"attempts\").path(attid).path(\"state\").queryParam(\"user.name\", webserviceUserName).accept(MediaType.APPLICATION_JSON).get(ClientResponse.class);\r\n                assertEquals(MediaType.APPLICATION_JSON_TYPE + \"; \" + JettyUtils.UTF_8, response.getType().toString());\r\n                JSONObject json = response.getEntity(JSONObject.class);\r\n                assertEquals(\"incorrect number of elements\", 1, json.length());\r\n                assertEquals(att.getState().toString(), json.get(\"state\"));\r\n            }\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\webapp",
  "methodName" : "testGetTaskAttemptIdXMLState",
  "errType" : null,
  "containingMethodsNum" : 20,
  "sourceCodeText" : "void testGetTaskAttemptIdXMLState() throws Exception\n{\r\n    WebResource r = resource();\r\n    Map<JobId, Job> jobsMap = appContext.getAllJobs();\r\n    for (JobId id : jobsMap.keySet()) {\r\n        String jobId = MRApps.toString(id);\r\n        for (Task task : jobsMap.get(id).getTasks().values()) {\r\n            String tid = MRApps.toString(task.getID());\r\n            for (TaskAttempt att : task.getAttempts().values()) {\r\n                TaskAttemptId attemptid = att.getID();\r\n                String attid = MRApps.toString(attemptid);\r\n                ClientResponse response = r.path(\"ws\").path(\"v1\").path(\"mapreduce\").path(\"jobs\").path(jobId).path(\"tasks\").path(tid).path(\"attempts\").path(attid).path(\"state\").queryParam(\"user.name\", webserviceUserName).accept(MediaType.APPLICATION_XML).get(ClientResponse.class);\r\n                assertEquals(MediaType.APPLICATION_XML_TYPE + \"; \" + JettyUtils.UTF_8, response.getType().toString());\r\n                String xml = response.getEntity(String.class);\r\n                DocumentBuilderFactory dbf = DocumentBuilderFactory.newInstance();\r\n                DocumentBuilder db = dbf.newDocumentBuilder();\r\n                InputSource is = new InputSource();\r\n                is.setCharacterStream(new StringReader(xml));\r\n                Document dom = db.parse(is);\r\n                NodeList nodes = dom.getElementsByTagName(\"jobTaskAttemptState\");\r\n                assertEquals(1, nodes.getLength());\r\n                String state = WebServicesTestUtils.getXmlString((Element) nodes.item(0), \"state\");\r\n                assertEquals(att.getState().toString(), state);\r\n            }\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\webapp",
  "methodName" : "testPutTaskAttemptIdState",
  "errType" : null,
  "containingMethodsNum" : 14,
  "sourceCodeText" : "void testPutTaskAttemptIdState() throws Exception\n{\r\n    WebResource r = resource();\r\n    Map<JobId, Job> jobsMap = appContext.getAllJobs();\r\n    for (JobId id : jobsMap.keySet()) {\r\n        String jobId = MRApps.toString(id);\r\n        for (Task task : jobsMap.get(id).getTasks().values()) {\r\n            String tid = MRApps.toString(task.getID());\r\n            for (TaskAttempt att : task.getAttempts().values()) {\r\n                TaskAttemptId attemptid = att.getID();\r\n                String attid = MRApps.toString(attemptid);\r\n                ClientResponse response = r.path(\"ws\").path(\"v1\").path(\"mapreduce\").path(\"jobs\").path(jobId).path(\"tasks\").path(tid).path(\"attempts\").path(attid).path(\"state\").queryParam(\"user.name\", webserviceUserName).accept(MediaType.APPLICATION_JSON).type(MediaType.APPLICATION_JSON).put(ClientResponse.class, \"{\\\"state\\\":\\\"KILLED\\\"}\");\r\n                assertEquals(MediaType.APPLICATION_JSON_TYPE + \"; \" + JettyUtils.UTF_8, response.getType().toString());\r\n                JSONObject json = response.getEntity(JSONObject.class);\r\n                assertEquals(\"incorrect number of elements\", 1, json.length());\r\n                assertEquals(TaskAttemptState.KILLED.toString(), json.get(\"state\"));\r\n            }\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\webapp",
  "methodName" : "testPutTaskAttemptIdXMLState",
  "errType" : null,
  "containingMethodsNum" : 20,
  "sourceCodeText" : "void testPutTaskAttemptIdXMLState() throws Exception\n{\r\n    WebResource r = resource();\r\n    Map<JobId, Job> jobsMap = appContext.getAllJobs();\r\n    for (JobId id : jobsMap.keySet()) {\r\n        String jobId = MRApps.toString(id);\r\n        for (Task task : jobsMap.get(id).getTasks().values()) {\r\n            String tid = MRApps.toString(task.getID());\r\n            for (TaskAttempt att : task.getAttempts().values()) {\r\n                TaskAttemptId attemptid = att.getID();\r\n                String attid = MRApps.toString(attemptid);\r\n                ClientResponse response = r.path(\"ws\").path(\"v1\").path(\"mapreduce\").path(\"jobs\").path(jobId).path(\"tasks\").path(tid).path(\"attempts\").path(attid).path(\"state\").queryParam(\"user.name\", webserviceUserName).accept(MediaType.APPLICATION_XML_TYPE).type(MediaType.APPLICATION_XML_TYPE).put(ClientResponse.class, \"<jobTaskAttemptState><state>KILLED\" + \"</state></jobTaskAttemptState>\");\r\n                assertEquals(MediaType.APPLICATION_XML_TYPE + \"; \" + JettyUtils.UTF_8, response.getType().toString());\r\n                String xml = response.getEntity(String.class);\r\n                DocumentBuilderFactory dbf = DocumentBuilderFactory.newInstance();\r\n                DocumentBuilder db = dbf.newDocumentBuilder();\r\n                InputSource is = new InputSource();\r\n                is.setCharacterStream(new StringReader(xml));\r\n                Document dom = db.parse(is);\r\n                NodeList nodes = dom.getElementsByTagName(\"jobTaskAttemptState\");\r\n                assertEquals(1, nodes.getLength());\r\n                String state = WebServicesTestUtils.getXmlString((Element) nodes.item(0), \"state\");\r\n                assertEquals(TaskAttemptState.KILLED.toString(), state);\r\n            }\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\job\\impl",
  "methodName" : "testCommandLine",
  "errType" : null,
  "containingMethodsNum" : 10,
  "sourceCodeText" : "void testCommandLine() throws Exception\n{\r\n    MyMRApp app = new MyMRApp(1, 0, true, this.getClass().getName(), true);\r\n    Configuration conf = new Configuration();\r\n    conf.setBoolean(MRConfig.MAPREDUCE_APP_SUBMISSION_CROSS_PLATFORM, true);\r\n    Job job = app.submit(conf);\r\n    app.waitForState(job, JobState.SUCCEEDED);\r\n    app.verifyCompleted();\r\n    Assert.assertEquals(\"[\" + MRApps.crossPlatformify(\"JAVA_HOME\") + \"/bin/java\" + \" -Djava.net.preferIPv4Stack=true\" + \" -Dhadoop.metrics.log.level=WARN \" + \"  -Xmx820m -Djava.io.tmpdir=\" + MRApps.crossPlatformify(\"PWD\") + \"/tmp\" + \" -Dlog4j.configuration=container-log4j.properties\" + \" -Dyarn.app.container.log.dir=<LOG_DIR>\" + \" -Dyarn.app.container.log.filesize=0\" + \" -Dhadoop.root.logger=INFO,CLA -Dhadoop.root.logfile=syslog\" + \" org.apache.hadoop.mapred.YarnChild 127.0.0.1\" + \" 54321\" + \" attempt_0_0000_m_000000_0\" + \" 0\" + \" 1><LOG_DIR>/stdout\" + \" 2><LOG_DIR>/stderr ]\", app.launchCmdList.get(0));\r\n    Assert.assertTrue(\"HADOOP_ROOT_LOGGER not set for job\", app.cmdEnvironment.containsKey(\"HADOOP_ROOT_LOGGER\"));\r\n    Assert.assertEquals(\"INFO,console\", app.cmdEnvironment.get(\"HADOOP_ROOT_LOGGER\"));\r\n    Assert.assertTrue(\"HADOOP_CLIENT_OPTS not set for job\", app.cmdEnvironment.containsKey(\"HADOOP_CLIENT_OPTS\"));\r\n    Assert.assertEquals(\"\", app.cmdEnvironment.get(\"HADOOP_CLIENT_OPTS\"));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\job\\impl",
  "methodName" : "testReduceCommandLineWithSeparateShuffle",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void testReduceCommandLineWithSeparateShuffle() throws Exception\n{\r\n    final Configuration conf = new Configuration();\r\n    conf.setBoolean(MRJobConfig.REDUCE_SEPARATE_SHUFFLE_LOG, true);\r\n    testReduceCommandLine(conf);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\job\\impl",
  "methodName" : "testReduceCommandLineWithSeparateCRLAShuffle",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void testReduceCommandLineWithSeparateCRLAShuffle() throws Exception\n{\r\n    final Configuration conf = new Configuration();\r\n    conf.setBoolean(MRJobConfig.REDUCE_SEPARATE_SHUFFLE_LOG, true);\r\n    conf.setLong(MRJobConfig.SHUFFLE_LOG_KB, 1L);\r\n    conf.setInt(MRJobConfig.SHUFFLE_LOG_BACKUPS, 3);\r\n    testReduceCommandLine(conf);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\job\\impl",
  "methodName" : "testReduceCommandLine",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testReduceCommandLine() throws Exception\n{\r\n    final Configuration conf = new Configuration();\r\n    testReduceCommandLine(conf);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\job\\impl",
  "methodName" : "testReduceCommandLine",
  "errType" : null,
  "containingMethodsNum" : 12,
  "sourceCodeText" : "void testReduceCommandLine(Configuration conf) throws Exception\n{\r\n    MyMRApp app = new MyMRApp(0, 1, true, this.getClass().getName(), true);\r\n    conf.setBoolean(MRConfig.MAPREDUCE_APP_SUBMISSION_CROSS_PLATFORM, true);\r\n    Job job = app.submit(conf);\r\n    app.waitForState(job, JobState.SUCCEEDED);\r\n    app.verifyCompleted();\r\n    final long shuffleLogSize = conf.getLong(MRJobConfig.SHUFFLE_LOG_KB, 0L) * 1024L;\r\n    final int shuffleBackups = conf.getInt(MRJobConfig.SHUFFLE_LOG_BACKUPS, 0);\r\n    final String appenderName = shuffleLogSize > 0L && shuffleBackups > 0 ? \"shuffleCRLA\" : \"shuffleCLA\";\r\n    Assert.assertEquals(\"[\" + MRApps.crossPlatformify(\"JAVA_HOME\") + \"/bin/java\" + \" -Djava.net.preferIPv4Stack=true\" + \" -Dhadoop.metrics.log.level=WARN \" + \"  -Xmx820m -Djava.io.tmpdir=\" + MRApps.crossPlatformify(\"PWD\") + \"/tmp\" + \" -Dlog4j.configuration=container-log4j.properties\" + \" -Dyarn.app.container.log.dir=<LOG_DIR>\" + \" -Dyarn.app.container.log.filesize=0\" + \" -Dhadoop.root.logger=INFO,CLA -Dhadoop.root.logfile=syslog\" + \" -Dyarn.app.mapreduce.shuffle.logger=INFO,\" + appenderName + \" -Dyarn.app.mapreduce.shuffle.logfile=syslog.shuffle\" + \" -Dyarn.app.mapreduce.shuffle.log.filesize=\" + shuffleLogSize + \" -Dyarn.app.mapreduce.shuffle.log.backups=\" + shuffleBackups + \" org.apache.hadoop.mapred.YarnChild 127.0.0.1\" + \" 54321\" + \" attempt_0_0000_r_000000_0\" + \" 0\" + \" 1><LOG_DIR>/stdout\" + \" 2><LOG_DIR>/stderr ]\", app.launchCmdList.get(0));\r\n    Assert.assertTrue(\"HADOOP_ROOT_LOGGER not set for job\", app.cmdEnvironment.containsKey(\"HADOOP_ROOT_LOGGER\"));\r\n    Assert.assertEquals(\"INFO,console\", app.cmdEnvironment.get(\"HADOOP_ROOT_LOGGER\"));\r\n    Assert.assertTrue(\"HADOOP_CLIENT_OPTS not set for job\", app.cmdEnvironment.containsKey(\"HADOOP_CLIENT_OPTS\"));\r\n    Assert.assertEquals(\"\", app.cmdEnvironment.get(\"HADOOP_CLIENT_OPTS\"));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\job\\impl",
  "methodName" : "testCommandLineWithLog4JConifg",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void testCommandLineWithLog4JConifg() throws Exception\n{\r\n    MyMRApp app = new MyMRApp(1, 0, true, this.getClass().getName(), true);\r\n    Configuration conf = new Configuration();\r\n    conf.setBoolean(MRConfig.MAPREDUCE_APP_SUBMISSION_CROSS_PLATFORM, true);\r\n    String testLogPropertieFile = \"test-log4j.properties\";\r\n    String testLogPropertiePath = \"../\" + \"test-log4j.properties\";\r\n    conf.set(MRJobConfig.MAPREDUCE_JOB_LOG4J_PROPERTIES_FILE, testLogPropertiePath);\r\n    Job job = app.submit(conf);\r\n    app.waitForState(job, JobState.SUCCEEDED);\r\n    app.verifyCompleted();\r\n    Assert.assertEquals(\"[\" + MRApps.crossPlatformify(\"JAVA_HOME\") + \"/bin/java\" + \" -Djava.net.preferIPv4Stack=true\" + \" -Dhadoop.metrics.log.level=WARN \" + \"  -Xmx820m -Djava.io.tmpdir=\" + MRApps.crossPlatformify(\"PWD\") + \"/tmp\" + \" -Dlog4j.configuration=\" + testLogPropertieFile + \" -Dyarn.app.container.log.dir=<LOG_DIR>\" + \" -Dyarn.app.container.log.filesize=0\" + \" -Dhadoop.root.logger=INFO,CLA -Dhadoop.root.logfile=syslog\" + \" org.apache.hadoop.mapred.YarnChild 127.0.0.1\" + \" 54321\" + \" attempt_0_0000_m_000000_0\" + \" 0\" + \" 1><LOG_DIR>/stdout\" + \" 2><LOG_DIR>/stderr ]\", app.launchCmdList.get(0));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\job\\impl",
  "methodName" : "testAutoHeapSizes",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void testAutoHeapSizes() throws Exception\n{\r\n    testAutoHeapSize(-1, -1, null);\r\n    testAutoHeapSize(512, 768, null);\r\n    testAutoHeapSize(100, 768, null);\r\n    testAutoHeapSize(512, 100, null);\r\n    testAutoHeapSize(512, 768, \"-Xmx100m\");\r\n    testAutoHeapSize(512, 768, \"-Xmx500m\");\r\n    testAutoHeapSize(-1, -1, \"-Xmx100m\");\r\n    testAutoHeapSize(-1, -1, \"-Xmx500m\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\job\\impl",
  "methodName" : "testAutoHeapSize",
  "errType" : null,
  "containingMethodsNum" : 18,
  "sourceCodeText" : "void testAutoHeapSize(int mapMb, int redMb, String xmxArg) throws Exception\n{\r\n    JobConf conf = new JobConf();\r\n    float heapRatio = conf.getFloat(MRJobConfig.HEAP_MEMORY_MB_RATIO, MRJobConfig.DEFAULT_HEAP_MEMORY_MB_RATIO);\r\n    Assert.assertNull(\"Default map java opts!\", conf.get(MRJobConfig.MAP_JAVA_OPTS));\r\n    Assert.assertNull(\"Default reduce java opts!\", conf.get(MRJobConfig.REDUCE_JAVA_OPTS));\r\n    if (mapMb > 0) {\r\n        conf.setInt(MRJobConfig.MAP_MEMORY_MB, mapMb);\r\n    } else {\r\n        mapMb = conf.getMemoryRequired(TaskType.MAP);\r\n    }\r\n    if (redMb > 0) {\r\n        conf.setInt(MRJobConfig.REDUCE_MEMORY_MB, redMb);\r\n    } else {\r\n        redMb = conf.getMemoryRequired(TaskType.REDUCE);\r\n    }\r\n    if (xmxArg != null) {\r\n        conf.set(MRJobConfig.MAP_JAVA_OPTS, xmxArg);\r\n        conf.set(MRJobConfig.REDUCE_JAVA_OPTS, xmxArg);\r\n    }\r\n    MyMRApp app = new MyMRApp(1, 1, true, this.getClass().getName(), true);\r\n    Job job = app.submit(conf);\r\n    app.waitForState(job, JobState.SUCCEEDED);\r\n    app.verifyCompleted();\r\n    for (String cmd : app.launchCmdList) {\r\n        final boolean isMap = cmd.contains(\"_m_\");\r\n        int heapMb;\r\n        if (xmxArg == null) {\r\n            heapMb = (int) (Math.ceil((isMap ? mapMb : redMb) * heapRatio));\r\n        } else {\r\n            final String javaOpts = conf.get(isMap ? MRJobConfig.MAP_JAVA_OPTS : MRJobConfig.REDUCE_JAVA_OPTS);\r\n            heapMb = JobConf.parseMaximumHeapSizeMB(javaOpts);\r\n        }\r\n        Assert.assertEquals(\"Incorrect heapsize in the command opts\", heapMb, JobConf.parseMaximumHeapSizeMB(cmd));\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\job\\impl",
  "methodName" : "testEnvironmentVariables",
  "errType" : null,
  "containingMethodsNum" : 25,
  "sourceCodeText" : "void testEnvironmentVariables() throws Exception\n{\r\n    MyMRApp app = new MyMRApp(1, 0, true, this.getClass().getName(), true);\r\n    Configuration conf = new Configuration();\r\n    conf.set(JobConf.MAPRED_MAP_TASK_ENV, \"HADOOP_CLIENT_OPTS=test\");\r\n    conf.setStrings(MRJobConfig.MAP_LOG_LEVEL, \"WARN\");\r\n    conf.setBoolean(MRConfig.MAPREDUCE_APP_SUBMISSION_CROSS_PLATFORM, false);\r\n    Job job = app.submit(conf);\r\n    app.waitForState(job, JobState.SUCCEEDED);\r\n    app.verifyCompleted();\r\n    Assert.assertTrue(\"HADOOP_ROOT_LOGGER not set for job\", app.cmdEnvironment.containsKey(\"HADOOP_ROOT_LOGGER\"));\r\n    Assert.assertEquals(\"WARN,console\", app.cmdEnvironment.get(\"HADOOP_ROOT_LOGGER\"));\r\n    Assert.assertTrue(\"HADOOP_CLIENT_OPTS not set for job\", app.cmdEnvironment.containsKey(\"HADOOP_CLIENT_OPTS\"));\r\n    Assert.assertEquals(\"test\", app.cmdEnvironment.get(\"HADOOP_CLIENT_OPTS\"));\r\n    app = new MyMRApp(1, 0, true, this.getClass().getName(), true);\r\n    conf = new Configuration();\r\n    conf.set(JobConf.MAPRED_MAP_TASK_ENV, \"HADOOP_ROOT_LOGGER=trace\");\r\n    job = app.submit(conf);\r\n    app.waitForState(job, JobState.SUCCEEDED);\r\n    app.verifyCompleted();\r\n    Assert.assertTrue(\"HADOOP_ROOT_LOGGER not set for job\", app.cmdEnvironment.containsKey(\"HADOOP_ROOT_LOGGER\"));\r\n    Assert.assertEquals(\"trace\", app.cmdEnvironment.get(\"HADOOP_ROOT_LOGGER\"));\r\n    app = new MyMRApp(1, 0, true, this.getClass().getName(), true);\r\n    conf = new Configuration();\r\n    conf.set(JobConf.MAPRED_MAP_TASK_ENV + \".HADOOP_ROOT_LOGGER\", \"DEBUG,console\");\r\n    job = app.submit(conf);\r\n    app.waitForState(job, JobState.SUCCEEDED);\r\n    app.verifyCompleted();\r\n    Assert.assertTrue(\"HADOOP_ROOT_LOGGER not set for job\", app.cmdEnvironment.containsKey(\"HADOOP_ROOT_LOGGER\"));\r\n    Assert.assertEquals(\"DEBUG,console\", app.cmdEnvironment.get(\"HADOOP_ROOT_LOGGER\"));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\webapp",
  "methodName" : "setUp",
  "errType" : null,
  "containingMethodsNum" : 16,
  "sourceCodeText" : "void setUp() throws IOException\n{\r\n    AppContext context = mock(AppContext.class);\r\n    when(context.getApplicationID()).thenReturn(ApplicationId.newInstance(0, 0));\r\n    when(context.getApplicationName()).thenReturn(\"AppName\");\r\n    when(context.getUser()).thenReturn(\"User\");\r\n    when(context.getStartTime()).thenReturn(System.currentTimeMillis());\r\n    job = mock(Job.class);\r\n    Task task = mock(Task.class);\r\n    when(job.getTask(any(TaskId.class))).thenReturn(task);\r\n    when(job.loadConfFile()).thenReturn(new Configuration());\r\n    when(job.getConfFile()).thenReturn(new Path(\"/\"));\r\n    JobId jobID = MRApps.toJobID(\"job_01_01\");\r\n    when(context.getJob(jobID)).thenReturn(job);\r\n    when(job.checkAccess(any(UserGroupInformation.class), any(JobACL.class))).thenReturn(true);\r\n    App app = new App(context);\r\n    Configuration configuration = new Configuration();\r\n    ctx = mock(RequestContext.class);\r\n    appController = new AppControllerForTest(app, configuration, ctx);\r\n    appController.getProperty().put(AMParams.JOB_ID, \"job_01_01\");\r\n    appController.getProperty().put(AMParams.TASK_ID, taskId);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\webapp",
  "methodName" : "testBadRequest",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void testBadRequest()\n{\r\n    String message = \"test string\";\r\n    appController.badRequest(message);\r\n    verifyExpectations(message);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\webapp",
  "methodName" : "testBadRequestWithNullMessage",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void testBadRequestWithNullMessage()\n{\r\n    appController.badRequest(null);\r\n    verifyExpectations(StringUtils.EMPTY);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\webapp",
  "methodName" : "verifyExpectations",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void verifyExpectations(String message)\n{\r\n    verify(ctx).setStatus(400);\r\n    assertEquals(\"application_0_0000\", appController.getProperty().get(\"app.id\"));\r\n    assertNotNull(appController.getProperty().get(\"rm.web\"));\r\n    assertEquals(\"Bad request: \" + message, appController.getProperty().get(\"title\"));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\webapp",
  "methodName" : "testInfo",
  "errType" : null,
  "containingMethodsNum" : 15,
  "sourceCodeText" : "void testInfo()\n{\r\n    appController.info();\r\n    Iterator<ResponseInfo.Item> iterator = appController.getResponseInfo().iterator();\r\n    ResponseInfo.Item item = iterator.next();\r\n    assertEquals(\"Application ID:\", item.key);\r\n    assertEquals(\"application_0_0000\", item.value);\r\n    item = iterator.next();\r\n    assertEquals(\"Application Name:\", item.key);\r\n    assertEquals(\"AppName\", item.value);\r\n    item = iterator.next();\r\n    assertEquals(\"User:\", item.key);\r\n    assertEquals(\"User\", item.value);\r\n    item = iterator.next();\r\n    assertEquals(\"Started on:\", item.key);\r\n    item = iterator.next();\r\n    assertEquals(\"Elasped: \", item.key);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\webapp",
  "methodName" : "testGetJob",
  "errType" : null,
  "containingMethodsNum" : 11,
  "sourceCodeText" : "void testGetJob()\n{\r\n    when(job.checkAccess(any(UserGroupInformation.class), any(JobACL.class))).thenReturn(false);\r\n    appController.job();\r\n    verify(appController.response()).setContentType(MimeType.TEXT);\r\n    assertEquals(\"Access denied: User user does not have permission to view job job_01_01\", appController.getData());\r\n    when(job.checkAccess(any(UserGroupInformation.class), any(JobACL.class))).thenReturn(true);\r\n    appController.getProperty().remove(AMParams.JOB_ID);\r\n    appController.job();\r\n    assertEquals(\"Access denied: User user does not have permission to view job job_01_01Bad Request: Missing job ID\", appController.getData());\r\n    appController.getProperty().put(AMParams.JOB_ID, \"job_01_01\");\r\n    appController.job();\r\n    assertEquals(JobPage.class, appController.getClazz());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\webapp",
  "methodName" : "testGetJobCounters",
  "errType" : null,
  "containingMethodsNum" : 11,
  "sourceCodeText" : "void testGetJobCounters()\n{\r\n    when(job.checkAccess(any(UserGroupInformation.class), any(JobACL.class))).thenReturn(false);\r\n    appController.jobCounters();\r\n    verify(appController.response()).setContentType(MimeType.TEXT);\r\n    assertEquals(\"Access denied: User user does not have permission to view job job_01_01\", appController.getData());\r\n    when(job.checkAccess(any(UserGroupInformation.class), any(JobACL.class))).thenReturn(true);\r\n    appController.getProperty().remove(AMParams.JOB_ID);\r\n    appController.jobCounters();\r\n    assertEquals(\"Access denied: User user does not have permission to view job job_01_01Bad Request: Missing job ID\", appController.getData());\r\n    appController.getProperty().put(AMParams.JOB_ID, \"job_01_01\");\r\n    appController.jobCounters();\r\n    assertEquals(CountersPage.class, appController.getClazz());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\webapp",
  "methodName" : "testGetTaskCounters",
  "errType" : null,
  "containingMethodsNum" : 11,
  "sourceCodeText" : "void testGetTaskCounters()\n{\r\n    when(job.checkAccess(any(UserGroupInformation.class), any(JobACL.class))).thenReturn(false);\r\n    appController.taskCounters();\r\n    verify(appController.response()).setContentType(MimeType.TEXT);\r\n    assertEquals(\"Access denied: User user does not have permission to view job job_01_01\", appController.getData());\r\n    when(job.checkAccess(any(UserGroupInformation.class), any(JobACL.class))).thenReturn(true);\r\n    appController.getProperty().remove(AMParams.TASK_ID);\r\n    appController.taskCounters();\r\n    assertEquals(\"Access denied: User user does not have permission to view job job_01_01missing task ID\", appController.getData());\r\n    appController.getProperty().put(AMParams.TASK_ID, taskId);\r\n    appController.taskCounters();\r\n    assertEquals(CountersPage.class, appController.getClazz());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\webapp",
  "methodName" : "testGetSingleJobCounter",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void testGetSingleJobCounter() throws IOException\n{\r\n    appController.singleJobCounter();\r\n    assertEquals(SingleCounterPage.class, appController.getClazz());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\webapp",
  "methodName" : "testGetSingleTaskCounter",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void testGetSingleTaskCounter() throws IOException\n{\r\n    appController.singleTaskCounter();\r\n    assertEquals(SingleCounterPage.class, appController.getClazz());\r\n    assertNotNull(appController.getProperty().get(AppController.COUNTER_GROUP));\r\n    assertNotNull(appController.getProperty().get(AppController.COUNTER_NAME));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\webapp",
  "methodName" : "testTasks",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void testTasks()\n{\r\n    appController.tasks();\r\n    assertEquals(TasksPage.class, appController.getClazz());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\webapp",
  "methodName" : "testTask",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void testTask()\n{\r\n    appController.task();\r\n    assertEquals(\"Attempts for \" + taskId, appController.getProperty().get(\"title\"));\r\n    assertEquals(TaskPage.class, appController.getClazz());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\webapp",
  "methodName" : "testConfiguration",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void testConfiguration()\n{\r\n    appController.conf();\r\n    assertEquals(JobConfPage.class, appController.getClazz());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\webapp",
  "methodName" : "testDownloadConfiguration",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void testDownloadConfiguration()\n{\r\n    appController.downloadConf();\r\n    String jobConfXml = appController.getData();\r\n    assertTrue(\"Error downloading the job configuration file.\", !jobConfXml.contains(\"Error\"));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\webapp",
  "methodName" : "testAttempts",
  "errType" : null,
  "containingMethodsNum" : 18,
  "sourceCodeText" : "void testAttempts()\n{\r\n    appController.getProperty().remove(AMParams.TASK_TYPE);\r\n    when(job.checkAccess(any(UserGroupInformation.class), any(JobACL.class))).thenReturn(false);\r\n    appController.attempts();\r\n    verify(appController.response()).setContentType(MimeType.TEXT);\r\n    assertEquals(\"Access denied: User user does not have permission to view job job_01_01\", appController.getData());\r\n    when(job.checkAccess(any(UserGroupInformation.class), any(JobACL.class))).thenReturn(true);\r\n    appController.getProperty().remove(AMParams.TASK_ID);\r\n    appController.attempts();\r\n    assertEquals(\"Access denied: User user does not have permission to view job job_01_01\", appController.getData());\r\n    appController.getProperty().put(AMParams.TASK_ID, taskId);\r\n    appController.attempts();\r\n    assertEquals(\"Bad request: missing task-type.\", appController.getProperty().get(\"title\"));\r\n    appController.getProperty().put(AMParams.TASK_TYPE, \"m\");\r\n    appController.attempts();\r\n    assertEquals(\"Bad request: missing attempt-state.\", appController.getProperty().get(\"title\"));\r\n    appController.getProperty().put(AMParams.ATTEMPT_STATE, \"State\");\r\n    appController.attempts();\r\n    assertEquals(AttemptsPage.class, appController.getClazz());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\webapp",
  "methodName" : "render",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void render()\n{\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\job\\impl",
  "methodName" : "testShuffleProviders",
  "errType" : null,
  "containingMethodsNum" : 26,
  "sourceCodeText" : "void testShuffleProviders() throws Exception\n{\r\n    ApplicationId appId = ApplicationId.newInstance(1, 1);\r\n    JobId jobId = MRBuilderUtils.newJobId(appId, 1);\r\n    TaskId taskId = MRBuilderUtils.newTaskId(jobId, 1, TaskType.MAP);\r\n    Path jobFile = mock(Path.class);\r\n    EventHandler eventHandler = mock(EventHandler.class);\r\n    TaskAttemptListener taListener = mock(TaskAttemptListener.class);\r\n    when(taListener.getAddress()).thenReturn(new InetSocketAddress(\"localhost\", 0));\r\n    JobConf jobConf = new JobConf();\r\n    jobConf.setClass(\"fs.file.impl\", StubbedFS.class, FileSystem.class);\r\n    jobConf.setBoolean(\"fs.file.impl.disable.cache\", true);\r\n    jobConf.set(JobConf.MAPRED_MAP_TASK_ENV, \"\");\r\n    jobConf.set(YarnConfiguration.NM_AUX_SERVICES, TestShuffleHandler1.MAPREDUCE_TEST_SHUFFLE_SERVICEID + \",\" + TestShuffleHandler2.MAPREDUCE_TEST_SHUFFLE_SERVICEID);\r\n    String serviceName = TestShuffleHandler1.MAPREDUCE_TEST_SHUFFLE_SERVICEID;\r\n    String serviceStr = String.format(YarnConfiguration.NM_AUX_SERVICE_FMT, serviceName);\r\n    jobConf.set(serviceStr, TestShuffleHandler1.class.getName());\r\n    serviceName = TestShuffleHandler2.MAPREDUCE_TEST_SHUFFLE_SERVICEID;\r\n    serviceStr = String.format(YarnConfiguration.NM_AUX_SERVICE_FMT, serviceName);\r\n    jobConf.set(serviceStr, TestShuffleHandler2.class.getName());\r\n    jobConf.set(MRJobConfig.MAPREDUCE_JOB_SHUFFLE_PROVIDER_SERVICES, TestShuffleHandler1.MAPREDUCE_TEST_SHUFFLE_SERVICEID + \",\" + TestShuffleHandler2.MAPREDUCE_TEST_SHUFFLE_SERVICEID);\r\n    Credentials credentials = new Credentials();\r\n    Token<JobTokenIdentifier> jobToken = new Token<JobTokenIdentifier>((\"tokenid\").getBytes(), (\"tokenpw\").getBytes(), new Text(\"tokenkind\"), new Text(\"tokenservice\"));\r\n    TaskAttemptImpl taImpl = new MapTaskAttemptImpl(taskId, 1, eventHandler, jobFile, 1, mock(TaskSplitMetaInfo.class), jobConf, taListener, jobToken, credentials, SystemClock.getInstance(), null);\r\n    jobConf.set(MRJobConfig.APPLICATION_ATTEMPT_ID, taImpl.getID().toString());\r\n    ContainerLaunchContext launchCtx = TaskAttemptImpl.createContainerLaunchContext(null, jobConf, jobToken, taImpl.createRemoteTask(), TypeConverter.fromYarn(jobId), mock(WrappedJvmID.class), taListener, credentials);\r\n    Map<String, ByteBuffer> serviceDataMap = launchCtx.getServiceData();\r\n    Assert.assertNotNull(\"TestShuffleHandler1 is missing\", serviceDataMap.get(TestShuffleHandler1.MAPREDUCE_TEST_SHUFFLE_SERVICEID));\r\n    Assert.assertNotNull(\"TestShuffleHandler2 is missing\", serviceDataMap.get(TestShuffleHandler2.MAPREDUCE_TEST_SHUFFLE_SERVICEID));\r\n    Assert.assertTrue(\"mismatch number of services in map\", serviceDataMap.size() == 3);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app",
  "methodName" : "newJobName",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String newJobName()\n{\r\n    return newAppName();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app",
  "methodName" : "newJobs",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "Map<JobId, Job> newJobs(int numJobs, int numTasksPerJob, int numAttemptsPerTask)\n{\r\n    Map<JobId, Job> map = Maps.newHashMap();\r\n    for (int j = 0; j < numJobs; ++j) {\r\n        ApplicationId appID = MockJobs.newAppID(j);\r\n        Job job = newJob(appID, j, numTasksPerJob, numAttemptsPerTask);\r\n        map.put(job.getID(), job);\r\n    }\r\n    return map;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app",
  "methodName" : "newJobs",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "Map<JobId, Job> newJobs(ApplicationId appID, int numJobsPerApp, int numTasksPerJob, int numAttemptsPerTask)\n{\r\n    Map<JobId, Job> map = Maps.newHashMap();\r\n    for (int j = 0; j < numJobsPerApp; ++j) {\r\n        Job job = newJob(appID, j, numTasksPerJob, numAttemptsPerTask);\r\n        map.put(job.getID(), job);\r\n    }\r\n    return map;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app",
  "methodName" : "newJobs",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "Map<JobId, Job> newJobs(ApplicationId appID, int numJobsPerApp, int numTasksPerJob, int numAttemptsPerTask, boolean hasFailedTasks)\n{\r\n    Map<JobId, Job> map = Maps.newHashMap();\r\n    for (int j = 0; j < numJobsPerApp; ++j) {\r\n        Job job = newJob(appID, j, numTasksPerJob, numAttemptsPerTask, null, hasFailedTasks);\r\n        map.put(job.getID(), job);\r\n    }\r\n    return map;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app",
  "methodName" : "newJobID",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "JobId newJobID(ApplicationId appID, int i)\n{\r\n    JobId id = Records.newRecord(JobId.class);\r\n    id.setAppId(appID);\r\n    id.setId(i);\r\n    return id;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app",
  "methodName" : "newJobReport",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "JobReport newJobReport(JobId id)\n{\r\n    JobReport report = Records.newRecord(JobReport.class);\r\n    report.setJobId(id);\r\n    report.setSubmitTime(System.currentTimeMillis() - DT);\r\n    report.setStartTime(System.currentTimeMillis() - (int) (Math.random() * DT));\r\n    report.setFinishTime(System.currentTimeMillis() + (int) (Math.random() * DT) + 1);\r\n    report.setMapProgress((float) Math.random());\r\n    report.setReduceProgress((float) Math.random());\r\n    report.setJobState(JOB_STATES.next());\r\n    return report;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app",
  "methodName" : "newTaskReport",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "TaskReport newTaskReport(TaskId id)\n{\r\n    TaskReport report = Records.newRecord(TaskReport.class);\r\n    report.setTaskId(id);\r\n    report.setStartTime(System.currentTimeMillis() - (int) (Math.random() * DT));\r\n    report.setFinishTime(System.currentTimeMillis() + (int) (Math.random() * DT) + 1);\r\n    report.setProgress((float) Math.random());\r\n    report.setStatus(\"Moving average: \" + Math.random());\r\n    report.setCounters(TypeConverter.toYarn(newCounters()));\r\n    report.setTaskState(TASK_STATES.next());\r\n    return report;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app",
  "methodName" : "newTaskAttemptReport",
  "errType" : null,
  "containingMethodsNum" : 16,
  "sourceCodeText" : "TaskAttemptReport newTaskAttemptReport(TaskAttemptId id)\n{\r\n    ApplicationAttemptId appAttemptId = ApplicationAttemptId.newInstance(id.getTaskId().getJobId().getAppId(), 0);\r\n    ContainerId containerId = ContainerId.newContainerId(appAttemptId, 0);\r\n    TaskAttemptReport report = Records.newRecord(TaskAttemptReport.class);\r\n    report.setTaskAttemptId(id);\r\n    report.setStartTime(System.currentTimeMillis() - (int) (Math.random() * DT));\r\n    report.setFinishTime(System.currentTimeMillis() + (int) (Math.random() * DT) + 1);\r\n    if (id.getTaskId().getTaskType() == TaskType.REDUCE) {\r\n        report.setShuffleFinishTime((report.getFinishTime() + report.getStartTime()) / 2);\r\n        report.setSortFinishTime((report.getFinishTime() + report.getShuffleFinishTime()) / 2);\r\n    }\r\n    report.setPhase(PHASES.next());\r\n    report.setTaskAttemptState(TASK_ATTEMPT_STATES.next());\r\n    report.setProgress((float) Math.random());\r\n    report.setCounters(TypeConverter.toYarn(newCounters()));\r\n    report.setContainerId(containerId);\r\n    report.setDiagnosticInfo(DIAGS.next());\r\n    report.setStateString(\"Moving average \" + Math.random());\r\n    return report;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app",
  "methodName" : "newCounters",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "Counters newCounters()\n{\r\n    Counters hc = new Counters();\r\n    for (JobCounter c : JobCounter.values()) {\r\n        hc.findCounter(c).setValue((long) (Math.random() * 1000));\r\n    }\r\n    for (TaskCounter c : TaskCounter.values()) {\r\n        hc.findCounter(c).setValue((long) (Math.random() * 1000));\r\n    }\r\n    int nc = FileSystemCounter.values().length * 4;\r\n    for (int i = 0; i < nc; ++i) {\r\n        for (FileSystemCounter c : FileSystemCounter.values()) {\r\n            hc.findCounter(FS_SCHEMES.next(), c).setValue((long) (Math.random() * DT));\r\n        }\r\n    }\r\n    for (int i = 0; i < 2 * 3; ++i) {\r\n        hc.findCounter(USER_COUNTER_GROUPS.next(), USER_COUNTERS.next()).setValue((long) (Math.random() * 100000));\r\n    }\r\n    return hc;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app",
  "methodName" : "newTaskAttempts",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "Map<TaskAttemptId, TaskAttempt> newTaskAttempts(TaskId tid, int m)\n{\r\n    Map<TaskAttemptId, TaskAttempt> map = Maps.newHashMap();\r\n    for (int i = 0; i < m; ++i) {\r\n        TaskAttempt ta = newTaskAttempt(tid, i);\r\n        map.put(ta.getID(), ta);\r\n    }\r\n    return map;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app",
  "methodName" : "newTaskAttempt",
  "errType" : null,
  "containingMethodsNum" : 17,
  "sourceCodeText" : "TaskAttempt newTaskAttempt(TaskId tid, int i)\n{\r\n    final TaskAttemptId taid = Records.newRecord(TaskAttemptId.class);\r\n    taid.setTaskId(tid);\r\n    taid.setId(i);\r\n    final TaskAttemptReport report = newTaskAttemptReport(taid);\r\n    return new TaskAttempt() {\r\n\r\n        @Override\r\n        public NodeId getNodeId() throws UnsupportedOperationException {\r\n            throw new UnsupportedOperationException();\r\n        }\r\n\r\n        @Override\r\n        public TaskAttemptId getID() {\r\n            return taid;\r\n        }\r\n\r\n        @Override\r\n        public TaskAttemptReport getReport() {\r\n            return report;\r\n        }\r\n\r\n        @Override\r\n        public long getLaunchTime() {\r\n            return report.getStartTime();\r\n        }\r\n\r\n        @Override\r\n        public long getFinishTime() {\r\n            return report.getFinishTime();\r\n        }\r\n\r\n        @Override\r\n        public int getShufflePort() {\r\n            return ShuffleHandler.DEFAULT_SHUFFLE_PORT;\r\n        }\r\n\r\n        @Override\r\n        public Counters getCounters() {\r\n            if (report != null && report.getCounters() != null) {\r\n                return new Counters(TypeConverter.fromYarn(report.getCounters()));\r\n            }\r\n            return null;\r\n        }\r\n\r\n        @Override\r\n        public float getProgress() {\r\n            return report.getProgress();\r\n        }\r\n\r\n        @Override\r\n        public Phase getPhase() {\r\n            return report.getPhase();\r\n        }\r\n\r\n        @Override\r\n        public TaskAttemptState getState() {\r\n            return report.getTaskAttemptState();\r\n        }\r\n\r\n        @Override\r\n        public boolean isFinished() {\r\n            switch(report.getTaskAttemptState()) {\r\n                case SUCCEEDED:\r\n                case FAILED:\r\n                case KILLED:\r\n                    return true;\r\n            }\r\n            return false;\r\n        }\r\n\r\n        @Override\r\n        public ContainerId getAssignedContainerID() {\r\n            ApplicationAttemptId appAttemptId = ApplicationAttemptId.newInstance(taid.getTaskId().getJobId().getAppId(), 0);\r\n            ContainerId id = ContainerId.newContainerId(appAttemptId, 0);\r\n            return id;\r\n        }\r\n\r\n        @Override\r\n        public String getNodeHttpAddress() {\r\n            return \"localhost:8042\";\r\n        }\r\n\r\n        @Override\r\n        public List<String> getDiagnostics() {\r\n            return Lists.newArrayList(report.getDiagnosticInfo());\r\n        }\r\n\r\n        @Override\r\n        public String getAssignedContainerMgrAddress() {\r\n            return \"localhost:9998\";\r\n        }\r\n\r\n        @Override\r\n        public long getShuffleFinishTime() {\r\n            return report.getShuffleFinishTime();\r\n        }\r\n\r\n        @Override\r\n        public long getSortFinishTime() {\r\n            return report.getSortFinishTime();\r\n        }\r\n\r\n        @Override\r\n        public String getNodeRackName() {\r\n            return \"/default-rack\";\r\n        }\r\n    };\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app",
  "methodName" : "newTasks",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "Map<TaskId, Task> newTasks(JobId jid, int n, int m, boolean hasFailedTasks)\n{\r\n    Map<TaskId, Task> map = Maps.newHashMap();\r\n    for (int i = 0; i < n; ++i) {\r\n        Task task = newTask(jid, i, m, hasFailedTasks);\r\n        map.put(task.getID(), task);\r\n    }\r\n    return map;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app",
  "methodName" : "newTask",
  "errType" : null,
  "containingMethodsNum" : 12,
  "sourceCodeText" : "Task newTask(JobId jid, int i, int m, final boolean hasFailedTasks)\n{\r\n    final TaskId tid = Records.newRecord(TaskId.class);\r\n    tid.setJobId(jid);\r\n    tid.setId(i);\r\n    tid.setTaskType(TASK_TYPES.next());\r\n    final TaskReport report = newTaskReport(tid);\r\n    final Map<TaskAttemptId, TaskAttempt> attempts = newTaskAttempts(tid, m);\r\n    return new Task() {\r\n\r\n        @Override\r\n        public TaskId getID() {\r\n            return tid;\r\n        }\r\n\r\n        @Override\r\n        public TaskReport getReport() {\r\n            return report;\r\n        }\r\n\r\n        @Override\r\n        public Counters getCounters() {\r\n            if (hasFailedTasks) {\r\n                return null;\r\n            }\r\n            return new Counters(TypeConverter.fromYarn(report.getCounters()));\r\n        }\r\n\r\n        @Override\r\n        public float getProgress() {\r\n            return report.getProgress();\r\n        }\r\n\r\n        @Override\r\n        public TaskType getType() {\r\n            return tid.getTaskType();\r\n        }\r\n\r\n        @Override\r\n        public Map<TaskAttemptId, TaskAttempt> getAttempts() {\r\n            return attempts;\r\n        }\r\n\r\n        @Override\r\n        public TaskAttempt getAttempt(TaskAttemptId attemptID) {\r\n            return attempts.get(attemptID);\r\n        }\r\n\r\n        @Override\r\n        public boolean isFinished() {\r\n            switch(report.getTaskState()) {\r\n                case SUCCEEDED:\r\n                case KILLED:\r\n                case FAILED:\r\n                    return true;\r\n            }\r\n            return false;\r\n        }\r\n\r\n        @Override\r\n        public boolean canCommit(TaskAttemptId taskAttemptID) {\r\n            return false;\r\n        }\r\n\r\n        @Override\r\n        public TaskState getState() {\r\n            return report.getTaskState();\r\n        }\r\n    };\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app",
  "methodName" : "getCounters",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "Counters getCounters(Collection<Task> tasks)\n{\r\n    List<Task> completedTasks = new ArrayList<Task>();\r\n    for (Task task : tasks) {\r\n        if (task.getCounters() != null) {\r\n            completedTasks.add(task);\r\n        }\r\n    }\r\n    Counters counters = new Counters();\r\n    return JobImpl.incrTaskCounters(counters, completedTasks);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app",
  "methodName" : "getTaskCount",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "TaskCount getTaskCount(Collection<Task> tasks)\n{\r\n    TaskCount tc = new TaskCount();\r\n    for (Task task : tasks) {\r\n        tc.incr(task);\r\n    }\r\n    return tc;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app",
  "methodName" : "newJob",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Job newJob(ApplicationId appID, int i, int n, int m)\n{\r\n    return newJob(appID, i, n, m, null);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app",
  "methodName" : "newJob",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Job newJob(ApplicationId appID, int i, int n, int m, Path confFile)\n{\r\n    return newJob(appID, i, n, m, confFile, false);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app",
  "methodName" : "newJob",
  "errType" : null,
  "containingMethodsNum" : 16,
  "sourceCodeText" : "Job newJob(ApplicationId appID, int i, int n, int m, Path confFile, boolean hasFailedTasks)\n{\r\n    final JobId id = newJobID(appID, i);\r\n    final String name = newJobName();\r\n    final JobReport report = newJobReport(id);\r\n    final Map<TaskId, Task> tasks = newTasks(id, n, m, hasFailedTasks);\r\n    final TaskCount taskCount = getTaskCount(tasks.values());\r\n    final Counters counters = getCounters(tasks.values());\r\n    final Path configFile = confFile;\r\n    Map<JobACL, AccessControlList> tmpJobACLs = new HashMap<JobACL, AccessControlList>();\r\n    final Configuration conf = new Configuration();\r\n    conf.set(JobACL.VIEW_JOB.getAclName(), \"testuser\");\r\n    conf.setBoolean(MRConfig.MR_ACLS_ENABLED, true);\r\n    JobACLsManager aclsManager = new JobACLsManager(conf);\r\n    tmpJobACLs = aclsManager.constructJobACLs(conf);\r\n    final Map<JobACL, AccessControlList> jobACLs = tmpJobACLs;\r\n    return new Job() {\r\n\r\n        @Override\r\n        public JobId getID() {\r\n            return id;\r\n        }\r\n\r\n        @Override\r\n        public String getName() {\r\n            return name;\r\n        }\r\n\r\n        @Override\r\n        public JobState getState() {\r\n            return report.getJobState();\r\n        }\r\n\r\n        @Override\r\n        public JobReport getReport() {\r\n            return report;\r\n        }\r\n\r\n        @Override\r\n        public float getProgress() {\r\n            return 0;\r\n        }\r\n\r\n        @Override\r\n        public Counters getAllCounters() {\r\n            return counters;\r\n        }\r\n\r\n        @Override\r\n        public Map<TaskId, Task> getTasks() {\r\n            return tasks;\r\n        }\r\n\r\n        @Override\r\n        public Task getTask(TaskId taskID) {\r\n            return tasks.get(taskID);\r\n        }\r\n\r\n        @Override\r\n        public int getTotalMaps() {\r\n            return taskCount.maps;\r\n        }\r\n\r\n        @Override\r\n        public int getTotalReduces() {\r\n            return taskCount.reduces;\r\n        }\r\n\r\n        @Override\r\n        public int getCompletedMaps() {\r\n            return taskCount.completedMaps;\r\n        }\r\n\r\n        @Override\r\n        public int getCompletedReduces() {\r\n            return taskCount.completedReduces;\r\n        }\r\n\r\n        @Override\r\n        public boolean isUber() {\r\n            return false;\r\n        }\r\n\r\n        @Override\r\n        public TaskAttemptCompletionEvent[] getTaskAttemptCompletionEvents(int fromEventId, int maxEvents) {\r\n            return null;\r\n        }\r\n\r\n        @Override\r\n        public TaskCompletionEvent[] getMapAttemptCompletionEvents(int startIndex, int maxEvents) {\r\n            return null;\r\n        }\r\n\r\n        @Override\r\n        public Map<TaskId, Task> getTasks(TaskType taskType) {\r\n            throw new UnsupportedOperationException(\"Not supported yet.\");\r\n        }\r\n\r\n        @Override\r\n        public List<String> getDiagnostics() {\r\n            return Collections.<String>emptyList();\r\n        }\r\n\r\n        @Override\r\n        public boolean checkAccess(UserGroupInformation callerUGI, JobACL jobOperation) {\r\n            return true;\r\n        }\r\n\r\n        @Override\r\n        public String getUserName() {\r\n            return \"mock\";\r\n        }\r\n\r\n        @Override\r\n        public String getQueueName() {\r\n            return \"mockqueue\";\r\n        }\r\n\r\n        @Override\r\n        public Path getConfFile() {\r\n            return configFile;\r\n        }\r\n\r\n        @Override\r\n        public Map<JobACL, AccessControlList> getJobACLs() {\r\n            return jobACLs;\r\n        }\r\n\r\n        @Override\r\n        public List<AMInfo> getAMInfos() {\r\n            List<AMInfo> amInfoList = new LinkedList<AMInfo>();\r\n            amInfoList.add(createAMInfo(1));\r\n            amInfoList.add(createAMInfo(2));\r\n            return amInfoList;\r\n        }\r\n\r\n        @Override\r\n        public Configuration loadConfFile() throws IOException {\r\n            FileContext fc = FileContext.getFileContext(configFile.toUri(), conf);\r\n            Configuration jobConf = new Configuration(false);\r\n            jobConf.addResource(fc.open(configFile), configFile.toString());\r\n            return jobConf;\r\n        }\r\n\r\n        @Override\r\n        public void setQueueName(String queueName) {\r\n        }\r\n\r\n        @Override\r\n        public void setJobPriority(Priority priority) {\r\n        }\r\n\r\n        public int getFailedMaps() {\r\n            return 0;\r\n        }\r\n\r\n        @Override\r\n        public int getFailedReduces() {\r\n            return 0;\r\n        }\r\n\r\n        @Override\r\n        public int getKilledMaps() {\r\n            return 0;\r\n        }\r\n\r\n        @Override\r\n        public int getKilledReduces() {\r\n            return 0;\r\n        }\r\n    };\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app",
  "methodName" : "createAMInfo",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "AMInfo createAMInfo(int attempt)\n{\r\n    ApplicationAttemptId appAttemptId = ApplicationAttemptId.newInstance(ApplicationId.newInstance(100, 1), attempt);\r\n    ContainerId containerId = ContainerId.newContainerId(appAttemptId, 1);\r\n    return MRBuilderUtils.newAMInfo(appAttemptId, System.currentTimeMillis(), containerId, NM_HOST, NM_PORT, NM_HTTP_PORT);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\launcher",
  "methodName" : "setup",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void setup() throws IOException\n{\r\n    serviceResponse.clear();\r\n    serviceResponse.put(ShuffleHandler.MAPREDUCE_SHUFFLE_SERVICEID, ShuffleHandler.serializeMetaData(80));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\launcher",
  "methodName" : "makeContainerId",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "ContainerId makeContainerId(long ts, int appId, int attemptId, int id)\n{\r\n    return ContainerId.newContainerId(ApplicationAttemptId.newInstance(ApplicationId.newInstance(ts, appId), attemptId), id);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\launcher",
  "methodName" : "makeTaskAttemptId",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "TaskAttemptId makeTaskAttemptId(long ts, int appId, int taskId, TaskType taskType, int id)\n{\r\n    ApplicationId aID = ApplicationId.newInstance(ts, appId);\r\n    JobId jID = MRBuilderUtils.newJobId(aID, id);\r\n    TaskId tID = MRBuilderUtils.newTaskId(jID, taskId, taskType);\r\n    return MRBuilderUtils.newTaskAttemptId(tID, id);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\launcher",
  "methodName" : "testHandle",
  "errType" : null,
  "containingMethodsNum" : 32,
  "sourceCodeText" : "void testHandle() throws Exception\n{\r\n    LOG.info(\"STARTING testHandle\");\r\n    AppContext mockContext = mock(AppContext.class);\r\n    @SuppressWarnings(\"unchecked\")\r\n    EventHandler<Event> mockEventHandler = mock(EventHandler.class);\r\n    when(mockContext.getEventHandler()).thenReturn(mockEventHandler);\r\n    String cmAddress = \"127.0.0.1:8000\";\r\n    ContainerManagementProtocolClient mockCM = mock(ContainerManagementProtocolClient.class);\r\n    ContainerLauncherImplUnderTest ut = new ContainerLauncherImplUnderTest(mockContext, mockCM);\r\n    Configuration conf = new Configuration();\r\n    ut.init(conf);\r\n    ut.start();\r\n    try {\r\n        ContainerId contId = makeContainerId(0l, 0, 0, 1);\r\n        TaskAttemptId taskAttemptId = makeTaskAttemptId(0l, 0, 0, TaskType.MAP, 0);\r\n        StartContainersResponse startResp = recordFactory.newRecordInstance(StartContainersResponse.class);\r\n        startResp.setAllServicesMetaData(serviceResponse);\r\n        LOG.info(\"inserting launch event\");\r\n        ContainerRemoteLaunchEvent mockLaunchEvent = mock(ContainerRemoteLaunchEvent.class);\r\n        when(mockLaunchEvent.getType()).thenReturn(EventType.CONTAINER_REMOTE_LAUNCH);\r\n        when(mockLaunchEvent.getContainerID()).thenReturn(contId);\r\n        when(mockLaunchEvent.getTaskAttemptID()).thenReturn(taskAttemptId);\r\n        when(mockLaunchEvent.getContainerMgrAddress()).thenReturn(cmAddress);\r\n        when(mockCM.startContainers(any(StartContainersRequest.class))).thenReturn(startResp);\r\n        when(mockLaunchEvent.getContainerToken()).thenReturn(createNewContainerToken(contId, cmAddress));\r\n        ut.handle(mockLaunchEvent);\r\n        ut.waitForPoolToIdle();\r\n        verify(mockCM).startContainers(any(StartContainersRequest.class));\r\n        LOG.info(\"inserting cleanup event\");\r\n        ContainerLauncherEvent mockCleanupEvent = mock(ContainerLauncherEvent.class);\r\n        when(mockCleanupEvent.getType()).thenReturn(EventType.CONTAINER_REMOTE_CLEANUP);\r\n        when(mockCleanupEvent.getContainerID()).thenReturn(contId);\r\n        when(mockCleanupEvent.getTaskAttemptID()).thenReturn(taskAttemptId);\r\n        when(mockCleanupEvent.getContainerMgrAddress()).thenReturn(cmAddress);\r\n        ut.handle(mockCleanupEvent);\r\n        ut.waitForPoolToIdle();\r\n        verify(mockCM).stopContainers(any(StopContainersRequest.class));\r\n    } finally {\r\n        ut.stop();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\launcher",
  "methodName" : "testOutOfOrder",
  "errType" : null,
  "containingMethodsNum" : 32,
  "sourceCodeText" : "void testOutOfOrder() throws Exception\n{\r\n    LOG.info(\"STARTING testOutOfOrder\");\r\n    AppContext mockContext = mock(AppContext.class);\r\n    @SuppressWarnings(\"unchecked\")\r\n    EventHandler<Event> mockEventHandler = mock(EventHandler.class);\r\n    when(mockContext.getEventHandler()).thenReturn(mockEventHandler);\r\n    ContainerManagementProtocolClient mockCM = mock(ContainerManagementProtocolClient.class);\r\n    ContainerLauncherImplUnderTest ut = new ContainerLauncherImplUnderTest(mockContext, mockCM);\r\n    Configuration conf = new Configuration();\r\n    ut.init(conf);\r\n    ut.start();\r\n    try {\r\n        ContainerId contId = makeContainerId(0l, 0, 0, 1);\r\n        TaskAttemptId taskAttemptId = makeTaskAttemptId(0l, 0, 0, TaskType.MAP, 0);\r\n        String cmAddress = \"127.0.0.1:8000\";\r\n        StartContainersResponse startResp = recordFactory.newRecordInstance(StartContainersResponse.class);\r\n        startResp.setAllServicesMetaData(serviceResponse);\r\n        LOG.info(\"inserting cleanup event\");\r\n        ContainerLauncherEvent mockCleanupEvent = mock(ContainerLauncherEvent.class);\r\n        when(mockCleanupEvent.getType()).thenReturn(EventType.CONTAINER_REMOTE_CLEANUP);\r\n        when(mockCleanupEvent.getContainerID()).thenReturn(contId);\r\n        when(mockCleanupEvent.getTaskAttemptID()).thenReturn(taskAttemptId);\r\n        when(mockCleanupEvent.getContainerMgrAddress()).thenReturn(cmAddress);\r\n        ut.handle(mockCleanupEvent);\r\n        ut.waitForPoolToIdle();\r\n        verify(mockCM, never()).stopContainers(any(StopContainersRequest.class));\r\n        LOG.info(\"inserting launch event\");\r\n        ContainerRemoteLaunchEvent mockLaunchEvent = mock(ContainerRemoteLaunchEvent.class);\r\n        when(mockLaunchEvent.getType()).thenReturn(EventType.CONTAINER_REMOTE_LAUNCH);\r\n        when(mockLaunchEvent.getContainerID()).thenReturn(contId);\r\n        when(mockLaunchEvent.getTaskAttemptID()).thenReturn(taskAttemptId);\r\n        when(mockLaunchEvent.getContainerMgrAddress()).thenReturn(cmAddress);\r\n        when(mockCM.startContainers(any(StartContainersRequest.class))).thenReturn(startResp);\r\n        when(mockLaunchEvent.getContainerToken()).thenReturn(createNewContainerToken(contId, cmAddress));\r\n        ut.handle(mockLaunchEvent);\r\n        ut.waitForPoolToIdle();\r\n        verify(mockCM, never()).startContainers(any(StartContainersRequest.class));\r\n    } finally {\r\n        ut.stop();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\launcher",
  "methodName" : "testMyShutdown",
  "errType" : null,
  "containingMethodsNum" : 24,
  "sourceCodeText" : "void testMyShutdown() throws Exception\n{\r\n    LOG.info(\"in test Shutdown\");\r\n    AppContext mockContext = mock(AppContext.class);\r\n    @SuppressWarnings(\"unchecked\")\r\n    EventHandler<Event> mockEventHandler = mock(EventHandler.class);\r\n    when(mockContext.getEventHandler()).thenReturn(mockEventHandler);\r\n    ContainerManagementProtocolClient mockCM = mock(ContainerManagementProtocolClient.class);\r\n    ContainerLauncherImplUnderTest ut = new ContainerLauncherImplUnderTest(mockContext, mockCM);\r\n    Configuration conf = new Configuration();\r\n    ut.init(conf);\r\n    ut.start();\r\n    try {\r\n        ContainerId contId = makeContainerId(0l, 0, 0, 1);\r\n        TaskAttemptId taskAttemptId = makeTaskAttemptId(0l, 0, 0, TaskType.MAP, 0);\r\n        String cmAddress = \"127.0.0.1:8000\";\r\n        StartContainersResponse startResp = recordFactory.newRecordInstance(StartContainersResponse.class);\r\n        startResp.setAllServicesMetaData(serviceResponse);\r\n        LOG.info(\"inserting launch event\");\r\n        ContainerRemoteLaunchEvent mockLaunchEvent = mock(ContainerRemoteLaunchEvent.class);\r\n        when(mockLaunchEvent.getType()).thenReturn(EventType.CONTAINER_REMOTE_LAUNCH);\r\n        when(mockLaunchEvent.getContainerID()).thenReturn(contId);\r\n        when(mockLaunchEvent.getTaskAttemptID()).thenReturn(taskAttemptId);\r\n        when(mockLaunchEvent.getContainerMgrAddress()).thenReturn(cmAddress);\r\n        when(mockCM.startContainers(any(StartContainersRequest.class))).thenReturn(startResp);\r\n        when(mockLaunchEvent.getContainerToken()).thenReturn(createNewContainerToken(contId, cmAddress));\r\n        ut.handle(mockLaunchEvent);\r\n        ut.waitForPoolToIdle();\r\n        verify(mockCM).startContainers(any(StartContainersRequest.class));\r\n    } finally {\r\n        ut.stop();\r\n        verify(mockCM).stopContainers(any(StopContainersRequest.class));\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\launcher",
  "methodName" : "testContainerCleaned",
  "errType" : null,
  "containingMethodsNum" : 35,
  "sourceCodeText" : "void testContainerCleaned() throws Exception\n{\r\n    LOG.info(\"STARTING testContainerCleaned\");\r\n    CyclicBarrier startLaunchBarrier = new CyclicBarrier(2);\r\n    CyclicBarrier completeLaunchBarrier = new CyclicBarrier(2);\r\n    AppContext mockContext = mock(AppContext.class);\r\n    EventHandler mockEventHandler = mock(EventHandler.class);\r\n    when(mockContext.getEventHandler()).thenReturn(mockEventHandler);\r\n    ContainerManagementProtocolClient mockCM = new ContainerManagerForTest(startLaunchBarrier, completeLaunchBarrier);\r\n    ContainerLauncherImplUnderTest ut = new ContainerLauncherImplUnderTest(mockContext, mockCM);\r\n    Configuration conf = new Configuration();\r\n    ut.init(conf);\r\n    ut.start();\r\n    try {\r\n        ContainerId contId = makeContainerId(0l, 0, 0, 1);\r\n        TaskAttemptId taskAttemptId = makeTaskAttemptId(0l, 0, 0, TaskType.MAP, 0);\r\n        String cmAddress = \"127.0.0.1:8000\";\r\n        StartContainersResponse startResp = recordFactory.newRecordInstance(StartContainersResponse.class);\r\n        startResp.setAllServicesMetaData(serviceResponse);\r\n        LOG.info(\"inserting launch event\");\r\n        ContainerRemoteLaunchEvent mockLaunchEvent = mock(ContainerRemoteLaunchEvent.class);\r\n        when(mockLaunchEvent.getType()).thenReturn(EventType.CONTAINER_REMOTE_LAUNCH);\r\n        when(mockLaunchEvent.getContainerID()).thenReturn(contId);\r\n        when(mockLaunchEvent.getTaskAttemptID()).thenReturn(taskAttemptId);\r\n        when(mockLaunchEvent.getContainerMgrAddress()).thenReturn(cmAddress);\r\n        when(mockLaunchEvent.getContainerToken()).thenReturn(createNewContainerToken(contId, cmAddress));\r\n        ut.handle(mockLaunchEvent);\r\n        startLaunchBarrier.await();\r\n        LOG.info(\"inserting cleanup event\");\r\n        ContainerLauncherEvent mockCleanupEvent = mock(ContainerLauncherEvent.class);\r\n        when(mockCleanupEvent.getType()).thenReturn(EventType.CONTAINER_REMOTE_CLEANUP);\r\n        when(mockCleanupEvent.getContainerID()).thenReturn(contId);\r\n        when(mockCleanupEvent.getTaskAttemptID()).thenReturn(taskAttemptId);\r\n        when(mockCleanupEvent.getContainerMgrAddress()).thenReturn(cmAddress);\r\n        ut.handle(mockCleanupEvent);\r\n        completeLaunchBarrier.await();\r\n        ut.waitForPoolToIdle();\r\n        ArgumentCaptor<Event> arg = ArgumentCaptor.forClass(Event.class);\r\n        verify(mockEventHandler, atLeast(2)).handle(arg.capture());\r\n        boolean containerCleaned = false;\r\n        for (int i = 0; i < arg.getAllValues().size(); i++) {\r\n            LOG.info(arg.getAllValues().get(i).toString());\r\n            Event currentEvent = arg.getAllValues().get(i);\r\n            if (currentEvent.getType() == TaskAttemptEventType.TA_CONTAINER_CLEANED) {\r\n                containerCleaned = true;\r\n            }\r\n        }\r\n        assert (containerCleaned);\r\n    } finally {\r\n        ut.stop();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\launcher",
  "methodName" : "createNewContainerToken",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "Token createNewContainerToken(ContainerId contId, String containerManagerAddr)\n{\r\n    long currentTime = System.currentTimeMillis();\r\n    return MRApp.newContainerToken(NodeId.newInstance(\"127.0.0.1\", 1234), \"password\".getBytes(), new ContainerTokenIdentifier(contId, containerManagerAddr, \"user\", Resource.newInstance(1024, 1), currentTime + 10000L, 123, currentTime, Priority.newInstance(0), 0));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app",
  "methodName" : "testTaskTimeout",
  "errType" : null,
  "containingMethodsNum" : 17,
  "sourceCodeText" : "void testTaskTimeout() throws InterruptedException\n{\r\n    EventHandler mockHandler = mock(EventHandler.class);\r\n    Clock clock = SystemClock.getInstance();\r\n    TaskHeartbeatHandler hb = new TaskHeartbeatHandler(mockHandler, clock, 1);\r\n    Configuration conf = new Configuration();\r\n    conf.setInt(MRJobConfig.TASK_TIMEOUT, 10);\r\n    conf.setLong(MRJobConfig.TASK_PROGRESS_REPORT_INTERVAL, 5);\r\n    conf.setInt(MRJobConfig.TASK_TIMEOUT_CHECK_INTERVAL_MS, 10);\r\n    conf.setDouble(MRJobConfig.TASK_LOG_PROGRESS_DELTA_THRESHOLD, 0.01);\r\n    hb.init(conf);\r\n    hb.start();\r\n    try {\r\n        ApplicationId appId = ApplicationId.newInstance(0L, 5);\r\n        JobId jobId = MRBuilderUtils.newJobId(appId, 4);\r\n        TaskId tid = MRBuilderUtils.newTaskId(jobId, 3, TaskType.MAP);\r\n        TaskAttemptId taid = MRBuilderUtils.newTaskAttemptId(tid, 2);\r\n        hb.register(taid);\r\n        hb.progressing(taid);\r\n        Thread.sleep(100);\r\n        verify(mockHandler, times(2)).handle(any(Event.class));\r\n    } finally {\r\n        hb.stop();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app",
  "methodName" : "testTaskTimeoutDisable",
  "errType" : null,
  "containingMethodsNum" : 19,
  "sourceCodeText" : "void testTaskTimeoutDisable() throws InterruptedException\n{\r\n    EventHandler mockHandler = mock(EventHandler.class);\r\n    Clock clock = SystemClock.getInstance();\r\n    TaskHeartbeatHandler hb = new TaskHeartbeatHandler(mockHandler, clock, 1);\r\n    Configuration conf = new Configuration();\r\n    conf.setLong(MRJobConfig.TASK_STUCK_TIMEOUT_MS, 0);\r\n    conf.setInt(MRJobConfig.TASK_TIMEOUT, 0);\r\n    conf.setLong(MRJobConfig.TASK_PROGRESS_REPORT_INTERVAL, 0);\r\n    conf.setInt(MRJobConfig.TASK_TIMEOUT_CHECK_INTERVAL_MS, 10);\r\n    hb.init(conf);\r\n    hb.start();\r\n    try {\r\n        ApplicationId appId = ApplicationId.newInstance(0L, 5);\r\n        JobId jobId = MRBuilderUtils.newJobId(appId, 4);\r\n        TaskId tid = MRBuilderUtils.newTaskId(jobId, 3, TaskType.MAP);\r\n        TaskAttemptId taid = MRBuilderUtils.newTaskAttemptId(tid, 2);\r\n        hb.register(taid);\r\n        ConcurrentMap<TaskAttemptId, TaskHeartbeatHandler.ReportTime> runningAttempts = hb.getRunningAttempts();\r\n        for (Map.Entry<TaskAttemptId, TaskHeartbeatHandler.ReportTime> entry : runningAttempts.entrySet()) {\r\n            assertFalse(entry.getValue().isReported());\r\n        }\r\n        Thread.sleep(100);\r\n        verify(mockHandler, never()).handle(any(Event.class));\r\n    } finally {\r\n        hb.stop();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app",
  "methodName" : "testTaskStuck",
  "errType" : null,
  "containingMethodsNum" : 19,
  "sourceCodeText" : "void testTaskStuck() throws InterruptedException\n{\r\n    EventHandler mockHandler = mock(EventHandler.class);\r\n    Clock clock = SystemClock.getInstance();\r\n    TaskHeartbeatHandler hb = new TaskHeartbeatHandler(mockHandler, clock, 1);\r\n    Configuration conf = new Configuration();\r\n    conf.setLong(MRJobConfig.TASK_STUCK_TIMEOUT_MS, 10);\r\n    conf.setInt(MRJobConfig.TASK_TIMEOUT, 1000);\r\n    conf.setLong(MRJobConfig.TASK_PROGRESS_REPORT_INTERVAL, 5);\r\n    conf.setInt(MRJobConfig.TASK_TIMEOUT_CHECK_INTERVAL_MS, 10);\r\n    hb.init(conf);\r\n    hb.start();\r\n    try {\r\n        ApplicationId appId = ApplicationId.newInstance(0L, 5);\r\n        JobId jobId = MRBuilderUtils.newJobId(appId, 4);\r\n        TaskId tid = MRBuilderUtils.newTaskId(jobId, 3, TaskType.MAP);\r\n        TaskAttemptId taid = MRBuilderUtils.newTaskAttemptId(tid, 2);\r\n        hb.register(taid);\r\n        ConcurrentMap<TaskAttemptId, TaskHeartbeatHandler.ReportTime> runningAttempts = hb.getRunningAttempts();\r\n        for (Map.Entry<TaskAttemptId, TaskHeartbeatHandler.ReportTime> entry : runningAttempts.entrySet()) {\r\n            assertFalse(entry.getValue().isReported());\r\n        }\r\n        Thread.sleep(100);\r\n        verify(mockHandler, times(2)).handle(any(Event.class));\r\n    } finally {\r\n        hb.stop();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app",
  "methodName" : "testTaskTimeoutConfigSmallerThanTaskProgressReportInterval",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testTaskTimeoutConfigSmallerThanTaskProgressReportInterval()\n{\r\n    testTaskTimeoutWrtProgressReportInterval(1000L, 5000L);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app",
  "methodName" : "testTaskTimeoutConfigBiggerThanTaskProgressReportInterval",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testTaskTimeoutConfigBiggerThanTaskProgressReportInterval()\n{\r\n    testTaskTimeoutWrtProgressReportInterval(5000L, 1000L);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app",
  "methodName" : "testTaskTimeoutConfigWithoutTaskProgressReportInterval",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void testTaskTimeoutConfigWithoutTaskProgressReportInterval()\n{\r\n    final long taskTimeoutConfiged = 2000L;\r\n    final Configuration conf = new Configuration();\r\n    conf.setLong(MRJobConfig.TASK_TIMEOUT, taskTimeoutConfiged);\r\n    final long expectedTimeout = taskTimeoutConfiged;\r\n    verifyTaskTimeoutConfig(conf, expectedTimeout);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app",
  "methodName" : "testTaskUnregistered",
  "errType" : null,
  "containingMethodsNum" : 19,
  "sourceCodeText" : "void testTaskUnregistered() throws Exception\n{\r\n    EventHandler mockHandler = mock(EventHandler.class);\r\n    ControlledClock clock = new ControlledClock();\r\n    clock.setTime(0);\r\n    final TaskHeartbeatHandler hb = new TaskHeartbeatHandler(mockHandler, clock, 1);\r\n    Configuration conf = new Configuration();\r\n    conf.setInt(MRJobConfig.TASK_TIMEOUT_CHECK_INTERVAL_MS, 1);\r\n    conf.setDouble(MRJobConfig.TASK_LOG_PROGRESS_DELTA_THRESHOLD, 0.01);\r\n    hb.init(conf);\r\n    hb.start();\r\n    try {\r\n        ApplicationId appId = ApplicationId.newInstance(0L, 5);\r\n        JobId jobId = MRBuilderUtils.newJobId(appId, 4);\r\n        TaskId tid = MRBuilderUtils.newTaskId(jobId, 3, TaskType.MAP);\r\n        final TaskAttemptId taid = MRBuilderUtils.newTaskAttemptId(tid, 2);\r\n        Assert.assertFalse(hb.hasRecentlyUnregistered(taid));\r\n        hb.register(taid);\r\n        Assert.assertFalse(hb.hasRecentlyUnregistered(taid));\r\n        hb.unregister(taid);\r\n        Assert.assertTrue(hb.hasRecentlyUnregistered(taid));\r\n        long unregisterTimeout = conf.getLong(MRJobConfig.TASK_EXIT_TIMEOUT, MRJobConfig.TASK_EXIT_TIMEOUT_DEFAULT);\r\n        clock.setTime(unregisterTimeout + 1);\r\n        GenericTestUtils.waitFor(new Supplier<Boolean>() {\r\n\r\n            @Override\r\n            public Boolean get() {\r\n                return !hb.hasRecentlyUnregistered(taid);\r\n            }\r\n        }, 10, 10000);\r\n    } finally {\r\n        hb.stop();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app",
  "methodName" : "testTaskTimeoutWrtProgressReportInterval",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void testTaskTimeoutWrtProgressReportInterval(long timeoutConfig, long taskreportInterval)\n{\r\n    final Configuration conf = new Configuration();\r\n    conf.setLong(MRJobConfig.TASK_TIMEOUT, timeoutConfig);\r\n    conf.setLong(MRJobConfig.TASK_PROGRESS_REPORT_INTERVAL, taskreportInterval);\r\n    final long expectedTimeout = Math.max(timeoutConfig, taskreportInterval * 2);\r\n    verifyTaskTimeoutConfig(conf, expectedTimeout);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app",
  "methodName" : "verifyTaskTimeoutConfig",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void verifyTaskTimeoutConfig(final Configuration conf, final long expectedTimeout)\n{\r\n    final TaskHeartbeatHandler hb = new TaskHeartbeatHandler(null, SystemClock.getInstance(), 1);\r\n    hb.init(conf);\r\n    Assert.assertTrue(\"The value of the task timeout is incorrect.\", hb.getTaskTimeOut() == expectedTimeout);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app",
  "methodName" : "tearDown",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void tearDown()\n{\r\n    conf.setBoolean(MRJobConfig.PRESERVE_FAILED_TASK_FILES, false);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app",
  "methodName" : "testDeletionofStagingOnUnregistrationFailure",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void testDeletionofStagingOnUnregistrationFailure() throws IOException\n{\r\n    testDeletionofStagingOnUnregistrationFailure(2, false);\r\n    testDeletionofStagingOnUnregistrationFailure(1, false);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app",
  "methodName" : "testDeletionofStagingOnUnregistrationFailure",
  "errType" : null,
  "containingMethodsNum" : 18,
  "sourceCodeText" : "void testDeletionofStagingOnUnregistrationFailure(int maxAttempts, boolean shouldHaveDeleted) throws IOException\n{\r\n    conf.set(MRJobConfig.MAPREDUCE_JOB_DIR, stagingJobDir);\r\n    fs = mock(FileSystem.class);\r\n    when(fs.delete(any(Path.class), anyBoolean())).thenReturn(true);\r\n    String user = UserGroupInformation.getCurrentUser().getShortUserName();\r\n    Path stagingDir = MRApps.getStagingAreaDir(conf, user);\r\n    when(fs.exists(stagingDir)).thenReturn(true);\r\n    ApplicationId appId = ApplicationId.newInstance(0, 1);\r\n    ApplicationAttemptId attemptId = ApplicationAttemptId.newInstance(appId, 1);\r\n    JobId jobid = recordFactory.newRecordInstance(JobId.class);\r\n    jobid.setAppId(appId);\r\n    TestMRApp appMaster = new TestMRApp(attemptId, null, JobStateInternal.RUNNING, maxAttempts);\r\n    appMaster.crushUnregistration = true;\r\n    appMaster.init(conf);\r\n    appMaster.start();\r\n    appMaster.shutDownJob();\r\n    ((RunningAppContext) appMaster.getContext()).resetIsLastAMRetry();\r\n    if (shouldHaveDeleted) {\r\n        assertTrue(appMaster.isLastAMRetry());\r\n        verify(fs).delete(stagingJobPath, true);\r\n    } else {\r\n        assertFalse(appMaster.isLastAMRetry());\r\n        verify(fs, never()).delete(stagingJobPath, true);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app",
  "methodName" : "testDeletionofStaging",
  "errType" : null,
  "containingMethodsNum" : 17,
  "sourceCodeText" : "void testDeletionofStaging() throws IOException\n{\r\n    conf.set(MRJobConfig.MAPREDUCE_JOB_DIR, stagingJobDir);\r\n    fs = mock(FileSystem.class);\r\n    when(fs.delete(any(Path.class), anyBoolean())).thenReturn(true);\r\n    String user = UserGroupInformation.getCurrentUser().getShortUserName();\r\n    Path stagingDir = MRApps.getStagingAreaDir(conf, user);\r\n    when(fs.exists(stagingDir)).thenReturn(true);\r\n    ApplicationId appId = ApplicationId.newInstance(System.currentTimeMillis(), 0);\r\n    ApplicationAttemptId attemptId = ApplicationAttemptId.newInstance(appId, 1);\r\n    JobId jobid = recordFactory.newRecordInstance(JobId.class);\r\n    jobid.setAppId(appId);\r\n    ContainerAllocator mockAlloc = mock(ContainerAllocator.class);\r\n    Assert.assertTrue(MRJobConfig.DEFAULT_MR_AM_MAX_ATTEMPTS > 1);\r\n    MRAppMaster appMaster = new TestMRApp(attemptId, mockAlloc, JobStateInternal.RUNNING, MRJobConfig.DEFAULT_MR_AM_MAX_ATTEMPTS);\r\n    appMaster.init(conf);\r\n    appMaster.start();\r\n    appMaster.shutDownJob();\r\n    assertTrue(((TestMRApp) appMaster).getTestIsLastAMRetry());\r\n    verify(fs).delete(stagingJobPath, true);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app",
  "methodName" : "testNoDeletionofStagingOnReboot",
  "errType" : null,
  "containingMethodsNum" : 15,
  "sourceCodeText" : "void testNoDeletionofStagingOnReboot() throws IOException\n{\r\n    conf.set(MRJobConfig.MAPREDUCE_JOB_DIR, stagingJobDir);\r\n    fs = mock(FileSystem.class);\r\n    when(fs.delete(any(Path.class), anyBoolean())).thenReturn(true);\r\n    String user = UserGroupInformation.getCurrentUser().getShortUserName();\r\n    Path stagingDir = MRApps.getStagingAreaDir(conf, user);\r\n    when(fs.exists(stagingDir)).thenReturn(true);\r\n    ApplicationId appId = ApplicationId.newInstance(System.currentTimeMillis(), 0);\r\n    ApplicationAttemptId attemptId = ApplicationAttemptId.newInstance(appId, 1);\r\n    ContainerAllocator mockAlloc = mock(ContainerAllocator.class);\r\n    Assert.assertTrue(MRJobConfig.DEFAULT_MR_AM_MAX_ATTEMPTS > 1);\r\n    MRAppMaster appMaster = new TestMRApp(attemptId, mockAlloc, JobStateInternal.REBOOT, MRJobConfig.DEFAULT_MR_AM_MAX_ATTEMPTS);\r\n    appMaster.init(conf);\r\n    appMaster.start();\r\n    appMaster.shutDownJob();\r\n    assertFalse(((TestMRApp) appMaster).getTestIsLastAMRetry());\r\n    verify(fs, times(0)).delete(stagingJobPath, true);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app",
  "methodName" : "testDeletionofStagingOnReboot",
  "errType" : null,
  "containingMethodsNum" : 14,
  "sourceCodeText" : "void testDeletionofStagingOnReboot() throws IOException\n{\r\n    conf.set(MRJobConfig.MAPREDUCE_JOB_DIR, stagingJobDir);\r\n    fs = mock(FileSystem.class);\r\n    when(fs.delete(any(Path.class), anyBoolean())).thenReturn(true);\r\n    String user = UserGroupInformation.getCurrentUser().getShortUserName();\r\n    Path stagingDir = MRApps.getStagingAreaDir(conf, user);\r\n    when(fs.exists(stagingDir)).thenReturn(true);\r\n    ApplicationId appId = ApplicationId.newInstance(System.currentTimeMillis(), 0);\r\n    ApplicationAttemptId attemptId = ApplicationAttemptId.newInstance(appId, 1);\r\n    ContainerAllocator mockAlloc = mock(ContainerAllocator.class);\r\n    MRAppMaster appMaster = new TestMRApp(attemptId, mockAlloc, JobStateInternal.REBOOT, 1);\r\n    appMaster.init(conf);\r\n    appMaster.start();\r\n    appMaster.shutDownJob();\r\n    assertTrue(((TestMRApp) appMaster).getTestIsLastAMRetry());\r\n    verify(fs).delete(stagingJobPath, true);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app",
  "methodName" : "testDeletionofStagingOnKill",
  "errType" : null,
  "containingMethodsNum" : 14,
  "sourceCodeText" : "void testDeletionofStagingOnKill() throws IOException\n{\r\n    conf.set(MRJobConfig.MAPREDUCE_JOB_DIR, stagingJobDir);\r\n    fs = mock(FileSystem.class);\r\n    when(fs.delete(any(Path.class), anyBoolean())).thenReturn(true);\r\n    String user = UserGroupInformation.getCurrentUser().getShortUserName();\r\n    Path stagingDir = MRApps.getStagingAreaDir(conf, user);\r\n    when(fs.exists(stagingDir)).thenReturn(true);\r\n    ApplicationId appId = ApplicationId.newInstance(System.currentTimeMillis(), 0);\r\n    ApplicationAttemptId attemptId = ApplicationAttemptId.newInstance(appId, 0);\r\n    JobId jobid = recordFactory.newRecordInstance(JobId.class);\r\n    jobid.setAppId(appId);\r\n    ContainerAllocator mockAlloc = mock(ContainerAllocator.class);\r\n    MRAppMaster appMaster = new TestMRApp(attemptId, mockAlloc);\r\n    appMaster.init(conf);\r\n    MRAppMaster.MRAppMasterShutdownHook hook = new MRAppMaster.MRAppMasterShutdownHook(appMaster);\r\n    hook.run();\r\n    verify(fs, times(0)).delete(stagingJobPath, true);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app",
  "methodName" : "testDeletionofStagingOnKillLastTry",
  "errType" : null,
  "containingMethodsNum" : 16,
  "sourceCodeText" : "void testDeletionofStagingOnKillLastTry() throws IOException\n{\r\n    conf.set(MRJobConfig.MAPREDUCE_JOB_DIR, stagingJobDir);\r\n    fs = mock(FileSystem.class);\r\n    when(fs.delete(any(Path.class), anyBoolean())).thenReturn(true);\r\n    String user = UserGroupInformation.getCurrentUser().getShortUserName();\r\n    Path stagingDir = MRApps.getStagingAreaDir(conf, user);\r\n    when(fs.exists(stagingDir)).thenReturn(true);\r\n    ApplicationId appId = ApplicationId.newInstance(System.currentTimeMillis(), 0);\r\n    ApplicationAttemptId attemptId = ApplicationAttemptId.newInstance(appId, 1);\r\n    JobId jobid = recordFactory.newRecordInstance(JobId.class);\r\n    jobid.setAppId(appId);\r\n    ContainerAllocator mockAlloc = mock(ContainerAllocator.class);\r\n    MRAppMaster appMaster = new TestMRApp(attemptId, mockAlloc);\r\n    appMaster.init(conf);\r\n    assertTrue(\"appMaster.isLastAMRetry() is false\", appMaster.isLastAMRetry());\r\n    MRAppMaster.MRAppMasterShutdownHook hook = new MRAppMaster.MRAppMasterShutdownHook(appMaster);\r\n    hook.run();\r\n    assertTrue(\"MRAppMaster isn't stopped\", appMaster.isInState(Service.STATE.STOPPED));\r\n    verify(fs).delete(stagingJobPath, true);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app",
  "methodName" : "testByPreserveFailedStaging",
  "errType" : null,
  "containingMethodsNum" : 18,
  "sourceCodeText" : "void testByPreserveFailedStaging() throws IOException\n{\r\n    conf.set(MRJobConfig.MAPREDUCE_JOB_DIR, stagingJobDir);\r\n    conf.setBoolean(MRJobConfig.PRESERVE_FAILED_TASK_FILES, true);\r\n    fs = mock(FileSystem.class);\r\n    when(fs.delete(any(Path.class), anyBoolean())).thenReturn(true);\r\n    String user = UserGroupInformation.getCurrentUser().getShortUserName();\r\n    Path stagingDir = MRApps.getStagingAreaDir(conf, user);\r\n    when(fs.exists(stagingDir)).thenReturn(true);\r\n    ApplicationId appId = ApplicationId.newInstance(System.currentTimeMillis(), 0);\r\n    ApplicationAttemptId attemptId = ApplicationAttemptId.newInstance(appId, 1);\r\n    JobId jobid = recordFactory.newRecordInstance(JobId.class);\r\n    jobid.setAppId(appId);\r\n    ContainerAllocator mockAlloc = mock(ContainerAllocator.class);\r\n    Assert.assertTrue(MRJobConfig.DEFAULT_MR_AM_MAX_ATTEMPTS > 1);\r\n    MRAppMaster appMaster = new TestMRApp(attemptId, mockAlloc, JobStateInternal.FAILED, MRJobConfig.DEFAULT_MR_AM_MAX_ATTEMPTS);\r\n    appMaster.init(conf);\r\n    appMaster.start();\r\n    appMaster.shutDownJob();\r\n    assertTrue(((TestMRApp) appMaster).getTestIsLastAMRetry());\r\n    verify(fs, times(0)).delete(stagingJobPath, true);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app",
  "methodName" : "testPreservePatternMatchedStaging",
  "errType" : null,
  "containingMethodsNum" : 18,
  "sourceCodeText" : "void testPreservePatternMatchedStaging() throws IOException\n{\r\n    conf.set(MRJobConfig.MAPREDUCE_JOB_DIR, stagingJobDir);\r\n    conf.set(MRJobConfig.PRESERVE_FILES_PATTERN, \"JobDir\");\r\n    fs = mock(FileSystem.class);\r\n    when(fs.delete(any(Path.class), anyBoolean())).thenReturn(true);\r\n    String user = UserGroupInformation.getCurrentUser().getShortUserName();\r\n    Path stagingDir = MRApps.getStagingAreaDir(conf, user);\r\n    when(fs.exists(stagingDir)).thenReturn(true);\r\n    ApplicationId appId = ApplicationId.newInstance(System.currentTimeMillis(), 0);\r\n    ApplicationAttemptId attemptId = ApplicationAttemptId.newInstance(appId, 1);\r\n    JobId jobid = recordFactory.newRecordInstance(JobId.class);\r\n    jobid.setAppId(appId);\r\n    ContainerAllocator mockAlloc = mock(ContainerAllocator.class);\r\n    Assert.assertTrue(MRJobConfig.DEFAULT_MR_AM_MAX_ATTEMPTS > 1);\r\n    MRAppMaster appMaster = new TestMRApp(attemptId, mockAlloc, JobStateInternal.RUNNING, MRJobConfig.DEFAULT_MR_AM_MAX_ATTEMPTS);\r\n    appMaster.init(conf);\r\n    appMaster.start();\r\n    appMaster.shutDownJob();\r\n    assertTrue(((TestMRApp) appMaster).getTestIsLastAMRetry());\r\n    verify(fs, times(0)).delete(stagingJobPath, true);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app",
  "methodName" : "testNotPreserveNotPatternMatchedStaging",
  "errType" : null,
  "containingMethodsNum" : 18,
  "sourceCodeText" : "void testNotPreserveNotPatternMatchedStaging() throws IOException\n{\r\n    conf.set(MRJobConfig.MAPREDUCE_JOB_DIR, stagingJobDir);\r\n    conf.set(MRJobConfig.PRESERVE_FILES_PATTERN, \"NotMatching\");\r\n    fs = mock(FileSystem.class);\r\n    when(fs.delete(any(Path.class), anyBoolean())).thenReturn(true);\r\n    String user = UserGroupInformation.getCurrentUser().getShortUserName();\r\n    Path stagingDir = MRApps.getStagingAreaDir(conf, user);\r\n    when(fs.exists(stagingDir)).thenReturn(true);\r\n    ApplicationId appId = ApplicationId.newInstance(System.currentTimeMillis(), 0);\r\n    ApplicationAttemptId attemptId = ApplicationAttemptId.newInstance(appId, 1);\r\n    JobId jobid = recordFactory.newRecordInstance(JobId.class);\r\n    jobid.setAppId(appId);\r\n    ContainerAllocator mockAlloc = mock(ContainerAllocator.class);\r\n    Assert.assertTrue(MRJobConfig.DEFAULT_MR_AM_MAX_ATTEMPTS > 1);\r\n    MRAppMaster appMaster = new TestMRApp(attemptId, mockAlloc, JobStateInternal.RUNNING, MRJobConfig.DEFAULT_MR_AM_MAX_ATTEMPTS);\r\n    appMaster.init(conf);\r\n    appMaster.start();\r\n    appMaster.shutDownJob();\r\n    assertTrue(((TestMRApp) appMaster).getTestIsLastAMRetry());\r\n    verify(fs, times(1)).delete(stagingJobPath, true);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app",
  "methodName" : "testPreservePatternMatchedAndFailedStaging",
  "errType" : null,
  "containingMethodsNum" : 19,
  "sourceCodeText" : "void testPreservePatternMatchedAndFailedStaging() throws IOException\n{\r\n    conf.set(MRJobConfig.MAPREDUCE_JOB_DIR, stagingJobDir);\r\n    conf.set(MRJobConfig.PRESERVE_FILES_PATTERN, \"JobDir\");\r\n    conf.setBoolean(MRJobConfig.PRESERVE_FAILED_TASK_FILES, true);\r\n    fs = mock(FileSystem.class);\r\n    when(fs.delete(any(Path.class), anyBoolean())).thenReturn(true);\r\n    String user = UserGroupInformation.getCurrentUser().getShortUserName();\r\n    Path stagingDir = MRApps.getStagingAreaDir(conf, user);\r\n    when(fs.exists(stagingDir)).thenReturn(true);\r\n    ApplicationId appId = ApplicationId.newInstance(System.currentTimeMillis(), 0);\r\n    ApplicationAttemptId attemptId = ApplicationAttemptId.newInstance(appId, 1);\r\n    JobId jobid = recordFactory.newRecordInstance(JobId.class);\r\n    jobid.setAppId(appId);\r\n    ContainerAllocator mockAlloc = mock(ContainerAllocator.class);\r\n    Assert.assertTrue(MRJobConfig.DEFAULT_MR_AM_MAX_ATTEMPTS > 1);\r\n    MRAppMaster appMaster = new TestMRApp(attemptId, mockAlloc, JobStateInternal.RUNNING, MRJobConfig.DEFAULT_MR_AM_MAX_ATTEMPTS);\r\n    appMaster.init(conf);\r\n    appMaster.start();\r\n    appMaster.shutDownJob();\r\n    assertTrue(((TestMRApp) appMaster).getTestIsLastAMRetry());\r\n    verify(fs, times(0)).delete(stagingJobPath, true);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app",
  "methodName" : "getStubbedHeartbeatHandler",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "RMHeartbeatHandler getStubbedHeartbeatHandler(final AppContext appContext)\n{\r\n    return new RMHeartbeatHandler() {\r\n\r\n        @Override\r\n        public long getLastHeartbeatTime() {\r\n            return appContext.getClock().getTime();\r\n        }\r\n\r\n        @Override\r\n        public void runOnNextHeartbeat(Runnable callback) {\r\n            callback.run();\r\n        }\r\n    };\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app",
  "methodName" : "testStagingCleanupOrder",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void testStagingCleanupOrder() throws Exception\n{\r\n    MRAppTestCleanup app = new MRAppTestCleanup(1, 1, true, this.getClass().getName(), true);\r\n    JobImpl job = (JobImpl) app.submit(new Configuration());\r\n    app.waitForState(job, JobState.SUCCEEDED);\r\n    app.verifyCompleted();\r\n    int waitTime = 20 * 1000;\r\n    while (waitTime > 0 && app.numStops < 2) {\r\n        Thread.sleep(100);\r\n        waitTime -= 100;\r\n    }\r\n    Assert.assertEquals(1, app.ContainerAllocatorStopped);\r\n    Assert.assertEquals(2, app.stagingDirCleanedup);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app",
  "methodName" : "testFetchFailure",
  "errType" : null,
  "containingMethodsNum" : 52,
  "sourceCodeText" : "void testFetchFailure() throws Exception\n{\r\n    MRApp app = new MRApp(1, 1, false, this.getClass().getName(), true);\r\n    Configuration conf = new Configuration();\r\n    conf.setBoolean(MRJobConfig.JOB_UBERTASK_ENABLE, false);\r\n    Job job = app.submit(conf);\r\n    app.waitForState(job, JobState.RUNNING);\r\n    Assert.assertEquals(\"Num tasks not correct\", 2, job.getTasks().size());\r\n    Iterator<Task> it = job.getTasks().values().iterator();\r\n    Task mapTask = it.next();\r\n    Task reduceTask = it.next();\r\n    app.waitForState(mapTask, TaskState.RUNNING);\r\n    TaskAttempt mapAttempt1 = mapTask.getAttempts().values().iterator().next();\r\n    app.waitForState(mapAttempt1, TaskAttemptState.RUNNING);\r\n    app.getContext().getEventHandler().handle(new TaskAttemptEvent(mapAttempt1.getID(), TaskAttemptEventType.TA_DONE));\r\n    app.waitForState(mapTask, TaskState.SUCCEEDED);\r\n    final int checkIntervalMillis = 10;\r\n    final int waitForMillis = 800;\r\n    GenericTestUtils.waitFor(new Supplier<Boolean>() {\r\n\r\n        @Override\r\n        public Boolean get() {\r\n            TaskAttemptCompletionEvent[] events = job.getTaskAttemptCompletionEvents(0, 100);\r\n            return events.length >= 1;\r\n        }\r\n    }, checkIntervalMillis, waitForMillis);\r\n    TaskAttemptCompletionEvent[] events = job.getTaskAttemptCompletionEvents(0, 100);\r\n    Assert.assertEquals(\"Num completion events not correct\", 1, events.length);\r\n    Assert.assertEquals(\"Event status not correct\", TaskAttemptCompletionEventStatus.SUCCEEDED, events[0].getStatus());\r\n    app.waitForState(reduceTask, TaskState.RUNNING);\r\n    TaskAttempt reduceAttempt = reduceTask.getAttempts().values().iterator().next();\r\n    app.waitForState(reduceAttempt, TaskAttemptState.RUNNING);\r\n    sendFetchFailure(app, reduceAttempt, mapAttempt1, \"host\");\r\n    sendFetchFailure(app, reduceAttempt, mapAttempt1, \"host\");\r\n    sendFetchFailure(app, reduceAttempt, mapAttempt1, \"host\");\r\n    app.waitForState(mapTask, TaskState.RUNNING);\r\n    Assert.assertEquals(\"Map TaskAttempt state not correct\", TaskAttemptState.FAILED, mapAttempt1.getState());\r\n    Assert.assertEquals(\"Num attempts in Map Task not correct\", 2, mapTask.getAttempts().size());\r\n    Iterator<TaskAttempt> atIt = mapTask.getAttempts().values().iterator();\r\n    atIt.next();\r\n    TaskAttempt mapAttempt2 = atIt.next();\r\n    app.waitForState(mapAttempt2, TaskAttemptState.RUNNING);\r\n    app.getContext().getEventHandler().handle(new TaskAttemptEvent(mapAttempt2.getID(), TaskAttemptEventType.TA_DONE));\r\n    app.waitForState(mapTask, TaskState.SUCCEEDED);\r\n    app.getContext().getEventHandler().handle(new TaskAttemptEvent(reduceAttempt.getID(), TaskAttemptEventType.TA_DONE));\r\n    app.waitForState(job, JobState.SUCCEEDED);\r\n    Assert.assertEquals(\"Event status not correct\", TaskAttemptCompletionEventStatus.OBSOLETE, events[0].getStatus());\r\n    events = job.getTaskAttemptCompletionEvents(0, 100);\r\n    Assert.assertEquals(\"Num completion events not correct\", 4, events.length);\r\n    Assert.assertEquals(\"Event map attempt id not correct\", mapAttempt1.getID(), events[0].getAttemptId());\r\n    Assert.assertEquals(\"Event map attempt id not correct\", mapAttempt1.getID(), events[1].getAttemptId());\r\n    Assert.assertEquals(\"Event map attempt id not correct\", mapAttempt2.getID(), events[2].getAttemptId());\r\n    Assert.assertEquals(\"Event redude attempt id not correct\", reduceAttempt.getID(), events[3].getAttemptId());\r\n    Assert.assertEquals(\"Event status not correct for map attempt1\", TaskAttemptCompletionEventStatus.OBSOLETE, events[0].getStatus());\r\n    Assert.assertEquals(\"Event status not correct for map attempt1\", TaskAttemptCompletionEventStatus.FAILED, events[1].getStatus());\r\n    Assert.assertEquals(\"Event status not correct for map attempt2\", TaskAttemptCompletionEventStatus.SUCCEEDED, events[2].getStatus());\r\n    Assert.assertEquals(\"Event status not correct for reduce attempt1\", TaskAttemptCompletionEventStatus.SUCCEEDED, events[3].getStatus());\r\n    TaskCompletionEvent[] mapEvents = job.getMapAttemptCompletionEvents(0, 2);\r\n    TaskCompletionEvent[] convertedEvents = TypeConverter.fromYarn(events);\r\n    Assert.assertEquals(\"Incorrect number of map events\", 2, mapEvents.length);\r\n    Assert.assertArrayEquals(\"Unexpected map events\", Arrays.copyOfRange(convertedEvents, 0, 2), mapEvents);\r\n    mapEvents = job.getMapAttemptCompletionEvents(2, 200);\r\n    Assert.assertEquals(\"Incorrect number of map events\", 1, mapEvents.length);\r\n    Assert.assertEquals(\"Unexpected map event\", convertedEvents[2], mapEvents[0]);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app",
  "methodName" : "testFetchFailureWithRecovery",
  "errType" : null,
  "containingMethodsNum" : 43,
  "sourceCodeText" : "void testFetchFailureWithRecovery() throws Exception\n{\r\n    int runCount = 0;\r\n    MRApp app = new MRAppWithHistory(1, 1, false, this.getClass().getName(), true, ++runCount);\r\n    Configuration conf = new Configuration();\r\n    conf.setBoolean(MRJobConfig.JOB_UBERTASK_ENABLE, false);\r\n    Job job = app.submit(conf);\r\n    app.waitForState(job, JobState.RUNNING);\r\n    Assert.assertEquals(\"Num tasks not correct\", 2, job.getTasks().size());\r\n    Iterator<Task> it = job.getTasks().values().iterator();\r\n    Task mapTask = it.next();\r\n    Task reduceTask = it.next();\r\n    app.waitForState(mapTask, TaskState.RUNNING);\r\n    TaskAttempt mapAttempt1 = mapTask.getAttempts().values().iterator().next();\r\n    app.waitForState(mapAttempt1, TaskAttemptState.RUNNING);\r\n    app.getContext().getEventHandler().handle(new TaskAttemptEvent(mapAttempt1.getID(), TaskAttemptEventType.TA_DONE));\r\n    app.waitForState(mapTask, TaskState.SUCCEEDED);\r\n    TaskAttemptCompletionEvent[] events = job.getTaskAttemptCompletionEvents(0, 100);\r\n    Assert.assertEquals(\"Num completion events not correct\", 1, events.length);\r\n    Assert.assertEquals(\"Event status not correct\", TaskAttemptCompletionEventStatus.SUCCEEDED, events[0].getStatus());\r\n    app.waitForState(reduceTask, TaskState.RUNNING);\r\n    TaskAttempt reduceAttempt = reduceTask.getAttempts().values().iterator().next();\r\n    app.waitForState(reduceAttempt, TaskAttemptState.RUNNING);\r\n    sendFetchFailure(app, reduceAttempt, mapAttempt1, \"host\");\r\n    sendFetchFailure(app, reduceAttempt, mapAttempt1, \"host\");\r\n    sendFetchFailure(app, reduceAttempt, mapAttempt1, \"host\");\r\n    app.waitForState(mapTask, TaskState.RUNNING);\r\n    app.stop();\r\n    app = new MRAppWithHistory(1, 1, false, this.getClass().getName(), false, ++runCount);\r\n    conf = new Configuration();\r\n    conf.setBoolean(MRJobConfig.MR_AM_JOB_RECOVERY_ENABLE, true);\r\n    conf.setBoolean(MRJobConfig.JOB_UBERTASK_ENABLE, false);\r\n    job = app.submit(conf);\r\n    app.waitForState(job, JobState.RUNNING);\r\n    Assert.assertEquals(\"Num tasks not correct\", 2, job.getTasks().size());\r\n    it = job.getTasks().values().iterator();\r\n    mapTask = it.next();\r\n    reduceTask = it.next();\r\n    app.waitForState(mapTask, TaskState.RUNNING);\r\n    mapAttempt1 = mapTask.getAttempts().values().iterator().next();\r\n    app.waitForState(mapAttempt1, TaskAttemptState.RUNNING);\r\n    app.getContext().getEventHandler().handle(new TaskAttemptEvent(mapAttempt1.getID(), TaskAttemptEventType.TA_DONE));\r\n    app.waitForState(mapTask, TaskState.SUCCEEDED);\r\n    reduceAttempt = reduceTask.getAttempts().values().iterator().next();\r\n    app.getContext().getEventHandler().handle(new TaskAttemptEvent(reduceAttempt.getID(), TaskAttemptEventType.TA_DONE));\r\n    app.waitForState(job, JobState.SUCCEEDED);\r\n    events = job.getTaskAttemptCompletionEvents(0, 100);\r\n    Assert.assertEquals(\"Num completion events not correct\", 2, events.length);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app",
  "methodName" : "testFetchFailureMultipleReduces",
  "errType" : null,
  "containingMethodsNum" : 68,
  "sourceCodeText" : "void testFetchFailureMultipleReduces() throws Exception\n{\r\n    MRApp app = new MRApp(1, 3, false, this.getClass().getName(), true);\r\n    Configuration conf = new Configuration();\r\n    conf.setBoolean(MRJobConfig.JOB_UBERTASK_ENABLE, false);\r\n    Job job = app.submit(conf);\r\n    app.waitForState(job, JobState.RUNNING);\r\n    Assert.assertEquals(\"Num tasks not correct\", 4, job.getTasks().size());\r\n    Iterator<Task> it = job.getTasks().values().iterator();\r\n    Task mapTask = it.next();\r\n    Task reduceTask = it.next();\r\n    Task reduceTask2 = it.next();\r\n    Task reduceTask3 = it.next();\r\n    app.waitForState(mapTask, TaskState.RUNNING);\r\n    TaskAttempt mapAttempt1 = mapTask.getAttempts().values().iterator().next();\r\n    app.waitForState(mapAttempt1, TaskAttemptState.RUNNING);\r\n    app.getContext().getEventHandler().handle(new TaskAttemptEvent(mapAttempt1.getID(), TaskAttemptEventType.TA_DONE));\r\n    app.waitForState(mapTask, TaskState.SUCCEEDED);\r\n    TaskAttemptCompletionEvent[] events = job.getTaskAttemptCompletionEvents(0, 100);\r\n    Assert.assertEquals(\"Num completion events not correct\", 1, events.length);\r\n    Assert.assertEquals(\"Event status not correct\", TaskAttemptCompletionEventStatus.SUCCEEDED, events[0].getStatus());\r\n    app.waitForState(reduceTask, TaskState.RUNNING);\r\n    app.waitForState(reduceTask2, TaskState.RUNNING);\r\n    app.waitForState(reduceTask3, TaskState.RUNNING);\r\n    TaskAttempt reduceAttempt = reduceTask.getAttempts().values().iterator().next();\r\n    app.waitForState(reduceAttempt, TaskAttemptState.RUNNING);\r\n    updateStatus(app, reduceAttempt, Phase.SHUFFLE);\r\n    TaskAttempt reduceAttempt2 = reduceTask2.getAttempts().values().iterator().next();\r\n    app.waitForState(reduceAttempt2, TaskAttemptState.RUNNING);\r\n    updateStatus(app, reduceAttempt2, Phase.SHUFFLE);\r\n    TaskAttempt reduceAttempt3 = reduceTask3.getAttempts().values().iterator().next();\r\n    app.waitForState(reduceAttempt3, TaskAttemptState.RUNNING);\r\n    updateStatus(app, reduceAttempt3, Phase.SHUFFLE);\r\n    sendFetchFailure(app, reduceAttempt, mapAttempt1, \"host1\");\r\n    sendFetchFailure(app, reduceAttempt2, mapAttempt1, \"host2\");\r\n    assertEquals(TaskState.SUCCEEDED, mapTask.getState());\r\n    updateStatus(app, reduceAttempt2, Phase.REDUCE);\r\n    updateStatus(app, reduceAttempt3, Phase.REDUCE);\r\n    sendFetchFailure(app, reduceAttempt3, mapAttempt1, \"host3\");\r\n    app.waitForState(mapTask, TaskState.RUNNING);\r\n    Assert.assertEquals(\"Map TaskAttempt state not correct\", TaskAttemptState.FAILED, mapAttempt1.getState());\r\n    assertThat(mapAttempt1.getDiagnostics().get(0)).isEqualTo(\"Too many fetch failures. Failing the attempt. \" + \"Last failure reported by \" + reduceAttempt3.getID().toString() + \" from host host3\");\r\n    Assert.assertEquals(\"Num attempts in Map Task not correct\", 2, mapTask.getAttempts().size());\r\n    Iterator<TaskAttempt> atIt = mapTask.getAttempts().values().iterator();\r\n    atIt.next();\r\n    TaskAttempt mapAttempt2 = atIt.next();\r\n    app.waitForState(mapAttempt2, TaskAttemptState.RUNNING);\r\n    app.getContext().getEventHandler().handle(new TaskAttemptEvent(mapAttempt2.getID(), TaskAttemptEventType.TA_DONE));\r\n    app.waitForState(mapTask, TaskState.SUCCEEDED);\r\n    app.getContext().getEventHandler().handle(new TaskAttemptEvent(reduceAttempt.getID(), TaskAttemptEventType.TA_DONE));\r\n    app.getContext().getEventHandler().handle(new TaskAttemptEvent(reduceAttempt2.getID(), TaskAttemptEventType.TA_DONE));\r\n    app.getContext().getEventHandler().handle(new TaskAttemptEvent(reduceAttempt3.getID(), TaskAttemptEventType.TA_DONE));\r\n    app.waitForState(job, JobState.SUCCEEDED);\r\n    Assert.assertEquals(\"Event status not correct\", TaskAttemptCompletionEventStatus.OBSOLETE, events[0].getStatus());\r\n    events = job.getTaskAttemptCompletionEvents(0, 100);\r\n    Assert.assertEquals(\"Num completion events not correct\", 6, events.length);\r\n    Assert.assertEquals(\"Event map attempt id not correct\", mapAttempt1.getID(), events[0].getAttemptId());\r\n    Assert.assertEquals(\"Event map attempt id not correct\", mapAttempt1.getID(), events[1].getAttemptId());\r\n    Assert.assertEquals(\"Event map attempt id not correct\", mapAttempt2.getID(), events[2].getAttemptId());\r\n    Assert.assertEquals(\"Event reduce attempt id not correct\", reduceAttempt.getID(), events[3].getAttemptId());\r\n    Assert.assertEquals(\"Event status not correct for map attempt1\", TaskAttemptCompletionEventStatus.OBSOLETE, events[0].getStatus());\r\n    Assert.assertEquals(\"Event status not correct for map attempt1\", TaskAttemptCompletionEventStatus.FAILED, events[1].getStatus());\r\n    Assert.assertEquals(\"Event status not correct for map attempt2\", TaskAttemptCompletionEventStatus.SUCCEEDED, events[2].getStatus());\r\n    Assert.assertEquals(\"Event status not correct for reduce attempt1\", TaskAttemptCompletionEventStatus.SUCCEEDED, events[3].getStatus());\r\n    TaskCompletionEvent[] mapEvents = job.getMapAttemptCompletionEvents(0, 2);\r\n    TaskCompletionEvent[] convertedEvents = TypeConverter.fromYarn(events);\r\n    Assert.assertEquals(\"Incorrect number of map events\", 2, mapEvents.length);\r\n    Assert.assertArrayEquals(\"Unexpected map events\", Arrays.copyOfRange(convertedEvents, 0, 2), mapEvents);\r\n    mapEvents = job.getMapAttemptCompletionEvents(2, 200);\r\n    Assert.assertEquals(\"Incorrect number of map events\", 1, mapEvents.length);\r\n    Assert.assertEquals(\"Unexpected map event\", convertedEvents[2], mapEvents[0]);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app",
  "methodName" : "updateStatus",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void updateStatus(MRApp app, TaskAttempt attempt, Phase phase)\n{\r\n    TaskAttemptStatusUpdateEvent.TaskAttemptStatus status = new TaskAttemptStatusUpdateEvent.TaskAttemptStatus();\r\n    status.counters = new Counters();\r\n    status.fetchFailedMaps = new ArrayList<TaskAttemptId>();\r\n    status.id = attempt.getID();\r\n    status.mapFinishTime = 0;\r\n    status.phase = phase;\r\n    status.progress = 0.5f;\r\n    status.shuffleFinishTime = 0;\r\n    status.sortFinishTime = 0;\r\n    status.stateString = \"OK\";\r\n    status.taskState = attempt.getState();\r\n    TaskAttemptStatusUpdateEvent event = new TaskAttemptStatusUpdateEvent(attempt.getID(), new AtomicReference<>(status));\r\n    app.getContext().getEventHandler().handle(event);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app",
  "methodName" : "sendFetchFailure",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void sendFetchFailure(MRApp app, TaskAttempt reduceAttempt, TaskAttempt mapAttempt, String hostname)\n{\r\n    app.getContext().getEventHandler().handle(new JobTaskAttemptFetchFailureEvent(reduceAttempt.getID(), Arrays.asList(new TaskAttemptId[] { mapAttempt.getID() }), hostname));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\metrics",
  "methodName" : "tearDown",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void tearDown()\n{\r\n    DefaultMetricsSystem.shutdown();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\metrics",
  "methodName" : "testNames",
  "errType" : null,
  "containingMethodsNum" : 49,
  "sourceCodeText" : "void testNames()\n{\r\n    Job job = mock(Job.class);\r\n    Task mapTask = mock(Task.class);\r\n    when(mapTask.getType()).thenReturn(TaskType.MAP);\r\n    Task reduceTask = mock(Task.class);\r\n    when(reduceTask.getType()).thenReturn(TaskType.REDUCE);\r\n    MRAppMetrics metrics = MRAppMetrics.create();\r\n    metrics.submittedJob(job);\r\n    metrics.waitingTask(mapTask);\r\n    metrics.waitingTask(reduceTask);\r\n    metrics.preparingJob(job);\r\n    metrics.submittedJob(job);\r\n    metrics.waitingTask(mapTask);\r\n    metrics.waitingTask(reduceTask);\r\n    metrics.preparingJob(job);\r\n    metrics.submittedJob(job);\r\n    metrics.waitingTask(mapTask);\r\n    metrics.waitingTask(reduceTask);\r\n    metrics.preparingJob(job);\r\n    metrics.endPreparingJob(job);\r\n    metrics.endPreparingJob(job);\r\n    metrics.endPreparingJob(job);\r\n    metrics.runningJob(job);\r\n    metrics.launchedTask(mapTask);\r\n    metrics.runningTask(mapTask);\r\n    metrics.failedTask(mapTask);\r\n    metrics.endWaitingTask(reduceTask);\r\n    metrics.endRunningTask(mapTask);\r\n    metrics.endRunningJob(job);\r\n    metrics.failedJob(job);\r\n    metrics.runningJob(job);\r\n    metrics.launchedTask(mapTask);\r\n    metrics.runningTask(mapTask);\r\n    metrics.killedTask(mapTask);\r\n    metrics.endWaitingTask(reduceTask);\r\n    metrics.endRunningTask(mapTask);\r\n    metrics.endRunningJob(job);\r\n    metrics.killedJob(job);\r\n    metrics.runningJob(job);\r\n    metrics.launchedTask(mapTask);\r\n    metrics.runningTask(mapTask);\r\n    metrics.completedTask(mapTask);\r\n    metrics.endRunningTask(mapTask);\r\n    metrics.launchedTask(reduceTask);\r\n    metrics.runningTask(reduceTask);\r\n    metrics.completedTask(reduceTask);\r\n    metrics.endRunningTask(reduceTask);\r\n    metrics.endRunningJob(job);\r\n    metrics.completedJob(job);\r\n    checkMetrics(3, 1, 1, 1, 0, 0, 3, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\metrics",
  "methodName" : "checkMetrics",
  "errType" : null,
  "containingMethodsNum" : 19,
  "sourceCodeText" : "void checkMetrics(int jobsSubmitted, int jobsCompleted, int jobsFailed, int jobsKilled, int jobsPreparing, int jobsRunning, int mapsLaunched, int mapsCompleted, int mapsFailed, int mapsKilled, int mapsRunning, int mapsWaiting, int reducesLaunched, int reducesCompleted, int reducesFailed, int reducesKilled, int reducesRunning, int reducesWaiting)\n{\r\n    MetricsRecordBuilder rb = getMetrics(\"MRAppMetrics\");\r\n    assertCounter(\"JobsSubmitted\", jobsSubmitted, rb);\r\n    assertCounter(\"JobsCompleted\", jobsCompleted, rb);\r\n    assertCounter(\"JobsFailed\", jobsFailed, rb);\r\n    assertCounter(\"JobsKilled\", jobsKilled, rb);\r\n    assertGauge(\"JobsPreparing\", jobsPreparing, rb);\r\n    assertGauge(\"JobsRunning\", jobsRunning, rb);\r\n    assertCounter(\"MapsLaunched\", mapsLaunched, rb);\r\n    assertCounter(\"MapsCompleted\", mapsCompleted, rb);\r\n    assertCounter(\"MapsFailed\", mapsFailed, rb);\r\n    assertCounter(\"MapsKilled\", mapsKilled, rb);\r\n    assertGauge(\"MapsRunning\", mapsRunning, rb);\r\n    assertGauge(\"MapsWaiting\", mapsWaiting, rb);\r\n    assertCounter(\"ReducesLaunched\", reducesLaunched, rb);\r\n    assertCounter(\"ReducesCompleted\", reducesCompleted, rb);\r\n    assertCounter(\"ReducesFailed\", reducesFailed, rb);\r\n    assertCounter(\"ReducesKilled\", reducesKilled, rb);\r\n    assertGauge(\"ReducesRunning\", reducesRunning, rb);\r\n    assertGauge(\"ReducesWaiting\", reducesWaiting, rb);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app",
  "methodName" : "test",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 55,
  "sourceCodeText" : "void test() throws Exception\n{\r\n    MRAppWithClientService app = new MRAppWithClientService(1, 0, false);\r\n    Configuration conf = new Configuration();\r\n    Job job = app.submit(conf);\r\n    app.waitForState(job, JobState.RUNNING);\r\n    Assert.assertEquals(\"Num tasks not correct\", 1, job.getTasks().size());\r\n    Iterator<Task> it = job.getTasks().values().iterator();\r\n    Task task = it.next();\r\n    app.waitForState(task, TaskState.RUNNING);\r\n    TaskAttempt attempt = task.getAttempts().values().iterator().next();\r\n    app.waitForState(attempt, TaskAttemptState.RUNNING);\r\n    String diagnostic1 = \"Diagnostic1\";\r\n    String diagnostic2 = \"Diagnostic2\";\r\n    app.getContext().getEventHandler().handle(new TaskAttemptDiagnosticsUpdateEvent(attempt.getID(), diagnostic1));\r\n    TaskAttemptStatus taskAttemptStatus = new TaskAttemptStatus();\r\n    taskAttemptStatus.id = attempt.getID();\r\n    taskAttemptStatus.progress = 0.5f;\r\n    taskAttemptStatus.stateString = \"RUNNING\";\r\n    taskAttemptStatus.taskState = TaskAttemptState.RUNNING;\r\n    taskAttemptStatus.phase = Phase.MAP;\r\n    app.getContext().getEventHandler().handle(new TaskAttemptStatusUpdateEvent(attempt.getID(), new AtomicReference<>(taskAttemptStatus)));\r\n    YarnRPC rpc = YarnRPC.create(conf);\r\n    MRClientProtocol proxy = (MRClientProtocol) rpc.getProxy(MRClientProtocol.class, app.clientService.getBindAddress(), conf);\r\n    GetCountersRequest gcRequest = recordFactory.newRecordInstance(GetCountersRequest.class);\r\n    gcRequest.setJobId(job.getID());\r\n    Assert.assertNotNull(\"Counters is null\", proxy.getCounters(gcRequest).getCounters());\r\n    GetJobReportRequest gjrRequest = recordFactory.newRecordInstance(GetJobReportRequest.class);\r\n    gjrRequest.setJobId(job.getID());\r\n    JobReport jr = proxy.getJobReport(gjrRequest).getJobReport();\r\n    verifyJobReport(jr);\r\n    GetTaskAttemptCompletionEventsRequest gtaceRequest = recordFactory.newRecordInstance(GetTaskAttemptCompletionEventsRequest.class);\r\n    gtaceRequest.setJobId(job.getID());\r\n    gtaceRequest.setFromEventId(0);\r\n    gtaceRequest.setMaxEvents(10);\r\n    Assert.assertNotNull(\"TaskCompletionEvents is null\", proxy.getTaskAttemptCompletionEvents(gtaceRequest).getCompletionEventList());\r\n    GetDiagnosticsRequest gdRequest = recordFactory.newRecordInstance(GetDiagnosticsRequest.class);\r\n    gdRequest.setTaskAttemptId(attempt.getID());\r\n    Assert.assertNotNull(\"Diagnostics is null\", proxy.getDiagnostics(gdRequest).getDiagnosticsList());\r\n    GetTaskAttemptReportRequest gtarRequest = recordFactory.newRecordInstance(GetTaskAttemptReportRequest.class);\r\n    gtarRequest.setTaskAttemptId(attempt.getID());\r\n    TaskAttemptReport tar = proxy.getTaskAttemptReport(gtarRequest).getTaskAttemptReport();\r\n    verifyTaskAttemptReport(tar);\r\n    GetTaskReportRequest gtrRequest = recordFactory.newRecordInstance(GetTaskReportRequest.class);\r\n    gtrRequest.setTaskId(task.getID());\r\n    Assert.assertNotNull(\"TaskReport is null\", proxy.getTaskReport(gtrRequest).getTaskReport());\r\n    GetTaskReportsRequest gtreportsRequest = recordFactory.newRecordInstance(GetTaskReportsRequest.class);\r\n    gtreportsRequest.setJobId(job.getID());\r\n    gtreportsRequest.setTaskType(TaskType.MAP);\r\n    Assert.assertNotNull(\"TaskReports for map is null\", proxy.getTaskReports(gtreportsRequest).getTaskReportList());\r\n    gtreportsRequest = recordFactory.newRecordInstance(GetTaskReportsRequest.class);\r\n    gtreportsRequest.setJobId(job.getID());\r\n    gtreportsRequest.setTaskType(TaskType.REDUCE);\r\n    Assert.assertNotNull(\"TaskReports for reduce is null\", proxy.getTaskReports(gtreportsRequest).getTaskReportList());\r\n    List<String> diag = proxy.getDiagnostics(gdRequest).getDiagnosticsList();\r\n    Assert.assertEquals(\"Num diagnostics not correct\", 1, diag.size());\r\n    Assert.assertEquals(\"Diag 1 not correct\", diagnostic1, diag.get(0).toString());\r\n    TaskReport taskReport = proxy.getTaskReport(gtrRequest).getTaskReport();\r\n    Assert.assertEquals(\"Num diagnostics not correct\", 1, taskReport.getDiagnosticsCount());\r\n    app.getContext().getEventHandler().handle(new TaskAttemptEvent(task.getAttempts().values().iterator().next().getID(), TaskAttemptEventType.TA_DONE));\r\n    app.waitForState(job, JobState.SUCCEEDED);\r\n    gtreportsRequest = recordFactory.newRecordInstance(GetTaskReportsRequest.class);\r\n    gtreportsRequest.setJobId(TypeConverter.toYarn(JobID.forName(\"job_1415730144495_0001\")));\r\n    gtreportsRequest.setTaskType(TaskType.REDUCE);\r\n    try {\r\n        proxy.getTaskReports(gtreportsRequest);\r\n        fail(\"IOException not thrown for invalid job id\");\r\n    } catch (IOException e) {\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app",
  "methodName" : "testViewAclOnlyCannotModify",
  "errType" : [ "AccessControlException", "AccessControlException", "AccessControlException", "AccessControlException" ],
  "containingMethodsNum" : 30,
  "sourceCodeText" : "void testViewAclOnlyCannotModify() throws Exception\n{\r\n    final MRAppWithClientService app = new MRAppWithClientService(1, 0, false);\r\n    final Configuration conf = new Configuration();\r\n    conf.setBoolean(MRConfig.MR_ACLS_ENABLED, true);\r\n    conf.set(MRJobConfig.JOB_ACL_VIEW_JOB, \"viewonlyuser\");\r\n    Job job = app.submit(conf);\r\n    app.waitForState(job, JobState.RUNNING);\r\n    Assert.assertEquals(\"Num tasks not correct\", 1, job.getTasks().size());\r\n    Iterator<Task> it = job.getTasks().values().iterator();\r\n    Task task = it.next();\r\n    app.waitForState(task, TaskState.RUNNING);\r\n    TaskAttempt attempt = task.getAttempts().values().iterator().next();\r\n    app.waitForState(attempt, TaskAttemptState.RUNNING);\r\n    UserGroupInformation viewOnlyUser = UserGroupInformation.createUserForTesting(\"viewonlyuser\", new String[] {});\r\n    Assert.assertTrue(\"viewonlyuser cannot view job\", job.checkAccess(viewOnlyUser, JobACL.VIEW_JOB));\r\n    Assert.assertFalse(\"viewonlyuser can modify job\", job.checkAccess(viewOnlyUser, JobACL.MODIFY_JOB));\r\n    MRClientProtocol client = viewOnlyUser.doAs(new PrivilegedExceptionAction<MRClientProtocol>() {\r\n\r\n        @Override\r\n        public MRClientProtocol run() throws Exception {\r\n            YarnRPC rpc = YarnRPC.create(conf);\r\n            return (MRClientProtocol) rpc.getProxy(MRClientProtocol.class, app.clientService.getBindAddress(), conf);\r\n        }\r\n    });\r\n    KillJobRequest killJobRequest = recordFactory.newRecordInstance(KillJobRequest.class);\r\n    killJobRequest.setJobId(app.getJobId());\r\n    try {\r\n        client.killJob(killJobRequest);\r\n        fail(\"viewonlyuser killed job\");\r\n    } catch (AccessControlException e) {\r\n    }\r\n    KillTaskRequest killTaskRequest = recordFactory.newRecordInstance(KillTaskRequest.class);\r\n    killTaskRequest.setTaskId(task.getID());\r\n    try {\r\n        client.killTask(killTaskRequest);\r\n        fail(\"viewonlyuser killed task\");\r\n    } catch (AccessControlException e) {\r\n    }\r\n    KillTaskAttemptRequest killTaskAttemptRequest = recordFactory.newRecordInstance(KillTaskAttemptRequest.class);\r\n    killTaskAttemptRequest.setTaskAttemptId(attempt.getID());\r\n    try {\r\n        client.killTaskAttempt(killTaskAttemptRequest);\r\n        fail(\"viewonlyuser killed task attempt\");\r\n    } catch (AccessControlException e) {\r\n    }\r\n    FailTaskAttemptRequest failTaskAttemptRequest = recordFactory.newRecordInstance(FailTaskAttemptRequest.class);\r\n    failTaskAttemptRequest.setTaskAttemptId(attempt.getID());\r\n    try {\r\n        client.failTaskAttempt(failTaskAttemptRequest);\r\n        fail(\"viewonlyuser killed task attempt\");\r\n    } catch (AccessControlException e) {\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 4,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app",
  "methodName" : "verifyJobReport",
  "errType" : null,
  "containingMethodsNum" : 12,
  "sourceCodeText" : "void verifyJobReport(JobReport jr)\n{\r\n    Assert.assertNotNull(\"JobReport is null\", jr);\r\n    List<AMInfo> amInfos = jr.getAMInfos();\r\n    Assert.assertEquals(1, amInfos.size());\r\n    Assert.assertEquals(JobState.RUNNING, jr.getJobState());\r\n    AMInfo amInfo = amInfos.get(0);\r\n    Assert.assertEquals(MRApp.NM_HOST, amInfo.getNodeManagerHost());\r\n    Assert.assertEquals(MRApp.NM_PORT, amInfo.getNodeManagerPort());\r\n    Assert.assertEquals(MRApp.NM_HTTP_PORT, amInfo.getNodeManagerHttpPort());\r\n    Assert.assertEquals(1, amInfo.getAppAttemptId().getAttemptId());\r\n    Assert.assertEquals(1, amInfo.getContainerId().getApplicationAttemptId().getAttemptId());\r\n    Assert.assertTrue(amInfo.getStartTime() > 0);\r\n    Assert.assertFalse(jr.isUber());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app",
  "methodName" : "verifyTaskAttemptReport",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void verifyTaskAttemptReport(TaskAttemptReport tar)\n{\r\n    Assert.assertEquals(TaskAttemptState.RUNNING, tar.getTaskAttemptState());\r\n    Assert.assertNotNull(\"TaskAttemptReport is null\", tar);\r\n    Assert.assertEquals(MRApp.NM_HOST, tar.getNodeManagerHost());\r\n    Assert.assertEquals(MRApp.NM_PORT, tar.getNodeManagerPort());\r\n    Assert.assertEquals(MRApp.NM_HTTP_PORT, tar.getNodeManagerHttpPort());\r\n    Assert.assertEquals(1, tar.getContainerId().getApplicationAttemptId().getAttemptId());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app",
  "methodName" : "main",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void main(String[] args) throws Exception\n{\r\n    TestMRClientService t = new TestMRClientService();\r\n    t.test();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\webapp",
  "methodName" : "getClazz",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Class<?> getClazz()\n{\r\n    return clazz;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\webapp",
  "methodName" : "getInstance",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "T getInstance(Class<T> cls)\n{\r\n    clazz = cls;\r\n    if (cls.equals(ResponseInfo.class)) {\r\n        return (T) responseInfo;\r\n    }\r\n    return (T) view;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\webapp",
  "methodName" : "getResponseInfo",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "ResponseInfo getResponseInfo()\n{\r\n    return responseInfo;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\webapp",
  "methodName" : "get",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String get(String key, String defaultValue)\n{\r\n    String result = properties.get(key);\r\n    if (result == null) {\r\n        result = defaultValue;\r\n    }\r\n    return result;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\webapp",
  "methodName" : "set",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void set(String key, String value)\n{\r\n    properties.put(key, value);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\webapp",
  "methodName" : "request",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "HttpServletRequest request()\n{\r\n    HttpServletRequest result = mock(HttpServletRequest.class);\r\n    when(result.getRemoteUser()).thenReturn(\"user\");\r\n    return result;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\webapp",
  "methodName" : "response",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "HttpServletResponse response()\n{\r\n    if (response == null) {\r\n        response = mock(HttpServletResponse.class);\r\n    }\r\n    return response;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\webapp",
  "methodName" : "getProperty",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Map<String, String> getProperty()\n{\r\n    return properties;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\webapp",
  "methodName" : "getData",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "String getData()\n{\r\n    writer.flush();\r\n    return data.toString();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\webapp",
  "methodName" : "writer",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "PrintWriter writer()\n{\r\n    if (writer == null) {\r\n        writer = new PrintWriter(data);\r\n    }\r\n    return writer;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\webapp",
  "methodName" : "setUp",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void setUp() throws Exception\n{\r\n    super.setUp();\r\n    GuiceServletConfig.setInjector(Guice.createInjector(new WebServletModule()));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\webapp",
  "methodName" : "testTaskAttempts",
  "errType" : null,
  "containingMethodsNum" : 10,
  "sourceCodeText" : "void testTaskAttempts() throws JSONException, Exception\n{\r\n    WebResource r = resource();\r\n    Map<JobId, Job> jobsMap = appContext.getAllJobs();\r\n    for (JobId id : jobsMap.keySet()) {\r\n        String jobId = MRApps.toString(id);\r\n        for (Task task : jobsMap.get(id).getTasks().values()) {\r\n            String tid = MRApps.toString(task.getID());\r\n            ClientResponse response = r.path(\"ws\").path(\"v1\").path(\"mapreduce\").path(\"jobs\").path(jobId).path(\"tasks\").path(tid).path(\"attempts\").accept(MediaType.APPLICATION_JSON).get(ClientResponse.class);\r\n            assertEquals(MediaType.APPLICATION_JSON_TYPE + \"; \" + JettyUtils.UTF_8, response.getType().toString());\r\n            JSONObject json = response.getEntity(JSONObject.class);\r\n            verifyAMTaskAttempts(json, task);\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\webapp",
  "methodName" : "testTaskAttemptsSlash",
  "errType" : null,
  "containingMethodsNum" : 10,
  "sourceCodeText" : "void testTaskAttemptsSlash() throws JSONException, Exception\n{\r\n    WebResource r = resource();\r\n    Map<JobId, Job> jobsMap = appContext.getAllJobs();\r\n    for (JobId id : jobsMap.keySet()) {\r\n        String jobId = MRApps.toString(id);\r\n        for (Task task : jobsMap.get(id).getTasks().values()) {\r\n            String tid = MRApps.toString(task.getID());\r\n            ClientResponse response = r.path(\"ws\").path(\"v1\").path(\"mapreduce\").path(\"jobs\").path(jobId).path(\"tasks\").path(tid).path(\"attempts/\").accept(MediaType.APPLICATION_JSON).get(ClientResponse.class);\r\n            assertEquals(MediaType.APPLICATION_JSON_TYPE + \"; \" + JettyUtils.UTF_8, response.getType().toString());\r\n            JSONObject json = response.getEntity(JSONObject.class);\r\n            verifyAMTaskAttempts(json, task);\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\webapp",
  "methodName" : "testTaskAttemptsDefault",
  "errType" : null,
  "containingMethodsNum" : 10,
  "sourceCodeText" : "void testTaskAttemptsDefault() throws JSONException, Exception\n{\r\n    WebResource r = resource();\r\n    Map<JobId, Job> jobsMap = appContext.getAllJobs();\r\n    for (JobId id : jobsMap.keySet()) {\r\n        String jobId = MRApps.toString(id);\r\n        for (Task task : jobsMap.get(id).getTasks().values()) {\r\n            String tid = MRApps.toString(task.getID());\r\n            ClientResponse response = r.path(\"ws\").path(\"v1\").path(\"mapreduce\").path(\"jobs\").path(jobId).path(\"tasks\").path(tid).path(\"attempts\").get(ClientResponse.class);\r\n            assertEquals(MediaType.APPLICATION_JSON_TYPE + \"; \" + JettyUtils.UTF_8, response.getType().toString());\r\n            JSONObject json = response.getEntity(JSONObject.class);\r\n            verifyAMTaskAttempts(json, task);\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\webapp",
  "methodName" : "testTaskAttemptsXML",
  "errType" : null,
  "containingMethodsNum" : 17,
  "sourceCodeText" : "void testTaskAttemptsXML() throws JSONException, Exception\n{\r\n    WebResource r = resource();\r\n    Map<JobId, Job> jobsMap = appContext.getAllJobs();\r\n    for (JobId id : jobsMap.keySet()) {\r\n        String jobId = MRApps.toString(id);\r\n        for (Task task : jobsMap.get(id).getTasks().values()) {\r\n            String tid = MRApps.toString(task.getID());\r\n            ClientResponse response = r.path(\"ws\").path(\"v1\").path(\"mapreduce\").path(\"jobs\").path(jobId).path(\"tasks\").path(tid).path(\"attempts\").accept(MediaType.APPLICATION_XML).get(ClientResponse.class);\r\n            assertEquals(MediaType.APPLICATION_XML_TYPE + \"; \" + JettyUtils.UTF_8, response.getType().toString());\r\n            String xml = response.getEntity(String.class);\r\n            DocumentBuilderFactory dbf = DocumentBuilderFactory.newInstance();\r\n            DocumentBuilder db = dbf.newDocumentBuilder();\r\n            InputSource is = new InputSource();\r\n            is.setCharacterStream(new StringReader(xml));\r\n            Document dom = db.parse(is);\r\n            NodeList attempts = dom.getElementsByTagName(\"taskAttempts\");\r\n            assertEquals(\"incorrect number of elements\", 1, attempts.getLength());\r\n            NodeList nodes = dom.getElementsByTagName(\"taskAttempt\");\r\n            verifyAMTaskAttemptsXML(nodes, task);\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\webapp",
  "methodName" : "testTaskAttemptId",
  "errType" : null,
  "containingMethodsNum" : 15,
  "sourceCodeText" : "void testTaskAttemptId() throws JSONException, Exception\n{\r\n    WebResource r = resource();\r\n    Map<JobId, Job> jobsMap = appContext.getAllJobs();\r\n    for (JobId id : jobsMap.keySet()) {\r\n        String jobId = MRApps.toString(id);\r\n        for (Task task : jobsMap.get(id).getTasks().values()) {\r\n            String tid = MRApps.toString(task.getID());\r\n            for (TaskAttempt att : task.getAttempts().values()) {\r\n                TaskAttemptId attemptid = att.getID();\r\n                String attid = MRApps.toString(attemptid);\r\n                ClientResponse response = r.path(\"ws\").path(\"v1\").path(\"mapreduce\").path(\"jobs\").path(jobId).path(\"tasks\").path(tid).path(\"attempts\").path(attid).accept(MediaType.APPLICATION_JSON).get(ClientResponse.class);\r\n                assertEquals(MediaType.APPLICATION_JSON_TYPE + \"; \" + JettyUtils.UTF_8, response.getType().toString());\r\n                JSONObject json = response.getEntity(JSONObject.class);\r\n                assertEquals(\"incorrect number of elements\", 1, json.length());\r\n                JSONObject info = json.getJSONObject(\"taskAttempt\");\r\n                verifyAMTaskAttempt(info, att, task.getType());\r\n            }\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\webapp",
  "methodName" : "testTaskAttemptIdSlash",
  "errType" : null,
  "containingMethodsNum" : 15,
  "sourceCodeText" : "void testTaskAttemptIdSlash() throws JSONException, Exception\n{\r\n    WebResource r = resource();\r\n    Map<JobId, Job> jobsMap = appContext.getAllJobs();\r\n    for (JobId id : jobsMap.keySet()) {\r\n        String jobId = MRApps.toString(id);\r\n        for (Task task : jobsMap.get(id).getTasks().values()) {\r\n            String tid = MRApps.toString(task.getID());\r\n            for (TaskAttempt att : task.getAttempts().values()) {\r\n                TaskAttemptId attemptid = att.getID();\r\n                String attid = MRApps.toString(attemptid);\r\n                ClientResponse response = r.path(\"ws\").path(\"v1\").path(\"mapreduce\").path(\"jobs\").path(jobId).path(\"tasks\").path(tid).path(\"attempts\").path(attid + \"/\").accept(MediaType.APPLICATION_JSON).get(ClientResponse.class);\r\n                assertEquals(MediaType.APPLICATION_JSON_TYPE + \"; \" + JettyUtils.UTF_8, response.getType().toString());\r\n                JSONObject json = response.getEntity(JSONObject.class);\r\n                assertEquals(\"incorrect number of elements\", 1, json.length());\r\n                JSONObject info = json.getJSONObject(\"taskAttempt\");\r\n                verifyAMTaskAttempt(info, att, task.getType());\r\n            }\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\webapp",
  "methodName" : "testTaskAttemptIdDefault",
  "errType" : null,
  "containingMethodsNum" : 15,
  "sourceCodeText" : "void testTaskAttemptIdDefault() throws JSONException, Exception\n{\r\n    WebResource r = resource();\r\n    Map<JobId, Job> jobsMap = appContext.getAllJobs();\r\n    for (JobId id : jobsMap.keySet()) {\r\n        String jobId = MRApps.toString(id);\r\n        for (Task task : jobsMap.get(id).getTasks().values()) {\r\n            String tid = MRApps.toString(task.getID());\r\n            for (TaskAttempt att : task.getAttempts().values()) {\r\n                TaskAttemptId attemptid = att.getID();\r\n                String attid = MRApps.toString(attemptid);\r\n                ClientResponse response = r.path(\"ws\").path(\"v1\").path(\"mapreduce\").path(\"jobs\").path(jobId).path(\"tasks\").path(tid).path(\"attempts\").path(attid).get(ClientResponse.class);\r\n                assertEquals(MediaType.APPLICATION_JSON_TYPE + \"; \" + JettyUtils.UTF_8, response.getType().toString());\r\n                JSONObject json = response.getEntity(JSONObject.class);\r\n                assertEquals(\"incorrect number of elements\", 1, json.length());\r\n                JSONObject info = json.getJSONObject(\"taskAttempt\");\r\n                verifyAMTaskAttempt(info, att, task.getType());\r\n            }\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\webapp",
  "methodName" : "testTaskAttemptIdXML",
  "errType" : null,
  "containingMethodsNum" : 20,
  "sourceCodeText" : "void testTaskAttemptIdXML() throws JSONException, Exception\n{\r\n    WebResource r = resource();\r\n    Map<JobId, Job> jobsMap = appContext.getAllJobs();\r\n    for (JobId id : jobsMap.keySet()) {\r\n        String jobId = MRApps.toString(id);\r\n        for (Task task : jobsMap.get(id).getTasks().values()) {\r\n            String tid = MRApps.toString(task.getID());\r\n            for (TaskAttempt att : task.getAttempts().values()) {\r\n                TaskAttemptId attemptid = att.getID();\r\n                String attid = MRApps.toString(attemptid);\r\n                ClientResponse response = r.path(\"ws\").path(\"v1\").path(\"mapreduce\").path(\"jobs\").path(jobId).path(\"tasks\").path(tid).path(\"attempts\").path(attid).accept(MediaType.APPLICATION_XML).get(ClientResponse.class);\r\n                assertEquals(MediaType.APPLICATION_XML_TYPE + \"; \" + JettyUtils.UTF_8, response.getType().toString());\r\n                String xml = response.getEntity(String.class);\r\n                DocumentBuilderFactory dbf = DocumentBuilderFactory.newInstance();\r\n                DocumentBuilder db = dbf.newDocumentBuilder();\r\n                InputSource is = new InputSource();\r\n                is.setCharacterStream(new StringReader(xml));\r\n                Document dom = db.parse(is);\r\n                NodeList nodes = dom.getElementsByTagName(\"taskAttempt\");\r\n                for (int i = 0; i < nodes.getLength(); i++) {\r\n                    Element element = (Element) nodes.item(i);\r\n                    verifyAMTaskAttemptXML(element, att, task.getType());\r\n                }\r\n            }\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\webapp",
  "methodName" : "testTaskAttemptIdBogus",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testTaskAttemptIdBogus() throws JSONException, Exception\n{\r\n    testTaskAttemptIdErrorGeneric(\"bogusid\", \"java.lang.Exception: TaskAttemptId string : bogusid is not properly formed\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\webapp",
  "methodName" : "testTaskAttemptIdNonExist",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testTaskAttemptIdNonExist() throws JSONException, Exception\n{\r\n    testTaskAttemptIdErrorGeneric(\"attempt_0_12345_m_000000_0\", \"java.lang.Exception: Error getting info on task attempt id attempt_0_12345_m_000000_0\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\webapp",
  "methodName" : "testTaskAttemptIdInvalid",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testTaskAttemptIdInvalid() throws JSONException, Exception\n{\r\n    testTaskAttemptIdErrorGeneric(\"attempt_0_12345_d_000000_0\", \"java.lang.Exception: Bad TaskType identifier. TaskAttemptId string : attempt_0_12345_d_000000_0 is not properly formed.\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\webapp",
  "methodName" : "testTaskAttemptIdInvalid2",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testTaskAttemptIdInvalid2() throws JSONException, Exception\n{\r\n    testTaskAttemptIdErrorGeneric(\"attempt_12345_m_000000_0\", \"java.lang.Exception: TaskAttemptId string : attempt_12345_m_000000_0 is not properly formed\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\webapp",
  "methodName" : "testTaskAttemptIdInvalid3",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testTaskAttemptIdInvalid3() throws JSONException, Exception\n{\r\n    testTaskAttemptIdErrorGeneric(\"attempt_0_12345_m_000000\", \"java.lang.Exception: TaskAttemptId string : attempt_0_12345_m_000000 is not properly formed\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\webapp",
  "methodName" : "testTaskAttemptIdErrorGeneric",
  "errType" : [ "UniformInterfaceException" ],
  "containingMethodsNum" : 20,
  "sourceCodeText" : "void testTaskAttemptIdErrorGeneric(String attid, String error) throws JSONException, Exception\n{\r\n    WebResource r = resource();\r\n    Map<JobId, Job> jobsMap = appContext.getAllJobs();\r\n    for (JobId id : jobsMap.keySet()) {\r\n        String jobId = MRApps.toString(id);\r\n        for (Task task : jobsMap.get(id).getTasks().values()) {\r\n            String tid = MRApps.toString(task.getID());\r\n            try {\r\n                r.path(\"ws\").path(\"v1\").path(\"mapreduce\").path(\"jobs\").path(jobId).path(\"tasks\").path(tid).path(\"attempts\").path(attid).accept(MediaType.APPLICATION_JSON).get(JSONObject.class);\r\n                fail(\"should have thrown exception on invalid uri\");\r\n            } catch (UniformInterfaceException ue) {\r\n                ClientResponse response = ue.getResponse();\r\n                assertResponseStatusCode(Status.NOT_FOUND, response.getStatusInfo());\r\n                assertEquals(MediaType.APPLICATION_JSON_TYPE + \"; \" + JettyUtils.UTF_8, response.getType().toString());\r\n                JSONObject msg = response.getEntity(JSONObject.class);\r\n                JSONObject exception = msg.getJSONObject(\"RemoteException\");\r\n                assertEquals(\"incorrect number of elements\", 3, exception.length());\r\n                String message = exception.getString(\"message\");\r\n                String type = exception.getString(\"exception\");\r\n                String classname = exception.getString(\"javaClassName\");\r\n                WebServicesTestUtils.checkStringMatch(\"exception message\", error, message);\r\n                WebServicesTestUtils.checkStringMatch(\"exception type\", \"NotFoundException\", type);\r\n                WebServicesTestUtils.checkStringMatch(\"exception classname\", \"org.apache.hadoop.yarn.webapp.NotFoundException\", classname);\r\n            }\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\webapp",
  "methodName" : "verifyAMTaskAttemptXML",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void verifyAMTaskAttemptXML(Element element, TaskAttempt att, TaskType ttype)\n{\r\n    verifyTaskAttemptGeneric(att, ttype, WebServicesTestUtils.getXmlString(element, \"id\"), WebServicesTestUtils.getXmlString(element, \"state\"), WebServicesTestUtils.getXmlString(element, \"type\"), WebServicesTestUtils.getXmlString(element, \"rack\"), WebServicesTestUtils.getXmlString(element, \"nodeHttpAddress\"), WebServicesTestUtils.getXmlString(element, \"diagnostics\"), WebServicesTestUtils.getXmlString(element, \"assignedContainerId\"), WebServicesTestUtils.getXmlLong(element, \"startTime\"), WebServicesTestUtils.getXmlLong(element, \"finishTime\"), WebServicesTestUtils.getXmlLong(element, \"elapsedTime\"), WebServicesTestUtils.getXmlFloat(element, \"progress\"));\r\n    if (ttype == TaskType.REDUCE) {\r\n        verifyReduceTaskAttemptGeneric(att, WebServicesTestUtils.getXmlLong(element, \"shuffleFinishTime\"), WebServicesTestUtils.getXmlLong(element, \"mergeFinishTime\"), WebServicesTestUtils.getXmlLong(element, \"elapsedShuffleTime\"), WebServicesTestUtils.getXmlLong(element, \"elapsedMergeTime\"), WebServicesTestUtils.getXmlLong(element, \"elapsedReduceTime\"));\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\webapp",
  "methodName" : "verifyAMTaskAttempt",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void verifyAMTaskAttempt(JSONObject info, TaskAttempt att, TaskType ttype) throws JSONException\n{\r\n    if (ttype == TaskType.REDUCE) {\r\n        assertEquals(\"incorrect number of elements\", 17, info.length());\r\n    } else {\r\n        assertEquals(\"incorrect number of elements\", 12, info.length());\r\n    }\r\n    verifyTaskAttemptGeneric(att, ttype, info.getString(\"id\"), info.getString(\"state\"), info.getString(\"type\"), info.getString(\"rack\"), info.getString(\"nodeHttpAddress\"), info.getString(\"diagnostics\"), info.getString(\"assignedContainerId\"), info.getLong(\"startTime\"), info.getLong(\"finishTime\"), info.getLong(\"elapsedTime\"), (float) info.getDouble(\"progress\"));\r\n    if (ttype == TaskType.REDUCE) {\r\n        verifyReduceTaskAttemptGeneric(att, info.getLong(\"shuffleFinishTime\"), info.getLong(\"mergeFinishTime\"), info.getLong(\"elapsedShuffleTime\"), info.getLong(\"elapsedMergeTime\"), info.getLong(\"elapsedReduceTime\"));\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\webapp",
  "methodName" : "verifyAMTaskAttempts",
  "errType" : null,
  "containingMethodsNum" : 12,
  "sourceCodeText" : "void verifyAMTaskAttempts(JSONObject json, Task task) throws JSONException\n{\r\n    assertEquals(\"incorrect number of elements\", 1, json.length());\r\n    JSONObject attempts = json.getJSONObject(\"taskAttempts\");\r\n    assertEquals(\"incorrect number of elements\", 1, json.length());\r\n    JSONArray arr = attempts.getJSONArray(\"taskAttempt\");\r\n    for (TaskAttempt att : task.getAttempts().values()) {\r\n        TaskAttemptId id = att.getID();\r\n        String attid = MRApps.toString(id);\r\n        Boolean found = false;\r\n        for (int i = 0; i < arr.length(); i++) {\r\n            JSONObject info = arr.getJSONObject(i);\r\n            if (attid.matches(info.getString(\"id\"))) {\r\n                found = true;\r\n                verifyAMTaskAttempt(info, att, task.getType());\r\n            }\r\n        }\r\n        assertTrue(\"task attempt with id: \" + attid + \" not in web service output\", found);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\webapp",
  "methodName" : "verifyAMTaskAttemptsXML",
  "errType" : null,
  "containingMethodsNum" : 10,
  "sourceCodeText" : "void verifyAMTaskAttemptsXML(NodeList nodes, Task task)\n{\r\n    assertEquals(\"incorrect number of elements\", 1, nodes.getLength());\r\n    for (TaskAttempt att : task.getAttempts().values()) {\r\n        TaskAttemptId id = att.getID();\r\n        String attid = MRApps.toString(id);\r\n        Boolean found = false;\r\n        for (int i = 0; i < nodes.getLength(); i++) {\r\n            Element element = (Element) nodes.item(i);\r\n            assertFalse(\"task attempt should not contain any attributes, it can lead to incorrect JSON marshaling\", element.hasAttributes());\r\n            if (attid.matches(WebServicesTestUtils.getXmlString(element, \"id\"))) {\r\n                found = true;\r\n                verifyAMTaskAttemptXML(element, att, task.getType());\r\n            }\r\n        }\r\n        assertTrue(\"task with id: \" + attid + \" not in web service output\", found);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\webapp",
  "methodName" : "verifyTaskAttemptGeneric",
  "errType" : null,
  "containingMethodsNum" : 17,
  "sourceCodeText" : "void verifyTaskAttemptGeneric(TaskAttempt ta, TaskType ttype, String id, String state, String type, String rack, String nodeHttpAddress, String diagnostics, String assignedContainerId, long startTime, long finishTime, long elapsedTime, float progress)\n{\r\n    TaskAttemptId attid = ta.getID();\r\n    String attemptId = MRApps.toString(attid);\r\n    WebServicesTestUtils.checkStringMatch(\"id\", attemptId, id);\r\n    WebServicesTestUtils.checkStringMatch(\"type\", ttype.toString(), type);\r\n    WebServicesTestUtils.checkStringMatch(\"state\", ta.getState().toString(), state);\r\n    WebServicesTestUtils.checkStringMatch(\"rack\", ta.getNodeRackName(), rack);\r\n    WebServicesTestUtils.checkStringMatch(\"nodeHttpAddress\", ta.getNodeHttpAddress(), nodeHttpAddress);\r\n    String expectDiag = \"\";\r\n    List<String> diagnosticsList = ta.getDiagnostics();\r\n    if (diagnosticsList != null && !diagnostics.isEmpty()) {\r\n        StringBuffer b = new StringBuffer();\r\n        for (String diag : diagnosticsList) {\r\n            b.append(diag);\r\n        }\r\n        expectDiag = b.toString();\r\n    }\r\n    WebServicesTestUtils.checkStringMatch(\"diagnostics\", expectDiag, diagnostics);\r\n    WebServicesTestUtils.checkStringMatch(\"assignedContainerId\", ta.getAssignedContainerID().toString(), assignedContainerId);\r\n    assertEquals(\"startTime wrong\", ta.getLaunchTime(), startTime);\r\n    assertEquals(\"finishTime wrong\", ta.getFinishTime(), finishTime);\r\n    assertEquals(\"elapsedTime wrong\", finishTime - startTime, elapsedTime);\r\n    assertEquals(\"progress wrong\", ta.getProgress() * 100, progress, 1e-3f);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\webapp",
  "methodName" : "verifyReduceTaskAttemptGeneric",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void verifyReduceTaskAttemptGeneric(TaskAttempt ta, long shuffleFinishTime, long mergeFinishTime, long elapsedShuffleTime, long elapsedMergeTime, long elapsedReduceTime)\n{\r\n    assertEquals(\"shuffleFinishTime wrong\", ta.getShuffleFinishTime(), shuffleFinishTime);\r\n    assertEquals(\"mergeFinishTime wrong\", ta.getSortFinishTime(), mergeFinishTime);\r\n    assertEquals(\"elapsedShuffleTime wrong\", ta.getShuffleFinishTime() - ta.getLaunchTime(), elapsedShuffleTime);\r\n    assertEquals(\"elapsedMergeTime wrong\", ta.getSortFinishTime() - ta.getShuffleFinishTime(), elapsedMergeTime);\r\n    assertEquals(\"elapsedReduceTime wrong\", ta.getFinishTime() - ta.getSortFinishTime(), elapsedReduceTime);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\webapp",
  "methodName" : "testTaskAttemptIdCounters",
  "errType" : null,
  "containingMethodsNum" : 15,
  "sourceCodeText" : "void testTaskAttemptIdCounters() throws JSONException, Exception\n{\r\n    WebResource r = resource();\r\n    Map<JobId, Job> jobsMap = appContext.getAllJobs();\r\n    for (JobId id : jobsMap.keySet()) {\r\n        String jobId = MRApps.toString(id);\r\n        for (Task task : jobsMap.get(id).getTasks().values()) {\r\n            String tid = MRApps.toString(task.getID());\r\n            for (TaskAttempt att : task.getAttempts().values()) {\r\n                TaskAttemptId attemptid = att.getID();\r\n                String attid = MRApps.toString(attemptid);\r\n                ClientResponse response = r.path(\"ws\").path(\"v1\").path(\"mapreduce\").path(\"jobs\").path(jobId).path(\"tasks\").path(tid).path(\"attempts\").path(attid).path(\"counters\").accept(MediaType.APPLICATION_JSON).get(ClientResponse.class);\r\n                assertEquals(MediaType.APPLICATION_JSON_TYPE + \"; \" + JettyUtils.UTF_8, response.getType().toString());\r\n                JSONObject json = response.getEntity(JSONObject.class);\r\n                assertEquals(\"incorrect number of elements\", 1, json.length());\r\n                JSONObject info = json.getJSONObject(\"jobTaskAttemptCounters\");\r\n                verifyAMJobTaskAttemptCounters(info, att);\r\n            }\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\webapp",
  "methodName" : "testTaskAttemptIdXMLCounters",
  "errType" : null,
  "containingMethodsNum" : 18,
  "sourceCodeText" : "void testTaskAttemptIdXMLCounters() throws JSONException, Exception\n{\r\n    WebResource r = resource();\r\n    Map<JobId, Job> jobsMap = appContext.getAllJobs();\r\n    for (JobId id : jobsMap.keySet()) {\r\n        String jobId = MRApps.toString(id);\r\n        for (Task task : jobsMap.get(id).getTasks().values()) {\r\n            String tid = MRApps.toString(task.getID());\r\n            for (TaskAttempt att : task.getAttempts().values()) {\r\n                TaskAttemptId attemptid = att.getID();\r\n                String attid = MRApps.toString(attemptid);\r\n                ClientResponse response = r.path(\"ws\").path(\"v1\").path(\"mapreduce\").path(\"jobs\").path(jobId).path(\"tasks\").path(tid).path(\"attempts\").path(attid).path(\"counters\").accept(MediaType.APPLICATION_XML).get(ClientResponse.class);\r\n                assertEquals(MediaType.APPLICATION_XML_TYPE + \"; \" + JettyUtils.UTF_8, response.getType().toString());\r\n                String xml = response.getEntity(String.class);\r\n                DocumentBuilderFactory dbf = DocumentBuilderFactory.newInstance();\r\n                DocumentBuilder db = dbf.newDocumentBuilder();\r\n                InputSource is = new InputSource();\r\n                is.setCharacterStream(new StringReader(xml));\r\n                Document dom = db.parse(is);\r\n                NodeList nodes = dom.getElementsByTagName(\"jobTaskAttemptCounters\");\r\n                verifyAMTaskCountersXML(nodes, att);\r\n            }\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\webapp",
  "methodName" : "verifyAMJobTaskAttemptCounters",
  "errType" : null,
  "containingMethodsNum" : 14,
  "sourceCodeText" : "void verifyAMJobTaskAttemptCounters(JSONObject info, TaskAttempt att) throws JSONException\n{\r\n    assertEquals(\"incorrect number of elements\", 2, info.length());\r\n    WebServicesTestUtils.checkStringMatch(\"id\", MRApps.toString(att.getID()), info.getString(\"id\"));\r\n    JSONArray counterGroups = info.getJSONArray(\"taskAttemptCounterGroup\");\r\n    for (int i = 0; i < counterGroups.length(); i++) {\r\n        JSONObject counterGroup = counterGroups.getJSONObject(i);\r\n        String name = counterGroup.getString(\"counterGroupName\");\r\n        assertTrue(\"name not set\", (name != null && !name.isEmpty()));\r\n        JSONArray counters = counterGroup.getJSONArray(\"counter\");\r\n        for (int j = 0; j < counters.length(); j++) {\r\n            JSONObject counter = counters.getJSONObject(j);\r\n            String counterName = counter.getString(\"name\");\r\n            assertTrue(\"name not set\", (counterName != null && !counterName.isEmpty()));\r\n            long value = counter.getLong(\"value\");\r\n            assertTrue(\"value  >= 0\", value >= 0);\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\webapp",
  "methodName" : "verifyAMTaskCountersXML",
  "errType" : null,
  "containingMethodsNum" : 16,
  "sourceCodeText" : "void verifyAMTaskCountersXML(NodeList nodes, TaskAttempt att)\n{\r\n    for (int i = 0; i < nodes.getLength(); i++) {\r\n        Element element = (Element) nodes.item(i);\r\n        WebServicesTestUtils.checkStringMatch(\"id\", MRApps.toString(att.getID()), WebServicesTestUtils.getXmlString(element, \"id\"));\r\n        NodeList groups = element.getElementsByTagName(\"taskAttemptCounterGroup\");\r\n        for (int j = 0; j < groups.getLength(); j++) {\r\n            Element counters = (Element) groups.item(j);\r\n            assertNotNull(\"should have counters in the web service info\", counters);\r\n            String name = WebServicesTestUtils.getXmlString(counters, \"counterGroupName\");\r\n            assertTrue(\"name not set\", (name != null && !name.isEmpty()));\r\n            NodeList counterArr = counters.getElementsByTagName(\"counter\");\r\n            for (int z = 0; z < counterArr.getLength(); z++) {\r\n                Element counter = (Element) counterArr.item(z);\r\n                String counterName = WebServicesTestUtils.getXmlString(counter, \"name\");\r\n                assertTrue(\"counter name not set\", (counterName != null && !counterName.isEmpty()));\r\n                long value = WebServicesTestUtils.getXmlLong(counter, \"value\");\r\n                assertTrue(\"value not >= 0\", value >= 0);\r\n            }\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app",
  "methodName" : "testNumRetries",
  "errType" : null,
  "containingMethodsNum" : 10,
  "sourceCodeText" : "void testNumRetries(Configuration conf)\n{\r\n    conf.set(MRJobConfig.MR_JOB_END_NOTIFICATION_MAX_ATTEMPTS, \"0\");\r\n    conf.set(MRJobConfig.MR_JOB_END_RETRY_ATTEMPTS, \"10\");\r\n    setConf(conf);\r\n    Assert.assertTrue(\"Expected numTries to be 0, but was \" + numTries, numTries == 0);\r\n    conf.set(MRJobConfig.MR_JOB_END_NOTIFICATION_MAX_ATTEMPTS, \"1\");\r\n    setConf(conf);\r\n    Assert.assertTrue(\"Expected numTries to be 1, but was \" + numTries, numTries == 1);\r\n    conf.set(MRJobConfig.MR_JOB_END_NOTIFICATION_MAX_ATTEMPTS, \"20\");\r\n    setConf(conf);\r\n    Assert.assertTrue(\"Expected numTries to be 11, but was \" + numTries, numTries == 11);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app",
  "methodName" : "testWaitInterval",
  "errType" : null,
  "containingMethodsNum" : 10,
  "sourceCodeText" : "void testWaitInterval(Configuration conf)\n{\r\n    conf.set(MRJobConfig.MR_JOB_END_NOTIFICATION_MAX_RETRY_INTERVAL, \"5000\");\r\n    conf.set(MRJobConfig.MR_JOB_END_RETRY_INTERVAL, \"1000\");\r\n    setConf(conf);\r\n    Assert.assertTrue(\"Expected waitInterval to be 1000, but was \" + waitInterval, waitInterval == 1000);\r\n    conf.set(MRJobConfig.MR_JOB_END_RETRY_INTERVAL, \"10000\");\r\n    setConf(conf);\r\n    Assert.assertTrue(\"Expected waitInterval to be 5000, but was \" + waitInterval, waitInterval == 5000);\r\n    conf.set(MRJobConfig.MR_JOB_END_RETRY_INTERVAL, \"-10\");\r\n    setConf(conf);\r\n    Assert.assertTrue(\"Expected waitInterval to be 5000, but was \" + waitInterval, waitInterval == 5000);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app",
  "methodName" : "testTimeout",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void testTimeout(Configuration conf)\n{\r\n    conf.set(MRJobConfig.MR_JOB_END_NOTIFICATION_TIMEOUT, \"1000\");\r\n    setConf(conf);\r\n    Assert.assertTrue(\"Expected timeout to be 1000, but was \" + timeout, timeout == 1000);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app",
  "methodName" : "testProxyConfiguration",
  "errType" : null,
  "containingMethodsNum" : 18,
  "sourceCodeText" : "void testProxyConfiguration(Configuration conf)\n{\r\n    conf.set(MRJobConfig.MR_JOB_END_NOTIFICATION_PROXY, \"somehost\");\r\n    setConf(conf);\r\n    Assert.assertTrue(\"Proxy shouldn't be set because port wasn't specified\", proxyToUse.type() == Proxy.Type.DIRECT);\r\n    conf.set(MRJobConfig.MR_JOB_END_NOTIFICATION_PROXY, \"somehost:someport\");\r\n    setConf(conf);\r\n    Assert.assertTrue(\"Proxy shouldn't be set because port wasn't numeric\", proxyToUse.type() == Proxy.Type.DIRECT);\r\n    conf.set(MRJobConfig.MR_JOB_END_NOTIFICATION_PROXY, \"somehost:1000\");\r\n    setConf(conf);\r\n    Assert.assertEquals(\"Proxy should have been set but wasn't \", \"HTTP @ somehost:1000\", proxyToUse.toString());\r\n    conf.set(MRJobConfig.MR_JOB_END_NOTIFICATION_PROXY, \"socks@somehost:1000\");\r\n    setConf(conf);\r\n    Assert.assertEquals(\"Proxy should have been socks but wasn't \", \"SOCKS @ somehost:1000\", proxyToUse.toString());\r\n    conf.set(MRJobConfig.MR_JOB_END_NOTIFICATION_PROXY, \"SOCKS@somehost:1000\");\r\n    setConf(conf);\r\n    Assert.assertEquals(\"Proxy should have been socks but wasn't \", \"SOCKS @ somehost:1000\", proxyToUse.toString());\r\n    conf.set(MRJobConfig.MR_JOB_END_NOTIFICATION_PROXY, \"sfafn@somehost:1000\");\r\n    setConf(conf);\r\n    Assert.assertEquals(\"Proxy should have been http but wasn't \", \"HTTP @ somehost:1000\", proxyToUse.toString());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app",
  "methodName" : "checkConfiguration",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void checkConfiguration()\n{\r\n    Configuration conf = new Configuration();\r\n    testNumRetries(conf);\r\n    testWaitInterval(conf);\r\n    testTimeout(conf);\r\n    testProxyConfiguration(conf);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app",
  "methodName" : "notifyURLOnce",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "boolean notifyURLOnce()\n{\r\n    boolean success = super.notifyURLOnce();\r\n    notificationCount++;\r\n    return success;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app",
  "methodName" : "testNotifyRetries",
  "errType" : null,
  "containingMethodsNum" : 22,
  "sourceCodeText" : "void testNotifyRetries() throws InterruptedException\n{\r\n    JobConf conf = new JobConf();\r\n    conf.set(MRJobConfig.MR_JOB_END_RETRY_ATTEMPTS, \"0\");\r\n    conf.set(MRJobConfig.MR_JOB_END_NOTIFICATION_MAX_ATTEMPTS, \"1\");\r\n    conf.set(MRJobConfig.MR_JOB_END_NOTIFICATION_URL, \"http://nonexistent\");\r\n    conf.set(MRJobConfig.MR_JOB_END_RETRY_INTERVAL, \"5000\");\r\n    conf.set(MRJobConfig.MR_JOB_END_NOTIFICATION_MAX_RETRY_INTERVAL, \"5000\");\r\n    JobReport jobReport = mock(JobReport.class);\r\n    long startTime = System.currentTimeMillis();\r\n    this.notificationCount = 0;\r\n    this.setConf(conf);\r\n    this.notify(jobReport);\r\n    long endTime = System.currentTimeMillis();\r\n    Assert.assertEquals(\"Only 1 try was expected but was : \" + this.notificationCount, 1, this.notificationCount);\r\n    Assert.assertTrue(\"Should have taken more than 5 seconds it took \" + (endTime - startTime), endTime - startTime > 5000);\r\n    conf.set(MRJobConfig.MR_JOB_END_NOTIFICATION_MAX_ATTEMPTS, \"3\");\r\n    conf.set(MRJobConfig.MR_JOB_END_RETRY_ATTEMPTS, \"3\");\r\n    conf.set(MRJobConfig.MR_JOB_END_RETRY_INTERVAL, \"3000\");\r\n    conf.set(MRJobConfig.MR_JOB_END_NOTIFICATION_MAX_RETRY_INTERVAL, \"3000\");\r\n    startTime = System.currentTimeMillis();\r\n    this.notificationCount = 0;\r\n    this.setConf(conf);\r\n    this.notify(jobReport);\r\n    endTime = System.currentTimeMillis();\r\n    Assert.assertEquals(\"Only 3 retries were expected but was : \" + this.notificationCount, 3, this.notificationCount);\r\n    Assert.assertTrue(\"Should have taken more than 9 seconds it took \" + (endTime - startTime), endTime - startTime > 9000);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app",
  "methodName" : "testNotificationOnLastRetry",
  "errType" : null,
  "containingMethodsNum" : 13,
  "sourceCodeText" : "void testNotificationOnLastRetry(boolean withRuntimeException) throws Exception\n{\r\n    HttpServer2 server = startHttpServer();\r\n    MRApp app = spy(new MRAppWithCustomContainerAllocator(2, 2, true, this.getClass().getName(), true, 2, true));\r\n    doNothing().when(app).sysexit();\r\n    JobConf conf = new JobConf();\r\n    conf.set(JobContext.MR_JOB_END_NOTIFICATION_URL, JobEndServlet.baseUrl + \"jobend?jobid=$jobId&status=$jobStatus\");\r\n    JobImpl job = (JobImpl) app.submit(conf);\r\n    app.waitForInternalState(job, JobStateInternal.SUCCEEDED);\r\n    if (withRuntimeException) {\r\n        YarnRuntimeException runtimeException = new YarnRuntimeException(new ClosedChannelException());\r\n        doThrow(runtimeException).when(app).stop();\r\n    }\r\n    app.shutDownJob();\r\n    Assert.assertTrue(app.isLastAMRetry());\r\n    Assert.assertEquals(1, JobEndServlet.calledTimes);\r\n    Assert.assertEquals(\"jobid=\" + job.getID() + \"&status=SUCCEEDED\", JobEndServlet.requestUri.getQuery());\r\n    Assert.assertEquals(JobState.SUCCEEDED.toString(), JobEndServlet.foundJobState);\r\n    server.stop();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app",
  "methodName" : "testNotificationOnLastRetryNormalShutdown",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testNotificationOnLastRetryNormalShutdown() throws Exception\n{\r\n    testNotificationOnLastRetry(false);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app",
  "methodName" : "testNotificationOnLastRetryShutdownWithRuntimeException",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testNotificationOnLastRetryShutdownWithRuntimeException() throws Exception\n{\r\n    testNotificationOnLastRetry(true);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app",
  "methodName" : "testAbsentNotificationOnNotLastRetryUnregistrationFailure",
  "errType" : null,
  "containingMethodsNum" : 15,
  "sourceCodeText" : "void testAbsentNotificationOnNotLastRetryUnregistrationFailure() throws Exception\n{\r\n    HttpServer2 server = startHttpServer();\r\n    MRApp app = spy(new MRAppWithCustomContainerAllocator(2, 2, false, this.getClass().getName(), true, 1, false));\r\n    doNothing().when(app).sysexit();\r\n    JobConf conf = new JobConf();\r\n    conf.set(JobContext.MR_JOB_END_NOTIFICATION_URL, JobEndServlet.baseUrl + \"jobend?jobid=$jobId&status=$jobStatus\");\r\n    JobImpl job = (JobImpl) app.submit(conf);\r\n    app.waitForState(job, JobState.RUNNING);\r\n    app.getContext().getEventHandler().handle(new JobEvent(app.getJobId(), JobEventType.JOB_AM_REBOOT));\r\n    app.waitForInternalState(job, JobStateInternal.REBOOT);\r\n    app.shutDownJob();\r\n    app.waitForState(job, JobState.RUNNING);\r\n    Assert.assertFalse(app.isLastAMRetry());\r\n    Assert.assertEquals(0, JobEndServlet.calledTimes);\r\n    Assert.assertNull(JobEndServlet.requestUri);\r\n    Assert.assertNull(JobEndServlet.foundJobState);\r\n    server.stop();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app",
  "methodName" : "testNotificationOnLastRetryUnregistrationFailure",
  "errType" : null,
  "containingMethodsNum" : 14,
  "sourceCodeText" : "void testNotificationOnLastRetryUnregistrationFailure() throws Exception\n{\r\n    HttpServer2 server = startHttpServer();\r\n    MRApp app = spy(new MRAppWithCustomContainerAllocator(2, 2, false, this.getClass().getName(), true, 2, false));\r\n    app.isLastAMRetry = true;\r\n    doNothing().when(app).sysexit();\r\n    JobConf conf = new JobConf();\r\n    conf.set(JobContext.MR_JOB_END_NOTIFICATION_URL, JobEndServlet.baseUrl + \"jobend?jobid=$jobId&status=$jobStatus\");\r\n    JobImpl job = (JobImpl) app.submit(conf);\r\n    app.waitForState(job, JobState.RUNNING);\r\n    app.getContext().getEventHandler().handle(new JobEvent(app.getJobId(), JobEventType.JOB_AM_REBOOT));\r\n    app.waitForInternalState(job, JobStateInternal.REBOOT);\r\n    app.waitForServiceToStop(10000);\r\n    Assert.assertFalse(app.isLastAMRetry());\r\n    Assert.assertEquals(0, JobEndServlet.calledTimes);\r\n    Assert.assertNull(JobEndServlet.requestUri);\r\n    Assert.assertNull(JobEndServlet.foundJobState);\r\n    server.stop();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app",
  "methodName" : "testCustomNotifierClass",
  "errType" : null,
  "containingMethodsNum" : 10,
  "sourceCodeText" : "void testCustomNotifierClass() throws InterruptedException\n{\r\n    JobConf conf = new JobConf();\r\n    conf.set(MRJobConfig.MR_JOB_END_NOTIFICATION_URL, \"http://example.com?jobId=$jobId&jobStatus=$jobStatus\");\r\n    conf.set(MRJobConfig.MR_JOB_END_NOTIFICATION_CUSTOM_NOTIFIER_CLASS, CustomNotifier.class.getName());\r\n    this.setConf(conf);\r\n    JobReport jobReport = mock(JobReport.class);\r\n    JobId jobId = mock(JobId.class);\r\n    when(jobId.toString()).thenReturn(\"mock-Id\");\r\n    when(jobReport.getJobId()).thenReturn(jobId);\r\n    when(jobReport.getJobState()).thenReturn(JobState.SUCCEEDED);\r\n    CustomNotifier.urlToNotify = null;\r\n    this.notify(jobReport);\r\n    final URL urlToNotify = CustomNotifier.urlToNotify;\r\n    Assert.assertEquals(\"http://example.com?jobId=mock-Id&jobStatus=SUCCEEDED\", urlToNotify.toString());\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app",
  "methodName" : "startHttpServer",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "HttpServer2 startHttpServer() throws Exception\n{\r\n    new File(System.getProperty(\"build.webapps\", \"build/webapps\") + \"/test\").mkdirs();\r\n    HttpServer2 server = new HttpServer2.Builder().setName(\"test\").addEndpoint(URI.create(\"http://localhost:0\")).setFindPort(true).build();\r\n    server.addServlet(\"jobend\", \"/jobend\", JobEndServlet.class);\r\n    server.start();\r\n    JobEndServlet.calledTimes = 0;\r\n    JobEndServlet.requestUri = null;\r\n    JobEndServlet.baseUrl = \"http://localhost:\" + server.getConnectorAddress(0).getPort() + \"/\";\r\n    JobEndServlet.foundJobState = null;\r\n    return server;\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\rm",
  "methodName" : "testRMContainerAllocatorExceptionIsHandled",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void testRMContainerAllocatorExceptionIsHandled() throws Exception\n{\r\n    ClientService mockClientService = mock(ClientService.class);\r\n    AppContext mockContext = mock(AppContext.class);\r\n    MockRMCommunicator mockRMCommunicator = new MockRMCommunicator(mockClientService, mockContext);\r\n    RMCommunicator communicator = spy(mockRMCommunicator);\r\n    Clock mockClock = mock(Clock.class);\r\n    when(mockContext.getClock()).thenReturn(mockClock);\r\n    doThrow(new RMContainerAllocationException(\"Test\")).doNothing().when(communicator).heartbeat();\r\n    when(mockClock.getTime()).thenReturn(1L).thenThrow(new AssertionError(\"GetClock called second time, when it should not have since the \" + \"thread should have quit\"));\r\n    AllocatorRunnable testRunnable = communicator.new AllocatorRunnable();\r\n    testRunnable.run();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\rm",
  "methodName" : "testRMContainerAllocatorYarnRuntimeExceptionIsHandled",
  "errType" : null,
  "containingMethodsNum" : 9,
  "sourceCodeText" : "void testRMContainerAllocatorYarnRuntimeExceptionIsHandled() throws Exception\n{\r\n    ClientService mockClientService = mock(ClientService.class);\r\n    AppContext mockContext = mock(AppContext.class);\r\n    MockRMCommunicator mockRMCommunicator = new MockRMCommunicator(mockClientService, mockContext);\r\n    final RMCommunicator communicator = spy(mockRMCommunicator);\r\n    Clock mockClock = mock(Clock.class);\r\n    when(mockContext.getClock()).thenReturn(mockClock);\r\n    doThrow(new YarnRuntimeException(\"Test\")).doNothing().when(communicator).heartbeat();\r\n    when(mockClock.getTime()).thenReturn(1L).thenAnswer((Answer<Long>) invocation -> {\r\n        communicator.stop();\r\n        return 2L;\r\n    }).thenThrow(new AssertionError(\"GetClock called second time, when it should not \" + \"have since the thread should have quit\"));\r\n    AllocatorRunnable testRunnable = communicator.new AllocatorRunnable();\r\n    testRunnable.run();\r\n    verify(mockClock, times(2)).getTime();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app",
  "methodName" : "setup",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void setup() throws AccessControlException, FileNotFoundException, IllegalArgumentException, IOException\n{\r\n    DefaultMetricsSystem.setMiniClusterMode(true);\r\n    File dir = new File(stagingDir);\r\n    stagingDir = dir.getAbsolutePath();\r\n    localFS = FileContext.getLocalFSFileContext();\r\n    localFS.delete(testDir, true);\r\n    new File(testDir.toString()).mkdir();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app",
  "methodName" : "prepare",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void prepare() throws IOException\n{\r\n    File dir = new File(stagingDir);\r\n    if (dir.exists()) {\r\n        FileUtils.deleteDirectory(dir);\r\n    }\r\n    dir.mkdirs();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app",
  "methodName" : "cleanup",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void cleanup() throws IOException\n{\r\n    localFS.delete(testDir, true);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app",
  "methodName" : "testMRAppMasterForDifferentUser",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void testMRAppMasterForDifferentUser() throws IOException, InterruptedException\n{\r\n    String applicationAttemptIdStr = \"appattempt_1317529182569_0004_000001\";\r\n    String containerIdStr = \"container_1317529182569_0004_000001_1\";\r\n    String userName = \"TestAppMasterUser\";\r\n    ApplicationAttemptId applicationAttemptId = ApplicationAttemptId.fromString(applicationAttemptIdStr);\r\n    ContainerId containerId = ContainerId.fromString(containerIdStr);\r\n    MRAppMasterTest appMaster = new MRAppMasterTest(applicationAttemptId, containerId, \"host\", -1, -1, System.currentTimeMillis());\r\n    JobConf conf = new JobConf();\r\n    conf.set(MRJobConfig.MR_AM_STAGING_DIR, stagingDir);\r\n    MRAppMaster.initAndStartAppMaster(appMaster, conf, userName);\r\n    Path userPath = new Path(stagingDir, userName);\r\n    Path userStagingPath = new Path(userPath, \".staging\");\r\n    assertEquals(userStagingPath.toString(), appMaster.stagingDirPath.toString());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app",
  "methodName" : "testMRAppMasterMidLock",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 16,
  "sourceCodeText" : "void testMRAppMasterMidLock() throws IOException, InterruptedException\n{\r\n    String applicationAttemptIdStr = \"appattempt_1317529182569_0004_000002\";\r\n    String containerIdStr = \"container_1317529182569_0004_000002_1\";\r\n    String userName = \"TestAppMasterUser\";\r\n    JobConf conf = new JobConf();\r\n    conf.set(MRJobConfig.MR_AM_STAGING_DIR, stagingDir);\r\n    conf.setInt(org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.FILEOUTPUTCOMMITTER_ALGORITHM_VERSION, 1);\r\n    ApplicationAttemptId applicationAttemptId = ApplicationAttemptId.fromString(applicationAttemptIdStr);\r\n    JobId jobId = TypeConverter.toYarn(TypeConverter.fromYarn(applicationAttemptId.getApplicationId()));\r\n    Path start = MRApps.getStartJobCommitFile(conf, userName, jobId);\r\n    FileSystem fs = FileSystem.get(conf);\r\n    fs.create(start).close();\r\n    ContainerId containerId = ContainerId.fromString(containerIdStr);\r\n    MRAppMaster appMaster = new MRAppMasterTest(applicationAttemptId, containerId, \"host\", -1, -1, System.currentTimeMillis(), false, false);\r\n    boolean caught = false;\r\n    try {\r\n        MRAppMaster.initAndStartAppMaster(appMaster, conf, userName);\r\n    } catch (IOException e) {\r\n        LOG.info(\"Caught expected Exception\", e);\r\n        caught = true;\r\n    }\r\n    assertTrue(caught);\r\n    assertTrue(appMaster.errorHappenedShutDown);\r\n    assertEquals(JobStateInternal.ERROR, appMaster.forcedState);\r\n    appMaster.stop();\r\n    verifyFailedStatus((MRAppMasterTest) appMaster, \"FAILED\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app",
  "methodName" : "testMRAppMasterJobLaunchTime",
  "errType" : null,
  "containingMethodsNum" : 18,
  "sourceCodeText" : "void testMRAppMasterJobLaunchTime() throws IOException, InterruptedException\n{\r\n    String applicationAttemptIdStr = \"appattempt_1317529182569_0004_000002\";\r\n    String containerIdStr = \"container_1317529182569_0004_000002_1\";\r\n    String userName = \"TestAppMasterUser\";\r\n    JobConf conf = new JobConf();\r\n    conf.set(MRJobConfig.MR_AM_STAGING_DIR, stagingDir);\r\n    conf.setInt(MRJobConfig.NUM_REDUCES, 0);\r\n    conf.set(JHAdminConfig.MR_HS_JHIST_FORMAT, \"json\");\r\n    ApplicationAttemptId applicationAttemptId = ApplicationAttemptId.fromString(applicationAttemptIdStr);\r\n    JobId jobId = TypeConverter.toYarn(TypeConverter.fromYarn(applicationAttemptId.getApplicationId()));\r\n    File dir = new File(MRApps.getStagingAreaDir(conf, userName).toString(), jobId.toString());\r\n    dir.mkdirs();\r\n    File historyFile = new File(JobHistoryUtils.getStagingJobHistoryFile(new Path(dir.toURI().toString()), jobId, (applicationAttemptId.getAttemptId() - 1)).toUri().getRawPath());\r\n    historyFile.createNewFile();\r\n    FSDataOutputStream out = new FSDataOutputStream(new FileOutputStream(historyFile), null);\r\n    EventWriter writer = new EventWriter(out, EventWriter.WriteMode.JSON);\r\n    writer.close();\r\n    FileSystem fs = FileSystem.get(conf);\r\n    JobSplitWriter.createSplitFiles(new Path(dir.getAbsolutePath()), conf, fs, new org.apache.hadoop.mapred.InputSplit[0]);\r\n    ContainerId containerId = ContainerId.fromString(containerIdStr);\r\n    MRAppMasterTestLaunchTime appMaster = new MRAppMasterTestLaunchTime(applicationAttemptId, containerId, \"host\", -1, -1, System.currentTimeMillis());\r\n    MRAppMaster.initAndStartAppMaster(appMaster, conf, userName);\r\n    appMaster.stop();\r\n    assertTrue(\"Job launch time should not be negative.\", appMaster.jobLaunchTime.get() >= 0);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app",
  "methodName" : "testMRAppMasterSuccessLock",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 17,
  "sourceCodeText" : "void testMRAppMasterSuccessLock() throws IOException, InterruptedException\n{\r\n    String applicationAttemptIdStr = \"appattempt_1317529182569_0004_000002\";\r\n    String containerIdStr = \"container_1317529182569_0004_000002_1\";\r\n    String userName = \"TestAppMasterUser\";\r\n    JobConf conf = new JobConf();\r\n    conf.set(MRJobConfig.MR_AM_STAGING_DIR, stagingDir);\r\n    ApplicationAttemptId applicationAttemptId = ApplicationAttemptId.fromString(applicationAttemptIdStr);\r\n    JobId jobId = TypeConverter.toYarn(TypeConverter.fromYarn(applicationAttemptId.getApplicationId()));\r\n    Path start = MRApps.getStartJobCommitFile(conf, userName, jobId);\r\n    Path end = MRApps.getEndJobCommitSuccessFile(conf, userName, jobId);\r\n    FileSystem fs = FileSystem.get(conf);\r\n    fs.create(start).close();\r\n    fs.create(end).close();\r\n    ContainerId containerId = ContainerId.fromString(containerIdStr);\r\n    MRAppMaster appMaster = new MRAppMasterTest(applicationAttemptId, containerId, \"host\", -1, -1, System.currentTimeMillis(), false, false);\r\n    boolean caught = false;\r\n    try {\r\n        MRAppMaster.initAndStartAppMaster(appMaster, conf, userName);\r\n    } catch (IOException e) {\r\n        LOG.info(\"Caught expected Exception\", e);\r\n        caught = true;\r\n    }\r\n    assertTrue(caught);\r\n    assertTrue(appMaster.errorHappenedShutDown);\r\n    assertEquals(JobStateInternal.SUCCEEDED, appMaster.forcedState);\r\n    appMaster.stop();\r\n    verifyFailedStatus((MRAppMasterTest) appMaster, \"SUCCEEDED\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app",
  "methodName" : "testMRAppMasterFailLock",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 17,
  "sourceCodeText" : "void testMRAppMasterFailLock() throws IOException, InterruptedException\n{\r\n    String applicationAttemptIdStr = \"appattempt_1317529182569_0004_000002\";\r\n    String containerIdStr = \"container_1317529182569_0004_000002_1\";\r\n    String userName = \"TestAppMasterUser\";\r\n    JobConf conf = new JobConf();\r\n    conf.set(MRJobConfig.MR_AM_STAGING_DIR, stagingDir);\r\n    ApplicationAttemptId applicationAttemptId = ApplicationAttemptId.fromString(applicationAttemptIdStr);\r\n    JobId jobId = TypeConverter.toYarn(TypeConverter.fromYarn(applicationAttemptId.getApplicationId()));\r\n    Path start = MRApps.getStartJobCommitFile(conf, userName, jobId);\r\n    Path end = MRApps.getEndJobCommitFailureFile(conf, userName, jobId);\r\n    FileSystem fs = FileSystem.get(conf);\r\n    fs.create(start).close();\r\n    fs.create(end).close();\r\n    ContainerId containerId = ContainerId.fromString(containerIdStr);\r\n    MRAppMaster appMaster = new MRAppMasterTest(applicationAttemptId, containerId, \"host\", -1, -1, System.currentTimeMillis(), false, false);\r\n    boolean caught = false;\r\n    try {\r\n        MRAppMaster.initAndStartAppMaster(appMaster, conf, userName);\r\n    } catch (IOException e) {\r\n        LOG.info(\"Caught expected Exception\", e);\r\n        caught = true;\r\n    }\r\n    assertTrue(caught);\r\n    assertTrue(appMaster.errorHappenedShutDown);\r\n    assertEquals(JobStateInternal.FAILED, appMaster.forcedState);\r\n    appMaster.stop();\r\n    verifyFailedStatus((MRAppMasterTest) appMaster, \"FAILED\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app",
  "methodName" : "testMRAppMasterMissingStaging",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 12,
  "sourceCodeText" : "void testMRAppMasterMissingStaging() throws IOException, InterruptedException\n{\r\n    String applicationAttemptIdStr = \"appattempt_1317529182569_0004_000002\";\r\n    String containerIdStr = \"container_1317529182569_0004_000002_1\";\r\n    String userName = \"TestAppMasterUser\";\r\n    JobConf conf = new JobConf();\r\n    conf.set(MRJobConfig.MR_AM_STAGING_DIR, stagingDir);\r\n    ApplicationAttemptId applicationAttemptId = ApplicationAttemptId.fromString(applicationAttemptIdStr);\r\n    File dir = new File(stagingDir);\r\n    if (dir.exists()) {\r\n        FileUtils.deleteDirectory(dir);\r\n    }\r\n    ContainerId containerId = ContainerId.fromString(containerIdStr);\r\n    MRAppMaster appMaster = new MRAppMasterTest(applicationAttemptId, containerId, \"host\", -1, -1, System.currentTimeMillis(), false, false);\r\n    boolean caught = false;\r\n    try {\r\n        MRAppMaster.initAndStartAppMaster(appMaster, conf, userName);\r\n    } catch (IOException e) {\r\n        LOG.info(\"Caught expected Exception\", e);\r\n        caught = true;\r\n    }\r\n    assertTrue(caught);\r\n    assertTrue(appMaster.errorHappenedShutDown);\r\n    assertEquals(JobStateInternal.ERROR, appMaster.forcedState);\r\n    appMaster.stop();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app",
  "methodName" : "testMRAppMasterMaxAppAttempts",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void testMRAppMasterMaxAppAttempts() throws IOException, InterruptedException\n{\r\n    Boolean[] expectedBools = new Boolean[] { false, false, false };\r\n    String applicationAttemptIdStr = \"appattempt_1317529182569_0004_000002\";\r\n    String containerIdStr = \"container_1317529182569_0004_000002_1\";\r\n    String userName = \"TestAppMasterUser\";\r\n    ApplicationAttemptId applicationAttemptId = ApplicationAttemptId.fromString(applicationAttemptIdStr);\r\n    ContainerId containerId = ContainerId.fromString(containerIdStr);\r\n    JobConf conf = new JobConf();\r\n    conf.set(MRJobConfig.MR_AM_STAGING_DIR, stagingDir);\r\n    File stagingDir = new File(MRApps.getStagingAreaDir(conf, userName).toString());\r\n    stagingDir.mkdirs();\r\n    for (int i = 0; i < expectedBools.length; ++i) {\r\n        MRAppMasterTest appMaster = new MRAppMasterTest(applicationAttemptId, containerId, \"host\", -1, -1, System.currentTimeMillis(), false, true);\r\n        MRAppMaster.initAndStartAppMaster(appMaster, conf, userName);\r\n        assertEquals(\"isLastAMRetry is correctly computed.\", expectedBools[i], appMaster.isLastAMRetry());\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app",
  "methodName" : "setNewEnvironmentHack",
  "errType" : [ "NoSuchFieldException" ],
  "containingMethodsNum" : 19,
  "sourceCodeText" : "void setNewEnvironmentHack(Map<String, String> newenv) throws Exception\n{\r\n    try {\r\n        Class<?> cl = Class.forName(\"java.lang.ProcessEnvironment\");\r\n        Field field = cl.getDeclaredField(\"theEnvironment\");\r\n        field.setAccessible(true);\r\n        Map<String, String> env = (Map<String, String>) field.get(null);\r\n        env.clear();\r\n        env.putAll(newenv);\r\n        Field ciField = cl.getDeclaredField(\"theCaseInsensitiveEnvironment\");\r\n        ciField.setAccessible(true);\r\n        Map<String, String> cienv = (Map<String, String>) ciField.get(null);\r\n        cienv.clear();\r\n        cienv.putAll(newenv);\r\n    } catch (NoSuchFieldException e) {\r\n        Class[] classes = Collections.class.getDeclaredClasses();\r\n        Map<String, String> env = System.getenv();\r\n        for (Class cl : classes) {\r\n            if (\"java.util.Collections$UnmodifiableMap\".equals(cl.getName())) {\r\n                Field field = cl.getDeclaredField(\"m\");\r\n                field.setAccessible(true);\r\n                Object obj = field.get(env);\r\n                Map<String, String> map = (Map<String, String>) obj;\r\n                map.clear();\r\n                map.putAll(newenv);\r\n            }\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app",
  "methodName" : "testMRAppMasterCredentials",
  "errType" : null,
  "containingMethodsNum" : 40,
  "sourceCodeText" : "void testMRAppMasterCredentials() throws Exception\n{\r\n    GenericTestUtils.setRootLogLevel(Level.DEBUG);\r\n    Credentials credentials = new Credentials();\r\n    byte[] identifier = \"MyIdentifier\".getBytes();\r\n    byte[] password = \"MyPassword\".getBytes();\r\n    Text kind = new Text(\"MyTokenKind\");\r\n    Text service = new Text(\"host:port\");\r\n    Token<? extends TokenIdentifier> myToken = new Token<TokenIdentifier>(identifier, password, kind, service);\r\n    Text tokenAlias = new Text(\"myToken\");\r\n    credentials.addToken(tokenAlias, myToken);\r\n    Text appTokenService = new Text(\"localhost:0\");\r\n    Token<AMRMTokenIdentifier> appToken = new Token<AMRMTokenIdentifier>(identifier, password, AMRMTokenIdentifier.KIND_NAME, appTokenService);\r\n    credentials.addToken(appTokenService, appToken);\r\n    Text keyAlias = new Text(\"mySecretKeyAlias\");\r\n    credentials.addSecretKey(keyAlias, \"mySecretKey\".getBytes());\r\n    Token<? extends TokenIdentifier> storedToken = credentials.getToken(tokenAlias);\r\n    JobConf conf = new JobConf();\r\n    Path tokenFilePath = new Path(testDir, \"tokens-file\");\r\n    Map<String, String> newEnv = new HashMap<String, String>();\r\n    newEnv.put(UserGroupInformation.HADOOP_TOKEN_FILE_LOCATION, tokenFilePath.toUri().getPath());\r\n    setNewEnvironmentHack(newEnv);\r\n    credentials.writeTokenStorageFile(tokenFilePath, conf);\r\n    ApplicationId appId = ApplicationId.newInstance(12345, 56);\r\n    ApplicationAttemptId applicationAttemptId = ApplicationAttemptId.newInstance(appId, 1);\r\n    ContainerId containerId = ContainerId.newContainerId(applicationAttemptId, 546);\r\n    String userName = UserGroupInformation.getCurrentUser().getShortUserName();\r\n    File stagingDir = new File(MRApps.getStagingAreaDir(conf, userName).toString());\r\n    stagingDir.mkdirs();\r\n    UserGroupInformation.setLoginUser(null);\r\n    MRAppMasterTest appMaster = new MRAppMasterTest(applicationAttemptId, containerId, \"host\", -1, -1, System.currentTimeMillis(), false, true);\r\n    MRAppMaster.initAndStartAppMaster(appMaster, conf, userName);\r\n    Credentials appMasterCreds = appMaster.getCredentials();\r\n    Assert.assertNotNull(appMasterCreds);\r\n    Assert.assertEquals(1, appMasterCreds.numberOfSecretKeys());\r\n    Assert.assertEquals(1, appMasterCreds.numberOfTokens());\r\n    Token<? extends TokenIdentifier> usedToken = appMasterCreds.getToken(tokenAlias);\r\n    Assert.assertNotNull(usedToken);\r\n    Assert.assertEquals(storedToken, usedToken);\r\n    byte[] usedKey = appMasterCreds.getSecretKey(keyAlias);\r\n    Assert.assertNotNull(usedKey);\r\n    Assert.assertEquals(\"mySecretKey\", new String(usedKey));\r\n    Credentials confCredentials = conf.getCredentials();\r\n    Assert.assertEquals(1, confCredentials.numberOfSecretKeys());\r\n    Assert.assertEquals(1, confCredentials.numberOfTokens());\r\n    Assert.assertEquals(storedToken, confCredentials.getToken(tokenAlias));\r\n    Assert.assertEquals(\"mySecretKey\", new String(confCredentials.getSecretKey(keyAlias)));\r\n    Credentials ugiCredentials = appMaster.getUgi().getCredentials();\r\n    Assert.assertEquals(1, ugiCredentials.numberOfSecretKeys());\r\n    Assert.assertEquals(2, ugiCredentials.numberOfTokens());\r\n    Assert.assertEquals(storedToken, ugiCredentials.getToken(tokenAlias));\r\n    Assert.assertEquals(appToken, ugiCredentials.getToken(appTokenService));\r\n    Assert.assertEquals(\"mySecretKey\", new String(ugiCredentials.getSecretKey(keyAlias)));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app",
  "methodName" : "testMRAppMasterShutDownJob",
  "errType" : null,
  "containingMethodsNum" : 17,
  "sourceCodeText" : "void testMRAppMasterShutDownJob() throws Exception, InterruptedException\n{\r\n    String applicationAttemptIdStr = \"appattempt_1317529182569_0004_000002\";\r\n    String containerIdStr = \"container_1317529182569_0004_000002_1\";\r\n    String userName = \"TestAppMasterUser\";\r\n    ApplicationAttemptId applicationAttemptId = ApplicationAttemptId.fromString(applicationAttemptIdStr);\r\n    ContainerId containerId = ContainerId.fromString(containerIdStr);\r\n    JobConf conf = new JobConf();\r\n    conf.set(MRJobConfig.MR_AM_STAGING_DIR, stagingDir);\r\n    File stagingDir = new File(MRApps.getStagingAreaDir(conf, userName).toString());\r\n    stagingDir.mkdirs();\r\n    MRAppMasterTest appMaster = spy(new MRAppMasterTest(applicationAttemptId, containerId, \"host\", -1, -1, System.currentTimeMillis(), false, true));\r\n    MRAppMaster.initAndStartAppMaster(appMaster, conf, userName);\r\n    doReturn(conf).when(appMaster).getConfig();\r\n    appMaster.isLastAMRetry = true;\r\n    doNothing().when(appMaster).serviceStop();\r\n    appMaster.shutDownJob();\r\n    Assert.assertTrue(\"Expected shutDownJob to terminate.\", ExitUtil.terminateCalled());\r\n    Assert.assertEquals(\"Expected shutDownJob to exit with status code of 0.\", 0, ExitUtil.getFirstExitException().status);\r\n    ExitUtil.resetFirstExitException();\r\n    String msg = \"Injected Exception\";\r\n    doThrow(new RuntimeException(msg)).when(appMaster).notifyIsLastAMRetry(anyBoolean());\r\n    appMaster.shutDownJob();\r\n    assertTrue(\"Expected message from ExitUtil.ExitException to be \" + msg, ExitUtil.getFirstExitException().getMessage().contains(msg));\r\n    Assert.assertEquals(\"Expected shutDownJob to exit with status code of 1.\", 1, ExitUtil.getFirstExitException().status);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app",
  "methodName" : "verifyFailedStatus",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void verifyFailedStatus(MRAppMasterTest appMaster, String expectedJobState)\n{\r\n    ArgumentCaptor<JobHistoryEvent> captor = ArgumentCaptor.forClass(JobHistoryEvent.class);\r\n    verify(appMaster.spyHistoryService, times(2)).handleEvent(captor.capture());\r\n    HistoryEvent event = captor.getValue().getHistoryEvent();\r\n    assertTrue(event instanceof JobUnsuccessfulCompletionEvent);\r\n    assertThat(((JobUnsuccessfulCompletionEvent) event).getStatus()).isEqualTo(expectedJobState);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app",
  "methodName" : "serviceInit",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void serviceInit(Configuration conf) throws Exception\n{\r\n    if (!overrideInit) {\r\n        super.serviceInit(conf);\r\n    }\r\n    this.conf = conf;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app",
  "methodName" : "createContainerAllocator",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "ContainerAllocator createContainerAllocator(final ClientService clientService, final AppContext context)\n{\r\n    return mockContainerAllocator;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app",
  "methodName" : "createCommitterEventHandler",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "EventHandler<CommitterEvent> createCommitterEventHandler(AppContext context, OutputCommitter committer)\n{\r\n    return mockCommitterEventHandler;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app",
  "methodName" : "getRMHeartbeatHandler",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "RMHeartbeatHandler getRMHeartbeatHandler()\n{\r\n    return mockRMHeartbeatHandler;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app",
  "methodName" : "serviceStart",
  "errType" : [ "Exception" ],
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void serviceStart() throws Exception\n{\r\n    if (overrideStart) {\r\n        try {\r\n            UserGroupInformation ugi = UserGroupInformation.getCurrentUser();\r\n            String user = ugi.getShortUserName();\r\n            stagingDirPath = MRApps.getStagingAreaDir(conf, user);\r\n        } catch (Exception e) {\r\n            fail(e.getMessage());\r\n        }\r\n    } else {\r\n        super.serviceStart();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app",
  "methodName" : "getCredentials",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Credentials getCredentials()\n{\r\n    return super.getCredentials();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app",
  "methodName" : "getUgi",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "UserGroupInformation getUgi()\n{\r\n    return currentUser;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app",
  "methodName" : "createJobHistoryHandler",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "EventHandler<JobHistoryEvent> createJobHistoryHandler(AppContext context)\n{\r\n    spyHistoryService = Mockito.spy((JobHistoryEventHandler) super.createJobHistoryHandler(context));\r\n    spyHistoryService.setForcejobCompletion(this.isLastAMRetry);\r\n    return spyHistoryService;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app",
  "methodName" : "createCommitterEventHandler",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "EventHandler<CommitterEvent> createCommitterEventHandler(AppContext context, OutputCommitter committer)\n{\r\n    return new CommitterEventHandler(context, committer, getRMHeartbeatHandler()) {\r\n\r\n        @Override\r\n        public void handle(CommitterEvent event) {\r\n        }\r\n    };\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app",
  "methodName" : "createJobHistoryHandler",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "EventHandler<JobHistoryEvent> createJobHistoryHandler(AppContext context)\n{\r\n    return new JobHistoryEventHandler(context, getStartCount()) {\r\n\r\n        @Override\r\n        public void handle(JobHistoryEvent event) {\r\n            if (event.getHistoryEvent().getEventType() == EventType.JOB_INITED) {\r\n                JobInitedEvent jie = (JobInitedEvent) event.getHistoryEvent();\r\n                jobLaunchTime.set(jie.getLaunchTime());\r\n            }\r\n            super.handle(event);\r\n        }\r\n    };\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\api\\records",
  "methodName" : "testSetRawCounters",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void testSetRawCounters()\n{\r\n    TaskAttemptReport report = Records.newRecord(TaskAttemptReport.class);\r\n    org.apache.hadoop.mapreduce.Counters rCounters = MockJobs.newCounters();\r\n    report.setRawCounters(rCounters);\r\n    Counters counters = report.getCounters();\r\n    assertNotEquals(null, counters);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\api\\records",
  "methodName" : "testBuildImplicitRawCounters",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void testBuildImplicitRawCounters()\n{\r\n    TaskAttemptReportPBImpl report = new TaskAttemptReportPBImpl();\r\n    org.apache.hadoop.mapreduce.Counters rCounters = MockJobs.newCounters();\r\n    report.setRawCounters(rCounters);\r\n    MRProtos.TaskAttemptReportProto protoVal = report.getProto();\r\n    Counters counters = report.getCounters();\r\n    assertTrue(protoVal.hasCounters());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\api\\records",
  "methodName" : "testCountersOverRawCounters",
  "errType" : null,
  "containingMethodsNum" : 9,
  "sourceCodeText" : "void testCountersOverRawCounters()\n{\r\n    TaskAttemptReport report = Records.newRecord(TaskAttemptReport.class);\r\n    org.apache.hadoop.mapreduce.Counters rCounters = MockJobs.newCounters();\r\n    Counters altCounters = TypeConverter.toYarn(rCounters);\r\n    report.setRawCounters(rCounters);\r\n    report.setCounters(altCounters);\r\n    Counters counters = report.getCounters();\r\n    assertNotEquals(null, counters);\r\n    assertNotEquals(rCounters, altCounters);\r\n    assertEquals(counters, altCounters);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\api\\records",
  "methodName" : "testUninitializedCounters",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void testUninitializedCounters()\n{\r\n    TaskAttemptReport report = Records.newRecord(TaskAttemptReport.class);\r\n    assertThat(report.getCounters()).isNull();\r\n    assertThat(report.getRawCounters()).isNull();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\api\\records",
  "methodName" : "testSetRawCountersToNull",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void testSetRawCountersToNull()\n{\r\n    TaskAttemptReport report = Records.newRecord(TaskAttemptReport.class);\r\n    report.setRawCounters(null);\r\n    assertThat(report.getCounters()).isNull();\r\n    assertThat(report.getRawCounters()).isNull();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\api\\records",
  "methodName" : "testSetCountersToNull",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void testSetCountersToNull()\n{\r\n    TaskAttemptReport report = Records.newRecord(TaskAttemptReport.class);\r\n    report.setCounters(null);\r\n    assertThat(report.getCounters()).isNull();\r\n    assertThat(report.getRawCounters()).isNull();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\api\\records",
  "methodName" : "testSetNonNullCountersToNull",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void testSetNonNullCountersToNull()\n{\r\n    TaskAttemptReport report = Records.newRecord(TaskAttemptReport.class);\r\n    org.apache.hadoop.mapreduce.Counters rCounters = MockJobs.newCounters();\r\n    report.setRawCounters(rCounters);\r\n    Counters counters = report.getCounters();\r\n    assertNotEquals(null, counters);\r\n    report.setCounters(null);\r\n    assertThat(report.getCounters()).isNull();\r\n    assertThat(report.getRawCounters()).isNull();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\api\\records",
  "methodName" : "testSetNonNullRawCountersToNull",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void testSetNonNullRawCountersToNull()\n{\r\n    TaskAttemptReport report = Records.newRecord(TaskAttemptReport.class);\r\n    org.apache.hadoop.mapreduce.Counters rCounters = MockJobs.newCounters();\r\n    report.setRawCounters(rCounters);\r\n    Counters counters = report.getCounters();\r\n    assertNotEquals(null, counters);\r\n    report.setRawCounters(null);\r\n    assertThat(report.getCounters()).isNull();\r\n    assertThat(report.getRawCounters()).isNull();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "delete",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void delete(File dir) throws IOException\n{\r\n    Configuration conf = new Configuration();\r\n    FileSystem fs = FileSystem.getLocal(conf);\r\n    Path p = fs.makeQualified(new Path(dir.getAbsolutePath()));\r\n    fs.delete(p, true);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "setupTestDirs",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void setupTestDirs() throws IOException\n{\r\n    testWorkDir = new File(\"target\", TestLocalContainerLauncher.class.getCanonicalName());\r\n    testWorkDir.delete();\r\n    testWorkDir.mkdirs();\r\n    testWorkDir = testWorkDir.getAbsoluteFile();\r\n    for (int i = 0; i < localDirs.length; i++) {\r\n        final File dir = new File(testWorkDir, \"local-\" + i);\r\n        dir.mkdirs();\r\n        localDirs[i] = dir.toString();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "cleanupTestDirs",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void cleanupTestDirs() throws IOException\n{\r\n    if (testWorkDir != null) {\r\n        delete(testWorkDir);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "testKillJob",
  "errType" : null,
  "containingMethodsNum" : 32,
  "sourceCodeText" : "void testKillJob() throws Exception\n{\r\n    JobConf conf = new JobConf();\r\n    AppContext context = mock(AppContext.class);\r\n    final CountDownLatch isDone = new CountDownLatch(1);\r\n    EventHandler<Event> handler = new EventHandler<Event>() {\r\n\r\n        @Override\r\n        public void handle(Event event) {\r\n            LOG.info(\"handling event \" + event.getClass() + \" with type \" + event.getType());\r\n            if (event instanceof TaskAttemptEvent) {\r\n                if (event.getType() == TaskAttemptEventType.TA_CONTAINER_CLEANED) {\r\n                    isDone.countDown();\r\n                }\r\n            }\r\n        }\r\n    };\r\n    when(context.getEventHandler()).thenReturn(handler);\r\n    LocalContainerLauncher launcher = new LocalContainerLauncher(context, mock(TaskUmbilicalProtocol.class));\r\n    launcher.init(conf);\r\n    launcher.start();\r\n    JobId jobId = MRBuilderUtils.newJobId(System.currentTimeMillis(), 1, 1);\r\n    TaskId taskId = MRBuilderUtils.newTaskId(jobId, 1, TaskType.MAP);\r\n    TaskAttemptId taId = MRBuilderUtils.newTaskAttemptId(taskId, 0);\r\n    Job job = mock(Job.class);\r\n    when(job.getTotalMaps()).thenReturn(1);\r\n    when(job.getTotalReduces()).thenReturn(0);\r\n    Map<JobId, Job> jobs = new HashMap<JobId, Job>();\r\n    jobs.put(jobId, job);\r\n    when(context.getAllJobs()).thenReturn(jobs);\r\n    org.apache.hadoop.mapreduce.v2.app.job.Task ytask = mock(org.apache.hadoop.mapreduce.v2.app.job.Task.class);\r\n    when(ytask.getType()).thenReturn(TaskType.MAP);\r\n    when(job.getTask(taskId)).thenReturn(ytask);\r\n    MapTask mapTask = mock(MapTask.class);\r\n    when(mapTask.isMapOrReduce()).thenReturn(true);\r\n    when(mapTask.isMapTask()).thenReturn(true);\r\n    TaskAttemptID taskID = TypeConverter.fromYarn(taId);\r\n    when(mapTask.getTaskID()).thenReturn(taskID);\r\n    when(mapTask.getJobID()).thenReturn(taskID.getJobID());\r\n    doAnswer(new Answer<Void>() {\r\n\r\n        @Override\r\n        public Void answer(InvocationOnMock invocation) throws Throwable {\r\n            LOG.info(\"sleeping for 5 minutes...\");\r\n            Thread.sleep(5 * 60 * 1000);\r\n            return null;\r\n        }\r\n    }).when(mapTask).run(isA(JobConf.class), isA(TaskUmbilicalProtocol.class));\r\n    ContainerLauncherEvent launchEvent = new ContainerRemoteLaunchEvent(taId, null, createMockContainer(), mapTask);\r\n    launcher.handle(launchEvent);\r\n    Thread.sleep(200);\r\n    ContainerLauncherEvent cleanupEvent = new ContainerLauncherEvent(taId, null, null, null, ContainerLauncher.EventType.CONTAINER_REMOTE_CLEANUP);\r\n    launcher.handle(cleanupEvent);\r\n    isDone.await();\r\n    launcher.close();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "createMockContainer",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "Container createMockContainer()\n{\r\n    Container container = mock(Container.class);\r\n    NodeId nodeId = NodeId.newInstance(\"foo.bar.org\", 1234);\r\n    when(container.getNodeId()).thenReturn(nodeId);\r\n    return container;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "testRenameMapOutputForReduce",
  "errType" : null,
  "containingMethodsNum" : 14,
  "sourceCodeText" : "void testRenameMapOutputForReduce() throws Exception\n{\r\n    final JobConf conf = new JobConf();\r\n    final MROutputFiles mrOutputFiles = new MROutputFiles();\r\n    mrOutputFiles.setConf(conf);\r\n    conf.set(MRConfig.LOCAL_DIR, localDirs[0].toString());\r\n    final Path mapOut = mrOutputFiles.getOutputFileForWrite(1);\r\n    conf.set(MRConfig.LOCAL_DIR, localDirs[1].toString());\r\n    final Path mapOutIdx = mrOutputFiles.getOutputIndexFileForWrite(1);\r\n    Assert.assertNotEquals(\"Paths must be different!\", mapOut.getParent(), mapOutIdx.getParent());\r\n    conf.setStrings(MRConfig.LOCAL_DIR, localDirs);\r\n    final FileContext lfc = FileContext.getLocalFSFileContext(conf);\r\n    lfc.create(mapOut, EnumSet.of(CREATE)).close();\r\n    lfc.create(mapOutIdx, EnumSet.of(CREATE)).close();\r\n    final JobId jobId = MRBuilderUtils.newJobId(12345L, 1, 2);\r\n    final TaskId tid = MRBuilderUtils.newTaskId(jobId, 0, TaskType.MAP);\r\n    final TaskAttemptId taid = MRBuilderUtils.newTaskAttemptId(tid, 0);\r\n    LocalContainerLauncher.renameMapOutputForReduce(conf, taid, mrOutputFiles);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\webapp",
  "methodName" : "setJob",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setJob(Job job)\n{\r\n    super.setJob(job);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\webapp",
  "methodName" : "setTask",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setTask(Task task)\n{\r\n    super.setTask(task);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\webapp",
  "methodName" : "addParameter",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void addParameter(String name, String value)\n{\r\n    params.put(name, value);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\webapp",
  "methodName" : "url",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String url(String... parts)\n{\r\n    String result = \"url://\";\r\n    for (String string : parts) {\r\n        result += string + \":\";\r\n    }\r\n    return result;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\webapp",
  "methodName" : "setUp",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void setUp() throws Exception\n{\r\n    super.setUp();\r\n    GuiceServletConfig.setInjector(Guice.createInjector(new WebServletModule()));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\webapp",
  "methodName" : "testTasks",
  "errType" : null,
  "containingMethodsNum" : 12,
  "sourceCodeText" : "void testTasks() throws JSONException, Exception\n{\r\n    WebResource r = resource();\r\n    Map<JobId, Job> jobsMap = appContext.getAllJobs();\r\n    for (JobId id : jobsMap.keySet()) {\r\n        String jobId = MRApps.toString(id);\r\n        ClientResponse response = r.path(\"ws\").path(\"v1\").path(\"mapreduce\").path(\"jobs\").path(jobId).path(\"tasks\").accept(MediaType.APPLICATION_JSON).get(ClientResponse.class);\r\n        assertEquals(MediaType.APPLICATION_JSON_TYPE + \"; \" + JettyUtils.UTF_8, response.getType().toString());\r\n        JSONObject json = response.getEntity(JSONObject.class);\r\n        assertEquals(\"incorrect number of elements\", 1, json.length());\r\n        JSONObject tasks = json.getJSONObject(\"tasks\");\r\n        JSONArray arr = tasks.getJSONArray(\"task\");\r\n        assertEquals(\"incorrect number of elements\", 2, arr.length());\r\n        verifyAMTask(arr, jobsMap.get(id), null);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\webapp",
  "methodName" : "testTasksDefault",
  "errType" : null,
  "containingMethodsNum" : 12,
  "sourceCodeText" : "void testTasksDefault() throws JSONException, Exception\n{\r\n    WebResource r = resource();\r\n    Map<JobId, Job> jobsMap = appContext.getAllJobs();\r\n    for (JobId id : jobsMap.keySet()) {\r\n        String jobId = MRApps.toString(id);\r\n        ClientResponse response = r.path(\"ws\").path(\"v1\").path(\"mapreduce\").path(\"jobs\").path(jobId).path(\"tasks\").get(ClientResponse.class);\r\n        assertEquals(MediaType.APPLICATION_JSON_TYPE + \"; \" + JettyUtils.UTF_8, response.getType().toString());\r\n        JSONObject json = response.getEntity(JSONObject.class);\r\n        assertEquals(\"incorrect number of elements\", 1, json.length());\r\n        JSONObject tasks = json.getJSONObject(\"tasks\");\r\n        JSONArray arr = tasks.getJSONArray(\"task\");\r\n        assertEquals(\"incorrect number of elements\", 2, arr.length());\r\n        verifyAMTask(arr, jobsMap.get(id), null);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\webapp",
  "methodName" : "testTasksSlash",
  "errType" : null,
  "containingMethodsNum" : 12,
  "sourceCodeText" : "void testTasksSlash() throws JSONException, Exception\n{\r\n    WebResource r = resource();\r\n    Map<JobId, Job> jobsMap = appContext.getAllJobs();\r\n    for (JobId id : jobsMap.keySet()) {\r\n        String jobId = MRApps.toString(id);\r\n        ClientResponse response = r.path(\"ws\").path(\"v1\").path(\"mapreduce\").path(\"jobs\").path(jobId).path(\"tasks/\").accept(MediaType.APPLICATION_JSON).get(ClientResponse.class);\r\n        assertEquals(MediaType.APPLICATION_JSON_TYPE + \"; \" + JettyUtils.UTF_8, response.getType().toString());\r\n        JSONObject json = response.getEntity(JSONObject.class);\r\n        assertEquals(\"incorrect number of elements\", 1, json.length());\r\n        JSONObject tasks = json.getJSONObject(\"tasks\");\r\n        JSONArray arr = tasks.getJSONArray(\"task\");\r\n        assertEquals(\"incorrect number of elements\", 2, arr.length());\r\n        verifyAMTask(arr, jobsMap.get(id), null);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\webapp",
  "methodName" : "testTasksXML",
  "errType" : null,
  "containingMethodsNum" : 15,
  "sourceCodeText" : "void testTasksXML() throws JSONException, Exception\n{\r\n    WebResource r = resource();\r\n    Map<JobId, Job> jobsMap = appContext.getAllJobs();\r\n    for (JobId id : jobsMap.keySet()) {\r\n        String jobId = MRApps.toString(id);\r\n        ClientResponse response = r.path(\"ws\").path(\"v1\").path(\"mapreduce\").path(\"jobs\").path(jobId).path(\"tasks\").accept(MediaType.APPLICATION_XML).get(ClientResponse.class);\r\n        assertEquals(MediaType.APPLICATION_XML_TYPE + \"; \" + JettyUtils.UTF_8, response.getType().toString());\r\n        String xml = response.getEntity(String.class);\r\n        DocumentBuilderFactory dbf = DocumentBuilderFactory.newInstance();\r\n        DocumentBuilder db = dbf.newDocumentBuilder();\r\n        InputSource is = new InputSource();\r\n        is.setCharacterStream(new StringReader(xml));\r\n        Document dom = db.parse(is);\r\n        NodeList tasks = dom.getElementsByTagName(\"tasks\");\r\n        assertEquals(\"incorrect number of elements\", 1, tasks.getLength());\r\n        NodeList task = dom.getElementsByTagName(\"task\");\r\n        verifyAMTaskXML(task, jobsMap.get(id));\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\webapp",
  "methodName" : "testTasksQueryMap",
  "errType" : null,
  "containingMethodsNum" : 12,
  "sourceCodeText" : "void testTasksQueryMap() throws JSONException, Exception\n{\r\n    WebResource r = resource();\r\n    Map<JobId, Job> jobsMap = appContext.getAllJobs();\r\n    for (JobId id : jobsMap.keySet()) {\r\n        String jobId = MRApps.toString(id);\r\n        String type = \"m\";\r\n        ClientResponse response = r.path(\"ws\").path(\"v1\").path(\"mapreduce\").path(\"jobs\").path(jobId).path(\"tasks\").queryParam(\"type\", type).accept(MediaType.APPLICATION_JSON).get(ClientResponse.class);\r\n        assertEquals(MediaType.APPLICATION_JSON_TYPE + \"; \" + JettyUtils.UTF_8, response.getType().toString());\r\n        JSONObject json = response.getEntity(JSONObject.class);\r\n        assertEquals(\"incorrect number of elements\", 1, json.length());\r\n        JSONObject tasks = json.getJSONObject(\"tasks\");\r\n        JSONArray arr = tasks.getJSONArray(\"task\");\r\n        assertEquals(\"incorrect number of elements\", 1, arr.length());\r\n        verifyAMTask(arr, jobsMap.get(id), type);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\webapp",
  "methodName" : "testTasksQueryReduce",
  "errType" : null,
  "containingMethodsNum" : 12,
  "sourceCodeText" : "void testTasksQueryReduce() throws JSONException, Exception\n{\r\n    WebResource r = resource();\r\n    Map<JobId, Job> jobsMap = appContext.getAllJobs();\r\n    for (JobId id : jobsMap.keySet()) {\r\n        String jobId = MRApps.toString(id);\r\n        String type = \"r\";\r\n        ClientResponse response = r.path(\"ws\").path(\"v1\").path(\"mapreduce\").path(\"jobs\").path(jobId).path(\"tasks\").queryParam(\"type\", type).accept(MediaType.APPLICATION_JSON).get(ClientResponse.class);\r\n        assertEquals(MediaType.APPLICATION_JSON_TYPE + \"; \" + JettyUtils.UTF_8, response.getType().toString());\r\n        JSONObject json = response.getEntity(JSONObject.class);\r\n        assertEquals(\"incorrect number of elements\", 1, json.length());\r\n        JSONObject tasks = json.getJSONObject(\"tasks\");\r\n        JSONArray arr = tasks.getJSONArray(\"task\");\r\n        assertEquals(\"incorrect number of elements\", 1, arr.length());\r\n        verifyAMTask(arr, jobsMap.get(id), type);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\webapp",
  "methodName" : "testTasksQueryInvalid",
  "errType" : [ "UniformInterfaceException" ],
  "containingMethodsNum" : 18,
  "sourceCodeText" : "void testTasksQueryInvalid() throws JSONException, Exception\n{\r\n    WebResource r = resource();\r\n    Map<JobId, Job> jobsMap = appContext.getAllJobs();\r\n    for (JobId id : jobsMap.keySet()) {\r\n        String jobId = MRApps.toString(id);\r\n        String tasktype = \"reduce\";\r\n        try {\r\n            r.path(\"ws\").path(\"v1\").path(\"mapreduce\").path(\"jobs\").path(jobId).path(\"tasks\").queryParam(\"type\", tasktype).accept(MediaType.APPLICATION_JSON).get(JSONObject.class);\r\n            fail(\"should have thrown exception on invalid uri\");\r\n        } catch (UniformInterfaceException ue) {\r\n            ClientResponse response = ue.getResponse();\r\n            assertResponseStatusCode(Status.BAD_REQUEST, response.getStatusInfo());\r\n            assertEquals(MediaType.APPLICATION_JSON_TYPE + \"; \" + JettyUtils.UTF_8, response.getType().toString());\r\n            JSONObject msg = response.getEntity(JSONObject.class);\r\n            JSONObject exception = msg.getJSONObject(\"RemoteException\");\r\n            assertEquals(\"incorrect number of elements\", 3, exception.length());\r\n            String message = exception.getString(\"message\");\r\n            String type = exception.getString(\"exception\");\r\n            String classname = exception.getString(\"javaClassName\");\r\n            WebServicesTestUtils.checkStringMatch(\"exception message\", \"java.lang.Exception: tasktype must be either m or r\", message);\r\n            WebServicesTestUtils.checkStringMatch(\"exception type\", \"BadRequestException\", type);\r\n            WebServicesTestUtils.checkStringMatch(\"exception classname\", \"org.apache.hadoop.yarn.webapp.BadRequestException\", classname);\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\webapp",
  "methodName" : "testTaskId",
  "errType" : null,
  "containingMethodsNum" : 12,
  "sourceCodeText" : "void testTaskId() throws JSONException, Exception\n{\r\n    WebResource r = resource();\r\n    Map<JobId, Job> jobsMap = appContext.getAllJobs();\r\n    for (JobId id : jobsMap.keySet()) {\r\n        String jobId = MRApps.toString(id);\r\n        for (Task task : jobsMap.get(id).getTasks().values()) {\r\n            String tid = MRApps.toString(task.getID());\r\n            ClientResponse response = r.path(\"ws\").path(\"v1\").path(\"mapreduce\").path(\"jobs\").path(jobId).path(\"tasks\").path(tid).accept(MediaType.APPLICATION_JSON).get(ClientResponse.class);\r\n            assertEquals(MediaType.APPLICATION_JSON_TYPE + \"; \" + JettyUtils.UTF_8, response.getType().toString());\r\n            JSONObject json = response.getEntity(JSONObject.class);\r\n            assertEquals(\"incorrect number of elements\", 1, json.length());\r\n            JSONObject info = json.getJSONObject(\"task\");\r\n            verifyAMSingleTask(info, task);\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\webapp",
  "methodName" : "testTaskIdSlash",
  "errType" : null,
  "containingMethodsNum" : 12,
  "sourceCodeText" : "void testTaskIdSlash() throws JSONException, Exception\n{\r\n    WebResource r = resource();\r\n    Map<JobId, Job> jobsMap = appContext.getAllJobs();\r\n    for (JobId id : jobsMap.keySet()) {\r\n        String jobId = MRApps.toString(id);\r\n        for (Task task : jobsMap.get(id).getTasks().values()) {\r\n            String tid = MRApps.toString(task.getID());\r\n            ClientResponse response = r.path(\"ws\").path(\"v1\").path(\"mapreduce\").path(\"jobs\").path(jobId).path(\"tasks\").path(tid + \"/\").accept(MediaType.APPLICATION_JSON).get(ClientResponse.class);\r\n            assertEquals(MediaType.APPLICATION_JSON_TYPE + \"; \" + JettyUtils.UTF_8, response.getType().toString());\r\n            JSONObject json = response.getEntity(JSONObject.class);\r\n            assertEquals(\"incorrect number of elements\", 1, json.length());\r\n            JSONObject info = json.getJSONObject(\"task\");\r\n            verifyAMSingleTask(info, task);\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\webapp",
  "methodName" : "testTaskIdDefault",
  "errType" : null,
  "containingMethodsNum" : 12,
  "sourceCodeText" : "void testTaskIdDefault() throws JSONException, Exception\n{\r\n    WebResource r = resource();\r\n    Map<JobId, Job> jobsMap = appContext.getAllJobs();\r\n    for (JobId id : jobsMap.keySet()) {\r\n        String jobId = MRApps.toString(id);\r\n        for (Task task : jobsMap.get(id).getTasks().values()) {\r\n            String tid = MRApps.toString(task.getID());\r\n            ClientResponse response = r.path(\"ws\").path(\"v1\").path(\"mapreduce\").path(\"jobs\").path(jobId).path(\"tasks\").path(tid).get(ClientResponse.class);\r\n            assertEquals(MediaType.APPLICATION_JSON_TYPE + \"; \" + JettyUtils.UTF_8, response.getType().toString());\r\n            JSONObject json = response.getEntity(JSONObject.class);\r\n            assertEquals(\"incorrect number of elements\", 1, json.length());\r\n            JSONObject info = json.getJSONObject(\"task\");\r\n            verifyAMSingleTask(info, task);\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\webapp",
  "methodName" : "testTaskIdBogus",
  "errType" : [ "UniformInterfaceException" ],
  "containingMethodsNum" : 18,
  "sourceCodeText" : "void testTaskIdBogus() throws JSONException, Exception\n{\r\n    WebResource r = resource();\r\n    Map<JobId, Job> jobsMap = appContext.getAllJobs();\r\n    for (JobId id : jobsMap.keySet()) {\r\n        String jobId = MRApps.toString(id);\r\n        String tid = \"bogustaskid\";\r\n        try {\r\n            r.path(\"ws\").path(\"v1\").path(\"mapreduce\").path(\"jobs\").path(jobId).path(\"tasks\").path(tid).get(JSONObject.class);\r\n            fail(\"should have thrown exception on invalid uri\");\r\n        } catch (UniformInterfaceException ue) {\r\n            ClientResponse response = ue.getResponse();\r\n            assertResponseStatusCode(Status.NOT_FOUND, response.getStatusInfo());\r\n            assertEquals(MediaType.APPLICATION_JSON_TYPE + \"; \" + JettyUtils.UTF_8, response.getType().toString());\r\n            JSONObject msg = response.getEntity(JSONObject.class);\r\n            JSONObject exception = msg.getJSONObject(\"RemoteException\");\r\n            assertEquals(\"incorrect number of elements\", 3, exception.length());\r\n            String message = exception.getString(\"message\");\r\n            String type = exception.getString(\"exception\");\r\n            String classname = exception.getString(\"javaClassName\");\r\n            WebServicesTestUtils.checkStringEqual(\"exception message\", \"java.lang.Exception: TaskId string : \" + \"bogustaskid is not properly formed\" + \"\\nReason: java.util.regex.Matcher[pattern=\" + TaskID.TASK_ID_REGEX + \" region=0,11 lastmatch=]\", message);\r\n            WebServicesTestUtils.checkStringMatch(\"exception type\", \"NotFoundException\", type);\r\n            WebServicesTestUtils.checkStringMatch(\"exception classname\", \"org.apache.hadoop.yarn.webapp.NotFoundException\", classname);\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\webapp",
  "methodName" : "testTaskIdNonExist",
  "errType" : [ "UniformInterfaceException" ],
  "containingMethodsNum" : 18,
  "sourceCodeText" : "void testTaskIdNonExist() throws JSONException, Exception\n{\r\n    WebResource r = resource();\r\n    Map<JobId, Job> jobsMap = appContext.getAllJobs();\r\n    for (JobId id : jobsMap.keySet()) {\r\n        String jobId = MRApps.toString(id);\r\n        String tid = \"task_0_0000_m_000000\";\r\n        try {\r\n            r.path(\"ws\").path(\"v1\").path(\"mapreduce\").path(\"jobs\").path(jobId).path(\"tasks\").path(tid).get(JSONObject.class);\r\n            fail(\"should have thrown exception on invalid uri\");\r\n        } catch (UniformInterfaceException ue) {\r\n            ClientResponse response = ue.getResponse();\r\n            assertResponseStatusCode(Status.NOT_FOUND, response.getStatusInfo());\r\n            assertEquals(MediaType.APPLICATION_JSON_TYPE + \"; \" + JettyUtils.UTF_8, response.getType().toString());\r\n            JSONObject msg = response.getEntity(JSONObject.class);\r\n            JSONObject exception = msg.getJSONObject(\"RemoteException\");\r\n            assertEquals(\"incorrect number of elements\", 3, exception.length());\r\n            String message = exception.getString(\"message\");\r\n            String type = exception.getString(\"exception\");\r\n            String classname = exception.getString(\"javaClassName\");\r\n            WebServicesTestUtils.checkStringMatch(\"exception message\", \"java.lang.Exception: task not found with id task_0_0000_m_000000\", message);\r\n            WebServicesTestUtils.checkStringMatch(\"exception type\", \"NotFoundException\", type);\r\n            WebServicesTestUtils.checkStringMatch(\"exception classname\", \"org.apache.hadoop.yarn.webapp.NotFoundException\", classname);\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\webapp",
  "methodName" : "testTaskIdInvalid",
  "errType" : [ "UniformInterfaceException" ],
  "containingMethodsNum" : 18,
  "sourceCodeText" : "void testTaskIdInvalid() throws JSONException, Exception\n{\r\n    WebResource r = resource();\r\n    Map<JobId, Job> jobsMap = appContext.getAllJobs();\r\n    for (JobId id : jobsMap.keySet()) {\r\n        String jobId = MRApps.toString(id);\r\n        String tid = \"task_0_0000_d_000000\";\r\n        try {\r\n            r.path(\"ws\").path(\"v1\").path(\"mapreduce\").path(\"jobs\").path(jobId).path(\"tasks\").path(tid).get(JSONObject.class);\r\n            fail(\"should have thrown exception on invalid uri\");\r\n        } catch (UniformInterfaceException ue) {\r\n            ClientResponse response = ue.getResponse();\r\n            assertResponseStatusCode(Status.NOT_FOUND, response.getStatusInfo());\r\n            assertEquals(MediaType.APPLICATION_JSON_TYPE + \"; \" + JettyUtils.UTF_8, response.getType().toString());\r\n            JSONObject msg = response.getEntity(JSONObject.class);\r\n            JSONObject exception = msg.getJSONObject(\"RemoteException\");\r\n            assertEquals(\"incorrect number of elements\", 3, exception.length());\r\n            String message = exception.getString(\"message\");\r\n            String type = exception.getString(\"exception\");\r\n            String classname = exception.getString(\"javaClassName\");\r\n            WebServicesTestUtils.checkStringEqual(\"exception message\", \"java.lang.Exception: TaskId string : \" + \"task_0_0000_d_000000 is not properly formed\" + \"\\nReason: java.util.regex.Matcher[pattern=\" + TaskID.TASK_ID_REGEX + \" region=0,20 lastmatch=]\", message);\r\n            WebServicesTestUtils.checkStringMatch(\"exception type\", \"NotFoundException\", type);\r\n            WebServicesTestUtils.checkStringMatch(\"exception classname\", \"org.apache.hadoop.yarn.webapp.NotFoundException\", classname);\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\webapp",
  "methodName" : "testTaskIdInvalid2",
  "errType" : [ "UniformInterfaceException" ],
  "containingMethodsNum" : 18,
  "sourceCodeText" : "void testTaskIdInvalid2() throws JSONException, Exception\n{\r\n    WebResource r = resource();\r\n    Map<JobId, Job> jobsMap = appContext.getAllJobs();\r\n    for (JobId id : jobsMap.keySet()) {\r\n        String jobId = MRApps.toString(id);\r\n        String tid = \"task_0_m_000000\";\r\n        try {\r\n            r.path(\"ws\").path(\"v1\").path(\"mapreduce\").path(\"jobs\").path(jobId).path(\"tasks\").path(tid).get(JSONObject.class);\r\n            fail(\"should have thrown exception on invalid uri\");\r\n        } catch (UniformInterfaceException ue) {\r\n            ClientResponse response = ue.getResponse();\r\n            assertResponseStatusCode(Status.NOT_FOUND, response.getStatusInfo());\r\n            assertEquals(MediaType.APPLICATION_JSON_TYPE + \"; \" + JettyUtils.UTF_8, response.getType().toString());\r\n            JSONObject msg = response.getEntity(JSONObject.class);\r\n            JSONObject exception = msg.getJSONObject(\"RemoteException\");\r\n            assertEquals(\"incorrect number of elements\", 3, exception.length());\r\n            String message = exception.getString(\"message\");\r\n            String type = exception.getString(\"exception\");\r\n            String classname = exception.getString(\"javaClassName\");\r\n            WebServicesTestUtils.checkStringEqual(\"exception message\", \"java.lang.Exception: TaskId string : \" + \"task_0_m_000000 is not properly formed\" + \"\\nReason: java.util.regex.Matcher[pattern=\" + TaskID.TASK_ID_REGEX + \" region=0,15 lastmatch=]\", message);\r\n            WebServicesTestUtils.checkStringMatch(\"exception type\", \"NotFoundException\", type);\r\n            WebServicesTestUtils.checkStringMatch(\"exception classname\", \"org.apache.hadoop.yarn.webapp.NotFoundException\", classname);\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\webapp",
  "methodName" : "testTaskIdInvalid3",
  "errType" : [ "UniformInterfaceException" ],
  "containingMethodsNum" : 18,
  "sourceCodeText" : "void testTaskIdInvalid3() throws JSONException, Exception\n{\r\n    WebResource r = resource();\r\n    Map<JobId, Job> jobsMap = appContext.getAllJobs();\r\n    for (JobId id : jobsMap.keySet()) {\r\n        String jobId = MRApps.toString(id);\r\n        String tid = \"task_0_0000_m\";\r\n        try {\r\n            r.path(\"ws\").path(\"v1\").path(\"mapreduce\").path(\"jobs\").path(jobId).path(\"tasks\").path(tid).get(JSONObject.class);\r\n            fail(\"should have thrown exception on invalid uri\");\r\n        } catch (UniformInterfaceException ue) {\r\n            ClientResponse response = ue.getResponse();\r\n            assertResponseStatusCode(Status.NOT_FOUND, response.getStatusInfo());\r\n            assertEquals(MediaType.APPLICATION_JSON_TYPE + \"; \" + JettyUtils.UTF_8, response.getType().toString());\r\n            JSONObject msg = response.getEntity(JSONObject.class);\r\n            JSONObject exception = msg.getJSONObject(\"RemoteException\");\r\n            assertEquals(\"incorrect number of elements\", 3, exception.length());\r\n            String message = exception.getString(\"message\");\r\n            String type = exception.getString(\"exception\");\r\n            String classname = exception.getString(\"javaClassName\");\r\n            WebServicesTestUtils.checkStringEqual(\"exception message\", \"java.lang.Exception: TaskId string : \" + \"task_0_0000_m is not properly formed\" + \"\\nReason: java.util.regex.Matcher[pattern=\" + TaskID.TASK_ID_REGEX + \" region=0,13 lastmatch=]\", message);\r\n            WebServicesTestUtils.checkStringMatch(\"exception type\", \"NotFoundException\", type);\r\n            WebServicesTestUtils.checkStringMatch(\"exception classname\", \"org.apache.hadoop.yarn.webapp.NotFoundException\", classname);\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\webapp",
  "methodName" : "testTaskIdXML",
  "errType" : null,
  "containingMethodsNum" : 17,
  "sourceCodeText" : "void testTaskIdXML() throws JSONException, Exception\n{\r\n    WebResource r = resource();\r\n    Map<JobId, Job> jobsMap = appContext.getAllJobs();\r\n    for (JobId id : jobsMap.keySet()) {\r\n        String jobId = MRApps.toString(id);\r\n        for (Task task : jobsMap.get(id).getTasks().values()) {\r\n            String tid = MRApps.toString(task.getID());\r\n            ClientResponse response = r.path(\"ws\").path(\"v1\").path(\"mapreduce\").path(\"jobs\").path(jobId).path(\"tasks\").path(tid).accept(MediaType.APPLICATION_XML).get(ClientResponse.class);\r\n            assertEquals(MediaType.APPLICATION_XML_TYPE + \"; \" + JettyUtils.UTF_8, response.getType().toString());\r\n            String xml = response.getEntity(String.class);\r\n            DocumentBuilderFactory dbf = DocumentBuilderFactory.newInstance();\r\n            DocumentBuilder db = dbf.newDocumentBuilder();\r\n            InputSource is = new InputSource();\r\n            is.setCharacterStream(new StringReader(xml));\r\n            Document dom = db.parse(is);\r\n            NodeList nodes = dom.getElementsByTagName(\"task\");\r\n            for (int i = 0; i < nodes.getLength(); i++) {\r\n                Element element = (Element) nodes.item(i);\r\n                verifyAMSingleTaskXML(element, task);\r\n            }\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\webapp",
  "methodName" : "verifyAMSingleTask",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void verifyAMSingleTask(JSONObject info, Task task) throws JSONException\n{\r\n    assertEquals(\"incorrect number of elements\", 9, info.length());\r\n    verifyTaskGeneric(task, info.getString(\"id\"), info.getString(\"state\"), info.getString(\"type\"), info.getString(\"successfulAttempt\"), info.getLong(\"startTime\"), info.getLong(\"finishTime\"), info.getLong(\"elapsedTime\"), (float) info.getDouble(\"progress\"), info.getString(\"status\"));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\webapp",
  "methodName" : "verifyAMTask",
  "errType" : null,
  "containingMethodsNum" : 10,
  "sourceCodeText" : "void verifyAMTask(JSONArray arr, Job job, String type) throws JSONException\n{\r\n    for (Task task : job.getTasks().values()) {\r\n        TaskId id = task.getID();\r\n        String tid = MRApps.toString(id);\r\n        Boolean found = false;\r\n        if (type != null && task.getType() == MRApps.taskType(type)) {\r\n            for (int i = 0; i < arr.length(); i++) {\r\n                JSONObject info = arr.getJSONObject(i);\r\n                if (tid.matches(info.getString(\"id\"))) {\r\n                    found = true;\r\n                    verifyAMSingleTask(info, task);\r\n                }\r\n            }\r\n            assertTrue(\"task with id: \" + tid + \" not in web service output\", found);\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\webapp",
  "methodName" : "verifyTaskGeneric",
  "errType" : null,
  "containingMethodsNum" : 12,
  "sourceCodeText" : "void verifyTaskGeneric(Task task, String id, String state, String type, String successfulAttempt, long startTime, long finishTime, long elapsedTime, float progress, String status)\n{\r\n    TaskId taskid = task.getID();\r\n    String tid = MRApps.toString(taskid);\r\n    TaskReport report = task.getReport();\r\n    WebServicesTestUtils.checkStringMatch(\"id\", tid, id);\r\n    WebServicesTestUtils.checkStringMatch(\"type\", task.getType().toString(), type);\r\n    WebServicesTestUtils.checkStringMatch(\"state\", report.getTaskState().toString(), state);\r\n    assertNotNull(\"successfulAttempt null\", successfulAttempt);\r\n    assertEquals(\"startTime wrong\", report.getStartTime(), startTime);\r\n    assertEquals(\"finishTime wrong\", report.getFinishTime(), finishTime);\r\n    assertEquals(\"elapsedTime wrong\", finishTime - startTime, elapsedTime);\r\n    assertEquals(\"progress wrong\", report.getProgress() * 100, progress, 1e-3f);\r\n    assertEquals(\"status wrong\", report.getStatus(), status);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\webapp",
  "methodName" : "verifyAMSingleTaskXML",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void verifyAMSingleTaskXML(Element element, Task task)\n{\r\n    verifyTaskGeneric(task, WebServicesTestUtils.getXmlString(element, \"id\"), WebServicesTestUtils.getXmlString(element, \"state\"), WebServicesTestUtils.getXmlString(element, \"type\"), WebServicesTestUtils.getXmlString(element, \"successfulAttempt\"), WebServicesTestUtils.getXmlLong(element, \"startTime\"), WebServicesTestUtils.getXmlLong(element, \"finishTime\"), WebServicesTestUtils.getXmlLong(element, \"elapsedTime\"), WebServicesTestUtils.getXmlFloat(element, \"progress\"), WebServicesTestUtils.getXmlString(element, \"status\"));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\webapp",
  "methodName" : "verifyAMTaskXML",
  "errType" : null,
  "containingMethodsNum" : 9,
  "sourceCodeText" : "void verifyAMTaskXML(NodeList nodes, Job job)\n{\r\n    assertEquals(\"incorrect number of elements\", 2, nodes.getLength());\r\n    for (Task task : job.getTasks().values()) {\r\n        TaskId id = task.getID();\r\n        String tid = MRApps.toString(id);\r\n        Boolean found = false;\r\n        for (int i = 0; i < nodes.getLength(); i++) {\r\n            Element element = (Element) nodes.item(i);\r\n            if (tid.matches(WebServicesTestUtils.getXmlString(element, \"id\"))) {\r\n                found = true;\r\n                verifyAMSingleTaskXML(element, task);\r\n            }\r\n        }\r\n        assertTrue(\"task with id: \" + tid + \" not in web service output\", found);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\webapp",
  "methodName" : "testTaskIdCounters",
  "errType" : null,
  "containingMethodsNum" : 12,
  "sourceCodeText" : "void testTaskIdCounters() throws JSONException, Exception\n{\r\n    WebResource r = resource();\r\n    Map<JobId, Job> jobsMap = appContext.getAllJobs();\r\n    for (JobId id : jobsMap.keySet()) {\r\n        String jobId = MRApps.toString(id);\r\n        for (Task task : jobsMap.get(id).getTasks().values()) {\r\n            String tid = MRApps.toString(task.getID());\r\n            ClientResponse response = r.path(\"ws\").path(\"v1\").path(\"mapreduce\").path(\"jobs\").path(jobId).path(\"tasks\").path(tid).path(\"counters\").accept(MediaType.APPLICATION_JSON).get(ClientResponse.class);\r\n            assertEquals(MediaType.APPLICATION_JSON_TYPE + \"; \" + JettyUtils.UTF_8, response.getType().toString());\r\n            JSONObject json = response.getEntity(JSONObject.class);\r\n            assertEquals(\"incorrect number of elements\", 1, json.length());\r\n            JSONObject info = json.getJSONObject(\"jobTaskCounters\");\r\n            verifyAMJobTaskCounters(info, task);\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\webapp",
  "methodName" : "testTaskIdCountersSlash",
  "errType" : null,
  "containingMethodsNum" : 12,
  "sourceCodeText" : "void testTaskIdCountersSlash() throws JSONException, Exception\n{\r\n    WebResource r = resource();\r\n    Map<JobId, Job> jobsMap = appContext.getAllJobs();\r\n    for (JobId id : jobsMap.keySet()) {\r\n        String jobId = MRApps.toString(id);\r\n        for (Task task : jobsMap.get(id).getTasks().values()) {\r\n            String tid = MRApps.toString(task.getID());\r\n            ClientResponse response = r.path(\"ws\").path(\"v1\").path(\"mapreduce\").path(\"jobs\").path(jobId).path(\"tasks\").path(tid).path(\"counters/\").accept(MediaType.APPLICATION_JSON).get(ClientResponse.class);\r\n            assertEquals(MediaType.APPLICATION_JSON_TYPE + \"; \" + JettyUtils.UTF_8, response.getType().toString());\r\n            JSONObject json = response.getEntity(JSONObject.class);\r\n            assertEquals(\"incorrect number of elements\", 1, json.length());\r\n            JSONObject info = json.getJSONObject(\"jobTaskCounters\");\r\n            verifyAMJobTaskCounters(info, task);\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\webapp",
  "methodName" : "testTaskIdCountersDefault",
  "errType" : null,
  "containingMethodsNum" : 12,
  "sourceCodeText" : "void testTaskIdCountersDefault() throws JSONException, Exception\n{\r\n    WebResource r = resource();\r\n    Map<JobId, Job> jobsMap = appContext.getAllJobs();\r\n    for (JobId id : jobsMap.keySet()) {\r\n        String jobId = MRApps.toString(id);\r\n        for (Task task : jobsMap.get(id).getTasks().values()) {\r\n            String tid = MRApps.toString(task.getID());\r\n            ClientResponse response = r.path(\"ws\").path(\"v1\").path(\"mapreduce\").path(\"jobs\").path(jobId).path(\"tasks\").path(tid).path(\"counters\").get(ClientResponse.class);\r\n            assertEquals(MediaType.APPLICATION_JSON_TYPE + \"; \" + JettyUtils.UTF_8, response.getType().toString());\r\n            JSONObject json = response.getEntity(JSONObject.class);\r\n            assertEquals(\"incorrect number of elements\", 1, json.length());\r\n            JSONObject info = json.getJSONObject(\"jobTaskCounters\");\r\n            verifyAMJobTaskCounters(info, task);\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\webapp",
  "methodName" : "testJobTaskCountersXML",
  "errType" : null,
  "containingMethodsNum" : 15,
  "sourceCodeText" : "void testJobTaskCountersXML() throws Exception\n{\r\n    WebResource r = resource();\r\n    Map<JobId, Job> jobsMap = appContext.getAllJobs();\r\n    for (JobId id : jobsMap.keySet()) {\r\n        String jobId = MRApps.toString(id);\r\n        for (Task task : jobsMap.get(id).getTasks().values()) {\r\n            String tid = MRApps.toString(task.getID());\r\n            ClientResponse response = r.path(\"ws\").path(\"v1\").path(\"mapreduce\").path(\"jobs\").path(jobId).path(\"tasks\").path(tid).path(\"counters\").accept(MediaType.APPLICATION_XML).get(ClientResponse.class);\r\n            assertEquals(MediaType.APPLICATION_XML_TYPE + \"; \" + JettyUtils.UTF_8, response.getType().toString());\r\n            String xml = response.getEntity(String.class);\r\n            DocumentBuilderFactory dbf = DocumentBuilderFactory.newInstance();\r\n            DocumentBuilder db = dbf.newDocumentBuilder();\r\n            InputSource is = new InputSource();\r\n            is.setCharacterStream(new StringReader(xml));\r\n            Document dom = db.parse(is);\r\n            NodeList info = dom.getElementsByTagName(\"jobTaskCounters\");\r\n            verifyAMTaskCountersXML(info, task);\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\webapp",
  "methodName" : "verifyAMJobTaskCounters",
  "errType" : null,
  "containingMethodsNum" : 14,
  "sourceCodeText" : "void verifyAMJobTaskCounters(JSONObject info, Task task) throws JSONException\n{\r\n    assertEquals(\"incorrect number of elements\", 2, info.length());\r\n    WebServicesTestUtils.checkStringMatch(\"id\", MRApps.toString(task.getID()), info.getString(\"id\"));\r\n    JSONArray counterGroups = info.getJSONArray(\"taskCounterGroup\");\r\n    for (int i = 0; i < counterGroups.length(); i++) {\r\n        JSONObject counterGroup = counterGroups.getJSONObject(i);\r\n        String name = counterGroup.getString(\"counterGroupName\");\r\n        assertTrue(\"name not set\", (name != null && !name.isEmpty()));\r\n        JSONArray counters = counterGroup.getJSONArray(\"counter\");\r\n        for (int j = 0; j < counters.length(); j++) {\r\n            JSONObject counter = counters.getJSONObject(j);\r\n            String counterName = counter.getString(\"name\");\r\n            assertTrue(\"name not set\", (counterName != null && !counterName.isEmpty()));\r\n            long value = counter.getLong(\"value\");\r\n            assertTrue(\"value  >= 0\", value >= 0);\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\webapp",
  "methodName" : "verifyAMTaskCountersXML",
  "errType" : null,
  "containingMethodsNum" : 16,
  "sourceCodeText" : "void verifyAMTaskCountersXML(NodeList nodes, Task task)\n{\r\n    for (int i = 0; i < nodes.getLength(); i++) {\r\n        Element element = (Element) nodes.item(i);\r\n        WebServicesTestUtils.checkStringMatch(\"id\", MRApps.toString(task.getID()), WebServicesTestUtils.getXmlString(element, \"id\"));\r\n        NodeList groups = element.getElementsByTagName(\"taskCounterGroup\");\r\n        for (int j = 0; j < groups.getLength(); j++) {\r\n            Element counters = (Element) groups.item(j);\r\n            assertNotNull(\"should have counters in the web service info\", counters);\r\n            String name = WebServicesTestUtils.getXmlString(counters, \"counterGroupName\");\r\n            assertTrue(\"name not set\", (name != null && !name.isEmpty()));\r\n            NodeList counterArr = counters.getElementsByTagName(\"counter\");\r\n            for (int z = 0; z < counterArr.getLength(); z++) {\r\n                Element counter = (Element) counterArr.item(z);\r\n                String counterName = WebServicesTestUtils.getXmlString(counter, \"name\");\r\n                assertTrue(\"counter name not set\", (counterName != null && !counterName.isEmpty()));\r\n                long value = WebServicesTestUtils.getXmlLong(counter, \"value\");\r\n                assertTrue(\"value not >= 0\", value >= 0);\r\n            }\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\local",
  "methodName" : "testRMConnectionRetry",
  "errType" : [ "YarnException", "YarnRuntimeException" ],
  "containingMethodsNum" : 13,
  "sourceCodeText" : "void testRMConnectionRetry() throws Exception\n{\r\n    ApplicationMasterProtocol mockScheduler = mock(ApplicationMasterProtocol.class);\r\n    when(mockScheduler.allocate(isA(AllocateRequest.class))).thenThrow(RPCUtil.getRemoteException(new IOException(\"forcefail\")));\r\n    Configuration conf = new Configuration();\r\n    LocalContainerAllocator lca = new StubbedLocalContainerAllocator(mockScheduler);\r\n    lca.init(conf);\r\n    lca.start();\r\n    try {\r\n        lca.heartbeat();\r\n        Assert.fail(\"heartbeat was supposed to throw\");\r\n    } catch (YarnException e) {\r\n    } finally {\r\n        lca.stop();\r\n    }\r\n    conf.setLong(MRJobConfig.MR_AM_TO_RM_WAIT_INTERVAL_MS, 0);\r\n    lca = new StubbedLocalContainerAllocator(mockScheduler);\r\n    lca.init(conf);\r\n    lca.start();\r\n    try {\r\n        lca.heartbeat();\r\n        Assert.fail(\"heartbeat was supposed to throw\");\r\n    } catch (YarnRuntimeException e) {\r\n    } finally {\r\n        lca.stop();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 2,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\local",
  "methodName" : "testAllocResponseId",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void testAllocResponseId() throws Exception\n{\r\n    ApplicationMasterProtocol scheduler = new MockScheduler();\r\n    Configuration conf = new Configuration();\r\n    LocalContainerAllocator lca = new StubbedLocalContainerAllocator(scheduler);\r\n    lca.init(conf);\r\n    lca.start();\r\n    lca.heartbeat();\r\n    lca.heartbeat();\r\n    lca.close();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\local",
  "methodName" : "testAMRMTokenUpdate",
  "errType" : null,
  "containingMethodsNum" : 19,
  "sourceCodeText" : "void testAMRMTokenUpdate() throws Exception\n{\r\n    Configuration conf = new Configuration();\r\n    ApplicationAttemptId attemptId = ApplicationAttemptId.newInstance(ApplicationId.newInstance(1, 1), 1);\r\n    AMRMTokenIdentifier oldTokenId = new AMRMTokenIdentifier(attemptId, 1);\r\n    AMRMTokenIdentifier newTokenId = new AMRMTokenIdentifier(attemptId, 2);\r\n    Token<AMRMTokenIdentifier> oldToken = new Token<AMRMTokenIdentifier>(oldTokenId.getBytes(), \"oldpassword\".getBytes(), oldTokenId.getKind(), new Text());\r\n    Token<AMRMTokenIdentifier> newToken = new Token<AMRMTokenIdentifier>(newTokenId.getBytes(), \"newpassword\".getBytes(), newTokenId.getKind(), new Text());\r\n    MockScheduler scheduler = new MockScheduler();\r\n    scheduler.amToken = newToken;\r\n    final LocalContainerAllocator lca = new StubbedLocalContainerAllocator(scheduler);\r\n    lca.init(conf);\r\n    lca.start();\r\n    UserGroupInformation testUgi = UserGroupInformation.createUserForTesting(\"someuser\", new String[0]);\r\n    testUgi.addToken(oldToken);\r\n    testUgi.doAs(new PrivilegedExceptionAction<Void>() {\r\n\r\n        @Override\r\n        public Void run() throws Exception {\r\n            lca.heartbeat();\r\n            return null;\r\n        }\r\n    });\r\n    lca.close();\r\n    int tokenCount = 0;\r\n    Token<? extends TokenIdentifier> ugiToken = null;\r\n    for (Token<? extends TokenIdentifier> token : testUgi.getTokens()) {\r\n        if (AMRMTokenIdentifier.KIND_NAME.equals(token.getKind())) {\r\n            ugiToken = token;\r\n            ++tokenCount;\r\n        }\r\n    }\r\n    Assert.assertEquals(\"too many AMRM tokens\", 1, tokenCount);\r\n    Assert.assertArrayEquals(\"token identifier not updated\", newToken.getIdentifier(), ugiToken.getIdentifier());\r\n    Assert.assertArrayEquals(\"token password not updated\", newToken.getPassword(), ugiToken.getPassword());\r\n    Assert.assertEquals(\"AMRM token service not updated\", new Text(ClientRMProxy.getAMRMTokenService(conf)), ugiToken.getService());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\local",
  "methodName" : "testAllocatedContainerResourceIsNotNull",
  "errType" : null,
  "containingMethodsNum" : 14,
  "sourceCodeText" : "void testAllocatedContainerResourceIsNotNull()\n{\r\n    ArgumentCaptor<TaskAttemptContainerAssignedEvent> containerAssignedCaptor = ArgumentCaptor.forClass(TaskAttemptContainerAssignedEvent.class);\r\n    @SuppressWarnings(\"unchecked\")\r\n    EventHandler<Event> eventHandler = mock(EventHandler.class);\r\n    AppContext context = mock(AppContext.class);\r\n    when(context.getEventHandler()).thenReturn(eventHandler);\r\n    ContainerId containerId = ContainerId.fromString(\"container_1427562107907_0002_01_000001\");\r\n    LocalContainerAllocator containerAllocator = new LocalContainerAllocator(mock(ClientService.class), context, \"localhost\", -1, -1, containerId);\r\n    ContainerAllocatorEvent containerAllocatorEvent = createContainerRequestEvent();\r\n    containerAllocator.handle(containerAllocatorEvent);\r\n    verify(eventHandler, times(1)).handle(containerAssignedCaptor.capture());\r\n    Container container = containerAssignedCaptor.getValue().getContainer();\r\n    Resource containerResource = container.getResource();\r\n    Assert.assertNotNull(containerResource);\r\n    assertThat(containerResource.getMemorySize()).isEqualTo(0);\r\n    assertThat(containerResource.getVirtualCores()).isEqualTo(0);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\local",
  "methodName" : "createContainerRequestEvent",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "ContainerAllocatorEvent createContainerRequestEvent()\n{\r\n    TaskAttemptId taskAttemptId = mock(TaskAttemptId.class);\r\n    TaskId taskId = mock(TaskId.class);\r\n    when(taskAttemptId.getTaskId()).thenReturn(taskId);\r\n    return new ContainerAllocatorEvent(taskAttemptId, ContainerAllocator.EventType.CONTAINER_REQ);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app",
  "methodName" : "setup",
  "errType" : null,
  "containingMethodsNum" : 14,
  "sourceCodeText" : "void setup()\n{\r\n    ApplicationId appId = ApplicationId.newInstance(200, 1);\r\n    ApplicationAttemptId appAttemptId = ApplicationAttemptId.newInstance(appId, 1);\r\n    jid = MRBuilderUtils.newJobId(appId, 1);\r\n    mActxt = mock(RunningAppContext.class);\r\n    @SuppressWarnings(\"unchecked\")\r\n    EventHandler<Event> ea = mock(EventHandler.class);\r\n    when(mActxt.getEventHandler()).thenReturn(ea);\r\n    for (int i = 0; i < 40; ++i) {\r\n        ContainerId cId = ContainerId.newContainerId(appAttemptId, i);\r\n        if (0 == i % 7) {\r\n            preemptedContainers.add(cId);\r\n        }\r\n        TaskId tId = 0 == i % 2 ? MRBuilderUtils.newTaskId(jid, i / 2, TaskType.MAP) : MRBuilderUtils.newTaskId(jid, i / 2 + 1, TaskType.REDUCE);\r\n        assignedContainers.put(cId, MRBuilderUtils.newTaskAttemptId(tId, 0));\r\n        contToResourceMap.put(cId, Resource.newInstance(2 * minAlloc, 2));\r\n    }\r\n    for (Map.Entry<ContainerId, TaskAttemptId> ent : assignedContainers.entrySet()) {\r\n        System.out.println(\"cont:\" + ent.getKey().getContainerId() + \" type:\" + ent.getValue().getTaskId().getTaskType() + \" res:\" + contToResourceMap.get(ent.getKey()).getMemorySize() + \"MB\");\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app",
  "methodName" : "testStrictPreemptionContract",
  "errType" : null,
  "containingMethodsNum" : 11,
  "sourceCodeText" : "void testStrictPreemptionContract()\n{\r\n    final Map<ContainerId, TaskAttemptId> containers = assignedContainers;\r\n    AMPreemptionPolicy.Context mPctxt = new AMPreemptionPolicy.Context() {\r\n\r\n        @Override\r\n        public TaskAttemptId getTaskAttempt(ContainerId cId) {\r\n            return containers.get(cId);\r\n        }\r\n\r\n        @Override\r\n        public List<Container> getContainers(TaskType t) {\r\n            List<Container> p = new ArrayList<Container>();\r\n            for (Map.Entry<ContainerId, TaskAttemptId> ent : assignedContainers.entrySet()) {\r\n                if (ent.getValue().getTaskId().getTaskType().equals(t)) {\r\n                    p.add(Container.newInstance(ent.getKey(), null, null, contToResourceMap.get(ent.getKey()), Priority.newInstance(0), null));\r\n                }\r\n            }\r\n            return p;\r\n        }\r\n    };\r\n    PreemptionMessage pM = generatePreemptionMessage(preemptedContainers, contToResourceMap, Resource.newInstance(1024, 1), true);\r\n    CheckpointAMPreemptionPolicy policy = new CheckpointAMPreemptionPolicy();\r\n    policy.init(mActxt);\r\n    policy.preempt(mPctxt, pM);\r\n    for (ContainerId c : preemptedContainers) {\r\n        TaskAttemptId t = assignedContainers.get(c);\r\n        if (TaskType.MAP.equals(t.getTaskId().getTaskType())) {\r\n            assert policy.isPreempted(t) == false;\r\n        } else {\r\n            assert policy.isPreempted(t);\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app",
  "methodName" : "testPreemptionContract",
  "errType" : null,
  "containingMethodsNum" : 24,
  "sourceCodeText" : "void testPreemptionContract()\n{\r\n    final Map<ContainerId, TaskAttemptId> containers = assignedContainers;\r\n    AMPreemptionPolicy.Context mPctxt = new AMPreemptionPolicy.Context() {\r\n\r\n        @Override\r\n        public TaskAttemptId getTaskAttempt(ContainerId cId) {\r\n            return containers.get(cId);\r\n        }\r\n\r\n        @Override\r\n        public List<Container> getContainers(TaskType t) {\r\n            List<Container> p = new ArrayList<Container>();\r\n            for (Map.Entry<ContainerId, TaskAttemptId> ent : assignedContainers.entrySet()) {\r\n                if (ent.getValue().getTaskId().getTaskType().equals(t)) {\r\n                    p.add(Container.newInstance(ent.getKey(), null, null, contToResourceMap.get(ent.getKey()), Priority.newInstance(0), null));\r\n                }\r\n            }\r\n            return p;\r\n        }\r\n    };\r\n    PreemptionMessage pM = generatePreemptionMessage(preemptedContainers, contToResourceMap, Resource.newInstance(minAlloc, 1), false);\r\n    CheckpointAMPreemptionPolicy policy = new CheckpointAMPreemptionPolicy();\r\n    policy.init(mActxt);\r\n    int supposedMemPreemption = (int) pM.getContract().getResourceRequest().get(0).getResourceRequest().getCapability().getMemorySize() * pM.getContract().getResourceRequest().get(0).getResourceRequest().getNumContainers();\r\n    policy.preempt(mPctxt, pM);\r\n    List<TaskAttemptId> preempting = validatePreemption(pM, policy, supposedMemPreemption);\r\n    policy.preempt(mPctxt, pM);\r\n    List<TaskAttemptId> preempting2 = validatePreemption(pM, policy, supposedMemPreemption);\r\n    assert preempting2.equals(preempting);\r\n    policy.handleCompletedContainer(preempting.get(0));\r\n    policy.handleCompletedContainer(preempting.get(1));\r\n    Iterator<Map.Entry<ContainerId, TaskAttemptId>> it = assignedContainers.entrySet().iterator();\r\n    while (it.hasNext()) {\r\n        Map.Entry<ContainerId, TaskAttemptId> ent = it.next();\r\n        if (ent.getValue().equals(preempting.get(0)) || ent.getValue().equals(preempting.get(1)))\r\n            it.remove();\r\n    }\r\n    policy.preempt(mPctxt, pM);\r\n    List<TaskAttemptId> preempting3 = validatePreemption(pM, policy, supposedMemPreemption);\r\n    assert preempting3.equals(preempting2) == false;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app",
  "methodName" : "validatePreemption",
  "errType" : null,
  "containingMethodsNum" : 10,
  "sourceCodeText" : "List<TaskAttemptId> validatePreemption(PreemptionMessage pM, CheckpointAMPreemptionPolicy policy, int supposedMemPreemption)\n{\r\n    Resource effectivelyPreempted = Resource.newInstance(0, 0);\r\n    List<TaskAttemptId> preempting = new ArrayList<TaskAttemptId>();\r\n    for (Map.Entry<ContainerId, TaskAttemptId> ent : assignedContainers.entrySet()) {\r\n        if (policy.isPreempted(ent.getValue())) {\r\n            Resources.addTo(effectivelyPreempted, contToResourceMap.get(ent.getKey()));\r\n            if (policy.isPreempted(ent.getValue())) {\r\n                assertEquals(TaskType.REDUCE, ent.getValue().getTaskId().getTaskType());\r\n                preempting.add(ent.getValue());\r\n            }\r\n        }\r\n    }\r\n    assert (effectivelyPreempted.getMemorySize() >= supposedMemPreemption) : \" preempted: \" + effectivelyPreempted.getMemorySize();\r\n    assert effectivelyPreempted.getMemorySize() <= supposedMemPreemption + minAlloc;\r\n    return preempting;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app",
  "methodName" : "generatePreemptionMessage",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "PreemptionMessage generatePreemptionMessage(Set<ContainerId> containerToPreempt, HashMap<ContainerId, Resource> resPerCont, Resource minimumAllocation, boolean strict)\n{\r\n    Set<ContainerId> currentContPreemption = Collections.unmodifiableSet(new HashSet<ContainerId>(containerToPreempt));\r\n    containerToPreempt.clear();\r\n    Resource tot = Resource.newInstance(0, 0);\r\n    for (ContainerId c : currentContPreemption) {\r\n        Resources.addTo(tot, resPerCont.get(c));\r\n    }\r\n    int numCont = (int) Math.ceil(tot.getMemorySize() / (double) minimumAllocation.getMemorySize());\r\n    ResourceRequest rr = ResourceRequest.newInstance(Priority.newInstance(0), ResourceRequest.ANY, minimumAllocation, numCont);\r\n    if (strict) {\r\n        return generatePreemptionMessage(new Allocation(null, null, currentContPreemption, null, null));\r\n    }\r\n    return generatePreemptionMessage(new Allocation(null, null, null, currentContPreemption, Collections.singletonList(rr)));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app",
  "methodName" : "generatePreemptionMessage",
  "errType" : null,
  "containingMethodsNum" : 26,
  "sourceCodeText" : "PreemptionMessage generatePreemptionMessage(Allocation allocation)\n{\r\n    PreemptionMessage pMsg = null;\r\n    if (allocation.getStrictContainerPreemptions() != null) {\r\n        pMsg = recordFactory.newRecordInstance(PreemptionMessage.class);\r\n        StrictPreemptionContract pStrict = recordFactory.newRecordInstance(StrictPreemptionContract.class);\r\n        Set<PreemptionContainer> pCont = new HashSet<PreemptionContainer>();\r\n        for (ContainerId cId : allocation.getStrictContainerPreemptions()) {\r\n            PreemptionContainer pc = recordFactory.newRecordInstance(PreemptionContainer.class);\r\n            pc.setId(cId);\r\n            pCont.add(pc);\r\n        }\r\n        pStrict.setContainers(pCont);\r\n        pMsg.setStrictContract(pStrict);\r\n    }\r\n    if (allocation.getResourcePreemptions() != null && allocation.getResourcePreemptions().size() > 0 && allocation.getContainerPreemptions() != null && allocation.getContainerPreemptions().size() > 0) {\r\n        if (pMsg == null) {\r\n            pMsg = recordFactory.newRecordInstance(PreemptionMessage.class);\r\n        }\r\n        PreemptionContract contract = recordFactory.newRecordInstance(PreemptionContract.class);\r\n        Set<PreemptionContainer> pCont = new HashSet<PreemptionContainer>();\r\n        for (ContainerId cId : allocation.getContainerPreemptions()) {\r\n            PreemptionContainer pc = recordFactory.newRecordInstance(PreemptionContainer.class);\r\n            pc.setId(cId);\r\n            pCont.add(pc);\r\n        }\r\n        List<PreemptionResourceRequest> pRes = new ArrayList<PreemptionResourceRequest>();\r\n        for (ResourceRequest crr : allocation.getResourcePreemptions()) {\r\n            PreemptionResourceRequest prr = recordFactory.newRecordInstance(PreemptionResourceRequest.class);\r\n            prr.setResourceRequest(crr);\r\n            pRes.add(prr);\r\n        }\r\n        contract.setContainers(pCont);\r\n        contract.setResourceRequest(pRes);\r\n        pMsg.setContract(contract);\r\n    }\r\n    return pMsg;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\job\\impl",
  "methodName" : "cleanup",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void cleanup()\n{\r\n    UserGroupInformation.reset();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\job\\impl",
  "methodName" : "testAttemptContainerRequest",
  "errType" : null,
  "containingMethodsNum" : 30,
  "sourceCodeText" : "void testAttemptContainerRequest() throws Exception\n{\r\n    final Text SECRET_KEY_ALIAS = new Text(\"secretkeyalias\");\r\n    final byte[] SECRET_KEY = (\"secretkey\").getBytes();\r\n    Map<ApplicationAccessType, String> acls = new HashMap<ApplicationAccessType, String>(1);\r\n    acls.put(ApplicationAccessType.VIEW_APP, \"otheruser\");\r\n    ApplicationId appId = ApplicationId.newInstance(1, 1);\r\n    JobId jobId = MRBuilderUtils.newJobId(appId, 1);\r\n    TaskId taskId = MRBuilderUtils.newTaskId(jobId, 1, TaskType.MAP);\r\n    Path jobFile = mock(Path.class);\r\n    EventHandler eventHandler = mock(EventHandler.class);\r\n    TaskAttemptListener taListener = mock(TaskAttemptListener.class);\r\n    when(taListener.getAddress()).thenReturn(new InetSocketAddress(\"localhost\", 0));\r\n    JobConf jobConf = new JobConf();\r\n    jobConf.setClass(\"fs.file.impl\", StubbedFS.class, FileSystem.class);\r\n    jobConf.setBoolean(\"fs.file.impl.disable.cache\", true);\r\n    jobConf.set(JobConf.MAPRED_MAP_TASK_ENV, \"\");\r\n    jobConf.set(CommonConfigurationKeysPublic.HADOOP_SECURITY_AUTHENTICATION, \"kerberos\");\r\n    UserGroupInformation.setConfiguration(jobConf);\r\n    Credentials credentials = new Credentials();\r\n    credentials.addSecretKey(SECRET_KEY_ALIAS, SECRET_KEY);\r\n    Token<JobTokenIdentifier> jobToken = new Token<JobTokenIdentifier>((\"tokenid\").getBytes(), (\"tokenpw\").getBytes(), new Text(\"tokenkind\"), new Text(\"tokenservice\"));\r\n    TaskAttemptImpl taImpl = new MapTaskAttemptImpl(taskId, 1, eventHandler, jobFile, 1, mock(TaskSplitMetaInfo.class), jobConf, taListener, jobToken, credentials, SystemClock.getInstance(), null);\r\n    jobConf.set(MRJobConfig.APPLICATION_ATTEMPT_ID, taImpl.getID().toString());\r\n    ContainerLaunchContext launchCtx = TaskAttemptImpl.createContainerLaunchContext(acls, jobConf, jobToken, taImpl.createRemoteTask(), TypeConverter.fromYarn(jobId), mock(WrappedJvmID.class), taListener, credentials);\r\n    Assert.assertEquals(\"ACLs mismatch\", acls, launchCtx.getApplicationACLs());\r\n    Credentials launchCredentials = new Credentials();\r\n    DataInputByteBuffer dibb = new DataInputByteBuffer();\r\n    dibb.reset(launchCtx.getTokens());\r\n    launchCredentials.readTokenStorageStream(dibb);\r\n    for (Token<? extends TokenIdentifier> token : credentials.getAllTokens()) {\r\n        Token<? extends TokenIdentifier> launchToken = launchCredentials.getToken(token.getService());\r\n        Assert.assertNotNull(\"Token \" + token.getService() + \" is missing\", launchToken);\r\n        Assert.assertEquals(\"Token \" + token.getService() + \" mismatch\", token, launchToken);\r\n    }\r\n    Assert.assertNotNull(\"Secret key missing\", launchCredentials.getSecretKey(SECRET_KEY_ALIAS));\r\n    Assert.assertTrue(\"Secret key mismatch\", Arrays.equals(SECRET_KEY, launchCredentials.getSecretKey(SECRET_KEY_ALIAS)));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\webapp",
  "methodName" : "setUp",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void setUp() throws Exception\n{\r\n    super.setUp();\r\n    GuiceServletConfig.setInjector(Guice.createInjector(new WebServletModule()));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\webapp",
  "methodName" : "testAM",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void testAM() throws JSONException, Exception\n{\r\n    WebResource r = resource();\r\n    ClientResponse response = r.path(\"ws\").path(\"v1\").path(\"mapreduce\").accept(MediaType.APPLICATION_JSON).get(ClientResponse.class);\r\n    assertEquals(MediaType.APPLICATION_JSON + \"; \" + JettyUtils.UTF_8, response.getType().toString());\r\n    JSONObject json = response.getEntity(JSONObject.class);\r\n    assertEquals(\"incorrect number of elements\", 1, json.length());\r\n    verifyAMInfo(json.getJSONObject(\"info\"), appContext);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\webapp",
  "methodName" : "testAMSlash",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void testAMSlash() throws JSONException, Exception\n{\r\n    WebResource r = resource();\r\n    ClientResponse response = r.path(\"ws\").path(\"v1\").path(\"mapreduce/\").accept(MediaType.APPLICATION_JSON).get(ClientResponse.class);\r\n    assertEquals(MediaType.APPLICATION_JSON + \"; \" + JettyUtils.UTF_8, response.getType().toString());\r\n    JSONObject json = response.getEntity(JSONObject.class);\r\n    assertEquals(\"incorrect number of elements\", 1, json.length());\r\n    verifyAMInfo(json.getJSONObject(\"info\"), appContext);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\webapp",
  "methodName" : "testAMDefault",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void testAMDefault() throws JSONException, Exception\n{\r\n    WebResource r = resource();\r\n    ClientResponse response = r.path(\"ws\").path(\"v1\").path(\"mapreduce/\").get(ClientResponse.class);\r\n    assertEquals(MediaType.APPLICATION_JSON + \"; \" + JettyUtils.UTF_8, response.getType().toString());\r\n    JSONObject json = response.getEntity(JSONObject.class);\r\n    assertEquals(\"incorrect number of elements\", 1, json.length());\r\n    verifyAMInfo(json.getJSONObject(\"info\"), appContext);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\webapp",
  "methodName" : "testAMXML",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void testAMXML() throws JSONException, Exception\n{\r\n    WebResource r = resource();\r\n    ClientResponse response = r.path(\"ws\").path(\"v1\").path(\"mapreduce\").accept(MediaType.APPLICATION_XML).get(ClientResponse.class);\r\n    assertEquals(MediaType.APPLICATION_XML + \"; \" + JettyUtils.UTF_8, response.getType().toString());\r\n    String xml = response.getEntity(String.class);\r\n    verifyAMInfoXML(xml, appContext);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\webapp",
  "methodName" : "testInfo",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void testInfo() throws JSONException, Exception\n{\r\n    WebResource r = resource();\r\n    ClientResponse response = r.path(\"ws\").path(\"v1\").path(\"mapreduce\").path(\"info\").accept(MediaType.APPLICATION_JSON).get(ClientResponse.class);\r\n    assertEquals(MediaType.APPLICATION_JSON + \"; \" + JettyUtils.UTF_8, response.getType().toString());\r\n    JSONObject json = response.getEntity(JSONObject.class);\r\n    assertEquals(\"incorrect number of elements\", 1, json.length());\r\n    verifyAMInfo(json.getJSONObject(\"info\"), appContext);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\webapp",
  "methodName" : "testInfoSlash",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void testInfoSlash() throws JSONException, Exception\n{\r\n    WebResource r = resource();\r\n    ClientResponse response = r.path(\"ws\").path(\"v1\").path(\"mapreduce\").path(\"info/\").accept(MediaType.APPLICATION_JSON).get(ClientResponse.class);\r\n    assertEquals(MediaType.APPLICATION_JSON + \"; \" + JettyUtils.UTF_8, response.getType().toString());\r\n    JSONObject json = response.getEntity(JSONObject.class);\r\n    assertEquals(\"incorrect number of elements\", 1, json.length());\r\n    verifyAMInfo(json.getJSONObject(\"info\"), appContext);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\webapp",
  "methodName" : "testInfoDefault",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void testInfoDefault() throws JSONException, Exception\n{\r\n    WebResource r = resource();\r\n    ClientResponse response = r.path(\"ws\").path(\"v1\").path(\"mapreduce\").path(\"info/\").get(ClientResponse.class);\r\n    assertEquals(MediaType.APPLICATION_JSON + \"; \" + JettyUtils.UTF_8, response.getType().toString());\r\n    JSONObject json = response.getEntity(JSONObject.class);\r\n    assertEquals(\"incorrect number of elements\", 1, json.length());\r\n    verifyAMInfo(json.getJSONObject(\"info\"), appContext);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\webapp",
  "methodName" : "testInfoXML",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void testInfoXML() throws JSONException, Exception\n{\r\n    WebResource r = resource();\r\n    ClientResponse response = r.path(\"ws\").path(\"v1\").path(\"mapreduce\").path(\"info/\").accept(MediaType.APPLICATION_XML).get(ClientResponse.class);\r\n    assertEquals(MediaType.APPLICATION_XML + \"; \" + JettyUtils.UTF_8, response.getType().toString());\r\n    String xml = response.getEntity(String.class);\r\n    verifyAMInfoXML(xml, appContext);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\webapp",
  "methodName" : "testInvalidUri",
  "errType" : [ "UniformInterfaceException" ],
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void testInvalidUri() throws JSONException, Exception\n{\r\n    WebResource r = resource();\r\n    String responseStr = \"\";\r\n    try {\r\n        responseStr = r.path(\"ws\").path(\"v1\").path(\"mapreduce\").path(\"bogus\").accept(MediaType.APPLICATION_JSON).get(String.class);\r\n        fail(\"should have thrown exception on invalid uri\");\r\n    } catch (UniformInterfaceException ue) {\r\n        ClientResponse response = ue.getResponse();\r\n        assertResponseStatusCode(Status.NOT_FOUND, response.getStatusInfo());\r\n        WebServicesTestUtils.checkStringMatch(\"error string exists and shouldn't\", \"\", responseStr);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\webapp",
  "methodName" : "testInvalidUri2",
  "errType" : [ "UniformInterfaceException" ],
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void testInvalidUri2() throws JSONException, Exception\n{\r\n    WebResource r = resource();\r\n    String responseStr = \"\";\r\n    try {\r\n        responseStr = r.path(\"ws\").path(\"v1\").path(\"invalid\").accept(MediaType.APPLICATION_JSON).get(String.class);\r\n        fail(\"should have thrown exception on invalid uri\");\r\n    } catch (UniformInterfaceException ue) {\r\n        ClientResponse response = ue.getResponse();\r\n        assertResponseStatusCode(Status.NOT_FOUND, response.getStatusInfo());\r\n        WebServicesTestUtils.checkStringMatch(\"error string exists and shouldn't\", \"\", responseStr);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\webapp",
  "methodName" : "testInvalidAccept",
  "errType" : [ "UniformInterfaceException" ],
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void testInvalidAccept() throws JSONException, Exception\n{\r\n    WebResource r = resource();\r\n    String responseStr = \"\";\r\n    try {\r\n        responseStr = r.path(\"ws\").path(\"v1\").path(\"mapreduce\").accept(MediaType.TEXT_PLAIN).get(String.class);\r\n        fail(\"should have thrown exception on invalid uri\");\r\n    } catch (UniformInterfaceException ue) {\r\n        ClientResponse response = ue.getResponse();\r\n        assertResponseStatusCode(Status.INTERNAL_SERVER_ERROR, response.getStatusInfo());\r\n        WebServicesTestUtils.checkStringMatch(\"error string exists and shouldn't\", \"\", responseStr);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\webapp",
  "methodName" : "testBlacklistedNodes",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void testBlacklistedNodes() throws JSONException, Exception\n{\r\n    WebResource r = resource();\r\n    ClientResponse response = r.path(\"ws\").path(\"v1\").path(\"mapreduce\").path(\"blacklistednodes\").accept(MediaType.APPLICATION_JSON).get(ClientResponse.class);\r\n    assertEquals(MediaType.APPLICATION_JSON + \"; \" + JettyUtils.UTF_8, response.getType().toString());\r\n    JSONObject json = response.getEntity(JSONObject.class);\r\n    assertEquals(\"incorrect number of elements\", 1, json.length());\r\n    verifyBlacklistedNodesInfo(json, appContext);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\webapp",
  "methodName" : "testBlacklistedNodesXML",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void testBlacklistedNodesXML() throws Exception\n{\r\n    WebResource r = resource();\r\n    ClientResponse response = r.path(\"ws\").path(\"v1\").path(\"mapreduce\").path(\"blacklistednodes\").accept(MediaType.APPLICATION_XML).get(ClientResponse.class);\r\n    assertEquals(MediaType.APPLICATION_XML + \"; \" + JettyUtils.UTF_8, response.getType().toString());\r\n    String xml = response.getEntity(String.class);\r\n    verifyBlacklistedNodesInfoXML(xml, appContext);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\webapp",
  "methodName" : "verifyAMInfo",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void verifyAMInfo(JSONObject info, AppContext ctx) throws JSONException\n{\r\n    assertEquals(\"incorrect number of elements\", 5, info.length());\r\n    verifyAMInfoGeneric(ctx, info.getString(\"appId\"), info.getString(\"user\"), info.getString(\"name\"), info.getLong(\"startedOn\"), info.getLong(\"elapsedTime\"));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\webapp",
  "methodName" : "verifyAMInfoXML",
  "errType" : null,
  "containingMethodsNum" : 9,
  "sourceCodeText" : "void verifyAMInfoXML(String xml, AppContext ctx) throws JSONException, Exception\n{\r\n    DocumentBuilderFactory dbf = DocumentBuilderFactory.newInstance();\r\n    DocumentBuilder db = dbf.newDocumentBuilder();\r\n    InputSource is = new InputSource();\r\n    is.setCharacterStream(new StringReader(xml));\r\n    Document dom = db.parse(is);\r\n    NodeList nodes = dom.getElementsByTagName(\"info\");\r\n    assertEquals(\"incorrect number of elements\", 1, nodes.getLength());\r\n    for (int i = 0; i < nodes.getLength(); i++) {\r\n        Element element = (Element) nodes.item(i);\r\n        verifyAMInfoGeneric(ctx, WebServicesTestUtils.getXmlString(element, \"appId\"), WebServicesTestUtils.getXmlString(element, \"user\"), WebServicesTestUtils.getXmlString(element, \"name\"), WebServicesTestUtils.getXmlLong(element, \"startedOn\"), WebServicesTestUtils.getXmlLong(element, \"elapsedTime\"));\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\webapp",
  "methodName" : "verifyAMInfoGeneric",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void verifyAMInfoGeneric(AppContext ctx, String id, String user, String name, long startedOn, long elapsedTime)\n{\r\n    WebServicesTestUtils.checkStringMatch(\"id\", ctx.getApplicationID().toString(), id);\r\n    WebServicesTestUtils.checkStringMatch(\"user\", ctx.getUser().toString(), user);\r\n    WebServicesTestUtils.checkStringMatch(\"name\", ctx.getApplicationName(), name);\r\n    assertEquals(\"startedOn incorrect\", ctx.getStartTime(), startedOn);\r\n    assertTrue(\"elapsedTime not greater then 0\", (elapsedTime > 0));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\webapp",
  "methodName" : "verifyBlacklistedNodesInfo",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void verifyBlacklistedNodesInfo(JSONObject blacklist, AppContext ctx) throws JSONException, Exception\n{\r\n    JSONArray array = blacklist.getJSONArray(\"blacklistedNodes\");\r\n    assertEquals(array.length(), ctx.getBlacklistedNodes().size());\r\n    for (int i = 0; i < array.length(); i++) {\r\n        assertTrue(ctx.getBlacklistedNodes().contains(array.getString(i)));\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\webapp",
  "methodName" : "verifyBlacklistedNodesInfoXML",
  "errType" : null,
  "containingMethodsNum" : 12,
  "sourceCodeText" : "void verifyBlacklistedNodesInfoXML(String xml, AppContext ctx) throws JSONException, Exception\n{\r\n    DocumentBuilderFactory dbf = DocumentBuilderFactory.newInstance();\r\n    DocumentBuilder db = dbf.newDocumentBuilder();\r\n    InputSource is = new InputSource();\r\n    is.setCharacterStream(new StringReader(xml));\r\n    Document dom = db.parse(is);\r\n    NodeList infonodes = dom.getElementsByTagName(\"blacklistednodesinfo\");\r\n    assertEquals(\"incorrect number of elements\", 1, infonodes.getLength());\r\n    NodeList nodes = dom.getElementsByTagName(\"blacklistedNodes\");\r\n    Set<String> blacklistedNodes = ctx.getBlacklistedNodes();\r\n    assertEquals(\"incorrect number of elements\", blacklistedNodes.size(), nodes.getLength());\r\n    for (int i = 0; i < nodes.getLength(); i++) {\r\n        Element element = (Element) nodes.item(i);\r\n        assertTrue(blacklistedNodes.contains(element.getFirstChild().getNodeValue()));\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\commit",
  "methodName" : "setup",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setup()\n{\r\n    File dir = new File(stagingDir);\r\n    stagingDir = dir.getAbsolutePath();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\commit",
  "methodName" : "cleanup",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void cleanup() throws IOException\n{\r\n    File dir = new File(stagingDir);\r\n    if (dir.exists()) {\r\n        FileUtils.deleteDirectory(dir);\r\n    }\r\n    dir.mkdirs();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\commit",
  "methodName" : "testCommitWindow",
  "errType" : null,
  "containingMethodsNum" : 31,
  "sourceCodeText" : "void testCommitWindow() throws Exception\n{\r\n    Configuration conf = new Configuration();\r\n    conf.set(MRJobConfig.MR_AM_STAGING_DIR, stagingDir);\r\n    AsyncDispatcher dispatcher = new AsyncDispatcher();\r\n    dispatcher.init(conf);\r\n    dispatcher.start();\r\n    TestingJobEventHandler jeh = new TestingJobEventHandler();\r\n    dispatcher.register(JobEventType.class, jeh);\r\n    SystemClock clock = SystemClock.getInstance();\r\n    AppContext appContext = mock(AppContext.class);\r\n    ApplicationAttemptId attemptid = ApplicationAttemptId.fromString(\"appattempt_1234567890000_0001_0\");\r\n    when(appContext.getApplicationID()).thenReturn(attemptid.getApplicationId());\r\n    when(appContext.getApplicationAttemptId()).thenReturn(attemptid);\r\n    when(appContext.getEventHandler()).thenReturn(dispatcher.getEventHandler());\r\n    when(appContext.getClock()).thenReturn(clock);\r\n    OutputCommitter committer = mock(OutputCommitter.class);\r\n    TestingRMHeartbeatHandler rmhh = new TestingRMHeartbeatHandler();\r\n    CommitterEventHandler ceh = new CommitterEventHandler(appContext, committer, rmhh);\r\n    ceh.init(conf);\r\n    ceh.start();\r\n    ceh.handle(new CommitterJobCommitEvent(null, null));\r\n    long timeToWaitMs = 5000;\r\n    while (rmhh.getNumCallbacks() != 1 && timeToWaitMs > 0) {\r\n        Thread.sleep(10);\r\n        timeToWaitMs -= 10;\r\n    }\r\n    Assert.assertEquals(\"committer did not register a heartbeat callback\", 1, rmhh.getNumCallbacks());\r\n    verify(committer, never()).commitJob(any(JobContext.class));\r\n    Assert.assertEquals(\"committer should not have committed\", 0, jeh.numCommitCompletedEvents);\r\n    rmhh.setLastHeartbeatTime(clock.getTime());\r\n    timeToWaitMs = 5000;\r\n    while (jeh.numCommitCompletedEvents != 1 && timeToWaitMs > 0) {\r\n        Thread.sleep(10);\r\n        timeToWaitMs -= 10;\r\n    }\r\n    Assert.assertEquals(\"committer did not complete commit after RM hearbeat\", 1, jeh.numCommitCompletedEvents);\r\n    verify(committer, times(1)).commitJob(any());\r\n    cleanup();\r\n    ceh.handle(new CommitterJobCommitEvent(null, null));\r\n    timeToWaitMs = 5000;\r\n    while (jeh.numCommitCompletedEvents != 2 && timeToWaitMs > 0) {\r\n        Thread.sleep(10);\r\n        timeToWaitMs -= 10;\r\n    }\r\n    Assert.assertEquals(\"committer did not commit\", 2, jeh.numCommitCompletedEvents);\r\n    verify(committer, times(2)).commitJob(any());\r\n    ceh.stop();\r\n    dispatcher.stop();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\commit",
  "methodName" : "testBasic",
  "errType" : null,
  "containingMethodsNum" : 27,
  "sourceCodeText" : "void testBasic() throws Exception\n{\r\n    AppContext mockContext = mock(AppContext.class);\r\n    OutputCommitter mockCommitter = mock(OutputCommitter.class);\r\n    Clock mockClock = mock(Clock.class);\r\n    CommitterEventHandler handler = new CommitterEventHandler(mockContext, mockCommitter, new TestingRMHeartbeatHandler());\r\n    YarnConfiguration conf = new YarnConfiguration();\r\n    conf.set(MRJobConfig.MR_AM_STAGING_DIR, stagingDir);\r\n    JobContext mockJobContext = mock(JobContext.class);\r\n    ApplicationAttemptId attemptid = ApplicationAttemptId.fromString(\"appattempt_1234567890000_0001_0\");\r\n    JobId jobId = TypeConverter.toYarn(TypeConverter.fromYarn(attemptid.getApplicationId()));\r\n    WaitForItHandler waitForItHandler = new WaitForItHandler();\r\n    when(mockContext.getApplicationID()).thenReturn(attemptid.getApplicationId());\r\n    when(mockContext.getApplicationAttemptId()).thenReturn(attemptid);\r\n    when(mockContext.getEventHandler()).thenReturn(waitForItHandler);\r\n    when(mockContext.getClock()).thenReturn(mockClock);\r\n    handler.init(conf);\r\n    handler.start();\r\n    try {\r\n        handler.handle(new CommitterJobCommitEvent(jobId, mockJobContext));\r\n        String user = UserGroupInformation.getCurrentUser().getShortUserName();\r\n        Path startCommitFile = MRApps.getStartJobCommitFile(conf, user, jobId);\r\n        Path endCommitSuccessFile = MRApps.getEndJobCommitSuccessFile(conf, user, jobId);\r\n        Path endCommitFailureFile = MRApps.getEndJobCommitFailureFile(conf, user, jobId);\r\n        Event e = waitForItHandler.getAndClearEvent();\r\n        assertNotNull(e);\r\n        assertTrue(e instanceof JobCommitCompletedEvent);\r\n        FileSystem fs = FileSystem.get(conf);\r\n        assertTrue(startCommitFile.toString(), fs.exists(startCommitFile));\r\n        assertTrue(endCommitSuccessFile.toString(), fs.exists(endCommitSuccessFile));\r\n        assertFalse(endCommitFailureFile.toString(), fs.exists(endCommitFailureFile));\r\n        verify(mockCommitter).commitJob(any(JobContext.class));\r\n    } finally {\r\n        handler.stop();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\commit",
  "methodName" : "testFailure",
  "errType" : null,
  "containingMethodsNum" : 28,
  "sourceCodeText" : "void testFailure() throws Exception\n{\r\n    AppContext mockContext = mock(AppContext.class);\r\n    OutputCommitter mockCommitter = mock(OutputCommitter.class);\r\n    Clock mockClock = mock(Clock.class);\r\n    CommitterEventHandler handler = new CommitterEventHandler(mockContext, mockCommitter, new TestingRMHeartbeatHandler());\r\n    YarnConfiguration conf = new YarnConfiguration();\r\n    conf.set(MRJobConfig.MR_AM_STAGING_DIR, stagingDir);\r\n    JobContext mockJobContext = mock(JobContext.class);\r\n    ApplicationAttemptId attemptid = ApplicationAttemptId.fromString(\"appattempt_1234567890000_0001_0\");\r\n    JobId jobId = TypeConverter.toYarn(TypeConverter.fromYarn(attemptid.getApplicationId()));\r\n    WaitForItHandler waitForItHandler = new WaitForItHandler();\r\n    when(mockContext.getApplicationID()).thenReturn(attemptid.getApplicationId());\r\n    when(mockContext.getApplicationAttemptId()).thenReturn(attemptid);\r\n    when(mockContext.getEventHandler()).thenReturn(waitForItHandler);\r\n    when(mockContext.getClock()).thenReturn(mockClock);\r\n    doThrow(new YarnRuntimeException(\"Intentional Failure\")).when(mockCommitter).commitJob(any(JobContext.class));\r\n    handler.init(conf);\r\n    handler.start();\r\n    try {\r\n        handler.handle(new CommitterJobCommitEvent(jobId, mockJobContext));\r\n        String user = UserGroupInformation.getCurrentUser().getShortUserName();\r\n        Path startCommitFile = MRApps.getStartJobCommitFile(conf, user, jobId);\r\n        Path endCommitSuccessFile = MRApps.getEndJobCommitSuccessFile(conf, user, jobId);\r\n        Path endCommitFailureFile = MRApps.getEndJobCommitFailureFile(conf, user, jobId);\r\n        Event e = waitForItHandler.getAndClearEvent();\r\n        assertNotNull(e);\r\n        assertTrue(e instanceof JobCommitFailedEvent);\r\n        FileSystem fs = FileSystem.get(conf);\r\n        assertTrue(fs.exists(startCommitFile));\r\n        assertFalse(fs.exists(endCommitSuccessFile));\r\n        assertTrue(fs.exists(endCommitFailureFile));\r\n        verify(mockCommitter).commitJob(any(JobContext.class));\r\n    } finally {\r\n        handler.stop();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app",
  "methodName" : "testAMInfosWithoutRecoveryEnabled",
  "errType" : null,
  "containingMethodsNum" : 25,
  "sourceCodeText" : "void testAMInfosWithoutRecoveryEnabled() throws Exception\n{\r\n    int runCount = 0;\r\n    MRApp app = new MRAppWithHistory(1, 0, false, this.getClass().getName(), true, ++runCount);\r\n    Configuration conf = new Configuration();\r\n    conf.setBoolean(MRJobConfig.JOB_UBERTASK_ENABLE, false);\r\n    Job job = app.submit(conf);\r\n    app.waitForState(job, JobState.RUNNING);\r\n    long am1StartTime = app.getAllAMInfos().get(0).getStartTime();\r\n    Assert.assertEquals(\"No of tasks not correct\", 1, job.getTasks().size());\r\n    Iterator<Task> it = job.getTasks().values().iterator();\r\n    Task mapTask = it.next();\r\n    app.waitForState(mapTask, TaskState.RUNNING);\r\n    TaskAttempt taskAttempt = mapTask.getAttempts().values().iterator().next();\r\n    app.waitForState(taskAttempt, TaskAttemptState.RUNNING);\r\n    app.stop();\r\n    app = new MRAppWithHistory(1, 0, false, this.getClass().getName(), false, ++runCount);\r\n    conf = new Configuration();\r\n    conf.setBoolean(MRJobConfig.MR_AM_JOB_RECOVERY_ENABLE, false);\r\n    conf.setBoolean(MRJobConfig.JOB_UBERTASK_ENABLE, false);\r\n    job = app.submit(conf);\r\n    app.waitForState(job, JobState.RUNNING);\r\n    Assert.assertEquals(\"No of tasks not correct\", 1, job.getTasks().size());\r\n    it = job.getTasks().values().iterator();\r\n    mapTask = it.next();\r\n    List<AMInfo> amInfos = app.getAllAMInfos();\r\n    Assert.assertEquals(2, amInfos.size());\r\n    AMInfo amInfoOne = amInfos.get(0);\r\n    Assert.assertEquals(am1StartTime, amInfoOne.getStartTime());\r\n    app.stop();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\rm",
  "methodName" : "createRequest",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "ContainerRequestEvent createRequest(JobId jobId, int taskAttemptId, Resource resource, String[] hosts)\n{\r\n    return createRequest(jobId, taskAttemptId, resource, hosts, false, false);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\rm",
  "methodName" : "createRequest",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "ContainerRequestEvent createRequest(JobId jobId, int taskAttemptId, Resource resource, String[] hosts, boolean earlierFailedAttempt, boolean reduce)\n{\r\n    final TaskId taskId;\r\n    if (reduce) {\r\n        taskId = MRBuilderUtils.newTaskId(jobId, 0, TaskType.REDUCE);\r\n    } else {\r\n        taskId = MRBuilderUtils.newTaskId(jobId, 0, TaskType.MAP);\r\n    }\r\n    TaskAttemptId attemptId = MRBuilderUtils.newTaskAttemptId(taskId, taskAttemptId);\r\n    if (earlierFailedAttempt) {\r\n        return ContainerRequestEvent.createContainerRequestEventForFailedContainer(attemptId, resource);\r\n    }\r\n    return new ContainerRequestEvent(attemptId, resource, hosts, new String[] { NetworkTopology.DEFAULT_RACK });\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app",
  "methodName" : "setupClass",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void setupClass() throws Exception\n{\r\n    testRootDir = GenericTestUtils.setupTestRootDir(TestRecovery.class);\r\n    outputDir = new Path(testRootDir.getAbsolutePath(), \"out\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app",
  "methodName" : "testCrashed",
  "errType" : null,
  "containingMethodsNum" : 97,
  "sourceCodeText" : "void testCrashed() throws Exception\n{\r\n    int runCount = 0;\r\n    long am1StartTimeEst = System.currentTimeMillis();\r\n    MRApp app = new MRAppWithHistory(2, 1, false, this.getClass().getName(), true, ++runCount);\r\n    Configuration conf = new Configuration();\r\n    conf.setBoolean(\"mapred.mapper.new-api\", true);\r\n    conf.setBoolean(\"mapred.reducer.new-api\", true);\r\n    conf.setBoolean(MRJobConfig.JOB_UBERTASK_ENABLE, false);\r\n    conf.set(FileOutputFormat.OUTDIR, outputDir.toString());\r\n    Job job = app.submit(conf);\r\n    app.waitForState(job, JobState.RUNNING);\r\n    long jobStartTime = job.getReport().getStartTime();\r\n    Assert.assertEquals(\"No of tasks not correct\", 3, job.getTasks().size());\r\n    Iterator<Task> it = job.getTasks().values().iterator();\r\n    Task mapTask1 = it.next();\r\n    Task mapTask2 = it.next();\r\n    Task reduceTask = it.next();\r\n    app.waitForState(mapTask1, TaskState.RUNNING);\r\n    app.waitForState(mapTask2, TaskState.RUNNING);\r\n    TaskAttempt task1Attempt1 = mapTask1.getAttempts().values().iterator().next();\r\n    TaskAttempt task2Attempt = mapTask2.getAttempts().values().iterator().next();\r\n    app.waitForState(task1Attempt1, TaskAttemptState.RUNNING);\r\n    app.waitForState(task2Attempt, TaskAttemptState.RUNNING);\r\n    app.waitForState(reduceTask, TaskState.RUNNING);\r\n    app.getContext().getEventHandler().handle(new TaskAttemptFailEvent(task1Attempt1.getID()));\r\n    app.waitForState(task1Attempt1, TaskAttemptState.FAILED);\r\n    int timeOut = 0;\r\n    while (mapTask1.getAttempts().size() != 2 && timeOut++ < 10) {\r\n        Thread.sleep(2000);\r\n        LOG.info(\"Waiting for next attempt to start\");\r\n    }\r\n    Assert.assertEquals(2, mapTask1.getAttempts().size());\r\n    Iterator<TaskAttempt> itr = mapTask1.getAttempts().values().iterator();\r\n    itr.next();\r\n    TaskAttempt task1Attempt2 = itr.next();\r\n    waitForContainerAssignment(task1Attempt2);\r\n    app.getContext().getEventHandler().handle(new TaskAttemptEvent(task1Attempt2.getID(), TaskAttemptEventType.TA_CONTAINER_LAUNCH_FAILED));\r\n    app.waitForState(task1Attempt2, TaskAttemptState.FAILED);\r\n    timeOut = 0;\r\n    while (mapTask1.getAttempts().size() != 3 && timeOut++ < 10) {\r\n        Thread.sleep(2000);\r\n        LOG.info(\"Waiting for next attempt to start\");\r\n    }\r\n    Assert.assertEquals(3, mapTask1.getAttempts().size());\r\n    itr = mapTask1.getAttempts().values().iterator();\r\n    itr.next();\r\n    itr.next();\r\n    TaskAttempt task1Attempt3 = itr.next();\r\n    app.waitForState(task1Attempt3, TaskAttemptState.RUNNING);\r\n    app.getContext().getEventHandler().handle(new TaskAttemptEvent(task1Attempt3.getID(), TaskAttemptEventType.TA_KILL));\r\n    app.waitForState(task1Attempt3, TaskAttemptState.KILLED);\r\n    timeOut = 0;\r\n    while (mapTask1.getAttempts().size() != 4 && timeOut++ < 10) {\r\n        Thread.sleep(2000);\r\n        LOG.info(\"Waiting for next attempt to start\");\r\n    }\r\n    Assert.assertEquals(4, mapTask1.getAttempts().size());\r\n    itr = mapTask1.getAttempts().values().iterator();\r\n    itr.next();\r\n    itr.next();\r\n    itr.next();\r\n    TaskAttempt task1Attempt4 = itr.next();\r\n    app.waitForState(task1Attempt4, TaskAttemptState.RUNNING);\r\n    app.getContext().getEventHandler().handle(new TaskAttemptEvent(task1Attempt4.getID(), TaskAttemptEventType.TA_DONE));\r\n    app.waitForState(mapTask1, TaskState.SUCCEEDED);\r\n    long task1StartTime = mapTask1.getReport().getStartTime();\r\n    long task1FinishTime = mapTask1.getReport().getFinishTime();\r\n    app.stop();\r\n    long am2StartTimeEst = System.currentTimeMillis();\r\n    app = new MRAppWithHistory(2, 1, false, this.getClass().getName(), false, ++runCount);\r\n    conf = new Configuration();\r\n    conf.setBoolean(MRJobConfig.MR_AM_JOB_RECOVERY_ENABLE, true);\r\n    conf.setBoolean(\"mapred.mapper.new-api\", true);\r\n    conf.setBoolean(\"mapred.reducer.new-api\", true);\r\n    conf.set(FileOutputFormat.OUTDIR, outputDir.toString());\r\n    conf.setBoolean(MRJobConfig.JOB_UBERTASK_ENABLE, false);\r\n    job = app.submit(conf);\r\n    app.waitForState(job, JobState.RUNNING);\r\n    Assert.assertEquals(\"No of tasks not correct\", 3, job.getTasks().size());\r\n    it = job.getTasks().values().iterator();\r\n    mapTask1 = it.next();\r\n    mapTask2 = it.next();\r\n    reduceTask = it.next();\r\n    app.waitForState(mapTask1, TaskState.SUCCEEDED);\r\n    app.waitForState(mapTask2, TaskState.RUNNING);\r\n    task2Attempt = mapTask2.getAttempts().values().iterator().next();\r\n    app.waitForState(task2Attempt, TaskAttemptState.RUNNING);\r\n    app.getContext().getEventHandler().handle(new TaskAttemptEvent(mapTask2.getAttempts().values().iterator().next().getID(), TaskAttemptEventType.TA_DONE));\r\n    app.waitForState(mapTask2, TaskState.SUCCEEDED);\r\n    app.waitForState(reduceTask, TaskState.RUNNING);\r\n    app.getContext().getEventHandler().handle(new TaskAttemptEvent(reduceTask.getAttempts().values().iterator().next().getID(), TaskAttemptEventType.TA_DONE));\r\n    app.waitForState(job, JobState.SUCCEEDED);\r\n    app.verifyCompleted();\r\n    Assert.assertEquals(\"Job Start time not correct\", jobStartTime, job.getReport().getStartTime());\r\n    Assert.assertEquals(\"Task Start time not correct\", task1StartTime, mapTask1.getReport().getStartTime());\r\n    Assert.assertEquals(\"Task Finish time not correct\", task1FinishTime, mapTask1.getReport().getFinishTime());\r\n    Assert.assertEquals(2, job.getAMInfos().size());\r\n    int attemptNum = 1;\r\n    for (AMInfo amInfo : job.getAMInfos()) {\r\n        Assert.assertEquals(attemptNum++, amInfo.getAppAttemptId().getAttemptId());\r\n        Assert.assertEquals(amInfo.getAppAttemptId(), amInfo.getContainerId().getApplicationAttemptId());\r\n        Assert.assertEquals(MRApp.NM_HOST, amInfo.getNodeManagerHost());\r\n        Assert.assertEquals(MRApp.NM_PORT, amInfo.getNodeManagerPort());\r\n        Assert.assertEquals(MRApp.NM_HTTP_PORT, amInfo.getNodeManagerHttpPort());\r\n    }\r\n    long am1StartTimeReal = job.getAMInfos().get(0).getStartTime();\r\n    long am2StartTimeReal = job.getAMInfos().get(1).getStartTime();\r\n    Assert.assertTrue(am1StartTimeReal >= am1StartTimeEst && am1StartTimeReal <= am2StartTimeEst);\r\n    Assert.assertTrue(am2StartTimeReal >= am2StartTimeEst && am2StartTimeReal <= System.currentTimeMillis());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app",
  "methodName" : "waitForContainerAssignment",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void waitForContainerAssignment(final TaskAttempt task1Attempt2) throws TimeoutException, InterruptedException\n{\r\n    GenericTestUtils.waitFor(new Supplier<Boolean>() {\r\n\r\n        @Override\r\n        public Boolean get() {\r\n            return task1Attempt2.getAssignedContainerID() != null;\r\n        }\r\n    }, 10, 10000);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app",
  "methodName" : "testCrashOfMapsOnlyJob",
  "errType" : null,
  "containingMethodsNum" : 47,
  "sourceCodeText" : "void testCrashOfMapsOnlyJob() throws Exception\n{\r\n    int runCount = 0;\r\n    MRApp app = new MRAppWithHistory(3, 0, false, this.getClass().getName(), true, ++runCount);\r\n    Configuration conf = new Configuration();\r\n    conf.setBoolean(\"mapred.mapper.new-api\", true);\r\n    conf.setBoolean(MRJobConfig.JOB_UBERTASK_ENABLE, false);\r\n    conf.set(FileOutputFormat.OUTDIR, outputDir.toString());\r\n    Job job = app.submit(conf);\r\n    app.waitForState(job, JobState.RUNNING);\r\n    Assert.assertEquals(\"No of tasks not correct\", 3, job.getTasks().size());\r\n    Iterator<Task> it = job.getTasks().values().iterator();\r\n    Task mapTask1 = it.next();\r\n    Task mapTask2 = it.next();\r\n    Task mapTask3 = it.next();\r\n    app.waitForState(mapTask1, TaskState.RUNNING);\r\n    app.waitForState(mapTask2, TaskState.RUNNING);\r\n    app.waitForState(mapTask3, TaskState.RUNNING);\r\n    TaskAttempt task1Attempt = mapTask1.getAttempts().values().iterator().next();\r\n    TaskAttempt task2Attempt = mapTask2.getAttempts().values().iterator().next();\r\n    TaskAttempt task3Attempt = mapTask3.getAttempts().values().iterator().next();\r\n    app.waitForState(task1Attempt, TaskAttemptState.RUNNING);\r\n    app.waitForState(task2Attempt, TaskAttemptState.RUNNING);\r\n    app.waitForState(task3Attempt, TaskAttemptState.RUNNING);\r\n    app.getContext().getEventHandler().handle(new TaskAttemptEvent(task1Attempt.getID(), TaskAttemptEventType.TA_DONE));\r\n    app.getContext().getEventHandler().handle(new TaskAttemptEvent(task2Attempt.getID(), TaskAttemptEventType.TA_DONE));\r\n    app.waitForState(mapTask1, TaskState.SUCCEEDED);\r\n    app.waitForState(mapTask2, TaskState.SUCCEEDED);\r\n    app.stop();\r\n    app = new MRAppWithHistory(2, 1, false, this.getClass().getName(), false, ++runCount);\r\n    conf = new Configuration();\r\n    conf.setBoolean(MRJobConfig.MR_AM_JOB_RECOVERY_ENABLE, true);\r\n    conf.setBoolean(\"mapred.mapper.new-api\", true);\r\n    conf.set(FileOutputFormat.OUTDIR, outputDir.toString());\r\n    conf.setInt(MRJobConfig.NUM_REDUCES, 0);\r\n    conf.setBoolean(MRJobConfig.JOB_UBERTASK_ENABLE, false);\r\n    job = app.submit(conf);\r\n    app.waitForState(job, JobState.RUNNING);\r\n    Assert.assertEquals(\"No of tasks not correct\", 3, job.getTasks().size());\r\n    it = job.getTasks().values().iterator();\r\n    mapTask1 = it.next();\r\n    mapTask2 = it.next();\r\n    mapTask3 = it.next();\r\n    app.waitForState(mapTask1, TaskState.SUCCEEDED);\r\n    app.waitForState(mapTask2, TaskState.SUCCEEDED);\r\n    app.waitForState(mapTask3, TaskState.RUNNING);\r\n    task3Attempt = mapTask3.getAttempts().values().iterator().next();\r\n    app.waitForState(task3Attempt, TaskAttemptState.RUNNING);\r\n    app.getContext().getEventHandler().handle(new TaskAttemptEvent(mapTask3.getAttempts().values().iterator().next().getID(), TaskAttemptEventType.TA_DONE));\r\n    app.waitForState(mapTask3, TaskState.SUCCEEDED);\r\n    app.waitForState(job, JobState.SUCCEEDED);\r\n    app.verifyCompleted();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app",
  "methodName" : "testRecoverySuccessUsingCustomOutputCommitter",
  "errType" : null,
  "containingMethodsNum" : 47,
  "sourceCodeText" : "void testRecoverySuccessUsingCustomOutputCommitter() throws Exception\n{\r\n    int runCount = 0;\r\n    MRApp app = new MRAppWithHistory(3, 0, false, this.getClass().getName(), true, ++runCount);\r\n    Configuration conf = new Configuration();\r\n    conf.setClass(\"mapred.output.committer.class\", TestFileOutputCommitter.class, org.apache.hadoop.mapred.OutputCommitter.class);\r\n    conf.set(FileOutputFormat.OUTDIR, outputDir.toString());\r\n    conf.setBoolean(\"want.am.recovery\", true);\r\n    Job job = app.submit(conf);\r\n    app.waitForState(job, JobState.RUNNING);\r\n    Assert.assertEquals(\"No of tasks not correct\", 3, job.getTasks().size());\r\n    Iterator<Task> it = job.getTasks().values().iterator();\r\n    Task mapTask1 = it.next();\r\n    Task mapTask2 = it.next();\r\n    Task mapTask3 = it.next();\r\n    app.waitForState(mapTask1, TaskState.RUNNING);\r\n    app.waitForState(mapTask2, TaskState.RUNNING);\r\n    app.waitForState(mapTask3, TaskState.RUNNING);\r\n    TaskAttempt task1Attempt = mapTask1.getAttempts().values().iterator().next();\r\n    TaskAttempt task2Attempt = mapTask2.getAttempts().values().iterator().next();\r\n    TaskAttempt task3Attempt = mapTask3.getAttempts().values().iterator().next();\r\n    app.waitForState(task1Attempt, TaskAttemptState.RUNNING);\r\n    app.waitForState(task2Attempt, TaskAttemptState.RUNNING);\r\n    app.waitForState(task3Attempt, TaskAttemptState.RUNNING);\r\n    app.getContext().getEventHandler().handle(new TaskAttemptEvent(task1Attempt.getID(), TaskAttemptEventType.TA_DONE));\r\n    app.getContext().getEventHandler().handle(new TaskAttemptEvent(task2Attempt.getID(), TaskAttemptEventType.TA_DONE));\r\n    app.waitForState(mapTask1, TaskState.SUCCEEDED);\r\n    app.waitForState(mapTask2, TaskState.SUCCEEDED);\r\n    app.stop();\r\n    app = new MRAppWithHistory(2, 1, false, this.getClass().getName(), false, ++runCount);\r\n    conf = new Configuration();\r\n    conf.setClass(\"mapred.output.committer.class\", TestFileOutputCommitter.class, org.apache.hadoop.mapred.OutputCommitter.class);\r\n    conf.setBoolean(\"want.am.recovery\", true);\r\n    conf.set(FileOutputFormat.OUTDIR, outputDir.toString());\r\n    conf.setInt(MRJobConfig.NUM_REDUCES, 0);\r\n    conf.setBoolean(MRJobConfig.JOB_UBERTASK_ENABLE, false);\r\n    job = app.submit(conf);\r\n    app.waitForState(job, JobState.RUNNING);\r\n    Assert.assertEquals(\"No of tasks not correct\", 3, job.getTasks().size());\r\n    it = job.getTasks().values().iterator();\r\n    mapTask1 = it.next();\r\n    mapTask2 = it.next();\r\n    mapTask3 = it.next();\r\n    app.waitForState(mapTask1, TaskState.SUCCEEDED);\r\n    app.waitForState(mapTask2, TaskState.SUCCEEDED);\r\n    app.waitForState(mapTask3, TaskState.RUNNING);\r\n    task3Attempt = mapTask3.getAttempts().values().iterator().next();\r\n    app.waitForState(task3Attempt, TaskAttemptState.RUNNING);\r\n    app.getContext().getEventHandler().handle(new TaskAttemptEvent(mapTask3.getAttempts().values().iterator().next().getID(), TaskAttemptEventType.TA_DONE));\r\n    app.waitForState(mapTask3, TaskState.SUCCEEDED);\r\n    app.waitForState(job, JobState.SUCCEEDED);\r\n    app.verifyCompleted();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app",
  "methodName" : "testRecoveryWithSpillEncryption",
  "errType" : null,
  "containingMethodsNum" : 34,
  "sourceCodeText" : "void testRecoveryWithSpillEncryption() throws Exception\n{\r\n    int runCount = 0;\r\n    MRApp app = new MRAppWithHistory(1, 1, false, this.getClass().getName(), true, ++runCount) {\r\n    };\r\n    Configuration conf = MRJobConfUtil.initEncryptedIntermediateConfigsForTesting(null);\r\n    conf.setBoolean(MRJobConfig.MR_AM_JOB_RECOVERY_ENABLE, true);\r\n    conf.setBoolean(\"mapred.mapper.new-api\", true);\r\n    conf.setBoolean(\"mapred.reducer.new-api\", true);\r\n    conf.setBoolean(MRJobConfig.JOB_UBERTASK_ENABLE, false);\r\n    conf.set(FileOutputFormat.OUTDIR, outputDir.toString());\r\n    Job jobAttempt1 = app.submit(conf);\r\n    app.waitForState(jobAttempt1, JobState.RUNNING);\r\n    Iterator<Task> tasks = jobAttempt1.getTasks().values().iterator();\r\n    Task mapper = tasks.next();\r\n    app.waitForState(mapper, TaskState.RUNNING);\r\n    TaskAttempt mapAttempt = mapper.getAttempts().values().iterator().next();\r\n    app.waitForState(mapAttempt, TaskAttemptState.RUNNING);\r\n    app.getContext().getEventHandler().handle(new TaskAttemptEvent(mapAttempt.getID(), TaskAttemptEventType.TA_DONE));\r\n    app.waitForState(mapper, TaskState.SUCCEEDED);\r\n    app.stop();\r\n    app = new MRAppWithHistory(1, 1, false, this.getClass().getName(), false, ++runCount);\r\n    Job jobAttempt2 = app.submit(conf);\r\n    Assert.assertTrue(\"Recovery from previous job attempt is processed even \" + \"though intermediate data encryption is enabled.\", !app.recovered());\r\n    app.waitForState(jobAttempt2, JobState.RUNNING);\r\n    tasks = jobAttempt2.getTasks().values().iterator();\r\n    mapper = tasks.next();\r\n    Task reducer = tasks.next();\r\n    app.waitForState(mapper, TaskState.RUNNING);\r\n    mapAttempt = mapper.getAttempts().values().iterator().next();\r\n    app.waitForState(mapAttempt, TaskAttemptState.RUNNING);\r\n    app.getContext().getEventHandler().handle(new TaskAttemptEvent(mapAttempt.getID(), TaskAttemptEventType.TA_DONE));\r\n    app.waitForState(mapper, TaskState.SUCCEEDED);\r\n    TaskAttempt redAttempt = reducer.getAttempts().values().iterator().next();\r\n    app.waitForState(redAttempt, TaskAttemptState.RUNNING);\r\n    app.getContext().getEventHandler().handle(new TaskAttemptEvent(redAttempt.getID(), TaskAttemptEventType.TA_DONE));\r\n    app.waitForState(reducer, TaskState.SUCCEEDED);\r\n    app.waitForState(jobAttempt2, JobState.SUCCEEDED);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app",
  "methodName" : "testRecoveryFailsUsingCustomOutputCommitter",
  "errType" : null,
  "containingMethodsNum" : 49,
  "sourceCodeText" : "void testRecoveryFailsUsingCustomOutputCommitter() throws Exception\n{\r\n    int runCount = 0;\r\n    MRApp app = new MRAppWithHistory(3, 0, false, this.getClass().getName(), true, ++runCount);\r\n    Configuration conf = new Configuration();\r\n    conf.setClass(\"mapred.output.committer.class\", TestFileOutputCommitter.class, org.apache.hadoop.mapred.OutputCommitter.class);\r\n    conf.set(FileOutputFormat.OUTDIR, outputDir.toString());\r\n    conf.setBoolean(\"want.am.recovery\", false);\r\n    Job job = app.submit(conf);\r\n    app.waitForState(job, JobState.RUNNING);\r\n    Assert.assertEquals(\"No of tasks not correct\", 3, job.getTasks().size());\r\n    Iterator<Task> it = job.getTasks().values().iterator();\r\n    Task mapTask1 = it.next();\r\n    Task mapTask2 = it.next();\r\n    Task mapTask3 = it.next();\r\n    app.waitForState(mapTask1, TaskState.RUNNING);\r\n    app.waitForState(mapTask2, TaskState.RUNNING);\r\n    app.waitForState(mapTask3, TaskState.RUNNING);\r\n    TaskAttempt task1Attempt = mapTask1.getAttempts().values().iterator().next();\r\n    TaskAttempt task2Attempt = mapTask2.getAttempts().values().iterator().next();\r\n    TaskAttempt task3Attempt = mapTask3.getAttempts().values().iterator().next();\r\n    app.waitForState(task1Attempt, TaskAttemptState.RUNNING);\r\n    app.waitForState(task2Attempt, TaskAttemptState.RUNNING);\r\n    app.waitForState(task3Attempt, TaskAttemptState.RUNNING);\r\n    app.getContext().getEventHandler().handle(new TaskAttemptEvent(task1Attempt.getID(), TaskAttemptEventType.TA_DONE));\r\n    app.getContext().getEventHandler().handle(new TaskAttemptEvent(task2Attempt.getID(), TaskAttemptEventType.TA_DONE));\r\n    app.waitForState(mapTask1, TaskState.SUCCEEDED);\r\n    app.waitForState(mapTask2, TaskState.SUCCEEDED);\r\n    app.stop();\r\n    app = new MRAppWithHistory(2, 1, false, this.getClass().getName(), false, ++runCount);\r\n    conf = new Configuration();\r\n    conf.setClass(\"mapred.output.committer.class\", TestFileOutputCommitter.class, org.apache.hadoop.mapred.OutputCommitter.class);\r\n    conf.setBoolean(\"want.am.recovery\", false);\r\n    conf.set(FileOutputFormat.OUTDIR, outputDir.toString());\r\n    conf.setInt(MRJobConfig.NUM_REDUCES, 0);\r\n    conf.setBoolean(MRJobConfig.JOB_UBERTASK_ENABLE, false);\r\n    job = app.submit(conf);\r\n    app.waitForState(job, JobState.RUNNING);\r\n    Assert.assertEquals(\"No of tasks not correct\", 3, job.getTasks().size());\r\n    it = job.getTasks().values().iterator();\r\n    mapTask1 = it.next();\r\n    mapTask2 = it.next();\r\n    mapTask3 = it.next();\r\n    app.waitForState(mapTask1, TaskState.RUNNING);\r\n    app.waitForState(mapTask2, TaskState.RUNNING);\r\n    app.waitForState(mapTask3, TaskState.RUNNING);\r\n    task3Attempt = mapTask3.getAttempts().values().iterator().next();\r\n    app.waitForState(task3Attempt, TaskAttemptState.RUNNING);\r\n    app.getContext().getEventHandler().handle(new TaskAttemptEvent(mapTask1.getAttempts().values().iterator().next().getID(), TaskAttemptEventType.TA_DONE));\r\n    app.getContext().getEventHandler().handle(new TaskAttemptEvent(mapTask2.getAttempts().values().iterator().next().getID(), TaskAttemptEventType.TA_DONE));\r\n    app.getContext().getEventHandler().handle(new TaskAttemptEvent(mapTask3.getAttempts().values().iterator().next().getID(), TaskAttemptEventType.TA_DONE));\r\n    app.waitForState(mapTask3, TaskState.SUCCEEDED);\r\n    app.waitForState(job, JobState.SUCCEEDED);\r\n    app.verifyCompleted();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app",
  "methodName" : "testMultipleCrashes",
  "errType" : null,
  "containingMethodsNum" : 61,
  "sourceCodeText" : "void testMultipleCrashes() throws Exception\n{\r\n    int runCount = 0;\r\n    MRApp app = new MRAppWithHistory(2, 1, false, this.getClass().getName(), true, ++runCount);\r\n    Configuration conf = new Configuration();\r\n    conf.setBoolean(\"mapred.mapper.new-api\", true);\r\n    conf.setBoolean(\"mapred.reducer.new-api\", true);\r\n    conf.setBoolean(MRJobConfig.JOB_UBERTASK_ENABLE, false);\r\n    conf.set(FileOutputFormat.OUTDIR, outputDir.toString());\r\n    Job job = app.submit(conf);\r\n    app.waitForState(job, JobState.RUNNING);\r\n    Assert.assertEquals(\"No of tasks not correct\", 3, job.getTasks().size());\r\n    Iterator<Task> it = job.getTasks().values().iterator();\r\n    Task mapTask1 = it.next();\r\n    Task mapTask2 = it.next();\r\n    Task reduceTask = it.next();\r\n    app.waitForState(mapTask1, TaskState.RUNNING);\r\n    app.waitForState(mapTask2, TaskState.RUNNING);\r\n    TaskAttempt task1Attempt1 = mapTask1.getAttempts().values().iterator().next();\r\n    TaskAttempt task2Attempt = mapTask2.getAttempts().values().iterator().next();\r\n    app.waitForState(task1Attempt1, TaskAttemptState.RUNNING);\r\n    app.waitForState(task2Attempt, TaskAttemptState.RUNNING);\r\n    Assert.assertEquals(\"Reduce Task state not correct\", TaskState.RUNNING, reduceTask.getReport().getTaskState());\r\n    app.getContext().getEventHandler().handle(new TaskAttemptEvent(task1Attempt1.getID(), TaskAttemptEventType.TA_DONE));\r\n    app.waitForState(mapTask1, TaskState.SUCCEEDED);\r\n    app.stop();\r\n    app = new MRAppWithHistory(2, 1, false, this.getClass().getName(), false, ++runCount);\r\n    conf = new Configuration();\r\n    conf.setBoolean(MRJobConfig.MR_AM_JOB_RECOVERY_ENABLE, true);\r\n    conf.setBoolean(\"mapred.mapper.new-api\", true);\r\n    conf.setBoolean(\"mapred.reducer.new-api\", true);\r\n    conf.set(FileOutputFormat.OUTDIR, outputDir.toString());\r\n    conf.setBoolean(MRJobConfig.JOB_UBERTASK_ENABLE, false);\r\n    job = app.submit(conf);\r\n    app.waitForState(job, JobState.RUNNING);\r\n    Assert.assertEquals(\"No of tasks not correct\", 3, job.getTasks().size());\r\n    it = job.getTasks().values().iterator();\r\n    mapTask1 = it.next();\r\n    mapTask2 = it.next();\r\n    reduceTask = it.next();\r\n    app.waitForState(mapTask1, TaskState.SUCCEEDED);\r\n    app.waitForState(mapTask2, TaskState.RUNNING);\r\n    task2Attempt = mapTask2.getAttempts().values().iterator().next();\r\n    app.waitForState(task2Attempt, TaskAttemptState.RUNNING);\r\n    app.getContext().getEventHandler().handle(new TaskAttemptEvent(mapTask2.getAttempts().values().iterator().next().getID(), TaskAttemptEventType.TA_DONE));\r\n    app.waitForState(mapTask2, TaskState.SUCCEEDED);\r\n    app.stop();\r\n    app = new MRAppWithHistory(2, 1, false, this.getClass().getName(), false, ++runCount);\r\n    conf = new Configuration();\r\n    conf.setBoolean(MRJobConfig.MR_AM_JOB_RECOVERY_ENABLE, true);\r\n    conf.setBoolean(\"mapred.mapper.new-api\", true);\r\n    conf.setBoolean(\"mapred.reducer.new-api\", true);\r\n    conf.set(FileOutputFormat.OUTDIR, outputDir.toString());\r\n    conf.setBoolean(MRJobConfig.JOB_UBERTASK_ENABLE, false);\r\n    job = app.submit(conf);\r\n    app.waitForState(job, JobState.RUNNING);\r\n    Assert.assertEquals(\"No of tasks not correct\", 3, job.getTasks().size());\r\n    it = job.getTasks().values().iterator();\r\n    mapTask1 = it.next();\r\n    mapTask2 = it.next();\r\n    reduceTask = it.next();\r\n    app.waitForState(mapTask1, TaskState.SUCCEEDED);\r\n    app.waitForState(mapTask2, TaskState.SUCCEEDED);\r\n    app.waitForState(reduceTask, TaskState.RUNNING);\r\n    app.getContext().getEventHandler().handle(new TaskAttemptEvent(reduceTask.getAttempts().values().iterator().next().getID(), TaskAttemptEventType.TA_DONE));\r\n    app.waitForState(job, JobState.SUCCEEDED);\r\n    app.verifyCompleted();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app",
  "methodName" : "testOutputRecovery",
  "errType" : null,
  "containingMethodsNum" : 48,
  "sourceCodeText" : "void testOutputRecovery() throws Exception\n{\r\n    int runCount = 0;\r\n    MRApp app = new MRAppWithHistory(1, 2, false, this.getClass().getName(), true, ++runCount);\r\n    Configuration conf = new Configuration();\r\n    conf.setBoolean(\"mapred.mapper.new-api\", true);\r\n    conf.setBoolean(\"mapred.reducer.new-api\", true);\r\n    conf.setBoolean(MRJobConfig.JOB_UBERTASK_ENABLE, false);\r\n    conf.set(FileOutputFormat.OUTDIR, outputDir.toString());\r\n    Job job = app.submit(conf);\r\n    app.waitForState(job, JobState.RUNNING);\r\n    Assert.assertEquals(\"No of tasks not correct\", 3, job.getTasks().size());\r\n    Iterator<Task> it = job.getTasks().values().iterator();\r\n    Task mapTask1 = it.next();\r\n    Task reduceTask1 = it.next();\r\n    app.waitForState(mapTask1, TaskState.RUNNING);\r\n    TaskAttempt task1Attempt1 = mapTask1.getAttempts().values().iterator().next();\r\n    app.waitForState(task1Attempt1, TaskAttemptState.RUNNING);\r\n    app.getContext().getEventHandler().handle(new TaskAttemptEvent(task1Attempt1.getID(), TaskAttemptEventType.TA_DONE));\r\n    app.waitForState(mapTask1, TaskState.SUCCEEDED);\r\n    Assert.assertEquals(5467, task1Attempt1.getShufflePort());\r\n    app.waitForState(reduceTask1, TaskState.RUNNING);\r\n    TaskAttempt reduce1Attempt1 = reduceTask1.getAttempts().values().iterator().next();\r\n    writeOutput(reduce1Attempt1, conf);\r\n    app.getContext().getEventHandler().handle(new TaskAttemptEvent(reduce1Attempt1.getID(), TaskAttemptEventType.TA_DONE));\r\n    app.waitForState(reduceTask1, TaskState.SUCCEEDED);\r\n    app.stop();\r\n    app = new MRAppWithHistory(1, 2, false, this.getClass().getName(), false, ++runCount);\r\n    conf = new Configuration();\r\n    conf.setBoolean(MRJobConfig.MR_AM_JOB_RECOVERY_ENABLE, true);\r\n    conf.setBoolean(\"mapred.mapper.new-api\", true);\r\n    conf.setBoolean(\"mapred.reducer.new-api\", true);\r\n    conf.set(FileOutputFormat.OUTDIR, outputDir.toString());\r\n    conf.setBoolean(MRJobConfig.JOB_UBERTASK_ENABLE, false);\r\n    job = app.submit(conf);\r\n    app.waitForState(job, JobState.RUNNING);\r\n    Assert.assertEquals(\"No of tasks not correct\", 3, job.getTasks().size());\r\n    it = job.getTasks().values().iterator();\r\n    mapTask1 = it.next();\r\n    reduceTask1 = it.next();\r\n    Task reduceTask2 = it.next();\r\n    app.waitForState(mapTask1, TaskState.SUCCEEDED);\r\n    task1Attempt1 = mapTask1.getAttempts().values().iterator().next();\r\n    Assert.assertEquals(5467, task1Attempt1.getShufflePort());\r\n    app.waitForState(reduceTask1, TaskState.SUCCEEDED);\r\n    app.waitForState(reduceTask2, TaskState.RUNNING);\r\n    TaskAttempt reduce2Attempt = reduceTask2.getAttempts().values().iterator().next();\r\n    app.waitForState(reduce2Attempt, TaskAttemptState.RUNNING);\r\n    app.getContext().getEventHandler().handle(new TaskAttemptEvent(reduce2Attempt.getID(), TaskAttemptEventType.TA_DONE));\r\n    app.waitForState(reduceTask2, TaskState.SUCCEEDED);\r\n    app.waitForState(job, JobState.SUCCEEDED);\r\n    app.verifyCompleted();\r\n    validateOutput();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app",
  "methodName" : "testPreviousJobOutputCleanedWhenNoRecovery",
  "errType" : null,
  "containingMethodsNum" : 17,
  "sourceCodeText" : "void testPreviousJobOutputCleanedWhenNoRecovery() throws Exception\n{\r\n    int runCount = 0;\r\n    MRApp app = new MRAppWithHistory(1, 2, false, this.getClass().getName(), true, ++runCount);\r\n    Configuration conf = new Configuration();\r\n    conf.setBoolean(MRJobConfig.MR_AM_JOB_RECOVERY_ENABLE, false);\r\n    conf.setClass(\"mapred.output.committer.class\", TestFileOutputCommitter.class, org.apache.hadoop.mapred.OutputCommitter.class);\r\n    conf.setBoolean(MRJobConfig.JOB_UBERTASK_ENABLE, false);\r\n    conf.set(FileOutputFormat.OUTDIR, outputDir.toString());\r\n    Job job = app.submit(conf);\r\n    app.waitForState(job, JobState.RUNNING);\r\n    Assert.assertEquals(\"No of tasks not correct\", 3, job.getTasks().size());\r\n    app.stop();\r\n    app.close();\r\n    app = new MRAppWithHistory(1, 2, false, this.getClass().getName(), false, ++runCount);\r\n    job = app.submit(conf);\r\n    app.waitForState(job, JobState.RUNNING);\r\n    Assert.assertEquals(\"No of tasks not correct\", 3, job.getTasks().size());\r\n    TestFileOutputCommitter committer = (TestFileOutputCommitter) app.getCommitter();\r\n    assertTrue(\"commiter.abortJob() has not been called\", committer.isAbortJobCalled());\r\n    app.close();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app",
  "methodName" : "testPreviousJobIsNotCleanedWhenRecovery",
  "errType" : null,
  "containingMethodsNum" : 18,
  "sourceCodeText" : "void testPreviousJobIsNotCleanedWhenRecovery() throws Exception\n{\r\n    int runCount = 0;\r\n    MRApp app = new MRAppWithHistory(1, 2, false, this.getClass().getName(), true, ++runCount);\r\n    Configuration conf = new Configuration();\r\n    conf.setBoolean(MRJobConfig.MR_AM_JOB_RECOVERY_ENABLE, true);\r\n    conf.setClass(\"mapred.output.committer.class\", TestFileOutputCommitter.class, org.apache.hadoop.mapred.OutputCommitter.class);\r\n    conf.setBoolean(MRJobConfig.JOB_UBERTASK_ENABLE, false);\r\n    conf.setBoolean(\"want.am.recovery\", true);\r\n    conf.set(FileOutputFormat.OUTDIR, outputDir.toString());\r\n    Job job = app.submit(conf);\r\n    app.waitForState(job, JobState.RUNNING);\r\n    Assert.assertEquals(\"No of tasks not correct\", 3, job.getTasks().size());\r\n    app.stop();\r\n    app.close();\r\n    app = new MRAppWithHistory(1, 2, false, this.getClass().getName(), false, ++runCount);\r\n    job = app.submit(conf);\r\n    app.waitForState(job, JobState.RUNNING);\r\n    Assert.assertEquals(\"No of tasks not correct\", 3, job.getTasks().size());\r\n    TestFileOutputCommitter committer = (TestFileOutputCommitter) app.getCommitter();\r\n    assertFalse(\"commiter.abortJob() has been called\", committer.isAbortJobCalled());\r\n    app.close();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app",
  "methodName" : "testOutputRecoveryMapsOnly",
  "errType" : null,
  "containingMethodsNum" : 50,
  "sourceCodeText" : "void testOutputRecoveryMapsOnly() throws Exception\n{\r\n    int runCount = 0;\r\n    MRApp app = new MRAppWithHistory(2, 1, false, this.getClass().getName(), true, ++runCount);\r\n    Configuration conf = new Configuration();\r\n    conf.setBoolean(\"mapred.mapper.new-api\", true);\r\n    conf.setBoolean(\"mapred.reducer.new-api\", true);\r\n    conf.setBoolean(MRJobConfig.JOB_UBERTASK_ENABLE, false);\r\n    conf.set(FileOutputFormat.OUTDIR, outputDir.toString());\r\n    Job job = app.submit(conf);\r\n    app.waitForState(job, JobState.RUNNING);\r\n    Assert.assertEquals(\"No of tasks not correct\", 3, job.getTasks().size());\r\n    Iterator<Task> it = job.getTasks().values().iterator();\r\n    Task mapTask1 = it.next();\r\n    Task mapTask2 = it.next();\r\n    Task reduceTask1 = it.next();\r\n    app.waitForState(mapTask1, TaskState.RUNNING);\r\n    TaskAttempt task1Attempt1 = mapTask1.getAttempts().values().iterator().next();\r\n    app.waitForState(task1Attempt1, TaskAttemptState.RUNNING);\r\n    writeBadOutput(task1Attempt1, conf);\r\n    app.getContext().getEventHandler().handle(new TaskAttemptEvent(task1Attempt1.getID(), TaskAttemptEventType.TA_DONE));\r\n    app.waitForState(mapTask1, TaskState.SUCCEEDED);\r\n    Assert.assertEquals(5467, task1Attempt1.getShufflePort());\r\n    app.stop();\r\n    app = new MRAppWithHistory(2, 1, false, this.getClass().getName(), false, ++runCount);\r\n    conf = new Configuration();\r\n    conf.setBoolean(MRJobConfig.MR_AM_JOB_RECOVERY_ENABLE, true);\r\n    conf.setBoolean(\"mapred.mapper.new-api\", true);\r\n    conf.setBoolean(\"mapred.reducer.new-api\", true);\r\n    conf.set(FileOutputFormat.OUTDIR, outputDir.toString());\r\n    conf.setBoolean(MRJobConfig.JOB_UBERTASK_ENABLE, false);\r\n    job = app.submit(conf);\r\n    app.waitForState(job, JobState.RUNNING);\r\n    Assert.assertEquals(\"No of tasks not correct\", 3, job.getTasks().size());\r\n    it = job.getTasks().values().iterator();\r\n    mapTask1 = it.next();\r\n    mapTask2 = it.next();\r\n    reduceTask1 = it.next();\r\n    app.waitForState(mapTask1, TaskState.SUCCEEDED);\r\n    task1Attempt1 = mapTask1.getAttempts().values().iterator().next();\r\n    Assert.assertEquals(5467, task1Attempt1.getShufflePort());\r\n    app.waitForState(mapTask2, TaskState.RUNNING);\r\n    TaskAttempt task2Attempt1 = mapTask2.getAttempts().values().iterator().next();\r\n    app.waitForState(task2Attempt1, TaskAttemptState.RUNNING);\r\n    app.getContext().getEventHandler().handle(new TaskAttemptEvent(task2Attempt1.getID(), TaskAttemptEventType.TA_DONE));\r\n    app.waitForState(mapTask2, TaskState.SUCCEEDED);\r\n    Assert.assertEquals(5467, task2Attempt1.getShufflePort());\r\n    app.waitForState(reduceTask1, TaskState.RUNNING);\r\n    TaskAttempt reduce1Attempt1 = reduceTask1.getAttempts().values().iterator().next();\r\n    writeOutput(reduce1Attempt1, conf);\r\n    app.getContext().getEventHandler().handle(new TaskAttemptEvent(reduce1Attempt1.getID(), TaskAttemptEventType.TA_DONE));\r\n    app.waitForState(reduceTask1, TaskState.SUCCEEDED);\r\n    app.waitForState(job, JobState.SUCCEEDED);\r\n    app.verifyCompleted();\r\n    validateOutput();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app",
  "methodName" : "testRecoveryWithOldCommiter",
  "errType" : null,
  "containingMethodsNum" : 48,
  "sourceCodeText" : "void testRecoveryWithOldCommiter() throws Exception\n{\r\n    int runCount = 0;\r\n    MRApp app = new MRAppWithHistory(1, 2, false, this.getClass().getName(), true, ++runCount);\r\n    Configuration conf = new Configuration();\r\n    conf.setBoolean(\"mapred.mapper.new-api\", false);\r\n    conf.setBoolean(\"mapred.reducer.new-api\", false);\r\n    conf.setBoolean(MRJobConfig.JOB_UBERTASK_ENABLE, false);\r\n    conf.set(FileOutputFormat.OUTDIR, outputDir.toString());\r\n    Job job = app.submit(conf);\r\n    app.waitForState(job, JobState.RUNNING);\r\n    Assert.assertEquals(\"No of tasks not correct\", 3, job.getTasks().size());\r\n    Iterator<Task> it = job.getTasks().values().iterator();\r\n    Task mapTask1 = it.next();\r\n    Task reduceTask1 = it.next();\r\n    app.waitForState(mapTask1, TaskState.RUNNING);\r\n    TaskAttempt task1Attempt1 = mapTask1.getAttempts().values().iterator().next();\r\n    app.waitForState(task1Attempt1, TaskAttemptState.RUNNING);\r\n    app.getContext().getEventHandler().handle(new TaskAttemptEvent(task1Attempt1.getID(), TaskAttemptEventType.TA_DONE));\r\n    app.waitForState(mapTask1, TaskState.SUCCEEDED);\r\n    Assert.assertEquals(5467, task1Attempt1.getShufflePort());\r\n    app.waitForState(reduceTask1, TaskState.RUNNING);\r\n    TaskAttempt reduce1Attempt1 = reduceTask1.getAttempts().values().iterator().next();\r\n    writeOutput(reduce1Attempt1, conf);\r\n    app.getContext().getEventHandler().handle(new TaskAttemptEvent(reduce1Attempt1.getID(), TaskAttemptEventType.TA_DONE));\r\n    app.waitForState(reduceTask1, TaskState.SUCCEEDED);\r\n    app.stop();\r\n    app = new MRAppWithHistory(1, 2, false, this.getClass().getName(), false, ++runCount);\r\n    conf = new Configuration();\r\n    conf.setBoolean(MRJobConfig.MR_AM_JOB_RECOVERY_ENABLE, true);\r\n    conf.setBoolean(\"mapred.mapper.new-api\", false);\r\n    conf.setBoolean(\"mapred.reducer.new-api\", false);\r\n    conf.set(FileOutputFormat.OUTDIR, outputDir.toString());\r\n    conf.setBoolean(MRJobConfig.JOB_UBERTASK_ENABLE, false);\r\n    job = app.submit(conf);\r\n    app.waitForState(job, JobState.RUNNING);\r\n    Assert.assertEquals(\"No of tasks not correct\", 3, job.getTasks().size());\r\n    it = job.getTasks().values().iterator();\r\n    mapTask1 = it.next();\r\n    reduceTask1 = it.next();\r\n    Task reduceTask2 = it.next();\r\n    app.waitForState(mapTask1, TaskState.SUCCEEDED);\r\n    task1Attempt1 = mapTask1.getAttempts().values().iterator().next();\r\n    Assert.assertEquals(5467, task1Attempt1.getShufflePort());\r\n    app.waitForState(reduceTask1, TaskState.SUCCEEDED);\r\n    app.waitForState(reduceTask2, TaskState.RUNNING);\r\n    TaskAttempt reduce2Attempt = reduceTask2.getAttempts().values().iterator().next();\r\n    app.waitForState(reduce2Attempt, TaskAttemptState.RUNNING);\r\n    app.getContext().getEventHandler().handle(new TaskAttemptEvent(reduce2Attempt.getID(), TaskAttemptEventType.TA_DONE));\r\n    app.waitForState(reduceTask2, TaskState.SUCCEEDED);\r\n    app.waitForState(job, JobState.SUCCEEDED);\r\n    app.verifyCompleted();\r\n    validateOutput();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app",
  "methodName" : "testSpeculative",
  "errType" : null,
  "containingMethodsNum" : 78,
  "sourceCodeText" : "void testSpeculative() throws Exception\n{\r\n    int runCount = 0;\r\n    long am1StartTimeEst = System.currentTimeMillis();\r\n    MRApp app = new MRAppWithHistory(2, 1, false, this.getClass().getName(), true, ++runCount);\r\n    Configuration conf = new Configuration();\r\n    conf.setBoolean(\"mapred.mapper.new-api\", true);\r\n    conf.setBoolean(\"mapred.reducer.new-api\", true);\r\n    conf.setBoolean(MRJobConfig.JOB_UBERTASK_ENABLE, false);\r\n    conf.set(FileOutputFormat.OUTDIR, outputDir.toString());\r\n    Job job = app.submit(conf);\r\n    app.waitForState(job, JobState.RUNNING);\r\n    long jobStartTime = job.getReport().getStartTime();\r\n    Assert.assertEquals(\"No of tasks not correct\", 3, job.getTasks().size());\r\n    Iterator<Task> it = job.getTasks().values().iterator();\r\n    Task mapTask1 = it.next();\r\n    Task mapTask2 = it.next();\r\n    Task reduceTask = it.next();\r\n    app.waitForState(mapTask1, TaskState.RUNNING);\r\n    app.waitForState(mapTask2, TaskState.RUNNING);\r\n    app.getContext().getEventHandler().handle(new TaskEvent(mapTask1.getID(), TaskEventType.T_ADD_SPEC_ATTEMPT));\r\n    int timeOut = 0;\r\n    while (mapTask1.getAttempts().size() != 2 && timeOut++ < 10) {\r\n        Thread.sleep(1000);\r\n        LOG.info(\"Waiting for next attempt to start\");\r\n    }\r\n    Iterator<TaskAttempt> t1it = mapTask1.getAttempts().values().iterator();\r\n    TaskAttempt task1Attempt1 = t1it.next();\r\n    TaskAttempt task1Attempt2 = t1it.next();\r\n    TaskAttempt task2Attempt = mapTask2.getAttempts().values().iterator().next();\r\n    waitForContainerAssignment(task1Attempt2);\r\n    ContainerId t1a2contId = task1Attempt2.getAssignedContainerID();\r\n    LOG.info(t1a2contId.toString());\r\n    LOG.info(task1Attempt1.getID().toString());\r\n    LOG.info(task1Attempt2.getID().toString());\r\n    app.getContext().getEventHandler().handle(new TaskAttemptContainerLaunchedEvent(task1Attempt2.getID(), runCount));\r\n    app.waitForState(task1Attempt1, TaskAttemptState.RUNNING);\r\n    app.waitForState(task1Attempt2, TaskAttemptState.RUNNING);\r\n    app.waitForState(task2Attempt, TaskAttemptState.RUNNING);\r\n    app.waitForState(reduceTask, TaskState.RUNNING);\r\n    app.getContext().getEventHandler().handle(new TaskAttemptEvent(task1Attempt1.getID(), TaskAttemptEventType.TA_DONE));\r\n    app.waitForState(task1Attempt1, TaskAttemptState.SUCCEEDED);\r\n    app.waitForState(mapTask1, TaskState.SUCCEEDED);\r\n    long task1StartTime = mapTask1.getReport().getStartTime();\r\n    long task1FinishTime = mapTask1.getReport().getFinishTime();\r\n    app.stop();\r\n    long am2StartTimeEst = System.currentTimeMillis();\r\n    app = new MRAppWithHistory(2, 1, false, this.getClass().getName(), false, ++runCount);\r\n    conf = new Configuration();\r\n    conf.setBoolean(MRJobConfig.MR_AM_JOB_RECOVERY_ENABLE, true);\r\n    conf.setBoolean(\"mapred.mapper.new-api\", true);\r\n    conf.setBoolean(\"mapred.reducer.new-api\", true);\r\n    conf.set(FileOutputFormat.OUTDIR, outputDir.toString());\r\n    conf.setBoolean(MRJobConfig.JOB_UBERTASK_ENABLE, false);\r\n    job = app.submit(conf);\r\n    app.waitForState(job, JobState.RUNNING);\r\n    Assert.assertEquals(\"No of tasks not correct\", 3, job.getTasks().size());\r\n    it = job.getTasks().values().iterator();\r\n    mapTask1 = it.next();\r\n    mapTask2 = it.next();\r\n    reduceTask = it.next();\r\n    app.waitForState(mapTask1, TaskState.SUCCEEDED);\r\n    app.waitForState(mapTask2, TaskState.RUNNING);\r\n    task2Attempt = mapTask2.getAttempts().values().iterator().next();\r\n    app.waitForState(task2Attempt, TaskAttemptState.RUNNING);\r\n    app.getContext().getEventHandler().handle(new TaskAttemptEvent(mapTask2.getAttempts().values().iterator().next().getID(), TaskAttemptEventType.TA_DONE));\r\n    app.waitForState(mapTask2, TaskState.SUCCEEDED);\r\n    app.waitForState(reduceTask, TaskState.RUNNING);\r\n    app.getContext().getEventHandler().handle(new TaskAttemptEvent(reduceTask.getAttempts().values().iterator().next().getID(), TaskAttemptEventType.TA_DONE));\r\n    app.waitForState(job, JobState.SUCCEEDED);\r\n    app.verifyCompleted();\r\n    Assert.assertEquals(\"Job Start time not correct\", jobStartTime, job.getReport().getStartTime());\r\n    Assert.assertEquals(\"Task Start time not correct\", task1StartTime, mapTask1.getReport().getStartTime());\r\n    Assert.assertEquals(\"Task Finish time not correct\", task1FinishTime, mapTask1.getReport().getFinishTime());\r\n    Assert.assertEquals(2, job.getAMInfos().size());\r\n    int attemptNum = 1;\r\n    for (AMInfo amInfo : job.getAMInfos()) {\r\n        Assert.assertEquals(attemptNum++, amInfo.getAppAttemptId().getAttemptId());\r\n        Assert.assertEquals(amInfo.getAppAttemptId(), amInfo.getContainerId().getApplicationAttemptId());\r\n        Assert.assertEquals(MRApp.NM_HOST, amInfo.getNodeManagerHost());\r\n        Assert.assertEquals(MRApp.NM_PORT, amInfo.getNodeManagerPort());\r\n        Assert.assertEquals(MRApp.NM_HTTP_PORT, amInfo.getNodeManagerHttpPort());\r\n    }\r\n    long am1StartTimeReal = job.getAMInfos().get(0).getStartTime();\r\n    long am2StartTimeReal = job.getAMInfos().get(1).getStartTime();\r\n    Assert.assertTrue(am1StartTimeReal >= am1StartTimeEst && am1StartTimeReal <= am2StartTimeEst);\r\n    Assert.assertTrue(am2StartTimeReal >= am2StartTimeEst && am2StartTimeReal <= System.currentTimeMillis());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app",
  "methodName" : "testRecoveryWithoutShuffleSecret",
  "errType" : null,
  "containingMethodsNum" : 48,
  "sourceCodeText" : "void testRecoveryWithoutShuffleSecret() throws Exception\n{\r\n    int runCount = 0;\r\n    MRApp app = new MRAppNoShuffleSecret(2, 1, false, this.getClass().getName(), true, ++runCount);\r\n    Configuration conf = new Configuration();\r\n    conf.setBoolean(\"mapred.mapper.new-api\", true);\r\n    conf.setBoolean(\"mapred.reducer.new-api\", true);\r\n    conf.setBoolean(MRJobConfig.JOB_UBERTASK_ENABLE, false);\r\n    conf.set(FileOutputFormat.OUTDIR, outputDir.toString());\r\n    Job job = app.submit(conf);\r\n    app.waitForState(job, JobState.RUNNING);\r\n    Assert.assertEquals(\"No of tasks not correct\", 3, job.getTasks().size());\r\n    Iterator<Task> it = job.getTasks().values().iterator();\r\n    Task mapTask1 = it.next();\r\n    Task mapTask2 = it.next();\r\n    Task reduceTask = it.next();\r\n    app.waitForState(mapTask1, TaskState.RUNNING);\r\n    app.waitForState(mapTask2, TaskState.RUNNING);\r\n    TaskAttempt task1Attempt = mapTask1.getAttempts().values().iterator().next();\r\n    TaskAttempt task2Attempt = mapTask2.getAttempts().values().iterator().next();\r\n    app.waitForState(task1Attempt, TaskAttemptState.RUNNING);\r\n    app.waitForState(task2Attempt, TaskAttemptState.RUNNING);\r\n    app.waitForState(reduceTask, TaskState.RUNNING);\r\n    app.getContext().getEventHandler().handle(new TaskAttemptEvent(task1Attempt.getID(), TaskAttemptEventType.TA_DONE));\r\n    app.waitForState(mapTask1, TaskState.SUCCEEDED);\r\n    app.stop();\r\n    app = new MRAppNoShuffleSecret(2, 1, false, this.getClass().getName(), false, ++runCount);\r\n    conf = new Configuration();\r\n    conf.setBoolean(MRJobConfig.MR_AM_JOB_RECOVERY_ENABLE, true);\r\n    conf.setBoolean(\"mapred.mapper.new-api\", true);\r\n    conf.setBoolean(\"mapred.reducer.new-api\", true);\r\n    conf.set(FileOutputFormat.OUTDIR, outputDir.toString());\r\n    conf.setBoolean(MRJobConfig.JOB_UBERTASK_ENABLE, false);\r\n    job = app.submit(conf);\r\n    app.waitForState(job, JobState.RUNNING);\r\n    Assert.assertEquals(\"No of tasks not correct\", 3, job.getTasks().size());\r\n    it = job.getTasks().values().iterator();\r\n    mapTask1 = it.next();\r\n    mapTask2 = it.next();\r\n    reduceTask = it.next();\r\n    app.waitForState(mapTask1, TaskState.RUNNING);\r\n    app.waitForState(mapTask2, TaskState.RUNNING);\r\n    task2Attempt = mapTask2.getAttempts().values().iterator().next();\r\n    app.waitForState(task2Attempt, TaskAttemptState.RUNNING);\r\n    app.getContext().getEventHandler().handle(new TaskAttemptEvent(mapTask2.getAttempts().values().iterator().next().getID(), TaskAttemptEventType.TA_DONE));\r\n    app.waitForState(mapTask2, TaskState.SUCCEEDED);\r\n    app.waitForState(mapTask1, TaskState.RUNNING);\r\n    app.getContext().getEventHandler().handle(new TaskAttemptEvent(mapTask1.getAttempts().values().iterator().next().getID(), TaskAttemptEventType.TA_DONE));\r\n    app.waitForState(mapTask1, TaskState.SUCCEEDED);\r\n    app.waitForState(reduceTask, TaskState.RUNNING);\r\n    app.getContext().getEventHandler().handle(new TaskAttemptEvent(reduceTask.getAttempts().values().iterator().next().getID(), TaskAttemptEventType.TA_DONE));\r\n    app.waitForState(job, JobState.SUCCEEDED);\r\n    app.verifyCompleted();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app",
  "methodName" : "testRecoverySuccessAttempt",
  "errType" : null,
  "containingMethodsNum" : 28,
  "sourceCodeText" : "void testRecoverySuccessAttempt()\n{\r\n    LOG.info(\"--- START: testRecoverySuccessAttempt ---\");\r\n    long clusterTimestamp = System.currentTimeMillis();\r\n    EventHandler mockEventHandler = mock(EventHandler.class);\r\n    MapTaskImpl recoverMapTask = getMockMapTask(clusterTimestamp, mockEventHandler);\r\n    TaskId taskId = recoverMapTask.getID();\r\n    JobID jobID = new JobID(Long.toString(clusterTimestamp), 1);\r\n    TaskID taskID = new TaskID(jobID, org.apache.hadoop.mapreduce.TaskType.MAP, taskId.getId());\r\n    Map<TaskAttemptID, TaskAttemptInfo> mockTaskAttempts = new HashMap<TaskAttemptID, TaskAttemptInfo>();\r\n    TaskAttemptID taId1 = new TaskAttemptID(taskID, 2);\r\n    TaskAttemptInfo mockTAinfo1 = getMockTaskAttemptInfo(taId1, TaskAttemptState.SUCCEEDED);\r\n    mockTaskAttempts.put(taId1, mockTAinfo1);\r\n    TaskAttemptID taId2 = new TaskAttemptID(taskID, 1);\r\n    TaskAttemptInfo mockTAinfo2 = getMockTaskAttemptInfo(taId2, TaskAttemptState.FAILED);\r\n    mockTaskAttempts.put(taId2, mockTAinfo2);\r\n    OutputCommitter mockCommitter = mock(OutputCommitter.class);\r\n    TaskInfo mockTaskInfo = mock(TaskInfo.class);\r\n    when(mockTaskInfo.getTaskStatus()).thenReturn(\"SUCCEEDED\");\r\n    when(mockTaskInfo.getTaskId()).thenReturn(taskID);\r\n    when(mockTaskInfo.getAllTaskAttempts()).thenReturn(mockTaskAttempts);\r\n    recoverMapTask.handle(new TaskRecoverEvent(taskId, mockTaskInfo, mockCommitter, true));\r\n    ArgumentCaptor<Event> arg = ArgumentCaptor.forClass(Event.class);\r\n    verify(mockEventHandler, atLeast(1)).handle((org.apache.hadoop.yarn.event.Event) arg.capture());\r\n    Map<TaskAttemptID, TaskAttemptState> finalAttemptStates = new HashMap<TaskAttemptID, TaskAttemptState>();\r\n    finalAttemptStates.put(taId1, TaskAttemptState.SUCCEEDED);\r\n    finalAttemptStates.put(taId2, TaskAttemptState.FAILED);\r\n    List<EventType> jobHistoryEvents = new ArrayList<EventType>();\r\n    jobHistoryEvents.add(EventType.TASK_STARTED);\r\n    jobHistoryEvents.add(EventType.MAP_ATTEMPT_STARTED);\r\n    jobHistoryEvents.add(EventType.MAP_ATTEMPT_FINISHED);\r\n    jobHistoryEvents.add(EventType.MAP_ATTEMPT_STARTED);\r\n    jobHistoryEvents.add(EventType.MAP_ATTEMPT_FAILED);\r\n    jobHistoryEvents.add(EventType.TASK_FINISHED);\r\n    recoveryChecker(recoverMapTask, TaskState.SUCCEEDED, finalAttemptStates, arg, jobHistoryEvents, 2L, 1L);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app",
  "methodName" : "testRecoveryAllFailAttempts",
  "errType" : null,
  "containingMethodsNum" : 28,
  "sourceCodeText" : "void testRecoveryAllFailAttempts()\n{\r\n    LOG.info(\"--- START: testRecoveryAllFailAttempts ---\");\r\n    long clusterTimestamp = System.currentTimeMillis();\r\n    EventHandler mockEventHandler = mock(EventHandler.class);\r\n    MapTaskImpl recoverMapTask = getMockMapTask(clusterTimestamp, mockEventHandler);\r\n    TaskId taskId = recoverMapTask.getID();\r\n    JobID jobID = new JobID(Long.toString(clusterTimestamp), 1);\r\n    TaskID taskID = new TaskID(jobID, org.apache.hadoop.mapreduce.TaskType.MAP, taskId.getId());\r\n    Map<TaskAttemptID, TaskAttemptInfo> mockTaskAttempts = new HashMap<TaskAttemptID, TaskAttemptInfo>();\r\n    TaskAttemptID taId1 = new TaskAttemptID(taskID, 2);\r\n    TaskAttemptInfo mockTAinfo1 = getMockTaskAttemptInfo(taId1, TaskAttemptState.FAILED);\r\n    mockTaskAttempts.put(taId1, mockTAinfo1);\r\n    TaskAttemptID taId2 = new TaskAttemptID(taskID, 1);\r\n    TaskAttemptInfo mockTAinfo2 = getMockTaskAttemptInfo(taId2, TaskAttemptState.FAILED);\r\n    mockTaskAttempts.put(taId2, mockTAinfo2);\r\n    OutputCommitter mockCommitter = mock(OutputCommitter.class);\r\n    TaskInfo mockTaskInfo = mock(TaskInfo.class);\r\n    when(mockTaskInfo.getTaskStatus()).thenReturn(\"FAILED\");\r\n    when(mockTaskInfo.getTaskId()).thenReturn(taskID);\r\n    when(mockTaskInfo.getAllTaskAttempts()).thenReturn(mockTaskAttempts);\r\n    recoverMapTask.handle(new TaskRecoverEvent(taskId, mockTaskInfo, mockCommitter, true));\r\n    ArgumentCaptor<Event> arg = ArgumentCaptor.forClass(Event.class);\r\n    verify(mockEventHandler, atLeast(1)).handle((org.apache.hadoop.yarn.event.Event) arg.capture());\r\n    Map<TaskAttemptID, TaskAttemptState> finalAttemptStates = new HashMap<TaskAttemptID, TaskAttemptState>();\r\n    finalAttemptStates.put(taId1, TaskAttemptState.FAILED);\r\n    finalAttemptStates.put(taId2, TaskAttemptState.FAILED);\r\n    List<EventType> jobHistoryEvents = new ArrayList<EventType>();\r\n    jobHistoryEvents.add(EventType.TASK_STARTED);\r\n    jobHistoryEvents.add(EventType.MAP_ATTEMPT_STARTED);\r\n    jobHistoryEvents.add(EventType.MAP_ATTEMPT_FAILED);\r\n    jobHistoryEvents.add(EventType.MAP_ATTEMPT_STARTED);\r\n    jobHistoryEvents.add(EventType.MAP_ATTEMPT_FAILED);\r\n    jobHistoryEvents.add(EventType.TASK_FAILED);\r\n    recoveryChecker(recoverMapTask, TaskState.FAILED, finalAttemptStates, arg, jobHistoryEvents, 2L, 2L);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app",
  "methodName" : "testRecoveryTaskSuccessAllAttemptsFail",
  "errType" : null,
  "containingMethodsNum" : 28,
  "sourceCodeText" : "void testRecoveryTaskSuccessAllAttemptsFail()\n{\r\n    LOG.info(\"--- START:  testRecoveryTaskSuccessAllAttemptsFail ---\");\r\n    long clusterTimestamp = System.currentTimeMillis();\r\n    EventHandler mockEventHandler = mock(EventHandler.class);\r\n    MapTaskImpl recoverMapTask = getMockMapTask(clusterTimestamp, mockEventHandler);\r\n    TaskId taskId = recoverMapTask.getID();\r\n    JobID jobID = new JobID(Long.toString(clusterTimestamp), 1);\r\n    TaskID taskID = new TaskID(jobID, org.apache.hadoop.mapreduce.TaskType.MAP, taskId.getId());\r\n    Map<TaskAttemptID, TaskAttemptInfo> mockTaskAttempts = new HashMap<TaskAttemptID, TaskAttemptInfo>();\r\n    TaskAttemptID taId1 = new TaskAttemptID(taskID, 2);\r\n    TaskAttemptInfo mockTAinfo1 = getMockTaskAttemptInfo(taId1, TaskAttemptState.FAILED);\r\n    mockTaskAttempts.put(taId1, mockTAinfo1);\r\n    TaskAttemptID taId2 = new TaskAttemptID(taskID, 1);\r\n    TaskAttemptInfo mockTAinfo2 = getMockTaskAttemptInfo(taId2, TaskAttemptState.FAILED);\r\n    mockTaskAttempts.put(taId2, mockTAinfo2);\r\n    OutputCommitter mockCommitter = mock(OutputCommitter.class);\r\n    TaskInfo mockTaskInfo = mock(TaskInfo.class);\r\n    when(mockTaskInfo.getTaskStatus()).thenReturn(\"SUCCEEDED\");\r\n    when(mockTaskInfo.getTaskId()).thenReturn(taskID);\r\n    when(mockTaskInfo.getAllTaskAttempts()).thenReturn(mockTaskAttempts);\r\n    recoverMapTask.handle(new TaskRecoverEvent(taskId, mockTaskInfo, mockCommitter, true));\r\n    ArgumentCaptor<Event> arg = ArgumentCaptor.forClass(Event.class);\r\n    verify(mockEventHandler, atLeast(1)).handle((org.apache.hadoop.yarn.event.Event) arg.capture());\r\n    Map<TaskAttemptID, TaskAttemptState> finalAttemptStates = new HashMap<TaskAttemptID, TaskAttemptState>();\r\n    finalAttemptStates.put(taId1, TaskAttemptState.FAILED);\r\n    finalAttemptStates.put(taId2, TaskAttemptState.FAILED);\r\n    TaskAttemptID taId3 = new TaskAttemptID(taskID, 2000);\r\n    finalAttemptStates.put(taId3, TaskAttemptState.NEW);\r\n    List<EventType> jobHistoryEvents = new ArrayList<EventType>();\r\n    jobHistoryEvents.add(EventType.TASK_STARTED);\r\n    jobHistoryEvents.add(EventType.MAP_ATTEMPT_STARTED);\r\n    jobHistoryEvents.add(EventType.MAP_ATTEMPT_FAILED);\r\n    jobHistoryEvents.add(EventType.MAP_ATTEMPT_STARTED);\r\n    jobHistoryEvents.add(EventType.MAP_ATTEMPT_FAILED);\r\n    recoveryChecker(recoverMapTask, TaskState.RUNNING, finalAttemptStates, arg, jobHistoryEvents, 2L, 2L);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app",
  "methodName" : "testRecoveryTaskSuccessAllAttemptsSucceed",
  "errType" : null,
  "containingMethodsNum" : 28,
  "sourceCodeText" : "void testRecoveryTaskSuccessAllAttemptsSucceed()\n{\r\n    LOG.info(\"--- START:  testRecoveryTaskSuccessAllAttemptsFail ---\");\r\n    long clusterTimestamp = System.currentTimeMillis();\r\n    EventHandler mockEventHandler = mock(EventHandler.class);\r\n    MapTaskImpl recoverMapTask = getMockMapTask(clusterTimestamp, mockEventHandler);\r\n    TaskId taskId = recoverMapTask.getID();\r\n    JobID jobID = new JobID(Long.toString(clusterTimestamp), 1);\r\n    TaskID taskID = new TaskID(jobID, org.apache.hadoop.mapreduce.TaskType.MAP, taskId.getId());\r\n    Map<TaskAttemptID, TaskAttemptInfo> mockTaskAttempts = new HashMap<TaskAttemptID, TaskAttemptInfo>();\r\n    TaskAttemptID taId1 = new TaskAttemptID(taskID, 2);\r\n    TaskAttemptInfo mockTAinfo1 = getMockTaskAttemptInfo(taId1, TaskAttemptState.SUCCEEDED);\r\n    mockTaskAttempts.put(taId1, mockTAinfo1);\r\n    TaskAttemptID taId2 = new TaskAttemptID(taskID, 1);\r\n    TaskAttemptInfo mockTAinfo2 = getMockTaskAttemptInfo(taId2, TaskAttemptState.SUCCEEDED);\r\n    mockTaskAttempts.put(taId2, mockTAinfo2);\r\n    OutputCommitter mockCommitter = mock(OutputCommitter.class);\r\n    TaskInfo mockTaskInfo = mock(TaskInfo.class);\r\n    when(mockTaskInfo.getTaskStatus()).thenReturn(\"SUCCEEDED\");\r\n    when(mockTaskInfo.getTaskId()).thenReturn(taskID);\r\n    when(mockTaskInfo.getAllTaskAttempts()).thenReturn(mockTaskAttempts);\r\n    recoverMapTask.handle(new TaskRecoverEvent(taskId, mockTaskInfo, mockCommitter, true));\r\n    ArgumentCaptor<Event> arg = ArgumentCaptor.forClass(Event.class);\r\n    verify(mockEventHandler, atLeast(1)).handle((org.apache.hadoop.yarn.event.Event) arg.capture());\r\n    Map<TaskAttemptID, TaskAttemptState> finalAttemptStates = new HashMap<TaskAttemptID, TaskAttemptState>();\r\n    finalAttemptStates.put(taId1, TaskAttemptState.SUCCEEDED);\r\n    finalAttemptStates.put(taId2, TaskAttemptState.SUCCEEDED);\r\n    List<EventType> jobHistoryEvents = new ArrayList<EventType>();\r\n    jobHistoryEvents.add(EventType.TASK_STARTED);\r\n    jobHistoryEvents.add(EventType.MAP_ATTEMPT_STARTED);\r\n    jobHistoryEvents.add(EventType.MAP_ATTEMPT_FINISHED);\r\n    jobHistoryEvents.add(EventType.MAP_ATTEMPT_STARTED);\r\n    jobHistoryEvents.add(EventType.MAP_ATTEMPT_FINISHED);\r\n    jobHistoryEvents.add(EventType.TASK_FINISHED);\r\n    recoveryChecker(recoverMapTask, TaskState.SUCCEEDED, finalAttemptStates, arg, jobHistoryEvents, 2L, 0L);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app",
  "methodName" : "testRecoveryAllAttemptsKilled",
  "errType" : null,
  "containingMethodsNum" : 28,
  "sourceCodeText" : "void testRecoveryAllAttemptsKilled()\n{\r\n    LOG.info(\"--- START:  testRecoveryAllAttemptsKilled ---\");\r\n    long clusterTimestamp = System.currentTimeMillis();\r\n    EventHandler mockEventHandler = mock(EventHandler.class);\r\n    MapTaskImpl recoverMapTask = getMockMapTask(clusterTimestamp, mockEventHandler);\r\n    TaskId taskId = recoverMapTask.getID();\r\n    JobID jobID = new JobID(Long.toString(clusterTimestamp), 1);\r\n    TaskID taskID = new TaskID(jobID, org.apache.hadoop.mapreduce.TaskType.MAP, taskId.getId());\r\n    Map<TaskAttemptID, TaskAttemptInfo> mockTaskAttempts = new HashMap<TaskAttemptID, TaskAttemptInfo>();\r\n    TaskAttemptID taId1 = new TaskAttemptID(taskID, 2);\r\n    TaskAttemptInfo mockTAinfo1 = getMockTaskAttemptInfo(taId1, TaskAttemptState.KILLED);\r\n    mockTaskAttempts.put(taId1, mockTAinfo1);\r\n    TaskAttemptID taId2 = new TaskAttemptID(taskID, 1);\r\n    TaskAttemptInfo mockTAinfo2 = getMockTaskAttemptInfo(taId2, TaskAttemptState.KILLED);\r\n    mockTaskAttempts.put(taId2, mockTAinfo2);\r\n    OutputCommitter mockCommitter = mock(OutputCommitter.class);\r\n    TaskInfo mockTaskInfo = mock(TaskInfo.class);\r\n    when(mockTaskInfo.getTaskStatus()).thenReturn(\"KILLED\");\r\n    when(mockTaskInfo.getTaskId()).thenReturn(taskID);\r\n    when(mockTaskInfo.getAllTaskAttempts()).thenReturn(mockTaskAttempts);\r\n    recoverMapTask.handle(new TaskRecoverEvent(taskId, mockTaskInfo, mockCommitter, true));\r\n    ArgumentCaptor<Event> arg = ArgumentCaptor.forClass(Event.class);\r\n    verify(mockEventHandler, atLeast(1)).handle((org.apache.hadoop.yarn.event.Event) arg.capture());\r\n    Map<TaskAttemptID, TaskAttemptState> finalAttemptStates = new HashMap<TaskAttemptID, TaskAttemptState>();\r\n    finalAttemptStates.put(taId1, TaskAttemptState.KILLED);\r\n    finalAttemptStates.put(taId2, TaskAttemptState.KILLED);\r\n    List<EventType> jobHistoryEvents = new ArrayList<EventType>();\r\n    jobHistoryEvents.add(EventType.TASK_STARTED);\r\n    jobHistoryEvents.add(EventType.MAP_ATTEMPT_STARTED);\r\n    jobHistoryEvents.add(EventType.MAP_ATTEMPT_KILLED);\r\n    jobHistoryEvents.add(EventType.MAP_ATTEMPT_STARTED);\r\n    jobHistoryEvents.add(EventType.MAP_ATTEMPT_KILLED);\r\n    jobHistoryEvents.add(EventType.TASK_FAILED);\r\n    recoveryChecker(recoverMapTask, TaskState.KILLED, finalAttemptStates, arg, jobHistoryEvents, 2L, 0L);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app",
  "methodName" : "recoveryChecker",
  "errType" : null,
  "containingMethodsNum" : 23,
  "sourceCodeText" : "void recoveryChecker(MapTaskImpl checkTask, TaskState finalState, Map<TaskAttemptID, TaskAttemptState> finalAttemptStates, ArgumentCaptor<Event> arg, List<EventType> expectedJobHistoryEvents, long expectedMapLaunches, long expectedFailedMaps)\n{\r\n    assertEquals(\"Final State of Task\", finalState, checkTask.getState());\r\n    Map<TaskAttemptId, TaskAttempt> recoveredAttempts = checkTask.getAttempts();\r\n    assertEquals(\"Expected Number of Task Attempts\", finalAttemptStates.size(), recoveredAttempts.size());\r\n    for (TaskAttemptID taID : finalAttemptStates.keySet()) {\r\n        assertEquals(\"Expected Task Attempt State\", finalAttemptStates.get(taID), recoveredAttempts.get(TypeConverter.toYarn(taID)).getState());\r\n    }\r\n    Iterator<Event> ie = arg.getAllValues().iterator();\r\n    int eventNum = 0;\r\n    long totalLaunchedMaps = 0;\r\n    long totalFailedMaps = 0;\r\n    boolean jobTaskEventReceived = false;\r\n    while (ie.hasNext()) {\r\n        Object current = ie.next();\r\n        ++eventNum;\r\n        LOG.info(eventNum + \" \" + current.getClass().getName());\r\n        if (current instanceof JobHistoryEvent) {\r\n            JobHistoryEvent jhe = (JobHistoryEvent) current;\r\n            LOG.info(expectedJobHistoryEvents.get(0).toString() + \" \" + jhe.getHistoryEvent().getEventType().toString() + \" \" + jhe.getJobID());\r\n            assertEquals(expectedJobHistoryEvents.get(0), jhe.getHistoryEvent().getEventType());\r\n            expectedJobHistoryEvents.remove(0);\r\n        } else if (current instanceof JobCounterUpdateEvent) {\r\n            JobCounterUpdateEvent jcue = (JobCounterUpdateEvent) current;\r\n            boolean containsUpdates = jcue.getCounterUpdates().size() > 0;\r\n            if (containsUpdates) {\r\n                LOG.info(\"JobCounterUpdateEvent \" + jcue.getCounterUpdates().get(0).getCounterKey() + \" \" + jcue.getCounterUpdates().get(0).getIncrementValue());\r\n                if (jcue.getCounterUpdates().get(0).getCounterKey() == JobCounter.NUM_FAILED_MAPS) {\r\n                    totalFailedMaps += jcue.getCounterUpdates().get(0).getIncrementValue();\r\n                } else if (jcue.getCounterUpdates().get(0).getCounterKey() == JobCounter.TOTAL_LAUNCHED_MAPS) {\r\n                    totalLaunchedMaps += jcue.getCounterUpdates().get(0).getIncrementValue();\r\n                }\r\n            }\r\n        } else if (current instanceof JobTaskEvent) {\r\n            JobTaskEvent jte = (JobTaskEvent) current;\r\n            assertEquals(jte.getState(), finalState);\r\n            jobTaskEventReceived = true;\r\n        }\r\n    }\r\n    assertTrue(jobTaskEventReceived || (finalState == TaskState.RUNNING));\r\n    assertEquals(\"Did not process all expected JobHistoryEvents\", 0, expectedJobHistoryEvents.size());\r\n    assertEquals(\"Expected Map Launches\", expectedMapLaunches, totalLaunchedMaps);\r\n    assertEquals(\"Expected Failed Maps\", expectedFailedMaps, totalFailedMaps);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app",
  "methodName" : "getMockMapTask",
  "errType" : null,
  "containingMethodsNum" : 13,
  "sourceCodeText" : "MapTaskImpl getMockMapTask(long clusterTimestamp, EventHandler eh)\n{\r\n    ApplicationId appId = ApplicationId.newInstance(clusterTimestamp, 1);\r\n    JobId jobId = MRBuilderUtils.newJobId(appId, 1);\r\n    int partitions = 2;\r\n    Path remoteJobConfFile = mock(Path.class);\r\n    JobConf conf = new JobConf();\r\n    TaskAttemptListener taskAttemptListener = mock(TaskAttemptListener.class);\r\n    Token<JobTokenIdentifier> jobToken = (Token<JobTokenIdentifier>) mock(Token.class);\r\n    Credentials credentials = null;\r\n    Clock clock = SystemClock.getInstance();\r\n    int appAttemptId = 3;\r\n    MRAppMetrics metrics = mock(MRAppMetrics.class);\r\n    Resource minContainerRequirements = mock(Resource.class);\r\n    when(minContainerRequirements.getMemorySize()).thenReturn(1000L);\r\n    ClusterInfo clusterInfo = mock(ClusterInfo.class);\r\n    AppContext appContext = mock(AppContext.class);\r\n    when(appContext.getClusterInfo()).thenReturn(clusterInfo);\r\n    TaskSplitMetaInfo taskSplitMetaInfo = mock(TaskSplitMetaInfo.class);\r\n    MapTaskImpl mapTask = new MapTaskImpl(jobId, partitions, eh, remoteJobConfFile, conf, taskSplitMetaInfo, taskAttemptListener, jobToken, credentials, clock, appAttemptId, metrics, appContext);\r\n    return mapTask;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app",
  "methodName" : "getMockTaskAttemptInfo",
  "errType" : null,
  "containingMethodsNum" : 22,
  "sourceCodeText" : "TaskAttemptInfo getMockTaskAttemptInfo(TaskAttemptID tai, TaskAttemptState tas)\n{\r\n    ContainerId ci = mock(ContainerId.class);\r\n    Counters counters = mock(Counters.class);\r\n    TaskType tt = TaskType.MAP;\r\n    long finishTime = System.currentTimeMillis();\r\n    TaskAttemptInfo mockTAinfo = mock(TaskAttemptInfo.class);\r\n    when(mockTAinfo.getAttemptId()).thenReturn(tai);\r\n    when(mockTAinfo.getContainerId()).thenReturn(ci);\r\n    when(mockTAinfo.getCounters()).thenReturn(counters);\r\n    when(mockTAinfo.getError()).thenReturn(\"\");\r\n    when(mockTAinfo.getFinishTime()).thenReturn(finishTime);\r\n    when(mockTAinfo.getHostname()).thenReturn(\"localhost\");\r\n    when(mockTAinfo.getHttpPort()).thenReturn(23);\r\n    when(mockTAinfo.getMapFinishTime()).thenReturn(finishTime - 1000L);\r\n    when(mockTAinfo.getPort()).thenReturn(24);\r\n    when(mockTAinfo.getRackname()).thenReturn(\"defaultRack\");\r\n    when(mockTAinfo.getShuffleFinishTime()).thenReturn(finishTime - 2000L);\r\n    when(mockTAinfo.getShufflePort()).thenReturn(25);\r\n    when(mockTAinfo.getSortFinishTime()).thenReturn(finishTime - 3000L);\r\n    when(mockTAinfo.getStartTime()).thenReturn(finishTime - 10000);\r\n    when(mockTAinfo.getState()).thenReturn(\"task in progress\");\r\n    when(mockTAinfo.getTaskStatus()).thenReturn(tas.toString());\r\n    when(mockTAinfo.getTaskType()).thenReturn(tt);\r\n    when(mockTAinfo.getTrackerName()).thenReturn(\"TrackerName\");\r\n    return mockTAinfo;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app",
  "methodName" : "writeBadOutput",
  "errType" : null,
  "containingMethodsNum" : 15,
  "sourceCodeText" : "void writeBadOutput(TaskAttempt attempt, Configuration conf) throws Exception\n{\r\n    TaskAttemptContext tContext = new TaskAttemptContextImpl(conf, TypeConverter.fromYarn(attempt.getID()));\r\n    TextOutputFormat<?, ?> theOutputFormat = new TextOutputFormat();\r\n    RecordWriter theRecordWriter = theOutputFormat.getRecordWriter(tContext);\r\n    NullWritable nullWritable = NullWritable.get();\r\n    try {\r\n        theRecordWriter.write(key2, val2);\r\n        theRecordWriter.write(null, nullWritable);\r\n        theRecordWriter.write(null, val2);\r\n        theRecordWriter.write(nullWritable, val1);\r\n        theRecordWriter.write(key1, nullWritable);\r\n        theRecordWriter.write(key2, null);\r\n        theRecordWriter.write(null, null);\r\n        theRecordWriter.write(key1, val1);\r\n    } finally {\r\n        theRecordWriter.close(tContext);\r\n    }\r\n    OutputFormat outputFormat = ReflectionUtils.newInstance(tContext.getOutputFormatClass(), conf);\r\n    OutputCommitter committer = outputFormat.getOutputCommitter(tContext);\r\n    committer.commitTask(tContext);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app",
  "methodName" : "writeOutput",
  "errType" : null,
  "containingMethodsNum" : 15,
  "sourceCodeText" : "void writeOutput(TaskAttempt attempt, Configuration conf) throws Exception\n{\r\n    TaskAttemptContext tContext = new TaskAttemptContextImpl(conf, TypeConverter.fromYarn(attempt.getID()));\r\n    TextOutputFormat<?, ?> theOutputFormat = new TextOutputFormat();\r\n    RecordWriter theRecordWriter = theOutputFormat.getRecordWriter(tContext);\r\n    NullWritable nullWritable = NullWritable.get();\r\n    try {\r\n        theRecordWriter.write(key1, val1);\r\n        theRecordWriter.write(null, nullWritable);\r\n        theRecordWriter.write(null, val1);\r\n        theRecordWriter.write(nullWritable, val2);\r\n        theRecordWriter.write(key2, nullWritable);\r\n        theRecordWriter.write(key1, null);\r\n        theRecordWriter.write(null, null);\r\n        theRecordWriter.write(key2, val2);\r\n    } finally {\r\n        theRecordWriter.close(tContext);\r\n    }\r\n    OutputFormat outputFormat = ReflectionUtils.newInstance(tContext.getOutputFormatClass(), conf);\r\n    OutputCommitter committer = outputFormat.getOutputCommitter(tContext);\r\n    committer.commitTask(tContext);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app",
  "methodName" : "validateOutput",
  "errType" : null,
  "containingMethodsNum" : 9,
  "sourceCodeText" : "void validateOutput() throws IOException\n{\r\n    File expectedFile = new File(new Path(outputDir, partFile).toString());\r\n    StringBuffer expectedOutput = new StringBuffer();\r\n    expectedOutput.append(key1).append('\\t').append(val1).append(\"\\n\");\r\n    expectedOutput.append(val1).append(\"\\n\");\r\n    expectedOutput.append(val2).append(\"\\n\");\r\n    expectedOutput.append(key2).append(\"\\n\");\r\n    expectedOutput.append(key1).append(\"\\n\");\r\n    expectedOutput.append(key2).append('\\t').append(val2).append(\"\\n\");\r\n    String output = slurp(expectedFile);\r\n    assertThat(output).isEqualTo(expectedOutput.toString());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app",
  "methodName" : "slurp",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "String slurp(File f) throws IOException\n{\r\n    int len = (int) f.length();\r\n    byte[] buf = new byte[len];\r\n    FileInputStream in = new FileInputStream(f);\r\n    String contents = null;\r\n    try {\r\n        in.read(buf, 0, len);\r\n        contents = new String(buf, \"UTF-8\");\r\n    } finally {\r\n        in.close();\r\n    }\r\n    return contents;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app",
  "methodName" : "main",
  "errType" : null,
  "containingMethodsNum" : 12,
  "sourceCodeText" : "void main(String[] arg) throws Exception\n{\r\n    TestRecovery test = new TestRecovery();\r\n    test.testCrashed();\r\n    test.testMultipleCrashes();\r\n    test.testOutputRecovery();\r\n    test.testOutputRecoveryMapsOnly();\r\n    test.testRecoveryWithOldCommiter();\r\n    test.testSpeculative();\r\n    test.testRecoveryWithoutShuffleSecret();\r\n    test.testRecoverySuccessAttempt();\r\n    test.testRecoveryAllFailAttempts();\r\n    test.testRecoveryTaskSuccessAllAttemptsFail();\r\n    test.testRecoveryTaskSuccessAllAttemptsSucceed();\r\n    test.testRecoveryAllAttemptsKilled();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "setUp",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void setUp() throws Exception\n{\r\n    task = mock(Task.class);\r\n    umbilical = mock(TaskUmbilicalProtocol.class);\r\n    conf = new Configuration();\r\n    when(task.getConf()).thenReturn(conf);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "testReportErrorWhenCapacityExceptionNotHappenByDefault",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testReportErrorWhenCapacityExceptionNotHappenByDefault() throws IOException\n{\r\n    Exception exception = new RuntimeException(new IOException());\r\n    verifyReportError(exception, false);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "testReportErrorWhenCapacityExceptionNotHappenAndFastFailDisabled",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void testReportErrorWhenCapacityExceptionNotHappenAndFastFailDisabled() throws IOException\n{\r\n    Exception exception = new RuntimeException(new IOException());\r\n    conf.setBoolean(KILL_LIMIT_EXCEED_CONF_NAME, false);\r\n    verifyReportError(exception, false);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "testReportErrorWhenCapacityExceptionNotHappenAndFastFailEnabled",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void testReportErrorWhenCapacityExceptionNotHappenAndFastFailEnabled() throws IOException\n{\r\n    Exception exception = new RuntimeException(new IOException());\r\n    conf.setBoolean(KILL_LIMIT_EXCEED_CONF_NAME, true);\r\n    verifyReportError(exception, false);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "testReportErrorWhenCapacityExceptionHappenByDefault",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testReportErrorWhenCapacityExceptionHappenByDefault() throws IOException\n{\r\n    Exception exception = new RuntimeException(new ClusterStorageCapacityExceededException());\r\n    verifyReportError(exception, false);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "testReportErrorWhenCapacityExceptionHappenAndFastFailDisabled",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void testReportErrorWhenCapacityExceptionHappenAndFastFailDisabled() throws IOException\n{\r\n    Exception exception = new RuntimeException(new ClusterStorageCapacityExceededException());\r\n    conf.setBoolean(KILL_LIMIT_EXCEED_CONF_NAME, false);\r\n    verifyReportError(exception, false);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "testReportErrorWhenCapacityExceptionHappenAndFastFailEnabled",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void testReportErrorWhenCapacityExceptionHappenAndFastFailEnabled() throws IOException\n{\r\n    Exception exception = new RuntimeException(new ClusterStorageCapacityExceededException());\r\n    conf.setBoolean(KILL_LIMIT_EXCEED_CONF_NAME, true);\r\n    verifyReportError(exception, true);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "testReportErrorWhenCapacityExceptionHappenInThirdOfExceptionChain",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void testReportErrorWhenCapacityExceptionHappenInThirdOfExceptionChain() throws IOException\n{\r\n    Exception exception = new RuntimeException(new IllegalStateException(new ClusterStorageCapacityExceededException()));\r\n    conf.setBoolean(KILL_LIMIT_EXCEED_CONF_NAME, true);\r\n    verifyReportError(exception, true);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapred",
  "methodName" : "verifyReportError",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void verifyReportError(Exception exception, boolean fastFail) throws IOException\n{\r\n    YarnChild.reportError(exception, task, umbilical);\r\n    verify(umbilical).fatalError(any(), anyString(), eq(fastFail));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app",
  "methodName" : "coreTestEstimator",
  "errType" : null,
  "containingMethodsNum" : 50,
  "sourceCodeText" : "void coreTestEstimator(TaskRuntimeEstimator testedEstimator, int expectedSpeculations)\n{\r\n    estimator = testedEstimator;\r\n    clock = new ControlledClock();\r\n    dispatcher = new AsyncDispatcher();\r\n    Configuration conf = new Configuration();\r\n    dispatcher.init(conf);\r\n    myJob = null;\r\n    slotsInUse.set(0);\r\n    completedMaps.set(0);\r\n    completedReduces.set(0);\r\n    successfulSpeculations.set(0);\r\n    taskTimeSavedBySpeculation.set(0);\r\n    clock.tickMsec(1000);\r\n    myAppContext = new MyAppContext(MAP_TASKS, REDUCE_TASKS);\r\n    myJob = myAppContext.getAllJobs().values().iterator().next();\r\n    estimator.contextualize(conf, myAppContext);\r\n    conf.setLong(MRJobConfig.SPECULATIVE_RETRY_AFTER_NO_SPECULATE, 500L);\r\n    conf.setLong(MRJobConfig.SPECULATIVE_RETRY_AFTER_SPECULATE, 5000L);\r\n    conf.setDouble(MRJobConfig.SPECULATIVECAP_RUNNING_TASKS, 0.1);\r\n    conf.setDouble(MRJobConfig.SPECULATIVECAP_TOTAL_TASKS, 0.001);\r\n    conf.setInt(MRJobConfig.SPECULATIVE_MINIMUM_ALLOWED_TASKS, 5);\r\n    speculator = new DefaultSpeculator(conf, myAppContext, estimator, clock);\r\n    Assert.assertEquals(\"wrong SPECULATIVE_RETRY_AFTER_NO_SPECULATE value\", 500L, speculator.getSoonestRetryAfterNoSpeculate());\r\n    Assert.assertEquals(\"wrong SPECULATIVE_RETRY_AFTER_SPECULATE value\", 5000L, speculator.getSoonestRetryAfterSpeculate());\r\n    assertThat(speculator.getProportionRunningTasksSpeculatable()).isCloseTo(0.1, offset(0.00001));\r\n    assertThat(speculator.getProportionTotalTasksSpeculatable()).isCloseTo(0.001, offset(0.00001));\r\n    Assert.assertEquals(\"wrong SPECULATIVE_MINIMUM_ALLOWED_TASKS value\", 5, speculator.getMinimumAllowedSpeculativeTasks());\r\n    dispatcher.register(Speculator.EventType.class, speculator);\r\n    dispatcher.register(TaskEventType.class, new SpeculationRequestEventHandler());\r\n    dispatcher.start();\r\n    speculator.init(conf);\r\n    speculator.start();\r\n    int undoneMaps = MAP_TASKS;\r\n    int undoneReduces = REDUCE_TASKS;\r\n    List<Task> allTasksSequence = new LinkedList<Task>();\r\n    allTasksSequence.addAll(myJob.getTasks(TaskType.MAP).values());\r\n    allTasksSequence.addAll(myJob.getTasks(TaskType.REDUCE).values());\r\n    while (undoneMaps + undoneReduces > 0) {\r\n        undoneMaps = 0;\r\n        undoneReduces = 0;\r\n        for (Task task : allTasksSequence) {\r\n            if (!task.isFinished()) {\r\n                if (task.getType() == TaskType.MAP) {\r\n                    ++undoneMaps;\r\n                } else {\r\n                    ++undoneReduces;\r\n                }\r\n            }\r\n            for (TaskAttempt attempt : task.getAttempts().values()) {\r\n                if (attempt.getState() == TaskAttemptState.NEW && INITIAL_NUMBER_FREE_SLOTS - slotsInUse.get() >= taskTypeSlots(task.getType())) {\r\n                    MyTaskAttemptImpl attemptImpl = (MyTaskAttemptImpl) attempt;\r\n                    SpeculatorEvent event = new SpeculatorEvent(attempt.getID(), false, clock.getTime());\r\n                    speculator.handle(event);\r\n                    attemptImpl.startUp();\r\n                } else {\r\n                    TaskAttemptStatus status = new TaskAttemptStatus();\r\n                    status.id = attempt.getID();\r\n                    status.progress = attempt.getProgress();\r\n                    status.stateString = attempt.getState().name();\r\n                    status.taskState = attempt.getState();\r\n                    SpeculatorEvent event = new SpeculatorEvent(status, clock.getTime());\r\n                    speculator.handle(event);\r\n                }\r\n            }\r\n        }\r\n        long startTime = System.currentTimeMillis();\r\n        while (!speculator.eventQueueEmpty()) {\r\n            Thread.yield();\r\n            if (System.currentTimeMillis() > startTime + 130000) {\r\n                return;\r\n            }\r\n        }\r\n        clock.tickMsec(1000L);\r\n        if (clock.getTime() % 10000L == 0L) {\r\n            speculator.scanForSpeculations();\r\n        }\r\n    }\r\n    Assert.assertEquals(\"We got the wrong number of successful speculations.\", expectedSpeculations, successfulSpeculations.get());\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app",
  "methodName" : "testLegacyEstimator",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testLegacyEstimator() throws Exception\n{\r\n    TaskRuntimeEstimator specificEstimator = new LegacyTaskRuntimeEstimator();\r\n    coreTestEstimator(specificEstimator, 3);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app",
  "methodName" : "testExponentialEstimator",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testExponentialEstimator() throws Exception\n{\r\n    TaskRuntimeEstimator specificEstimator = new ExponentiallySmoothedTaskRuntimeEstimator();\r\n    coreTestEstimator(specificEstimator, 3);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app",
  "methodName" : "testSimpleExponentialEstimator",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testSimpleExponentialEstimator() throws Exception\n{\r\n    TaskRuntimeEstimator specificEstimator = new SimpleExponentialTaskRuntimeEstimator();\r\n    coreTestEstimator(specificEstimator, 3);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app",
  "methodName" : "taskTypeSlots",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int taskTypeSlots(TaskType type)\n{\r\n    return type == TaskType.MAP ? MAP_SLOT_REQUIREMENT : REDUCE_SLOT_REQUIREMENT;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app",
  "methodName" : "addAttempt",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void addAttempt(Task task)\n{\r\n    MyTaskImpl myTask = (MyTaskImpl) task;\r\n    myTask.addAttempt();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app",
  "methodName" : "testMapReduce",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void testMapReduce() throws Exception\n{\r\n    MRApp app = new MRApp(2, 2, true, this.getClass().getName(), true);\r\n    Job job = app.submit(new Configuration());\r\n    app.waitForState(job, JobState.SUCCEEDED);\r\n    app.verifyCompleted();\r\n    Assert.assertEquals(System.getProperty(\"user.name\"), job.getUserName());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app",
  "methodName" : "testZeroMaps",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void testZeroMaps() throws Exception\n{\r\n    MRApp app = new MRApp(0, 1, true, this.getClass().getName(), true);\r\n    Job job = app.submit(new Configuration());\r\n    app.waitForState(job, JobState.SUCCEEDED);\r\n    app.verifyCompleted();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app",
  "methodName" : "testZeroMapReduces",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void testZeroMapReduces() throws Exception\n{\r\n    MRApp app = new MRApp(0, 0, true, this.getClass().getName(), true);\r\n    Job job = app.submit(new Configuration());\r\n    app.waitForState(job, JobState.SUCCEEDED);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app",
  "methodName" : "testCommitPending",
  "errType" : null,
  "containingMethodsNum" : 15,
  "sourceCodeText" : "void testCommitPending() throws Exception\n{\r\n    MRApp app = new MRApp(1, 0, false, this.getClass().getName(), true);\r\n    Job job = app.submit(new Configuration());\r\n    app.waitForState(job, JobState.RUNNING);\r\n    Assert.assertEquals(\"Num tasks not correct\", 1, job.getTasks().size());\r\n    Iterator<Task> it = job.getTasks().values().iterator();\r\n    Task task = it.next();\r\n    app.waitForState(task, TaskState.RUNNING);\r\n    TaskAttempt attempt = task.getAttempts().values().iterator().next();\r\n    app.waitForState(attempt, TaskAttemptState.RUNNING);\r\n    app.getContext().getEventHandler().handle(new TaskAttemptEvent(attempt.getID(), TaskAttemptEventType.TA_COMMIT_PENDING));\r\n    app.waitForState(attempt, TaskAttemptState.COMMIT_PENDING);\r\n    app.getContext().getEventHandler().handle(new TaskAttemptEvent(attempt.getID(), TaskAttemptEventType.TA_COMMIT_PENDING));\r\n    app.waitForState(attempt, TaskAttemptState.COMMIT_PENDING);\r\n    app.getContext().getEventHandler().handle(new TaskAttemptEvent(task.getAttempts().values().iterator().next().getID(), TaskAttemptEventType.TA_DONE));\r\n    app.waitForState(job, JobState.SUCCEEDED);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app",
  "methodName" : "testCompletedMapsForReduceSlowstart",
  "errType" : null,
  "containingMethodsNum" : 23,
  "sourceCodeText" : "void testCompletedMapsForReduceSlowstart() throws Exception\n{\r\n    MRApp app = new MRApp(2, 1, false, this.getClass().getName(), true);\r\n    Configuration conf = new Configuration();\r\n    conf.setFloat(MRJobConfig.COMPLETED_MAPS_FOR_REDUCE_SLOWSTART, 0.5f);\r\n    conf.setBoolean(MRJobConfig.JOB_UBERTASK_ENABLE, false);\r\n    Job job = app.submit(conf);\r\n    app.waitForState(job, JobState.RUNNING);\r\n    Assert.assertEquals(\"Num tasks not correct\", 3, job.getTasks().size());\r\n    Iterator<Task> it = job.getTasks().values().iterator();\r\n    Task mapTask1 = it.next();\r\n    Task mapTask2 = it.next();\r\n    Task reduceTask = it.next();\r\n    app.waitForState(mapTask1, TaskState.RUNNING);\r\n    app.waitForState(mapTask2, TaskState.RUNNING);\r\n    TaskAttempt task1Attempt = mapTask1.getAttempts().values().iterator().next();\r\n    TaskAttempt task2Attempt = mapTask2.getAttempts().values().iterator().next();\r\n    app.waitForState(task1Attempt, TaskAttemptState.RUNNING);\r\n    app.waitForState(task2Attempt, TaskAttemptState.RUNNING);\r\n    Assert.assertEquals(\"Reduce Task state not correct\", TaskState.NEW, reduceTask.getReport().getTaskState());\r\n    app.getContext().getEventHandler().handle(new TaskAttemptEvent(mapTask1.getAttempts().values().iterator().next().getID(), TaskAttemptEventType.TA_DONE));\r\n    app.waitForState(mapTask1, TaskState.SUCCEEDED);\r\n    app.waitForState(reduceTask, TaskState.RUNNING);\r\n    app.getContext().getEventHandler().handle(new TaskAttemptEvent(mapTask2.getAttempts().values().iterator().next().getID(), TaskAttemptEventType.TA_DONE));\r\n    app.getContext().getEventHandler().handle(new TaskAttemptEvent(reduceTask.getAttempts().values().iterator().next().getID(), TaskAttemptEventType.TA_DONE));\r\n    app.waitForState(job, JobState.SUCCEEDED);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app",
  "methodName" : "testUpdatedNodes",
  "errType" : null,
  "containingMethodsNum" : 85,
  "sourceCodeText" : "void testUpdatedNodes() throws Exception\n{\r\n    int runCount = 0;\r\n    AsyncDispatcher dispatcher = new AsyncDispatcher();\r\n    dispatcher.init(new Configuration());\r\n    Dispatcher disp = Mockito.spy(dispatcher);\r\n    MRApp app = new MRAppWithHistory(2, 2, false, this.getClass().getName(), true, ++runCount, disp);\r\n    Configuration conf = new Configuration();\r\n    conf.setFloat(MRJobConfig.COMPLETED_MAPS_FOR_REDUCE_SLOWSTART, 0.5f);\r\n    conf.setBoolean(MRJobConfig.JOB_UBERTASK_ENABLE, false);\r\n    ContainerAllocEventHandler handler = new ContainerAllocEventHandler();\r\n    disp.register(ContainerAllocator.EventType.class, handler);\r\n    final Job job1 = app.submit(conf);\r\n    app.waitForState(job1, JobState.RUNNING);\r\n    Assert.assertEquals(\"Num tasks not correct\", 4, job1.getTasks().size());\r\n    Iterator<Task> it = job1.getTasks().values().iterator();\r\n    Task mapTask1 = it.next();\r\n    Task mapTask2 = it.next();\r\n    app.waitForState(mapTask1, TaskState.RUNNING);\r\n    app.waitForState(mapTask2, TaskState.RUNNING);\r\n    TaskAttempt task1Attempt = mapTask1.getAttempts().values().iterator().next();\r\n    TaskAttempt task2Attempt = mapTask2.getAttempts().values().iterator().next();\r\n    NodeId node1 = task1Attempt.getNodeId();\r\n    NodeId node2 = task2Attempt.getNodeId();\r\n    Assert.assertEquals(node1, node2);\r\n    app.getContext().getEventHandler().handle(new TaskAttemptEvent(task1Attempt.getID(), TaskAttemptEventType.TA_DONE));\r\n    app.getContext().getEventHandler().handle(new TaskAttemptEvent(task2Attempt.getID(), TaskAttemptEventType.TA_DONE));\r\n    app.waitForState(mapTask1, TaskState.SUCCEEDED);\r\n    app.waitForState(mapTask2, TaskState.SUCCEEDED);\r\n    final int checkIntervalMillis = 100;\r\n    final int waitForMillis = 800;\r\n    waitFor(new Supplier<Boolean>() {\r\n\r\n        @Override\r\n        public Boolean get() {\r\n            TaskAttemptCompletionEvent[] events = job1.getTaskAttemptCompletionEvents(0, 100);\r\n            return events.length == 2;\r\n        }\r\n    }, checkIntervalMillis, waitForMillis);\r\n    TaskAttemptCompletionEvent[] events = job1.getTaskAttemptCompletionEvents(0, 100);\r\n    Assert.assertEquals(\"Expecting 2 completion events for success\", 2, events.length);\r\n    ArrayList<NodeReport> updatedNodes = new ArrayList<NodeReport>();\r\n    NodeReport nr = RecordFactoryProvider.getRecordFactory(null).newRecordInstance(NodeReport.class);\r\n    nr.setNodeId(node1);\r\n    nr.setNodeState(NodeState.UNHEALTHY);\r\n    updatedNodes.add(nr);\r\n    app.getContext().getEventHandler().handle(new JobUpdatedNodesEvent(job1.getID(), updatedNodes));\r\n    app.waitForState(task1Attempt, TaskAttemptState.KILLED);\r\n    app.waitForState(task2Attempt, TaskAttemptState.KILLED);\r\n    waitFor(new Supplier<Boolean>() {\r\n\r\n        @Override\r\n        public Boolean get() {\r\n            TaskAttemptCompletionEvent[] events = job1.getTaskAttemptCompletionEvents(0, 100);\r\n            return events.length == 4;\r\n        }\r\n    }, checkIntervalMillis, waitForMillis);\r\n    events = job1.getTaskAttemptCompletionEvents(0, 100);\r\n    Assert.assertEquals(\"Expecting 2 more completion events for killed\", 4, events.length);\r\n    handler.waitForFailedMapContainerReqEvents(2);\r\n    app.waitForState(mapTask1, TaskState.RUNNING);\r\n    app.waitForState(mapTask2, TaskState.RUNNING);\r\n    Iterator<TaskAttempt> itr = mapTask1.getAttempts().values().iterator();\r\n    itr.next();\r\n    task1Attempt = itr.next();\r\n    app.getContext().getEventHandler().handle(new TaskAttemptEvent(task1Attempt.getID(), TaskAttemptEventType.TA_DONE));\r\n    app.waitForState(mapTask1, TaskState.SUCCEEDED);\r\n    app.waitForState(mapTask2, TaskState.RUNNING);\r\n    waitFor(new Supplier<Boolean>() {\r\n\r\n        @Override\r\n        public Boolean get() {\r\n            TaskAttemptCompletionEvent[] events = job1.getTaskAttemptCompletionEvents(0, 100);\r\n            return events.length == 5;\r\n        }\r\n    }, checkIntervalMillis, waitForMillis);\r\n    events = job1.getTaskAttemptCompletionEvents(0, 100);\r\n    Assert.assertEquals(\"Expecting 1 more completion events for success\", 5, events.length);\r\n    app.stop();\r\n    app = new MRAppWithHistory(2, 2, false, this.getClass().getName(), false, ++runCount, (Dispatcher) new AsyncDispatcher());\r\n    conf = new Configuration();\r\n    conf.setBoolean(MRJobConfig.MR_AM_JOB_RECOVERY_ENABLE, true);\r\n    conf.setBoolean(MRJobConfig.JOB_UBERTASK_ENABLE, false);\r\n    final Job job2 = app.submit(conf);\r\n    app.waitForState(job2, JobState.RUNNING);\r\n    Assert.assertEquals(\"No of tasks not correct\", 4, job2.getTasks().size());\r\n    it = job2.getTasks().values().iterator();\r\n    mapTask1 = it.next();\r\n    mapTask2 = it.next();\r\n    Task reduceTask1 = it.next();\r\n    Task reduceTask2 = it.next();\r\n    app.waitForState(mapTask1, TaskState.SUCCEEDED);\r\n    app.waitForState(mapTask2, TaskState.RUNNING);\r\n    waitFor(new Supplier<Boolean>() {\r\n\r\n        @Override\r\n        public Boolean get() {\r\n            TaskAttemptCompletionEvent[] events = job2.getTaskAttemptCompletionEvents(0, 100);\r\n            return events.length == 2;\r\n        }\r\n    }, checkIntervalMillis, waitForMillis);\r\n    events = job2.getTaskAttemptCompletionEvents(0, 100);\r\n    Assert.assertEquals(\"Expecting 2 completion events for killed & success of map1\", 2, events.length);\r\n    task2Attempt = mapTask2.getAttempts().values().iterator().next();\r\n    app.getContext().getEventHandler().handle(new TaskAttemptEvent(task2Attempt.getID(), TaskAttemptEventType.TA_DONE));\r\n    app.waitForState(mapTask2, TaskState.SUCCEEDED);\r\n    waitFor(new Supplier<Boolean>() {\r\n\r\n        @Override\r\n        public Boolean get() {\r\n            TaskAttemptCompletionEvent[] events = job2.getTaskAttemptCompletionEvents(0, 100);\r\n            return events.length == 3;\r\n        }\r\n    }, checkIntervalMillis, waitForMillis);\r\n    events = job2.getTaskAttemptCompletionEvents(0, 100);\r\n    Assert.assertEquals(\"Expecting 1 more completion events for success\", 3, events.length);\r\n    app.waitForState(reduceTask1, TaskState.RUNNING);\r\n    app.waitForState(reduceTask2, TaskState.RUNNING);\r\n    TaskAttempt task3Attempt = reduceTask1.getAttempts().values().iterator().next();\r\n    app.getContext().getEventHandler().handle(new TaskAttemptEvent(task3Attempt.getID(), TaskAttemptEventType.TA_DONE));\r\n    app.waitForState(reduceTask1, TaskState.SUCCEEDED);\r\n    app.getContext().getEventHandler().handle(new TaskAttemptEvent(task3Attempt.getID(), TaskAttemptEventType.TA_KILL));\r\n    app.waitForState(reduceTask1, TaskState.SUCCEEDED);\r\n    TaskAttempt task4Attempt = reduceTask2.getAttempts().values().iterator().next();\r\n    app.getContext().getEventHandler().handle(new TaskAttemptEvent(task4Attempt.getID(), TaskAttemptEventType.TA_DONE));\r\n    app.waitForState(reduceTask2, TaskState.SUCCEEDED);\r\n    waitFor(new Supplier<Boolean>() {\r\n\r\n        @Override\r\n        public Boolean get() {\r\n            TaskAttemptCompletionEvent[] events = job2.getTaskAttemptCompletionEvents(0, 100);\r\n            return events.length == 5;\r\n        }\r\n    }, checkIntervalMillis, waitForMillis);\r\n    events = job2.getTaskAttemptCompletionEvents(0, 100);\r\n    Assert.assertEquals(\"Expecting 2 more completion events for reduce success\", 5, events.length);\r\n    app.waitForState(job2, JobState.SUCCEEDED);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app",
  "methodName" : "waitFor",
  "errType" : [ "TimeoutException" ],
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void waitFor(Supplier<Boolean> predicate, int checkIntervalMillis, int checkTotalMillis) throws InterruptedException\n{\r\n    try {\r\n        GenericTestUtils.waitFor(predicate, checkIntervalMillis, checkTotalMillis);\r\n    } catch (TimeoutException ex) {\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app",
  "methodName" : "testJobError",
  "errType" : null,
  "containingMethodsNum" : 9,
  "sourceCodeText" : "void testJobError() throws Exception\n{\r\n    MRApp app = new MRApp(1, 0, false, this.getClass().getName(), true);\r\n    Job job = app.submit(new Configuration());\r\n    app.waitForState(job, JobState.RUNNING);\r\n    Assert.assertEquals(\"Num tasks not correct\", 1, job.getTasks().size());\r\n    Iterator<Task> it = job.getTasks().values().iterator();\r\n    Task task = it.next();\r\n    app.waitForState(task, TaskState.RUNNING);\r\n    app.getContext().getEventHandler().handle(new TaskEvent(task.getID(), TaskEventType.T_SCHEDULE));\r\n    app.waitForState(job, JobState.ERROR);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app",
  "methodName" : "testJobSuccess",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void testJobSuccess() throws Exception\n{\r\n    MRApp app = new MRApp(2, 2, true, this.getClass().getName(), true, false);\r\n    JobImpl job = (JobImpl) app.submit(new Configuration());\r\n    app.waitForInternalState(job, JobStateInternal.SUCCEEDED);\r\n    Assert.assertEquals(JobState.RUNNING, job.getState());\r\n    app.successfullyUnregistered.set(true);\r\n    app.waitForState(job, JobState.SUCCEEDED);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app",
  "methodName" : "testJobRebootNotLastRetryOnUnregistrationFailure",
  "errType" : null,
  "containingMethodsNum" : 9,
  "sourceCodeText" : "void testJobRebootNotLastRetryOnUnregistrationFailure() throws Exception\n{\r\n    MRApp app = new MRApp(1, 0, false, this.getClass().getName(), true);\r\n    Job job = app.submit(new Configuration());\r\n    app.waitForState(job, JobState.RUNNING);\r\n    Assert.assertEquals(\"Num tasks not correct\", 1, job.getTasks().size());\r\n    Iterator<Task> it = job.getTasks().values().iterator();\r\n    Task task = it.next();\r\n    app.waitForState(task, TaskState.RUNNING);\r\n    app.getContext().getEventHandler().handle(new JobEvent(job.getID(), JobEventType.JOB_AM_REBOOT));\r\n    app.waitForState(job, JobState.RUNNING);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app",
  "methodName" : "testJobRebootOnLastRetryOnUnregistrationFailure",
  "errType" : null,
  "containingMethodsNum" : 10,
  "sourceCodeText" : "void testJobRebootOnLastRetryOnUnregistrationFailure() throws Exception\n{\r\n    MRApp app = new MRApp(1, 0, false, this.getClass().getName(), true, 2, false);\r\n    Configuration conf = new Configuration();\r\n    Job job = app.submit(conf);\r\n    app.waitForState(job, JobState.RUNNING);\r\n    Assert.assertEquals(\"Num tasks not correct\", 1, job.getTasks().size());\r\n    Iterator<Task> it = job.getTasks().values().iterator();\r\n    Task task = it.next();\r\n    app.waitForState(task, TaskState.RUNNING);\r\n    app.getContext().getEventHandler().handle(new JobEvent(job.getID(), JobEventType.JOB_AM_REBOOT));\r\n    app.waitForInternalState((JobImpl) job, JobStateInternal.REBOOT);\r\n    app.waitForState(job, JobState.RUNNING);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app",
  "methodName" : "testCountersOnJobFinish",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void testCountersOnJobFinish() throws Exception\n{\r\n    MRAppWithSpiedJob app = new MRAppWithSpiedJob(1, 1, true, this.getClass().getName(), true);\r\n    JobImpl job = (JobImpl) app.submit(new Configuration());\r\n    app.waitForState(job, JobState.SUCCEEDED);\r\n    app.verifyCompleted();\r\n    System.out.println(job.getAllCounters());\r\n    job.getAllCounters();\r\n    job.getAllCounters();\r\n    verify(job, times(1)).constructFinalFullcounters();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app",
  "methodName" : "checkJobStateTypeConversion",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void checkJobStateTypeConversion()\n{\r\n    for (JobState state : JobState.values()) {\r\n        TypeConverter.fromYarn(state);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app",
  "methodName" : "checkTaskStateTypeConversion",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void checkTaskStateTypeConversion()\n{\r\n    for (TaskState state : TaskState.values()) {\r\n        TypeConverter.fromYarn(state);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app",
  "methodName" : "testContainerPassThrough",
  "errType" : null,
  "containingMethodsNum" : 10,
  "sourceCodeText" : "void testContainerPassThrough() throws Exception\n{\r\n    MRApp app = new MRApp(0, 1, true, this.getClass().getName(), true) {\r\n\r\n        @Override\r\n        protected ContainerLauncher createContainerLauncher(AppContext context) {\r\n            return new MockContainerLauncher() {\r\n\r\n                @Override\r\n                public void handle(ContainerLauncherEvent event) {\r\n                    if (event instanceof ContainerRemoteLaunchEvent) {\r\n                        containerObtainedByContainerLauncher = ((ContainerRemoteLaunchEvent) event).getAllocatedContainer();\r\n                    }\r\n                    super.handle(event);\r\n                }\r\n            };\r\n        }\r\n    };\r\n    Job job = app.submit(new Configuration());\r\n    app.waitForState(job, JobState.SUCCEEDED);\r\n    app.verifyCompleted();\r\n    Collection<Task> tasks = job.getTasks().values();\r\n    Collection<TaskAttempt> taskAttempts = tasks.iterator().next().getAttempts().values();\r\n    TaskAttemptImpl taskAttempt = (TaskAttemptImpl) taskAttempts.iterator().next();\r\n    Assert.assertTrue(taskAttempt.container == containerObtainedByContainerLauncher);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app",
  "methodName" : "main",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void main(String[] args) throws Exception\n{\r\n    TestMRApp t = new TestMRApp();\r\n    t.testMapReduce();\r\n    t.testZeroMapReduces();\r\n    t.testCommitPending();\r\n    t.testCompletedMapsForReduceSlowstart();\r\n    t.testJobError();\r\n    t.testCountersOnJobFinish();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app",
  "methodName" : "testComponentStopOrder",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void testComponentStopOrder() throws Exception\n{\r\n    @SuppressWarnings(\"resource\")\r\n    TestMRApp app = new TestMRApp(1, 1, true, this.getClass().getName(), true);\r\n    JobImpl job = (JobImpl) app.submit(new Configuration());\r\n    app.waitForState(job, JobState.SUCCEEDED);\r\n    app.verifyCompleted();\r\n    int waitTime = 20 * 1000;\r\n    while (waitTime > 0 && app.numStops < 2) {\r\n        Thread.sleep(100);\r\n        waitTime -= 100;\r\n    }\r\n    Assert.assertEquals(1, app.JobHistoryEventHandlerStopped);\r\n    Assert.assertEquals(2, app.clientServiceStopped);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\webapp",
  "methodName" : "tearDown",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void tearDown()\n{\r\n    TEST_DIR.delete();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\webapp",
  "methodName" : "testAppControllerIndex",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void testAppControllerIndex()\n{\r\n    AppContext ctx = new MockAppContext(0, 1, 1, 1);\r\n    Injector injector = WebAppTests.createMockInjector(AppContext.class, ctx);\r\n    AppController controller = injector.getInstance(AppController.class);\r\n    controller.index();\r\n    assertEquals(ctx.getApplicationID().toString(), controller.get(APP_ID, \"\"));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\webapp",
  "methodName" : "testAppView",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testAppView()\n{\r\n    WebAppTests.testPage(AppView.class, AppContext.class, new MockAppContext(0, 1, 1, 1));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\webapp",
  "methodName" : "testJobView",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void testJobView()\n{\r\n    AppContext appContext = new MockAppContext(0, 1, 1, 1);\r\n    Map<String, String> params = getJobParams(appContext);\r\n    WebAppTests.testPage(JobPage.class, AppContext.class, appContext, params);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\webapp",
  "methodName" : "testTasksView",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void testTasksView()\n{\r\n    AppContext appContext = new MockAppContext(0, 1, 1, 1);\r\n    Map<String, String> params = getTaskParams(appContext);\r\n    WebAppTests.testPage(TasksPage.class, AppContext.class, appContext, params);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\webapp",
  "methodName" : "testTaskView",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void testTaskView()\n{\r\n    AppContext appContext = new MockAppContext(0, 1, 1, 1);\r\n    Map<String, String> params = getTaskParams(appContext);\r\n    App app = new App(appContext);\r\n    app.setJob(appContext.getAllJobs().values().iterator().next());\r\n    app.setTask(app.getJob().getTasks().values().iterator().next());\r\n    WebAppTests.testPage(TaskPage.class, App.class, app, params);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\webapp",
  "methodName" : "getJobParams",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "Map<String, String> getJobParams(AppContext appContext)\n{\r\n    JobId jobId = appContext.getAllJobs().entrySet().iterator().next().getKey();\r\n    Map<String, String> params = new HashMap<String, String>();\r\n    params.put(AMParams.JOB_ID, MRApps.toString(jobId));\r\n    return params;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\webapp",
  "methodName" : "getTaskParams",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "Map<String, String> getTaskParams(AppContext appContext)\n{\r\n    JobId jobId = appContext.getAllJobs().entrySet().iterator().next().getKey();\r\n    Entry<TaskId, Task> e = appContext.getJob(jobId).getTasks().entrySet().iterator().next();\r\n    e.getValue().getType();\r\n    Map<String, String> params = new HashMap<String, String>();\r\n    params.put(AMParams.JOB_ID, MRApps.toString(jobId));\r\n    params.put(AMParams.TASK_ID, MRApps.toString(e.getKey()));\r\n    params.put(AMParams.TASK_TYPE, MRApps.taskSymbol(e.getValue().getType()));\r\n    return params;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\webapp",
  "methodName" : "testConfView",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testConfView()\n{\r\n    WebAppTests.testPage(JobConfPage.class, AppContext.class, new MockAppContext(0, 1, 1, 1));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\webapp",
  "methodName" : "testCountersView",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void testCountersView()\n{\r\n    AppContext appContext = new MockAppContext(0, 1, 1, 1);\r\n    Map<String, String> params = getJobParams(appContext);\r\n    WebAppTests.testPage(CountersPage.class, AppContext.class, appContext, params);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\webapp",
  "methodName" : "testSingleCounterView",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void testSingleCounterView()\n{\r\n    AppContext appContext = new MockAppContext(0, 1, 1, 1);\r\n    Job job = appContext.getAllJobs().values().iterator().next();\r\n    Task failedTask = MockJobs.newTask(job.getID(), 2, 1, true);\r\n    Map<TaskId, Task> tasks = job.getTasks();\r\n    tasks.put(failedTask.getID(), failedTask);\r\n    Map<String, String> params = getJobParams(appContext);\r\n    params.put(AMParams.COUNTER_GROUP, \"org.apache.hadoop.mapreduce.FileSystemCounter\");\r\n    params.put(AMParams.COUNTER_NAME, \"HDFS_WRITE_OPS\");\r\n    WebAppTests.testPage(SingleCounterPage.class, AppContext.class, appContext, params);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\webapp",
  "methodName" : "testTaskCountersView",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void testTaskCountersView()\n{\r\n    AppContext appContext = new MockAppContext(0, 1, 1, 1);\r\n    Map<String, String> params = getTaskParams(appContext);\r\n    WebAppTests.testPage(CountersPage.class, AppContext.class, appContext, params);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\webapp",
  "methodName" : "testSingleTaskCounterView",
  "errType" : null,
  "containingMethodsNum" : 9,
  "sourceCodeText" : "void testSingleTaskCounterView()\n{\r\n    AppContext appContext = new MockAppContext(0, 1, 1, 2);\r\n    Map<String, String> params = getTaskParams(appContext);\r\n    params.put(AMParams.COUNTER_GROUP, \"org.apache.hadoop.mapreduce.FileSystemCounter\");\r\n    params.put(AMParams.COUNTER_NAME, \"HDFS_WRITE_OPS\");\r\n    TaskId taskID = MRApps.toTaskID(params.get(AMParams.TASK_ID));\r\n    Job job = appContext.getJob(taskID.getJobId());\r\n    Task task = job.getTask(taskID);\r\n    TaskAttempt attempt = task.getAttempts().values().iterator().next();\r\n    attempt.getReport().setCounters(null);\r\n    WebAppTests.testPage(SingleCounterPage.class, AppContext.class, appContext, params);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\webapp",
  "methodName" : "testMRWebAppSSLDisabled",
  "errType" : [ "SSLException" ],
  "containingMethodsNum" : 13,
  "sourceCodeText" : "void testMRWebAppSSLDisabled() throws Exception\n{\r\n    MRApp app = new MRApp(2, 2, true, this.getClass().getName(), true) {\r\n\r\n        @Override\r\n        protected ClientService createClientService(AppContext context) {\r\n            return new MRClientService(context);\r\n        }\r\n    };\r\n    Configuration conf = new Configuration();\r\n    conf.set(YarnConfiguration.YARN_HTTP_POLICY_KEY, Policy.HTTPS_ONLY.name());\r\n    Job job = app.submit(conf);\r\n    String hostPort = NetUtils.getHostPortString(((MRClientService) app.getClientService()).getWebApp().getListenerAddress());\r\n    URL httpUrl = new URL(\"http://\" + hostPort);\r\n    HttpURLConnection conn = (HttpURLConnection) httpUrl.openConnection();\r\n    InputStream in = conn.getInputStream();\r\n    ByteArrayOutputStream out = new ByteArrayOutputStream();\r\n    IOUtils.copyBytes(in, out, 1024);\r\n    Assert.assertTrue(out.toString().contains(\"MapReduce Application\"));\r\n    URL httpsUrl = new URL(\"https://\" + hostPort);\r\n    try {\r\n        HttpURLConnection httpsConn = (HttpURLConnection) httpsUrl.openConnection();\r\n        httpsConn.getInputStream();\r\n        Assert.fail(\"https:// is not accessible, expected to fail\");\r\n    } catch (SSLException e) {\r\n    }\r\n    app.waitForState(job, JobState.SUCCEEDED);\r\n    app.verifyCompleted();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\webapp",
  "methodName" : "testMRWebAppSSLEnabled",
  "errType" : [ "SocketException" ],
  "containingMethodsNum" : 21,
  "sourceCodeText" : "void testMRWebAppSSLEnabled() throws Exception\n{\r\n    MRApp app = new MRApp(2, 2, true, this.getClass().getName(), true) {\r\n\r\n        @Override\r\n        protected ClientService createClientService(AppContext context) {\r\n            return new MRClientService(context);\r\n        }\r\n    };\r\n    Configuration conf = new Configuration();\r\n    conf.setBoolean(MRJobConfig.MR_AM_WEBAPP_HTTPS_ENABLED, true);\r\n    KeyPair keyPair = KeyStoreTestUtil.generateKeyPair(\"RSA\");\r\n    Certificate cert = KeyStoreTestUtil.generateCertificate(\"CN=foo\", keyPair, 5, \"SHA512WITHRSA\");\r\n    File keystoreFile = new File(TEST_DIR, \"server.keystore\");\r\n    keystoreFile.getParentFile().mkdirs();\r\n    KeyStoreTestUtil.createKeyStore(keystoreFile.getAbsolutePath(), \"password\", \"server\", keyPair.getPrivate(), cert);\r\n    environmentVariables.set(\"KEYSTORE_FILE_LOCATION\", keystoreFile.getAbsolutePath());\r\n    environmentVariables.set(\"KEYSTORE_PASSWORD\", \"password\");\r\n    Job job = app.submit(conf);\r\n    String hostPort = NetUtils.getHostPortString(((MRClientService) app.getClientService()).getWebApp().getListenerAddress());\r\n    URL httpsUrl = new URL(\"https://\" + hostPort);\r\n    HttpsURLConnection httpsConn = (HttpsURLConnection) httpsUrl.openConnection();\r\n    KeyStoreTestUtil.setAllowAllSSL(httpsConn);\r\n    InputStream in = httpsConn.getInputStream();\r\n    ByteArrayOutputStream out = new ByteArrayOutputStream();\r\n    IOUtils.copyBytes(in, out, 1024);\r\n    Assert.assertTrue(out.toString().contains(\"MapReduce Application\"));\r\n    URL httpUrl = new URL(\"http://\" + hostPort);\r\n    try {\r\n        HttpURLConnection httpConn = (HttpURLConnection) httpUrl.openConnection();\r\n        httpConn.getResponseCode();\r\n        Assert.fail(\"http:// is not accessible, expected to fail\");\r\n    } catch (SocketException e) {\r\n    }\r\n    app.waitForState(job, JobState.SUCCEEDED);\r\n    app.verifyCompleted();\r\n    keystoreFile.delete();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\webapp",
  "methodName" : "testMRWebAppSSLEnabledWithClientAuth",
  "errType" : [ "SSLException" ],
  "containingMethodsNum" : 32,
  "sourceCodeText" : "void testMRWebAppSSLEnabledWithClientAuth() throws Exception\n{\r\n    MRApp app = new MRApp(2, 2, true, this.getClass().getName(), true) {\r\n\r\n        @Override\r\n        protected ClientService createClientService(AppContext context) {\r\n            return new MRClientService(context);\r\n        }\r\n    };\r\n    Configuration conf = new Configuration();\r\n    conf.setBoolean(MRJobConfig.MR_AM_WEBAPP_HTTPS_ENABLED, true);\r\n    conf.setBoolean(MRJobConfig.MR_AM_WEBAPP_HTTPS_CLIENT_AUTH, true);\r\n    KeyPair keyPair = KeyStoreTestUtil.generateKeyPair(\"RSA\");\r\n    Certificate cert = KeyStoreTestUtil.generateCertificate(\"CN=foo\", keyPair, 5, \"SHA512WITHRSA\");\r\n    File keystoreFile = new File(TEST_DIR, \"server.keystore\");\r\n    keystoreFile.getParentFile().mkdirs();\r\n    KeyStoreTestUtil.createKeyStore(keystoreFile.getAbsolutePath(), \"password\", \"server\", keyPair.getPrivate(), cert);\r\n    environmentVariables.set(\"KEYSTORE_FILE_LOCATION\", keystoreFile.getAbsolutePath());\r\n    environmentVariables.set(\"KEYSTORE_PASSWORD\", \"password\");\r\n    KeyPair clientKeyPair = KeyStoreTestUtil.generateKeyPair(\"RSA\");\r\n    X509Certificate clientCert = KeyStoreTestUtil.generateCertificate(\"CN=bar\", clientKeyPair, 5, \"SHA512WITHRSA\");\r\n    File truststoreFile = new File(TEST_DIR, \"client.truststore\");\r\n    truststoreFile.getParentFile().mkdirs();\r\n    KeyStoreTestUtil.createTrustStore(truststoreFile.getAbsolutePath(), \"password\", \"client\", clientCert);\r\n    environmentVariables.set(\"TRUSTSTORE_FILE_LOCATION\", truststoreFile.getAbsolutePath());\r\n    environmentVariables.set(\"TRUSTSTORE_PASSWORD\", \"password\");\r\n    Job job = app.submit(conf);\r\n    String hostPort = NetUtils.getHostPortString(((MRClientService) app.getClientService()).getWebApp().getListenerAddress());\r\n    URL httpsUrl = new URL(\"https://\" + hostPort);\r\n    HttpsURLConnection httpsConn = (HttpsURLConnection) httpsUrl.openConnection();\r\n    KeyStoreTestUtil.setAllowAllSSL(httpsConn, clientCert, clientKeyPair);\r\n    InputStream in = httpsConn.getInputStream();\r\n    ByteArrayOutputStream out = new ByteArrayOutputStream();\r\n    IOUtils.copyBytes(in, out, 1024);\r\n    Assert.assertTrue(out.toString().contains(\"MapReduce Application\"));\r\n    KeyPair otherClientKeyPair = KeyStoreTestUtil.generateKeyPair(\"RSA\");\r\n    X509Certificate otherClientCert = KeyStoreTestUtil.generateCertificate(\"CN=bar\", otherClientKeyPair, 5, \"SHA512WITHRSA\");\r\n    KeyStoreTestUtil.setAllowAllSSL(httpsConn, otherClientCert, clientKeyPair);\r\n    try {\r\n        HttpURLConnection httpConn = (HttpURLConnection) httpsUrl.openConnection();\r\n        httpConn.getResponseCode();\r\n        Assert.fail(\"Wrong client certificate, expected to fail\");\r\n    } catch (SSLException e) {\r\n    }\r\n    app.waitForState(job, JobState.SUCCEEDED);\r\n    app.verifyCompleted();\r\n    keystoreFile.delete();\r\n    truststoreFile.delete();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\webapp",
  "methodName" : "testMRWebAppRedirection",
  "errType" : null,
  "containingMethodsNum" : 16,
  "sourceCodeText" : "void testMRWebAppRedirection() throws Exception\n{\r\n    String[] schemePrefix = { WebAppUtils.HTTP_PREFIX, WebAppUtils.HTTPS_PREFIX };\r\n    for (String scheme : schemePrefix) {\r\n        MRApp app = new MRApp(2, 2, true, this.getClass().getName(), true) {\r\n\r\n            @Override\r\n            protected ClientService createClientService(AppContext context) {\r\n                return new MRClientService(context);\r\n            }\r\n        };\r\n        Configuration conf = new Configuration();\r\n        conf.set(YarnConfiguration.PROXY_ADDRESS, \"9.9.9.9\");\r\n        conf.set(YarnConfiguration.YARN_HTTP_POLICY_KEY, scheme.equals(WebAppUtils.HTTPS_PREFIX) ? Policy.HTTPS_ONLY.name() : Policy.HTTP_ONLY.name());\r\n        webProxyBase = \"/proxy/\" + app.getAppID();\r\n        conf.set(\"hadoop.http.filter.initializers\", TestAMFilterInitializer.class.getName());\r\n        Job job = app.submit(conf);\r\n        String hostPort = NetUtils.getHostPortString(((MRClientService) app.getClientService()).getWebApp().getListenerAddress());\r\n        URL httpUrl = new URL(\"http://\" + hostPort + \"/mapreduce\");\r\n        HttpURLConnection conn = (HttpURLConnection) httpUrl.openConnection();\r\n        conn.setInstanceFollowRedirects(false);\r\n        conn.connect();\r\n        String expectedURL = scheme + conf.get(YarnConfiguration.PROXY_ADDRESS) + ProxyUriUtils.getPath(app.getAppID(), \"/mapreduce\", true);\r\n        Assert.assertEquals(expectedURL, conn.getHeaderField(HttpHeaders.LOCATION));\r\n        Assert.assertEquals(HttpStatus.SC_MOVED_TEMPORARILY, conn.getResponseCode());\r\n        app.waitForState(job, JobState.SUCCEEDED);\r\n        app.verifyCompleted();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-mapreduce-project\\hadoop-mapreduce-client\\hadoop-mapreduce-client-app\\src\\test\\java\\org\\apache\\hadoop\\mapreduce\\v2\\app\\webapp",
  "methodName" : "main",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void main(String[] args)\n{\r\n    WebApps.$for(\"yarn\", AppContext.class, new MockAppContext(0, 8, 88, 4)).at(58888).inDevMode().start(new AMWebApp()).joinThread();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
} ]
[ {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-streaming\\src\\test\\java\\org\\apache\\hadoop\\streaming",
  "methodName" : "go",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void go() throws IOException\n{\r\n    String prevKey = null;\r\n    int sum = 0;\r\n    String key = readString();\r\n    while (key != null) {\r\n        if (prevKey != null && !key.equals(prevKey)) {\r\n            System.out.println(prevKey + \"\\t\" + sum);\r\n            sum = 0;\r\n        }\r\n        sum += readInt();\r\n        prevKey = key;\r\n        key = readString();\r\n    }\r\n    System.out.println(prevKey + \"\\t\" + sum);\r\n    System.out.flush();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-streaming\\src\\test\\java\\org\\apache\\hadoop\\streaming",
  "methodName" : "main",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void main(String[] args) throws IOException\n{\r\n    RawBytesReduceApp app = new RawBytesReduceApp();\r\n    app.go();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-streaming\\src\\test\\java\\org\\apache\\hadoop\\streaming",
  "methodName" : "readString",
  "errType" : [ "EOFException" ],
  "containingMethodsNum" : 2,
  "sourceCodeText" : "String readString() throws IOException\n{\r\n    int length;\r\n    try {\r\n        length = dis.readInt();\r\n    } catch (EOFException eof) {\r\n        return null;\r\n    }\r\n    byte[] bytes = new byte[length];\r\n    dis.readFully(bytes);\r\n    return new String(bytes, \"UTF-8\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-streaming\\src\\test\\java\\org\\apache\\hadoop\\streaming",
  "methodName" : "readInt",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "int readInt() throws IOException\n{\r\n    dis.readInt();\r\n    IntWritable iw = new IntWritable();\r\n    iw.readFields(dis);\r\n    return iw.get();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-streaming\\src\\test\\java\\org\\apache\\hadoop\\streaming",
  "methodName" : "testMultipleCachefiles",
  "errType" : null,
  "containingMethodsNum" : 41,
  "sourceCodeText" : "void testMultipleCachefiles() throws Exception\n{\r\n    boolean mayExit = false;\r\n    MiniMRCluster mr = null;\r\n    MiniDFSCluster dfs = null;\r\n    try {\r\n        Configuration conf = new Configuration();\r\n        dfs = new MiniDFSCluster.Builder(conf).build();\r\n        FileSystem fileSys = dfs.getFileSystem();\r\n        String namenode = fileSys.getUri().toString();\r\n        mr = new MiniMRCluster(1, namenode, 3);\r\n        List<String> args = new ArrayList<String>();\r\n        for (Map.Entry<String, String> entry : mr.createJobConf()) {\r\n            args.add(\"-jobconf\");\r\n            args.add(entry.getKey() + \"=\" + entry.getValue());\r\n        }\r\n        String[] argv = new String[] { \"-input\", INPUT_FILE, \"-output\", OUTPUT_DIR, \"-mapper\", map, \"-reducer\", reduce, \"-jobconf\", \"stream.tmpdir=\" + System.getProperty(\"test.build.data\", \"/tmp\"), \"-jobconf\", JobConf.MAPRED_MAP_TASK_JAVA_OPTS + \"=\" + \"-Dcontrib.name=\" + System.getProperty(\"contrib.name\") + \" \" + \"-Dbuild.test=\" + System.getProperty(\"build.test\") + \" \" + conf.get(JobConf.MAPRED_MAP_TASK_JAVA_OPTS, conf.get(JobConf.MAPRED_TASK_JAVA_OPTS, \"\")), \"-jobconf\", JobConf.MAPRED_REDUCE_TASK_JAVA_OPTS + \"=\" + \"-Dcontrib.name=\" + System.getProperty(\"contrib.name\") + \" \" + \"-Dbuild.test=\" + System.getProperty(\"build.test\") + \" \" + conf.get(JobConf.MAPRED_REDUCE_TASK_JAVA_OPTS, conf.get(JobConf.MAPRED_TASK_JAVA_OPTS, \"\")), \"-cacheFile\", fileSys.getUri() + CACHE_FILE + \"#\" + mapString, \"-cacheFile\", fileSys.getUri() + CACHE_FILE_2 + \"#\" + mapString2, \"-jobconf\", \"mapred.jar=\" + TestStreaming.STREAMING_JAR };\r\n        for (String arg : argv) {\r\n            args.add(arg);\r\n        }\r\n        argv = args.toArray(new String[args.size()]);\r\n        fileSys.delete(new Path(OUTPUT_DIR), true);\r\n        DataOutputStream file = fileSys.create(new Path(INPUT_FILE));\r\n        file.writeBytes(mapString + \"\\n\");\r\n        file.writeBytes(mapString2 + \"\\n\");\r\n        file.close();\r\n        file = fileSys.create(new Path(CACHE_FILE));\r\n        file.writeBytes(cacheString + \"\\n\");\r\n        file.close();\r\n        file = fileSys.create(new Path(CACHE_FILE_2));\r\n        file.writeBytes(cacheString2 + \"\\n\");\r\n        file.close();\r\n        job = new StreamJob(argv, mayExit);\r\n        job.go();\r\n        fileSys = dfs.getFileSystem();\r\n        String line = null;\r\n        String line2 = null;\r\n        Path[] fileList = FileUtil.stat2Paths(fileSys.listStatus(new Path(OUTPUT_DIR), new Utils.OutputFileUtils.OutputFilesFilter()));\r\n        for (int i = 0; i < fileList.length; i++) {\r\n            System.out.println(fileList[i].toString());\r\n            BufferedReader bread = new BufferedReader(new InputStreamReader(fileSys.open(fileList[i])));\r\n            line = bread.readLine();\r\n            System.out.println(line);\r\n            line2 = bread.readLine();\r\n            System.out.println(line2);\r\n        }\r\n        assertEquals(cacheString + \"\\t\", line);\r\n        assertEquals(cacheString2 + \"\\t\", line2);\r\n    } finally {\r\n        if (dfs != null) {\r\n            dfs.shutdown();\r\n        }\r\n        if (mr != null) {\r\n            mr.shutdown();\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-streaming\\src\\test\\java\\org\\apache\\hadoop\\streaming",
  "methodName" : "main",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void main(String[] args) throws Exception\n{\r\n    new TestMultipleCachefiles().testMultipleCachefiles();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-streaming\\src\\test\\java\\org\\apache\\hadoop\\streaming",
  "methodName" : "setUp",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void setUp() throws IOException\n{\r\n    args.clear();\r\n    super.setUp();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-streaming\\src\\test\\java\\org\\apache\\hadoop\\streaming",
  "methodName" : "genArgs",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "String[] genArgs()\n{\r\n    args.add(\"-jobconf\");\r\n    args.add(MRJobConfig.MAP_OUTPUT_KEY_CLASS + \"=org.apache.hadoop.io.LongWritable\");\r\n    args.add(\"-jobconf\");\r\n    args.add(MRJobConfig.OUTPUT_KEY_CLASS + \"=org.apache.hadoop.io.LongWritable\");\r\n    args.add(\"-outputformat\");\r\n    args.add(\"org.apache.hadoop.mapred.SequenceFileOutputFormat\");\r\n    return super.genArgs();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-streaming\\src\\test\\java\\org\\apache\\hadoop\\streaming",
  "methodName" : "checkOutput",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void checkOutput() throws IOException\n{\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-streaming\\src\\test\\java\\org\\apache\\hadoop\\streaming",
  "methodName" : "testJavaMapperAndJavaReducer",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testJavaMapperAndJavaReducer() throws Exception\n{\r\n    map = \"org.apache.hadoop.mapred.lib.IdentityMapper\";\r\n    reduce = \"org.apache.hadoop.mapred.lib.IdentityReducer\";\r\n    super.testCommandLine();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-streaming\\src\\test\\java\\org\\apache\\hadoop\\streaming",
  "methodName" : "testJavaMapperAndJavaReducerAndZeroReduces",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void testJavaMapperAndJavaReducerAndZeroReduces() throws Exception\n{\r\n    map = \"org.apache.hadoop.mapred.lib.IdentityMapper\";\r\n    reduce = \"org.apache.hadoop.mapred.lib.IdentityReducer\";\r\n    args.add(\"-numReduceTasks\");\r\n    args.add(\"0\");\r\n    super.testCommandLine();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-streaming\\src\\test\\java\\org\\apache\\hadoop\\streaming",
  "methodName" : "testJavaMapperWithReduceNone",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testJavaMapperWithReduceNone() throws Exception\n{\r\n    map = \"org.apache.hadoop.mapred.lib.IdentityMapper\";\r\n    reduce = \"NONE\";\r\n    super.testCommandLine();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-streaming\\src\\test\\java\\org\\apache\\hadoop\\streaming",
  "methodName" : "testJavaMapperAndCommandReducer",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testJavaMapperAndCommandReducer() throws Exception\n{\r\n    map = \"org.apache.hadoop.mapred.lib.IdentityMapper\";\r\n    reduce = CAT;\r\n    super.testCommandLine();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-streaming\\src\\test\\java\\org\\apache\\hadoop\\streaming",
  "methodName" : "testJavaMapperAndCommandReducerAndZeroReduces",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void testJavaMapperAndCommandReducerAndZeroReduces() throws Exception\n{\r\n    map = \"org.apache.hadoop.mapred.lib.IdentityMapper\";\r\n    reduce = CAT;\r\n    args.add(\"-numReduceTasks\");\r\n    args.add(\"0\");\r\n    super.testCommandLine();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-streaming\\src\\test\\java\\org\\apache\\hadoop\\streaming",
  "methodName" : "testCommandMapperAndJavaReducer",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void testCommandMapperAndJavaReducer() throws Exception\n{\r\n    map = CAT;\r\n    reduce = MyReducer.class.getName();\r\n    super.testCommandLine();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-streaming\\src\\test\\java\\org\\apache\\hadoop\\streaming",
  "methodName" : "testCommandMapperAndJavaReducerAndZeroReduces",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void testCommandMapperAndJavaReducerAndZeroReduces() throws Exception\n{\r\n    map = CAT;\r\n    reduce = MyReducer.class.getName();\r\n    args.add(\"-numReduceTasks\");\r\n    args.add(\"0\");\r\n    super.testCommandLine();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-streaming\\src\\test\\java\\org\\apache\\hadoop\\streaming",
  "methodName" : "testCommandMapperWithReduceNone",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testCommandMapperWithReduceNone() throws Exception\n{\r\n    map = CAT;\r\n    reduce = \"NONE\";\r\n    super.testCommandLine();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-streaming\\src\\test\\java\\org\\apache\\hadoop\\streaming",
  "methodName" : "testCommandMapperAndCommandReducer",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testCommandMapperAndCommandReducer() throws Exception\n{\r\n    map = CAT;\r\n    reduce = CAT;\r\n    super.testCommandLine();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-streaming\\src\\test\\java\\org\\apache\\hadoop\\streaming",
  "methodName" : "testCommandMapperAndCommandReducerAndZeroReduces",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void testCommandMapperAndCommandReducerAndZeroReduces() throws Exception\n{\r\n    map = CAT;\r\n    reduce = CAT;\r\n    args.add(\"-numReduceTasks\");\r\n    args.add(\"0\");\r\n    super.testCommandLine();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-streaming\\src\\test\\java\\org\\apache\\hadoop\\streaming",
  "methodName" : "testDefaultToIdentityReducer",
  "errType" : null,
  "containingMethodsNum" : 9,
  "sourceCodeText" : "void testDefaultToIdentityReducer() throws Exception\n{\r\n    args.add(\"-mapper\");\r\n    args.add(map);\r\n    args.add(\"-jobconf\");\r\n    args.add(\"mapreduce.task.files.preserve.failedtasks=true\");\r\n    args.add(\"-jobconf\");\r\n    args.add(\"stream.tmpdir=\" + System.getProperty(\"test.build.data\", \"/tmp\"));\r\n    args.add(\"-inputformat\");\r\n    args.add(TextInputFormat.class.getName());\r\n    super.testCommandLine();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-streaming\\src\\test\\java\\org\\apache\\hadoop\\streaming",
  "methodName" : "testCommandLine",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void testCommandLine()\n{\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-streaming\\src\\test\\java\\org\\apache\\hadoop\\streaming",
  "methodName" : "go",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void go(boolean fail) throws IOException\n{\r\n    BufferedReader in = new BufferedReader(new InputStreamReader(System.in));\r\n    String line;\r\n    while ((line = in.readLine()) != null) {\r\n        System.out.println(line);\r\n    }\r\n    if (fail) {\r\n        throw new RuntimeException(\"Intentionally failing task\");\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-streaming\\src\\test\\java\\org\\apache\\hadoop\\streaming",
  "methodName" : "main",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void main(String[] args) throws IOException\n{\r\n    boolean fail = true;\r\n    if (args.length >= 1 && \"false\".equals(args[0])) {\r\n        fail = false;\r\n    }\r\n    FailApp app = new FailApp();\r\n    app.go(fail);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-streaming\\src\\test\\java\\org\\apache\\hadoop\\streaming",
  "methodName" : "testCommandLine",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void testCommandLine() throws Exception\n{\r\n    super.testCommandLine();\r\n    validateCounters();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-streaming\\src\\test\\java\\org\\apache\\hadoop\\streaming",
  "methodName" : "validateCounters",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void validateCounters() throws IOException\n{\r\n    Counters counters = job.running_.getCounters();\r\n    assertNotNull(\"Counters\", counters);\r\n    Group group = counters.getGroup(\"UserCounters\");\r\n    assertNotNull(\"Group\", group);\r\n    Counter counter = group.getCounterForName(\"InputLines\");\r\n    assertNotNull(\"Counter\", counter);\r\n    assertEquals(3, counter.getCounter());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-streaming\\src\\test\\java\\org\\apache\\hadoop\\streaming",
  "methodName" : "testCreateJobWithExtraArgs",
  "errType" : null,
  "containingMethodsNum" : 10,
  "sourceCodeText" : "void testCreateJobWithExtraArgs() throws IOException\n{\r\n    ArrayList<String> dummyArgs = new ArrayList<String>();\r\n    dummyArgs.add(\"-input\");\r\n    dummyArgs.add(\"dummy\");\r\n    dummyArgs.add(\"-output\");\r\n    dummyArgs.add(\"dummy\");\r\n    dummyArgs.add(\"-mapper\");\r\n    dummyArgs.add(\"dummy\");\r\n    dummyArgs.add(\"dummy\");\r\n    dummyArgs.add(\"-reducer\");\r\n    dummyArgs.add(\"dummy\");\r\n    StreamJob.createJob(dummyArgs.toArray(new String[] {}));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-streaming\\src\\test\\java\\org\\apache\\hadoop\\streaming",
  "methodName" : "testCreateJob",
  "errType" : null,
  "containingMethodsNum" : 22,
  "sourceCodeText" : "void testCreateJob() throws IOException\n{\r\n    JobConf job;\r\n    ArrayList<String> dummyArgs = new ArrayList<String>();\r\n    dummyArgs.add(\"-input\");\r\n    dummyArgs.add(\"dummy\");\r\n    dummyArgs.add(\"-output\");\r\n    dummyArgs.add(\"dummy\");\r\n    dummyArgs.add(\"-mapper\");\r\n    dummyArgs.add(\"dummy\");\r\n    dummyArgs.add(\"-reducer\");\r\n    dummyArgs.add(\"dummy\");\r\n    ArrayList<String> args;\r\n    args = new ArrayList<String>(dummyArgs);\r\n    args.add(\"-inputformat\");\r\n    args.add(\"org.apache.hadoop.mapred.KeyValueTextInputFormat\");\r\n    job = StreamJob.createJob(args.toArray(new String[] {}));\r\n    assertEquals(KeyValueTextInputFormat.class, job.getInputFormat().getClass());\r\n    args = new ArrayList<String>(dummyArgs);\r\n    args.add(\"-inputformat\");\r\n    args.add(\"org.apache.hadoop.mapred.SequenceFileInputFormat\");\r\n    job = StreamJob.createJob(args.toArray(new String[] {}));\r\n    assertEquals(SequenceFileInputFormat.class, job.getInputFormat().getClass());\r\n    args = new ArrayList<String>(dummyArgs);\r\n    args.add(\"-inputformat\");\r\n    args.add(\"org.apache.hadoop.mapred.KeyValueTextInputFormat\");\r\n    args.add(\"-inputreader\");\r\n    args.add(\"StreamXmlRecordReader,begin=<doc>,end=</doc>\");\r\n    job = StreamJob.createJob(args.toArray(new String[] {}));\r\n    assertEquals(StreamInputFormat.class, job.getInputFormat().getClass());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-streaming\\src\\test\\java\\org\\apache\\hadoop\\streaming",
  "methodName" : "testOptions",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void testOptions() throws Exception\n{\r\n    StreamJob streamJob = new StreamJob();\r\n    assertEquals(1, streamJob.run(new String[0]));\r\n    assertEquals(0, streamJob.run(new String[] { \"-help\" }));\r\n    assertEquals(0, streamJob.run(new String[] { \"-info\" }));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-streaming\\src\\test\\java\\org\\apache\\hadoop\\streaming",
  "methodName" : "genArgs",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "String[] genArgs(File input, File output, int preLines, int duringLines, int postLines)\n{\r\n    return new String[] { \"-input\", input.getAbsolutePath(), \"-output\", output.getAbsolutePath(), \"-mapper\", UtilTest.makeJavaCommand(StderrApp.class, new String[] { Integer.toString(preLines), Integer.toString(duringLines), Integer.toString(postLines) }), \"-reducer\", StreamJob.REDUCE_NONE, \"-jobconf\", \"mapreduce.task.files.preserve.failedtasks=true\", \"-jobconf\", \"mapreduce.task.timeout=5000\", \"-jobconf\", \"stream.tmpdir=\" + System.getProperty(\"test.build.data\", \"/tmp\") };\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-streaming\\src\\test\\java\\org\\apache\\hadoop\\streaming",
  "methodName" : "setupInput",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "File setupInput(String base, boolean hasInput) throws IOException\n{\r\n    File input = new File(base + \"-input.txt\");\r\n    UtilTest.recursiveDelete(input);\r\n    FileOutputStream in = new FileOutputStream(input.getAbsoluteFile());\r\n    if (hasInput) {\r\n        in.write(\"hello\\n\".getBytes());\r\n    }\r\n    in.close();\r\n    return input;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-streaming\\src\\test\\java\\org\\apache\\hadoop\\streaming",
  "methodName" : "setupOutput",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "File setupOutput(String base) throws IOException\n{\r\n    File output = new File(base + \"-out\");\r\n    UtilTest.recursiveDelete(output);\r\n    return output;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-streaming\\src\\test\\java\\org\\apache\\hadoop\\streaming",
  "methodName" : "runStreamJob",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void runStreamJob(String baseName, boolean hasInput, int preLines, int duringLines, int postLines) throws Exception\n{\r\n    File input = setupInput(baseName, hasInput);\r\n    File output = setupOutput(baseName);\r\n    boolean mayExit = false;\r\n    int returnStatus = 0;\r\n    StreamJob job = new StreamJob(genArgs(input, output, preLines, duringLines, postLines), mayExit);\r\n    returnStatus = job.go();\r\n    assertEquals(\"StreamJob success\", 0, returnStatus);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-streaming\\src\\test\\java\\org\\apache\\hadoop\\streaming",
  "methodName" : "testStderrNoInput",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testStderrNoInput() throws Exception\n{\r\n    runStreamJob(\"target/stderr-pre\", false, 10000, 0, 0);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-streaming\\src\\test\\java\\org\\apache\\hadoop\\streaming",
  "methodName" : "testStderrAfterOutput",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testStderrAfterOutput() throws Exception\n{\r\n    runStreamJob(\"target/stderr-post\", false, 0, 0, 10000);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-streaming\\src\\test\\java\\org\\apache\\hadoop\\streaming",
  "methodName" : "testStderrCountsAsProgress",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testStderrCountsAsProgress() throws Exception\n{\r\n    runStreamJob(\"target/stderr-progress\", true, 10, 1000, 0);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-streaming\\src\\test\\java\\org\\apache\\hadoop\\streaming",
  "methodName" : "go",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void go(int seconds) throws IOException, InterruptedException\n{\r\n    BufferedReader in = new BufferedReader(new InputStreamReader(System.in));\r\n    String line;\r\n    while ((line = in.readLine()) != null) {\r\n        Thread.sleep(seconds * 1000L);\r\n        System.out.println(line);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-streaming\\src\\test\\java\\org\\apache\\hadoop\\streaming",
  "methodName" : "main",
  "errType" : [ "NumberFormatException" ],
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void main(String[] args) throws IOException, InterruptedException\n{\r\n    int seconds = 5;\r\n    if (args.length >= 1) {\r\n        try {\r\n            seconds = Integer.parseInt(args[0]);\r\n        } catch (NumberFormatException e) {\r\n        }\r\n    }\r\n    DelayEchoApp app = new DelayEchoApp();\r\n    app.go(seconds);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-streaming\\src\\test\\java\\org\\apache\\hadoop\\streaming",
  "methodName" : "testFormat",
  "errType" : null,
  "containingMethodsNum" : 23,
  "sourceCodeText" : "void testFormat() throws IOException\n{\r\n    JobConf job = new JobConf(conf);\r\n    FileSystem fs = FileSystem.getLocal(conf);\r\n    Path dir = new Path(System.getProperty(\"test.build.data\", \".\") + \"/mapred\");\r\n    Path txtFile = new Path(dir, \"auto.txt\");\r\n    Path seqFile = new Path(dir, \"auto.seq\");\r\n    fs.delete(dir, true);\r\n    FileInputFormat.setInputPaths(job, dir);\r\n    Writer txtWriter = new OutputStreamWriter(fs.create(txtFile));\r\n    try {\r\n        for (int i = 0; i < LINES_COUNT; i++) {\r\n            txtWriter.write(\"\" + (10 * i));\r\n            txtWriter.write(\"\\n\");\r\n        }\r\n    } finally {\r\n        txtWriter.close();\r\n    }\r\n    SequenceFile.Writer seqWriter = SequenceFile.createWriter(fs, conf, seqFile, IntWritable.class, LongWritable.class);\r\n    try {\r\n        for (int i = 0; i < RECORDS_COUNT; i++) {\r\n            IntWritable key = new IntWritable(11 * i);\r\n            LongWritable value = new LongWritable(12 * i);\r\n            seqWriter.append(key, value);\r\n        }\r\n    } finally {\r\n        seqWriter.close();\r\n    }\r\n    AutoInputFormat format = new AutoInputFormat();\r\n    InputSplit[] splits = format.getSplits(job, SPLITS_COUNT);\r\n    for (InputSplit split : splits) {\r\n        RecordReader reader = format.getRecordReader(split, job, Reporter.NULL);\r\n        Object key = reader.createKey();\r\n        Object value = reader.createValue();\r\n        try {\r\n            while (reader.next(key, value)) {\r\n                if (key instanceof LongWritable) {\r\n                    assertEquals(\"Wrong value class.\", Text.class, value.getClass());\r\n                    assertTrue(\"Invalid value\", Integer.parseInt(((Text) value).toString()) % 10 == 0);\r\n                } else {\r\n                    assertEquals(\"Wrong key class.\", IntWritable.class, key.getClass());\r\n                    assertEquals(\"Wrong value class.\", LongWritable.class, value.getClass());\r\n                    assertTrue(\"Invalid key.\", ((IntWritable) key).get() % 11 == 0);\r\n                    assertTrue(\"Invalid value.\", ((LongWritable) value).get() % 12 == 0);\r\n                }\r\n            }\r\n        } finally {\r\n            reader.close();\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 3,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-streaming\\src\\test\\java\\org\\apache\\hadoop\\streaming",
  "methodName" : "createInput",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void createInput() throws IOException\n{\r\n    DataOutputStream out = new DataOutputStream(new FileOutputStream(INPUT_FILE.getAbsoluteFile()));\r\n    out.write(input.getBytes(\"UTF-8\"));\r\n    out.close();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-streaming\\src\\test\\java\\org\\apache\\hadoop\\streaming",
  "methodName" : "genArgs",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "String[] genArgs()\n{\r\n    return new String[] { \"-input\", INPUT_FILE.getAbsolutePath(), \"-output\", OUTPUT_DIR.getAbsolutePath(), \"-mapper\", map, \"-reducer\", \"org.apache.hadoop.mapred.lib.IdentityReducer\", \"-numReduceTasks\", \"0\", \"-jobconf\", \"mapreduce.task.files.preserve.failedtasks=true\", \"-jobconf\", \"mapreduce.job.maps=1\", \"-jobconf\", \"stream.tmpdir=\" + System.getProperty(\"test.build.data\", \"/tmp\") };\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-streaming\\src\\test\\java\\org\\apache\\hadoop\\streaming",
  "methodName" : "testCommandLine",
  "errType" : [ "Exception" ],
  "containingMethodsNum" : 11,
  "sourceCodeText" : "void testCommandLine() throws Exception\n{\r\n    String outFileName = \"part-00000\";\r\n    File outFile = null;\r\n    try {\r\n        try {\r\n            FileUtil.fullyDelete(OUTPUT_DIR.getAbsoluteFile());\r\n        } catch (Exception e) {\r\n        }\r\n        createInput();\r\n        boolean mayExit = false;\r\n        job = new StreamJob(genArgs(), mayExit);\r\n        job.go();\r\n        outFile = new File(OUTPUT_DIR, outFileName).getAbsoluteFile();\r\n        String output = StreamUtil.slurp(outFile);\r\n        System.err.println(\"outEx1=\" + outputExpect);\r\n        System.err.println(\"  out1=\" + output);\r\n        assertEquals(outputExpect, output);\r\n    } finally {\r\n        INPUT_FILE.delete();\r\n        FileUtil.fullyDelete(OUTPUT_DIR.getAbsoluteFile());\r\n    }\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 2,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-streaming\\src\\test\\java\\org\\apache\\hadoop\\streaming",
  "methodName" : "main",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void main(String[] args) throws Exception\n{\r\n    new TestStreamReduceNone().testCommandLine();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-streaming\\src\\test\\java\\org\\apache\\hadoop\\streaming",
  "methodName" : "go",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void go() throws IOException\n{\r\n    BufferedReader in = new BufferedReader(new InputStreamReader(System.in));\r\n    String line;\r\n    String prevLine = null;\r\n    while ((line = in.readLine()) != null) {\r\n        if (!line.equals(prevLine)) {\r\n            System.out.println(header + line);\r\n        }\r\n        prevLine = line;\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-streaming\\src\\test\\java\\org\\apache\\hadoop\\streaming",
  "methodName" : "main",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void main(String[] args) throws IOException\n{\r\n    String h = (args.length < 1) ? \"\" : args[0];\r\n    UniqApp app = new UniqApp(h);\r\n    app.go();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-streaming\\src\\test\\java\\org\\apache\\hadoop\\streaming",
  "methodName" : "createInput",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void createInput() throws IOException\n{\r\n    DataOutputStream out = new DataOutputStream(new FileOutputStream(INPUT_FILE.getAbsoluteFile()));\r\n    out.write(input.getBytes(\"UTF-8\"));\r\n    out.close();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-streaming\\src\\test\\java\\org\\apache\\hadoop\\streaming",
  "methodName" : "genArgs",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "String[] genArgs()\n{\r\n    return new String[] { \"-input\", INPUT_FILE.getAbsolutePath(), \"-output\", OUTPUT_DIR.getAbsolutePath(), \"-mapper\", map, \"-reducer\", reduce, \"-jobconf\", \"mapreduce.task.files.preserve.failedtasks=true\", \"-jobconf\", \"stream.tmpdir=\" + System.getProperty(\"test.build.data\", \"/tmp\"), \"-io\", \"typedbytes\" };\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-streaming\\src\\test\\java\\org\\apache\\hadoop\\streaming",
  "methodName" : "cleanupOutput",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void cleanupOutput() throws Exception\n{\r\n    FileUtil.fullyDelete(OUTPUT_DIR.getAbsoluteFile());\r\n    INPUT_FILE.delete();\r\n    createInput();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-streaming\\src\\test\\java\\org\\apache\\hadoop\\streaming",
  "methodName" : "testCommandLine",
  "errType" : null,
  "containingMethodsNum" : 10,
  "sourceCodeText" : "void testCommandLine() throws Exception\n{\r\n    StreamJob job = new StreamJob();\r\n    job.setConf(new Configuration());\r\n    job.run(genArgs());\r\n    File outFile = new File(OUTPUT_DIR, \"part-00000\").getAbsoluteFile();\r\n    String output = StreamUtil.slurp(outFile);\r\n    outFile.delete();\r\n    System.out.println(\"   map=\" + map);\r\n    System.out.println(\"reduce=\" + reduce);\r\n    System.err.println(\"outEx1=\" + outputExpect);\r\n    System.err.println(\"  out1=\" + output);\r\n    assertEquals(outputExpect, output);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-streaming\\src\\test\\java\\org\\apache\\hadoop\\streaming",
  "methodName" : "createInput",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void createInput() throws IOException\n{\r\n    DataOutputStream out = new DataOutputStream(new FileOutputStream(INPUT_FILE.getAbsoluteFile()));\r\n    out.write(input.getBytes(\"UTF-8\"));\r\n    out.close();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-streaming\\src\\test\\java\\org\\apache\\hadoop\\streaming",
  "methodName" : "genArgs",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "String[] genArgs()\n{\r\n    return new String[] { \"-input\", INPUT_FILE.getAbsolutePath(), \"-output\", OUTPUT_DIR.getAbsolutePath(), \"-mapper\", map, \"-reducer\", reduce, \"-jobconf\", \"mapreduce.task.files.preserve.failedtasks=true\", \"-jobconf\", \"stream.tmpdir=\" + System.getProperty(\"test.build.data\", \"/tmp\"), \"-inputformat\", \"KeyValueTextInputFormat\", \"-jobconf\", \"mapreduce.input.keyvaluelinerecordreader.key.value.separator=1\", \"-jobconf\", \"stream.map.input.field.separator=2\", \"-jobconf\", \"stream.map.output.field.separator=3\", \"-jobconf\", \"stream.reduce.input.field.separator=3\", \"-jobconf\", \"stream.reduce.output.field.separator=4\", \"-jobconf\", \"mapreduce.output.textoutputformat.separator=5\" };\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-streaming\\src\\test\\java\\org\\apache\\hadoop\\streaming",
  "methodName" : "testCommandLine",
  "errType" : [ "Exception" ],
  "containingMethodsNum" : 12,
  "sourceCodeText" : "void testCommandLine() throws Exception\n{\r\n    try {\r\n        try {\r\n            FileUtil.fullyDelete(OUTPUT_DIR.getAbsoluteFile());\r\n        } catch (Exception e) {\r\n        }\r\n        createInput();\r\n        boolean mayExit = false;\r\n        job = new StreamJob(genArgs(), mayExit);\r\n        job.go();\r\n        File outFile = new File(OUTPUT_DIR, \"part-00000\").getAbsoluteFile();\r\n        String output = StreamUtil.slurp(outFile);\r\n        outFile.delete();\r\n        System.err.println(\"outEx1=\" + outputExpect);\r\n        System.err.println(\"  out1=\" + output);\r\n        assertEquals(outputExpect, output);\r\n    } finally {\r\n        INPUT_FILE.delete();\r\n        FileUtil.fullyDelete(OUTPUT_DIR.getAbsoluteFile());\r\n    }\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 2,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-streaming\\src\\test\\java\\org\\apache\\hadoop\\streaming",
  "methodName" : "main",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void main(String[] args) throws Exception\n{\r\n    new TestStreamingSeparator().testCommandLine();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-streaming\\src\\test\\java\\org\\apache\\hadoop\\streaming",
  "methodName" : "setUp",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void setUp() throws IOException\n{\r\n    UtilTest.recursiveDelete(TEST_DIR);\r\n    assertTrue(TEST_DIR.mkdirs());\r\n    FileOutputStream out = new FileOutputStream(INPUT_FILE.getAbsoluteFile());\r\n    out.write(\"hello\\n\".getBytes());\r\n    out.close();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-streaming\\src\\test\\java\\org\\apache\\hadoop\\streaming",
  "methodName" : "runStreamJob",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void runStreamJob() throws Exception\n{\r\n    boolean mayExit = false;\r\n    int returnStatus = 0;\r\n    StreamJob job = new StreamJob(args, mayExit);\r\n    returnStatus = job.go();\r\n    assertEquals(\"Streaming Job expected to succeed\", 0, returnStatus);\r\n    job.running_.killJob();\r\n    job.running_.waitForCompletion();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-streaming\\src\\test\\java\\org\\apache\\hadoop\\streaming",
  "methodName" : "testBackgroundSubmitOk",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testBackgroundSubmitOk() throws Exception\n{\r\n    runStreamJob();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-streaming\\src\\test\\java\\org\\apache\\hadoop\\streaming",
  "methodName" : "go",
  "errType" : null,
  "containingMethodsNum" : 9,
  "sourceCodeText" : "void go() throws IOException\n{\r\n    TypedBytesInput tbinput = new TypedBytesInput(new DataInputStream(System.in));\r\n    TypedBytesOutput tboutput = new TypedBytesOutput(new DataOutputStream(System.out));\r\n    Object prevKey = null;\r\n    int sum = 0;\r\n    Object key = tbinput.read();\r\n    while (key != null) {\r\n        if (prevKey != null && !key.equals(prevKey)) {\r\n            tboutput.write(prevKey);\r\n            tboutput.write(sum);\r\n            sum = 0;\r\n        }\r\n        sum += (Integer) tbinput.read();\r\n        prevKey = key;\r\n        key = tbinput.read();\r\n    }\r\n    tboutput.write(prevKey);\r\n    tboutput.write(sum);\r\n    System.out.flush();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-streaming\\src\\test\\java\\org\\apache\\hadoop\\streaming",
  "methodName" : "main",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void main(String[] args) throws IOException\n{\r\n    TypedBytesReduceApp app = new TypedBytesReduceApp();\r\n    app.go();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-streaming\\src\\test\\java\\org\\apache\\hadoop\\streaming",
  "methodName" : "go",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void go() throws IOException\n{\r\n    BufferedReader in = new BufferedReader(new InputStreamReader(System.in));\r\n    String line;\r\n    while ((line = in.readLine()) != null) {\r\n        for (String part : line.split(find)) {\r\n            writeString(part);\r\n            writeInt(1);\r\n        }\r\n    }\r\n    System.out.flush();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-streaming\\src\\test\\java\\org\\apache\\hadoop\\streaming",
  "methodName" : "main",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void main(String[] args) throws IOException\n{\r\n    RawBytesMapApp app = new RawBytesMapApp(args[0].replace(\".\", \"\\\\.\"));\r\n    app.go();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-streaming\\src\\test\\java\\org\\apache\\hadoop\\streaming",
  "methodName" : "writeString",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void writeString(String str) throws IOException\n{\r\n    byte[] bytes = str.getBytes(\"UTF-8\");\r\n    dos.writeInt(bytes.length);\r\n    dos.write(bytes);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-streaming\\src\\test\\java\\org\\apache\\hadoop\\streaming",
  "methodName" : "writeInt",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void writeInt(int i) throws IOException\n{\r\n    dos.writeInt(4);\r\n    IntWritable iw = new IntWritable(i);\r\n    iw.write(dos);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-streaming\\src\\test\\java\\org\\apache\\hadoop\\streaming",
  "methodName" : "setUp",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void setUp() throws IOException\n{\r\n    conf = new JobConf();\r\n    conf.setBoolean(JTConfig.JT_RETIREJOBS, false);\r\n    conf.setBoolean(JTConfig.JT_PERSIST_JOBSTATUS, false);\r\n    mr = MiniMRClientClusterFactory.create(this.getClass(), 3, conf);\r\n    Path inFile = new Path(INPUT_FILE);\r\n    fs = inFile.getFileSystem(mr.getConfig());\r\n    clean(fs);\r\n    buildExpectedJobOutput();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-streaming\\src\\test\\java\\org\\apache\\hadoop\\streaming",
  "methodName" : "tearDown",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void tearDown() throws IOException\n{\r\n    if (fs != null) {\r\n        clean(fs);\r\n    }\r\n    if (mr != null) {\r\n        mr.stop();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-streaming\\src\\test\\java\\org\\apache\\hadoop\\streaming",
  "methodName" : "buildExpectedJobOutput",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void buildExpectedJobOutput()\n{\r\n    if (expectedOutput == null) {\r\n        expectedOutput = \"\";\r\n        for (int i = 1500; i >= 1; i--) {\r\n            expectedOutput = expectedOutput.concat(Integer.toString(i) + \" \");\r\n        }\r\n        expectedOutput = expectedOutput.trim();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-streaming\\src\\test\\java\\org\\apache\\hadoop\\streaming",
  "methodName" : "createInputAndScript",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void createInputAndScript(boolean isEmptyInput, String script) throws IOException\n{\r\n    makeInput(fs, isEmptyInput ? \"\" : input);\r\n    DataOutputStream file = fs.create(new Path(scriptFileName));\r\n    file.writeBytes(script);\r\n    file.close();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-streaming\\src\\test\\java\\org\\apache\\hadoop\\streaming",
  "methodName" : "genArgs",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String[] genArgs(String jobtracker, String rmAddress, String mapper, String reducer)\n{\r\n    return new String[] { \"-input\", INPUT_FILE, \"-output\", OUTPUT_DIR, \"-mapper\", mapper, \"-reducer\", reducer, \"-jobconf\", MRJobConfig.NUM_MAPS + \"=1\", \"-jobconf\", MRJobConfig.NUM_REDUCES + \"=1\", \"-jobconf\", MRJobConfig.PRESERVE_FAILED_TASK_FILES + \"=true\", \"-jobconf\", YarnConfiguration.RM_ADDRESS + \"=\" + rmAddress, \"-jobconf\", \"stream.tmpdir=\" + new Path(TEST_ROOT_DIR).toUri().getPath(), \"-jobconf\", JTConfig.JT_IPC_ADDRESS + \"=\" + jobtracker, \"-jobconf\", \"fs.default.name=file:///\", \"-jobconf\", \"mapred.jar=\" + TestStreaming.STREAMING_JAR, \"-jobconf\", \"mapreduce.framework.name=yarn\" };\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-streaming\\src\\test\\java\\org\\apache\\hadoop\\streaming",
  "methodName" : "makeInput",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void makeInput(FileSystem fs, String input) throws IOException\n{\r\n    Path inFile = new Path(INPUT_FILE);\r\n    DataOutputStream file = fs.create(inFile);\r\n    file.writeBytes(input);\r\n    file.close();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-streaming\\src\\test\\java\\org\\apache\\hadoop\\streaming",
  "methodName" : "deleteOutDir",
  "errType" : [ "Exception" ],
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void deleteOutDir(FileSystem fs)\n{\r\n    try {\r\n        Path outDir = new Path(OUTPUT_DIR);\r\n        fs.delete(outDir, true);\r\n    } catch (Exception e) {\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-streaming\\src\\test\\java\\org\\apache\\hadoop\\streaming",
  "methodName" : "clean",
  "errType" : [ "Exception" ],
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void clean(FileSystem fs)\n{\r\n    deleteOutDir(fs);\r\n    try {\r\n        Path file = new Path(INPUT_FILE);\r\n        if (fs.exists(file)) {\r\n            fs.delete(file, false);\r\n        }\r\n        file = new Path(scriptFile);\r\n        if (fs.exists(file)) {\r\n            fs.delete(file, false);\r\n        }\r\n    } catch (Exception e) {\r\n        e.printStackTrace();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-streaming\\src\\test\\java\\org\\apache\\hadoop\\streaming",
  "methodName" : "testReporting",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void testReporting() throws Exception\n{\r\n    testStreamJob(false);\r\n    testStreamJob(true);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-streaming\\src\\test\\java\\org\\apache\\hadoop\\streaming",
  "methodName" : "testStreamJob",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void testStreamJob(boolean isEmptyInput) throws Exception\n{\r\n    createInputAndScript(isEmptyInput, script);\r\n    map = scriptFileName;\r\n    reduce = \"/bin/cat\";\r\n    runStreamJob(TaskType.MAP, isEmptyInput);\r\n    deleteOutDir(fs);\r\n    map = \"/bin/cat\";\r\n    reduce = scriptFileName;\r\n    runStreamJob(TaskType.REDUCE, isEmptyInput);\r\n    clean(fs);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-streaming\\src\\test\\java\\org\\apache\\hadoop\\streaming",
  "methodName" : "runStreamJob",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void runStreamJob(TaskType type, boolean isEmptyInput) throws Exception\n{\r\n    StreamJob job = new StreamJob();\r\n    int returnValue = job.run(genArgs(mr.getConfig().get(JTConfig.JT_IPC_ADDRESS), mr.getConfig().get(YarnConfiguration.RM_ADDRESS), map, reduce));\r\n    assertEquals(0, returnValue);\r\n    int expectedCounterValue = 0;\r\n    if (type == TaskType.MAP || !isEmptyInput) {\r\n        validateTaskStatus(job, type);\r\n        validateJobOutput(job.getConf());\r\n        expectedCounterValue = 2;\r\n    }\r\n    validateUserCounter(job, expectedCounterValue);\r\n    validateTaskStderr(job, type);\r\n    deleteOutDir(fs);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-streaming\\src\\test\\java\\org\\apache\\hadoop\\streaming",
  "methodName" : "validateTaskStatus",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void validateTaskStatus(StreamJob job, TaskType type) throws IOException\n{\r\n    String finalPhaseInTask;\r\n    TaskReport[] reports;\r\n    if (type == TaskType.MAP) {\r\n        reports = job.jc_.getMapTaskReports(job.jobId_);\r\n        finalPhaseInTask = \"sort\";\r\n    } else {\r\n        reports = job.jc_.getReduceTaskReports(job.jobId_);\r\n        finalPhaseInTask = \"reduce\";\r\n    }\r\n    assertEquals(1, reports.length);\r\n    assertEquals(expectedStatus + \" > \" + finalPhaseInTask, reports[0].getState());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-streaming\\src\\test\\java\\org\\apache\\hadoop\\streaming",
  "methodName" : "validateJobOutput",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void validateJobOutput(Configuration conf) throws IOException\n{\r\n    String output = MapReduceTestUtil.readOutput(new Path(OUTPUT_DIR), conf).trim();\r\n    assertTrue(output.equals(expectedOutput));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-streaming\\src\\test\\java\\org\\apache\\hadoop\\streaming",
  "methodName" : "validateTaskStderr",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void validateTaskStderr(StreamJob job, TaskType type) throws IOException\n{\r\n    TaskAttemptID attemptId = new TaskAttemptID(new TaskID(job.jobId_, type, 0), 0);\r\n    String log = MapReduceTestUtil.readTaskLog(TaskLog.LogName.STDERR, attemptId, false);\r\n    assertTrue(log.equals(expectedStderr.trim()));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-streaming\\src\\test\\java\\org\\apache\\hadoop\\streaming",
  "methodName" : "validateUserCounter",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void validateUserCounter(StreamJob job, int expectedCounterValue) throws IOException\n{\r\n    Counters counters = job.running_.getCounters();\r\n    assertEquals(expectedCounterValue, counters.findCounter(\"myOwnCounterGroup\", \"myOwnCounter\").getValue());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-streaming\\src\\test\\java\\org\\apache\\hadoop\\streaming",
  "methodName" : "recursiveDelete",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void recursiveDelete(File file)\n{\r\n    file = file.getAbsoluteFile();\r\n    if (!file.exists())\r\n        return;\r\n    if (file.isDirectory()) {\r\n        for (File child : file.listFiles()) {\r\n            recursiveDelete(child);\r\n        }\r\n    }\r\n    if (!file.delete()) {\r\n        throw new RuntimeException(\"Failed to delete \" + file);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-streaming\\src\\test\\java\\org\\apache\\hadoop\\streaming",
  "methodName" : "checkUserDir",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void checkUserDir()\n{\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-streaming\\src\\test\\java\\org\\apache\\hadoop\\streaming",
  "methodName" : "redirectIfAntJunit",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void redirectIfAntJunit() throws IOException\n{\r\n    boolean fromAntJunit = System.getProperty(\"test.build.data\") != null;\r\n    if (fromAntJunit) {\r\n        new File(antTestDir_).mkdirs();\r\n        File outFile = new File(antTestDir_, testName_ + \".log\");\r\n        PrintStream out = new PrintStream(new FileOutputStream(outFile));\r\n        System.setOut(out);\r\n        System.setErr(out);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-streaming\\src\\test\\java\\org\\apache\\hadoop\\streaming",
  "methodName" : "collate",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "String collate(List<String> args, String sep)\n{\r\n    StringBuffer buf = new StringBuffer();\r\n    Iterator<String> it = args.iterator();\r\n    while (it.hasNext()) {\r\n        if (buf.length() > 0) {\r\n            buf.append(\" \");\r\n        }\r\n        buf.append(it.next());\r\n    }\r\n    return buf.toString();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-streaming\\src\\test\\java\\org\\apache\\hadoop\\streaming",
  "methodName" : "makeJavaCommand",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "String makeJavaCommand(Class<?> main, String[] argv)\n{\r\n    ArrayList<String> vargs = new ArrayList<String>();\r\n    File javaHomeBin = new File(System.getProperty(\"java.home\"), \"bin\");\r\n    File jvm = new File(javaHomeBin, \"java\");\r\n    vargs.add(jvm.toString());\r\n    vargs.add(\"-classpath\");\r\n    vargs.add(\"\\\"\" + System.getProperty(\"java.class.path\") + \"\\\"\");\r\n    vargs.add(\"-Xmx\" + Runtime.getRuntime().maxMemory());\r\n    vargs.add(main.getName());\r\n    for (int i = 0; i < argv.length; i++) {\r\n        vargs.add(argv[i]);\r\n    }\r\n    return collate(vargs, \" \");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-streaming\\src\\test\\java\\org\\apache\\hadoop\\streaming",
  "methodName" : "hasPerlSupport",
  "errType" : [ "Exception" ],
  "containingMethodsNum" : 4,
  "sourceCodeText" : "boolean hasPerlSupport()\n{\r\n    boolean hasPerl = false;\r\n    ShellCommandExecutor shexec = new ShellCommandExecutor(new String[] { \"perl\", \"-e\", \"print 42\" });\r\n    try {\r\n        shexec.execute();\r\n        if (shexec.getOutput().equals(\"42\")) {\r\n            hasPerl = true;\r\n        } else {\r\n            LOG.warn(\"Perl is installed, but isn't behaving as expected.\");\r\n        }\r\n    } catch (Exception e) {\r\n        LOG.warn(\"Could not run perl: \" + e);\r\n    }\r\n    return hasPerl;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-streaming\\src\\test\\java\\org\\apache\\hadoop\\streaming",
  "methodName" : "testLoading",
  "errType" : [ "Exception" ],
  "containingMethodsNum" : 19,
  "sourceCodeText" : "void testLoading() throws Exception\n{\r\n    Configuration conf = new Configuration();\r\n    MiniDFSCluster cluster = new MiniDFSCluster.Builder(conf).numDataNodes(2).build();\r\n    FileSystem fs = cluster.getFileSystem();\r\n    ByteArrayOutputStream out = new ByteArrayOutputStream();\r\n    TypedBytesOutput tboutput = new TypedBytesOutput(new DataOutputStream(out));\r\n    for (int i = 0; i < 100; i++) {\r\n        tboutput.write(new Long(i));\r\n        tboutput.write(\"\" + (10 * i));\r\n    }\r\n    InputStream isBackup = System.in;\r\n    ByteArrayInputStream in = new ByteArrayInputStream(out.toByteArray());\r\n    System.setIn(in);\r\n    LoadTypedBytes loadtb = new LoadTypedBytes(conf);\r\n    try {\r\n        Path root = new Path(\"/typedbytestest\");\r\n        assertTrue(fs.mkdirs(root));\r\n        assertTrue(fs.exists(root));\r\n        String[] args = new String[1];\r\n        args[0] = \"/typedbytestest/test.seq\";\r\n        int ret = loadtb.run(args);\r\n        assertEquals(\"Return value != 0.\", 0, ret);\r\n        Path file = new Path(root, \"test.seq\");\r\n        assertTrue(fs.exists(file));\r\n        SequenceFile.Reader reader = new SequenceFile.Reader(fs, file, conf);\r\n        int counter = 0;\r\n        TypedBytesWritable key = new TypedBytesWritable();\r\n        TypedBytesWritable value = new TypedBytesWritable();\r\n        while (reader.next(key, value)) {\r\n            assertEquals(Long.class, key.getValue().getClass());\r\n            assertEquals(String.class, value.getValue().getClass());\r\n            assertTrue(\"Invalid record.\", Integer.parseInt(value.toString()) % 10 == 0);\r\n            counter++;\r\n        }\r\n        assertEquals(\"Wrong number of records.\", 100, counter);\r\n    } finally {\r\n        try {\r\n            fs.close();\r\n        } catch (Exception e) {\r\n        }\r\n        System.setIn(isBackup);\r\n        cluster.shutdown();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 2,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-streaming\\src\\test\\java\\org\\apache\\hadoop\\streaming",
  "methodName" : "createInput",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void createInput() throws IOException\n{\r\n    DataOutputStream out = new DataOutputStream(new FileOutputStream(INPUT_FILE.getAbsoluteFile()));\r\n    out.write(input.getBytes(\"UTF-8\"));\r\n    out.close();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-streaming\\src\\test\\java\\org\\apache\\hadoop\\streaming",
  "methodName" : "genArgs",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "String[] genArgs()\n{\r\n    return new String[] { \"-input\", INPUT_FILE.getAbsolutePath(), \"-output\", OUTPUT_DIR.getAbsolutePath(), \"-mapper\", map, \"-reducer\", \"aggregate\", \"-jobconf\", MRJobConfig.PRESERVE_FAILED_TASK_FILES + \"=true\", \"-jobconf\", \"stream.tmpdir=\" + System.getProperty(\"test.build.data\", \"/tmp\") };\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-streaming\\src\\test\\java\\org\\apache\\hadoop\\streaming",
  "methodName" : "testCommandLine",
  "errType" : [ "Exception" ],
  "containingMethodsNum" : 12,
  "sourceCodeText" : "void testCommandLine() throws Exception\n{\r\n    try {\r\n        try {\r\n            FileUtil.fullyDelete(OUTPUT_DIR.getAbsoluteFile());\r\n        } catch (Exception e) {\r\n        }\r\n        createInput();\r\n        boolean mayExit = false;\r\n        job = new StreamJob(genArgs(), mayExit);\r\n        job.go();\r\n        File outFile = new File(OUTPUT_DIR, \"part-00000\").getAbsoluteFile();\r\n        String output = StreamUtil.slurp(outFile);\r\n        outFile.delete();\r\n        System.err.println(\"outEx1=\" + outputExpect);\r\n        System.err.println(\"  out1=\" + output);\r\n        assertEquals(outputExpect, output);\r\n    } finally {\r\n        INPUT_FILE.delete();\r\n        FileUtil.fullyDelete(OUTPUT_DIR.getAbsoluteFile());\r\n    }\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 2,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-streaming\\src\\test\\java\\org\\apache\\hadoop\\streaming",
  "methodName" : "main",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void main(String[] args) throws Exception\n{\r\n    new TestStreaming().testCommandLine();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-streaming\\src\\test\\java\\org\\apache\\hadoop\\streaming",
  "methodName" : "setupClass",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setupClass() throws Exception\n{\r\n    setupClassBase(TestStreamingBadRecords.class);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-streaming\\src\\test\\java\\org\\apache\\hadoop\\streaming",
  "methodName" : "setUp",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void setUp() throws Exception\n{\r\n    Properties props = new Properties();\r\n    props.setProperty(JTConfig.JT_RETIREJOBS, \"false\");\r\n    props.setProperty(JTConfig.JT_PERSIST_JOBSTATUS, \"false\");\r\n    startCluster(true, props);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-streaming\\src\\test\\java\\org\\apache\\hadoop\\streaming",
  "methodName" : "createInput",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void createInput() throws Exception\n{\r\n    OutputStream os = getFileSystem().create(new Path(getInputDir(), \"text.txt\"));\r\n    Writer wr = new OutputStreamWriter(os);\r\n    String prefix = new String(new byte[20 * 1024]);\r\n    for (int i = 1; i <= INPUTSIZE; i++) {\r\n        String str = \"\" + i;\r\n        int zerosToPrepend = 3 - str.length();\r\n        for (int j = 0; j < zerosToPrepend; j++) {\r\n            str = \"0\" + str;\r\n        }\r\n        wr.write(prefix + \"hey\" + str + \"\\n\");\r\n    }\r\n    wr.close();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-streaming\\src\\test\\java\\org\\apache\\hadoop\\streaming",
  "methodName" : "validateOutput",
  "errType" : null,
  "containingMethodsNum" : 26,
  "sourceCodeText" : "void validateOutput(RunningJob runningJob, boolean validateCount) throws Exception\n{\r\n    LOG.info(runningJob.getCounters().toString());\r\n    assertTrue(runningJob.isSuccessful());\r\n    if (validateCount) {\r\n        String counterGrp = \"org.apache.hadoop.mapred.Task$Counter\";\r\n        Counters counters = runningJob.getCounters();\r\n        assertEquals(counters.findCounter(counterGrp, \"MAP_SKIPPED_RECORDS\").getCounter(), MAPPER_BAD_RECORDS.size());\r\n        int mapRecs = INPUTSIZE - MAPPER_BAD_RECORDS.size();\r\n        assertEquals(counters.findCounter(counterGrp, \"MAP_INPUT_RECORDS\").getCounter(), mapRecs);\r\n        assertEquals(counters.findCounter(counterGrp, \"MAP_OUTPUT_RECORDS\").getCounter(), mapRecs);\r\n        int redRecs = mapRecs - REDUCER_BAD_RECORDS.size();\r\n        assertEquals(counters.findCounter(counterGrp, \"REDUCE_SKIPPED_RECORDS\").getCounter(), REDUCER_BAD_RECORDS.size());\r\n        assertEquals(counters.findCounter(counterGrp, \"REDUCE_SKIPPED_GROUPS\").getCounter(), REDUCER_BAD_RECORDS.size());\r\n        assertEquals(counters.findCounter(counterGrp, \"REDUCE_INPUT_GROUPS\").getCounter(), redRecs);\r\n        assertEquals(counters.findCounter(counterGrp, \"REDUCE_INPUT_RECORDS\").getCounter(), redRecs);\r\n        assertEquals(counters.findCounter(counterGrp, \"REDUCE_OUTPUT_RECORDS\").getCounter(), redRecs);\r\n    }\r\n    List<String> badRecs = new ArrayList<String>();\r\n    badRecs.addAll(MAPPER_BAD_RECORDS);\r\n    badRecs.addAll(REDUCER_BAD_RECORDS);\r\n    Path[] outputFiles = FileUtil.stat2Paths(getFileSystem().listStatus(getOutputDir(), new Utils.OutputFileUtils.OutputFilesFilter()));\r\n    if (outputFiles.length > 0) {\r\n        InputStream is = getFileSystem().open(outputFiles[0]);\r\n        BufferedReader reader = new BufferedReader(new InputStreamReader(is));\r\n        String line = reader.readLine();\r\n        int counter = 0;\r\n        while (line != null) {\r\n            counter++;\r\n            StringTokenizer tokeniz = new StringTokenizer(line, \"\\t\");\r\n            String value = tokeniz.nextToken();\r\n            int index = value.indexOf(\"hey\");\r\n            assertTrue(index > -1);\r\n            if (index > -1) {\r\n                String heyStr = value.substring(index);\r\n                assertTrue(!badRecs.contains(heyStr));\r\n            }\r\n            line = reader.readLine();\r\n        }\r\n        reader.close();\r\n        if (validateCount) {\r\n            assertEquals(INPUTSIZE - badRecs.size(), counter);\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-streaming\\src\\test\\java\\org\\apache\\hadoop\\streaming",
  "methodName" : "testNoOp",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void testNoOp()\n{\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-streaming\\src\\test\\java\\org\\apache\\hadoop\\streaming",
  "methodName" : "createInput",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void createInput() throws IOException\n{\r\n    DataOutputStream out = new DataOutputStream(new FileOutputStream(INPUT_FILE.getAbsoluteFile()));\r\n    out.write(input.getBytes(\"UTF-8\"));\r\n    out.close();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-streaming\\src\\test\\java\\org\\apache\\hadoop\\streaming",
  "methodName" : "genArgs",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "String[] genArgs()\n{\r\n    return new String[] { \"-input\", INPUT_FILE.getAbsolutePath(), \"-output\", OUTPUT_DIR.getAbsolutePath(), \"-mapper\", map, \"-reducer\", reduce, \"-partitioner\", KeyFieldBasedPartitioner.class.getCanonicalName(), \"-jobconf\", \"stream.map.output.field.separator=.\", \"-jobconf\", \"stream.num.map.output.key.fields=2\", \"-jobconf\", \"mapreduce.map.output.key.field.separator=.\", \"-jobconf\", \"num.key.fields.for.partition=1\", \"-jobconf\", \"mapreduce.job.reduces=2\", \"-jobconf\", \"mapreduce.task.files.preserve.failedtasks=true\", \"-jobconf\", \"stream.tmpdir=\" + System.getProperty(\"test.build.data\", \"/tmp\") };\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-streaming\\src\\test\\java\\org\\apache\\hadoop\\streaming",
  "methodName" : "testCommandLine",
  "errType" : [ "Exception" ],
  "containingMethodsNum" : 13,
  "sourceCodeText" : "void testCommandLine() throws Exception\n{\r\n    try {\r\n        FileUtil.fullyDelete(OUTPUT_DIR.getAbsoluteFile());\r\n    } catch (Exception e) {\r\n    }\r\n    try {\r\n        createInput();\r\n        boolean mayExit = false;\r\n        job = new StreamJob(genArgs(), mayExit);\r\n        job.go();\r\n        File outFile = new File(OUTPUT_DIR, \"part-00000\").getAbsoluteFile();\r\n        String output = StreamUtil.slurp(outFile);\r\n        outFile.delete();\r\n        System.err.println(\"outEx1=\" + outputExpect);\r\n        System.err.println(\"  out1=\" + output);\r\n        System.err.println(\"  equals=\" + outputExpect.compareTo(output));\r\n        assertEquals(outputExpect, output);\r\n    } finally {\r\n        INPUT_FILE.delete();\r\n        FileUtil.fullyDelete(OUTPUT_DIR.getAbsoluteFile());\r\n    }\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 2,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-streaming\\src\\test\\java\\org\\apache\\hadoop\\streaming",
  "methodName" : "main",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void main(String[] args) throws Exception\n{\r\n    new TestStreamDataProtocol().testCommandLine();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-streaming\\src\\test\\java\\org\\apache\\hadoop\\streaming",
  "methodName" : "testOutputOnlyKeys",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void testOutputOnlyKeys() throws Exception\n{\r\n    args.add(\"-jobconf\");\r\n    args.add(\"stream.reduce.input\" + \"=keyonlytext\");\r\n    args.add(\"-jobconf\");\r\n    args.add(\"stream.reduce.output\" + \"=keyonlytext\");\r\n    super.testCommandLine();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-streaming\\src\\test\\java\\org\\apache\\hadoop\\streaming",
  "methodName" : "getExpectedOutput",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String getExpectedOutput()\n{\r\n    return outputExpect.replaceAll(\"\\t\", \"\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-streaming\\src\\test\\java\\org\\apache\\hadoop\\streaming",
  "methodName" : "testCommandLine",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void testCommandLine()\n{\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-streaming\\src\\test\\java\\org\\apache\\hadoop\\streaming",
  "methodName" : "go",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void go() throws IOException\n{\r\n    TypedBytesInput tbinput = new TypedBytesInput(new DataInputStream(System.in));\r\n    TypedBytesOutput tboutput = new TypedBytesOutput(new DataOutputStream(System.out));\r\n    Object key = tbinput.readRaw();\r\n    while (key != null) {\r\n        Object value = tbinput.read();\r\n        for (String part : value.toString().split(find)) {\r\n            tboutput.write(part);\r\n            tboutput.write(1);\r\n        }\r\n        System.err.println(\"reporter:counter:UserCounters,InputLines,1\");\r\n        key = tbinput.readRaw();\r\n    }\r\n    System.out.flush();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-streaming\\src\\test\\java\\org\\apache\\hadoop\\streaming",
  "methodName" : "main",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void main(String[] args) throws IOException\n{\r\n    TypedBytesMapApp app = new TypedBytesMapApp(args[0].replace(\".\", \"\\\\.\"));\r\n    app.go();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-streaming\\src\\test\\java\\org\\apache\\hadoop\\streaming\\mapreduce",
  "methodName" : "assertOutput",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void assertOutput(String expectedOutput, String output) throws IOException\n{\r\n    String[] words = expectedOutput.split(\"\\t\\n\");\r\n    Set<String> expectedWords = new HashSet<String>(Arrays.asList(words));\r\n    words = output.split(\"\\t\\n\");\r\n    Set<String> returnedWords = new HashSet<String>(Arrays.asList(words));\r\n    assertTrue(returnedWords.containsAll(expectedWords));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-streaming\\src\\test\\java\\org\\apache\\hadoop\\streaming\\mapreduce",
  "methodName" : "checkOutput",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void checkOutput() throws IOException\n{\r\n    File outFile = new File(OUTPUT_DIR.toString());\r\n    Path outPath = new Path(outFile.getAbsolutePath(), \"part-r-00000\");\r\n    String output = slurpHadoop(outPath, fs);\r\n    fs.delete(outPath, true);\r\n    outputExpect = \"<PATTERN>\\n\" + outputExpect + \"</PATTERN>\";\r\n    System.err.println(\"outEx1=\" + outputExpect);\r\n    System.err.println(\"  out1=\" + output);\r\n    assertOutput(outputExpect, output);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-streaming\\src\\test\\java\\org\\apache\\hadoop\\streaming\\mapreduce",
  "methodName" : "slurpHadoop",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "String slurpHadoop(Path p, FileSystem fs) throws IOException\n{\r\n    int len = (int) fs.getFileStatus(p).getLen();\r\n    byte[] buf = new byte[len];\r\n    FSDataInputStream in = fs.open(p);\r\n    String contents = null;\r\n    try {\r\n        in.readFully(in.getPos(), buf);\r\n        contents = new String(buf, \"UTF-8\");\r\n    } finally {\r\n        in.close();\r\n    }\r\n    return contents;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-streaming\\src\\test\\java\\org\\apache\\hadoop\\streaming\\mapreduce",
  "methodName" : "createInput",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void createInput() throws IOException\n{\r\n    FileOutputStream out = new FileOutputStream(INPUT_FILE.getAbsoluteFile());\r\n    String dummyXmlStartTag = \"<PATTERN>\\n\";\r\n    String dummyXmlEndTag = \"</PATTERN>\\n\";\r\n    out.write(dummyXmlStartTag.getBytes(\"UTF-8\"));\r\n    out.write(input.getBytes(\"UTF-8\"));\r\n    out.write(dummyXmlEndTag.getBytes(\"UTF-8\"));\r\n    out.close();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-streaming\\src\\test\\java\\org\\apache\\hadoop\\streaming\\mapreduce",
  "methodName" : "testStreamXmlRecordReader",
  "errType" : null,
  "containingMethodsNum" : 20,
  "sourceCodeText" : "void testStreamXmlRecordReader() throws Exception\n{\r\n    Job job = Job.getInstance();\r\n    Configuration conf = job.getConfiguration();\r\n    job.setJarByClass(TestStreamXmlRecordReader.class);\r\n    job.setMapperClass(Mapper.class);\r\n    conf.set(\"stream.recordreader.class\", \"org.apache.hadoop.streaming.mapreduce.StreamXmlRecordReader\");\r\n    conf.set(\"stream.recordreader.begin\", \"<PATTERN>\");\r\n    conf.set(\"stream.recordreader.end\", \"</PATTERN>\");\r\n    job.setInputFormatClass(StreamInputFormat.class);\r\n    job.setMapOutputKeyClass(Text.class);\r\n    job.setMapOutputValueClass(Text.class);\r\n    job.setOutputKeyClass(Text.class);\r\n    job.setOutputValueClass(Text.class);\r\n    FileInputFormat.addInputPath(job, new Path(\"target/input.xml\"));\r\n    OUTPUT_DIR = new Path(\"target/output\");\r\n    fs = FileSystem.get(conf);\r\n    if (fs.exists(OUTPUT_DIR)) {\r\n        fs.delete(OUTPUT_DIR, true);\r\n    }\r\n    FileOutputFormat.setOutputPath(job, OUTPUT_DIR);\r\n    boolean ret = job.waitForCompletion(true);\r\n    assertEquals(true, ret);\r\n    checkOutput();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-streaming\\src\\test\\java\\org\\apache\\hadoop\\streaming\\mapreduce",
  "methodName" : "tearDown",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void tearDown() throws IOException\n{\r\n    fs.delete(OUTPUT_DIR, true);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-streaming\\src\\test\\java\\org\\apache\\hadoop\\streaming",
  "methodName" : "setInputOutput",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void setInputOutput()\n{\r\n    inputFile = INVALID_INPUT_FILE.getAbsolutePath();\r\n    outDir = OUTPUT_DIR.getAbsolutePath();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-streaming\\src\\test\\java\\org\\apache\\hadoop\\streaming",
  "methodName" : "testCommandLine",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void testCommandLine() throws IOException\n{\r\n    int returnStatus = runStreamJob();\r\n    assertEquals(\"Streaming Job Failure code expected\", 5, returnStatus);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-streaming\\src\\test\\java\\org\\apache\\hadoop\\streaming",
  "methodName" : "createInput",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void createInput() throws IOException\n{\r\n    DataOutputStream out = new DataOutputStream(new FileOutputStream(INPUT_FILE.getAbsoluteFile()));\r\n    for (int i = 0; i < 10000; ++i) {\r\n        out.write(input.getBytes(\"UTF-8\"));\r\n    }\r\n    out.close();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-streaming\\src\\test\\java\\org\\apache\\hadoop\\streaming",
  "methodName" : "genArgs",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "String[] genArgs()\n{\r\n    return new String[] { \"-input\", INPUT_FILE.getAbsolutePath(), \"-output\", OUTPUT_DIR.getAbsolutePath(), \"-mapper\", map, \"-reducer\", \"org.apache.hadoop.mapred.lib.IdentityReducer\", \"-numReduceTasks\", \"0\", \"-jobconf\", \"mapreduce.task.files.preserve.failedtasks=true\", \"-jobconf\", \"stream.tmpdir=\" + System.getProperty(\"test.build.data\", \"/tmp\") };\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-streaming\\src\\test\\java\\org\\apache\\hadoop\\streaming",
  "methodName" : "testUnconsumedInput",
  "errType" : [ "Exception" ],
  "containingMethodsNum" : 11,
  "sourceCodeText" : "void testUnconsumedInput() throws Exception\n{\r\n    String outFileName = \"part-00000\";\r\n    File outFile = null;\r\n    try {\r\n        try {\r\n            FileUtil.fullyDelete(OUTPUT_DIR.getAbsoluteFile());\r\n        } catch (Exception e) {\r\n        }\r\n        createInput();\r\n        Configuration conf = new Configuration();\r\n        conf.set(\"stream.minRecWrittenToEnableSkip_\", \"0\");\r\n        job = new StreamJob();\r\n        job.setConf(conf);\r\n        int exitCode = job.run(genArgs());\r\n        assertEquals(\"Job failed\", 0, exitCode);\r\n        outFile = new File(OUTPUT_DIR, outFileName).getAbsoluteFile();\r\n        String output = StreamUtil.slurp(outFile);\r\n        assertEquals(\"Output was truncated\", EXPECTED_OUTPUT_SIZE, StringUtils.countMatches(output, \"\\t\"));\r\n    } finally {\r\n        INPUT_FILE.delete();\r\n        FileUtil.fullyDelete(OUTPUT_DIR.getAbsoluteFile());\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 2,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-streaming\\src\\test\\java\\org\\apache\\hadoop\\streaming",
  "methodName" : "go",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void go() throws IOException\n{\r\n    testParentJobConfToEnvVars();\r\n    BufferedReader in = new BufferedReader(new InputStreamReader(System.in));\r\n    String line;\r\n    while ((line = in.readLine()) != null) {\r\n        String[] words = line.split(\" \");\r\n        for (int i = 0; i < words.length; i++) {\r\n            String out = \"LongValueSum:\" + words[i].trim() + \"\\t\" + \"1\";\r\n            System.out.println(out);\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-streaming\\src\\test\\java\\org\\apache\\hadoop\\streaming",
  "methodName" : "main",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void main(String[] args) throws IOException\n{\r\n    TrApp app = new StreamAggregate();\r\n    app.go();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-streaming\\src\\test\\java\\org\\apache\\hadoop\\streaming",
  "methodName" : "testFramework",
  "errType" : null,
  "containingMethodsNum" : 9,
  "sourceCodeText" : "void testFramework()\n{\r\n    JobConf jobConf = new JobConf();\r\n    jobConf.set(JTConfig.JT_IPC_ADDRESS, MRConfig.LOCAL_FRAMEWORK_NAME);\r\n    jobConf.set(MRConfig.FRAMEWORK_NAME, MRConfig.YARN_FRAMEWORK_NAME);\r\n    assertFalse(\"Expected 'isLocal' to be false\", StreamUtil.isLocalJobTracker(jobConf));\r\n    jobConf.set(JTConfig.JT_IPC_ADDRESS, MRConfig.LOCAL_FRAMEWORK_NAME);\r\n    jobConf.set(MRConfig.FRAMEWORK_NAME, MRConfig.CLASSIC_FRAMEWORK_NAME);\r\n    assertFalse(\"Expected 'isLocal' to be false\", StreamUtil.isLocalJobTracker(jobConf));\r\n    jobConf.set(JTConfig.JT_IPC_ADDRESS, \"jthost:9090\");\r\n    jobConf.set(MRConfig.FRAMEWORK_NAME, MRConfig.LOCAL_FRAMEWORK_NAME);\r\n    assertTrue(\"Expected 'isLocal' to be true\", StreamUtil.isLocalJobTracker(jobConf));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-streaming\\src\\test\\java\\org\\apache\\hadoop\\typedbytes",
  "methodName" : "testToString",
  "errType" : null,
  "containingMethodsNum" : 12,
  "sourceCodeText" : "void testToString()\n{\r\n    TypedBytesWritable tbw = new TypedBytesWritable();\r\n    tbw.setValue(true);\r\n    assertEquals(\"true\", tbw.toString());\r\n    tbw.setValue(12345);\r\n    assertEquals(\"12345\", tbw.toString());\r\n    tbw.setValue(123456789L);\r\n    assertEquals(\"123456789\", tbw.toString());\r\n    tbw.setValue((float) 1.23);\r\n    assertEquals(\"1.23\", tbw.toString());\r\n    tbw.setValue(1.23456789);\r\n    assertEquals(\"1.23456789\", tbw.toString());\r\n    tbw.setValue(\"random text\");\r\n    assertEquals(\"random text\", tbw.toString());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-streaming\\src\\test\\java\\org\\apache\\hadoop\\typedbytes",
  "methodName" : "testIO",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void testIO() throws IOException\n{\r\n    TypedBytesWritable tbw = new TypedBytesWritable();\r\n    tbw.setValue(12345);\r\n    ByteArrayOutputStream baos = new ByteArrayOutputStream();\r\n    DataOutput dout = new DataOutputStream(baos);\r\n    tbw.write(dout);\r\n    ByteArrayInputStream bais = new ByteArrayInputStream(baos.toByteArray());\r\n    DataInput din = new DataInputStream(bais);\r\n    TypedBytesWritable readTbw = new TypedBytesWritable();\r\n    readTbw.readFields(din);\r\n    assertEquals(tbw, readTbw);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-streaming\\src\\test\\java\\org\\apache\\hadoop\\streaming",
  "methodName" : "main",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void main(String[] args) throws IOException\n{\r\n    if (args.length < 1) {\r\n        System.err.println(\"Usage: OutputOnlyApp NUMRECORDS\");\r\n        return;\r\n    }\r\n    int numRecords = Integer.parseInt(args[0]);\r\n    while (numRecords-- > 0) {\r\n        System.out.println(\"key\\tvalue\");\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-streaming\\src\\test\\java\\org\\apache\\hadoop\\streaming",
  "methodName" : "main",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void main(String[] args)\n{\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-streaming\\src\\test\\java\\org\\apache\\hadoop\\streaming",
  "methodName" : "reduce",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void reduce(Object arg0, Iterator arg1, OutputCollector arg2, Reporter arg3) throws IOException\n{\r\n    int count = 0;\r\n    while (arg1.hasNext()) {\r\n        count += 1;\r\n        arg1.next();\r\n    }\r\n    arg2.collect(arg0, new Text(\"\" + count));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-streaming\\src\\test\\java\\org\\apache\\hadoop\\streaming",
  "methodName" : "configure",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void configure(JobConf arg0)\n{\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-streaming\\src\\test\\java\\org\\apache\\hadoop\\streaming",
  "methodName" : "close",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void close() throws IOException\n{\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-streaming\\src\\test\\java\\org\\apache\\hadoop\\streaming",
  "methodName" : "testSymLink",
  "errType" : null,
  "containingMethodsNum" : 33,
  "sourceCodeText" : "void testSymLink() throws Exception\n{\r\n    boolean mayExit = false;\r\n    MiniMRCluster mr = null;\r\n    MiniDFSCluster dfs = null;\r\n    try {\r\n        Configuration conf = new Configuration();\r\n        dfs = new MiniDFSCluster.Builder(conf).build();\r\n        FileSystem fileSys = dfs.getFileSystem();\r\n        String namenode = fileSys.getUri().toString();\r\n        mr = new MiniMRCluster(1, namenode, 3);\r\n        List<String> args = new ArrayList<String>();\r\n        for (Map.Entry<String, String> entry : mr.createJobConf()) {\r\n            args.add(\"-jobconf\");\r\n            args.add(entry.getKey() + \"=\" + entry.getValue());\r\n        }\r\n        String[] argv = new String[] { \"-input\", INPUT_FILE, \"-output\", OUTPUT_DIR, \"-mapper\", map, \"-reducer\", reduce, \"-jobconf\", \"stream.tmpdir=\" + System.getProperty(\"test.build.data\", \"/tmp\"), \"-jobconf\", JobConf.MAPRED_MAP_TASK_JAVA_OPTS + \"=\" + \"-Dcontrib.name=\" + System.getProperty(\"contrib.name\") + \" \" + \"-Dbuild.test=\" + System.getProperty(\"build.test\") + \" \" + conf.get(JobConf.MAPRED_MAP_TASK_JAVA_OPTS, conf.get(JobConf.MAPRED_TASK_JAVA_OPTS, \"\")), \"-jobconf\", JobConf.MAPRED_REDUCE_TASK_JAVA_OPTS + \"=\" + \"-Dcontrib.name=\" + System.getProperty(\"contrib.name\") + \" \" + \"-Dbuild.test=\" + System.getProperty(\"build.test\") + \" \" + conf.get(JobConf.MAPRED_REDUCE_TASK_JAVA_OPTS, conf.get(JobConf.MAPRED_TASK_JAVA_OPTS, \"\")), \"-cacheFile\", fileSys.getUri() + CACHE_FILE + \"#testlink\", \"-jobconf\", \"mapred.jar=\" + TestStreaming.STREAMING_JAR };\r\n        for (String arg : argv) {\r\n            args.add(arg);\r\n        }\r\n        argv = args.toArray(new String[args.size()]);\r\n        fileSys.delete(new Path(OUTPUT_DIR), true);\r\n        DataOutputStream file = fileSys.create(new Path(INPUT_FILE));\r\n        file.writeBytes(mapString);\r\n        file.close();\r\n        file = fileSys.create(new Path(CACHE_FILE));\r\n        file.writeBytes(cacheString);\r\n        file.close();\r\n        job = new StreamJob(argv, mayExit);\r\n        job.go();\r\n        fileSys = dfs.getFileSystem();\r\n        String line = null;\r\n        Path[] fileList = FileUtil.stat2Paths(fileSys.listStatus(new Path(OUTPUT_DIR), new Utils.OutputFileUtils.OutputFilesFilter()));\r\n        for (int i = 0; i < fileList.length; i++) {\r\n            System.out.println(fileList[i].toString());\r\n            BufferedReader bread = new BufferedReader(new InputStreamReader(fileSys.open(fileList[i])));\r\n            line = bread.readLine();\r\n            System.out.println(line);\r\n        }\r\n        assertEquals(cacheString + \"\\t\", line);\r\n    } finally {\r\n        if (dfs != null) {\r\n            dfs.shutdown();\r\n        }\r\n        if (mr != null) {\r\n            mr.shutdown();\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-streaming\\src\\test\\java\\org\\apache\\hadoop\\streaming",
  "methodName" : "main",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void main(String[] args) throws Exception\n{\r\n    new TestStreaming().testCommandLine();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-streaming\\src\\test\\java\\org\\apache\\hadoop\\streaming",
  "methodName" : "createInput",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void createInput() throws IOException\n{\r\n    GZIPOutputStream out = new GZIPOutputStream(new FileOutputStream(INPUT_FILE.getAbsoluteFile()));\r\n    out.write(input.getBytes(\"UTF-8\"));\r\n    out.close();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-streaming\\src\\test\\java\\org\\apache\\hadoop\\streaming",
  "methodName" : "testParentJobConfToEnvVars",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void testParentJobConfToEnvVars() throws IOException\n{\r\n    env = new Environment();\r\n    expect(\"mapreduce_jobtracker_address\", \"local\");\r\n    expectDefined(\"mapreduce_cluster_local_dir\");\r\n    expect(\"mapred_output_format_class\", \"org.apache.hadoop.mapred.TextOutputFormat\");\r\n    expect(\"mapreduce_job_output_key_class\", \"org.apache.hadoop.io.Text\");\r\n    expect(\"mapreduce_job_output_value_class\", \"org.apache.hadoop.io.Text\");\r\n    expect(\"mapreduce_task_ismap\", \"false\");\r\n    expectDefined(\"mapreduce_task_attempt_id\");\r\n    expectDefined(\"mapreduce_task_io_sort_factor\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-streaming\\src\\test\\java\\org\\apache\\hadoop\\streaming",
  "methodName" : "expect",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void expect(String evName, String evVal) throws IOException\n{\r\n    String got = env.getProperty(evName);\r\n    if (!evVal.equals(got)) {\r\n        String msg = \"FAIL evName=\" + evName + \" got=\" + got + \" expect=\" + evVal;\r\n        throw new IOException(msg);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-streaming\\src\\test\\java\\org\\apache\\hadoop\\streaming",
  "methodName" : "expectDefined",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void expectDefined(String evName) throws IOException\n{\r\n    String got = env.getProperty(evName);\r\n    if (got == null) {\r\n        String msg = \"FAIL evName=\" + evName + \" is undefined. Expect defined.\";\r\n        throw new IOException(msg);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-streaming\\src\\test\\java\\org\\apache\\hadoop\\streaming",
  "methodName" : "go",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void go() throws IOException\n{\r\n    testParentJobConfToEnvVars();\r\n    BufferedReader in = new BufferedReader(new InputStreamReader(System.in));\r\n    String line;\r\n    while ((line = in.readLine()) != null) {\r\n        String out = line.replace(find, replace);\r\n        System.out.println(out);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-streaming\\src\\test\\java\\org\\apache\\hadoop\\streaming",
  "methodName" : "main",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void main(String[] args) throws IOException\n{\r\n    args[0] = CUnescape(args[0]);\r\n    args[1] = CUnescape(args[1]);\r\n    TrAppReduce app = new TrAppReduce(args[0].charAt(0), args[1].charAt(0));\r\n    app.go();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-streaming\\src\\test\\java\\org\\apache\\hadoop\\streaming",
  "methodName" : "CUnescape",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String CUnescape(String s)\n{\r\n    if (s.equals(\"\\\\n\")) {\r\n        return \"\\n\";\r\n    } else {\r\n        return s;\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-streaming\\src\\test\\java\\org\\apache\\hadoop\\streaming\\io",
  "methodName" : "testKeyOnlyTextOutputReader",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void testKeyOnlyTextOutputReader() throws IOException\n{\r\n    String text = \"key,value\\nkey2,value2\\nnocomma\\n\";\r\n    PipeMapRed pipeMapRed = new MyPipeMapRed(text);\r\n    KeyOnlyTextOutputReader outputReader = new KeyOnlyTextOutputReader();\r\n    outputReader.initialize(pipeMapRed);\r\n    outputReader.readKeyValue();\r\n    Assert.assertEquals(new Text(\"key,value\"), outputReader.getCurrentKey());\r\n    outputReader.readKeyValue();\r\n    Assert.assertEquals(new Text(\"key2,value2\"), outputReader.getCurrentKey());\r\n    outputReader.readKeyValue();\r\n    Assert.assertEquals(new Text(\"nocomma\"), outputReader.getCurrentKey());\r\n    Assert.assertEquals(false, outputReader.readKeyValue());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-streaming\\src\\test\\java\\org\\apache\\hadoop\\streaming",
  "methodName" : "createInput",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void createInput() throws IOException\n{\r\n    DataOutputStream out = new DataOutputStream(new FileOutputStream(INPUT_FILE.getAbsoluteFile()));\r\n    out.write(input.getBytes(\"UTF-8\"));\r\n    out.close();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-streaming\\src\\test\\java\\org\\apache\\hadoop\\streaming",
  "methodName" : "genArgs",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "String[] genArgs(boolean ignoreKey)\n{\r\n    return new String[] { \"-input\", INPUT_FILE.getAbsolutePath(), \"-output\", OUTPUT_DIR.getAbsolutePath(), \"-mapper\", TestStreaming.CAT, \"-jobconf\", MRJobConfig.PRESERVE_FAILED_TASK_FILES + \"=true\", \"-jobconf\", \"stream.non.zero.exit.is.failure=true\", \"-jobconf\", \"stream.tmpdir=\" + System.getProperty(\"test.build.data\", \"/tmp\"), \"-jobconf\", \"stream.map.input.ignoreKey=\" + ignoreKey };\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-streaming\\src\\test\\java\\org\\apache\\hadoop\\streaming",
  "methodName" : "runStreamJob",
  "errType" : [ "Exception" ],
  "containingMethodsNum" : 11,
  "sourceCodeText" : "void runStreamJob(final String outputExpect, boolean ignoreKey) throws Exception\n{\r\n    String outFileName = \"part-00000\";\r\n    File outFile = null;\r\n    try {\r\n        try {\r\n            FileUtil.fullyDelete(OUTPUT_DIR.getAbsoluteFile());\r\n        } catch (Exception e) {\r\n        }\r\n        createInput();\r\n        boolean mayExit = false;\r\n        job = new StreamJob(genArgs(ignoreKey), mayExit);\r\n        job.go();\r\n        outFile = new File(OUTPUT_DIR, outFileName).getAbsoluteFile();\r\n        String output = StreamUtil.slurp(outFile);\r\n        System.err.println(\"outEx1=\" + outputExpect);\r\n        System.err.println(\"  out1=\" + output);\r\n        assertEquals(outputExpect, output);\r\n    } finally {\r\n        INPUT_FILE.delete();\r\n        FileUtil.fullyDelete(OUTPUT_DIR.getAbsoluteFile());\r\n    }\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 2,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-streaming\\src\\test\\java\\org\\apache\\hadoop\\streaming",
  "methodName" : "testCommandLineWithKey",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testCommandLineWithKey() throws Exception\n{\r\n    runStreamJob(outputWithKey, false);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-streaming\\src\\test\\java\\org\\apache\\hadoop\\streaming",
  "methodName" : "testCommandLineWithoutKey",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testCommandLineWithoutKey() throws Exception\n{\r\n    runStreamJob(outputWithoutKey, true);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-streaming\\src\\test\\java\\org\\apache\\hadoop\\streaming",
  "methodName" : "main",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void main(String[] args) throws Exception\n{\r\n    new TestStreamingKeyValue().testCommandLineWithKey();\r\n    new TestStreamingKeyValue().testCommandLineWithoutKey();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-streaming\\src\\test\\java\\org\\apache\\hadoop\\streaming",
  "methodName" : "createInput",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void createInput() throws IOException\n{\r\n    FileOutputStream out = new FileOutputStream(INPUT_FILE.getAbsoluteFile());\r\n    String dummyXmlStartTag = \"<PATTERN>\\n\";\r\n    String dummyXmlEndTag = \"</PATTERN>\\n\";\r\n    out.write(dummyXmlStartTag.getBytes(\"UTF-8\"));\r\n    out.write(input.getBytes(\"UTF-8\"));\r\n    out.write(dummyXmlEndTag.getBytes(\"UTF-8\"));\r\n    out.close();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-streaming\\src\\test\\java\\org\\apache\\hadoop\\streaming",
  "methodName" : "genArgs",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "String[] genArgs()\n{\r\n    args.add(\"-inputreader\");\r\n    args.add(\"StreamXmlRecordReader,begin=<xmltag>,end=</xmltag>\");\r\n    args.add(\"-jobconf\");\r\n    args.add(\"mapreduce.job.maps=1\");\r\n    return super.genArgs();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-streaming\\src\\test\\java\\org\\apache\\hadoop\\streaming",
  "methodName" : "createInput",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void createInput() throws IOException\n{\r\n    DataOutputStream out = new DataOutputStream(new FileOutputStream(INPUT_FILE.getAbsoluteFile()));\r\n    out.write(input.getBytes(\"UTF-8\"));\r\n    out.close();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-streaming\\src\\test\\java\\org\\apache\\hadoop\\streaming",
  "methodName" : "genArgs",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "String[] genArgs()\n{\r\n    return new String[] { \"-input\", INPUT_FILE.getAbsolutePath(), \"-output\", OUTPUT_DIR.getAbsolutePath(), \"-mapper\", map, \"-reducer\", reduce, \"-jobconf\", \"mapreduce.task.files.preserve.failedtasks=true\", \"-jobconf\", \"stream.tmpdir=\" + System.getProperty(\"test.build.data\", \"/tmp\"), \"-jobconf\", \"stream.map.output=rawbytes\", \"-jobconf\", \"stream.reduce.input=rawbytes\", \"-verbose\" };\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-streaming\\src\\test\\java\\org\\apache\\hadoop\\streaming",
  "methodName" : "testCommandLine",
  "errType" : [ "Exception" ],
  "containingMethodsNum" : 15,
  "sourceCodeText" : "void testCommandLine() throws Exception\n{\r\n    try {\r\n        try {\r\n            FileUtil.fullyDelete(OUTPUT_DIR.getAbsoluteFile());\r\n        } catch (Exception e) {\r\n        }\r\n        createInput();\r\n        OUTPUT_DIR.delete();\r\n        StreamJob job = new StreamJob();\r\n        job.setConf(new Configuration());\r\n        job.run(genArgs());\r\n        File outFile = new File(OUTPUT_DIR, \"part-00000\").getAbsoluteFile();\r\n        String output = StreamUtil.slurp(outFile);\r\n        outFile.delete();\r\n        System.out.println(\"   map=\" + map);\r\n        System.out.println(\"reduce=\" + reduce);\r\n        System.err.println(\"outEx1=\" + outputExpect);\r\n        System.err.println(\"  out1=\" + output);\r\n        assertEquals(outputExpect, output);\r\n    } finally {\r\n        INPUT_FILE.delete();\r\n        FileUtil.fullyDelete(OUTPUT_DIR.getAbsoluteFile());\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 2,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-streaming\\src\\test\\java\\org\\apache\\hadoop\\streaming",
  "methodName" : "testParentJobConfToEnvVars",
  "errType" : null,
  "containingMethodsNum" : 10,
  "sourceCodeText" : "void testParentJobConfToEnvVars() throws IOException\n{\r\n    env = new Environment();\r\n    expectDefined(\"mapreduce_cluster_local_dir\");\r\n    expect(\"mapreduce_map_output_key_class\", \"org.apache.hadoop.io.Text\");\r\n    expect(\"mapreduce_map_output_value_class\", \"org.apache.hadoop.io.Text\");\r\n    expect(\"mapreduce_task_ismap\", \"true\");\r\n    expectDefined(\"mapreduce_task_attempt_id\");\r\n    expectDefined(\"mapreduce_map_input_file\");\r\n    expectDefined(\"mapreduce_map_input_length\");\r\n    expectDefined(\"mapreduce_task_io_sort_factor\");\r\n    expect(\"map_input_file\", env.getProperty(\"mapreduce_map_input_file\"));\r\n    expect(\"map_input_length\", env.getProperty(\"mapreduce_map_input_length\"));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-streaming\\src\\test\\java\\org\\apache\\hadoop\\streaming",
  "methodName" : "expect",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void expect(String evName, String evVal) throws IOException\n{\r\n    String got = env.getProperty(evName);\r\n    if (!evVal.equals(got)) {\r\n        String msg = \"FAIL evName=\" + evName + \" got=\" + got + \" expect=\" + evVal;\r\n        throw new IOException(msg);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-streaming\\src\\test\\java\\org\\apache\\hadoop\\streaming",
  "methodName" : "expectDefined",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void expectDefined(String evName) throws IOException\n{\r\n    String got = env.getProperty(evName);\r\n    if (got == null) {\r\n        String msg = \"FAIL evName=\" + evName + \" is undefined. Expect defined.\";\r\n        throw new IOException(msg);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-streaming\\src\\test\\java\\org\\apache\\hadoop\\streaming",
  "methodName" : "go",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void go() throws IOException\n{\r\n    testParentJobConfToEnvVars();\r\n    BufferedReader in = new BufferedReader(new InputStreamReader(System.in));\r\n    String line;\r\n    while ((line = in.readLine()) != null) {\r\n        String out = line.replace(find, replace);\r\n        System.out.println(out);\r\n        System.err.println(\"reporter:counter:UserCounters,InputLines,1\");\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-streaming\\src\\test\\java\\org\\apache\\hadoop\\streaming",
  "methodName" : "main",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void main(String[] args) throws IOException\n{\r\n    args[0] = CUnescape(args[0]);\r\n    args[1] = CUnescape(args[1]);\r\n    TrApp app = new TrApp(args[0].charAt(0), args[1].charAt(0));\r\n    app.go();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-streaming\\src\\test\\java\\org\\apache\\hadoop\\streaming",
  "methodName" : "CUnescape",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String CUnescape(String s)\n{\r\n    if (s.equals(\"\\\\n\")) {\r\n        return \"\\n\";\r\n    } else {\r\n        return s;\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-streaming\\src\\test\\java\\org\\apache\\hadoop\\streaming",
  "methodName" : "setTestDir",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setTestDir(File testDir)\n{\r\n    TEST_DIR = testDir;\r\n    OUTPUT_DIR = new File(testDir, \"out\");\r\n    INPUT_FILE = new File(testDir, \"input.txt\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-streaming\\src\\test\\java\\org\\apache\\hadoop\\streaming",
  "methodName" : "setUp",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void setUp() throws IOException\n{\r\n    UtilTest.recursiveDelete(TEST_DIR);\r\n    assertTrue(\"Creating \" + TEST_DIR, TEST_DIR.mkdirs());\r\n    args.clear();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-streaming\\src\\test\\java\\org\\apache\\hadoop\\streaming",
  "methodName" : "tearDown",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void tearDown()\n{\r\n    UtilTest.recursiveDelete(TEST_DIR);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-streaming\\src\\test\\java\\org\\apache\\hadoop\\streaming",
  "methodName" : "getInputData",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getInputData()\n{\r\n    return input;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-streaming\\src\\test\\java\\org\\apache\\hadoop\\streaming",
  "methodName" : "createInput",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void createInput() throws IOException\n{\r\n    DataOutputStream out = getFileSystem().create(new Path(INPUT_FILE.getPath()));\r\n    out.write(getInputData().getBytes(\"UTF-8\"));\r\n    out.close();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-streaming\\src\\test\\java\\org\\apache\\hadoop\\streaming",
  "methodName" : "setInputOutput",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void setInputOutput()\n{\r\n    inputFile = INPUT_FILE.getPath();\r\n    outDir = OUTPUT_DIR.getPath();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-streaming\\src\\test\\java\\org\\apache\\hadoop\\streaming",
  "methodName" : "genArgs",
  "errType" : null,
  "containingMethodsNum" : 14,
  "sourceCodeText" : "String[] genArgs()\n{\r\n    args.add(\"-input\");\r\n    args.add(inputFile);\r\n    args.add(\"-output\");\r\n    args.add(outDir);\r\n    args.add(\"-mapper\");\r\n    args.add(map);\r\n    args.add(\"-reducer\");\r\n    args.add(reduce);\r\n    args.add(\"-jobconf\");\r\n    args.add(\"mapreduce.task.files.preserve.failedtasks=true\");\r\n    args.add(\"-jobconf\");\r\n    args.add(\"stream.tmpdir=\" + System.getProperty(\"test.build.data\", \"/tmp\"));\r\n    String[] str = new String[args.size()];\r\n    args.toArray(str);\r\n    return str;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-streaming\\src\\test\\java\\org\\apache\\hadoop\\streaming",
  "methodName" : "getConf",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Configuration getConf()\n{\r\n    return new Configuration();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-streaming\\src\\test\\java\\org\\apache\\hadoop\\streaming",
  "methodName" : "getFileSystem",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "FileSystem getFileSystem() throws IOException\n{\r\n    return FileSystem.get(getConf());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-streaming\\src\\test\\java\\org\\apache\\hadoop\\streaming",
  "methodName" : "getExpectedOutput",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getExpectedOutput()\n{\r\n    return outputExpect;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-streaming\\src\\test\\java\\org\\apache\\hadoop\\streaming",
  "methodName" : "checkOutput",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void checkOutput() throws IOException\n{\r\n    Path outPath = new Path(OUTPUT_DIR.getPath(), \"part-00000\");\r\n    FileSystem fs = getFileSystem();\r\n    String output = StreamUtil.slurpHadoop(outPath, fs);\r\n    fs.delete(outPath, true);\r\n    System.err.println(\"outEx1=\" + getExpectedOutput());\r\n    System.err.println(\"  out1=\" + output);\r\n    assertOutput(getExpectedOutput(), output);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-streaming\\src\\test\\java\\org\\apache\\hadoop\\streaming",
  "methodName" : "assertOutput",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void assertOutput(String expectedOutput, String output) throws IOException\n{\r\n    String[] words = expectedOutput.split(\"\\t\\n\");\r\n    Set<String> expectedWords = new HashSet<String>(Arrays.asList(words));\r\n    words = output.split(\"\\t\\n\");\r\n    Set<String> returnedWords = new HashSet<String>(Arrays.asList(words));\r\n    assertTrue(returnedWords.containsAll(expectedWords));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-streaming\\src\\test\\java\\org\\apache\\hadoop\\streaming",
  "methodName" : "runStreamJob",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "int runStreamJob() throws IOException\n{\r\n    setInputOutput();\r\n    createInput();\r\n    boolean mayExit = false;\r\n    job = new StreamJob(genArgs(), mayExit);\r\n    return job.go();\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-streaming\\src\\test\\java\\org\\apache\\hadoop\\streaming",
  "methodName" : "testCommandLine",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void testCommandLine() throws Exception\n{\r\n    int ret = runStreamJob();\r\n    assertEquals(0, ret);\r\n    checkOutput();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-streaming\\src\\test\\java\\org\\apache\\hadoop\\streaming",
  "methodName" : "testGoodClassOrNull",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void testGoodClassOrNull() throws Exception\n{\r\n    String NAME = \"ClassWithNoPackage\";\r\n    ClassLoader cl = TestClassWithNoPackage.class.getClassLoader();\r\n    String JAR = JarFinder.getJar(cl.loadClass(NAME));\r\n    Configuration conf = new Configuration();\r\n    conf.setClassLoader(new URLClassLoader(new URL[] { new URL(\"file\", null, JAR) }, null));\r\n    String defaultPackage = this.getClass().getPackage().getName();\r\n    Class c = StreamUtil.goodClassOrNull(conf, NAME, defaultPackage);\r\n    assertNotNull(\"Class \" + NAME + \" not found!\", c);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-streaming\\src\\test\\java\\org\\apache\\hadoop\\streaming",
  "methodName" : "main",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void main(String[] args) throws Exception\n{\r\n    new TestClassWithNoPackage().testGoodClassOrNull();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-streaming\\src\\test\\java\\org\\apache\\hadoop\\streaming",
  "methodName" : "testDumping",
  "errType" : [ "Exception" ],
  "containingMethodsNum" : 21,
  "sourceCodeText" : "void testDumping() throws Exception\n{\r\n    Configuration conf = new Configuration();\r\n    MiniDFSCluster cluster = new MiniDFSCluster.Builder(conf).numDataNodes(2).build();\r\n    FileSystem fs = cluster.getFileSystem();\r\n    PrintStream psBackup = System.out;\r\n    ByteArrayOutputStream out = new ByteArrayOutputStream();\r\n    PrintStream psOut = new PrintStream(out);\r\n    System.setOut(psOut);\r\n    DumpTypedBytes dumptb = new DumpTypedBytes(conf);\r\n    try {\r\n        Path root = new Path(\"/typedbytestest\");\r\n        assertTrue(fs.mkdirs(root));\r\n        assertTrue(fs.exists(root));\r\n        OutputStreamWriter writer = new OutputStreamWriter(fs.create(new Path(root, \"test.txt\")));\r\n        try {\r\n            for (int i = 0; i < 100; i++) {\r\n                writer.write(\"\" + (10 * i) + \"\\n\");\r\n            }\r\n        } finally {\r\n            writer.close();\r\n        }\r\n        String[] args = new String[1];\r\n        args[0] = \"/typedbytestest\";\r\n        int ret = dumptb.run(args);\r\n        assertEquals(\"Return value != 0.\", 0, ret);\r\n        ByteArrayInputStream in = new ByteArrayInputStream(out.toByteArray());\r\n        TypedBytesInput tbinput = new TypedBytesInput(new DataInputStream(in));\r\n        int counter = 0;\r\n        Object key = tbinput.read();\r\n        while (key != null) {\r\n            assertEquals(Long.class, key.getClass());\r\n            Object value = tbinput.read();\r\n            assertEquals(String.class, value.getClass());\r\n            assertTrue(\"Invalid output.\", Integer.parseInt(value.toString()) % 10 == 0);\r\n            counter++;\r\n            key = tbinput.read();\r\n        }\r\n        assertEquals(\"Wrong number of outputs.\", 100, counter);\r\n    } finally {\r\n        try {\r\n            fs.close();\r\n        } catch (Exception e) {\r\n        }\r\n        System.setOut(psBackup);\r\n        cluster.shutdown();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 3,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-streaming\\src\\test\\java\\org\\apache\\hadoop\\streaming",
  "methodName" : "go",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void go(int preWriteLines, int sleep, int postWriteLines) throws IOException\n{\r\n    go(preWriteLines, sleep, postWriteLines, false);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-streaming\\src\\test\\java\\org\\apache\\hadoop\\streaming",
  "methodName" : "go",
  "errType" : [ "InterruptedException" ],
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void go(int preWriteLines, int sleep, int postWriteLines, boolean status) throws IOException\n{\r\n    BufferedReader in = new BufferedReader(new InputStreamReader(System.in));\r\n    String line;\r\n    if (status) {\r\n        System.err.println(\"reporter:status:starting echo\");\r\n    }\r\n    while (preWriteLines > 0) {\r\n        --preWriteLines;\r\n        System.err.println(\"some stderr output before reading input, \" + preWriteLines + \" lines remaining, sleeping \" + sleep);\r\n        try {\r\n            Thread.sleep(sleep);\r\n        } catch (InterruptedException e) {\r\n        }\r\n    }\r\n    while ((line = in.readLine()) != null) {\r\n        System.out.println(line);\r\n    }\r\n    while (postWriteLines > 0) {\r\n        --postWriteLines;\r\n        System.err.println(\"some stderr output after reading input, lines remaining \" + postWriteLines);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-streaming\\src\\test\\java\\org\\apache\\hadoop\\streaming",
  "methodName" : "main",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void main(String[] args) throws IOException\n{\r\n    if (args.length < 3) {\r\n        System.err.println(\"Usage: StderrApp PREWRITE SLEEP POSTWRITE [STATUS]\");\r\n        return;\r\n    }\r\n    int preWriteLines = Integer.parseInt(args[0]);\r\n    int sleep = Integer.parseInt(args[1]);\r\n    int postWriteLines = Integer.parseInt(args[2]);\r\n    boolean status = args.length > 3 ? Boolean.parseBoolean(args[3]) : false;\r\n    go(preWriteLines, sleep, postWriteLines, status);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-streaming\\src\\test\\java\\org\\apache\\hadoop\\streaming",
  "methodName" : "setInputOutput",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setInputOutput()\n{\r\n    inputFile = INPUT_FILE;\r\n    outDir = OUTPUT_DIR;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-streaming\\src\\test\\java\\org\\apache\\hadoop\\streaming",
  "methodName" : "createInput",
  "errType" : null,
  "containingMethodsNum" : 16,
  "sourceCodeText" : "void createInput() throws IOException\n{\r\n    fileSys.delete(new Path(INPUT_DIR), true);\r\n    DataOutputStream dos = fileSys.create(new Path(INPUT_FILE));\r\n    String inputFileString = \"symlink1\" + File.separator + \"cacheArchive1\\nsymlink2\" + File.separator + \"cacheArchive2\";\r\n    dos.write(inputFileString.getBytes(\"UTF-8\"));\r\n    dos.close();\r\n    DataOutputStream out = fileSys.create(new Path(CACHE_ARCHIVE_1.toString()));\r\n    ZipOutputStream zos = new ZipOutputStream(out);\r\n    ZipEntry ze = new ZipEntry(CACHE_FILE_1.toString());\r\n    zos.putNextEntry(ze);\r\n    zos.write(input.getBytes(\"UTF-8\"));\r\n    zos.closeEntry();\r\n    zos.close();\r\n    out = fileSys.create(new Path(CACHE_ARCHIVE_2.toString()));\r\n    zos = new ZipOutputStream(out);\r\n    ze = new ZipEntry(CACHE_FILE_2.toString());\r\n    zos.putNextEntry(ze);\r\n    zos.write(input.getBytes(\"UTF-8\"));\r\n    zos.closeEntry();\r\n    zos.close();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-streaming\\src\\test\\java\\org\\apache\\hadoop\\streaming",
  "methodName" : "genArgs",
  "errType" : null,
  "containingMethodsNum" : 13,
  "sourceCodeText" : "String[] genArgs()\n{\r\n    String workDir = fileSys.getWorkingDirectory().toString() + \"/\";\r\n    String cache1 = workDir + CACHE_ARCHIVE_1 + \"#symlink1\";\r\n    String cache2 = workDir + CACHE_ARCHIVE_2 + \"#symlink2\";\r\n    for (Map.Entry<String, String> entry : mr.createJobConf()) {\r\n        args.add(\"-jobconf\");\r\n        args.add(entry.getKey() + \"=\" + entry.getValue());\r\n    }\r\n    args.add(\"-jobconf\");\r\n    args.add(\"mapreduce.job.reduces=1\");\r\n    args.add(\"-cacheArchive\");\r\n    args.add(cache1);\r\n    args.add(\"-cacheArchive\");\r\n    args.add(cache2);\r\n    args.add(\"-jobconf\");\r\n    args.add(\"mapred.jar=\" + STREAMING_JAR);\r\n    return super.genArgs();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-streaming\\src\\test\\java\\org\\apache\\hadoop\\streaming",
  "methodName" : "checkOutput",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void checkOutput() throws IOException\n{\r\n    StringBuffer output = new StringBuffer(256);\r\n    Path[] fileList = FileUtil.stat2Paths(fileSys.listStatus(new Path(OUTPUT_DIR)));\r\n    for (int i = 0; i < fileList.length; i++) {\r\n        LOG.info(\"Adding output from file: \" + fileList[i]);\r\n        output.append(StreamUtil.slurpHadoop(fileList[i], fileSys));\r\n    }\r\n    assertOutput(expectedOutput, output.toString());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-streaming\\src\\test\\java\\org\\apache\\hadoop\\streaming",
  "methodName" : "genArgs",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "String[] genArgs(boolean exitStatusIsFailure, boolean failMap)\n{\r\n    return new String[] { \"-input\", INPUT_FILE.getAbsolutePath(), \"-output\", OUTPUT_DIR.getAbsolutePath(), \"-mapper\", (failMap ? failingTask : echoTask), \"-reducer\", (failMap ? echoTask : failingTask), \"-jobconf\", \"mapreduce.task.files.preserve.failedtasks=true\", \"-jobconf\", \"stream.non.zero.exit.is.failure=\" + exitStatusIsFailure, \"-jobconf\", \"stream.tmpdir=\" + System.getProperty(\"test.build.data\", \"/tmp\"), \"-jobconf\", \"mapreduce.task.io.sort.mb=10\" };\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-streaming\\src\\test\\java\\org\\apache\\hadoop\\streaming",
  "methodName" : "setUp",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void setUp() throws IOException\n{\r\n    UtilTest.recursiveDelete(TEST_DIR);\r\n    assertTrue(TEST_DIR.mkdirs());\r\n    FileOutputStream out = new FileOutputStream(INPUT_FILE.getAbsoluteFile());\r\n    out.write(\"hello\\n\".getBytes());\r\n    out.close();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-streaming\\src\\test\\java\\org\\apache\\hadoop\\streaming",
  "methodName" : "runStreamJob",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void runStreamJob(boolean exitStatusIsFailure, boolean failMap) throws Exception\n{\r\n    boolean mayExit = false;\r\n    int returnStatus = 0;\r\n    StreamJob job = new StreamJob(genArgs(exitStatusIsFailure, failMap), mayExit);\r\n    returnStatus = job.go();\r\n    if (exitStatusIsFailure) {\r\n        assertEquals(\"Streaming Job failure code expected\", 1, returnStatus);\r\n    } else {\r\n        assertEquals(\"Streaming Job expected to succeed\", 0, returnStatus);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-streaming\\src\\test\\java\\org\\apache\\hadoop\\streaming",
  "methodName" : "testMapFailOk",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testMapFailOk() throws Exception\n{\r\n    runStreamJob(false, true);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-streaming\\src\\test\\java\\org\\apache\\hadoop\\streaming",
  "methodName" : "testMapFailNotOk",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testMapFailNotOk() throws Exception\n{\r\n    runStreamJob(true, true);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-streaming\\src\\test\\java\\org\\apache\\hadoop\\streaming",
  "methodName" : "testReduceFailOk",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testReduceFailOk() throws Exception\n{\r\n    runStreamJob(false, false);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-streaming\\src\\test\\java\\org\\apache\\hadoop\\streaming",
  "methodName" : "testReduceFailNotOk",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testReduceFailNotOk() throws Exception\n{\r\n    runStreamJob(true, false);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-streaming\\src\\test\\java\\org\\apache\\hadoop\\streaming",
  "methodName" : "setUp",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void setUp() throws IOException\n{\r\n    super.setUp();\r\n    FileSystem.closeAll();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-streaming\\src\\test\\java\\org\\apache\\hadoop\\streaming",
  "methodName" : "getConf",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Configuration getConf()\n{\r\n    conf = new Configuration();\r\n    conf.setLong(\"fs.local.block.size\", blockSize);\r\n    return conf;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-streaming\\src\\test\\java\\org\\apache\\hadoop\\streaming",
  "methodName" : "genArgs",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "String[] genArgs()\n{\r\n    args.add(\"-inputreader\");\r\n    args.add(\"StreamXmlRecordReader,begin=<line>,end=</line>,slowmatch=\" + isSlowMatch);\r\n    return super.genArgs();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-streaming\\src\\test\\java\\org\\apache\\hadoop\\streaming",
  "methodName" : "testStreamXmlMultiInnerFast",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void testStreamXmlMultiInnerFast() throws Exception\n{\r\n    if (hasPerl) {\r\n        blockSize = 60;\r\n        isSlowMatch = \"false\";\r\n        super.testCommandLine();\r\n    } else {\r\n        LOG.warn(\"No perl; skipping test.\");\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-streaming\\src\\test\\java\\org\\apache\\hadoop\\streaming",
  "methodName" : "testStreamXmlMultiOuterFast",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void testStreamXmlMultiOuterFast() throws Exception\n{\r\n    if (hasPerl) {\r\n        blockSize = 80;\r\n        isSlowMatch = \"false\";\r\n        super.testCommandLine();\r\n    } else {\r\n        LOG.warn(\"No perl; skipping test.\");\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-streaming\\src\\test\\java\\org\\apache\\hadoop\\streaming",
  "methodName" : "testStreamXmlMultiInnerSlow",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void testStreamXmlMultiInnerSlow() throws Exception\n{\r\n    if (hasPerl) {\r\n        blockSize = 60;\r\n        isSlowMatch = \"true\";\r\n        super.testCommandLine();\r\n    } else {\r\n        LOG.warn(\"No perl; skipping test.\");\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-streaming\\src\\test\\java\\org\\apache\\hadoop\\streaming",
  "methodName" : "testStreamXmlMultiOuterSlow",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void testStreamXmlMultiOuterSlow() throws Exception\n{\r\n    if (hasPerl) {\r\n        blockSize = 80;\r\n        isSlowMatch = \"true\";\r\n        super.testCommandLine();\r\n    } else {\r\n        LOG.warn(\"No perl; skipping test.\");\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-streaming\\src\\test\\java\\org\\apache\\hadoop\\streaming",
  "methodName" : "testCommandLine",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void testCommandLine()\n{\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-streaming\\src\\test\\java\\org\\apache\\hadoop\\streaming",
  "methodName" : "setUp",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void setUp() throws IOException\n{\r\n    FileSystem localFs = FileSystem.getLocal(conf);\r\n    DataOutputStream dos = localFs.create(new Path(\"target/sidefile\"));\r\n    dos.write(\"hello world\\n\".getBytes(\"UTF-8\"));\r\n    dos.close();\r\n    input = \"\";\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-streaming\\src\\test\\java\\org\\apache\\hadoop\\streaming",
  "methodName" : "tearDown",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void tearDown()\n{\r\n    if (mr != null) {\r\n        mr.shutdown();\r\n    }\r\n    if (dfs != null) {\r\n        dfs.shutdown();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-streaming\\src\\test\\java\\org\\apache\\hadoop\\streaming",
  "methodName" : "getExpectedOutput",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getExpectedOutput()\n{\r\n    return EXPECTED_OUTPUT;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-streaming\\src\\test\\java\\org\\apache\\hadoop\\streaming",
  "methodName" : "getConf",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Configuration getConf()\n{\r\n    return conf;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-streaming\\src\\test\\java\\org\\apache\\hadoop\\streaming",
  "methodName" : "genArgs",
  "errType" : null,
  "containingMethodsNum" : 11,
  "sourceCodeText" : "String[] genArgs()\n{\r\n    for (Map.Entry<String, String> entry : mr.createJobConf()) {\r\n        args.add(\"-jobconf\");\r\n        args.add(entry.getKey() + \"=\" + entry.getValue());\r\n    }\r\n    args.add(\"-file\");\r\n    args.add(new java.io.File(\"target/sidefile\").getAbsolutePath());\r\n    args.add(\"-numReduceTasks\");\r\n    args.add(\"0\");\r\n    args.add(\"-jobconf\");\r\n    args.add(\"mapred.jar=\" + STREAMING_JAR);\r\n    args.add(\"-verbose\");\r\n    return super.genArgs();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-streaming\\src\\test\\java\\org\\apache\\hadoop\\streaming",
  "methodName" : "genArgs",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "String[] genArgs()\n{\r\n    args.add(\"-combiner\");\r\n    args.add(combine);\r\n    return super.genArgs();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-tools\\hadoop-streaming\\src\\test\\java\\org\\apache\\hadoop\\streaming",
  "methodName" : "testCommandLine",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void testCommandLine() throws Exception\n{\r\n    super.testCommandLine();\r\n    String counterGrp = \"org.apache.hadoop.mapred.Task$Counter\";\r\n    Counters counters = job.running_.getCounters();\r\n    assertTrue(counters.findCounter(counterGrp, \"COMBINE_INPUT_RECORDS\").getValue() != 0);\r\n    assertTrue(counters.findCounter(counterGrp, \"COMBINE_OUTPUT_RECORDS\").getValue() != 0);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
} ]
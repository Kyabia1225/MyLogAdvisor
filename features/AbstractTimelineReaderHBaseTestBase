[ {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-yarn-project\\hadoop-yarn\\hadoop-yarn-server\\hadoop-yarn-server-timelineservice-hbase-tests\\src\\test\\java\\org\\apache\\hadoop\\yarn\\server\\timelineservice\\reader",
  "methodName" : "setup",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void setup() throws Exception\n{\r\n    util = new HBaseTestingUtility();\r\n    Configuration conf = util.getConfiguration();\r\n    conf.setInt(\"hfile.format.version\", 3);\r\n    util.startMiniCluster();\r\n    DataGeneratorForTest.createSchema(util.getConfiguration());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-yarn-project\\hadoop-yarn\\hadoop-yarn-server\\hadoop-yarn-server-timelineservice-hbase-tests\\src\\test\\java\\org\\apache\\hadoop\\yarn\\server\\timelineservice\\reader",
  "methodName" : "tearDown",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void tearDown() throws Exception\n{\r\n    if (server != null) {\r\n        server.stop();\r\n        server = null;\r\n    }\r\n    if (util != null) {\r\n        util.shutdownMiniCluster();\r\n    }\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-yarn-project\\hadoop-yarn\\hadoop-yarn-server\\hadoop-yarn-server-timelineservice-hbase-tests\\src\\test\\java\\org\\apache\\hadoop\\yarn\\server\\timelineservice\\reader",
  "methodName" : "initialize",
  "errType" : [ "Exception" ],
  "containingMethodsNum" : 11,
  "sourceCodeText" : "void initialize() throws Exception\n{\r\n    try {\r\n        Configuration config = util.getConfiguration();\r\n        config.setBoolean(YarnConfiguration.TIMELINE_SERVICE_ENABLED, true);\r\n        config.setFloat(YarnConfiguration.TIMELINE_SERVICE_VERSION, 2.0f);\r\n        config.set(YarnConfiguration.TIMELINE_SERVICE_READER_WEBAPP_ADDRESS, \"localhost:0\");\r\n        config.set(YarnConfiguration.RM_CLUSTER_ID, \"cluster1\");\r\n        config.set(YarnConfiguration.TIMELINE_SERVICE_READER_CLASS, \"org.apache.hadoop.yarn.server.timelineservice.storage.\" + \"HBaseTimelineReaderImpl\");\r\n        config.setInt(\"hfile.format.version\", 3);\r\n        server = new TimelineReaderServer() {\r\n\r\n            @Override\r\n            protected void addFilters(Configuration conf) {\r\n            }\r\n        };\r\n        server.init(config);\r\n        server.start();\r\n        serverPort = server.getWebServerPort();\r\n    } catch (Exception e) {\r\n        Assert.fail(\"Web server failed to start\");\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-yarn-project\\hadoop-yarn\\hadoop-yarn-server\\hadoop-yarn-server-timelineservice-hbase-tests\\src\\test\\java\\org\\apache\\hadoop\\yarn\\server\\timelineservice\\reader",
  "methodName" : "createClient",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Client createClient()\n{\r\n    ClientConfig cfg = new DefaultClientConfig();\r\n    cfg.getClasses().add(YarnJacksonJaxbJsonProvider.class);\r\n    return new Client(new URLConnectionClientHandler(new DummyURLConnectionFactory()), cfg);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-yarn-project\\hadoop-yarn\\hadoop-yarn-server\\hadoop-yarn-server-timelineservice-hbase-tests\\src\\test\\java\\org\\apache\\hadoop\\yarn\\server\\timelineservice\\reader",
  "methodName" : "getResponse",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "ClientResponse getResponse(Client client, URI uri) throws Exception\n{\r\n    ClientResponse resp = client.resource(uri).accept(MediaType.APPLICATION_JSON).type(MediaType.APPLICATION_JSON).get(ClientResponse.class);\r\n    if (resp == null || resp.getStatusInfo().getStatusCode() != ClientResponse.Status.OK.getStatusCode()) {\r\n        String msg = \"\";\r\n        if (resp != null) {\r\n            msg = String.valueOf(resp.getStatusInfo().getStatusCode());\r\n        }\r\n        throw new IOException(\"Incorrect response from timeline reader. \" + \"Status=\" + msg);\r\n    }\r\n    return resp;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-yarn-project\\hadoop-yarn\\hadoop-yarn-server\\hadoop-yarn-server-timelineservice-hbase-tests\\src\\test\\java\\org\\apache\\hadoop\\yarn\\server\\timelineservice\\reader",
  "methodName" : "verifyHttpResponse",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void verifyHttpResponse(Client client, URI uri, Status status)\n{\r\n    ClientResponse resp = client.resource(uri).accept(MediaType.APPLICATION_JSON).type(MediaType.APPLICATION_JSON).get(ClientResponse.class);\r\n    assertNotNull(resp);\r\n    assertTrue(\"Response from server should have been \" + status, resp.getStatusInfo().getStatusCode() == status.getStatusCode());\r\n    System.out.println(\"Response is: \" + resp.getEntity(String.class));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-yarn-project\\hadoop-yarn\\hadoop-yarn-server\\hadoop-yarn-server-timelineservice-hbase-tests\\src\\test\\java\\org\\apache\\hadoop\\yarn\\server\\timelineservice\\reader",
  "methodName" : "verifyFlowEntites",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "List<FlowActivityEntity> verifyFlowEntites(Client client, URI uri, int noOfEntities) throws Exception\n{\r\n    ClientResponse resp = getResponse(client, uri);\r\n    List<FlowActivityEntity> entities = resp.getEntity(new GenericType<List<FlowActivityEntity>>() {\r\n    });\r\n    assertNotNull(entities);\r\n    assertEquals(noOfEntities, entities.size());\r\n    return entities;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-yarn-project\\hadoop-yarn\\hadoop-yarn-server\\hadoop-yarn-server-timelineservice-hbase-tests\\src\\test\\java\\org\\apache\\hadoop\\yarn\\server\\timelineservice\\reader",
  "methodName" : "getHBaseTestingUtility",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "HBaseTestingUtility getHBaseTestingUtility()\n{\r\n    return util;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-yarn-project\\hadoop-yarn\\hadoop-yarn-server\\hadoop-yarn-server-timelineservice-hbase-tests\\src\\test\\java\\org\\apache\\hadoop\\yarn\\server\\timelineservice\\reader",
  "methodName" : "getServerPort",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "int getServerPort()\n{\r\n    return serverPort;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-yarn-project\\hadoop-yarn\\hadoop-yarn-server\\hadoop-yarn-server-timelineservice-hbase-tests\\src\\test\\java\\org\\apache\\hadoop\\yarn\\server\\timelineservice\\storage",
  "methodName" : "setupBeforeClass",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void setupBeforeClass() throws Exception\n{\r\n    util = new HBaseTestingUtility();\r\n    Configuration conf = util.getConfiguration();\r\n    conf.setInt(\"hfile.format.version\", 3);\r\n    util.startMiniCluster();\r\n    DataGeneratorForTest.createSchema(util.getConfiguration());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-yarn-project\\hadoop-yarn\\hadoop-yarn-server\\hadoop-yarn-server-timelineservice-hbase-tests\\src\\test\\java\\org\\apache\\hadoop\\yarn\\server\\timelineservice\\storage",
  "methodName" : "testDomainIdTable",
  "errType" : null,
  "containingMethodsNum" : 35,
  "sourceCodeText" : "void testDomainIdTable() throws Exception\n{\r\n    long l = System.currentTimeMillis();\r\n    HBaseTimelineWriterImpl hbi = null;\r\n    Configuration c1 = util.getConfiguration();\r\n    String clusterId = \"yarn-cluster\";\r\n    TimelineCollectorContext context = new TimelineCollectorContext(clusterId, null, null, null, null, null);\r\n    TimelineDomain domain2;\r\n    try {\r\n        hbi = new HBaseTimelineWriterImpl();\r\n        hbi.init(c1);\r\n        domain2 = new TimelineDomain();\r\n        domain2.setCreatedTime(l);\r\n        domain2.setDescription(\"domain-2\");\r\n        domain2.setId(\"domain-2\");\r\n        domain2.setModifiedTime(l);\r\n        domain2.setOwner(\"owner1\");\r\n        domain2.setReaders(\"user1,user2 group1,group2\");\r\n        domain2.setWriters(\"writer1,writer2\");\r\n        hbi.write(context, domain2);\r\n        hbi.flush();\r\n    } finally {\r\n        if (hbi != null) {\r\n            hbi.close();\r\n        }\r\n    }\r\n    Connection conn = ConnectionFactory.createConnection(c1);\r\n    Table table1 = conn.getTable(BaseTableRW.getTableName(c1, DomainTableRW.TABLE_NAME_CONF_NAME, DomainTableRW.DEFAULT_TABLE_NAME));\r\n    byte[] startRow = new DomainRowKey(clusterId, domain2.getId()).getRowKey();\r\n    Get g = new Get(startRow);\r\n    Result result = table1.get(g);\r\n    assertNotNull(result);\r\n    assertTrue(!result.isEmpty());\r\n    byte[] row = result.getRow();\r\n    DomainRowKey domainRowKey = DomainRowKey.parseRowKey(row);\r\n    assertEquals(domain2.getId(), domainRowKey.getDomainId());\r\n    assertEquals(clusterId, domainRowKey.getClusterId());\r\n    Long cTime = (Long) ColumnRWHelper.readResult(result, DomainColumn.CREATED_TIME);\r\n    String description = (String) ColumnRWHelper.readResult(result, DomainColumn.DESCRIPTION);\r\n    Long mTime = (Long) ColumnRWHelper.readResult(result, DomainColumn.MODIFICATION_TIME);\r\n    String owners = (String) ColumnRWHelper.readResult(result, DomainColumn.OWNER);\r\n    String readers = (String) ColumnRWHelper.readResult(result, DomainColumn.READERS);\r\n    String writers = (String) ColumnRWHelper.readResult(result, DomainColumn.WRITERS);\r\n    assertEquals(l, cTime.longValue());\r\n    assertEquals(l, mTime.longValue());\r\n    assertEquals(\"domain-2\", description);\r\n    assertEquals(\"owner1\", owners);\r\n    assertEquals(\"user1,user2 group1,group2\", readers);\r\n    assertEquals(\"writer1,writer2\", writers);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-yarn-project\\hadoop-yarn\\hadoop-yarn-server\\hadoop-yarn-server-timelineservice-hbase-tests\\src\\test\\java\\org\\apache\\hadoop\\yarn\\server\\timelineservice\\storage",
  "methodName" : "tearDownAfterClass",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void tearDownAfterClass() throws Exception\n{\r\n    if (util != null) {\r\n        util.shutdownMiniCluster();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-yarn-project\\hadoop-yarn\\hadoop-yarn-server\\hadoop-yarn-server-timelineservice-hbase-tests\\src\\test\\java\\org\\apache\\hadoop\\yarn\\server\\timelineservice\\storage",
  "methodName" : "testTimelineReaderHBaseUp",
  "errType" : null,
  "containingMethodsNum" : 10,
  "sourceCodeText" : "void testTimelineReaderHBaseUp() throws Exception\n{\r\n    HBaseTestingUtility util = new HBaseTestingUtility();\r\n    configure(util);\r\n    try {\r\n        util.startMiniCluster();\r\n        DataGeneratorForTest.createSchema(util.getConfiguration());\r\n        DataGeneratorForTest.loadApps(util, System.currentTimeMillis());\r\n        TimelineReaderServer server = getTimelineReaderServer();\r\n        server.init(util.getConfiguration());\r\n        HBaseTimelineReaderImpl htr = getHBaseTimelineReaderImpl(server);\r\n        server.start();\r\n        checkQuery(htr);\r\n    } finally {\r\n        util.shutdownMiniCluster();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-yarn-project\\hadoop-yarn\\hadoop-yarn-server\\hadoop-yarn-server-timelineservice-hbase-tests\\src\\test\\java\\org\\apache\\hadoop\\yarn\\server\\timelineservice\\storage",
  "methodName" : "testTimelineReaderInitWhenHBaseIsDown",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void testTimelineReaderInitWhenHBaseIsDown() throws TimeoutException, InterruptedException\n{\r\n    HBaseTestingUtility util = new HBaseTestingUtility();\r\n    configure(util);\r\n    TimelineReaderServer server = getTimelineReaderServer();\r\n    server.init(util.getConfiguration());\r\n    HBaseTimelineReaderImpl htr = getHBaseTimelineReaderImpl(server);\r\n    server.start();\r\n    waitForHBaseDown(htr);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-yarn-project\\hadoop-yarn\\hadoop-yarn-server\\hadoop-yarn-server-timelineservice-hbase-tests\\src\\test\\java\\org\\apache\\hadoop\\yarn\\server\\timelineservice\\storage",
  "methodName" : "testTimelineReaderDetectsHBaseDown",
  "errType" : null,
  "containingMethodsNum" : 11,
  "sourceCodeText" : "void testTimelineReaderDetectsHBaseDown() throws Exception\n{\r\n    HBaseTestingUtility util = new HBaseTestingUtility();\r\n    configure(util);\r\n    try {\r\n        util.startMiniCluster();\r\n        DataGeneratorForTest.createSchema(util.getConfiguration());\r\n        DataGeneratorForTest.loadApps(util, System.currentTimeMillis());\r\n        TimelineReaderServer server = getTimelineReaderServer();\r\n        server.init(util.getConfiguration());\r\n        HBaseTimelineReaderImpl htr = getHBaseTimelineReaderImpl(server);\r\n        util.shutdownMiniHBaseCluster();\r\n        server.start();\r\n        waitForHBaseDown(htr);\r\n    } finally {\r\n        util.shutdownMiniCluster();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-yarn-project\\hadoop-yarn\\hadoop-yarn-server\\hadoop-yarn-server-timelineservice-hbase-tests\\src\\test\\java\\org\\apache\\hadoop\\yarn\\server\\timelineservice\\storage",
  "methodName" : "testTimelineReaderDetectsZooKeeperDown",
  "errType" : null,
  "containingMethodsNum" : 11,
  "sourceCodeText" : "void testTimelineReaderDetectsZooKeeperDown() throws Exception\n{\r\n    HBaseTestingUtility util = new HBaseTestingUtility();\r\n    configure(util);\r\n    try {\r\n        util.startMiniCluster();\r\n        DataGeneratorForTest.createSchema(util.getConfiguration());\r\n        DataGeneratorForTest.loadApps(util, System.currentTimeMillis());\r\n        TimelineReaderServer server = getTimelineReaderServer();\r\n        server.init(util.getConfiguration());\r\n        HBaseTimelineReaderImpl htr = getHBaseTimelineReaderImpl(server);\r\n        util.shutdownMiniCluster();\r\n        server.start();\r\n        waitForHBaseDown(htr);\r\n    } finally {\r\n        util.shutdownMiniCluster();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-yarn-project\\hadoop-yarn\\hadoop-yarn-server\\hadoop-yarn-server-timelineservice-hbase-tests\\src\\test\\java\\org\\apache\\hadoop\\yarn\\server\\timelineservice\\storage",
  "methodName" : "testTimelineReaderRecoversAfterHBaseReturns",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 13,
  "sourceCodeText" : "void testTimelineReaderRecoversAfterHBaseReturns() throws Exception\n{\r\n    HBaseTestingUtility util = new HBaseTestingUtility();\r\n    configure(util);\r\n    try {\r\n        util.startMiniCluster();\r\n        DataGeneratorForTest.createSchema(util.getConfiguration());\r\n        DataGeneratorForTest.loadApps(util, System.currentTimeMillis());\r\n        TimelineReaderServer server = getTimelineReaderServer();\r\n        server.init(util.getConfiguration());\r\n        HBaseTimelineReaderImpl htr = getHBaseTimelineReaderImpl(server);\r\n        util.shutdownMiniHBaseCluster();\r\n        server.start();\r\n        waitForHBaseDown(htr);\r\n        util.startMiniHBaseCluster(1, 1);\r\n        GenericTestUtils.waitFor(() -> {\r\n            try {\r\n                htr.getTimelineStorageMonitor().checkStorageIsUp();\r\n                return true;\r\n            } catch (IOException e) {\r\n                return false;\r\n            }\r\n        }, 1000, 150000);\r\n    } finally {\r\n        util.shutdownMiniCluster();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 2,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-yarn-project\\hadoop-yarn\\hadoop-yarn-server\\hadoop-yarn-server-timelineservice-hbase-tests\\src\\test\\java\\org\\apache\\hadoop\\yarn\\server\\timelineservice\\storage",
  "methodName" : "waitForHBaseDown",
  "errType" : [ "IOException", "IOException" ],
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void waitForHBaseDown(HBaseTimelineReaderImpl htr) throws TimeoutException, InterruptedException\n{\r\n    try {\r\n        GenericTestUtils.waitFor(() -> {\r\n            try {\r\n                htr.getTimelineStorageMonitor().checkStorageIsUp();\r\n                return false;\r\n            } catch (IOException e) {\r\n                return true;\r\n            }\r\n        }, 1000, 150000);\r\n        checkQuery(htr);\r\n        Assert.fail(\"Query should fail when HBase is down\");\r\n    } catch (IOException e) {\r\n        Assert.assertEquals(\"HBase is down\", e.getMessage());\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 2,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-yarn-project\\hadoop-yarn\\hadoop-yarn-server\\hadoop-yarn-server-timelineservice-hbase-tests\\src\\test\\java\\org\\apache\\hadoop\\yarn\\server\\timelineservice\\storage",
  "methodName" : "checkQuery",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "Set<TimelineEntity> checkQuery(HBaseTimelineReaderImpl htr) throws IOException\n{\r\n    TimelineReaderContext context = new TimelineReaderContext(YarnConfiguration.DEFAULT_RM_CLUSTER_ID, null, null, null, null, TimelineEntityType.YARN_FLOW_ACTIVITY.toString(), null, null);\r\n    return htr.getEntities(context, MONITOR_FILTERS, DATA_TO_RETRIEVE);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-yarn-project\\hadoop-yarn\\hadoop-yarn-server\\hadoop-yarn-server-timelineservice-hbase-tests\\src\\test\\java\\org\\apache\\hadoop\\yarn\\server\\timelineservice\\storage",
  "methodName" : "configure",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void configure(HBaseTestingUtility util)\n{\r\n    Configuration config = util.getConfiguration();\r\n    config.setBoolean(YarnConfiguration.TIMELINE_SERVICE_ENABLED, true);\r\n    config.setFloat(YarnConfiguration.TIMELINE_SERVICE_VERSION, 2.0f);\r\n    config.set(YarnConfiguration.TIMELINE_SERVICE_READER_WEBAPP_ADDRESS, \"localhost:0\");\r\n    config.set(YarnConfiguration.RM_CLUSTER_ID, \"cluster1\");\r\n    config.set(YarnConfiguration.TIMELINE_SERVICE_READER_CLASS, \"org.apache.hadoop.yarn.server.timelineservice.storage.\" + \"HBaseTimelineReaderImpl\");\r\n    config.setInt(\"hfile.format.version\", 3);\r\n    config.setLong(TIMELINE_SERVICE_READER_STORAGE_MONITOR_INTERVAL_MS, 5000);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-yarn-project\\hadoop-yarn\\hadoop-yarn-server\\hadoop-yarn-server-timelineservice-hbase-tests\\src\\test\\java\\org\\apache\\hadoop\\yarn\\server\\timelineservice\\storage",
  "methodName" : "getTimelineReaderServer",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "TimelineReaderServer getTimelineReaderServer()\n{\r\n    return new TimelineReaderServer() {\r\n\r\n        @Override\r\n        protected void addFilters(Configuration conf) {\r\n        }\r\n    };\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-yarn-project\\hadoop-yarn\\hadoop-yarn-server\\hadoop-yarn-server-timelineservice-hbase-tests\\src\\test\\java\\org\\apache\\hadoop\\yarn\\server\\timelineservice\\storage",
  "methodName" : "getHBaseTimelineReaderImpl",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "HBaseTimelineReaderImpl getHBaseTimelineReaderImpl(TimelineReaderServer server)\n{\r\n    for (Service s : server.getServices()) {\r\n        if (s instanceof HBaseTimelineReaderImpl) {\r\n            return (HBaseTimelineReaderImpl) s;\r\n        }\r\n    }\r\n    throw new IllegalStateException(\"Couldn't find HBaseTimelineReaderImpl\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-yarn-project\\hadoop-yarn\\hadoop-yarn-server\\hadoop-yarn-server-timelineservice-hbase-tests\\src\\test\\java\\org\\apache\\hadoop\\yarn\\server\\timelineservice\\storage",
  "methodName" : "setupBeforeClass",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void setupBeforeClass() throws Exception\n{\r\n    util = new HBaseTestingUtility();\r\n    util.startMiniCluster();\r\n    DataGeneratorForTest.createSchema(util.getConfiguration());\r\n    DataGeneratorForTest.loadApps(util, CURRENT_TIME);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-yarn-project\\hadoop-yarn\\hadoop-yarn-server\\hadoop-yarn-server-timelineservice-hbase-tests\\src\\test\\java\\org\\apache\\hadoop\\yarn\\server\\timelineservice\\storage",
  "methodName" : "init",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void init() throws Exception\n{\r\n    reader = new HBaseTimelineReaderImpl();\r\n    reader.init(util.getConfiguration());\r\n    reader.start();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-yarn-project\\hadoop-yarn\\hadoop-yarn-server\\hadoop-yarn-server-timelineservice-hbase-tests\\src\\test\\java\\org\\apache\\hadoop\\yarn\\server\\timelineservice\\storage",
  "methodName" : "stop",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void stop() throws Exception\n{\r\n    if (reader != null) {\r\n        reader.stop();\r\n        reader.close();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-yarn-project\\hadoop-yarn\\hadoop-yarn-server\\hadoop-yarn-server-timelineservice-hbase-tests\\src\\test\\java\\org\\apache\\hadoop\\yarn\\server\\timelineservice\\storage",
  "methodName" : "matchMetrics",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void matchMetrics(Map<Long, Number> m1, Map<Long, Number> m2)\n{\r\n    assertEquals(m1.size(), m2.size());\r\n    for (Map.Entry<Long, Number> entry : m2.entrySet()) {\r\n        Number val = m1.get(entry.getKey());\r\n        assertNotNull(val);\r\n        assertEquals(val.longValue(), entry.getValue().longValue());\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-yarn-project\\hadoop-yarn\\hadoop-yarn-server\\hadoop-yarn-server-timelineservice-hbase-tests\\src\\test\\java\\org\\apache\\hadoop\\yarn\\server\\timelineservice\\storage",
  "methodName" : "isApplicationRowKeyCorrect",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "boolean isApplicationRowKeyCorrect(byte[] rowKey, String cluster, String user, String flow, Long runid, String appName)\n{\r\n    ApplicationRowKey key = ApplicationRowKey.parseRowKey(rowKey);\r\n    assertEquals(cluster, key.getClusterId());\r\n    assertEquals(user, key.getUserId());\r\n    assertEquals(flow, key.getFlowName());\r\n    assertEquals(runid, key.getFlowRunId());\r\n    assertEquals(appName, key.getAppId());\r\n    return true;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-yarn-project\\hadoop-yarn\\hadoop-yarn-server\\hadoop-yarn-server-timelineservice-hbase-tests\\src\\test\\java\\org\\apache\\hadoop\\yarn\\server\\timelineservice\\storage",
  "methodName" : "testWriteNullApplicationToHBase",
  "errType" : null,
  "containingMethodsNum" : 21,
  "sourceCodeText" : "void testWriteNullApplicationToHBase() throws Exception\n{\r\n    TimelineEntities te = new TimelineEntities();\r\n    ApplicationEntity entity = new ApplicationEntity();\r\n    String appId = \"application_1000178881110_2002\";\r\n    entity.setId(appId);\r\n    long cTime = 1425016501000L;\r\n    entity.setCreatedTime(cTime);\r\n    Map<String, Object> infoMap = new HashMap<String, Object>();\r\n    infoMap.put(\"in fo M apK  ey1\", \"infoMapValue1\");\r\n    infoMap.put(\"infoMapKey2\", 10);\r\n    entity.addInfo(infoMap);\r\n    te.addEntity(entity);\r\n    HBaseTimelineWriterImpl hbi = null;\r\n    try {\r\n        Configuration c1 = util.getConfiguration();\r\n        hbi = new HBaseTimelineWriterImpl();\r\n        hbi.init(c1);\r\n        hbi.start();\r\n        String cluster = \"cluster_check_null_application\";\r\n        String user = \"user1check_null_application\";\r\n        String flow = null;\r\n        String flowVersion = \"AB7822C10F1111\";\r\n        long runid = 1002345678919L;\r\n        hbi.write(new TimelineCollectorContext(cluster, user, flow, flowVersion, runid, appId), te, UserGroupInformation.createRemoteUser(user));\r\n        hbi.stop();\r\n        Scan scan = new Scan();\r\n        scan.setStartRow(Bytes.toBytes(cluster));\r\n        scan.setStopRow(Bytes.toBytes(cluster + \"1\"));\r\n        Connection conn = ConnectionFactory.createConnection(c1);\r\n        ResultScanner resultScanner = new ApplicationTableRW().getResultScanner(c1, conn, scan);\r\n        assertTrue(resultScanner != null);\r\n        int count = 0;\r\n        for (Result rr = resultScanner.next(); rr != null; rr = resultScanner.next()) {\r\n            count++;\r\n        }\r\n        assertEquals(0, count);\r\n    } finally {\r\n        if (hbi != null) {\r\n            hbi.stop();\r\n            hbi.close();\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-yarn-project\\hadoop-yarn\\hadoop-yarn-server\\hadoop-yarn-server-timelineservice-hbase-tests\\src\\test\\java\\org\\apache\\hadoop\\yarn\\server\\timelineservice\\storage",
  "methodName" : "testWriteApplicationToHBase",
  "errType" : null,
  "containingMethodsNum" : 140,
  "sourceCodeText" : "void testWriteApplicationToHBase() throws Exception\n{\r\n    TimelineEntities te = new TimelineEntities();\r\n    ApplicationEntity entity = new ApplicationEntity();\r\n    String appId = \"application_1000178881110_2002\";\r\n    entity.setId(appId);\r\n    Long cTime = 1425016501000L;\r\n    entity.setCreatedTime(cTime);\r\n    Map<String, Object> infoMap = new HashMap<String, Object>();\r\n    infoMap.put(\"infoMapKey1\", \"infoMapValue1\");\r\n    infoMap.put(\"infoMapKey2\", 10);\r\n    entity.addInfo(infoMap);\r\n    String key = \"task\";\r\n    String value = \"is_related_to_entity_id_here\";\r\n    Set<String> isRelatedToSet = new HashSet<String>();\r\n    isRelatedToSet.add(value);\r\n    Map<String, Set<String>> isRelatedTo = new HashMap<String, Set<String>>();\r\n    isRelatedTo.put(key, isRelatedToSet);\r\n    entity.setIsRelatedToEntities(isRelatedTo);\r\n    key = \"container\";\r\n    value = \"relates_to_entity_id_here\";\r\n    Set<String> relatesToSet = new HashSet<String>();\r\n    relatesToSet.add(value);\r\n    value = \"relates_to_entity_id_here_Second\";\r\n    relatesToSet.add(value);\r\n    Map<String, Set<String>> relatesTo = new HashMap<String, Set<String>>();\r\n    relatesTo.put(key, relatesToSet);\r\n    entity.setRelatesToEntities(relatesTo);\r\n    Map<String, String> conf = new HashMap<String, String>();\r\n    conf.put(\"config_param1\", \"value1\");\r\n    conf.put(\"config_param2\", \"value2\");\r\n    entity.addConfigs(conf);\r\n    Set<TimelineMetric> metrics = new HashSet<>();\r\n    TimelineMetric m1 = new TimelineMetric();\r\n    m1.setId(\"MAP_SLOT_MILLIS\");\r\n    Map<Long, Number> metricValues = new HashMap<Long, Number>();\r\n    metricValues.put(CURRENT_TIME - 120000, 100000000);\r\n    metricValues.put(CURRENT_TIME - 100000, 200000000);\r\n    metricValues.put(CURRENT_TIME - 80000, 300000000);\r\n    metricValues.put(CURRENT_TIME - 60000, 400000000);\r\n    metricValues.put(CURRENT_TIME - 40000, 50000000000L);\r\n    metricValues.put(CURRENT_TIME - 20000, 60000000000L);\r\n    m1.setType(Type.TIME_SERIES);\r\n    m1.setValues(metricValues);\r\n    metrics.add(m1);\r\n    entity.addMetrics(metrics);\r\n    TimelineEntity aggEntity = new TimelineEntity();\r\n    String type = TimelineEntityType.YARN_APPLICATION.toString();\r\n    aggEntity.setId(appId);\r\n    aggEntity.setType(type);\r\n    long cTime2 = 1425016502000L;\r\n    aggEntity.setCreatedTime(cTime2);\r\n    TimelineMetric aggMetric = new TimelineMetric();\r\n    aggMetric.setId(\"MEM_USAGE\");\r\n    Map<Long, Number> aggMetricValues = new HashMap<Long, Number>();\r\n    long aggTs = CURRENT_TIME;\r\n    aggMetricValues.put(aggTs - 120000, 102400000L);\r\n    aggMetric.setType(Type.SINGLE_VALUE);\r\n    aggMetric.setRealtimeAggregationOp(TimelineMetricOperation.SUM);\r\n    aggMetric.setValues(aggMetricValues);\r\n    Set<TimelineMetric> aggMetrics = new HashSet<>();\r\n    aggMetrics.add(aggMetric);\r\n    entity.addMetrics(aggMetrics);\r\n    te.addEntity(entity);\r\n    HBaseTimelineWriterImpl hbi = null;\r\n    try {\r\n        Configuration c1 = util.getConfiguration();\r\n        hbi = new HBaseTimelineWriterImpl();\r\n        hbi.init(c1);\r\n        hbi.start();\r\n        String cluster = \"cluster_test_write_app\";\r\n        String user = \"user1\";\r\n        String flow = \"s!ome_f\\tlow  _n am!e\";\r\n        String flowVersion = \"AB7822C10F1111\";\r\n        long runid = 1002345678919L;\r\n        hbi.write(new TimelineCollectorContext(cluster, user, flow, flowVersion, runid, appId), te, UserGroupInformation.createRemoteUser(user));\r\n        entity = new ApplicationEntity();\r\n        appId = \"application_1000178881110_2002\";\r\n        entity.setId(appId);\r\n        Map<String, Object> infoMap1 = new HashMap<>();\r\n        infoMap1.put(\"infoMapKey3\", \"infoMapValue1\");\r\n        entity.addInfo(infoMap1);\r\n        te = new TimelineEntities();\r\n        te.addEntity(entity);\r\n        hbi.write(new TimelineCollectorContext(cluster, user, flow, flowVersion, runid, appId), te, UserGroupInformation.createRemoteUser(user));\r\n        hbi.stop();\r\n        infoMap.putAll(infoMap1);\r\n        ApplicationRowKey applicationRowKey = new ApplicationRowKey(cluster, user, flow, runid, appId);\r\n        byte[] rowKey = applicationRowKey.getRowKey();\r\n        Get get = new Get(rowKey);\r\n        get.setMaxVersions(Integer.MAX_VALUE);\r\n        Connection conn = ConnectionFactory.createConnection(c1);\r\n        Result result = new ApplicationTableRW().getResult(c1, conn, get);\r\n        assertTrue(result != null);\r\n        assertEquals(17, result.size());\r\n        byte[] row1 = result.getRow();\r\n        assertTrue(isApplicationRowKeyCorrect(row1, cluster, user, flow, runid, appId));\r\n        String id1 = ColumnRWHelper.readResult(result, ApplicationColumn.ID).toString();\r\n        assertEquals(appId, id1);\r\n        Long cTime1 = (Long) ColumnRWHelper.readResult(result, ApplicationColumn.CREATED_TIME);\r\n        assertEquals(cTime, cTime1);\r\n        Map<String, Object> infoColumns = ColumnRWHelper.readResults(result, ApplicationColumnPrefix.INFO, new StringKeyConverter());\r\n        assertEquals(infoMap, infoColumns);\r\n        for (Map.Entry<String, Set<String>> isRelatedToEntry : isRelatedTo.entrySet()) {\r\n            Object isRelatedToValue = ColumnRWHelper.readResult(result, ApplicationColumnPrefix.IS_RELATED_TO, isRelatedToEntry.getKey());\r\n            String compoundValue = isRelatedToValue.toString();\r\n            Set<String> isRelatedToValues = new HashSet<String>(Separator.VALUES.splitEncoded(compoundValue));\r\n            assertEquals(isRelatedTo.get(isRelatedToEntry.getKey()).size(), isRelatedToValues.size());\r\n            for (String v : isRelatedToEntry.getValue()) {\r\n                assertTrue(isRelatedToValues.contains(v));\r\n            }\r\n        }\r\n        for (Map.Entry<String, Set<String>> relatesToEntry : relatesTo.entrySet()) {\r\n            String compoundValue = ColumnRWHelper.readResult(result, ApplicationColumnPrefix.RELATES_TO, relatesToEntry.getKey()).toString();\r\n            Set<String> relatesToValues = new HashSet<String>(Separator.VALUES.splitEncoded(compoundValue));\r\n            assertEquals(relatesTo.get(relatesToEntry.getKey()).size(), relatesToValues.size());\r\n            for (String v : relatesToEntry.getValue()) {\r\n                assertTrue(relatesToValues.contains(v));\r\n            }\r\n        }\r\n        KeyConverter<String> stringKeyConverter = new StringKeyConverter();\r\n        Map<String, Object> configColumns = ColumnRWHelper.readResults(result, ApplicationColumnPrefix.CONFIG, stringKeyConverter);\r\n        assertEquals(conf, configColumns);\r\n        NavigableMap<String, NavigableMap<Long, Number>> metricsResult = ColumnRWHelper.readResultsWithTimestamps(result, ApplicationColumnPrefix.METRIC, stringKeyConverter);\r\n        NavigableMap<Long, Number> metricMap = metricsResult.get(m1.getId());\r\n        matchMetrics(metricValues, metricMap);\r\n        TimelineEntity e1 = reader.getEntity(new TimelineReaderContext(cluster, user, flow, runid, appId, entity.getType(), entity.getId()), new TimelineDataToRetrieve(null, null, EnumSet.of(TimelineReader.Field.ALL), Integer.MAX_VALUE, null, null));\r\n        assertNotNull(e1);\r\n        assertEquals(appId, e1.getId());\r\n        assertEquals(TimelineEntityType.YARN_APPLICATION.toString(), e1.getType());\r\n        assertEquals(cTime, e1.getCreatedTime());\r\n        Map<String, Object> infoMap2 = e1.getInfo();\r\n        infoMap2.remove(\"FROM_ID\");\r\n        assertEquals(infoMap, infoMap2);\r\n        Map<String, Set<String>> isRelatedTo2 = e1.getIsRelatedToEntities();\r\n        assertEquals(isRelatedTo, isRelatedTo2);\r\n        Map<String, Set<String>> relatesTo2 = e1.getRelatesToEntities();\r\n        assertEquals(relatesTo, relatesTo2);\r\n        Map<String, String> conf2 = e1.getConfigs();\r\n        assertEquals(conf, conf2);\r\n        Set<TimelineMetric> metrics2 = e1.getMetrics();\r\n        assertEquals(2, metrics2.size());\r\n        for (TimelineMetric metric2 : metrics2) {\r\n            Map<Long, Number> metricValues2 = metric2.getValues();\r\n            assertTrue(metric2.getId().equals(\"MAP_SLOT_MILLIS\") || metric2.getId().equals(\"MEM_USAGE\"));\r\n            if (metric2.getId().equals(\"MAP_SLOT_MILLIS\")) {\r\n                assertEquals(6, metricValues2.size());\r\n                matchMetrics(metricValues, metricValues2);\r\n            }\r\n            if (metric2.getId().equals(\"MEM_USAGE\")) {\r\n                assertEquals(1, metricValues2.size());\r\n                matchMetrics(aggMetricValues, metricValues2);\r\n            }\r\n        }\r\n        e1 = reader.getEntity(new TimelineReaderContext(cluster, user, flow, runid, appId, entity.getType(), entity.getId()), new TimelineDataToRetrieve(null, null, EnumSet.of(TimelineReader.Field.ALL), 3, null, null));\r\n        assertNotNull(e1);\r\n        assertEquals(appId, e1.getId());\r\n        assertEquals(TimelineEntityType.YARN_APPLICATION.toString(), e1.getType());\r\n        assertEquals(conf, e1.getConfigs());\r\n        metrics2 = e1.getMetrics();\r\n        assertEquals(2, metrics2.size());\r\n        for (TimelineMetric metric2 : metrics2) {\r\n            Map<Long, Number> metricValues2 = metric2.getValues();\r\n            assertTrue(metricValues2.size() <= 3);\r\n            assertTrue(metric2.getId().equals(\"MAP_SLOT_MILLIS\") || metric2.getId().equals(\"MEM_USAGE\"));\r\n        }\r\n        e1 = reader.getEntity(new TimelineReaderContext(cluster, user, flow, runid, appId, entity.getType(), entity.getId()), new TimelineDataToRetrieve(null, null, EnumSet.of(TimelineReader.Field.ALL), null, null, null));\r\n        assertNotNull(e1);\r\n        assertEquals(appId, e1.getId());\r\n        assertEquals(TimelineEntityType.YARN_APPLICATION.toString(), e1.getType());\r\n        assertEquals(cTime, e1.getCreatedTime());\r\n        infoMap2 = e1.getInfo();\r\n        infoMap2.remove(\"FROM_ID\");\r\n        assertEquals(infoMap, e1.getInfo());\r\n        assertEquals(isRelatedTo, e1.getIsRelatedToEntities());\r\n        assertEquals(relatesTo, e1.getRelatesToEntities());\r\n        assertEquals(conf, e1.getConfigs());\r\n        assertEquals(2, e1.getMetrics().size());\r\n        for (TimelineMetric metric : e1.getMetrics()) {\r\n            assertEquals(1, metric.getValues().size());\r\n            assertEquals(TimelineMetric.Type.SINGLE_VALUE, metric.getType());\r\n            assertTrue(metric.getId().equals(\"MAP_SLOT_MILLIS\") || metric.getId().equals(\"MEM_USAGE\"));\r\n            assertEquals(1, metric.getValues().size());\r\n            if (metric.getId().equals(\"MAP_SLOT_MILLIS\")) {\r\n                assertTrue(metric.getValues().containsKey(CURRENT_TIME - 20000));\r\n                assertEquals(metricValues.get(CURRENT_TIME - 20000), metric.getValues().get(CURRENT_TIME - 20000));\r\n            }\r\n            if (metric.getId().equals(\"MEM_USAGE\")) {\r\n                assertTrue(metric.getValues().containsKey(aggTs - 120000));\r\n                assertEquals(aggMetricValues.get(aggTs - 120000), metric.getValues().get(aggTs - 120000));\r\n            }\r\n        }\r\n    } finally {\r\n        if (hbi != null) {\r\n            hbi.stop();\r\n            hbi.close();\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-yarn-project\\hadoop-yarn\\hadoop-yarn-server\\hadoop-yarn-server-timelineservice-hbase-tests\\src\\test\\java\\org\\apache\\hadoop\\yarn\\server\\timelineservice\\storage",
  "methodName" : "testEvents",
  "errType" : null,
  "containingMethodsNum" : 43,
  "sourceCodeText" : "void testEvents() throws IOException\n{\r\n    TimelineEvent event = new TimelineEvent();\r\n    String eventId = ApplicationMetricsConstants.CREATED_EVENT_TYPE;\r\n    event.setId(eventId);\r\n    Long expTs = 1436512802000L;\r\n    event.setTimestamp(expTs);\r\n    String expKey = \"foo_event\";\r\n    Object expVal = \"test\";\r\n    event.addInfo(expKey, expVal);\r\n    final TimelineEntity entity = new ApplicationEntity();\r\n    entity.setId(HBaseTimelineSchemaUtils.convertApplicationIdToString(ApplicationId.newInstance(0, 1)));\r\n    entity.addEvent(event);\r\n    TimelineEntities entities = new TimelineEntities();\r\n    entities.addEntity(entity);\r\n    HBaseTimelineWriterImpl hbi = null;\r\n    try {\r\n        Configuration c1 = util.getConfiguration();\r\n        hbi = new HBaseTimelineWriterImpl();\r\n        hbi.init(c1);\r\n        hbi.start();\r\n        String cluster = \"cluster_test_events\";\r\n        String user = \"user2\";\r\n        String flow = \"other_flow_name\";\r\n        String flowVersion = \"1111F01C2287BA\";\r\n        long runid = 1009876543218L;\r\n        String appName = \"application_123465899910_1001\";\r\n        hbi.write(new TimelineCollectorContext(cluster, user, flow, flowVersion, runid, appName), entities, UserGroupInformation.createRemoteUser(user));\r\n        hbi.stop();\r\n        ApplicationRowKey applicationRowKey = new ApplicationRowKey(cluster, user, flow, runid, appName);\r\n        byte[] rowKey = applicationRowKey.getRowKey();\r\n        Get get = new Get(rowKey);\r\n        get.setMaxVersions(Integer.MAX_VALUE);\r\n        Connection conn = ConnectionFactory.createConnection(c1);\r\n        Result result = new ApplicationTableRW().getResult(c1, conn, get);\r\n        assertTrue(result != null);\r\n        byte[] row1 = result.getRow();\r\n        assertTrue(isApplicationRowKeyCorrect(row1, cluster, user, flow, runid, appName));\r\n        Map<EventColumnName, Object> eventsResult = ColumnRWHelper.readResults(result, ApplicationColumnPrefix.EVENT, new EventColumnNameConverter());\r\n        assertEquals(1, eventsResult.size());\r\n        for (Map.Entry<EventColumnName, Object> e : eventsResult.entrySet()) {\r\n            EventColumnName eventColumnName = e.getKey();\r\n            assertEquals(eventId, eventColumnName.getId());\r\n            assertEquals(expTs, eventColumnName.getTimestamp());\r\n            assertEquals(expKey, eventColumnName.getInfoKey());\r\n            Object value = e.getValue();\r\n            assertEquals(expVal, value.toString());\r\n        }\r\n        TimelineEntity e1 = reader.getEntity(new TimelineReaderContext(cluster, user, flow, runid, appName, entity.getType(), entity.getId()), new TimelineDataToRetrieve(null, null, EnumSet.of(Field.ALL), null, null, null));\r\n        TimelineEntity e2 = reader.getEntity(new TimelineReaderContext(cluster, user, null, null, appName, entity.getType(), entity.getId()), new TimelineDataToRetrieve(null, null, EnumSet.of(Field.ALL), null, null, null));\r\n        assertNotNull(e1);\r\n        assertNotNull(e2);\r\n        assertEquals(e1, e2);\r\n        NavigableSet<TimelineEvent> events = e1.getEvents();\r\n        assertEquals(1, events.size());\r\n        for (TimelineEvent e : events) {\r\n            assertEquals(eventId, e.getId());\r\n            assertEquals(expTs, Long.valueOf(e.getTimestamp()));\r\n            Map<String, Object> info = e.getInfo();\r\n            assertEquals(1, info.size());\r\n            for (Map.Entry<String, Object> infoEntry : info.entrySet()) {\r\n                assertEquals(expKey, infoEntry.getKey());\r\n                assertEquals(expVal, infoEntry.getValue());\r\n            }\r\n        }\r\n    } finally {\r\n        if (hbi != null) {\r\n            hbi.stop();\r\n            hbi.close();\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-yarn-project\\hadoop-yarn\\hadoop-yarn-server\\hadoop-yarn-server-timelineservice-hbase-tests\\src\\test\\java\\org\\apache\\hadoop\\yarn\\server\\timelineservice\\storage",
  "methodName" : "testNonIntegralMetricValues",
  "errType" : [ "IOException", "IOException" ],
  "containingMethodsNum" : 30,
  "sourceCodeText" : "void testNonIntegralMetricValues() throws IOException\n{\r\n    TimelineEntities teApp = new TimelineEntities();\r\n    ApplicationEntity entityApp = new ApplicationEntity();\r\n    String appId = \"application_1000178881110_2002\";\r\n    entityApp.setId(appId);\r\n    entityApp.setCreatedTime(1425016501000L);\r\n    Set<TimelineMetric> metricsApp = new HashSet<>();\r\n    TimelineMetric mApp = new TimelineMetric();\r\n    mApp.setId(\"MAP_SLOT_MILLIS\");\r\n    Map<Long, Number> metricAppValues = new HashMap<Long, Number>();\r\n    long ts = System.currentTimeMillis();\r\n    metricAppValues.put(ts - 20, 10.5);\r\n    metricAppValues.put(ts - 10, 20.5);\r\n    mApp.setType(Type.TIME_SERIES);\r\n    mApp.setValues(metricAppValues);\r\n    metricsApp.add(mApp);\r\n    entityApp.addMetrics(metricsApp);\r\n    teApp.addEntity(entityApp);\r\n    TimelineEntities teEntity = new TimelineEntities();\r\n    TimelineEntity entity = new TimelineEntity();\r\n    entity.setId(\"hello\");\r\n    entity.setType(\"world\");\r\n    entity.setCreatedTime(1425016501000L);\r\n    Set<TimelineMetric> metricsEntity = new HashSet<>();\r\n    TimelineMetric mEntity = new TimelineMetric();\r\n    mEntity.setId(\"MAP_SLOT_MILLIS\");\r\n    mEntity.addValue(ts - 20, 10.5);\r\n    metricsEntity.add(mEntity);\r\n    entity.addMetrics(metricsEntity);\r\n    teEntity.addEntity(entity);\r\n    HBaseTimelineWriterImpl hbi = null;\r\n    try {\r\n        Configuration c1 = util.getConfiguration();\r\n        hbi = new HBaseTimelineWriterImpl();\r\n        hbi.init(c1);\r\n        hbi.start();\r\n        TimelineCollectorContext context = new TimelineCollectorContext(\"c1\", \"u1\", \"f1\", \"v1\", 1002345678919L, appId);\r\n        UserGroupInformation user = UserGroupInformation.createRemoteUser(\"u1\");\r\n        try {\r\n            hbi.write(context, teApp, user);\r\n            Assert.fail(\"Expected an exception as metric values are non integral\");\r\n        } catch (IOException e) {\r\n        }\r\n        try {\r\n            hbi.write(context, teEntity, user);\r\n            Assert.fail(\"Expected an exception as metric values are non integral\");\r\n        } catch (IOException e) {\r\n        }\r\n        hbi.stop();\r\n    } finally {\r\n        if (hbi != null) {\r\n            hbi.stop();\r\n            hbi.close();\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 3,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-yarn-project\\hadoop-yarn\\hadoop-yarn-server\\hadoop-yarn-server-timelineservice-hbase-tests\\src\\test\\java\\org\\apache\\hadoop\\yarn\\server\\timelineservice\\storage",
  "methodName" : "testReadApps",
  "errType" : null,
  "containingMethodsNum" : 24,
  "sourceCodeText" : "void testReadApps() throws Exception\n{\r\n    TimelineEntity entity = reader.getEntity(new TimelineReaderContext(\"cluster1\", \"user1\", \"some_flow_name\", 1002345678919L, \"application_1111111111_2222\", TimelineEntityType.YARN_APPLICATION.toString(), null), new TimelineDataToRetrieve(null, null, EnumSet.of(Field.ALL), null, null, null));\r\n    assertNotNull(entity);\r\n    assertEquals(3, entity.getConfigs().size());\r\n    assertEquals(1, entity.getIsRelatedToEntities().size());\r\n    Set<TimelineEntity> entities = reader.getEntities(new TimelineReaderContext(\"cluster1\", \"user1\", \"some_flow_name\", 1002345678919L, null, TimelineEntityType.YARN_APPLICATION.toString(), null), new TimelineEntityFilters.Builder().build(), new TimelineDataToRetrieve(null, null, EnumSet.of(Field.ALL), null, null, null));\r\n    assertEquals(3, entities.size());\r\n    int cfgCnt = 0;\r\n    int metricCnt = 0;\r\n    int infoCnt = 0;\r\n    int eventCnt = 0;\r\n    int relatesToCnt = 0;\r\n    int isRelatedToCnt = 0;\r\n    for (TimelineEntity timelineEntity : entities) {\r\n        cfgCnt += (timelineEntity.getConfigs() == null) ? 0 : timelineEntity.getConfigs().size();\r\n        metricCnt += (timelineEntity.getMetrics() == null) ? 0 : timelineEntity.getMetrics().size();\r\n        infoCnt += (timelineEntity.getInfo() == null) ? 0 : timelineEntity.getInfo().size();\r\n        eventCnt += (timelineEntity.getEvents() == null) ? 0 : timelineEntity.getEvents().size();\r\n        relatesToCnt += (timelineEntity.getRelatesToEntities() == null) ? 0 : timelineEntity.getRelatesToEntities().size();\r\n        isRelatedToCnt += (timelineEntity.getIsRelatedToEntities() == null) ? 0 : timelineEntity.getIsRelatedToEntities().size();\r\n    }\r\n    assertEquals(5, cfgCnt);\r\n    assertEquals(3, metricCnt);\r\n    assertEquals(8, infoCnt);\r\n    assertEquals(4, eventCnt);\r\n    assertEquals(4, relatesToCnt);\r\n    assertEquals(4, isRelatedToCnt);\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-yarn-project\\hadoop-yarn\\hadoop-yarn-server\\hadoop-yarn-server-timelineservice-hbase-tests\\src\\test\\java\\org\\apache\\hadoop\\yarn\\server\\timelineservice\\storage",
  "methodName" : "testFilterAppsByCreatedTime",
  "errType" : null,
  "containingMethodsNum" : 15,
  "sourceCodeText" : "void testFilterAppsByCreatedTime() throws Exception\n{\r\n    Set<TimelineEntity> entities = reader.getEntities(new TimelineReaderContext(\"cluster1\", \"user1\", \"some_flow_name\", 1002345678919L, null, TimelineEntityType.YARN_APPLICATION.toString(), null), new TimelineEntityFilters.Builder().createdTimeBegin(1425016502000L).createTimeEnd(1425016502040L).build(), new TimelineDataToRetrieve());\r\n    assertEquals(3, entities.size());\r\n    for (TimelineEntity entity : entities) {\r\n        if (!entity.getId().equals(\"application_1111111111_2222\") && !entity.getId().equals(\"application_1111111111_3333\") && !entity.getId().equals(\"application_1111111111_4444\")) {\r\n            Assert.fail(\"Entities with ids' application_1111111111_2222, \" + \"application_1111111111_3333 and application_1111111111_4444\" + \" should be present\");\r\n        }\r\n    }\r\n    entities = reader.getEntities(new TimelineReaderContext(\"cluster1\", \"user1\", \"some_flow_name\", 1002345678919L, null, TimelineEntityType.YARN_APPLICATION.toString(), null), new TimelineEntityFilters.Builder().createdTimeBegin(1425016502015L).build(), new TimelineDataToRetrieve());\r\n    assertEquals(2, entities.size());\r\n    for (TimelineEntity entity : entities) {\r\n        if (!entity.getId().equals(\"application_1111111111_3333\") && !entity.getId().equals(\"application_1111111111_4444\")) {\r\n            Assert.fail(\"Apps with ids' application_1111111111_3333 and\" + \" application_1111111111_4444 should be present\");\r\n        }\r\n    }\r\n    entities = reader.getEntities(new TimelineReaderContext(\"cluster1\", \"user1\", \"some_flow_name\", 1002345678919L, null, TimelineEntityType.YARN_APPLICATION.toString(), null), new TimelineEntityFilters.Builder().createTimeEnd(1425016502015L).build(), new TimelineDataToRetrieve());\r\n    assertEquals(1, entities.size());\r\n    for (TimelineEntity entity : entities) {\r\n        if (!entity.getId().equals(\"application_1111111111_2222\")) {\r\n            Assert.fail(\"App with id application_1111111111_2222 should\" + \" be present\");\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-yarn-project\\hadoop-yarn\\hadoop-yarn-server\\hadoop-yarn-server-timelineservice-hbase-tests\\src\\test\\java\\org\\apache\\hadoop\\yarn\\server\\timelineservice\\storage",
  "methodName" : "testReadAppsDefaultView",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void testReadAppsDefaultView() throws Exception\n{\r\n    TimelineEntity e1 = reader.getEntity(new TimelineReaderContext(\"cluster1\", \"user1\", \"some_flow_name\", 1002345678919L, \"application_1111111111_2222\", TimelineEntityType.YARN_APPLICATION.toString(), null), new TimelineDataToRetrieve());\r\n    assertNotNull(e1);\r\n    assertEquals(1, e1.getInfo().size());\r\n    assertTrue(e1.getConfigs().isEmpty() && e1.getMetrics().isEmpty() && e1.getIsRelatedToEntities().isEmpty() && e1.getRelatesToEntities().isEmpty());\r\n    Set<TimelineEntity> es1 = reader.getEntities(new TimelineReaderContext(\"cluster1\", \"user1\", \"some_flow_name\", 1002345678919L, null, TimelineEntityType.YARN_APPLICATION.toString(), null), new TimelineEntityFilters.Builder().build(), new TimelineDataToRetrieve());\r\n    assertEquals(3, es1.size());\r\n    for (TimelineEntity e : es1) {\r\n        assertEquals(1, e1.getInfo().size());\r\n        assertTrue(e.getConfigs().isEmpty() && e.getMetrics().isEmpty() && e.getIsRelatedToEntities().isEmpty() && e.getRelatesToEntities().isEmpty());\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-yarn-project\\hadoop-yarn\\hadoop-yarn-server\\hadoop-yarn-server-timelineservice-hbase-tests\\src\\test\\java\\org\\apache\\hadoop\\yarn\\server\\timelineservice\\storage",
  "methodName" : "testReadAppsByFields",
  "errType" : null,
  "containingMethodsNum" : 12,
  "sourceCodeText" : "void testReadAppsByFields() throws Exception\n{\r\n    TimelineEntity e1 = reader.getEntity(new TimelineReaderContext(\"cluster1\", \"user1\", \"some_flow_name\", 1002345678919L, \"application_1111111111_2222\", TimelineEntityType.YARN_APPLICATION.toString(), null), new TimelineDataToRetrieve(null, null, EnumSet.of(Field.INFO, Field.CONFIGS), null, null, null));\r\n    assertNotNull(e1);\r\n    assertEquals(3, e1.getConfigs().size());\r\n    assertEquals(0, e1.getIsRelatedToEntities().size());\r\n    Set<TimelineEntity> es1 = reader.getEntities(new TimelineReaderContext(\"cluster1\", \"user1\", \"some_flow_name\", 1002345678919L, null, TimelineEntityType.YARN_APPLICATION.toString(), null), new TimelineEntityFilters.Builder().build(), new TimelineDataToRetrieve(null, null, EnumSet.of(Field.IS_RELATED_TO, Field.METRICS), null, null, null));\r\n    assertEquals(3, es1.size());\r\n    int metricsCnt = 0;\r\n    int isRelatedToCnt = 0;\r\n    int infoCnt = 0;\r\n    for (TimelineEntity entity : es1) {\r\n        metricsCnt += entity.getMetrics().size();\r\n        isRelatedToCnt += entity.getIsRelatedToEntities().size();\r\n        infoCnt += entity.getInfo().size();\r\n    }\r\n    assertEquals(3, infoCnt);\r\n    assertEquals(4, isRelatedToCnt);\r\n    assertEquals(3, metricsCnt);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-yarn-project\\hadoop-yarn\\hadoop-yarn-server\\hadoop-yarn-server-timelineservice-hbase-tests\\src\\test\\java\\org\\apache\\hadoop\\yarn\\server\\timelineservice\\storage",
  "methodName" : "testReadAppsIsRelatedTo",
  "errType" : null,
  "containingMethodsNum" : 49,
  "sourceCodeText" : "void testReadAppsIsRelatedTo() throws Exception\n{\r\n    TimelineFilterList irt = new TimelineFilterList(Operator.OR);\r\n    irt.addFilter(new TimelineKeyValuesFilter(TimelineCompareOp.EQUAL, \"task\", new HashSet<Object>(Arrays.asList(\"relatedto1\"))));\r\n    irt.addFilter(new TimelineKeyValuesFilter(TimelineCompareOp.EQUAL, \"task2\", new HashSet<Object>(Arrays.asList(\"relatedto4\"))));\r\n    Set<TimelineEntity> entities = reader.getEntities(new TimelineReaderContext(\"cluster1\", \"user1\", \"some_flow_name\", 1002345678919L, null, TimelineEntityType.YARN_APPLICATION.toString(), null), new TimelineEntityFilters.Builder().isRelatedTo(irt).build(), new TimelineDataToRetrieve(null, null, EnumSet.of(Field.ALL), null, null, null));\r\n    assertEquals(2, entities.size());\r\n    int isRelatedToCnt = 0;\r\n    for (TimelineEntity timelineEntity : entities) {\r\n        isRelatedToCnt += timelineEntity.getIsRelatedToEntities().size();\r\n        if (!timelineEntity.getId().equals(\"application_1111111111_2222\") && !timelineEntity.getId().equals(\"application_1111111111_3333\")) {\r\n            Assert.fail(\"Entity ids' should have been application_1111111111_2222\" + \" and application_1111111111_3333\");\r\n        }\r\n    }\r\n    assertEquals(3, isRelatedToCnt);\r\n    TimelineFilterList irt1 = new TimelineFilterList();\r\n    irt1.addFilter(new TimelineKeyValuesFilter(TimelineCompareOp.EQUAL, \"task1\", new HashSet<Object>(Arrays.asList(\"relatedto3\"))));\r\n    irt1.addFilter(new TimelineKeyValuesFilter(TimelineCompareOp.NOT_EQUAL, \"task1\", new HashSet<Object>(Arrays.asList(\"relatedto5\"))));\r\n    entities = reader.getEntities(new TimelineReaderContext(\"cluster1\", \"user1\", \"some_flow_name\", 1002345678919L, null, TimelineEntityType.YARN_APPLICATION.toString(), null), new TimelineEntityFilters.Builder().isRelatedTo(irt1).build(), new TimelineDataToRetrieve());\r\n    assertEquals(1, entities.size());\r\n    isRelatedToCnt = 0;\r\n    for (TimelineEntity timelineEntity : entities) {\r\n        isRelatedToCnt += timelineEntity.getIsRelatedToEntities().size();\r\n        if (!timelineEntity.getId().equals(\"application_1111111111_4444\")) {\r\n            Assert.fail(\"Entity id should have been application_1111111111_4444\");\r\n        }\r\n    }\r\n    assertEquals(0, isRelatedToCnt);\r\n    TimelineFilterList irt2 = new TimelineFilterList(Operator.OR);\r\n    irt2.addFilter(new TimelineKeyValuesFilter(TimelineCompareOp.EQUAL, \"task\", new HashSet<Object>(Arrays.asList(\"relatedto1\"))));\r\n    irt2.addFilter(new TimelineKeyValuesFilter(TimelineCompareOp.EQUAL, \"task2\", new HashSet<Object>(Arrays.asList(\"relatedto4\"))));\r\n    entities = reader.getEntities(new TimelineReaderContext(\"cluster1\", \"user1\", \"some_flow_name\", 1002345678919L, null, TimelineEntityType.YARN_APPLICATION.toString(), null), new TimelineEntityFilters.Builder().isRelatedTo(irt2).build(), new TimelineDataToRetrieve());\r\n    assertEquals(2, entities.size());\r\n    isRelatedToCnt = 0;\r\n    for (TimelineEntity timelineEntity : entities) {\r\n        isRelatedToCnt += timelineEntity.getIsRelatedToEntities().size();\r\n        if (!timelineEntity.getId().equals(\"application_1111111111_2222\") && !timelineEntity.getId().equals(\"application_1111111111_3333\")) {\r\n            Assert.fail(\"Entity ids' should have been application_1111111111_2222\" + \" and application_1111111111_3333\");\r\n        }\r\n    }\r\n    assertEquals(0, isRelatedToCnt);\r\n    TimelineFilterList irt3 = new TimelineFilterList();\r\n    irt3.addFilter(new TimelineKeyValuesFilter(TimelineCompareOp.EQUAL, \"task1\", new HashSet<Object>(Arrays.asList(\"relatedto3\", \"relatedto5\"))));\r\n    entities = reader.getEntities(new TimelineReaderContext(\"cluster1\", \"user1\", \"some_flow_name\", 1002345678919L, null, TimelineEntityType.YARN_APPLICATION.toString(), null), new TimelineEntityFilters.Builder().isRelatedTo(irt3).build(), new TimelineDataToRetrieve());\r\n    assertEquals(1, entities.size());\r\n    isRelatedToCnt = 0;\r\n    for (TimelineEntity timelineEntity : entities) {\r\n        isRelatedToCnt += timelineEntity.getIsRelatedToEntities().size();\r\n        if (!timelineEntity.getId().equals(\"application_1111111111_3333\")) {\r\n            Assert.fail(\"Entity id should have been application_1111111111_3333\");\r\n        }\r\n    }\r\n    assertEquals(0, isRelatedToCnt);\r\n    TimelineFilterList irt4 = new TimelineFilterList();\r\n    irt4.addFilter(new TimelineKeyValuesFilter(TimelineCompareOp.EQUAL, \"task1\", new HashSet<Object>(Arrays.asList(\"relatedto3\"))));\r\n    irt4.addFilter(new TimelineKeyValuesFilter(TimelineCompareOp.EQUAL, \"dummy_task\", new HashSet<Object>(Arrays.asList(\"relatedto5\"))));\r\n    entities = reader.getEntities(new TimelineReaderContext(\"cluster1\", \"user1\", \"some_flow_name\", 1002345678919L, null, TimelineEntityType.YARN_APPLICATION.toString(), null), new TimelineEntityFilters.Builder().isRelatedTo(irt4).build(), new TimelineDataToRetrieve());\r\n    assertEquals(0, entities.size());\r\n    TimelineFilterList irt5 = new TimelineFilterList();\r\n    irt5.addFilter(new TimelineKeyValuesFilter(TimelineCompareOp.EQUAL, \"task1\", new HashSet<Object>(Arrays.asList(\"relatedto3\", \"relatedto7\"))));\r\n    entities = reader.getEntities(new TimelineReaderContext(\"cluster1\", \"user1\", \"some_flow_name\", 1002345678919L, null, TimelineEntityType.YARN_APPLICATION.toString(), null), new TimelineEntityFilters.Builder().isRelatedTo(irt5).build(), new TimelineDataToRetrieve());\r\n    assertEquals(0, entities.size());\r\n    TimelineFilterList list1 = new TimelineFilterList();\r\n    list1.addFilter(new TimelineKeyValuesFilter(TimelineCompareOp.EQUAL, \"task\", new HashSet<Object>(Arrays.asList(\"relatedto1\"))));\r\n    list1.addFilter(new TimelineKeyValuesFilter(TimelineCompareOp.EQUAL, \"dummy_task\", new HashSet<Object>(Arrays.asList(\"relatedto4\"))));\r\n    TimelineFilterList list2 = new TimelineFilterList();\r\n    list2.addFilter(new TimelineKeyValuesFilter(TimelineCompareOp.EQUAL, \"task2\", new HashSet<Object>(Arrays.asList(\"relatedto4\"))));\r\n    TimelineFilterList irt6 = new TimelineFilterList(Operator.OR, list1, list2);\r\n    entities = reader.getEntities(new TimelineReaderContext(\"cluster1\", \"user1\", \"some_flow_name\", 1002345678919L, null, TimelineEntityType.YARN_APPLICATION.toString(), null), new TimelineEntityFilters.Builder().isRelatedTo(irt6).build(), new TimelineDataToRetrieve());\r\n    assertEquals(1, entities.size());\r\n    isRelatedToCnt = 0;\r\n    for (TimelineEntity timelineEntity : entities) {\r\n        isRelatedToCnt += timelineEntity.getIsRelatedToEntities().size();\r\n        if (!timelineEntity.getId().equals(\"application_1111111111_3333\")) {\r\n            Assert.fail(\"Entity id should have been application_1111111111_3333\");\r\n        }\r\n    }\r\n    assertEquals(0, isRelatedToCnt);\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-yarn-project\\hadoop-yarn\\hadoop-yarn-server\\hadoop-yarn-server-timelineservice-hbase-tests\\src\\test\\java\\org\\apache\\hadoop\\yarn\\server\\timelineservice\\storage",
  "methodName" : "testReadAppsRelatesTo",
  "errType" : null,
  "containingMethodsNum" : 60,
  "sourceCodeText" : "void testReadAppsRelatesTo() throws Exception\n{\r\n    TimelineFilterList rt = new TimelineFilterList(Operator.OR);\r\n    rt.addFilter(new TimelineKeyValuesFilter(TimelineCompareOp.EQUAL, \"container2\", new HashSet<Object>(Arrays.asList(\"relatesto7\"))));\r\n    rt.addFilter(new TimelineKeyValuesFilter(TimelineCompareOp.EQUAL, \"container1\", new HashSet<Object>(Arrays.asList(\"relatesto4\"))));\r\n    Set<TimelineEntity> entities = reader.getEntities(new TimelineReaderContext(\"cluster1\", \"user1\", \"some_flow_name\", 1002345678919L, null, TimelineEntityType.YARN_APPLICATION.toString(), null), new TimelineEntityFilters.Builder().relatesTo(rt).build(), new TimelineDataToRetrieve(null, null, EnumSet.of(Field.ALL), null, null, null));\r\n    assertEquals(2, entities.size());\r\n    int relatesToCnt = 0;\r\n    for (TimelineEntity timelineEntity : entities) {\r\n        relatesToCnt += timelineEntity.getRelatesToEntities().size();\r\n        if (!timelineEntity.getId().equals(\"application_1111111111_2222\") && !timelineEntity.getId().equals(\"application_1111111111_4444\")) {\r\n            Assert.fail(\"Entity ids' should have been application_1111111111_2222\" + \" and application_1111111111_4444\");\r\n        }\r\n    }\r\n    assertEquals(3, relatesToCnt);\r\n    TimelineFilterList rt1 = new TimelineFilterList();\r\n    rt1.addFilter(new TimelineKeyValuesFilter(TimelineCompareOp.EQUAL, \"container\", new HashSet<Object>(Arrays.asList(\"relatesto1\"))));\r\n    rt1.addFilter(new TimelineKeyValuesFilter(TimelineCompareOp.NOT_EQUAL, \"container\", new HashSet<Object>(Arrays.asList(\"relatesto3\"))));\r\n    entities = reader.getEntities(new TimelineReaderContext(\"cluster1\", \"user1\", \"some_flow_name\", 1002345678919L, null, TimelineEntityType.YARN_APPLICATION.toString(), null), new TimelineEntityFilters.Builder().relatesTo(rt1).build(), new TimelineDataToRetrieve());\r\n    assertEquals(1, entities.size());\r\n    relatesToCnt = 0;\r\n    for (TimelineEntity timelineEntity : entities) {\r\n        relatesToCnt += timelineEntity.getRelatesToEntities().size();\r\n        if (!timelineEntity.getId().equals(\"application_1111111111_3333\")) {\r\n            Assert.fail(\"Entity id should have been application_1111111111_3333\");\r\n        }\r\n    }\r\n    assertEquals(0, relatesToCnt);\r\n    TimelineFilterList rt2 = new TimelineFilterList(Operator.OR);\r\n    rt2.addFilter(new TimelineKeyValuesFilter(TimelineCompareOp.EQUAL, \"container2\", new HashSet<Object>(Arrays.asList(\"relatesto7\"))));\r\n    rt2.addFilter(new TimelineKeyValuesFilter(TimelineCompareOp.EQUAL, \"container1\", new HashSet<Object>(Arrays.asList(\"relatesto4\"))));\r\n    entities = reader.getEntities(new TimelineReaderContext(\"cluster1\", \"user1\", \"some_flow_name\", 1002345678919L, null, TimelineEntityType.YARN_APPLICATION.toString(), null), new TimelineEntityFilters.Builder().relatesTo(rt2).build(), new TimelineDataToRetrieve());\r\n    assertEquals(2, entities.size());\r\n    relatesToCnt = 0;\r\n    for (TimelineEntity timelineEntity : entities) {\r\n        relatesToCnt += timelineEntity.getRelatesToEntities().size();\r\n        if (!timelineEntity.getId().equals(\"application_1111111111_2222\") && !timelineEntity.getId().equals(\"application_1111111111_4444\")) {\r\n            Assert.fail(\"Entity ids' should have been application_1111111111_2222\" + \" and application_1111111111_4444\");\r\n        }\r\n    }\r\n    assertEquals(0, relatesToCnt);\r\n    TimelineFilterList rt3 = new TimelineFilterList();\r\n    rt3.addFilter(new TimelineKeyValuesFilter(TimelineCompareOp.EQUAL, \"container\", new HashSet<Object>(Arrays.asList(\"relatesto1\", \"relatesto3\"))));\r\n    entities = reader.getEntities(new TimelineReaderContext(\"cluster1\", \"user1\", \"some_flow_name\", 1002345678919L, null, TimelineEntityType.YARN_APPLICATION.toString(), null), new TimelineEntityFilters.Builder().relatesTo(rt3).build(), new TimelineDataToRetrieve());\r\n    assertEquals(1, entities.size());\r\n    relatesToCnt = 0;\r\n    for (TimelineEntity timelineEntity : entities) {\r\n        relatesToCnt += timelineEntity.getRelatesToEntities().size();\r\n        if (!timelineEntity.getId().equals(\"application_1111111111_2222\")) {\r\n            Assert.fail(\"Entity id should have been application_1111111111_2222\");\r\n        }\r\n    }\r\n    assertEquals(0, relatesToCnt);\r\n    TimelineFilterList rt4 = new TimelineFilterList();\r\n    rt4.addFilter(new TimelineKeyValuesFilter(TimelineCompareOp.EQUAL, \"container\", new HashSet<Object>(Arrays.asList(\"relatesto1\"))));\r\n    rt4.addFilter(new TimelineKeyValuesFilter(TimelineCompareOp.EQUAL, \"dummy_container\", new HashSet<Object>(Arrays.asList(\"relatesto5\"))));\r\n    entities = reader.getEntities(new TimelineReaderContext(\"cluster1\", \"user1\", \"some_flow_name\", 1002345678919L, null, TimelineEntityType.YARN_APPLICATION.toString(), null), new TimelineEntityFilters.Builder().relatesTo(rt4).build(), new TimelineDataToRetrieve());\r\n    assertEquals(0, entities.size());\r\n    TimelineFilterList rt5 = new TimelineFilterList();\r\n    rt5.addFilter(new TimelineKeyValuesFilter(TimelineCompareOp.EQUAL, \"container\", new HashSet<Object>(Arrays.asList(\"relatedto1\", \"relatesto8\"))));\r\n    entities = reader.getEntities(new TimelineReaderContext(\"cluster1\", \"user1\", \"some_flow_name\", 1002345678919L, null, TimelineEntityType.YARN_APPLICATION.toString(), null), new TimelineEntityFilters.Builder().relatesTo(rt5).build(), new TimelineDataToRetrieve());\r\n    assertEquals(0, entities.size());\r\n    TimelineFilterList list1 = new TimelineFilterList();\r\n    list1.addFilter(new TimelineKeyValuesFilter(TimelineCompareOp.EQUAL, \"container2\", new HashSet<Object>(Arrays.asList(\"relatesto7\"))));\r\n    list1.addFilter(new TimelineKeyValuesFilter(TimelineCompareOp.EQUAL, \"dummy_container\", new HashSet<Object>(Arrays.asList(\"relatesto4\"))));\r\n    TimelineFilterList list2 = new TimelineFilterList();\r\n    list2.addFilter(new TimelineKeyValuesFilter(TimelineCompareOp.EQUAL, \"container1\", new HashSet<Object>(Arrays.asList(\"relatesto4\"))));\r\n    TimelineFilterList rt6 = new TimelineFilterList(Operator.OR, list1, list2);\r\n    entities = reader.getEntities(new TimelineReaderContext(\"cluster1\", \"user1\", \"some_flow_name\", 1002345678919L, null, TimelineEntityType.YARN_APPLICATION.toString(), null), new TimelineEntityFilters.Builder().relatesTo(rt6).build(), new TimelineDataToRetrieve());\r\n    assertEquals(1, entities.size());\r\n    relatesToCnt = 0;\r\n    for (TimelineEntity timelineEntity : entities) {\r\n        relatesToCnt += timelineEntity.getRelatesToEntities().size();\r\n        if (!timelineEntity.getId().equals(\"application_1111111111_2222\")) {\r\n            Assert.fail(\"Entity id should have been application_1111111111_2222\");\r\n        }\r\n    }\r\n    assertEquals(0, relatesToCnt);\r\n    TimelineFilterList list3 = new TimelineFilterList();\r\n    list3.addFilter(new TimelineKeyValuesFilter(TimelineCompareOp.EQUAL, \"container\", new HashSet<Object>(Arrays.asList(\"relatesto1\"))));\r\n    list3.addFilter(new TimelineKeyValuesFilter(TimelineCompareOp.EQUAL, \"container1\", new HashSet<Object>(Arrays.asList(\"relatesto4\"))));\r\n    TimelineFilterList list4 = new TimelineFilterList();\r\n    list4.addFilter(new TimelineKeyValuesFilter(TimelineCompareOp.EQUAL, \"container\", new HashSet<Object>(Arrays.asList(\"relatesto1\"))));\r\n    list4.addFilter(new TimelineKeyValuesFilter(TimelineCompareOp.EQUAL, \"container\", new HashSet<Object>(Arrays.asList(\"relatesto2\"))));\r\n    TimelineFilterList combinedList = new TimelineFilterList(Operator.OR, list3, list4);\r\n    TimelineFilterList rt7 = new TimelineFilterList(Operator.AND, combinedList, new TimelineKeyValuesFilter(TimelineCompareOp.NOT_EQUAL, \"container\", new HashSet<Object>(Arrays.asList(\"relatesto3\"))));\r\n    entities = reader.getEntities(new TimelineReaderContext(\"cluster1\", \"user1\", \"some_flow_name\", 1002345678919L, null, TimelineEntityType.YARN_APPLICATION.toString(), null), new TimelineEntityFilters.Builder().relatesTo(rt7).build(), new TimelineDataToRetrieve());\r\n    assertEquals(1, entities.size());\r\n    relatesToCnt = 0;\r\n    for (TimelineEntity timelineEntity : entities) {\r\n        relatesToCnt += timelineEntity.getRelatesToEntities().size();\r\n        if (!timelineEntity.getId().equals(\"application_1111111111_3333\")) {\r\n            Assert.fail(\"Entity id should have been application_1111111111_3333\");\r\n        }\r\n    }\r\n    assertEquals(0, relatesToCnt);\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-yarn-project\\hadoop-yarn\\hadoop-yarn-server\\hadoop-yarn-server-timelineservice-hbase-tests\\src\\test\\java\\org\\apache\\hadoop\\yarn\\server\\timelineservice\\storage",
  "methodName" : "testReadAppsRelationsAndEventFiltersDefaultView",
  "errType" : null,
  "containingMethodsNum" : 15,
  "sourceCodeText" : "void testReadAppsRelationsAndEventFiltersDefaultView() throws Exception\n{\r\n    TimelineFilterList eventFilter = new TimelineFilterList();\r\n    eventFilter.addFilter(new TimelineExistsFilter(TimelineCompareOp.NOT_EQUAL, \"end_event\"));\r\n    TimelineFilterList relatesTo = new TimelineFilterList(Operator.OR);\r\n    relatesTo.addFilter(new TimelineKeyValuesFilter(TimelineCompareOp.EQUAL, \"container2\", new HashSet<Object>(Arrays.asList(\"relatesto7\"))));\r\n    relatesTo.addFilter(new TimelineKeyValuesFilter(TimelineCompareOp.EQUAL, \"container1\", new HashSet<Object>(Arrays.asList(\"relatesto4\"))));\r\n    TimelineFilterList isRelatedTo = new TimelineFilterList();\r\n    isRelatedTo.addFilter(new TimelineKeyValuesFilter(TimelineCompareOp.EQUAL, \"task1\", new HashSet<Object>(Arrays.asList(\"relatedto3\"))));\r\n    isRelatedTo.addFilter(new TimelineKeyValuesFilter(TimelineCompareOp.NOT_EQUAL, \"task1\", new HashSet<Object>(Arrays.asList(\"relatedto5\"))));\r\n    Set<TimelineEntity> entities = reader.getEntities(new TimelineReaderContext(\"cluster1\", \"user1\", \"some_flow_name\", 1002345678919L, null, TimelineEntityType.YARN_APPLICATION.toString(), null), new TimelineEntityFilters.Builder().relatesTo(relatesTo).isRelatedTo(isRelatedTo).eventFilters(eventFilter).build(), new TimelineDataToRetrieve());\r\n    assertEquals(1, entities.size());\r\n    int eventCnt = 0;\r\n    int isRelatedToCnt = 0;\r\n    int relatesToCnt = 0;\r\n    for (TimelineEntity timelineEntity : entities) {\r\n        eventCnt += timelineEntity.getEvents().size();\r\n        isRelatedToCnt += timelineEntity.getIsRelatedToEntities().size();\r\n        relatesToCnt += timelineEntity.getRelatesToEntities().size();\r\n        if (!timelineEntity.getId().equals(\"application_1111111111_4444\")) {\r\n            Assert.fail(\"Entity id should have been application_1111111111_4444\");\r\n        }\r\n    }\r\n    assertEquals(0, eventCnt);\r\n    assertEquals(0, isRelatedToCnt);\r\n    assertEquals(0, relatesToCnt);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-yarn-project\\hadoop-yarn\\hadoop-yarn-server\\hadoop-yarn-server-timelineservice-hbase-tests\\src\\test\\java\\org\\apache\\hadoop\\yarn\\server\\timelineservice\\storage",
  "methodName" : "testReadAppsConfigFilters",
  "errType" : null,
  "containingMethodsNum" : 24,
  "sourceCodeText" : "void testReadAppsConfigFilters() throws Exception\n{\r\n    TimelineFilterList list1 = new TimelineFilterList();\r\n    list1.addFilter(new TimelineKeyValueFilter(TimelineCompareOp.EQUAL, \"cfg_param1\", \"value1\"));\r\n    list1.addFilter(new TimelineKeyValueFilter(TimelineCompareOp.EQUAL, \"cfg_param2\", \"value2\"));\r\n    TimelineFilterList list2 = new TimelineFilterList();\r\n    list2.addFilter(new TimelineKeyValueFilter(TimelineCompareOp.EQUAL, \"cfg_param1\", \"value3\"));\r\n    list2.addFilter(new TimelineKeyValueFilter(TimelineCompareOp.EQUAL, \"config_param2\", \"value2\"));\r\n    TimelineFilterList confFilterList = new TimelineFilterList(Operator.OR, list1, list2);\r\n    Set<TimelineEntity> entities = reader.getEntities(new TimelineReaderContext(\"cluster1\", \"user1\", \"some_flow_name\", 1002345678919L, null, TimelineEntityType.YARN_APPLICATION.toString(), null), new TimelineEntityFilters.Builder().configFilters(confFilterList).build(), new TimelineDataToRetrieve(null, null, EnumSet.of(Field.CONFIGS), null, null, null));\r\n    assertEquals(2, entities.size());\r\n    int cfgCnt = 0;\r\n    for (TimelineEntity entity : entities) {\r\n        cfgCnt += entity.getConfigs().size();\r\n    }\r\n    assertEquals(5, cfgCnt);\r\n    entities = reader.getEntities(new TimelineReaderContext(\"cluster1\", \"user1\", \"some_flow_name\", 1002345678919L, null, TimelineEntityType.YARN_APPLICATION.toString(), null), new TimelineEntityFilters.Builder().configFilters(confFilterList).build(), new TimelineDataToRetrieve(null, null, EnumSet.of(Field.ALL), null, null, null));\r\n    assertEquals(2, entities.size());\r\n    cfgCnt = 0;\r\n    for (TimelineEntity entity : entities) {\r\n        cfgCnt += entity.getConfigs().size();\r\n    }\r\n    assertEquals(5, cfgCnt);\r\n    TimelineFilterList confFilterList1 = new TimelineFilterList(new TimelineKeyValueFilter(TimelineCompareOp.NOT_EQUAL, \"cfg_param1\", \"value1\"));\r\n    entities = reader.getEntities(new TimelineReaderContext(\"cluster1\", \"user1\", \"some_flow_name\", 1002345678919L, null, TimelineEntityType.YARN_APPLICATION.toString(), null), new TimelineEntityFilters.Builder().configFilters(confFilterList1).build(), new TimelineDataToRetrieve(null, null, EnumSet.of(Field.CONFIGS), null, null, null));\r\n    assertEquals(1, entities.size());\r\n    cfgCnt = 0;\r\n    for (TimelineEntity entity : entities) {\r\n        cfgCnt += entity.getConfigs().size();\r\n    }\r\n    assertEquals(3, cfgCnt);\r\n    TimelineFilterList confFilterList2 = new TimelineFilterList(new TimelineKeyValueFilter(TimelineCompareOp.NOT_EQUAL, \"cfg_param1\", \"value1\"), new TimelineKeyValueFilter(TimelineCompareOp.NOT_EQUAL, \"config_param2\", \"value2\"));\r\n    entities = reader.getEntities(new TimelineReaderContext(\"cluster1\", \"user1\", \"some_flow_name\", 1002345678919L, null, TimelineEntityType.YARN_APPLICATION.toString(), null), new TimelineEntityFilters.Builder().configFilters(confFilterList2).build(), new TimelineDataToRetrieve(null, null, EnumSet.of(Field.CONFIGS), null, null, null));\r\n    assertEquals(0, entities.size());\r\n    TimelineFilterList confFilterList3 = new TimelineFilterList(new TimelineKeyValueFilter(TimelineCompareOp.EQUAL, \"dummy_config\", \"value1\"));\r\n    entities = reader.getEntities(new TimelineReaderContext(\"cluster1\", \"user1\", \"some_flow_name\", 1002345678919L, null, TimelineEntityType.YARN_APPLICATION.toString(), null), new TimelineEntityFilters.Builder().configFilters(confFilterList3).build(), new TimelineDataToRetrieve(null, null, EnumSet.of(Field.CONFIGS), null, null, null));\r\n    assertEquals(0, entities.size());\r\n    TimelineFilterList confFilterList4 = new TimelineFilterList(new TimelineKeyValueFilter(TimelineCompareOp.NOT_EQUAL, \"dummy_config\", \"value1\"));\r\n    entities = reader.getEntities(new TimelineReaderContext(\"cluster1\", \"user1\", \"some_flow_name\", 1002345678919L, null, TimelineEntityType.YARN_APPLICATION.toString(), null), new TimelineEntityFilters.Builder().configFilters(confFilterList4).build(), new TimelineDataToRetrieve(null, null, EnumSet.of(Field.CONFIGS), null, null, null));\r\n    assertEquals(0, entities.size());\r\n    TimelineFilterList confFilterList5 = new TimelineFilterList(new TimelineKeyValueFilter(TimelineCompareOp.NOT_EQUAL, \"dummy_config\", \"value1\", false));\r\n    entities = reader.getEntities(new TimelineReaderContext(\"cluster1\", \"user1\", \"some_flow_name\", 1002345678919L, null, TimelineEntityType.YARN_APPLICATION.toString(), null), new TimelineEntityFilters.Builder().configFilters(confFilterList5).build(), new TimelineDataToRetrieve(null, null, EnumSet.of(Field.CONFIGS), null, null, null));\r\n    assertEquals(3, entities.size());\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-yarn-project\\hadoop-yarn\\hadoop-yarn-server\\hadoop-yarn-server-timelineservice-hbase-tests\\src\\test\\java\\org\\apache\\hadoop\\yarn\\server\\timelineservice\\storage",
  "methodName" : "testReadAppsEventFilters",
  "errType" : null,
  "containingMethodsNum" : 45,
  "sourceCodeText" : "void testReadAppsEventFilters() throws Exception\n{\r\n    TimelineFilterList ef = new TimelineFilterList();\r\n    ef.addFilter(new TimelineExistsFilter(TimelineCompareOp.EQUAL, \"update_event\"));\r\n    ef.addFilter(new TimelineExistsFilter(TimelineCompareOp.NOT_EQUAL, \"end_event\"));\r\n    Set<TimelineEntity> entities = reader.getEntities(new TimelineReaderContext(\"cluster1\", \"user1\", \"some_flow_name\", 1002345678919L, null, TimelineEntityType.YARN_APPLICATION.toString(), null), new TimelineEntityFilters.Builder().eventFilters(ef).build(), new TimelineDataToRetrieve(null, null, EnumSet.of(Field.ALL), null, null, null));\r\n    assertEquals(1, entities.size());\r\n    int eventCnt = 0;\r\n    for (TimelineEntity timelineEntity : entities) {\r\n        eventCnt += timelineEntity.getEvents().size();\r\n        if (!timelineEntity.getId().equals(\"application_1111111111_4444\")) {\r\n            Assert.fail(\"Entity id should have been application_1111111111_4444\");\r\n        }\r\n    }\r\n    assertEquals(1, eventCnt);\r\n    TimelineFilterList ef1 = new TimelineFilterList();\r\n    ef1.addFilter(new TimelineExistsFilter(TimelineCompareOp.EQUAL, \"update_event\"));\r\n    ef1.addFilter(new TimelineExistsFilter(TimelineCompareOp.NOT_EQUAL, \"end_event\"));\r\n    entities = reader.getEntities(new TimelineReaderContext(\"cluster1\", \"user1\", \"some_flow_name\", 1002345678919L, null, TimelineEntityType.YARN_APPLICATION.toString(), null), new TimelineEntityFilters.Builder().eventFilters(ef1).build(), new TimelineDataToRetrieve());\r\n    assertEquals(1, entities.size());\r\n    eventCnt = 0;\r\n    for (TimelineEntity timelineEntity : entities) {\r\n        eventCnt += timelineEntity.getEvents().size();\r\n        if (!timelineEntity.getId().equals(\"application_1111111111_4444\")) {\r\n            Assert.fail(\"Entity id should have been application_1111111111_4444\");\r\n        }\r\n    }\r\n    assertEquals(0, eventCnt);\r\n    TimelineFilterList ef2 = new TimelineFilterList();\r\n    ef2.addFilter(new TimelineExistsFilter(TimelineCompareOp.NOT_EQUAL, \"end_event\"));\r\n    entities = reader.getEntities(new TimelineReaderContext(\"cluster1\", \"user1\", \"some_flow_name\", 1002345678919L, null, TimelineEntityType.YARN_APPLICATION.toString(), null), new TimelineEntityFilters.Builder().eventFilters(ef2).build(), new TimelineDataToRetrieve());\r\n    assertEquals(2, entities.size());\r\n    eventCnt = 0;\r\n    for (TimelineEntity timelineEntity : entities) {\r\n        eventCnt += timelineEntity.getEvents().size();\r\n        if (!timelineEntity.getId().equals(\"application_1111111111_2222\") && !timelineEntity.getId().equals(\"application_1111111111_4444\")) {\r\n            Assert.fail(\"Entity ids' should have been application_1111111111_2222\" + \" and application_1111111111_4444\");\r\n        }\r\n    }\r\n    assertEquals(0, eventCnt);\r\n    TimelineFilterList ef3 = new TimelineFilterList();\r\n    ef3.addFilter(new TimelineExistsFilter(TimelineCompareOp.EQUAL, \"update_event\"));\r\n    ef3.addFilter(new TimelineExistsFilter(TimelineCompareOp.EQUAL, \"dummy_event\"));\r\n    entities = reader.getEntities(new TimelineReaderContext(\"cluster1\", \"user1\", \"some_flow_name\", 1002345678919L, null, TimelineEntityType.YARN_APPLICATION.toString(), null), new TimelineEntityFilters.Builder().eventFilters(ef3).build(), new TimelineDataToRetrieve());\r\n    assertEquals(0, entities.size());\r\n    TimelineFilterList list1 = new TimelineFilterList();\r\n    list1.addFilter(new TimelineExistsFilter(TimelineCompareOp.EQUAL, \"update_event\"));\r\n    list1.addFilter(new TimelineExistsFilter(TimelineCompareOp.EQUAL, \"dummy_event\"));\r\n    TimelineFilterList list2 = new TimelineFilterList();\r\n    list2.addFilter(new TimelineExistsFilter(TimelineCompareOp.EQUAL, \"start_event\"));\r\n    TimelineFilterList ef4 = new TimelineFilterList(Operator.OR, list1, list2);\r\n    entities = reader.getEntities(new TimelineReaderContext(\"cluster1\", \"user1\", \"some_flow_name\", 1002345678919L, null, TimelineEntityType.YARN_APPLICATION.toString(), null), new TimelineEntityFilters.Builder().eventFilters(ef4).build(), new TimelineDataToRetrieve());\r\n    assertEquals(1, entities.size());\r\n    eventCnt = 0;\r\n    for (TimelineEntity timelineEntity : entities) {\r\n        eventCnt += timelineEntity.getEvents().size();\r\n        if (!timelineEntity.getId().equals(\"application_1111111111_2222\")) {\r\n            Assert.fail(\"Entity id should have been application_1111111111_2222\");\r\n        }\r\n    }\r\n    assertEquals(0, eventCnt);\r\n    TimelineFilterList ef5 = new TimelineFilterList();\r\n    ef5.addFilter(new TimelineExistsFilter(TimelineCompareOp.NOT_EQUAL, \"update_event\"));\r\n    ef5.addFilter(new TimelineExistsFilter(TimelineCompareOp.NOT_EQUAL, \"end_event\"));\r\n    entities = reader.getEntities(new TimelineReaderContext(\"cluster1\", \"user1\", \"some_flow_name\", 1002345678919L, null, TimelineEntityType.YARN_APPLICATION.toString(), null), new TimelineEntityFilters.Builder().eventFilters(ef5).build(), new TimelineDataToRetrieve());\r\n    assertEquals(1, entities.size());\r\n    eventCnt = 0;\r\n    for (TimelineEntity timelineEntity : entities) {\r\n        eventCnt += timelineEntity.getEvents().size();\r\n        if (!timelineEntity.getId().equals(\"application_1111111111_2222\")) {\r\n            Assert.fail(\"Entity id should have been application_1111111111_2222\");\r\n        }\r\n    }\r\n    assertEquals(0, eventCnt);\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-yarn-project\\hadoop-yarn\\hadoop-yarn-server\\hadoop-yarn-server-timelineservice-hbase-tests\\src\\test\\java\\org\\apache\\hadoop\\yarn\\server\\timelineservice\\storage",
  "methodName" : "testReadAppsConfigPrefix",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void testReadAppsConfigPrefix() throws Exception\n{\r\n    TimelineFilterList list = new TimelineFilterList(Operator.OR, new TimelinePrefixFilter(TimelineCompareOp.EQUAL, \"cfg_\"));\r\n    TimelineEntity e1 = reader.getEntity(new TimelineReaderContext(\"cluster1\", \"user1\", \"some_flow_name\", 1002345678919L, \"application_1111111111_2222\", TimelineEntityType.YARN_APPLICATION.toString(), null), new TimelineDataToRetrieve(list, null, null, null, null, null));\r\n    assertNotNull(e1);\r\n    assertEquals(1, e1.getConfigs().size());\r\n    Set<TimelineEntity> es1 = reader.getEntities(new TimelineReaderContext(\"cluster1\", \"user1\", \"some_flow_name\", 1002345678919L, null, TimelineEntityType.YARN_APPLICATION.toString(), null), new TimelineEntityFilters.Builder().build(), new TimelineDataToRetrieve(list, null, null, null, null, null));\r\n    int cfgCnt = 0;\r\n    for (TimelineEntity entity : es1) {\r\n        cfgCnt += entity.getConfigs().size();\r\n        for (String confKey : entity.getConfigs().keySet()) {\r\n            assertTrue(\"Config key returned should start with cfg_\", confKey.startsWith(\"cfg_\"));\r\n        }\r\n    }\r\n    assertEquals(3, cfgCnt);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-yarn-project\\hadoop-yarn\\hadoop-yarn-server\\hadoop-yarn-server-timelineservice-hbase-tests\\src\\test\\java\\org\\apache\\hadoop\\yarn\\server\\timelineservice\\storage",
  "methodName" : "testReadAppsConfigFilterPrefix",
  "errType" : null,
  "containingMethodsNum" : 17,
  "sourceCodeText" : "void testReadAppsConfigFilterPrefix() throws Exception\n{\r\n    TimelineFilterList confFilterList = new TimelineFilterList();\r\n    confFilterList.addFilter(new TimelineKeyValueFilter(TimelineCompareOp.EQUAL, \"cfg_param1\", \"value1\"));\r\n    TimelineFilterList list = new TimelineFilterList(Operator.OR, new TimelinePrefixFilter(TimelineCompareOp.EQUAL, \"cfg_\"));\r\n    Set<TimelineEntity> entities = reader.getEntities(new TimelineReaderContext(\"cluster1\", \"user1\", \"some_flow_name\", 1002345678919L, null, TimelineEntityType.YARN_APPLICATION.toString(), null), new TimelineEntityFilters.Builder().configFilters(confFilterList).build(), new TimelineDataToRetrieve(list, null, null, null, null, null));\r\n    assertEquals(1, entities.size());\r\n    int cfgCnt = 0;\r\n    for (TimelineEntity entity : entities) {\r\n        cfgCnt += entity.getConfigs().size();\r\n        for (String confKey : entity.getConfigs().keySet()) {\r\n            assertTrue(\"Config key returned should start with cfg_\", confKey.startsWith(\"cfg_\"));\r\n        }\r\n    }\r\n    assertEquals(2, cfgCnt);\r\n    TimelineFilterList list1 = new TimelineFilterList();\r\n    list1.addFilter(new TimelineKeyValueFilter(TimelineCompareOp.EQUAL, \"cfg_param1\", \"value1\"));\r\n    list1.addFilter(new TimelineKeyValueFilter(TimelineCompareOp.EQUAL, \"cfg_param2\", \"value2\"));\r\n    TimelineFilterList list2 = new TimelineFilterList();\r\n    list2.addFilter(new TimelineKeyValueFilter(TimelineCompareOp.EQUAL, \"cfg_param1\", \"value3\"));\r\n    list2.addFilter(new TimelineKeyValueFilter(TimelineCompareOp.EQUAL, \"config_param2\", \"value2\"));\r\n    TimelineFilterList confsToRetrieve = new TimelineFilterList(Operator.OR, new TimelinePrefixFilter(TimelineCompareOp.EQUAL, \"config_\"));\r\n    TimelineFilterList confFilterList1 = new TimelineFilterList(Operator.OR, list1, list2);\r\n    entities = reader.getEntities(new TimelineReaderContext(\"cluster1\", \"user1\", \"some_flow_name\", 1002345678919L, null, TimelineEntityType.YARN_APPLICATION.toString(), null), new TimelineEntityFilters.Builder().configFilters(confFilterList1).build(), new TimelineDataToRetrieve(confsToRetrieve, null, null, null, null, null));\r\n    assertEquals(2, entities.size());\r\n    cfgCnt = 0;\r\n    for (TimelineEntity entity : entities) {\r\n        cfgCnt += entity.getConfigs().size();\r\n        for (String confKey : entity.getConfigs().keySet()) {\r\n            assertTrue(\"Config key returned should start with config_\", confKey.startsWith(\"config_\"));\r\n        }\r\n    }\r\n    assertEquals(2, cfgCnt);\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-yarn-project\\hadoop-yarn\\hadoop-yarn-server\\hadoop-yarn-server-timelineservice-hbase-tests\\src\\test\\java\\org\\apache\\hadoop\\yarn\\server\\timelineservice\\storage",
  "methodName" : "testReadAppsMetricFilters",
  "errType" : null,
  "containingMethodsNum" : 23,
  "sourceCodeText" : "void testReadAppsMetricFilters() throws Exception\n{\r\n    TimelineFilterList list1 = new TimelineFilterList();\r\n    list1.addFilter(new TimelineCompareFilter(TimelineCompareOp.GREATER_OR_EQUAL, \"MAP1_SLOT_MILLIS\", 50000000900L));\r\n    TimelineFilterList list2 = new TimelineFilterList();\r\n    list2.addFilter(new TimelineCompareFilter(TimelineCompareOp.LESS_THAN, \"MAP_SLOT_MILLIS\", 80000000000L));\r\n    list2.addFilter(new TimelineCompareFilter(TimelineCompareOp.EQUAL, \"MAP1_BYTES\", 50));\r\n    TimelineFilterList metricFilterList = new TimelineFilterList(Operator.OR, list1, list2);\r\n    Set<TimelineEntity> entities = reader.getEntities(new TimelineReaderContext(\"cluster1\", \"user1\", \"some_flow_name\", 1002345678919L, null, TimelineEntityType.YARN_APPLICATION.toString(), null), new TimelineEntityFilters.Builder().metricFilters(metricFilterList).build(), new TimelineDataToRetrieve(null, null, EnumSet.of(Field.METRICS), null, null, null));\r\n    assertEquals(2, entities.size());\r\n    int metricCnt = 0;\r\n    for (TimelineEntity entity : entities) {\r\n        metricCnt += entity.getMetrics().size();\r\n    }\r\n    assertEquals(3, metricCnt);\r\n    entities = reader.getEntities(new TimelineReaderContext(\"cluster1\", \"user1\", \"some_flow_name\", 1002345678919L, null, TimelineEntityType.YARN_APPLICATION.toString(), null), new TimelineEntityFilters.Builder().metricFilters(metricFilterList).build(), new TimelineDataToRetrieve(null, null, EnumSet.of(Field.ALL), null, null, null));\r\n    assertEquals(2, entities.size());\r\n    metricCnt = 0;\r\n    for (TimelineEntity entity : entities) {\r\n        metricCnt += entity.getMetrics().size();\r\n    }\r\n    assertEquals(3, metricCnt);\r\n    TimelineFilterList metricFilterList1 = new TimelineFilterList(new TimelineCompareFilter(TimelineCompareOp.LESS_OR_EQUAL, \"MAP_SLOT_MILLIS\", 80000000000L), new TimelineCompareFilter(TimelineCompareOp.NOT_EQUAL, \"MAP1_BYTES\", 30));\r\n    entities = reader.getEntities(new TimelineReaderContext(\"cluster1\", \"user1\", \"some_flow_name\", 1002345678919L, null, TimelineEntityType.YARN_APPLICATION.toString(), null), new TimelineEntityFilters.Builder().metricFilters(metricFilterList1).build(), new TimelineDataToRetrieve(null, null, EnumSet.of(Field.METRICS), null, null, null));\r\n    assertEquals(1, entities.size());\r\n    metricCnt = 0;\r\n    for (TimelineEntity entity : entities) {\r\n        metricCnt += entity.getMetrics().size();\r\n    }\r\n    assertEquals(2, metricCnt);\r\n    TimelineFilterList metricFilterList2 = new TimelineFilterList(new TimelineCompareFilter(TimelineCompareOp.LESS_THAN, \"MAP_SLOT_MILLIS\", 40000000000L), new TimelineCompareFilter(TimelineCompareOp.NOT_EQUAL, \"MAP1_BYTES\", 30));\r\n    entities = reader.getEntities(new TimelineReaderContext(\"cluster1\", \"user1\", \"some_flow_name\", 1002345678919L, null, TimelineEntityType.YARN_APPLICATION.toString(), null), new TimelineEntityFilters.Builder().metricFilters(metricFilterList2).build(), new TimelineDataToRetrieve(null, null, EnumSet.of(Field.METRICS), null, null, null));\r\n    assertEquals(0, entities.size());\r\n    TimelineFilterList metricFilterList3 = new TimelineFilterList(new TimelineCompareFilter(TimelineCompareOp.EQUAL, \"dummy_metric\", 5));\r\n    entities = reader.getEntities(new TimelineReaderContext(\"cluster1\", \"user1\", \"some_flow_name\", 1002345678919L, null, TimelineEntityType.YARN_APPLICATION.toString(), null), new TimelineEntityFilters.Builder().metricFilters(metricFilterList3).build(), new TimelineDataToRetrieve(null, null, EnumSet.of(Field.METRICS), null, null, null));\r\n    assertEquals(0, entities.size());\r\n    TimelineFilterList metricFilterList4 = new TimelineFilterList(new TimelineCompareFilter(TimelineCompareOp.NOT_EQUAL, \"dummy_metric\", 5));\r\n    entities = reader.getEntities(new TimelineReaderContext(\"cluster1\", \"user1\", \"some_flow_name\", 1002345678919L, null, TimelineEntityType.YARN_APPLICATION.toString(), null), new TimelineEntityFilters.Builder().metricFilters(metricFilterList4).build(), new TimelineDataToRetrieve(null, null, EnumSet.of(Field.METRICS), null, null, null));\r\n    assertEquals(0, entities.size());\r\n    TimelineFilterList metricFilterList5 = new TimelineFilterList(new TimelineCompareFilter(TimelineCompareOp.NOT_EQUAL, \"dummy_metric\", 5, false));\r\n    entities = reader.getEntities(new TimelineReaderContext(\"cluster1\", \"user1\", \"some_flow_name\", 1002345678919L, null, TimelineEntityType.YARN_APPLICATION.toString(), null), new TimelineEntityFilters.Builder().metricFilters(metricFilterList5).build(), new TimelineDataToRetrieve(null, null, EnumSet.of(Field.METRICS), null, null, null));\r\n    assertEquals(3, entities.size());\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-yarn-project\\hadoop-yarn\\hadoop-yarn-server\\hadoop-yarn-server-timelineservice-hbase-tests\\src\\test\\java\\org\\apache\\hadoop\\yarn\\server\\timelineservice\\storage",
  "methodName" : "testReadAppsMetricPrefix",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void testReadAppsMetricPrefix() throws Exception\n{\r\n    TimelineFilterList list = new TimelineFilterList(Operator.OR, new TimelinePrefixFilter(TimelineCompareOp.EQUAL, \"MAP1_\"));\r\n    TimelineEntity e1 = reader.getEntity(new TimelineReaderContext(\"cluster1\", \"user1\", \"some_flow_name\", 1002345678919L, \"application_1111111111_2222\", TimelineEntityType.YARN_APPLICATION.toString(), null), new TimelineDataToRetrieve(null, list, null, null, null, null));\r\n    assertNotNull(e1);\r\n    assertEquals(1, e1.getMetrics().size());\r\n    Set<TimelineEntity> es1 = reader.getEntities(new TimelineReaderContext(\"cluster1\", \"user1\", \"some_flow_name\", 1002345678919L, null, TimelineEntityType.YARN_APPLICATION.toString(), null), new TimelineEntityFilters.Builder().build(), new TimelineDataToRetrieve(null, list, null, null, null, null));\r\n    int metricCnt = 0;\r\n    for (TimelineEntity entity : es1) {\r\n        metricCnt += entity.getMetrics().size();\r\n        for (TimelineMetric metric : entity.getMetrics()) {\r\n            assertTrue(\"Metric Id returned should start with MAP1_\", metric.getId().startsWith(\"MAP1_\"));\r\n        }\r\n    }\r\n    assertEquals(2, metricCnt);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-yarn-project\\hadoop-yarn\\hadoop-yarn-server\\hadoop-yarn-server-timelineservice-hbase-tests\\src\\test\\java\\org\\apache\\hadoop\\yarn\\server\\timelineservice\\storage",
  "methodName" : "testReadAppsMetricFilterPrefix",
  "errType" : null,
  "containingMethodsNum" : 22,
  "sourceCodeText" : "void testReadAppsMetricFilterPrefix() throws Exception\n{\r\n    TimelineFilterList list = new TimelineFilterList(Operator.OR, new TimelinePrefixFilter(TimelineCompareOp.EQUAL, \"MAP1_\"));\r\n    TimelineFilterList metricFilterList = new TimelineFilterList();\r\n    metricFilterList.addFilter(new TimelineCompareFilter(TimelineCompareOp.GREATER_OR_EQUAL, \"MAP1_SLOT_MILLIS\", 0L));\r\n    Set<TimelineEntity> entities = reader.getEntities(new TimelineReaderContext(\"cluster1\", \"user1\", \"some_flow_name\", 1002345678919L, null, TimelineEntityType.YARN_APPLICATION.toString(), null), new TimelineEntityFilters.Builder().metricFilters(metricFilterList).build(), new TimelineDataToRetrieve(null, list, null, null, null, null));\r\n    int metricCnt = 0;\r\n    assertEquals(1, entities.size());\r\n    for (TimelineEntity entity : entities) {\r\n        metricCnt += entity.getMetrics().size();\r\n    }\r\n    assertEquals(1, metricCnt);\r\n    TimelineFilterList list1 = new TimelineFilterList();\r\n    list1.addFilter(new TimelineCompareFilter(TimelineCompareOp.GREATER_OR_EQUAL, \"MAP1_SLOT_MILLIS\", 50000000900L));\r\n    TimelineFilterList list2 = new TimelineFilterList();\r\n    list2.addFilter(new TimelineCompareFilter(TimelineCompareOp.LESS_THAN, \"MAP_SLOT_MILLIS\", 80000000000L));\r\n    list2.addFilter(new TimelineCompareFilter(TimelineCompareOp.EQUAL, \"MAP1_BYTES\", 50));\r\n    TimelineFilterList metricsToRetrieve = new TimelineFilterList(Operator.OR, new TimelinePrefixFilter(TimelineCompareOp.EQUAL, \"MAP1_\"));\r\n    TimelineFilterList metricFilterList1 = new TimelineFilterList(Operator.OR, list1, list2);\r\n    entities = reader.getEntities(new TimelineReaderContext(\"cluster1\", \"user1\", \"some_flow_name\", 1002345678919L, null, TimelineEntityType.YARN_APPLICATION.toString(), null), new TimelineEntityFilters.Builder().metricFilters(metricFilterList1).build(), new TimelineDataToRetrieve(null, metricsToRetrieve, null, null, null, null));\r\n    metricCnt = 0;\r\n    assertEquals(2, entities.size());\r\n    for (TimelineEntity entity : entities) {\r\n        metricCnt += entity.getMetrics().size();\r\n        for (TimelineMetric metric : entity.getMetrics()) {\r\n            assertTrue(\"Metric Id returned should start with MAP1_\", metric.getId().startsWith(\"MAP1_\"));\r\n        }\r\n    }\r\n    assertEquals(2, metricCnt);\r\n    entities = reader.getEntities(new TimelineReaderContext(\"cluster1\", \"user1\", \"some_flow_name\", 1002345678919L, null, TimelineEntityType.YARN_APPLICATION.toString(), null), new TimelineEntityFilters.Builder().metricFilters(metricFilterList1).build(), new TimelineDataToRetrieve(null, metricsToRetrieve, EnumSet.of(Field.METRICS), Integer.MAX_VALUE, null, null));\r\n    metricCnt = 0;\r\n    int metricValCnt = 0;\r\n    assertEquals(2, entities.size());\r\n    for (TimelineEntity entity : entities) {\r\n        metricCnt += entity.getMetrics().size();\r\n        for (TimelineMetric metric : entity.getMetrics()) {\r\n            metricValCnt += metric.getValues().size();\r\n            assertTrue(\"Metric Id returned should start with MAP1_\", metric.getId().startsWith(\"MAP1_\"));\r\n        }\r\n    }\r\n    assertEquals(2, metricCnt);\r\n    assertEquals(7, metricValCnt);\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-yarn-project\\hadoop-yarn\\hadoop-yarn-server\\hadoop-yarn-server-timelineservice-hbase-tests\\src\\test\\java\\org\\apache\\hadoop\\yarn\\server\\timelineservice\\storage",
  "methodName" : "testReadAppsMetricTimeRange",
  "errType" : null,
  "containingMethodsNum" : 33,
  "sourceCodeText" : "void testReadAppsMetricTimeRange() throws Exception\n{\r\n    Set<TimelineEntity> entities = reader.getEntities(new TimelineReaderContext(\"cluster1\", \"user1\", \"some_flow_name\", 1002345678919L, null, TimelineEntityType.YARN_APPLICATION.toString(), null), new TimelineEntityFilters.Builder().build(), new TimelineDataToRetrieve(null, null, EnumSet.of(Field.METRICS), 100, null, null));\r\n    assertEquals(3, entities.size());\r\n    int metricTimeSeriesCnt = 0;\r\n    int metricCnt = 0;\r\n    for (TimelineEntity entity : entities) {\r\n        metricCnt += entity.getMetrics().size();\r\n        for (TimelineMetric m : entity.getMetrics()) {\r\n            metricTimeSeriesCnt += m.getValues().size();\r\n        }\r\n    }\r\n    assertEquals(3, metricCnt);\r\n    assertEquals(13, metricTimeSeriesCnt);\r\n    entities = reader.getEntities(new TimelineReaderContext(\"cluster1\", \"user1\", \"some_flow_name\", 1002345678919L, null, TimelineEntityType.YARN_APPLICATION.toString(), null), new TimelineEntityFilters.Builder().build(), new TimelineDataToRetrieve(null, null, EnumSet.of(Field.METRICS), 100, CURRENT_TIME - 40000, CURRENT_TIME));\r\n    assertEquals(3, entities.size());\r\n    metricCnt = 0;\r\n    metricTimeSeriesCnt = 0;\r\n    for (TimelineEntity entity : entities) {\r\n        metricCnt += entity.getMetrics().size();\r\n        for (TimelineMetric m : entity.getMetrics()) {\r\n            for (Long ts : m.getValues().keySet()) {\r\n                assertTrue(ts >= CURRENT_TIME - 40000 && ts <= CURRENT_TIME);\r\n            }\r\n            metricTimeSeriesCnt += m.getValues().size();\r\n        }\r\n    }\r\n    assertEquals(3, metricCnt);\r\n    assertEquals(5, metricTimeSeriesCnt);\r\n    entities = reader.getEntities(new TimelineReaderContext(\"cluster1\", \"user1\", \"some_flow_name\", 1002345678919L, null, TimelineEntityType.YARN_APPLICATION.toString(), null), new TimelineEntityFilters.Builder().build(), new TimelineDataToRetrieve(null, null, EnumSet.of(Field.METRICS), null, CURRENT_TIME - 40000, CURRENT_TIME));\r\n    assertEquals(3, entities.size());\r\n    metricCnt = 0;\r\n    metricTimeSeriesCnt = 0;\r\n    for (TimelineEntity entity : entities) {\r\n        metricCnt += entity.getMetrics().size();\r\n        for (TimelineMetric m : entity.getMetrics()) {\r\n            for (Long ts : m.getValues().keySet()) {\r\n                assertTrue(ts >= CURRENT_TIME - 40000 && ts <= CURRENT_TIME);\r\n            }\r\n            metricTimeSeriesCnt += m.getValues().size();\r\n        }\r\n    }\r\n    assertEquals(3, metricCnt);\r\n    assertEquals(3, metricTimeSeriesCnt);\r\n    TimelineEntity entity = reader.getEntity(new TimelineReaderContext(\"cluster1\", \"user1\", \"some_flow_name\", 1002345678919L, \"application_1111111111_2222\", TimelineEntityType.YARN_APPLICATION.toString(), null), new TimelineDataToRetrieve(null, null, EnumSet.of(Field.METRICS), 100, CURRENT_TIME - 40000, CURRENT_TIME));\r\n    assertNotNull(entity);\r\n    assertEquals(2, entity.getMetrics().size());\r\n    metricTimeSeriesCnt = 0;\r\n    for (TimelineMetric m : entity.getMetrics()) {\r\n        for (Long ts : m.getValues().keySet()) {\r\n            assertTrue(ts >= CURRENT_TIME - 40000 && ts <= CURRENT_TIME);\r\n        }\r\n        metricTimeSeriesCnt += m.getValues().size();\r\n    }\r\n    assertEquals(3, metricTimeSeriesCnt);\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-yarn-project\\hadoop-yarn\\hadoop-yarn-server\\hadoop-yarn-server-timelineservice-hbase-tests\\src\\test\\java\\org\\apache\\hadoop\\yarn\\server\\timelineservice\\storage",
  "methodName" : "testReadAppsInfoFilters",
  "errType" : null,
  "containingMethodsNum" : 20,
  "sourceCodeText" : "void testReadAppsInfoFilters() throws Exception\n{\r\n    TimelineFilterList list1 = new TimelineFilterList();\r\n    list1.addFilter(new TimelineKeyValueFilter(TimelineCompareOp.EQUAL, \"infoMapKey3\", 85.85));\r\n    list1.addFilter(new TimelineKeyValueFilter(TimelineCompareOp.EQUAL, \"infoMapKey1\", \"infoMapValue2\"));\r\n    TimelineFilterList list2 = new TimelineFilterList();\r\n    list2.addFilter(new TimelineKeyValueFilter(TimelineCompareOp.EQUAL, \"infoMapKey1\", \"infoMapValue1\"));\r\n    list2.addFilter(new TimelineKeyValueFilter(TimelineCompareOp.EQUAL, \"infoMapKey2\", 10));\r\n    TimelineFilterList infoFilterList = new TimelineFilterList(Operator.OR, list1, list2);\r\n    Set<TimelineEntity> entities = reader.getEntities(new TimelineReaderContext(\"cluster1\", \"user1\", \"some_flow_name\", 1002345678919L, null, TimelineEntityType.YARN_APPLICATION.toString(), null), new TimelineEntityFilters.Builder().infoFilters(infoFilterList).build(), new TimelineDataToRetrieve(null, null, EnumSet.of(Field.INFO), null, null, null));\r\n    assertEquals(2, entities.size());\r\n    int infoCnt = 0;\r\n    for (TimelineEntity entity : entities) {\r\n        infoCnt += entity.getInfo().size();\r\n    }\r\n    assertEquals(7, infoCnt);\r\n    TimelineFilterList infoFilterList1 = new TimelineFilterList(new TimelineKeyValueFilter(TimelineCompareOp.NOT_EQUAL, \"infoMapKey1\", \"infoMapValue1\"));\r\n    entities = reader.getEntities(new TimelineReaderContext(\"cluster1\", \"user1\", \"some_flow_name\", 1002345678919L, null, TimelineEntityType.YARN_APPLICATION.toString(), null), new TimelineEntityFilters.Builder().infoFilters(infoFilterList1).build(), new TimelineDataToRetrieve(null, null, EnumSet.of(Field.INFO), null, null, null));\r\n    assertEquals(1, entities.size());\r\n    infoCnt = 0;\r\n    for (TimelineEntity entity : entities) {\r\n        infoCnt += entity.getInfo().size();\r\n    }\r\n    assertEquals(4, infoCnt);\r\n    TimelineFilterList infoFilterList2 = new TimelineFilterList(new TimelineKeyValueFilter(TimelineCompareOp.NOT_EQUAL, \"infoMapKey1\", \"infoMapValue2\"), new TimelineKeyValueFilter(TimelineCompareOp.NOT_EQUAL, \"infoMapKey3\", 85.85));\r\n    entities = reader.getEntities(new TimelineReaderContext(\"cluster1\", \"user1\", \"some_flow_name\", 1002345678919L, null, TimelineEntityType.YARN_APPLICATION.toString(), null), new TimelineEntityFilters.Builder().infoFilters(infoFilterList2).build(), new TimelineDataToRetrieve(null, null, EnumSet.of(Field.INFO), null, null, null));\r\n    assertEquals(0, entities.size());\r\n    TimelineFilterList infoFilterList3 = new TimelineFilterList(new TimelineKeyValueFilter(TimelineCompareOp.EQUAL, \"dummy_info\", \"some_value\"));\r\n    entities = reader.getEntities(new TimelineReaderContext(\"cluster1\", \"user1\", \"some_flow_name\", 1002345678919L, null, TimelineEntityType.YARN_APPLICATION.toString(), null), new TimelineEntityFilters.Builder().infoFilters(infoFilterList3).build(), new TimelineDataToRetrieve(null, null, EnumSet.of(Field.INFO), null, null, null));\r\n    assertEquals(0, entities.size());\r\n    TimelineFilterList infoFilterList4 = new TimelineFilterList(new TimelineKeyValueFilter(TimelineCompareOp.NOT_EQUAL, \"dummy_info\", \"some_value\"));\r\n    entities = reader.getEntities(new TimelineReaderContext(\"cluster1\", \"user1\", \"some_flow_name\", 1002345678919L, null, TimelineEntityType.YARN_APPLICATION.toString(), null), new TimelineEntityFilters.Builder().infoFilters(infoFilterList4).build(), new TimelineDataToRetrieve(null, null, EnumSet.of(Field.INFO), null, null, null));\r\n    assertEquals(0, entities.size());\r\n    TimelineFilterList infoFilterList5 = new TimelineFilterList(new TimelineKeyValueFilter(TimelineCompareOp.NOT_EQUAL, \"dummy_info\", \"some_value\", false));\r\n    entities = reader.getEntities(new TimelineReaderContext(\"cluster1\", \"user1\", \"some_flow_name\", 1002345678919L, null, TimelineEntityType.YARN_APPLICATION.toString(), null), new TimelineEntityFilters.Builder().infoFilters(infoFilterList5).build(), new TimelineDataToRetrieve(null, null, EnumSet.of(Field.INFO), null, null, null));\r\n    assertEquals(3, entities.size());\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-yarn-project\\hadoop-yarn\\hadoop-yarn-server\\hadoop-yarn-server-timelineservice-hbase-tests\\src\\test\\java\\org\\apache\\hadoop\\yarn\\server\\timelineservice\\storage",
  "methodName" : "tearDownAfterClass",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void tearDownAfterClass() throws Exception\n{\r\n    if (util != null) {\r\n        util.shutdownMiniCluster();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-yarn-project\\hadoop-yarn\\hadoop-yarn-server\\hadoop-yarn-server-timelineservice-hbase-tests\\src\\test\\java\\org\\apache\\hadoop\\yarn\\server\\timelineservice\\storage",
  "methodName" : "createSchema",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void createSchema(final Configuration conf) throws IOException\n{\r\n    conf.set(YarnConfiguration.FLOW_RUN_COPROCESSOR_JAR_HDFS_LOCATION, \" \");\r\n    HBaseTimelineSchemaCreator.createAllTables(conf, false);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-yarn-project\\hadoop-yarn\\hadoop-yarn-server\\hadoop-yarn-server-timelineservice-hbase-tests\\src\\test\\java\\org\\apache\\hadoop\\yarn\\server\\timelineservice\\storage",
  "methodName" : "loadApps",
  "errType" : null,
  "containingMethodsNum" : 59,
  "sourceCodeText" : "void loadApps(HBaseTestingUtility util, long ts) throws IOException\n{\r\n    TimelineEntities te = new TimelineEntities();\r\n    TimelineEntity entity = new TimelineEntity();\r\n    String id = \"application_1111111111_2222\";\r\n    entity.setId(id);\r\n    entity.setType(TimelineEntityType.YARN_APPLICATION.toString());\r\n    Long cTime = 1425016502000L;\r\n    entity.setCreatedTime(cTime);\r\n    entity.addInfo(getInfoMap3());\r\n    Set<String> isRelatedToSet = new HashSet<>();\r\n    isRelatedToSet.add(\"relatedto1\");\r\n    Map<String, Set<String>> isRelatedTo = new HashMap<>();\r\n    isRelatedTo.put(\"task\", isRelatedToSet);\r\n    entity.setIsRelatedToEntities(isRelatedTo);\r\n    Set<String> relatesToSet = new HashSet<>();\r\n    relatesToSet.add(\"relatesto1\");\r\n    relatesToSet.add(\"relatesto3\");\r\n    Map<String, Set<String>> relatesTo = new HashMap<>();\r\n    relatesTo.put(\"container\", relatesToSet);\r\n    Set<String> relatesToSet11 = new HashSet<>();\r\n    relatesToSet11.add(\"relatesto4\");\r\n    relatesTo.put(\"container1\", relatesToSet11);\r\n    entity.setRelatesToEntities(relatesTo);\r\n    Map<String, String> conf = new HashMap<>();\r\n    conf.put(\"config_param1\", \"value1\");\r\n    conf.put(\"config_param2\", \"value2\");\r\n    conf.put(\"cfg_param1\", \"value3\");\r\n    entity.addConfigs(conf);\r\n    Set<TimelineMetric> metrics = new HashSet<>();\r\n    metrics.add(getMetric4(ts));\r\n    TimelineMetric m12 = new TimelineMetric();\r\n    m12.setId(\"MAP1_BYTES\");\r\n    m12.addValue(ts, 50);\r\n    metrics.add(m12);\r\n    entity.addMetrics(metrics);\r\n    entity.addEvent(addStartEvent(ts));\r\n    te.addEntity(entity);\r\n    TimelineEntities te1 = new TimelineEntities();\r\n    TimelineEntity entity1 = new TimelineEntity();\r\n    String id1 = \"application_1111111111_3333\";\r\n    entity1.setId(id1);\r\n    entity1.setType(TimelineEntityType.YARN_APPLICATION.toString());\r\n    entity1.setCreatedTime(cTime + 20L);\r\n    entity1.addInfo(getInfoMap4());\r\n    Set<String> isRelatedToSet1 = new HashSet<>();\r\n    isRelatedToSet1.add(\"relatedto3\");\r\n    isRelatedToSet1.add(\"relatedto5\");\r\n    Map<String, Set<String>> isRelatedTo1 = new HashMap<>();\r\n    isRelatedTo1.put(\"task1\", isRelatedToSet1);\r\n    Set<String> isRelatedToSet11 = new HashSet<>();\r\n    isRelatedToSet11.add(\"relatedto4\");\r\n    isRelatedTo1.put(\"task2\", isRelatedToSet11);\r\n    entity1.setIsRelatedToEntities(isRelatedTo1);\r\n    Set<String> relatesToSet1 = new HashSet<>();\r\n    relatesToSet1.add(\"relatesto1\");\r\n    relatesToSet1.add(\"relatesto2\");\r\n    Map<String, Set<String>> relatesTo1 = new HashMap<>();\r\n    relatesTo1.put(\"container\", relatesToSet1);\r\n    entity1.setRelatesToEntities(relatesTo1);\r\n    Map<String, String> conf1 = new HashMap<>();\r\n    conf1.put(\"cfg_param1\", \"value1\");\r\n    conf1.put(\"cfg_param2\", \"value2\");\r\n    entity1.addConfigs(conf1);\r\n    entity1.addMetrics(getMetrics4(ts));\r\n    TimelineEvent event11 = new TimelineEvent();\r\n    event11.setId(\"end_event\");\r\n    event11.setTimestamp(ts);\r\n    entity1.addEvent(event11);\r\n    TimelineEvent event12 = new TimelineEvent();\r\n    event12.setId(\"update_event\");\r\n    event12.setTimestamp(ts - 10);\r\n    entity1.addEvent(event12);\r\n    te1.addEntity(entity1);\r\n    TimelineEntities te2 = new TimelineEntities();\r\n    te2.addEntity(getEntity4(cTime, ts));\r\n    HBaseTimelineWriterImpl hbi = null;\r\n    try {\r\n        hbi = new HBaseTimelineWriterImpl();\r\n        hbi.init(util.getConfiguration());\r\n        hbi.start();\r\n        UserGroupInformation remoteUser = UserGroupInformation.createRemoteUser(\"user1\");\r\n        hbi.write(new TimelineCollectorContext(\"cluster1\", \"user1\", \"some_flow_name\", \"AB7822C10F1111\", 1002345678919L, \"application_1111111111_2222\"), te, remoteUser);\r\n        hbi.write(new TimelineCollectorContext(\"cluster1\", \"user1\", \"some_flow_name\", \"AB7822C10F1111\", 1002345678919L, \"application_1111111111_3333\"), te1, remoteUser);\r\n        hbi.write(new TimelineCollectorContext(\"cluster1\", \"user1\", \"some_flow_name\", \"AB7822C10F1111\", 1002345678919L, \"application_1111111111_4444\"), te2, remoteUser);\r\n        hbi.stop();\r\n    } finally {\r\n        if (hbi != null) {\r\n            hbi.stop();\r\n            hbi.close();\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-yarn-project\\hadoop-yarn\\hadoop-yarn-server\\hadoop-yarn-server-timelineservice-hbase-tests\\src\\test\\java\\org\\apache\\hadoop\\yarn\\server\\timelineservice\\storage",
  "methodName" : "getMetrics4",
  "errType" : null,
  "containingMethodsNum" : 10,
  "sourceCodeText" : "Set<TimelineMetric> getMetrics4(long ts)\n{\r\n    Set<TimelineMetric> metrics1 = new HashSet<>();\r\n    TimelineMetric m2 = new TimelineMetric();\r\n    m2.setId(\"MAP1_SLOT_MILLIS\");\r\n    Map<Long, Number> metricValues1 = new HashMap<>();\r\n    metricValues1.put(ts - 120000, 100000000);\r\n    metricValues1.put(ts - 100000, 200000000);\r\n    metricValues1.put(ts - 80000, 300000000);\r\n    metricValues1.put(ts - 60000, 400000000);\r\n    metricValues1.put(ts - 40000, 50000000000L);\r\n    metricValues1.put(ts - 20000, 60000000000L);\r\n    m2.setType(Type.TIME_SERIES);\r\n    m2.setValues(metricValues1);\r\n    metrics1.add(m2);\r\n    return metrics1;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-yarn-project\\hadoop-yarn\\hadoop-yarn-server\\hadoop-yarn-server-timelineservice-hbase-tests\\src\\test\\java\\org\\apache\\hadoop\\yarn\\server\\timelineservice\\storage",
  "methodName" : "getEntity4",
  "errType" : null,
  "containingMethodsNum" : 12,
  "sourceCodeText" : "TimelineEntity getEntity4(long cTime, long ts)\n{\r\n    TimelineEntity entity2 = new TimelineEntity();\r\n    String id2 = \"application_1111111111_4444\";\r\n    entity2.setId(id2);\r\n    entity2.setType(TimelineEntityType.YARN_APPLICATION.toString());\r\n    entity2.setCreatedTime(cTime + 40L);\r\n    TimelineEvent event21 = new TimelineEvent();\r\n    event21.setId(\"update_event\");\r\n    event21.setTimestamp(ts - 20);\r\n    entity2.addEvent(event21);\r\n    Set<String> isRelatedToSet2 = new HashSet<String>();\r\n    isRelatedToSet2.add(\"relatedto3\");\r\n    Map<String, Set<String>> isRelatedTo2 = new HashMap<>();\r\n    isRelatedTo2.put(\"task1\", isRelatedToSet2);\r\n    entity2.setIsRelatedToEntities(isRelatedTo2);\r\n    Map<String, Set<String>> relatesTo3 = new HashMap<>();\r\n    Set<String> relatesToSet14 = new HashSet<String>();\r\n    relatesToSet14.add(\"relatesto7\");\r\n    relatesTo3.put(\"container2\", relatesToSet14);\r\n    entity2.setRelatesToEntities(relatesTo3);\r\n    return entity2;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-yarn-project\\hadoop-yarn\\hadoop-yarn-server\\hadoop-yarn-server-timelineservice-hbase-tests\\src\\test\\java\\org\\apache\\hadoop\\yarn\\server\\timelineservice\\storage",
  "methodName" : "getInfoMap4",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "Map<String, Object> getInfoMap4()\n{\r\n    Map<String, Object> infoMap1 = new HashMap<>();\r\n    infoMap1.put(\"infoMapKey1\", \"infoMapValue1\");\r\n    infoMap1.put(\"infoMapKey2\", 10);\r\n    return infoMap1;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-yarn-project\\hadoop-yarn\\hadoop-yarn-server\\hadoop-yarn-server-timelineservice-hbase-tests\\src\\test\\java\\org\\apache\\hadoop\\yarn\\server\\timelineservice\\storage",
  "methodName" : "getMetric4",
  "errType" : null,
  "containingMethodsNum" : 9,
  "sourceCodeText" : "TimelineMetric getMetric4(long ts)\n{\r\n    TimelineMetric m1 = new TimelineMetric();\r\n    m1.setId(\"MAP_SLOT_MILLIS\");\r\n    Map<Long, Number> metricValues = new HashMap<>();\r\n    metricValues.put(ts - 120000, 100000000);\r\n    metricValues.put(ts - 100000, 200000000);\r\n    metricValues.put(ts - 80000, 300000000);\r\n    metricValues.put(ts - 60000, 400000000);\r\n    metricValues.put(ts - 40000, 50000000000L);\r\n    metricValues.put(ts - 20000, 60000000000L);\r\n    m1.setType(Type.TIME_SERIES);\r\n    m1.setValues(metricValues);\r\n    return m1;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-yarn-project\\hadoop-yarn\\hadoop-yarn-server\\hadoop-yarn-server-timelineservice-hbase-tests\\src\\test\\java\\org\\apache\\hadoop\\yarn\\server\\timelineservice\\storage",
  "methodName" : "getInfoMap3",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "Map<String, Object> getInfoMap3()\n{\r\n    Map<String, Object> infoMap = new HashMap<>();\r\n    infoMap.put(\"infoMapKey1\", \"infoMapValue2\");\r\n    infoMap.put(\"infoMapKey2\", 20);\r\n    infoMap.put(\"infoMapKey3\", 85.85);\r\n    return infoMap;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-yarn-project\\hadoop-yarn\\hadoop-yarn-server\\hadoop-yarn-server-timelineservice-hbase-tests\\src\\test\\java\\org\\apache\\hadoop\\yarn\\server\\timelineservice\\storage",
  "methodName" : "getInfoMap1",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "Map<String, Object> getInfoMap1()\n{\r\n    Map<String, Object> infoMap = new HashMap<>();\r\n    infoMap.put(\"infoMapKey1\", \"infoMapValue2\");\r\n    infoMap.put(\"infoMapKey2\", 20);\r\n    infoMap.put(\"infoMapKey3\", 71.4);\r\n    return infoMap;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-yarn-project\\hadoop-yarn\\hadoop-yarn-server\\hadoop-yarn-server-timelineservice-hbase-tests\\src\\test\\java\\org\\apache\\hadoop\\yarn\\server\\timelineservice\\storage",
  "methodName" : "getRelatesTo1",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "Map<String, Set<String>> getRelatesTo1()\n{\r\n    Set<String> relatesToSet = new HashSet<String>();\r\n    relatesToSet.add(\"relatesto1\");\r\n    relatesToSet.add(\"relatesto3\");\r\n    Map<String, Set<String>> relatesTo = new HashMap<>();\r\n    relatesTo.put(\"container\", relatesToSet);\r\n    Set<String> relatesToSet11 = new HashSet<>();\r\n    relatesToSet11.add(\"relatesto4\");\r\n    relatesTo.put(\"container1\", relatesToSet11);\r\n    return relatesTo;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-yarn-project\\hadoop-yarn\\hadoop-yarn-server\\hadoop-yarn-server-timelineservice-hbase-tests\\src\\test\\java\\org\\apache\\hadoop\\yarn\\server\\timelineservice\\storage",
  "methodName" : "getConfig1",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "Map<String, String> getConfig1()\n{\r\n    Map<String, String> conf = new HashMap<>();\r\n    conf.put(\"config_param1\", \"value1\");\r\n    conf.put(\"config_param2\", \"value2\");\r\n    conf.put(\"cfg_param1\", \"value3\");\r\n    return conf;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-yarn-project\\hadoop-yarn\\hadoop-yarn-server\\hadoop-yarn-server-timelineservice-hbase-tests\\src\\test\\java\\org\\apache\\hadoop\\yarn\\server\\timelineservice\\storage",
  "methodName" : "getConfig2",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "Map<String, String> getConfig2()\n{\r\n    Map<String, String> conf1 = new HashMap<>();\r\n    conf1.put(\"cfg_param1\", \"value1\");\r\n    conf1.put(\"cfg_param2\", \"value2\");\r\n    return conf1;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-yarn-project\\hadoop-yarn\\hadoop-yarn-server\\hadoop-yarn-server-timelineservice-hbase-tests\\src\\test\\java\\org\\apache\\hadoop\\yarn\\server\\timelineservice\\storage",
  "methodName" : "getInfoMap2",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "Map<String, Object> getInfoMap2()\n{\r\n    Map<String, Object> infoMap1 = new HashMap<>();\r\n    infoMap1.put(\"infoMapKey1\", \"infoMapValue1\");\r\n    infoMap1.put(\"infoMapKey2\", 10);\r\n    return infoMap1;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-yarn-project\\hadoop-yarn\\hadoop-yarn-server\\hadoop-yarn-server-timelineservice-hbase-tests\\src\\test\\java\\org\\apache\\hadoop\\yarn\\server\\timelineservice\\storage",
  "methodName" : "getIsRelatedTo1",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "Map<String, Set<String>> getIsRelatedTo1()\n{\r\n    Set<String> isRelatedToSet = new HashSet<>();\r\n    isRelatedToSet.add(\"relatedto1\");\r\n    Map<String, Set<String>> isRelatedTo = new HashMap<>();\r\n    isRelatedTo.put(\"task\", isRelatedToSet);\r\n    return isRelatedTo;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-yarn-project\\hadoop-yarn\\hadoop-yarn-server\\hadoop-yarn-server-timelineservice-hbase-tests\\src\\test\\java\\org\\apache\\hadoop\\yarn\\server\\timelineservice\\storage",
  "methodName" : "getMetricValues1",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "Map<Long, Number> getMetricValues1(long ts)\n{\r\n    Map<Long, Number> metricValues = new HashMap<>();\r\n    metricValues.put(ts - 120000, 100000000);\r\n    metricValues.put(ts - 100000, 200000000);\r\n    metricValues.put(ts - 80000, 300000000);\r\n    metricValues.put(ts - 60000, 400000000);\r\n    metricValues.put(ts - 40000, 50000000000L);\r\n    metricValues.put(ts - 20000, 70000000000L);\r\n    return metricValues;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-yarn-project\\hadoop-yarn\\hadoop-yarn-server\\hadoop-yarn-server-timelineservice-hbase-tests\\src\\test\\java\\org\\apache\\hadoop\\yarn\\server\\timelineservice\\storage",
  "methodName" : "loadEntities",
  "errType" : null,
  "containingMethodsNum" : 69,
  "sourceCodeText" : "void loadEntities(HBaseTestingUtility util, long ts) throws IOException\n{\r\n    TimelineEntities te = new TimelineEntities();\r\n    TimelineEntity entity = new TimelineEntity();\r\n    String id = \"hello\";\r\n    String type = \"world\";\r\n    entity.setId(id);\r\n    entity.setType(type);\r\n    Long cTime = 1425016502000L;\r\n    entity.setCreatedTime(cTime);\r\n    entity.addInfo(getInfoMap1());\r\n    entity.setIsRelatedToEntities(getIsRelatedTo1());\r\n    entity.setRelatesToEntities(getRelatesTo1());\r\n    entity.addConfigs(getConfig1());\r\n    Set<TimelineMetric> metrics = new HashSet<>();\r\n    TimelineMetric m1 = new TimelineMetric();\r\n    m1.setId(\"MAP_SLOT_MILLIS\");\r\n    m1.setType(Type.TIME_SERIES);\r\n    m1.setValues(getMetricValues1(ts));\r\n    metrics.add(m1);\r\n    TimelineMetric m12 = new TimelineMetric();\r\n    m12.setId(\"MAP1_BYTES\");\r\n    m12.addValue(ts, 50);\r\n    metrics.add(m12);\r\n    entity.addMetrics(metrics);\r\n    entity.addEvent(addStartEvent(ts));\r\n    te.addEntity(entity);\r\n    TimelineEntity entity1 = new TimelineEntity();\r\n    String id1 = \"hello1\";\r\n    entity1.setId(id1);\r\n    entity1.setType(type);\r\n    entity1.setCreatedTime(cTime + 20L);\r\n    entity1.addInfo(getInfoMap2());\r\n    TimelineEvent event11 = new TimelineEvent();\r\n    event11.setId(\"end_event\");\r\n    event11.setTimestamp(ts);\r\n    entity1.addEvent(event11);\r\n    TimelineEvent event12 = new TimelineEvent();\r\n    event12.setId(\"update_event\");\r\n    event12.setTimestamp(ts - 10);\r\n    entity1.addEvent(event12);\r\n    entity1.setIsRelatedToEntities(getIsRelatedTo2());\r\n    Set<String> relatesToSet1 = new HashSet<String>();\r\n    relatesToSet1.add(\"relatesto1\");\r\n    relatesToSet1.add(\"relatesto2\");\r\n    Map<String, Set<String>> relatesTo1 = new HashMap<>();\r\n    relatesTo1.put(\"container\", relatesToSet1);\r\n    entity1.setRelatesToEntities(relatesTo1);\r\n    entity1.addConfigs(getConfig2());\r\n    Set<TimelineMetric> metrics1 = new HashSet<>();\r\n    TimelineMetric m2 = new TimelineMetric();\r\n    m2.setId(\"MAP1_SLOT_MILLIS\");\r\n    m2.setType(Type.TIME_SERIES);\r\n    m2.setValues(getMetricValues2(ts));\r\n    metrics1.add(m2);\r\n    entity1.addMetrics(metrics1);\r\n    te.addEntity(entity1);\r\n    te.addEntity(getEntity2(type, cTime, ts));\r\n    for (int i = 0; i < 10; i++) {\r\n        TimelineEntity entity3 = new TimelineEntity();\r\n        String id3 = \"typeTest\" + i;\r\n        entity3.setId(id3);\r\n        StringBuilder typeName = new StringBuilder(\"newType\");\r\n        for (int j = 0; j < (i % 3); j++) {\r\n            typeName.append(\" \").append(j);\r\n        }\r\n        entity3.setType(typeName.toString());\r\n        entity3.setCreatedTime(cTime + 80L + i);\r\n        te.addEntity(entity3);\r\n    }\r\n    TimelineEntities appTe1 = new TimelineEntities();\r\n    TimelineEntity entityApp1 = new TimelineEntity();\r\n    String appName1 = \"application_1231111111_1111\";\r\n    entityApp1.setId(appName1);\r\n    entityApp1.setType(TimelineEntityType.YARN_APPLICATION.toString());\r\n    entityApp1.setCreatedTime(cTime + 40L);\r\n    TimelineEvent appCreationEvent1 = new TimelineEvent();\r\n    appCreationEvent1.setId(ApplicationMetricsConstants.CREATED_EVENT_TYPE);\r\n    appCreationEvent1.setTimestamp(cTime);\r\n    entityApp1.addEvent(appCreationEvent1);\r\n    appTe1.addEntity(entityApp1);\r\n    TimelineEntities appTe2 = new TimelineEntities();\r\n    TimelineEntity entityApp2 = new TimelineEntity();\r\n    String appName2 = \"application_1231111111_1112\";\r\n    entityApp2.setId(appName2);\r\n    entityApp2.setType(TimelineEntityType.YARN_APPLICATION.toString());\r\n    entityApp2.setCreatedTime(cTime + 50L);\r\n    TimelineEvent appCreationEvent2 = new TimelineEvent();\r\n    appCreationEvent2.setId(ApplicationMetricsConstants.CREATED_EVENT_TYPE);\r\n    appCreationEvent2.setTimestamp(cTime);\r\n    entityApp2.addEvent(appCreationEvent2);\r\n    appTe2.addEntity(entityApp2);\r\n    HBaseTimelineWriterImpl hbi = null;\r\n    try {\r\n        hbi = new HBaseTimelineWriterImpl();\r\n        hbi.init(util.getConfiguration());\r\n        hbi.start();\r\n        UserGroupInformation user = UserGroupInformation.createRemoteUser(\"user1\");\r\n        TimelineCollectorContext context = new TimelineCollectorContext(\"cluster1\", \"user1\", \"some_flow_name\", \"AB7822C10F1111\", 1002345678919L, appName1);\r\n        hbi.write(context, te, user);\r\n        hbi.write(context, appTe1, user);\r\n        context = new TimelineCollectorContext(\"cluster1\", \"user1\", \"some_flow_name\", \"AB7822C10F1111\", 1002345678919L, appName2);\r\n        hbi.write(context, te, user);\r\n        hbi.write(context, appTe2, user);\r\n        hbi.stop();\r\n    } finally {\r\n        if (hbi != null) {\r\n            hbi.stop();\r\n            hbi.close();\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-yarn-project\\hadoop-yarn\\hadoop-yarn-server\\hadoop-yarn-server-timelineservice-hbase-tests\\src\\test\\java\\org\\apache\\hadoop\\yarn\\server\\timelineservice\\storage",
  "methodName" : "getEntity2",
  "errType" : null,
  "containingMethodsNum" : 12,
  "sourceCodeText" : "TimelineEntity getEntity2(String type, long cTime, long ts)\n{\r\n    TimelineEntity entity2 = new TimelineEntity();\r\n    String id2 = \"hello2\";\r\n    entity2.setId(id2);\r\n    entity2.setType(type);\r\n    entity2.setCreatedTime(cTime + 40L);\r\n    TimelineEvent event21 = new TimelineEvent();\r\n    event21.setId(\"update_event\");\r\n    event21.setTimestamp(ts - 20);\r\n    entity2.addEvent(event21);\r\n    Set<String> isRelatedToSet2 = new HashSet<>();\r\n    isRelatedToSet2.add(\"relatedto3\");\r\n    Map<String, Set<String>> isRelatedTo2 = new HashMap<>();\r\n    isRelatedTo2.put(\"task1\", isRelatedToSet2);\r\n    entity2.setIsRelatedToEntities(isRelatedTo2);\r\n    Map<String, Set<String>> relatesTo3 = new HashMap<>();\r\n    Set<String> relatesToSet14 = new HashSet<>();\r\n    relatesToSet14.add(\"relatesto7\");\r\n    relatesTo3.put(\"container2\", relatesToSet14);\r\n    entity2.setRelatesToEntities(relatesTo3);\r\n    return entity2;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-yarn-project\\hadoop-yarn\\hadoop-yarn-server\\hadoop-yarn-server-timelineservice-hbase-tests\\src\\test\\java\\org\\apache\\hadoop\\yarn\\server\\timelineservice\\storage",
  "methodName" : "addStartEvent",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "TimelineEvent addStartEvent(long ts)\n{\r\n    TimelineEvent event = new TimelineEvent();\r\n    event.setId(\"start_event\");\r\n    event.setTimestamp(ts);\r\n    return event;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-yarn-project\\hadoop-yarn\\hadoop-yarn-server\\hadoop-yarn-server-timelineservice-hbase-tests\\src\\test\\java\\org\\apache\\hadoop\\yarn\\server\\timelineservice\\storage",
  "methodName" : "getMetricValues2",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "Map<Long, Number> getMetricValues2(long ts1)\n{\r\n    Map<Long, Number> metricValues1 = new HashMap<>();\r\n    metricValues1.put(ts1 - 120000, 100000000);\r\n    metricValues1.put(ts1 - 100000, 200000000);\r\n    metricValues1.put(ts1 - 80000, 300000000);\r\n    metricValues1.put(ts1 - 60000, 400000000);\r\n    metricValues1.put(ts1 - 40000, 50000000000L);\r\n    metricValues1.put(ts1 - 20000, 60000000000L);\r\n    return metricValues1;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-yarn-project\\hadoop-yarn\\hadoop-yarn-server\\hadoop-yarn-server-timelineservice-hbase-tests\\src\\test\\java\\org\\apache\\hadoop\\yarn\\server\\timelineservice\\storage",
  "methodName" : "getIsRelatedTo2",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "Map<String, Set<String>> getIsRelatedTo2()\n{\r\n    Set<String> isRelatedToSet1 = new HashSet<>();\r\n    isRelatedToSet1.add(\"relatedto3\");\r\n    isRelatedToSet1.add(\"relatedto5\");\r\n    Map<String, Set<String>> isRelatedTo1 = new HashMap<>();\r\n    isRelatedTo1.put(\"task1\", isRelatedToSet1);\r\n    Set<String> isRelatedToSet11 = new HashSet<>();\r\n    isRelatedToSet11.add(\"relatedto4\");\r\n    isRelatedTo1.put(\"task2\", isRelatedToSet11);\r\n    return isRelatedTo1;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-yarn-project\\hadoop-yarn\\hadoop-yarn-server\\hadoop-yarn-server-timelineservice-hbase-tests\\src\\test\\java\\org\\apache\\hadoop\\yarn\\server\\timelineservice\\storage",
  "methodName" : "setupBeforeClass",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void setupBeforeClass() throws Exception\n{\r\n    util = new HBaseTestingUtility();\r\n    Configuration conf = util.getConfiguration();\r\n    conf.setInt(\"hfile.format.version\", 3);\r\n    util.startMiniCluster();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-yarn-project\\hadoop-yarn\\hadoop-yarn-server\\hadoop-yarn-server-timelineservice-hbase-tests\\src\\test\\java\\org\\apache\\hadoop\\yarn\\server\\timelineservice\\storage",
  "methodName" : "createWithDefaultPrefix",
  "errType" : null,
  "containingMethodsNum" : 14,
  "sourceCodeText" : "void createWithDefaultPrefix() throws IOException\n{\r\n    Configuration hbaseConf = util.getConfiguration();\r\n    DataGeneratorForTest.createSchema(hbaseConf);\r\n    Connection conn = null;\r\n    conn = ConnectionFactory.createConnection(hbaseConf);\r\n    Admin admin = conn.getAdmin();\r\n    TableName entityTableName = BaseTableRW.getTableName(hbaseConf, EntityTableRW.TABLE_NAME_CONF_NAME, EntityTableRW.DEFAULT_TABLE_NAME);\r\n    assertTrue(admin.tableExists(entityTableName));\r\n    assertTrue(entityTableName.getNameAsString().startsWith(YarnConfiguration.DEFAULT_TIMELINE_SERVICE_HBASE_SCHEMA_PREFIX));\r\n    Table entityTable = conn.getTable(BaseTableRW.getTableName(hbaseConf, EntityTableRW.TABLE_NAME_CONF_NAME, EntityTableRW.DEFAULT_TABLE_NAME));\r\n    assertNotNull(entityTable);\r\n    TableName flowRunTableName = BaseTableRW.getTableName(hbaseConf, FlowRunTableRW.TABLE_NAME_CONF_NAME, FlowRunTableRW.DEFAULT_TABLE_NAME);\r\n    assertTrue(admin.tableExists(flowRunTableName));\r\n    assertTrue(flowRunTableName.getNameAsString().startsWith(YarnConfiguration.DEFAULT_TIMELINE_SERVICE_HBASE_SCHEMA_PREFIX));\r\n    Table flowRunTable = conn.getTable(BaseTableRW.getTableName(hbaseConf, FlowRunTableRW.TABLE_NAME_CONF_NAME, FlowRunTableRW.DEFAULT_TABLE_NAME));\r\n    assertNotNull(flowRunTable);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-yarn-project\\hadoop-yarn\\hadoop-yarn-server\\hadoop-yarn-server-timelineservice-hbase-tests\\src\\test\\java\\org\\apache\\hadoop\\yarn\\server\\timelineservice\\storage",
  "methodName" : "createWithSetPrefix",
  "errType" : null,
  "containingMethodsNum" : 29,
  "sourceCodeText" : "void createWithSetPrefix() throws IOException\n{\r\n    Configuration hbaseConf = util.getConfiguration();\r\n    String prefix = \"unit-test.\";\r\n    hbaseConf.set(YarnConfiguration.TIMELINE_SERVICE_HBASE_SCHEMA_PREFIX_NAME, prefix);\r\n    DataGeneratorForTest.createSchema(hbaseConf);\r\n    Connection conn = null;\r\n    conn = ConnectionFactory.createConnection(hbaseConf);\r\n    Admin admin = conn.getAdmin();\r\n    TableName entityTableName = BaseTableRW.getTableName(hbaseConf, EntityTableRW.TABLE_NAME_CONF_NAME, EntityTableRW.DEFAULT_TABLE_NAME);\r\n    assertTrue(admin.tableExists(entityTableName));\r\n    assertTrue(entityTableName.getNameAsString().startsWith(prefix));\r\n    Table entityTable = conn.getTable(BaseTableRW.getTableName(hbaseConf, EntityTableRW.TABLE_NAME_CONF_NAME, EntityTableRW.DEFAULT_TABLE_NAME));\r\n    assertNotNull(entityTable);\r\n    TableName flowRunTableName = BaseTableRW.getTableName(hbaseConf, FlowRunTableRW.TABLE_NAME_CONF_NAME, FlowRunTableRW.DEFAULT_TABLE_NAME);\r\n    assertTrue(admin.tableExists(flowRunTableName));\r\n    assertTrue(flowRunTableName.getNameAsString().startsWith(prefix));\r\n    Table flowRunTable = conn.getTable(BaseTableRW.getTableName(hbaseConf, FlowRunTableRW.TABLE_NAME_CONF_NAME, FlowRunTableRW.DEFAULT_TABLE_NAME));\r\n    assertNotNull(flowRunTable);\r\n    hbaseConf.unset(YarnConfiguration.TIMELINE_SERVICE_HBASE_SCHEMA_PREFIX_NAME);\r\n    prefix = \"yet-another-unit-test.\";\r\n    hbaseConf.set(YarnConfiguration.TIMELINE_SERVICE_HBASE_SCHEMA_PREFIX_NAME, prefix);\r\n    DataGeneratorForTest.createSchema(hbaseConf);\r\n    entityTableName = BaseTableRW.getTableName(hbaseConf, EntityTableRW.TABLE_NAME_CONF_NAME, EntityTableRW.DEFAULT_TABLE_NAME);\r\n    assertTrue(admin.tableExists(entityTableName));\r\n    assertTrue(entityTableName.getNameAsString().startsWith(prefix));\r\n    entityTable = conn.getTable(BaseTableRW.getTableName(hbaseConf, EntityTableRW.TABLE_NAME_CONF_NAME, EntityTableRW.DEFAULT_TABLE_NAME));\r\n    assertNotNull(entityTable);\r\n    flowRunTableName = BaseTableRW.getTableName(hbaseConf, FlowRunTableRW.TABLE_NAME_CONF_NAME, FlowRunTableRW.DEFAULT_TABLE_NAME);\r\n    assertTrue(admin.tableExists(flowRunTableName));\r\n    assertTrue(flowRunTableName.getNameAsString().startsWith(prefix));\r\n    flowRunTable = conn.getTable(BaseTableRW.getTableName(hbaseConf, FlowRunTableRW.TABLE_NAME_CONF_NAME, FlowRunTableRW.DEFAULT_TABLE_NAME));\r\n    assertNotNull(flowRunTable);\r\n    hbaseConf.unset(YarnConfiguration.TIMELINE_SERVICE_HBASE_SCHEMA_PREFIX_NAME);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-yarn-project\\hadoop-yarn\\hadoop-yarn-server\\hadoop-yarn-server-timelineservice-hbase-tests\\src\\test\\java\\org\\apache\\hadoop\\yarn\\server\\timelineservice\\storage",
  "methodName" : "tearDownAfterClass",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void tearDownAfterClass() throws Exception\n{\r\n    if (util != null) {\r\n        util.shutdownMiniCluster();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-yarn-project\\hadoop-yarn\\hadoop-yarn-server\\hadoop-yarn-server-timelineservice-hbase-tests\\src\\test\\java\\org\\apache\\hadoop\\yarn\\server\\timelineservice\\storage\\flow",
  "methodName" : "getEntityMetricsApp1",
  "errType" : null,
  "containingMethodsNum" : 18,
  "sourceCodeText" : "TimelineEntity getEntityMetricsApp1(long insertTs, Configuration c1)\n{\r\n    TimelineEntity entity = new TimelineEntity();\r\n    String id = \"flowRunMetrics_test\";\r\n    String type = TimelineEntityType.YARN_APPLICATION.toString();\r\n    entity.setId(id);\r\n    entity.setType(type);\r\n    long cTime = 1425016501000L;\r\n    entity.setCreatedTime(cTime);\r\n    Set<TimelineMetric> metrics = new HashSet<>();\r\n    TimelineMetric m1 = new TimelineMetric();\r\n    m1.setId(METRIC_1);\r\n    Map<Long, Number> metricValues = new HashMap<Long, Number>();\r\n    long ts = insertTs;\r\n    for (int k = 1; k < 100; k++) {\r\n        metricValues.put(ts - k * 200000L, 20L);\r\n    }\r\n    metricValues.put(ts - 80000, 40L);\r\n    m1.setType(Type.TIME_SERIES);\r\n    m1.setValues(metricValues);\r\n    metrics.add(m1);\r\n    TimelineMetric m2 = new TimelineMetric();\r\n    m2.setId(METRIC_2);\r\n    metricValues = new HashMap<Long, Number>();\r\n    ts = System.currentTimeMillis();\r\n    for (int k = 1; k < 100; k++) {\r\n        metricValues.put(ts - k * 100000L, 31L);\r\n    }\r\n    metricValues.put(ts - 80000, 57L);\r\n    m2.setType(Type.TIME_SERIES);\r\n    m2.setValues(metricValues);\r\n    metrics.add(m2);\r\n    entity.addMetrics(metrics);\r\n    return entity;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-yarn-project\\hadoop-yarn\\hadoop-yarn-server\\hadoop-yarn-server-timelineservice-hbase-tests\\src\\test\\java\\org\\apache\\hadoop\\yarn\\server\\timelineservice\\storage\\flow",
  "methodName" : "getEntityMetricsApp1Complete",
  "errType" : null,
  "containingMethodsNum" : 19,
  "sourceCodeText" : "TimelineEntity getEntityMetricsApp1Complete(long insertTs, Configuration c1)\n{\r\n    TimelineEntity entity = new TimelineEntity();\r\n    String id = \"flowRunMetrics_test\";\r\n    String type = TimelineEntityType.YARN_APPLICATION.toString();\r\n    entity.setId(id);\r\n    entity.setType(type);\r\n    long cTime = 1425016501000L;\r\n    entity.setCreatedTime(cTime);\r\n    Set<TimelineMetric> metrics = new HashSet<>();\r\n    TimelineMetric m1 = new TimelineMetric();\r\n    m1.setId(METRIC_1);\r\n    Map<Long, Number> metricValues = new HashMap<Long, Number>();\r\n    long ts = insertTs;\r\n    metricValues.put(ts - 80000, 40L);\r\n    m1.setType(Type.TIME_SERIES);\r\n    m1.setValues(metricValues);\r\n    metrics.add(m1);\r\n    TimelineMetric m2 = new TimelineMetric();\r\n    m2.setId(METRIC_2);\r\n    metricValues = new HashMap<Long, Number>();\r\n    ts = insertTs;\r\n    metricValues.put(ts - 80000, 57L);\r\n    m2.setType(Type.TIME_SERIES);\r\n    m2.setValues(metricValues);\r\n    metrics.add(m2);\r\n    entity.addMetrics(metrics);\r\n    TimelineEvent event = new TimelineEvent();\r\n    event.setId(ApplicationMetricsConstants.FINISHED_EVENT_TYPE);\r\n    event.setTimestamp(insertTs);\r\n    event.addInfo(\"done\", \"insertTs=\" + insertTs);\r\n    entity.addEvent(event);\r\n    return entity;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-yarn-project\\hadoop-yarn\\hadoop-yarn-server\\hadoop-yarn-server-timelineservice-hbase-tests\\src\\test\\java\\org\\apache\\hadoop\\yarn\\server\\timelineservice\\storage\\flow",
  "methodName" : "getEntityMetricsApp1",
  "errType" : null,
  "containingMethodsNum" : 21,
  "sourceCodeText" : "TimelineEntity getEntityMetricsApp1(long insertTs)\n{\r\n    TimelineEntity entity = new TimelineEntity();\r\n    String id = \"flowRunMetrics_test\";\r\n    String type = TimelineEntityType.YARN_APPLICATION.toString();\r\n    entity.setId(id);\r\n    entity.setType(type);\r\n    long cTime = 1425016501000L;\r\n    entity.setCreatedTime(cTime);\r\n    Set<TimelineMetric> metrics = new HashSet<>();\r\n    TimelineMetric m1 = new TimelineMetric();\r\n    m1.setId(METRIC_1);\r\n    Map<Long, Number> metricValues = new HashMap<Long, Number>();\r\n    long ts = insertTs;\r\n    metricValues.put(ts - 100000, 2L);\r\n    metricValues.put(ts - 80000, 40L);\r\n    m1.setType(Type.TIME_SERIES);\r\n    m1.setValues(metricValues);\r\n    metrics.add(m1);\r\n    TimelineMetric m2 = new TimelineMetric();\r\n    m2.setId(METRIC_2);\r\n    metricValues = new HashMap<Long, Number>();\r\n    ts = insertTs;\r\n    metricValues.put(ts - 100000, 31L);\r\n    metricValues.put(ts - 80000, 57L);\r\n    m2.setType(Type.TIME_SERIES);\r\n    m2.setValues(metricValues);\r\n    metrics.add(m2);\r\n    entity.addMetrics(metrics);\r\n    TimelineEvent event = new TimelineEvent();\r\n    event.setId(ApplicationMetricsConstants.CREATED_EVENT_TYPE);\r\n    long endTs = 1439379885000L;\r\n    event.setTimestamp(endTs);\r\n    String expKey = \"foo_event_greater\";\r\n    String expVal = \"test_app_greater\";\r\n    event.addInfo(expKey, expVal);\r\n    entity.addEvent(event);\r\n    return entity;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-yarn-project\\hadoop-yarn\\hadoop-yarn-server\\hadoop-yarn-server-timelineservice-hbase-tests\\src\\test\\java\\org\\apache\\hadoop\\yarn\\server\\timelineservice\\storage\\flow",
  "methodName" : "getEntityMetricsApp2",
  "errType" : null,
  "containingMethodsNum" : 15,
  "sourceCodeText" : "TimelineEntity getEntityMetricsApp2(long insertTs)\n{\r\n    TimelineEntity entity = new TimelineEntity();\r\n    String id = \"flowRunMetrics_test\";\r\n    String type = TimelineEntityType.YARN_APPLICATION.toString();\r\n    entity.setId(id);\r\n    entity.setType(type);\r\n    long cTime = 1425016501000L;\r\n    entity.setCreatedTime(cTime);\r\n    Set<TimelineMetric> metrics = new HashSet<>();\r\n    TimelineMetric m1 = new TimelineMetric();\r\n    m1.setId(METRIC_1);\r\n    Map<Long, Number> metricValues = new HashMap<Long, Number>();\r\n    long ts = insertTs;\r\n    metricValues.put(ts - 100000, 5L);\r\n    metricValues.put(ts - 80000, 101L);\r\n    m1.setType(Type.TIME_SERIES);\r\n    m1.setValues(metricValues);\r\n    metrics.add(m1);\r\n    entity.addMetrics(metrics);\r\n    TimelineEvent event = new TimelineEvent();\r\n    event.setId(ApplicationMetricsConstants.CREATED_EVENT_TYPE);\r\n    long endTs = 1439379885000L;\r\n    event.setTimestamp(endTs);\r\n    String expKey = \"foo_event_greater\";\r\n    String expVal = \"test_app_greater\";\r\n    event.addInfo(expKey, expVal);\r\n    entity.addEvent(event);\r\n    return entity;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-yarn-project\\hadoop-yarn\\hadoop-yarn-server\\hadoop-yarn-server-timelineservice-hbase-tests\\src\\test\\java\\org\\apache\\hadoop\\yarn\\server\\timelineservice\\storage\\flow",
  "methodName" : "getEntity1",
  "errType" : null,
  "containingMethodsNum" : 24,
  "sourceCodeText" : "TimelineEntity getEntity1()\n{\r\n    TimelineEntity entity = new TimelineEntity();\r\n    String id = \"flowRunHello\";\r\n    String type = TimelineEntityType.YARN_APPLICATION.toString();\r\n    entity.setId(id);\r\n    entity.setType(type);\r\n    long cTime = 1425026901000L;\r\n    entity.setCreatedTime(cTime);\r\n    Set<TimelineMetric> metrics = new HashSet<>();\r\n    TimelineMetric m1 = new TimelineMetric();\r\n    m1.setId(METRIC_1);\r\n    Map<Long, Number> metricValues = new HashMap<Long, Number>();\r\n    long ts = System.currentTimeMillis();\r\n    metricValues.put(ts - 120000, 100000000L);\r\n    metricValues.put(ts - 100000, 200000000L);\r\n    metricValues.put(ts - 80000, 300000000L);\r\n    metricValues.put(ts - 60000, 400000000L);\r\n    metricValues.put(ts - 40000, 50000000000L);\r\n    metricValues.put(ts - 20000, 60000000000L);\r\n    m1.setType(Type.TIME_SERIES);\r\n    m1.setValues(metricValues);\r\n    metrics.add(m1);\r\n    entity.addMetrics(metrics);\r\n    TimelineEvent event = new TimelineEvent();\r\n    event.setId(ApplicationMetricsConstants.CREATED_EVENT_TYPE);\r\n    event.setTimestamp(cTime);\r\n    String expKey = \"foo_event\";\r\n    Object expVal = \"test\";\r\n    event.addInfo(expKey, expVal);\r\n    entity.addEvent(event);\r\n    event = new TimelineEvent();\r\n    event.setId(ApplicationMetricsConstants.FINISHED_EVENT_TYPE);\r\n    long expTs = cTime + 21600000;\r\n    event.setTimestamp(expTs);\r\n    event.addInfo(expKey, expVal);\r\n    entity.addEvent(event);\r\n    return entity;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-yarn-project\\hadoop-yarn\\hadoop-yarn-server\\hadoop-yarn-server-timelineservice-hbase-tests\\src\\test\\java\\org\\apache\\hadoop\\yarn\\server\\timelineservice\\storage\\flow",
  "methodName" : "getAFullEntity",
  "errType" : null,
  "containingMethodsNum" : 29,
  "sourceCodeText" : "TimelineEntity getAFullEntity(long ts, long endTs)\n{\r\n    TimelineEntity entity = new TimelineEntity();\r\n    String id = \"flowRunFullEntity\";\r\n    String type = TimelineEntityType.YARN_APPLICATION.toString();\r\n    entity.setId(id);\r\n    entity.setType(type);\r\n    entity.setCreatedTime(ts);\r\n    Set<TimelineMetric> metrics = new HashSet<>();\r\n    TimelineMetric m1 = new TimelineMetric();\r\n    m1.setId(METRIC_1);\r\n    Map<Long, Number> metricValues = new HashMap<Long, Number>();\r\n    metricValues.put(ts - 120000, 100000000L);\r\n    metricValues.put(ts - 100000, 200000000L);\r\n    metricValues.put(ts - 80000, 300000000L);\r\n    metricValues.put(ts - 60000, 400000000L);\r\n    metricValues.put(ts - 40000, 50000000000L);\r\n    metricValues.put(ts - 20000, 60000000000L);\r\n    m1.setType(Type.TIME_SERIES);\r\n    m1.setValues(metricValues);\r\n    metrics.add(m1);\r\n    TimelineMetric m2 = new TimelineMetric();\r\n    m2.setId(METRIC_2);\r\n    metricValues = new HashMap<Long, Number>();\r\n    metricValues.put(ts - 900000, 31L);\r\n    metricValues.put(ts - 30000, 57L);\r\n    m2.setType(Type.TIME_SERIES);\r\n    m2.setValues(metricValues);\r\n    metrics.add(m2);\r\n    entity.addMetrics(metrics);\r\n    TimelineEvent event = new TimelineEvent();\r\n    event.setId(ApplicationMetricsConstants.CREATED_EVENT_TYPE);\r\n    event.setTimestamp(ts);\r\n    String expKey = \"foo_event\";\r\n    Object expVal = \"test\";\r\n    event.addInfo(expKey, expVal);\r\n    entity.addEvent(event);\r\n    event = new TimelineEvent();\r\n    event.setId(ApplicationMetricsConstants.FINISHED_EVENT_TYPE);\r\n    long expTs = ts + 21600000;\r\n    event.setTimestamp(expTs);\r\n    event.addInfo(expKey, expVal);\r\n    entity.addEvent(event);\r\n    return entity;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-yarn-project\\hadoop-yarn\\hadoop-yarn-server\\hadoop-yarn-server-timelineservice-hbase-tests\\src\\test\\java\\org\\apache\\hadoop\\yarn\\server\\timelineservice\\storage\\flow",
  "methodName" : "getEntityGreaterStartTime",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "TimelineEntity getEntityGreaterStartTime(long startTs)\n{\r\n    TimelineEntity entity = new TimelineEntity();\r\n    entity.setCreatedTime(startTs);\r\n    entity.setId(\"flowRunHello with greater start time\");\r\n    String type = TimelineEntityType.YARN_APPLICATION.toString();\r\n    entity.setType(type);\r\n    TimelineEvent event = new TimelineEvent();\r\n    event.setId(ApplicationMetricsConstants.CREATED_EVENT_TYPE);\r\n    event.setTimestamp(startTs);\r\n    String expKey = \"foo_event_greater\";\r\n    String expVal = \"test_app_greater\";\r\n    event.addInfo(expKey, expVal);\r\n    entity.addEvent(event);\r\n    return entity;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-yarn-project\\hadoop-yarn\\hadoop-yarn-server\\hadoop-yarn-server-timelineservice-hbase-tests\\src\\test\\java\\org\\apache\\hadoop\\yarn\\server\\timelineservice\\storage\\flow",
  "methodName" : "getEntityMaxEndTime",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "TimelineEntity getEntityMaxEndTime(long endTs)\n{\r\n    TimelineEntity entity = new TimelineEntity();\r\n    entity.setId(\"flowRunHello Max End time\");\r\n    entity.setType(TimelineEntityType.YARN_APPLICATION.toString());\r\n    TimelineEvent event = new TimelineEvent();\r\n    event.setId(ApplicationMetricsConstants.FINISHED_EVENT_TYPE);\r\n    event.setTimestamp(endTs);\r\n    String expKey = \"foo_even_max_ finished\";\r\n    String expVal = \"test_app_max_finished\";\r\n    event.addInfo(expKey, expVal);\r\n    entity.addEvent(event);\r\n    return entity;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-yarn-project\\hadoop-yarn\\hadoop-yarn-server\\hadoop-yarn-server-timelineservice-hbase-tests\\src\\test\\java\\org\\apache\\hadoop\\yarn\\server\\timelineservice\\storage\\flow",
  "methodName" : "getEntityMinStartTime",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "TimelineEntity getEntityMinStartTime(long startTs)\n{\r\n    TimelineEntity entity = new TimelineEntity();\r\n    String id = \"flowRunHelloMInStartTime\";\r\n    String type = TimelineEntityType.YARN_APPLICATION.toString();\r\n    entity.setId(id);\r\n    entity.setType(type);\r\n    entity.setCreatedTime(startTs);\r\n    TimelineEvent event = new TimelineEvent();\r\n    event.setId(ApplicationMetricsConstants.CREATED_EVENT_TYPE);\r\n    event.setTimestamp(startTs);\r\n    entity.addEvent(event);\r\n    return entity;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-yarn-project\\hadoop-yarn\\hadoop-yarn-server\\hadoop-yarn-server-timelineservice-hbase-tests\\src\\test\\java\\org\\apache\\hadoop\\yarn\\server\\timelineservice\\storage\\flow",
  "methodName" : "getMinFlushEntity",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "TimelineEntity getMinFlushEntity(long startTs)\n{\r\n    TimelineEntity entity = new TimelineEntity();\r\n    String id = \"flowRunHelloFlushEntityMin\";\r\n    String type = TimelineEntityType.YARN_APPLICATION.toString();\r\n    entity.setId(id);\r\n    entity.setType(type);\r\n    entity.setCreatedTime(startTs);\r\n    TimelineEvent event = new TimelineEvent();\r\n    event.setId(ApplicationMetricsConstants.CREATED_EVENT_TYPE);\r\n    event.setTimestamp(startTs);\r\n    entity.addEvent(event);\r\n    return entity;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-yarn-project\\hadoop-yarn\\hadoop-yarn-server\\hadoop-yarn-server-timelineservice-hbase-tests\\src\\test\\java\\org\\apache\\hadoop\\yarn\\server\\timelineservice\\storage\\flow",
  "methodName" : "getMaxFlushEntity",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "TimelineEntity getMaxFlushEntity(long startTs)\n{\r\n    TimelineEntity entity = new TimelineEntity();\r\n    String id = \"flowRunHelloFlushEntityMax\";\r\n    String type = TimelineEntityType.YARN_APPLICATION.toString();\r\n    entity.setId(id);\r\n    entity.setType(type);\r\n    entity.setCreatedTime(startTs);\r\n    TimelineEvent event = new TimelineEvent();\r\n    event.setId(ApplicationMetricsConstants.FINISHED_EVENT_TYPE);\r\n    event.setTimestamp(startTs + END_TS_INCR);\r\n    entity.addEvent(event);\r\n    return entity;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-yarn-project\\hadoop-yarn\\hadoop-yarn-server\\hadoop-yarn-server-timelineservice-hbase-tests\\src\\test\\java\\org\\apache\\hadoop\\yarn\\server\\timelineservice\\storage\\flow",
  "methodName" : "getFlowApp1",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "TimelineEntity getFlowApp1(long appCreatedTime)\n{\r\n    TimelineEntity entity = new TimelineEntity();\r\n    String id = \"flowActivity_test\";\r\n    String type = TimelineEntityType.YARN_APPLICATION.toString();\r\n    entity.setId(id);\r\n    entity.setType(type);\r\n    entity.setCreatedTime(appCreatedTime);\r\n    TimelineEvent event = new TimelineEvent();\r\n    event.setId(ApplicationMetricsConstants.CREATED_EVENT_TYPE);\r\n    event.setTimestamp(appCreatedTime);\r\n    String expKey = \"foo_event\";\r\n    Object expVal = \"test\";\r\n    event.addInfo(expKey, expVal);\r\n    entity.addEvent(event);\r\n    return entity;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-yarn-project\\hadoop-yarn\\hadoop-yarn-server\\hadoop-yarn-server-timelineservice-hbase-tests\\src\\test\\java\\org\\apache\\hadoop\\yarn\\server\\timelineservice\\storage\\flow",
  "methodName" : "setupBeforeClass",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void setupBeforeClass() throws Exception\n{\r\n    util = new HBaseTestingUtility();\r\n    Configuration conf = util.getConfiguration();\r\n    conf.setInt(\"hfile.format.version\", 3);\r\n    util.startMiniCluster();\r\n    DataGeneratorForTest.createSchema(util.getConfiguration());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-yarn-project\\hadoop-yarn\\hadoop-yarn-server\\hadoop-yarn-server-timelineservice-hbase-tests\\src\\test\\java\\org\\apache\\hadoop\\yarn\\server\\timelineservice\\storage\\flow",
  "methodName" : "checkCoProcessorOff",
  "errType" : null,
  "containingMethodsNum" : 15,
  "sourceCodeText" : "void checkCoProcessorOff() throws Exception, InterruptedException\n{\r\n    Configuration hbaseConf = util.getConfiguration();\r\n    TableName table = BaseTableRW.getTableName(hbaseConf, FlowRunTableRW.TABLE_NAME_CONF_NAME, FlowRunTableRW.DEFAULT_TABLE_NAME);\r\n    Connection conn = null;\r\n    conn = ConnectionFactory.createConnection(hbaseConf);\r\n    Admin admin = conn.getAdmin();\r\n    if (admin == null) {\r\n        throw new IOException(\"Can't check tables since admin is null\");\r\n    }\r\n    if (admin.tableExists(table)) {\r\n        util.waitUntilAllRegionsAssigned(table);\r\n        checkCoprocessorExists(table, true);\r\n    }\r\n    table = BaseTableRW.getTableName(hbaseConf, FlowActivityTableRW.TABLE_NAME_CONF_NAME, FlowActivityTableRW.DEFAULT_TABLE_NAME);\r\n    if (admin.tableExists(table)) {\r\n        util.waitUntilAllRegionsAssigned(table);\r\n        checkCoprocessorExists(table, false);\r\n    }\r\n    table = BaseTableRW.getTableName(hbaseConf, EntityTableRW.TABLE_NAME_CONF_NAME, EntityTableRW.DEFAULT_TABLE_NAME);\r\n    if (admin.tableExists(table)) {\r\n        util.waitUntilAllRegionsAssigned(table);\r\n        checkCoprocessorExists(table, false);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-yarn-project\\hadoop-yarn\\hadoop-yarn-server\\hadoop-yarn-server-timelineservice-hbase-tests\\src\\test\\java\\org\\apache\\hadoop\\yarn\\server\\timelineservice\\storage\\flow",
  "methodName" : "checkCoprocessorExists",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void checkCoprocessorExists(TableName table, boolean exists) throws Exception\n{\r\n    HRegionServer server = util.getRSForFirstRegionInTable(table);\r\n    HBaseTimelineServerUtils.validateFlowRunCoprocessor(server, table, exists);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-yarn-project\\hadoop-yarn\\hadoop-yarn-server\\hadoop-yarn-server-timelineservice-hbase-tests\\src\\test\\java\\org\\apache\\hadoop\\yarn\\server\\timelineservice\\storage\\flow",
  "methodName" : "testWriteFlowRunMinMax",
  "errType" : null,
  "containingMethodsNum" : 36,
  "sourceCodeText" : "void testWriteFlowRunMinMax() throws Exception\n{\r\n    TimelineEntities te = new TimelineEntities();\r\n    te.addEntity(TestFlowDataGenerator.getEntity1());\r\n    HBaseTimelineWriterImpl hbi = null;\r\n    Configuration c1 = util.getConfiguration();\r\n    String cluster = \"testWriteFlowRunMinMaxToHBase_cluster1\";\r\n    String user = \"testWriteFlowRunMinMaxToHBase_user1\";\r\n    String flow = \"testing_flowRun_flow_name\";\r\n    String flowVersion = \"CF7022C10F1354\";\r\n    long runid = 1002345678919L;\r\n    String appName = \"application_100000000000_1111\";\r\n    long minStartTs = 1425026900000L;\r\n    long greaterStartTs = 30000000000000L;\r\n    long endTs = 1439750690000L;\r\n    TimelineEntity entityMinStartTime = TestFlowDataGenerator.getEntityMinStartTime(minStartTs);\r\n    try {\r\n        hbi = new HBaseTimelineWriterImpl();\r\n        hbi.init(c1);\r\n        UserGroupInformation remoteUser = UserGroupInformation.createRemoteUser(user);\r\n        hbi.write(new TimelineCollectorContext(cluster, user, flow, flowVersion, runid, appName), te, remoteUser);\r\n        te = new TimelineEntities();\r\n        te.addEntity(entityMinStartTime);\r\n        appName = \"application_100000000000_3333\";\r\n        hbi.write(new TimelineCollectorContext(cluster, user, flow, flowVersion, runid, appName), te, remoteUser);\r\n        TimelineEntity entityMaxEndTime = TestFlowDataGenerator.getEntityMaxEndTime(endTs);\r\n        te = new TimelineEntities();\r\n        te.addEntity(entityMaxEndTime);\r\n        appName = \"application_100000000000_4444\";\r\n        hbi.write(new TimelineCollectorContext(cluster, user, flow, flowVersion, runid, appName), te, remoteUser);\r\n        TimelineEntity entityGreaterStartTime = TestFlowDataGenerator.getEntityGreaterStartTime(greaterStartTs);\r\n        te = new TimelineEntities();\r\n        te.addEntity(entityGreaterStartTime);\r\n        appName = \"application_1000000000000000_2222\";\r\n        hbi.write(new TimelineCollectorContext(cluster, user, flow, flowVersion, runid, appName), te, remoteUser);\r\n        hbi.flush();\r\n    } finally {\r\n        if (hbi != null) {\r\n            hbi.close();\r\n        }\r\n    }\r\n    Connection conn = ConnectionFactory.createConnection(c1);\r\n    Table table1 = conn.getTable(BaseTableRW.getTableName(c1, FlowRunTableRW.TABLE_NAME_CONF_NAME, FlowRunTableRW.DEFAULT_TABLE_NAME));\r\n    byte[] startRow = new FlowRunRowKey(cluster, user, flow, runid).getRowKey();\r\n    Get g = new Get(startRow);\r\n    g.addColumn(FlowRunColumnFamily.INFO.getBytes(), FlowRunColumn.MIN_START_TIME.getColumnQualifierBytes());\r\n    g.addColumn(FlowRunColumnFamily.INFO.getBytes(), FlowRunColumn.MAX_END_TIME.getColumnQualifierBytes());\r\n    Result r1 = table1.get(g);\r\n    assertNotNull(r1);\r\n    assertTrue(!r1.isEmpty());\r\n    Map<byte[], byte[]> values = r1.getFamilyMap(FlowRunColumnFamily.INFO.getBytes());\r\n    assertEquals(2, r1.size());\r\n    long starttime = Bytes.toLong(values.get(FlowRunColumn.MIN_START_TIME.getColumnQualifierBytes()));\r\n    assertEquals(minStartTs, starttime);\r\n    assertEquals(endTs, Bytes.toLong(values.get(FlowRunColumn.MAX_END_TIME.getColumnQualifierBytes())));\r\n    HBaseTimelineReaderImpl hbr = null;\r\n    try {\r\n        hbr = new HBaseTimelineReaderImpl();\r\n        hbr.init(c1);\r\n        hbr.start();\r\n        TimelineEntity entity = hbr.getEntity(new TimelineReaderContext(cluster, user, flow, runid, null, TimelineEntityType.YARN_FLOW_RUN.toString(), null), new TimelineDataToRetrieve());\r\n        assertTrue(TimelineEntityType.YARN_FLOW_RUN.matches(entity.getType()));\r\n        FlowRunEntity flowRun = (FlowRunEntity) entity;\r\n        assertEquals(minStartTs, flowRun.getStartTime());\r\n        assertEquals(endTs, flowRun.getMaxEndTime());\r\n    } finally {\r\n        if (hbr != null) {\r\n            hbr.close();\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 2,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-yarn-project\\hadoop-yarn\\hadoop-yarn-server\\hadoop-yarn-server-timelineservice-hbase-tests\\src\\test\\java\\org\\apache\\hadoop\\yarn\\server\\timelineservice\\storage\\flow",
  "methodName" : "testWriteFlowRunMetricsOneFlow",
  "errType" : null,
  "containingMethodsNum" : 27,
  "sourceCodeText" : "void testWriteFlowRunMetricsOneFlow() throws Exception\n{\r\n    String cluster = \"testWriteFlowRunMetricsOneFlow_cluster1\";\r\n    String user = \"testWriteFlowRunMetricsOneFlow_user1\";\r\n    String flow = \"testing_flowRun_metrics_flow_name\";\r\n    String flowVersion = \"CF7022C10F1354\";\r\n    long runid = 1002345678919L;\r\n    TimelineEntities te = new TimelineEntities();\r\n    TimelineEntity entityApp1 = TestFlowDataGenerator.getEntityMetricsApp1(System.currentTimeMillis());\r\n    te.addEntity(entityApp1);\r\n    HBaseTimelineWriterImpl hbi = null;\r\n    Configuration c1 = util.getConfiguration();\r\n    try {\r\n        hbi = new HBaseTimelineWriterImpl();\r\n        hbi.init(c1);\r\n        UserGroupInformation remoteUser = UserGroupInformation.createRemoteUser(user);\r\n        String appName = \"application_11111111111111_1111\";\r\n        hbi.write(new TimelineCollectorContext(cluster, user, flow, flowVersion, runid, appName), te, remoteUser);\r\n        te = new TimelineEntities();\r\n        TimelineEntity entityApp2 = TestFlowDataGenerator.getEntityMetricsApp2(System.currentTimeMillis());\r\n        te.addEntity(entityApp2);\r\n        appName = \"application_11111111111111_2222\";\r\n        hbi.write(new TimelineCollectorContext(cluster, user, flow, flowVersion, runid, appName), te, remoteUser);\r\n        hbi.flush();\r\n    } finally {\r\n        if (hbi != null) {\r\n            hbi.close();\r\n        }\r\n    }\r\n    checkFlowRunTable(cluster, user, flow, runid, c1);\r\n    checkFlowRunTableBatchLimit(cluster, user, flow, runid, c1);\r\n    HBaseTimelineReaderImpl hbr = null;\r\n    try {\r\n        hbr = new HBaseTimelineReaderImpl();\r\n        hbr.init(c1);\r\n        hbr.start();\r\n        TimelineEntity entity = hbr.getEntity(new TimelineReaderContext(cluster, user, flow, runid, null, TimelineEntityType.YARN_FLOW_RUN.toString(), null), new TimelineDataToRetrieve());\r\n        assertTrue(TimelineEntityType.YARN_FLOW_RUN.matches(entity.getType()));\r\n        Set<TimelineMetric> metrics = entity.getMetrics();\r\n        assertEquals(2, metrics.size());\r\n        for (TimelineMetric metric : metrics) {\r\n            String id = metric.getId();\r\n            Map<Long, Number> values = metric.getValues();\r\n            assertEquals(1, values.size());\r\n            Number value = null;\r\n            for (Number n : values.values()) {\r\n                value = n;\r\n            }\r\n            switch(id) {\r\n                case METRIC1:\r\n                    assertEquals(141L, value);\r\n                    break;\r\n                case METRIC2:\r\n                    assertEquals(57L, value);\r\n                    break;\r\n                default:\r\n                    fail(\"unrecognized metric: \" + id);\r\n            }\r\n        }\r\n    } finally {\r\n        if (hbr != null) {\r\n            hbr.close();\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 2,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-yarn-project\\hadoop-yarn\\hadoop-yarn-server\\hadoop-yarn-server-timelineservice-hbase-tests\\src\\test\\java\\org\\apache\\hadoop\\yarn\\server\\timelineservice\\storage\\flow",
  "methodName" : "checkFlowRunTableBatchLimit",
  "errType" : null,
  "containingMethodsNum" : 66,
  "sourceCodeText" : "void checkFlowRunTableBatchLimit(String cluster, String user, String flow, long runid, Configuration c1) throws IOException\n{\r\n    Scan s = new Scan();\r\n    s.addFamily(FlowRunColumnFamily.INFO.getBytes());\r\n    byte[] startRow = new FlowRunRowKey(cluster, user, flow, runid).getRowKey();\r\n    s.setStartRow(startRow);\r\n    int batchLimit = 2;\r\n    s.setBatch(batchLimit);\r\n    String clusterStop = cluster + \"1\";\r\n    byte[] stopRow = new FlowRunRowKey(clusterStop, user, flow, runid).getRowKey();\r\n    s.setStopRow(stopRow);\r\n    Connection conn = ConnectionFactory.createConnection(c1);\r\n    Table table1 = conn.getTable(BaseTableRW.getTableName(c1, FlowRunTableRW.TABLE_NAME_CONF_NAME, FlowRunTableRW.DEFAULT_TABLE_NAME));\r\n    ResultScanner scanner = table1.getScanner(s);\r\n    int loopCount = 0;\r\n    for (Result result : scanner) {\r\n        assertNotNull(result);\r\n        assertTrue(!result.isEmpty());\r\n        assertTrue(result.rawCells().length <= batchLimit);\r\n        Map<byte[], byte[]> values = result.getFamilyMap(FlowRunColumnFamily.INFO.getBytes());\r\n        assertNotNull(values);\r\n        assertTrue(values.size() <= batchLimit);\r\n        loopCount++;\r\n    }\r\n    assertTrue(loopCount > 0);\r\n    s = new Scan();\r\n    s.addFamily(FlowRunColumnFamily.INFO.getBytes());\r\n    s.setStartRow(startRow);\r\n    batchLimit = 1;\r\n    s.setBatch(batchLimit);\r\n    s.setMaxResultsPerColumnFamily(2);\r\n    s.setStopRow(stopRow);\r\n    scanner = table1.getScanner(s);\r\n    loopCount = 0;\r\n    for (Result result : scanner) {\r\n        assertNotNull(result);\r\n        assertTrue(!result.isEmpty());\r\n        assertEquals(batchLimit, result.rawCells().length);\r\n        Map<byte[], byte[]> values = result.getFamilyMap(FlowRunColumnFamily.INFO.getBytes());\r\n        assertNotNull(values);\r\n        assertEquals(batchLimit, values.size());\r\n        loopCount++;\r\n    }\r\n    assertTrue(loopCount > 0);\r\n    s = new Scan();\r\n    s.addFamily(FlowRunColumnFamily.INFO.getBytes());\r\n    s.setStartRow(startRow);\r\n    batchLimit = 100;\r\n    s.setBatch(batchLimit);\r\n    s.setStopRow(stopRow);\r\n    scanner = table1.getScanner(s);\r\n    loopCount = 0;\r\n    for (Result result : scanner) {\r\n        assertNotNull(result);\r\n        assertTrue(!result.isEmpty());\r\n        assertTrue(result.rawCells().length <= batchLimit);\r\n        Map<byte[], byte[]> values = result.getFamilyMap(FlowRunColumnFamily.INFO.getBytes());\r\n        assertNotNull(values);\r\n        assertTrue(values.size() <= batchLimit);\r\n        assertTrue(values.size() == 3);\r\n        loopCount++;\r\n    }\r\n    assertTrue(loopCount == 1);\r\n    s = new Scan();\r\n    s.addFamily(FlowRunColumnFamily.INFO.getBytes());\r\n    s.setStartRow(startRow);\r\n    batchLimit = -671;\r\n    s.setBatch(batchLimit);\r\n    s.setStopRow(stopRow);\r\n    scanner = table1.getScanner(s);\r\n    loopCount = 0;\r\n    for (Result result : scanner) {\r\n        assertNotNull(result);\r\n        assertTrue(!result.isEmpty());\r\n        assertEquals(3, result.rawCells().length);\r\n        Map<byte[], byte[]> values = result.getFamilyMap(FlowRunColumnFamily.INFO.getBytes());\r\n        assertNotNull(values);\r\n        assertEquals(3, values.size());\r\n        loopCount++;\r\n    }\r\n    assertEquals(1, loopCount);\r\n    s = new Scan();\r\n    s.addFamily(FlowRunColumnFamily.INFO.getBytes());\r\n    s.setStartRow(startRow);\r\n    batchLimit = 0;\r\n    s.setBatch(batchLimit);\r\n    s.setStopRow(stopRow);\r\n    scanner = table1.getScanner(s);\r\n    loopCount = 0;\r\n    for (Result result : scanner) {\r\n        assertNotNull(result);\r\n        assertTrue(!result.isEmpty());\r\n        assertEquals(3, result.rawCells().length);\r\n        Map<byte[], byte[]> values = result.getFamilyMap(FlowRunColumnFamily.INFO.getBytes());\r\n        assertNotNull(values);\r\n        assertEquals(3, values.size());\r\n        loopCount++;\r\n    }\r\n    assertEquals(1, loopCount);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-yarn-project\\hadoop-yarn\\hadoop-yarn-server\\hadoop-yarn-server-timelineservice-hbase-tests\\src\\test\\java\\org\\apache\\hadoop\\yarn\\server\\timelineservice\\storage\\flow",
  "methodName" : "checkFlowRunTable",
  "errType" : null,
  "containingMethodsNum" : 19,
  "sourceCodeText" : "void checkFlowRunTable(String cluster, String user, String flow, long runid, Configuration c1) throws IOException\n{\r\n    Scan s = new Scan();\r\n    s.addFamily(FlowRunColumnFamily.INFO.getBytes());\r\n    byte[] startRow = new FlowRunRowKey(cluster, user, flow, runid).getRowKey();\r\n    s.setStartRow(startRow);\r\n    String clusterStop = cluster + \"1\";\r\n    byte[] stopRow = new FlowRunRowKey(clusterStop, user, flow, runid).getRowKey();\r\n    s.setStopRow(stopRow);\r\n    Connection conn = ConnectionFactory.createConnection(c1);\r\n    Table table1 = conn.getTable(BaseTableRW.getTableName(c1, FlowRunTableRW.TABLE_NAME_CONF_NAME, FlowRunTableRW.DEFAULT_TABLE_NAME));\r\n    ResultScanner scanner = table1.getScanner(s);\r\n    int rowCount = 0;\r\n    for (Result result : scanner) {\r\n        assertNotNull(result);\r\n        assertTrue(!result.isEmpty());\r\n        Map<byte[], byte[]> values = result.getFamilyMap(FlowRunColumnFamily.INFO.getBytes());\r\n        rowCount++;\r\n        byte[] q = ColumnHelper.getColumnQualifier(FlowRunColumnPrefix.METRIC.getColumnPrefixBytes(), METRIC1);\r\n        assertTrue(values.containsKey(q));\r\n        assertEquals(141L, Bytes.toLong(values.get(q)));\r\n        assertEquals(3, values.size());\r\n        q = ColumnHelper.getColumnQualifier(FlowRunColumnPrefix.METRIC.getColumnPrefixBytes(), METRIC2);\r\n        assertTrue(values.containsKey(q));\r\n        assertEquals(57L, Bytes.toLong(values.get(q)));\r\n    }\r\n    assertEquals(1, rowCount);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-yarn-project\\hadoop-yarn\\hadoop-yarn-server\\hadoop-yarn-server-timelineservice-hbase-tests\\src\\test\\java\\org\\apache\\hadoop\\yarn\\server\\timelineservice\\storage\\flow",
  "methodName" : "testWriteFlowRunMetricsPrefix",
  "errType" : null,
  "containingMethodsNum" : 29,
  "sourceCodeText" : "void testWriteFlowRunMetricsPrefix() throws Exception\n{\r\n    String cluster = \"testWriteFlowRunMetricsPrefix_cluster1\";\r\n    String user = \"testWriteFlowRunMetricsPrefix_user1\";\r\n    String flow = \"testWriteFlowRunMetricsPrefix_flow_name\";\r\n    String flowVersion = \"CF7022C10F1354\";\r\n    TimelineEntities te = new TimelineEntities();\r\n    TimelineEntity entityApp1 = TestFlowDataGenerator.getEntityMetricsApp1(System.currentTimeMillis());\r\n    te.addEntity(entityApp1);\r\n    HBaseTimelineWriterImpl hbi = null;\r\n    Configuration c1 = util.getConfiguration();\r\n    try {\r\n        hbi = new HBaseTimelineWriterImpl();\r\n        hbi.init(c1);\r\n        UserGroupInformation remoteUser = UserGroupInformation.createRemoteUser(user);\r\n        String appName = \"application_11111111111111_1111\";\r\n        hbi.write(new TimelineCollectorContext(cluster, user, flow, flowVersion, 1002345678919L, appName), te, remoteUser);\r\n        te = new TimelineEntities();\r\n        TimelineEntity entityApp2 = TestFlowDataGenerator.getEntityMetricsApp2(System.currentTimeMillis());\r\n        te.addEntity(entityApp2);\r\n        appName = \"application_11111111111111_2222\";\r\n        hbi.write(new TimelineCollectorContext(cluster, user, flow, flowVersion, 1002345678918L, appName), te, remoteUser);\r\n        hbi.flush();\r\n    } finally {\r\n        if (hbi != null) {\r\n            hbi.close();\r\n        }\r\n    }\r\n    HBaseTimelineReaderImpl hbr = null;\r\n    try {\r\n        hbr = new HBaseTimelineReaderImpl();\r\n        hbr.init(c1);\r\n        hbr.start();\r\n        TimelineFilterList metricsToRetrieve = new TimelineFilterList(Operator.OR, new TimelinePrefixFilter(TimelineCompareOp.EQUAL, METRIC1.substring(0, METRIC1.indexOf(\"_\") + 1)));\r\n        TimelineEntity entity = hbr.getEntity(new TimelineReaderContext(cluster, user, flow, 1002345678919L, null, TimelineEntityType.YARN_FLOW_RUN.toString(), null), new TimelineDataToRetrieve(null, metricsToRetrieve, null, null, null, null));\r\n        assertTrue(TimelineEntityType.YARN_FLOW_RUN.matches(entity.getType()));\r\n        Set<TimelineMetric> metrics = entity.getMetrics();\r\n        assertEquals(1, metrics.size());\r\n        for (TimelineMetric metric : metrics) {\r\n            String id = metric.getId();\r\n            Map<Long, Number> values = metric.getValues();\r\n            assertEquals(1, values.size());\r\n            Number value = null;\r\n            for (Number n : values.values()) {\r\n                value = n;\r\n            }\r\n            switch(id) {\r\n                case METRIC1:\r\n                    assertEquals(40L, value);\r\n                    break;\r\n                default:\r\n                    fail(\"unrecognized metric: \" + id);\r\n            }\r\n        }\r\n        Set<TimelineEntity> entities = hbr.getEntities(new TimelineReaderContext(cluster, user, flow, null, null, TimelineEntityType.YARN_FLOW_RUN.toString(), null), new TimelineEntityFilters.Builder().build(), new TimelineDataToRetrieve(null, metricsToRetrieve, null, null, null, null));\r\n        assertEquals(2, entities.size());\r\n        int metricCnt = 0;\r\n        for (TimelineEntity timelineEntity : entities) {\r\n            metricCnt += timelineEntity.getMetrics().size();\r\n        }\r\n        assertEquals(2, metricCnt);\r\n    } finally {\r\n        if (hbr != null) {\r\n            hbr.close();\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 2,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-yarn-project\\hadoop-yarn\\hadoop-yarn-server\\hadoop-yarn-server-timelineservice-hbase-tests\\src\\test\\java\\org\\apache\\hadoop\\yarn\\server\\timelineservice\\storage\\flow",
  "methodName" : "testWriteFlowRunsMetricFields",
  "errType" : null,
  "containingMethodsNum" : 29,
  "sourceCodeText" : "void testWriteFlowRunsMetricFields() throws Exception\n{\r\n    String cluster = \"testWriteFlowRunsMetricFields_cluster1\";\r\n    String user = \"testWriteFlowRunsMetricFields_user1\";\r\n    String flow = \"testWriteFlowRunsMetricFields_flow_name\";\r\n    String flowVersion = \"CF7022C10F1354\";\r\n    long runid = 1002345678919L;\r\n    TimelineEntities te = new TimelineEntities();\r\n    TimelineEntity entityApp1 = TestFlowDataGenerator.getEntityMetricsApp1(System.currentTimeMillis());\r\n    te.addEntity(entityApp1);\r\n    HBaseTimelineWriterImpl hbi = null;\r\n    Configuration c1 = util.getConfiguration();\r\n    try {\r\n        hbi = new HBaseTimelineWriterImpl();\r\n        hbi.init(c1);\r\n        UserGroupInformation remoteUser = UserGroupInformation.createRemoteUser(user);\r\n        String appName = \"application_11111111111111_1111\";\r\n        hbi.write(new TimelineCollectorContext(cluster, user, flow, flowVersion, runid, appName), te, remoteUser);\r\n        te = new TimelineEntities();\r\n        TimelineEntity entityApp2 = TestFlowDataGenerator.getEntityMetricsApp2(System.currentTimeMillis());\r\n        te.addEntity(entityApp2);\r\n        appName = \"application_11111111111111_2222\";\r\n        hbi.write(new TimelineCollectorContext(cluster, user, flow, flowVersion, runid, appName), te, remoteUser);\r\n        hbi.flush();\r\n    } finally {\r\n        if (hbi != null) {\r\n            hbi.close();\r\n        }\r\n    }\r\n    checkFlowRunTable(cluster, user, flow, runid, c1);\r\n    HBaseTimelineReaderImpl hbr = null;\r\n    try {\r\n        hbr = new HBaseTimelineReaderImpl();\r\n        hbr.init(c1);\r\n        hbr.start();\r\n        Set<TimelineEntity> entities = hbr.getEntities(new TimelineReaderContext(cluster, user, flow, runid, null, TimelineEntityType.YARN_FLOW_RUN.toString(), null), new TimelineEntityFilters.Builder().build(), new TimelineDataToRetrieve());\r\n        assertEquals(1, entities.size());\r\n        for (TimelineEntity timelineEntity : entities) {\r\n            assertEquals(0, timelineEntity.getMetrics().size());\r\n        }\r\n        entities = hbr.getEntities(new TimelineReaderContext(cluster, user, flow, runid, null, TimelineEntityType.YARN_FLOW_RUN.toString(), null), new TimelineEntityFilters.Builder().build(), new TimelineDataToRetrieve(null, null, EnumSet.of(Field.METRICS), null, null, null));\r\n        assertEquals(1, entities.size());\r\n        for (TimelineEntity timelineEntity : entities) {\r\n            Set<TimelineMetric> timelineMetrics = timelineEntity.getMetrics();\r\n            assertEquals(2, timelineMetrics.size());\r\n            for (TimelineMetric metric : timelineMetrics) {\r\n                String id = metric.getId();\r\n                Map<Long, Number> values = metric.getValues();\r\n                assertEquals(1, values.size());\r\n                Number value = null;\r\n                for (Number n : values.values()) {\r\n                    value = n;\r\n                }\r\n                switch(id) {\r\n                    case METRIC1:\r\n                        assertEquals(141L, value);\r\n                        break;\r\n                    case METRIC2:\r\n                        assertEquals(57L, value);\r\n                        break;\r\n                    default:\r\n                        fail(\"unrecognized metric: \" + id);\r\n                }\r\n            }\r\n        }\r\n    } finally {\r\n        if (hbr != null) {\r\n            hbr.close();\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 2,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-yarn-project\\hadoop-yarn\\hadoop-yarn-server\\hadoop-yarn-server-timelineservice-hbase-tests\\src\\test\\java\\org\\apache\\hadoop\\yarn\\server\\timelineservice\\storage\\flow",
  "methodName" : "testWriteFlowRunFlush",
  "errType" : null,
  "containingMethodsNum" : 19,
  "sourceCodeText" : "void testWriteFlowRunFlush() throws Exception\n{\r\n    String cluster = \"atestFlushFlowRun_cluster1\";\r\n    String user = \"atestFlushFlowRun__user1\";\r\n    String flow = \"atestFlushFlowRun_flow_name\";\r\n    String flowVersion = \"AF1021C19F1351\";\r\n    long runid = 1449526652000L;\r\n    int start = 10;\r\n    int count = 20000;\r\n    int appIdSuffix = 1;\r\n    HBaseTimelineWriterImpl hbi = null;\r\n    long insertTs = 1449796654827L - count;\r\n    long minTS = insertTs + 1;\r\n    long startTs = insertTs;\r\n    Configuration c1 = util.getConfiguration();\r\n    TimelineEntities te1 = null;\r\n    TimelineEntity entityApp1 = null;\r\n    TimelineEntity entityApp2 = null;\r\n    try {\r\n        hbi = new HBaseTimelineWriterImpl();\r\n        hbi.init(c1);\r\n        UserGroupInformation remoteUser = UserGroupInformation.createRemoteUser(user);\r\n        for (int i = start; i < count; i++) {\r\n            String appName = \"application_1060350000000_\" + appIdSuffix;\r\n            insertTs++;\r\n            te1 = new TimelineEntities();\r\n            entityApp1 = TestFlowDataGenerator.getMinFlushEntity(insertTs);\r\n            te1.addEntity(entityApp1);\r\n            entityApp2 = TestFlowDataGenerator.getMaxFlushEntity(insertTs);\r\n            te1.addEntity(entityApp2);\r\n            hbi.write(new TimelineCollectorContext(cluster, user, flow, flowVersion, runid, appName), te1, remoteUser);\r\n            Thread.sleep(1);\r\n            appName = \"application_1001199480000_7\" + appIdSuffix;\r\n            insertTs++;\r\n            appIdSuffix++;\r\n            te1 = new TimelineEntities();\r\n            entityApp1 = TestFlowDataGenerator.getMinFlushEntity(insertTs);\r\n            te1.addEntity(entityApp1);\r\n            entityApp2 = TestFlowDataGenerator.getMaxFlushEntity(insertTs);\r\n            te1.addEntity(entityApp2);\r\n            hbi.write(new TimelineCollectorContext(cluster, user, flow, flowVersion, runid, appName), te1, remoteUser);\r\n            if (i % 1000 == 0) {\r\n                hbi.flush();\r\n                checkMinMaxFlush(c1, minTS, startTs, count, cluster, user, flow, runid, false);\r\n            }\r\n        }\r\n    } finally {\r\n        if (hbi != null) {\r\n            hbi.flush();\r\n            hbi.close();\r\n        }\r\n        checkMinMaxFlush(c1, minTS, startTs, count, cluster, user, flow, runid, true);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-yarn-project\\hadoop-yarn\\hadoop-yarn-server\\hadoop-yarn-server-timelineservice-hbase-tests\\src\\test\\java\\org\\apache\\hadoop\\yarn\\server\\timelineservice\\storage\\flow",
  "methodName" : "checkMinMaxFlush",
  "errType" : null,
  "containingMethodsNum" : 13,
  "sourceCodeText" : "void checkMinMaxFlush(Configuration c1, long minTS, long startTs, int count, String cluster, String user, String flow, long runid, boolean checkMax) throws IOException\n{\r\n    Connection conn = ConnectionFactory.createConnection(c1);\r\n    Table table1 = conn.getTable(BaseTableRW.getTableName(c1, FlowRunTableRW.TABLE_NAME_CONF_NAME, FlowRunTableRW.DEFAULT_TABLE_NAME));\r\n    byte[] startRow = new FlowRunRowKey(cluster, user, flow, runid).getRowKey();\r\n    Get g = new Get(startRow);\r\n    g.addColumn(FlowRunColumnFamily.INFO.getBytes(), FlowRunColumn.MIN_START_TIME.getColumnQualifierBytes());\r\n    g.addColumn(FlowRunColumnFamily.INFO.getBytes(), FlowRunColumn.MAX_END_TIME.getColumnQualifierBytes());\r\n    Result r1 = table1.get(g);\r\n    assertNotNull(r1);\r\n    assertTrue(!r1.isEmpty());\r\n    Map<byte[], byte[]> values = r1.getFamilyMap(FlowRunColumnFamily.INFO.getBytes());\r\n    int start = 10;\r\n    assertEquals(2, r1.size());\r\n    long starttime = Bytes.toLong(values.get(FlowRunColumn.MIN_START_TIME.getColumnQualifierBytes()));\r\n    assertEquals(minTS, starttime);\r\n    if (checkMax) {\r\n        assertEquals(startTs + 2 * (count - start) + TestFlowDataGenerator.END_TS_INCR, Bytes.toLong(values.get(FlowRunColumn.MAX_END_TIME.getColumnQualifierBytes())));\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-yarn-project\\hadoop-yarn\\hadoop-yarn-server\\hadoop-yarn-server-timelineservice-hbase-tests\\src\\test\\java\\org\\apache\\hadoop\\yarn\\server\\timelineservice\\storage\\flow",
  "methodName" : "testFilterFlowRunsByCreatedTime",
  "errType" : null,
  "containingMethodsNum" : 29,
  "sourceCodeText" : "void testFilterFlowRunsByCreatedTime() throws Exception\n{\r\n    String cluster = \"cluster2\";\r\n    String user = \"user2\";\r\n    String flow = \"flow_name2\";\r\n    TimelineEntities te = new TimelineEntities();\r\n    TimelineEntity entityApp1 = TestFlowDataGenerator.getEntityMetricsApp1(System.currentTimeMillis());\r\n    entityApp1.setCreatedTime(1425016501000L);\r\n    te.addEntity(entityApp1);\r\n    HBaseTimelineWriterImpl hbi = null;\r\n    Configuration c1 = util.getConfiguration();\r\n    try {\r\n        hbi = new HBaseTimelineWriterImpl();\r\n        hbi.init(c1);\r\n        UserGroupInformation remoteUser = UserGroupInformation.createRemoteUser(user);\r\n        hbi.write(new TimelineCollectorContext(cluster, user, flow, \"CF7022C10F1354\", 1002345678919L, \"application_11111111111111_1111\"), te, remoteUser);\r\n        te = new TimelineEntities();\r\n        TimelineEntity entityApp2 = TestFlowDataGenerator.getEntityMetricsApp2(System.currentTimeMillis());\r\n        entityApp2.setCreatedTime(1425016502000L);\r\n        te.addEntity(entityApp2);\r\n        hbi.write(new TimelineCollectorContext(cluster, user, flow, \"CF7022C10F1354\", 1002345678918L, \"application_11111111111111_2222\"), te, remoteUser);\r\n        hbi.flush();\r\n    } finally {\r\n        if (hbi != null) {\r\n            hbi.close();\r\n        }\r\n    }\r\n    HBaseTimelineReaderImpl hbr = null;\r\n    try {\r\n        hbr = new HBaseTimelineReaderImpl();\r\n        hbr.init(c1);\r\n        hbr.start();\r\n        Set<TimelineEntity> entities = hbr.getEntities(new TimelineReaderContext(cluster, user, flow, null, null, TimelineEntityType.YARN_FLOW_RUN.toString(), null), new TimelineEntityFilters.Builder().createdTimeBegin(1425016501000L).createTimeEnd(1425016502001L).build(), new TimelineDataToRetrieve());\r\n        assertEquals(2, entities.size());\r\n        for (TimelineEntity entity : entities) {\r\n            if (!entity.getId().equals(\"user2@flow_name2/1002345678918\") && !entity.getId().equals(\"user2@flow_name2/1002345678919\")) {\r\n                fail(\"Entities with flow runs 1002345678918 and 1002345678919\" + \"should be present.\");\r\n            }\r\n        }\r\n        entities = hbr.getEntities(new TimelineReaderContext(cluster, user, flow, null, null, TimelineEntityType.YARN_FLOW_RUN.toString(), null), new TimelineEntityFilters.Builder().createdTimeBegin(1425016501050L).build(), new TimelineDataToRetrieve());\r\n        assertEquals(1, entities.size());\r\n        for (TimelineEntity entity : entities) {\r\n            if (!entity.getId().equals(\"user2@flow_name2/1002345678918\")) {\r\n                fail(\"Entity with flow run 1002345678918 should be present.\");\r\n            }\r\n        }\r\n        entities = hbr.getEntities(new TimelineReaderContext(cluster, user, flow, null, null, TimelineEntityType.YARN_FLOW_RUN.toString(), null), new TimelineEntityFilters.Builder().createTimeEnd(1425016501050L).build(), new TimelineDataToRetrieve());\r\n        assertEquals(1, entities.size());\r\n        for (TimelineEntity entity : entities) {\r\n            if (!entity.getId().equals(\"user2@flow_name2/1002345678919\")) {\r\n                fail(\"Entity with flow run 1002345678919 should be present.\");\r\n            }\r\n        }\r\n    } finally {\r\n        if (hbr != null) {\r\n            hbr.close();\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 2,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-yarn-project\\hadoop-yarn\\hadoop-yarn-server\\hadoop-yarn-server-timelineservice-hbase-tests\\src\\test\\java\\org\\apache\\hadoop\\yarn\\server\\timelineservice\\storage\\flow",
  "methodName" : "testMetricFilters",
  "errType" : null,
  "containingMethodsNum" : 37,
  "sourceCodeText" : "void testMetricFilters() throws Exception\n{\r\n    String cluster = \"cluster1\";\r\n    String user = \"user1\";\r\n    String flow = \"flow_name1\";\r\n    TimelineEntities te = new TimelineEntities();\r\n    TimelineEntity entityApp1 = TestFlowDataGenerator.getEntityMetricsApp1(System.currentTimeMillis());\r\n    te.addEntity(entityApp1);\r\n    HBaseTimelineWriterImpl hbi = null;\r\n    Configuration c1 = util.getConfiguration();\r\n    try {\r\n        hbi = new HBaseTimelineWriterImpl();\r\n        hbi.init(c1);\r\n        UserGroupInformation remoteUser = UserGroupInformation.createRemoteUser(user);\r\n        hbi.write(new TimelineCollectorContext(cluster, user, flow, \"CF7022C10F1354\", 1002345678919L, \"application_11111111111111_1111\"), te, remoteUser);\r\n        te = new TimelineEntities();\r\n        TimelineEntity entityApp2 = TestFlowDataGenerator.getEntityMetricsApp2(System.currentTimeMillis());\r\n        te.addEntity(entityApp2);\r\n        hbi.write(new TimelineCollectorContext(cluster, user, flow, \"CF7022C10F1354\", 1002345678918L, \"application_11111111111111_2222\"), te, remoteUser);\r\n        hbi.flush();\r\n    } finally {\r\n        if (hbi != null) {\r\n            hbi.close();\r\n        }\r\n    }\r\n    HBaseTimelineReaderImpl hbr = null;\r\n    try {\r\n        hbr = new HBaseTimelineReaderImpl();\r\n        hbr.init(c1);\r\n        hbr.start();\r\n        TimelineFilterList list1 = new TimelineFilterList();\r\n        list1.addFilter(new TimelineCompareFilter(TimelineCompareOp.GREATER_OR_EQUAL, METRIC1, 101));\r\n        TimelineFilterList list2 = new TimelineFilterList();\r\n        list2.addFilter(new TimelineCompareFilter(TimelineCompareOp.LESS_THAN, METRIC1, 43));\r\n        list2.addFilter(new TimelineCompareFilter(TimelineCompareOp.EQUAL, METRIC2, 57));\r\n        TimelineFilterList metricFilterList = new TimelineFilterList(Operator.OR, list1, list2);\r\n        Set<TimelineEntity> entities = hbr.getEntities(new TimelineReaderContext(cluster, user, flow, null, null, TimelineEntityType.YARN_FLOW_RUN.toString(), null), new TimelineEntityFilters.Builder().metricFilters(metricFilterList).build(), new TimelineDataToRetrieve(null, null, EnumSet.of(Field.METRICS), null, null, null));\r\n        assertEquals(2, entities.size());\r\n        int metricCnt = 0;\r\n        for (TimelineEntity entity : entities) {\r\n            metricCnt += entity.getMetrics().size();\r\n        }\r\n        assertEquals(3, metricCnt);\r\n        TimelineFilterList metricFilterList1 = new TimelineFilterList(new TimelineCompareFilter(TimelineCompareOp.LESS_OR_EQUAL, METRIC1, 127), new TimelineCompareFilter(TimelineCompareOp.NOT_EQUAL, METRIC2, 30));\r\n        entities = hbr.getEntities(new TimelineReaderContext(cluster, user, flow, null, null, TimelineEntityType.YARN_FLOW_RUN.toString(), null), new TimelineEntityFilters.Builder().metricFilters(metricFilterList1).build(), new TimelineDataToRetrieve(null, null, EnumSet.of(Field.METRICS), null, null, null));\r\n        assertEquals(1, entities.size());\r\n        metricCnt = 0;\r\n        for (TimelineEntity entity : entities) {\r\n            metricCnt += entity.getMetrics().size();\r\n        }\r\n        assertEquals(2, metricCnt);\r\n        TimelineFilterList metricFilterList2 = new TimelineFilterList(new TimelineCompareFilter(TimelineCompareOp.LESS_THAN, METRIC1, 32), new TimelineCompareFilter(TimelineCompareOp.NOT_EQUAL, METRIC2, 57));\r\n        entities = hbr.getEntities(new TimelineReaderContext(cluster, user, flow, null, null, TimelineEntityType.YARN_FLOW_RUN.toString(), null), new TimelineEntityFilters.Builder().metricFilters(metricFilterList2).build(), new TimelineDataToRetrieve(null, null, EnumSet.of(Field.METRICS), null, null, null));\r\n        assertEquals(0, entities.size());\r\n        TimelineFilterList metricFilterList3 = new TimelineFilterList(new TimelineCompareFilter(TimelineCompareOp.EQUAL, \"s_metric\", 32));\r\n        entities = hbr.getEntities(new TimelineReaderContext(cluster, user, flow, null, null, TimelineEntityType.YARN_FLOW_RUN.toString(), null), new TimelineEntityFilters.Builder().metricFilters(metricFilterList3).build(), new TimelineDataToRetrieve(null, null, EnumSet.of(Field.METRICS), null, null, null));\r\n        assertEquals(0, entities.size());\r\n        TimelineFilterList list3 = new TimelineFilterList();\r\n        list3.addFilter(new TimelineCompareFilter(TimelineCompareOp.GREATER_OR_EQUAL, METRIC1, 101));\r\n        TimelineFilterList list4 = new TimelineFilterList();\r\n        list4.addFilter(new TimelineCompareFilter(TimelineCompareOp.LESS_THAN, METRIC1, 43));\r\n        list4.addFilter(new TimelineCompareFilter(TimelineCompareOp.EQUAL, METRIC2, 57));\r\n        TimelineFilterList metricFilterList4 = new TimelineFilterList(Operator.OR, list3, list4);\r\n        TimelineFilterList metricsToRetrieve = new TimelineFilterList(Operator.OR, new TimelinePrefixFilter(TimelineCompareOp.EQUAL, METRIC2.substring(0, METRIC2.indexOf(\"_\") + 1)));\r\n        entities = hbr.getEntities(new TimelineReaderContext(cluster, user, flow, null, null, TimelineEntityType.YARN_FLOW_RUN.toString(), null), new TimelineEntityFilters.Builder().metricFilters(metricFilterList4).build(), new TimelineDataToRetrieve(null, metricsToRetrieve, EnumSet.of(Field.ALL), null, null, null));\r\n        assertEquals(2, entities.size());\r\n        metricCnt = 0;\r\n        for (TimelineEntity entity : entities) {\r\n            metricCnt += entity.getMetrics().size();\r\n        }\r\n        assertEquals(1, metricCnt);\r\n    } finally {\r\n        if (hbr != null) {\r\n            hbr.close();\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 2,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-yarn-project\\hadoop-yarn\\hadoop-yarn-server\\hadoop-yarn-server-timelineservice-hbase-tests\\src\\test\\java\\org\\apache\\hadoop\\yarn\\server\\timelineservice\\storage\\flow",
  "methodName" : "tearDownAfterClass",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void tearDownAfterClass() throws Exception\n{\r\n    if (util != null) {\r\n        util.shutdownMiniCluster();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-yarn-project\\hadoop-yarn\\hadoop-yarn-server\\hadoop-yarn-server-timelineservice-hbase-tests\\src\\test\\java\\org\\apache\\hadoop\\yarn\\server\\timelineservice\\storage",
  "methodName" : "testTimelineWriterHBaseDown",
  "errType" : [ "IOException", "IOException" ],
  "containingMethodsNum" : 20,
  "sourceCodeText" : "void testTimelineWriterHBaseDown() throws Exception\n{\r\n    HBaseTestingUtility util = new HBaseTestingUtility();\r\n    HBaseTimelineWriterImpl writer = new HBaseTimelineWriterImpl();\r\n    try {\r\n        util.startMiniCluster();\r\n        Configuration c1 = util.getConfiguration();\r\n        c1.setLong(TIMELINE_SERVICE_READER_STORAGE_MONITOR_INTERVAL_MS, 5000);\r\n        writer.init(c1);\r\n        writer.start();\r\n        DataGeneratorForTest.createSchema(util.getConfiguration());\r\n        TimelineStorageMonitor storageMonitor = writer.getTimelineStorageMonitor();\r\n        waitForHBaseToUp(storageMonitor);\r\n        try {\r\n            storageMonitor.checkStorageIsUp();\r\n        } catch (IOException e) {\r\n            Assert.fail(\"HBaseStorageMonitor failed to detect HBase Up\");\r\n        }\r\n        util.shutdownMiniHBaseCluster();\r\n        waitForHBaseToDown(storageMonitor);\r\n        TimelineEntities te = new TimelineEntities();\r\n        ApplicationEntity entity = new ApplicationEntity();\r\n        String appId = \"application_1000178881110_2002\";\r\n        entity.setId(appId);\r\n        Long cTime = 1425016501000L;\r\n        entity.setCreatedTime(cTime);\r\n        te.addEntity(entity);\r\n        boolean exceptionCaught = false;\r\n        try {\r\n            writer.write(new TimelineCollectorContext(\"ATS1\", \"user1\", \"flow2\", \"AB7822C10F1111\", 1002345678919L, appId), te, UserGroupInformation.createRemoteUser(\"user1\"));\r\n        } catch (IOException e) {\r\n            if (e.getMessage().equals(\"HBase is down\")) {\r\n                exceptionCaught = true;\r\n            }\r\n        }\r\n        assertTrue(\"HBaseStorageMonitor failed to detect HBase Down\", exceptionCaught);\r\n    } finally {\r\n        writer.stop();\r\n        util.shutdownMiniCluster();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 3,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-yarn-project\\hadoop-yarn\\hadoop-yarn-server\\hadoop-yarn-server-timelineservice-hbase-tests\\src\\test\\java\\org\\apache\\hadoop\\yarn\\server\\timelineservice\\storage",
  "methodName" : "waitForHBaseToUp",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void waitForHBaseToUp(TimelineStorageMonitor storageMonitor) throws Exception\n{\r\n    GenericTestUtils.waitFor(() -> {\r\n        try {\r\n            storageMonitor.checkStorageIsUp();\r\n            return true;\r\n        } catch (IOException e) {\r\n            return false;\r\n        }\r\n    }, 1000, 150000);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-yarn-project\\hadoop-yarn\\hadoop-yarn-server\\hadoop-yarn-server-timelineservice-hbase-tests\\src\\test\\java\\org\\apache\\hadoop\\yarn\\server\\timelineservice\\storage",
  "methodName" : "waitForHBaseToDown",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void waitForHBaseToDown(TimelineStorageMonitor storageMonitor) throws Exception\n{\r\n    GenericTestUtils.waitFor(() -> {\r\n        try {\r\n            storageMonitor.checkStorageIsUp();\r\n            return false;\r\n        } catch (IOException e) {\r\n            return true;\r\n        }\r\n    }, 1000, 150000);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-yarn-project\\hadoop-yarn\\hadoop-yarn-server\\hadoop-yarn-server-timelineservice-hbase-tests\\src\\test\\java\\org\\apache\\hadoop\\yarn\\server\\timelineservice\\reader",
  "methodName" : "setupBeforeClass",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void setupBeforeClass() throws Exception\n{\r\n    new MockUp<TimelineReaderMetrics>() {\r\n\r\n        @Mock\r\n        public TimelineReaderMetrics getInstance() {\r\n            return METRICS;\r\n        }\r\n    };\r\n    setup();\r\n    loadData();\r\n    initialize();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-yarn-project\\hadoop-yarn\\hadoop-yarn-server\\hadoop-yarn-server-timelineservice-hbase-tests\\src\\test\\java\\org\\apache\\hadoop\\yarn\\server\\timelineservice\\reader",
  "methodName" : "tearDownAfterClass",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void tearDownAfterClass() throws Exception\n{\r\n    tearDown();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-yarn-project\\hadoop-yarn\\hadoop-yarn-server\\hadoop-yarn-server-timelineservice-hbase-tests\\src\\test\\java\\org\\apache\\hadoop\\yarn\\server\\timelineservice\\reader",
  "methodName" : "loadData",
  "errType" : null,
  "containingMethodsNum" : 168,
  "sourceCodeText" : "void loadData() throws Exception\n{\r\n    String cluster = \"cluster1\";\r\n    String user = \"user1\";\r\n    String flow = \"flow_name\";\r\n    String flowVersion = \"CF7022C10F1354\";\r\n    Long runid = 1002345678919L;\r\n    Long runid1 = 1002345678920L;\r\n    TimelineEntities te = new TimelineEntities();\r\n    TimelineEntity entity = new TimelineEntity();\r\n    String id = \"application_1111111111_1111\";\r\n    String type = TimelineEntityType.YARN_APPLICATION.toString();\r\n    entity.setId(id);\r\n    entity.setType(type);\r\n    Long cTime = 1425016501000L;\r\n    entity.setCreatedTime(cTime);\r\n    entity.addConfig(\"cfg2\", \"value1\");\r\n    Set<TimelineMetric> metrics = new HashSet<>();\r\n    TimelineMetric m1 = new TimelineMetric();\r\n    m1.setId(\"MAP_SLOT_MILLIS\");\r\n    Map<Long, Number> metricValues = ImmutableMap.of(ts - 100000, (Number) 2, ts - 90000, 7, ts - 80000, 40);\r\n    m1.setType(Type.TIME_SERIES);\r\n    m1.setValues(metricValues);\r\n    metrics.add(m1);\r\n    m1 = new TimelineMetric();\r\n    m1.setId(\"MAP1_SLOT_MILLIS\");\r\n    metricValues = ImmutableMap.of(ts - 100000, (Number) 2, ts - 90000, 9, ts - 80000, 40);\r\n    m1.setType(Type.TIME_SERIES);\r\n    m1.setValues(metricValues);\r\n    metrics.add(m1);\r\n    m1 = new TimelineMetric();\r\n    m1.setId(\"HDFS_BYTES_READ\");\r\n    metricValues = ImmutableMap.of(ts - 100000, (Number) 31, ts - 80000, 57);\r\n    m1.setType(Type.TIME_SERIES);\r\n    m1.setValues(metricValues);\r\n    metrics.add(m1);\r\n    entity.addMetrics(metrics);\r\n    TimelineEvent event = new TimelineEvent();\r\n    event.setId(ApplicationMetricsConstants.CREATED_EVENT_TYPE);\r\n    event.setTimestamp(cTime);\r\n    String expKey = \"foo_event\";\r\n    Object expVal = \"test\";\r\n    event.addInfo(expKey, expVal);\r\n    entity.addEvent(event);\r\n    TimelineEvent event11 = new TimelineEvent();\r\n    event11.setId(ApplicationMetricsConstants.FINISHED_EVENT_TYPE);\r\n    Long expTs = 1425019501000L;\r\n    event11.setTimestamp(expTs);\r\n    entity.addEvent(event11);\r\n    te.addEntity(entity);\r\n    TimelineEntities te1 = new TimelineEntities();\r\n    TimelineEntity entity1 = new TimelineEntity();\r\n    id = \"application_1111111111_2222\";\r\n    type = TimelineEntityType.YARN_APPLICATION.toString();\r\n    entity1.setId(id);\r\n    entity1.setType(type);\r\n    cTime = 1425016501000L;\r\n    entity1.setCreatedTime(cTime);\r\n    entity1.addConfig(\"cfg1\", \"value1\");\r\n    metrics.clear();\r\n    TimelineMetric m2 = new TimelineMetric();\r\n    m2.setId(\"MAP_SLOT_MILLIS\");\r\n    metricValues = new HashMap<Long, Number>();\r\n    metricValues.put(ts - 100000, 5L);\r\n    metricValues.put(ts - 80000, 101L);\r\n    m2.setType(Type.TIME_SERIES);\r\n    m2.setValues(metricValues);\r\n    metrics.add(m2);\r\n    entity1.addMetrics(metrics);\r\n    TimelineEvent event1 = new TimelineEvent();\r\n    event1.setId(ApplicationMetricsConstants.CREATED_EVENT_TYPE);\r\n    event1.setTimestamp(cTime);\r\n    event1.addInfo(expKey, expVal);\r\n    entity1.addEvent(event1);\r\n    te1.addEntity(entity1);\r\n    String flow2 = \"flow_name2\";\r\n    String flowVersion2 = \"CF7022C10F1454\";\r\n    Long runid2 = 2102356789046L;\r\n    TimelineEntities te3 = new TimelineEntities();\r\n    TimelineEntity entity3 = new TimelineEntity();\r\n    id = \"application_11111111111111_2223\";\r\n    entity3.setId(id);\r\n    entity3.setType(type);\r\n    cTime = 1425016501037L;\r\n    entity3.setCreatedTime(cTime);\r\n    TimelineEvent event2 = new TimelineEvent();\r\n    event2.setId(ApplicationMetricsConstants.CREATED_EVENT_TYPE);\r\n    event2.setTimestamp(cTime);\r\n    event2.addInfo(\"foo_event\", \"test\");\r\n    entity3.addEvent(event2);\r\n    te3.addEntity(entity3);\r\n    TimelineEntities te4 = new TimelineEntities();\r\n    TimelineEntity entity4 = new TimelineEntity();\r\n    id = \"application_1111111111_2224\";\r\n    entity4.setId(id);\r\n    entity4.setType(type);\r\n    cTime = 1425016501034L;\r\n    entity4.setCreatedTime(cTime);\r\n    TimelineEvent event4 = new TimelineEvent();\r\n    event4.setId(ApplicationMetricsConstants.CREATED_EVENT_TYPE);\r\n    event4.setTimestamp(cTime);\r\n    event4.addInfo(\"foo_event\", \"test\");\r\n    entity4.addEvent(event4);\r\n    metrics.clear();\r\n    m2 = new TimelineMetric();\r\n    m2.setId(\"MAP_SLOT_MILLIS\");\r\n    metricValues = ImmutableMap.of(ts - 100000, (Number) 5L, ts - 80000, 101L);\r\n    m2.setType(Type.TIME_SERIES);\r\n    m2.setValues(metricValues);\r\n    metrics.add(m2);\r\n    entity4.addMetrics(metrics);\r\n    te4.addEntity(entity4);\r\n    TimelineEntities userEntities = new TimelineEntities();\r\n    TimelineEntity entity5 = new TimelineEntity();\r\n    entity5.setId(\"entity1\");\r\n    entity5.setType(\"type1\");\r\n    entity5.setCreatedTime(1425016501034L);\r\n    entity5.addConfigs(ImmutableMap.of(\"config_param1\", \"value1\", \"config_param2\", \"value2\", \"cfg_param1\", \"value3\"));\r\n    entity5.addInfo(ImmutableMap.of(\"info1\", (Object) \"cluster1\", \"info2\", 2.0, \"info3\", 35000, \"info4\", 36000));\r\n    metrics = new HashSet<>();\r\n    m1 = new TimelineMetric();\r\n    m1.setId(\"MAP_SLOT_MILLIS\");\r\n    metricValues = ImmutableMap.of(ts - 100000, (Number) 2, ts - 80000, 40);\r\n    m1.setType(Type.TIME_SERIES);\r\n    m1.setValues(metricValues);\r\n    metrics.add(m1);\r\n    m1 = new TimelineMetric();\r\n    m1.setId(\"HDFS_BYTES_READ\");\r\n    metricValues = ImmutableMap.of(ts - 100000, (Number) 31, ts - 80000, 57);\r\n    m1.setType(Type.TIME_SERIES);\r\n    m1.setValues(metricValues);\r\n    metrics.add(m1);\r\n    entity5.addMetrics(metrics);\r\n    TimelineEvent event51 = new TimelineEvent();\r\n    event51.setId(\"event1\");\r\n    event51.setTimestamp(cTime);\r\n    entity5.addEvent(event51);\r\n    TimelineEvent event52 = new TimelineEvent();\r\n    event52.setId(\"event2\");\r\n    event52.setTimestamp(cTime);\r\n    entity5.addEvent(event52);\r\n    TimelineEvent event53 = new TimelineEvent();\r\n    event53.setId(\"event3\");\r\n    event53.setTimestamp(cTime);\r\n    entity5.addEvent(event53);\r\n    TimelineEvent event54 = new TimelineEvent();\r\n    event54.setId(\"event4\");\r\n    event54.setTimestamp(cTime);\r\n    entity5.addEvent(event54);\r\n    Map<String, Set<String>> isRelatedTo1 = new HashMap<String, Set<String>>();\r\n    isRelatedTo1.put(\"type2\", new HashSet<>(Arrays.asList(\"entity21\", \"entity22\", \"entity23\", \"entity24\")));\r\n    isRelatedTo1.put(\"type4\", new HashSet<>(Arrays.asList(\"entity41\", \"entity42\")));\r\n    isRelatedTo1.put(\"type1\", new HashSet<>(Arrays.asList(\"entity14\", \"entity15\")));\r\n    isRelatedTo1.put(\"type3\", new HashSet<>(Arrays.asList(\"entity31\", \"entity35\", \"entity32\", \"entity33\")));\r\n    entity5.addIsRelatedToEntities(isRelatedTo1);\r\n    Map<String, Set<String>> relatesTo1 = new HashMap<String, Set<String>>();\r\n    relatesTo1.put(\"type2\", new HashSet<>(Arrays.asList(\"entity21\", \"entity22\", \"entity23\", \"entity24\")));\r\n    relatesTo1.put(\"type4\", new HashSet<>(Arrays.asList(\"entity41\", \"entity42\")));\r\n    relatesTo1.put(\"type1\", new HashSet<>(Arrays.asList(\"entity14\", \"entity15\")));\r\n    relatesTo1.put(\"type3\", new HashSet<>(Arrays.asList(\"entity31\", \"entity35\", \"entity32\", \"entity33\")));\r\n    entity5.addRelatesToEntities(relatesTo1);\r\n    userEntities.addEntity(new SubApplicationEntity(entity5));\r\n    TimelineEntity entity6 = new TimelineEntity();\r\n    entity6.setId(\"entity2\");\r\n    entity6.setType(\"type1\");\r\n    entity6.setCreatedTime(1425016501034L);\r\n    entity6.addConfigs(ImmutableMap.of(\"cfg_param3\", \"value1\", \"configuration_param2\", \"value2\", \"config_param1\", \"value3\"));\r\n    entity6.addInfo(ImmutableMap.of(\"info1\", (Object) \"cluster2\", \"info2\", 2.0, \"info4\", 35000));\r\n    metrics = new HashSet<>();\r\n    m1 = new TimelineMetric();\r\n    m1.setId(\"MAP1_SLOT_MILLIS\");\r\n    metricValues = ImmutableMap.of(ts - 100000, (Number) 12, ts - 80000, 140);\r\n    m1.setType(Type.TIME_SERIES);\r\n    m1.setValues(metricValues);\r\n    metrics.add(m1);\r\n    m1 = new TimelineMetric();\r\n    m1.setId(\"HDFS_BYTES_READ\");\r\n    metricValues = ImmutableMap.of(ts - 100000, (Number) 78, ts - 80000, 157);\r\n    m1.setType(Type.TIME_SERIES);\r\n    m1.setValues(metricValues);\r\n    metrics.add(m1);\r\n    m1 = new TimelineMetric();\r\n    m1.setId(\"MAP11_SLOT_MILLIS\");\r\n    m1.setType(Type.SINGLE_VALUE);\r\n    m1.addValue(ts - 100000, 122);\r\n    metrics.add(m1);\r\n    entity6.addMetrics(metrics);\r\n    TimelineEvent event61 = new TimelineEvent();\r\n    event61.setId(\"event1\");\r\n    event61.setTimestamp(cTime);\r\n    entity6.addEvent(event61);\r\n    TimelineEvent event62 = new TimelineEvent();\r\n    event62.setId(\"event5\");\r\n    event62.setTimestamp(cTime);\r\n    entity6.addEvent(event62);\r\n    TimelineEvent event63 = new TimelineEvent();\r\n    event63.setId(\"event3\");\r\n    event63.setTimestamp(cTime);\r\n    entity6.addEvent(event63);\r\n    TimelineEvent event64 = new TimelineEvent();\r\n    event64.setId(\"event6\");\r\n    event64.setTimestamp(cTime);\r\n    entity6.addEvent(event64);\r\n    Map<String, Set<String>> isRelatedTo2 = new HashMap<String, Set<String>>();\r\n    isRelatedTo2.put(\"type2\", new HashSet<>(Arrays.asList(\"entity21\", \"entity22\", \"entity23\", \"entity24\")));\r\n    isRelatedTo2.put(\"type5\", new HashSet<>(Arrays.asList(\"entity51\", \"entity52\")));\r\n    isRelatedTo2.put(\"type6\", new HashSet<>(Arrays.asList(\"entity61\", \"entity66\")));\r\n    isRelatedTo2.put(\"type3\", new HashSet<>(Collections.singletonList(\"entity31\")));\r\n    entity6.addIsRelatedToEntities(isRelatedTo2);\r\n    Map<String, Set<String>> relatesTo2 = new HashMap<String, Set<String>>();\r\n    relatesTo2.put(\"type2\", new HashSet<>(Arrays.asList(\"entity21\", \"entity22\", \"entity23\", \"entity24\")));\r\n    relatesTo2.put(\"type5\", new HashSet<>(Arrays.asList(\"entity51\", \"entity52\")));\r\n    relatesTo2.put(\"type6\", new HashSet<>(Arrays.asList(\"entity61\", \"entity66\")));\r\n    relatesTo2.put(\"type3\", new HashSet<>(Collections.singletonList(\"entity31\")));\r\n    entity6.addRelatesToEntities(relatesTo2);\r\n    userEntities.addEntity(new SubApplicationEntity(entity6));\r\n    for (long i = 1; i <= 10; i++) {\r\n        TimelineEntity userEntity = new TimelineEntity();\r\n        userEntity.setType(\"entitytype\");\r\n        userEntity.setId(\"entityid-\" + i);\r\n        userEntity.setIdPrefix(11 - i);\r\n        userEntity.setCreatedTime(ts);\r\n        userEntities.addEntity(new SubApplicationEntity(userEntity));\r\n    }\r\n    HBaseTimelineWriterImpl hbi = null;\r\n    Configuration c1 = getHBaseTestingUtility().getConfiguration();\r\n    UserGroupInformation remoteUser = UserGroupInformation.createRemoteUser(doAsUser);\r\n    try {\r\n        hbi = new HBaseTimelineWriterImpl();\r\n        hbi.init(c1);\r\n        hbi.write(new TimelineCollectorContext(cluster, user, flow, flowVersion, runid, entity.getId()), te, remoteUser);\r\n        hbi.write(new TimelineCollectorContext(cluster, user, flow, flowVersion, runid, entity1.getId()), te1, remoteUser);\r\n        hbi.write(new TimelineCollectorContext(cluster, user, flow, flowVersion, runid1, entity4.getId()), te4, remoteUser);\r\n        hbi.write(new TimelineCollectorContext(cluster, user, flow2, flowVersion2, runid2, entity3.getId()), te3, remoteUser);\r\n        hbi.write(new TimelineCollectorContext(cluster, user, flow, flowVersion, runid, \"application_1111111111_1111\"), userEntities, remoteUser);\r\n        writeApplicationEntities(hbi, ts);\r\n        hbi.flush();\r\n    } finally {\r\n        if (hbi != null) {\r\n            hbi.close();\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-yarn-project\\hadoop-yarn\\hadoop-yarn-server\\hadoop-yarn-server-timelineservice-hbase-tests\\src\\test\\java\\org\\apache\\hadoop\\yarn\\server\\timelineservice\\reader",
  "methodName" : "writeApplicationEntities",
  "errType" : null,
  "containingMethodsNum" : 11,
  "sourceCodeText" : "void writeApplicationEntities(HBaseTimelineWriterImpl hbi, long timestamp) throws IOException\n{\r\n    int count = 1;\r\n    for (long i = 1; i <= 3; i++) {\r\n        for (int j = 1; j <= 5; j++) {\r\n            TimelineEntities te = new TimelineEntities();\r\n            ApplicationId appId = BuilderUtils.newApplicationId(timestamp, count++);\r\n            ApplicationEntity appEntity = new ApplicationEntity();\r\n            appEntity.setId(HBaseTimelineSchemaUtils.convertApplicationIdToString(appId));\r\n            appEntity.setCreatedTime(timestamp);\r\n            TimelineEvent created = new TimelineEvent();\r\n            created.setId(ApplicationMetricsConstants.CREATED_EVENT_TYPE);\r\n            created.setTimestamp(timestamp);\r\n            appEntity.addEvent(created);\r\n            TimelineEvent finished = new TimelineEvent();\r\n            finished.setId(ApplicationMetricsConstants.FINISHED_EVENT_TYPE);\r\n            finished.setTimestamp(timestamp + i * j);\r\n            appEntity.addEvent(finished);\r\n            te.addEntity(appEntity);\r\n            hbi.write(new TimelineCollectorContext(\"cluster1\", \"user1\", \"flow1\", \"CF7022C10F1354\", i, appEntity.getId()), te, UserGroupInformation.createRemoteUser(\"user1\"));\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-yarn-project\\hadoop-yarn\\hadoop-yarn-server\\hadoop-yarn-server-timelineservice-hbase-tests\\src\\test\\java\\org\\apache\\hadoop\\yarn\\server\\timelineservice\\reader",
  "methodName" : "newEntity",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "TimelineEntity newEntity(String type, String id)\n{\r\n    TimelineEntity entity = new TimelineEntity();\r\n    entity.setIdentifier(new TimelineEntity.Identifier(type, id));\r\n    return entity;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-yarn-project\\hadoop-yarn\\hadoop-yarn-server\\hadoop-yarn-server-timelineservice-hbase-tests\\src\\test\\java\\org\\apache\\hadoop\\yarn\\server\\timelineservice\\reader",
  "methodName" : "newMetric",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "TimelineMetric newMetric(TimelineMetric.Type type, String id, long t, Number value)\n{\r\n    TimelineMetric metric = new TimelineMetric(type);\r\n    metric.setId(id);\r\n    metric.addValue(t, value);\r\n    return metric;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-yarn-project\\hadoop-yarn\\hadoop-yarn-server\\hadoop-yarn-server-timelineservice-hbase-tests\\src\\test\\java\\org\\apache\\hadoop\\yarn\\server\\timelineservice\\reader",
  "methodName" : "verifyMetricValues",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "boolean verifyMetricValues(Map<Long, Number> m1, Map<Long, Number> m2)\n{\r\n    for (Map.Entry<Long, Number> entry : m1.entrySet()) {\r\n        if (!m2.containsKey(entry.getKey())) {\r\n            return false;\r\n        }\r\n        if (m2.get(entry.getKey()).equals(entry.getValue())) {\r\n            return false;\r\n        }\r\n    }\r\n    return true;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-yarn-project\\hadoop-yarn\\hadoop-yarn-server\\hadoop-yarn-server-timelineservice-hbase-tests\\src\\test\\java\\org\\apache\\hadoop\\yarn\\server\\timelineservice\\reader",
  "methodName" : "verifyMetrics",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "boolean verifyMetrics(TimelineMetric m, TimelineMetric... metrics)\n{\r\n    for (TimelineMetric metric : metrics) {\r\n        if (!metric.equals(m)) {\r\n            continue;\r\n        }\r\n        if (!verifyMetricValues(metric.getValues(), m.getValues())) {\r\n            continue;\r\n        }\r\n        return true;\r\n    }\r\n    return false;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-yarn-project\\hadoop-yarn\\hadoop-yarn-server\\hadoop-yarn-server-timelineservice-hbase-tests\\src\\test\\java\\org\\apache\\hadoop\\yarn\\server\\timelineservice\\reader",
  "methodName" : "testGetFlowRun",
  "errType" : null,
  "containingMethodsNum" : 25,
  "sourceCodeText" : "void testGetFlowRun() throws Exception\n{\r\n    Client client = createClient();\r\n    try {\r\n        URI uri = URI.create(\"http://localhost:\" + getServerPort() + \"/ws/v2/\" + \"timeline/clusters/cluster1/users/user1/flows/flow_name/runs/\" + \"1002345678919\");\r\n        ClientResponse resp = getResponse(client, uri);\r\n        FlowRunEntity entity = resp.getEntity(FlowRunEntity.class);\r\n        assertEquals(MediaType.APPLICATION_JSON_TYPE + \"; charset=utf-8\", resp.getType().toString());\r\n        assertNotNull(entity);\r\n        assertEquals(\"user1@flow_name/1002345678919\", entity.getId());\r\n        assertEquals(3, entity.getMetrics().size());\r\n        TimelineMetric m1 = newMetric(TimelineMetric.Type.SINGLE_VALUE, \"HDFS_BYTES_READ\", ts - 80000, 57L);\r\n        TimelineMetric m2 = newMetric(TimelineMetric.Type.SINGLE_VALUE, \"MAP_SLOT_MILLIS\", ts - 80000, 141L);\r\n        TimelineMetric m3 = newMetric(TimelineMetric.Type.SINGLE_VALUE, \"MAP1_SLOT_MILLIS\", ts - 80000, 40L);\r\n        for (TimelineMetric metric : entity.getMetrics()) {\r\n            assertTrue(verifyMetrics(metric, m1, m2, m3));\r\n        }\r\n        uri = URI.create(\"http://localhost:\" + getServerPort() + \"/ws/v2/\" + \"timeline/users/user1/flows/flow_name/runs/1002345678919\");\r\n        resp = getResponse(client, uri);\r\n        entity = resp.getEntity(FlowRunEntity.class);\r\n        assertNotNull(entity);\r\n        assertEquals(\"user1@flow_name/1002345678919\", entity.getId());\r\n        assertEquals(3, entity.getMetrics().size());\r\n        m1 = newMetric(TimelineMetric.Type.SINGLE_VALUE, \"HDFS_BYTES_READ\", ts - 80000, 57L);\r\n        m2 = newMetric(TimelineMetric.Type.SINGLE_VALUE, \"MAP_SLOT_MILLIS\", ts - 80000, 141L);\r\n        m3 = newMetric(TimelineMetric.Type.SINGLE_VALUE, \"MAP1_SLOT_MILLIS\", ts - 80000, 40L);\r\n        for (TimelineMetric metric : entity.getMetrics()) {\r\n            assertTrue(verifyMetrics(metric, m1, m2, m3));\r\n        }\r\n    } finally {\r\n        client.destroy();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-yarn-project\\hadoop-yarn\\hadoop-yarn-server\\hadoop-yarn-server-timelineservice-hbase-tests\\src\\test\\java\\org\\apache\\hadoop\\yarn\\server\\timelineservice\\reader",
  "methodName" : "testGetFlowRuns",
  "errType" : null,
  "containingMethodsNum" : 51,
  "sourceCodeText" : "void testGetFlowRuns() throws Exception\n{\r\n    Client client = createClient();\r\n    try {\r\n        URI uri = URI.create(\"http://localhost:\" + getServerPort() + \"/ws/v2/\" + \"timeline/clusters/cluster1/users/user1/flows/flow_name/runs\");\r\n        ClientResponse resp = getResponse(client, uri);\r\n        Set<FlowRunEntity> entities = resp.getEntity(new GenericType<Set<FlowRunEntity>>() {\r\n        });\r\n        assertEquals(MediaType.APPLICATION_JSON_TYPE + \"; charset=utf-8\", resp.getType().toString());\r\n        assertNotNull(entities);\r\n        assertEquals(2, entities.size());\r\n        for (FlowRunEntity entity : entities) {\r\n            assertTrue(\"Id, run id or start time does not match.\", ((entity.getId().equals(\"user1@flow_name/1002345678919\")) && (entity.getRunId() == 1002345678919L) && (entity.getStartTime() == 1425016501000L)) || ((entity.getId().equals(\"user1@flow_name/1002345678920\")) && (entity.getRunId() == 1002345678920L) && (entity.getStartTime() == 1425016501034L)));\r\n            assertEquals(0, entity.getMetrics().size());\r\n        }\r\n        uri = URI.create(\"http://localhost:\" + getServerPort() + \"/ws/v2/timeline/\" + \"clusters/cluster1/users/user1/flows/flow_name/runs?limit=1\");\r\n        resp = getResponse(client, uri);\r\n        entities = resp.getEntity(new GenericType<Set<FlowRunEntity>>() {\r\n        });\r\n        assertEquals(MediaType.APPLICATION_JSON_TYPE + \"; charset=utf-8\", resp.getType().toString());\r\n        assertNotNull(entities);\r\n        assertEquals(1, entities.size());\r\n        for (FlowRunEntity entity : entities) {\r\n            assertTrue(\"Id, run id or start time does not match.\", entity.getId().equals(\"user1@flow_name/1002345678920\") && entity.getRunId() == 1002345678920L && entity.getStartTime() == 1425016501034L);\r\n            assertEquals(0, entity.getMetrics().size());\r\n        }\r\n        uri = URI.create(\"http://localhost:\" + getServerPort() + \"/ws/v2/\" + \"timeline/clusters/cluster1/users/user1/flows/flow_name/runs?\" + \"createdtimestart=1425016501030\");\r\n        resp = getResponse(client, uri);\r\n        entities = resp.getEntity(new GenericType<Set<FlowRunEntity>>() {\r\n        });\r\n        assertEquals(MediaType.APPLICATION_JSON_TYPE + \"; charset=utf-8\", resp.getType().toString());\r\n        assertNotNull(entities);\r\n        assertEquals(1, entities.size());\r\n        for (FlowRunEntity entity : entities) {\r\n            assertTrue(\"Id, run id or start time does not match.\", entity.getId().equals(\"user1@flow_name/1002345678920\") && entity.getRunId() == 1002345678920L && entity.getStartTime() == 1425016501034L);\r\n            assertEquals(0, entity.getMetrics().size());\r\n        }\r\n        uri = URI.create(\"http://localhost:\" + getServerPort() + \"/ws/v2/\" + \"timeline/clusters/cluster1/users/user1/flows/flow_name/runs?\" + \"createdtimestart=1425016500999&createdtimeend=1425016501035\");\r\n        resp = getResponse(client, uri);\r\n        entities = resp.getEntity(new GenericType<Set<FlowRunEntity>>() {\r\n        });\r\n        assertEquals(MediaType.APPLICATION_JSON_TYPE + \"; charset=utf-8\", resp.getType().toString());\r\n        assertNotNull(entities);\r\n        assertEquals(2, entities.size());\r\n        for (FlowRunEntity entity : entities) {\r\n            assertTrue(\"Id, run id or start time does not match.\", ((entity.getId().equals(\"user1@flow_name/1002345678919\")) && (entity.getRunId() == 1002345678919L) && (entity.getStartTime() == 1425016501000L)) || ((entity.getId().equals(\"user1@flow_name/1002345678920\")) && (entity.getRunId() == 1002345678920L) && (entity.getStartTime() == 1425016501034L)));\r\n            assertEquals(0, entity.getMetrics().size());\r\n        }\r\n        uri = URI.create(\"http://localhost:\" + getServerPort() + \"/ws/v2/\" + \"timeline/clusters/cluster1/users/user1/flows/flow_name/runs?\" + \"createdtimeend=1425016501030\");\r\n        resp = getResponse(client, uri);\r\n        entities = resp.getEntity(new GenericType<Set<FlowRunEntity>>() {\r\n        });\r\n        assertEquals(MediaType.APPLICATION_JSON_TYPE + \"; charset=utf-8\", resp.getType().toString());\r\n        assertNotNull(entities);\r\n        assertEquals(1, entities.size());\r\n        for (FlowRunEntity entity : entities) {\r\n            assertTrue(\"Id, run id or start time does not match.\", entity.getId().equals(\"user1@flow_name/1002345678919\") && entity.getRunId() == 1002345678919L && entity.getStartTime() == 1425016501000L);\r\n            assertEquals(0, entity.getMetrics().size());\r\n        }\r\n        uri = URI.create(\"http://localhost:\" + getServerPort() + \"/ws/v2/\" + \"timeline/clusters/cluster1/users/user1/flows/flow_name/runs?\" + \"fields=metrics\");\r\n        resp = getResponse(client, uri);\r\n        entities = resp.getEntity(new GenericType<Set<FlowRunEntity>>() {\r\n        });\r\n        assertEquals(MediaType.APPLICATION_JSON_TYPE + \"; charset=utf-8\", resp.getType().toString());\r\n        assertNotNull(entities);\r\n        assertEquals(2, entities.size());\r\n        for (FlowRunEntity entity : entities) {\r\n            assertTrue(\"Id, run id or start time does not match.\", ((entity.getId().equals(\"user1@flow_name/1002345678919\")) && (entity.getRunId() == 1002345678919L) && (entity.getStartTime() == 1425016501000L) && (entity.getMetrics().size() == 3)) || ((entity.getId().equals(\"user1@flow_name/1002345678920\")) && (entity.getRunId() == 1002345678920L) && (entity.getStartTime() == 1425016501034L) && (entity.getMetrics().size() == 1)));\r\n        }\r\n        uri = URI.create(\"http://localhost:\" + getServerPort() + \"/ws/v2/\" + \"timeline/clusters/cluster1/users/user1/flows/flow_name/runs?\" + \"fields=CONFIGS\");\r\n        verifyHttpResponse(client, uri, Status.BAD_REQUEST);\r\n    } finally {\r\n        client.destroy();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-yarn-project\\hadoop-yarn\\hadoop-yarn-server\\hadoop-yarn-server-timelineservice-hbase-tests\\src\\test\\java\\org\\apache\\hadoop\\yarn\\server\\timelineservice\\reader",
  "methodName" : "testGetFlowRunsMetricsToRetrieve",
  "errType" : null,
  "containingMethodsNum" : 22,
  "sourceCodeText" : "void testGetFlowRunsMetricsToRetrieve() throws Exception\n{\r\n    Client client = createClient();\r\n    try {\r\n        URI uri = URI.create(\"http://localhost:\" + getServerPort() + \"/ws/v2/\" + \"timeline/clusters/cluster1/users/user1/flows/flow_name/runs?\" + \"metricstoretrieve=MAP_,HDFS_\");\r\n        ClientResponse resp = getResponse(client, uri);\r\n        Set<FlowRunEntity> entities = resp.getEntity(new GenericType<Set<FlowRunEntity>>() {\r\n        });\r\n        assertEquals(MediaType.APPLICATION_JSON_TYPE + \"; charset=utf-8\", resp.getType().toString());\r\n        assertNotNull(entities);\r\n        assertEquals(2, entities.size());\r\n        int metricCnt = 0;\r\n        for (FlowRunEntity entity : entities) {\r\n            metricCnt += entity.getMetrics().size();\r\n            for (TimelineMetric metric : entity.getMetrics()) {\r\n                assertTrue(metric.getId().startsWith(\"MAP_\") || metric.getId().startsWith(\"HDFS_\"));\r\n            }\r\n        }\r\n        assertEquals(3, metricCnt);\r\n        uri = URI.create(\"http://localhost:\" + getServerPort() + \"/ws/v2/\" + \"timeline/clusters/cluster1/users/user1/flows/flow_name/runs?\" + \"metricstoretrieve=!(MAP_,HDFS_)\");\r\n        resp = getResponse(client, uri);\r\n        entities = resp.getEntity(new GenericType<Set<FlowRunEntity>>() {\r\n        });\r\n        assertEquals(MediaType.APPLICATION_JSON_TYPE + \"; charset=utf-8\", resp.getType().toString());\r\n        assertNotNull(entities);\r\n        assertEquals(2, entities.size());\r\n        metricCnt = 0;\r\n        for (FlowRunEntity entity : entities) {\r\n            metricCnt += entity.getMetrics().size();\r\n            for (TimelineMetric metric : entity.getMetrics()) {\r\n                assertTrue(metric.getId().startsWith(\"MAP1_\"));\r\n            }\r\n        }\r\n        assertEquals(1, metricCnt);\r\n    } finally {\r\n        client.destroy();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-yarn-project\\hadoop-yarn\\hadoop-yarn-server\\hadoop-yarn-server-timelineservice-hbase-tests\\src\\test\\java\\org\\apache\\hadoop\\yarn\\server\\timelineservice\\reader",
  "methodName" : "testGetEntitiesByUID",
  "errType" : null,
  "containingMethodsNum" : 63,
  "sourceCodeText" : "void testGetEntitiesByUID() throws Exception\n{\r\n    Client client = createClient();\r\n    try {\r\n        URI uri = URI.create(\"http://localhost:\" + getServerPort() + \"/ws/v2/\" + \"timeline/flows\");\r\n        ClientResponse resp = getResponse(client, uri);\r\n        Set<FlowActivityEntity> flowEntities = resp.getEntity(new GenericType<Set<FlowActivityEntity>>() {\r\n        });\r\n        assertNotNull(flowEntities);\r\n        assertEquals(3, flowEntities.size());\r\n        List<String> listFlowUIDs = new ArrayList<String>();\r\n        for (FlowActivityEntity entity : flowEntities) {\r\n            String flowUID = (String) entity.getInfo().get(TimelineReaderUtils.UID_KEY);\r\n            listFlowUIDs.add(flowUID);\r\n            assertEquals(TimelineUIDConverter.FLOW_UID.encodeUID(new TimelineReaderContext(entity.getCluster(), entity.getUser(), entity.getFlowName(), null, null, null, null)), flowUID);\r\n            assertTrue((entity.getId().endsWith(\"@flow_name\") && entity.getFlowRuns().size() == 2) || (entity.getId().endsWith(\"@flow_name2\") && entity.getFlowRuns().size() == 1) || (entity.getId().endsWith(\"@flow1\") && entity.getFlowRuns().size() == 3));\r\n        }\r\n        List<String> listFlowRunUIDs = new ArrayList<String>();\r\n        for (String flowUID : listFlowUIDs) {\r\n            uri = URI.create(\"http://localhost:\" + getServerPort() + \"/ws/v2/\" + \"timeline/flow-uid/\" + flowUID + \"/runs\");\r\n            resp = getResponse(client, uri);\r\n            Set<FlowRunEntity> frEntities = resp.getEntity(new GenericType<Set<FlowRunEntity>>() {\r\n            });\r\n            assertNotNull(frEntities);\r\n            for (FlowRunEntity entity : frEntities) {\r\n                String flowRunUID = (String) entity.getInfo().get(TimelineReaderUtils.UID_KEY);\r\n                listFlowRunUIDs.add(flowRunUID);\r\n                assertEquals(TimelineUIDConverter.FLOWRUN_UID.encodeUID(new TimelineReaderContext(\"cluster1\", entity.getUser(), entity.getName(), entity.getRunId(), null, null, null)), flowRunUID);\r\n            }\r\n        }\r\n        assertEquals(6, listFlowRunUIDs.size());\r\n        for (String flowRunUID : listFlowRunUIDs) {\r\n            uri = URI.create(\"http://localhost:\" + getServerPort() + \"/ws/v2/\" + \"timeline/run-uid/\" + flowRunUID);\r\n            resp = getResponse(client, uri);\r\n            FlowRunEntity entity = resp.getEntity(FlowRunEntity.class);\r\n            assertNotNull(entity);\r\n        }\r\n        List<String> listAppUIDs = new ArrayList<String>();\r\n        for (String flowRunUID : listFlowRunUIDs) {\r\n            TimelineReaderContext context = TimelineUIDConverter.FLOWRUN_UID.decodeUID(flowRunUID);\r\n            uri = URI.create(\"http://localhost:\" + getServerPort() + \"/ws/v2/\" + \"timeline/run-uid/\" + flowRunUID + \"/apps\");\r\n            resp = getResponse(client, uri);\r\n            Set<TimelineEntity> appEntities = resp.getEntity(new GenericType<Set<TimelineEntity>>() {\r\n            });\r\n            assertNotNull(appEntities);\r\n            for (TimelineEntity entity : appEntities) {\r\n                String appUID = (String) entity.getInfo().get(TimelineReaderUtils.UID_KEY);\r\n                listAppUIDs.add(appUID);\r\n                assertEquals(TimelineUIDConverter.APPLICATION_UID.encodeUID(new TimelineReaderContext(context.getClusterId(), context.getUserId(), context.getFlowName(), context.getFlowRunId(), entity.getId(), null, null)), appUID);\r\n            }\r\n        }\r\n        assertEquals(19, listAppUIDs.size());\r\n        for (String appUID : listAppUIDs) {\r\n            uri = URI.create(\"http://localhost:\" + getServerPort() + \"/ws/v2/\" + \"timeline/app-uid/\" + appUID);\r\n            resp = getResponse(client, uri);\r\n            TimelineEntity entity = resp.getEntity(TimelineEntity.class);\r\n            assertNotNull(entity);\r\n        }\r\n        List<String> listEntityUIDs = new ArrayList<String>();\r\n        for (String appUID : listAppUIDs) {\r\n            TimelineReaderContext context = TimelineUIDConverter.APPLICATION_UID.decodeUID(appUID);\r\n            uri = URI.create(\"http://localhost:\" + getServerPort() + \"/ws/v2/\" + \"timeline/app-uid/\" + appUID + \"/entities/type1\");\r\n            resp = getResponse(client, uri);\r\n            Set<TimelineEntity> entities = resp.getEntity(new GenericType<Set<TimelineEntity>>() {\r\n            });\r\n            assertNotNull(entities);\r\n            for (TimelineEntity entity : entities) {\r\n                String entityUID = (String) entity.getInfo().get(TimelineReaderUtils.UID_KEY);\r\n                listEntityUIDs.add(entityUID);\r\n                assertEquals(TimelineUIDConverter.GENERIC_ENTITY_UID.encodeUID(new TimelineReaderContext(context.getClusterId(), context.getUserId(), context.getFlowName(), context.getFlowRunId(), context.getAppId(), \"type1\", entity.getIdPrefix(), entity.getId())), entityUID);\r\n            }\r\n        }\r\n        assertEquals(2, listEntityUIDs.size());\r\n        for (String entityUID : listEntityUIDs) {\r\n            uri = URI.create(\"http://localhost:\" + getServerPort() + \"/ws/v2/\" + \"timeline/entity-uid/\" + entityUID);\r\n            resp = getResponse(client, uri);\r\n            TimelineEntity entity = resp.getEntity(TimelineEntity.class);\r\n            assertNotNull(entity);\r\n        }\r\n        uri = URI.create(\"http://localhost:\" + getServerPort() + \"/ws/v2/\" + \"timeline/flow-uid/dummy:flow/runs\");\r\n        verifyHttpResponse(client, uri, Status.BAD_REQUEST);\r\n        uri = URI.create(\"http://localhost:\" + getServerPort() + \"/ws/v2/\" + \"timeline/run-uid/dummy:flowrun\");\r\n        verifyHttpResponse(client, uri, Status.BAD_REQUEST);\r\n        uri = URI.create(\"http://localhost:\" + getServerPort() + \"/ws/v2/\" + \"timeline/run-uid/some:dummy:flow:123v456\");\r\n        verifyHttpResponse(client, uri, Status.BAD_REQUEST);\r\n        uri = URI.create(\"http://localhost:\" + getServerPort() + \"/ws/v2/\" + \"timeline/run-uid/dummy:flowrun/apps\");\r\n        verifyHttpResponse(client, uri, Status.BAD_REQUEST);\r\n        uri = URI.create(\"http://localhost:\" + getServerPort() + \"/ws/v2/\" + \"timeline/app-uid/dummy:app\");\r\n        verifyHttpResponse(client, uri, Status.BAD_REQUEST);\r\n        uri = URI.create(\"http://localhost:\" + getServerPort() + \"/ws/v2/\" + \"timeline/app-uid/dummy:app/entities/type1\");\r\n        verifyHttpResponse(client, uri, Status.BAD_REQUEST);\r\n        uri = URI.create(\"http://localhost:\" + getServerPort() + \"/ws/v2/\" + \"timeline/entity-uid/dummy:entity\");\r\n        verifyHttpResponse(client, uri, Status.BAD_REQUEST);\r\n    } finally {\r\n        client.destroy();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-yarn-project\\hadoop-yarn\\hadoop-yarn-server\\hadoop-yarn-server-timelineservice-hbase-tests\\src\\test\\java\\org\\apache\\hadoop\\yarn\\server\\timelineservice\\reader",
  "methodName" : "testUIDQueryWithAndWithoutFlowContextInfo",
  "errType" : null,
  "containingMethodsNum" : 46,
  "sourceCodeText" : "void testUIDQueryWithAndWithoutFlowContextInfo() throws Exception\n{\r\n    Client client = createClient();\r\n    try {\r\n        String appUIDWithFlowInfo = \"cluster1!user1!flow_name!1002345678919!application_1111111111_1111\";\r\n        URI uri = URI.create(\"http://localhost:\" + getServerPort() + \"/ws/v2/\" + \"timeline/app-uid/\" + appUIDWithFlowInfo);\r\n        ClientResponse resp = getResponse(client, uri);\r\n        TimelineEntity appEntity1 = resp.getEntity(TimelineEntity.class);\r\n        assertNotNull(appEntity1);\r\n        assertEquals(TimelineEntityType.YARN_APPLICATION.toString(), appEntity1.getType());\r\n        assertEquals(\"application_1111111111_1111\", appEntity1.getId());\r\n        uri = URI.create(\"http://localhost:\" + getServerPort() + \"/ws/v2/timeline/\" + \"app-uid/\" + appUIDWithFlowInfo + \"/entities/type1\");\r\n        resp = getResponse(client, uri);\r\n        Set<TimelineEntity> entities1 = resp.getEntity(new GenericType<Set<TimelineEntity>>() {\r\n        });\r\n        assertNotNull(entities1);\r\n        assertEquals(2, entities1.size());\r\n        for (TimelineEntity entity : entities1) {\r\n            assertNotNull(entity.getInfo());\r\n            assertEquals(2, entity.getInfo().size());\r\n            String uid = (String) entity.getInfo().get(TimelineReaderUtils.UID_KEY);\r\n            assertNotNull(uid);\r\n            assertTrue(uid.equals(appUIDWithFlowInfo + \"!type1!0!entity1\") || uid.equals(appUIDWithFlowInfo + \"!type1!0!entity2\"));\r\n        }\r\n        String appUIDWithoutFlowInfo = \"cluster1!application_1111111111_1111\";\r\n        uri = URI.create(\"http://localhost:\" + getServerPort() + \"/ws/v2/timeline/\" + \"app-uid/\" + appUIDWithoutFlowInfo);\r\n        resp = getResponse(client, uri);\r\n        TimelineEntity appEntity2 = resp.getEntity(TimelineEntity.class);\r\n        assertNotNull(appEntity2);\r\n        assertEquals(TimelineEntityType.YARN_APPLICATION.toString(), appEntity2.getType());\r\n        assertEquals(\"application_1111111111_1111\", appEntity2.getId());\r\n        uri = URI.create(\"http://localhost:\" + getServerPort() + \"/ws/v2/timeline/\" + \"app-uid/\" + appUIDWithoutFlowInfo + \"/entities/type1\");\r\n        resp = getResponse(client, uri);\r\n        Set<TimelineEntity> entities2 = resp.getEntity(new GenericType<Set<TimelineEntity>>() {\r\n        });\r\n        assertNotNull(entities2);\r\n        assertEquals(2, entities2.size());\r\n        for (TimelineEntity entity : entities2) {\r\n            assertNotNull(entity.getInfo());\r\n            assertEquals(2, entity.getInfo().size());\r\n            String uid = (String) entity.getInfo().get(TimelineReaderUtils.UID_KEY);\r\n            assertNotNull(uid);\r\n            assertTrue(uid.equals(appUIDWithoutFlowInfo + \"!type1!0!entity1\") || uid.equals(appUIDWithoutFlowInfo + \"!type1!0!entity2\"));\r\n        }\r\n        String entityUIDWithFlowInfo = appUIDWithFlowInfo + \"!type1!0!entity1\";\r\n        uri = URI.create(\"http://localhost:\" + getServerPort() + \"/ws/v2/timeline/\" + \"entity-uid/\" + entityUIDWithFlowInfo);\r\n        resp = getResponse(client, uri);\r\n        TimelineEntity singleEntity1 = resp.getEntity(TimelineEntity.class);\r\n        assertNotNull(singleEntity1);\r\n        assertEquals(\"type1\", singleEntity1.getType());\r\n        assertEquals(\"entity1\", singleEntity1.getId());\r\n        String entityUIDWithoutFlowInfo = appUIDWithoutFlowInfo + \"!type1!0!entity1\";\r\n        uri = URI.create(\"http://localhost:\" + getServerPort() + \"/ws/v2/timeline/\" + \"entity-uid/\" + entityUIDWithoutFlowInfo);\r\n        resp = getResponse(client, uri);\r\n        TimelineEntity singleEntity2 = resp.getEntity(TimelineEntity.class);\r\n        assertNotNull(singleEntity2);\r\n        assertEquals(\"type1\", singleEntity2.getType());\r\n        assertEquals(\"entity1\", singleEntity2.getId());\r\n    } finally {\r\n        client.destroy();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-yarn-project\\hadoop-yarn\\hadoop-yarn-server\\hadoop-yarn-server-timelineservice-hbase-tests\\src\\test\\java\\org\\apache\\hadoop\\yarn\\server\\timelineservice\\reader",
  "methodName" : "testUIDNotProperlyEscaped",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void testUIDNotProperlyEscaped() throws Exception\n{\r\n    Client client = createClient();\r\n    try {\r\n        String appUID = \"cluster1!user*1!flow_name!1002345678919!application_1111111111_1111\";\r\n        URI uri = URI.create(\"http://localhost:\" + getServerPort() + \"/ws/v2/\" + \"timeline/app-uid/\" + appUID);\r\n        verifyHttpResponse(client, uri, Status.BAD_REQUEST);\r\n    } finally {\r\n        client.destroy();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-yarn-project\\hadoop-yarn\\hadoop-yarn-server\\hadoop-yarn-server-timelineservice-hbase-tests\\src\\test\\java\\org\\apache\\hadoop\\yarn\\server\\timelineservice\\reader",
  "methodName" : "testGetFlows",
  "errType" : null,
  "containingMethodsNum" : 26,
  "sourceCodeText" : "void testGetFlows() throws Exception\n{\r\n    Client client = createClient();\r\n    try {\r\n        URI uri = URI.create(\"http://localhost:\" + getServerPort() + \"/ws/v2/\" + \"timeline/clusters/cluster1/flows\");\r\n        verifyFlowEntites(client, uri, 3, new int[] { 3, 2, 1 }, new String[] { \"flow1\", \"flow_name\", \"flow_name2\" });\r\n        uri = URI.create(\"http://localhost:\" + getServerPort() + \"/ws/v2/\" + \"timeline/flows/\");\r\n        verifyFlowEntites(client, uri, 3, new int[] { 3, 2, 1 }, new String[] { \"flow1\", \"flow_name\", \"flow_name2\" });\r\n        uri = URI.create(\"http://localhost:\" + getServerPort() + \"/ws/v2/\" + \"timeline/clusters/cluster1/flows?limit=1\");\r\n        verifyFlowEntites(client, uri, 1, new int[] { 3 }, new String[] { \"flow1\" });\r\n        long firstFlowActivity = HBaseTimelineSchemaUtils.getTopOfTheDayTimestamp(1425016501000L);\r\n        DateFormat fmt = TimelineReaderWebServices.DATE_FORMAT.get();\r\n        uri = URI.create(\"http://localhost:\" + getServerPort() + \"/ws/v2/\" + \"timeline/clusters/cluster1/flows?daterange=\" + fmt.format(firstFlowActivity) + \"-\" + fmt.format(dayTs));\r\n        verifyFlowEntites(client, uri, 3, new int[] { 3, 2, 1 }, new String[] { \"flow1\", \"flow_name\", \"flow_name2\" });\r\n        uri = URI.create(\"http://localhost:\" + getServerPort() + \"/ws/v2/\" + \"timeline/clusters/cluster1/flows?daterange=\" + fmt.format(dayTs + (4 * 86400000L)));\r\n        verifyFlowEntites(client, uri, 0, new int[] {}, new String[] {});\r\n        uri = URI.create(\"http://localhost:\" + getServerPort() + \"/ws/v2/\" + \"timeline/clusters/cluster1/flows?daterange=-\" + fmt.format(dayTs));\r\n        verifyFlowEntites(client, uri, 3, new int[] { 3, 2, 1 }, new String[] { \"flow1\", \"flow_name\", \"flow_name2\" });\r\n        uri = URI.create(\"http://localhost:\" + getServerPort() + \"/ws/v2/\" + \"timeline/clusters/cluster1/flows?daterange=\" + fmt.format(firstFlowActivity) + \"-\");\r\n        verifyFlowEntites(client, uri, 3, new int[] { 3, 2, 1 }, new String[] { \"flow1\", \"flow_name\", \"flow_name2\" });\r\n        uri = URI.create(\"http://localhost:\" + getServerPort() + \"/ws/v2/\" + \"timeline/clusters/cluster1/flows?daterange=20150711:20150714\");\r\n        verifyHttpResponse(client, uri, Status.BAD_REQUEST);\r\n        uri = URI.create(\"http://localhost:\" + getServerPort() + \"/ws/v2/\" + \"timeline/clusters/cluster1/flows?daterange=20150714-20150711\");\r\n        verifyHttpResponse(client, uri, Status.BAD_REQUEST);\r\n        uri = URI.create(\"http://localhost:\" + getServerPort() + \"/ws/v2/\" + \"timeline/clusters/cluster1/flows?daterange=2015071129-20150712\");\r\n        verifyHttpResponse(client, uri, Status.BAD_REQUEST);\r\n        uri = URI.create(\"http://localhost:\" + getServerPort() + \"/ws/v2/\" + \"timeline/clusters/cluster1/flows?daterange=20150711-2015071243\");\r\n        verifyHttpResponse(client, uri, Status.BAD_REQUEST);\r\n    } finally {\r\n        client.destroy();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-yarn-project\\hadoop-yarn\\hadoop-yarn-server\\hadoop-yarn-server-timelineservice-hbase-tests\\src\\test\\java\\org\\apache\\hadoop\\yarn\\server\\timelineservice\\reader",
  "methodName" : "testGetFlowsForPagination",
  "errType" : null,
  "containingMethodsNum" : 18,
  "sourceCodeText" : "void testGetFlowsForPagination() throws Exception\n{\r\n    Client client = createClient();\r\n    int noOfEntities = 3;\r\n    int limit = 2;\r\n    try {\r\n        String flowURI = \"http://localhost:\" + getServerPort() + \"/ws/v2/\" + \"timeline/clusters/cluster1/flows\";\r\n        URI uri = URI.create(flowURI);\r\n        List<FlowActivityEntity> flowEntites = verifyFlowEntites(client, uri, 3, new int[] { 3, 2, 1 }, new String[] { \"flow1\", \"flow_name\", \"flow_name2\" });\r\n        FlowActivityEntity fEntity1 = flowEntites.get(0);\r\n        FlowActivityEntity fEntity3 = flowEntites.get(noOfEntities - 1);\r\n        uri = URI.create(flowURI + \"?limit=\" + limit);\r\n        flowEntites = verifyFlowEntites(client, uri, limit);\r\n        assertEquals(fEntity1, flowEntites.get(0));\r\n        FlowActivityEntity fEntity2 = flowEntites.get(limit - 1);\r\n        uri = URI.create(flowURI + \"?limit=\" + limit + \"&fromid=\" + fEntity2.getInfo().get(TimelineReaderUtils.FROMID_KEY));\r\n        flowEntites = verifyFlowEntites(client, uri, noOfEntities - limit + 1);\r\n        assertEquals(fEntity2, flowEntites.get(0));\r\n        assertEquals(fEntity3, flowEntites.get(noOfEntities - limit));\r\n        uri = URI.create(flowURI + \"?limit=\" + limit + \"&fromid=\" + fEntity3.getInfo().get(TimelineReaderUtils.FROMID_KEY));\r\n        flowEntites = verifyFlowEntites(client, uri, 1);\r\n        assertEquals(fEntity3, flowEntites.get(0));\r\n    } finally {\r\n        client.destroy();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-yarn-project\\hadoop-yarn\\hadoop-yarn-server\\hadoop-yarn-server-timelineservice-hbase-tests\\src\\test\\java\\org\\apache\\hadoop\\yarn\\server\\timelineservice\\reader",
  "methodName" : "testGetApp",
  "errType" : null,
  "containingMethodsNum" : 22,
  "sourceCodeText" : "void testGetApp() throws Exception\n{\r\n    Client client = createClient();\r\n    try {\r\n        URI uri = URI.create(\"http://localhost:\" + getServerPort() + \"/ws/v2/\" + \"timeline/clusters/cluster1/apps/application_1111111111_1111?\" + \"userid=user1&fields=ALL&flowname=flow_name&flowrunid=1002345678919\");\r\n        ClientResponse resp = getResponse(client, uri);\r\n        TimelineEntity entity = resp.getEntity(TimelineEntity.class);\r\n        assertNotNull(entity);\r\n        assertEquals(\"application_1111111111_1111\", entity.getId());\r\n        assertEquals(3, entity.getMetrics().size());\r\n        TimelineMetric m1 = newMetric(TimelineMetric.Type.SINGLE_VALUE, \"HDFS_BYTES_READ\", ts - 80000, 57L);\r\n        TimelineMetric m2 = newMetric(TimelineMetric.Type.SINGLE_VALUE, \"MAP_SLOT_MILLIS\", ts - 80000, 40L);\r\n        TimelineMetric m3 = newMetric(TimelineMetric.Type.SINGLE_VALUE, \"MAP1_SLOT_MILLIS\", ts - 80000, 40L);\r\n        for (TimelineMetric metric : entity.getMetrics()) {\r\n            assertTrue(verifyMetrics(metric, m1, m2, m3));\r\n        }\r\n        uri = URI.create(\"http://localhost:\" + getServerPort() + \"/ws/v2/\" + \"timeline/apps/application_1111111111_2222?userid=user1\" + \"&fields=metrics&flowname=flow_name&flowrunid=1002345678919\");\r\n        resp = getResponse(client, uri);\r\n        entity = resp.getEntity(TimelineEntity.class);\r\n        assertNotNull(entity);\r\n        assertEquals(\"application_1111111111_2222\", entity.getId());\r\n        assertEquals(1, entity.getMetrics().size());\r\n        TimelineMetric m4 = newMetric(TimelineMetric.Type.SINGLE_VALUE, \"MAP_SLOT_MILLIS\", ts - 80000, 101L);\r\n        for (TimelineMetric metric : entity.getMetrics()) {\r\n            assertTrue(verifyMetrics(metric, m4));\r\n        }\r\n    } finally {\r\n        client.destroy();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-yarn-project\\hadoop-yarn\\hadoop-yarn-server\\hadoop-yarn-server-timelineservice-hbase-tests\\src\\test\\java\\org\\apache\\hadoop\\yarn\\server\\timelineservice\\reader",
  "methodName" : "testGetAppWithoutFlowInfo",
  "errType" : null,
  "containingMethodsNum" : 29,
  "sourceCodeText" : "void testGetAppWithoutFlowInfo() throws Exception\n{\r\n    Client client = createClient();\r\n    try {\r\n        URI uri = URI.create(\"http://localhost:\" + getServerPort() + \"/ws/v2/\" + \"timeline/clusters/cluster1/apps/application_1111111111_1111?\" + \"fields=ALL\");\r\n        ClientResponse resp = getResponse(client, uri);\r\n        TimelineEntity entity = resp.getEntity(TimelineEntity.class);\r\n        assertNotNull(entity);\r\n        assertEquals(\"application_1111111111_1111\", entity.getId());\r\n        assertEquals(1, entity.getConfigs().size());\r\n        assertEquals(3, entity.getMetrics().size());\r\n        TimelineMetric m1 = newMetric(TimelineMetric.Type.SINGLE_VALUE, \"HDFS_BYTES_READ\", ts - 80000, 57L);\r\n        TimelineMetric m2 = newMetric(TimelineMetric.Type.SINGLE_VALUE, \"MAP_SLOT_MILLIS\", ts - 80000, 40L);\r\n        TimelineMetric m3 = newMetric(TimelineMetric.Type.SINGLE_VALUE, \"MAP1_SLOT_MILLIS\", ts - 80000, 40L);\r\n        for (TimelineMetric metric : entity.getMetrics()) {\r\n            assertTrue(verifyMetrics(metric, m1, m2, m3));\r\n        }\r\n        uri = URI.create(\"http://localhost:\" + getServerPort() + \"/ws/v2/\" + \"timeline/clusters/cluster1/apps/application_1111111111_1111?\" + \"fields=ALL&metricslimit=10\");\r\n        resp = getResponse(client, uri);\r\n        entity = resp.getEntity(TimelineEntity.class);\r\n        assertNotNull(entity);\r\n        assertEquals(\"application_1111111111_1111\", entity.getId());\r\n        assertEquals(1, entity.getConfigs().size());\r\n        assertEquals(3, entity.getMetrics().size());\r\n        m1 = newMetric(TimelineMetric.Type.TIME_SERIES, \"HDFS_BYTES_READ\", ts - 100000, 31L);\r\n        m1.addValue(ts - 80000, 57L);\r\n        m2 = newMetric(TimelineMetric.Type.TIME_SERIES, \"MAP_SLOT_MILLIS\", ts - 100000, 2L);\r\n        m2.addValue(ts - 80000, 40L);\r\n        m3 = newMetric(TimelineMetric.Type.TIME_SERIES, \"MAP1_SLOT_MILLIS\", ts - 100000, 2L);\r\n        m3.addValue(ts - 80000, 40L);\r\n        for (TimelineMetric metric : entity.getMetrics()) {\r\n            assertTrue(verifyMetrics(metric, m1, m2, m3));\r\n        }\r\n    } finally {\r\n        client.destroy();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-yarn-project\\hadoop-yarn\\hadoop-yarn-server\\hadoop-yarn-server-timelineservice-hbase-tests\\src\\test\\java\\org\\apache\\hadoop\\yarn\\server\\timelineservice\\reader",
  "methodName" : "testGetEntityWithoutFlowInfo",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void testGetEntityWithoutFlowInfo() throws Exception\n{\r\n    Client client = createClient();\r\n    try {\r\n        URI uri = URI.create(\"http://localhost:\" + getServerPort() + \"/ws/v2/\" + \"timeline/clusters/cluster1/apps/application_1111111111_1111/\" + \"entities/type1/entity1\");\r\n        ClientResponse resp = getResponse(client, uri);\r\n        TimelineEntity entity = resp.getEntity(TimelineEntity.class);\r\n        assertNotNull(entity);\r\n        assertEquals(\"entity1\", entity.getId());\r\n        assertEquals(\"type1\", entity.getType());\r\n    } finally {\r\n        client.destroy();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-yarn-project\\hadoop-yarn\\hadoop-yarn-server\\hadoop-yarn-server-timelineservice-hbase-tests\\src\\test\\java\\org\\apache\\hadoop\\yarn\\server\\timelineservice\\reader",
  "methodName" : "testGetEntitiesWithoutFlowInfo",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void testGetEntitiesWithoutFlowInfo() throws Exception\n{\r\n    Client client = createClient();\r\n    try {\r\n        URI uri = URI.create(\"http://localhost:\" + getServerPort() + \"/ws/v2/\" + \"timeline/clusters/cluster1/apps/application_1111111111_1111/\" + \"entities/type1\");\r\n        ClientResponse resp = getResponse(client, uri);\r\n        Set<TimelineEntity> entities = resp.getEntity(new GenericType<Set<TimelineEntity>>() {\r\n        });\r\n        assertNotNull(entities);\r\n        assertEquals(2, entities.size());\r\n        for (TimelineEntity entity : entities) {\r\n            assertTrue(entity.getId().equals(\"entity1\") || entity.getId().equals(\"entity2\"));\r\n        }\r\n    } finally {\r\n        client.destroy();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-yarn-project\\hadoop-yarn\\hadoop-yarn-server\\hadoop-yarn-server-timelineservice-hbase-tests\\src\\test\\java\\org\\apache\\hadoop\\yarn\\server\\timelineservice\\reader",
  "methodName" : "testGetEntitiesDataToRetrieve",
  "errType" : null,
  "containingMethodsNum" : 56,
  "sourceCodeText" : "void testGetEntitiesDataToRetrieve() throws Exception\n{\r\n    Client client = createClient();\r\n    try {\r\n        URI uri = URI.create(\"http://localhost:\" + getServerPort() + \"/ws/v2/\" + \"timeline/clusters/cluster1/apps/application_1111111111_1111/\" + \"entities/type1?confstoretrieve=cfg_\");\r\n        ClientResponse resp = getResponse(client, uri);\r\n        Set<TimelineEntity> entities = resp.getEntity(new GenericType<Set<TimelineEntity>>() {\r\n        });\r\n        assertNotNull(entities);\r\n        assertEquals(2, entities.size());\r\n        int cfgCnt = 0;\r\n        for (TimelineEntity entity : entities) {\r\n            cfgCnt += entity.getConfigs().size();\r\n            for (String configKey : entity.getConfigs().keySet()) {\r\n                assertTrue(configKey.startsWith(\"cfg_\"));\r\n            }\r\n        }\r\n        assertEquals(2, cfgCnt);\r\n        uri = URI.create(\"http://localhost:\" + getServerPort() + \"/ws/v2/\" + \"timeline/clusters/cluster1/apps/application_1111111111_1111/\" + \"entities/type1?confstoretrieve=cfg_,config_\");\r\n        resp = getResponse(client, uri);\r\n        entities = resp.getEntity(new GenericType<Set<TimelineEntity>>() {\r\n        });\r\n        assertNotNull(entities);\r\n        assertEquals(2, entities.size());\r\n        cfgCnt = 0;\r\n        for (TimelineEntity entity : entities) {\r\n            cfgCnt += entity.getConfigs().size();\r\n            for (String configKey : entity.getConfigs().keySet()) {\r\n                assertTrue(configKey.startsWith(\"cfg_\") || configKey.startsWith(\"config_\"));\r\n            }\r\n        }\r\n        assertEquals(5, cfgCnt);\r\n        uri = URI.create(\"http://localhost:\" + getServerPort() + \"/ws/v2/\" + \"timeline/clusters/cluster1/apps/application_1111111111_1111/\" + \"entities/type1?confstoretrieve=!(cfg_,config_)\");\r\n        resp = getResponse(client, uri);\r\n        entities = resp.getEntity(new GenericType<Set<TimelineEntity>>() {\r\n        });\r\n        assertNotNull(entities);\r\n        assertEquals(2, entities.size());\r\n        cfgCnt = 0;\r\n        for (TimelineEntity entity : entities) {\r\n            cfgCnt += entity.getConfigs().size();\r\n            for (String configKey : entity.getConfigs().keySet()) {\r\n                assertTrue(configKey.startsWith(\"configuration_\"));\r\n            }\r\n        }\r\n        assertEquals(1, cfgCnt);\r\n        uri = URI.create(\"http://localhost:\" + getServerPort() + \"/ws/v2/\" + \"timeline/clusters/cluster1/apps/application_1111111111_1111/\" + \"entities/type1?metricstoretrieve=MAP_\");\r\n        resp = getResponse(client, uri);\r\n        entities = resp.getEntity(new GenericType<Set<TimelineEntity>>() {\r\n        });\r\n        assertNotNull(entities);\r\n        assertEquals(2, entities.size());\r\n        int metricCnt = 0;\r\n        for (TimelineEntity entity : entities) {\r\n            metricCnt += entity.getMetrics().size();\r\n            for (TimelineMetric metric : entity.getMetrics()) {\r\n                assertTrue(metric.getId().startsWith(\"MAP_\"));\r\n            }\r\n        }\r\n        assertEquals(1, metricCnt);\r\n        uri = URI.create(\"http://localhost:\" + getServerPort() + \"/ws/v2/\" + \"timeline/clusters/cluster1/apps/application_1111111111_1111/\" + \"entities/type1?metricstoretrieve=MAP1_,HDFS_\");\r\n        resp = getResponse(client, uri);\r\n        entities = resp.getEntity(new GenericType<Set<TimelineEntity>>() {\r\n        });\r\n        assertNotNull(entities);\r\n        assertEquals(2, entities.size());\r\n        metricCnt = 0;\r\n        for (TimelineEntity entity : entities) {\r\n            metricCnt += entity.getMetrics().size();\r\n            for (TimelineMetric metric : entity.getMetrics()) {\r\n                assertTrue(metric.getId().startsWith(\"MAP1_\") || metric.getId().startsWith(\"HDFS_\"));\r\n            }\r\n        }\r\n        assertEquals(3, metricCnt);\r\n        uri = URI.create(\"http://localhost:\" + getServerPort() + \"/ws/v2/\" + \"timeline/clusters/cluster1/apps/application_1111111111_1111/\" + \"entities/type1?metricstoretrieve=!(MAP1_,HDFS_)\");\r\n        resp = getResponse(client, uri);\r\n        entities = resp.getEntity(new GenericType<Set<TimelineEntity>>() {\r\n        });\r\n        assertNotNull(entities);\r\n        assertEquals(2, entities.size());\r\n        metricCnt = 0;\r\n        for (TimelineEntity entity : entities) {\r\n            metricCnt += entity.getMetrics().size();\r\n            for (TimelineMetric metric : entity.getMetrics()) {\r\n                assertTrue(metric.getId().startsWith(\"MAP_\") || metric.getId().startsWith(\"MAP11_\"));\r\n            }\r\n        }\r\n        assertEquals(2, metricCnt);\r\n    } finally {\r\n        client.destroy();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-yarn-project\\hadoop-yarn\\hadoop-yarn-server\\hadoop-yarn-server-timelineservice-hbase-tests\\src\\test\\java\\org\\apache\\hadoop\\yarn\\server\\timelineservice\\reader",
  "methodName" : "testGetEntitiesConfigFilters",
  "errType" : null,
  "containingMethodsNum" : 51,
  "sourceCodeText" : "void testGetEntitiesConfigFilters() throws Exception\n{\r\n    Client client = createClient();\r\n    try {\r\n        URI uri = URI.create(\"http://localhost:\" + getServerPort() + \"/ws/v2/\" + \"timeline/clusters/cluster1/apps/application_1111111111_1111/\" + \"entities/type1?conffilters=config_param1%20eq%20value1%20OR%20\" + \"config_param1%20eq%20value3\");\r\n        ClientResponse resp = getResponse(client, uri);\r\n        Set<TimelineEntity> entities = resp.getEntity(new GenericType<Set<TimelineEntity>>() {\r\n        });\r\n        assertNotNull(entities);\r\n        assertEquals(2, entities.size());\r\n        for (TimelineEntity entity : entities) {\r\n            assertTrue(entity.getId().equals(\"entity1\") || entity.getId().equals(\"entity2\"));\r\n        }\r\n        uri = URI.create(\"http://localhost:\" + getServerPort() + \"/ws/v2/\" + \"timeline/clusters/cluster1/apps/application_1111111111_1111/\" + \"entities/type1?conffilters=config_param1%20eq%20value1%20AND\" + \"%20configuration_param2%20eq%20value2\");\r\n        resp = getResponse(client, uri);\r\n        entities = resp.getEntity(new GenericType<Set<TimelineEntity>>() {\r\n        });\r\n        assertNotNull(entities);\r\n        assertEquals(0, entities.size());\r\n        uri = URI.create(\"http://localhost:\" + getServerPort() + \"/ws/v2/\" + \"timeline/clusters/cluster1/apps/application_1111111111_1111/\" + \"entities/type1?conffilters=(config_param1%20eq%20value1%20AND\" + \"%20configuration_param2%20eq%20value2)%20OR%20(config_param1%20eq\" + \"%20value3%20AND%20cfg_param3%20eq%20value1)\");\r\n        resp = getResponse(client, uri);\r\n        entities = resp.getEntity(new GenericType<Set<TimelineEntity>>() {\r\n        });\r\n        assertNotNull(entities);\r\n        assertEquals(1, entities.size());\r\n        int cfgCnt = 0;\r\n        for (TimelineEntity entity : entities) {\r\n            cfgCnt += entity.getConfigs().size();\r\n            assertEquals(\"entity2\", entity.getId());\r\n        }\r\n        assertEquals(0, cfgCnt);\r\n        uri = URI.create(\"http://localhost:\" + getServerPort() + \"/ws/v2/\" + \"timeline/clusters/cluster1/apps/application_1111111111_1111/\" + \"entities/type1?conffilters=(config_param1%20eq%20value1%20AND\" + \"%20configuration_param2%20eq%20value2)%20OR%20(config_param1%20eq\" + \"%20value3%20AND%20cfg_param3%20eq%20value1)&fields=CONFIGS\");\r\n        resp = getResponse(client, uri);\r\n        entities = resp.getEntity(new GenericType<Set<TimelineEntity>>() {\r\n        });\r\n        assertNotNull(entities);\r\n        assertEquals(1, entities.size());\r\n        cfgCnt = 0;\r\n        for (TimelineEntity entity : entities) {\r\n            cfgCnt += entity.getConfigs().size();\r\n            assertEquals(\"entity2\", entity.getId());\r\n        }\r\n        assertEquals(3, cfgCnt);\r\n        uri = URI.create(\"http://localhost:\" + getServerPort() + \"/ws/v2/\" + \"timeline/clusters/cluster1/apps/application_1111111111_1111/\" + \"entities/type1?conffilters=(config_param1%20eq%20value1%20AND\" + \"%20configuration_param2%20eq%20value2)%20OR%20(config_param1%20eq\" + \"%20value3%20AND%20cfg_param3%20eq%20value1)&confstoretrieve=cfg_,\" + \"configuration_\");\r\n        resp = getResponse(client, uri);\r\n        entities = resp.getEntity(new GenericType<Set<TimelineEntity>>() {\r\n        });\r\n        assertNotNull(entities);\r\n        assertEquals(1, entities.size());\r\n        cfgCnt = 0;\r\n        for (TimelineEntity entity : entities) {\r\n            cfgCnt += entity.getConfigs().size();\r\n            assertEquals(\"entity2\", entity.getId());\r\n            for (String configKey : entity.getConfigs().keySet()) {\r\n                assertTrue(configKey.startsWith(\"cfg_\") || configKey.startsWith(\"configuration_\"));\r\n            }\r\n        }\r\n        assertEquals(2, cfgCnt);\r\n        uri = URI.create(\"http://localhost:\" + getServerPort() + \"/ws/v2/\" + \"timeline/clusters/cluster1/apps/application_1111111111_1111/\" + \"entities/type1?conffilters=configuration_param2%20ne%20value3\");\r\n        resp = getResponse(client, uri);\r\n        entities = resp.getEntity(new GenericType<Set<TimelineEntity>>() {\r\n        });\r\n        assertNotNull(entities);\r\n        assertEquals(2, entities.size());\r\n        for (TimelineEntity entity : entities) {\r\n            assertTrue(entity.getId().equals(\"entity1\") || entity.getId().equals(\"entity2\"));\r\n        }\r\n        uri = URI.create(\"http://localhost:\" + getServerPort() + \"/ws/v2/\" + \"timeline/clusters/cluster1/apps/application_1111111111_1111/\" + \"entities/type1?conffilters=configuration_param2%20ene%20value3\");\r\n        resp = getResponse(client, uri);\r\n        entities = resp.getEntity(new GenericType<Set<TimelineEntity>>() {\r\n        });\r\n        assertNotNull(entities);\r\n        assertEquals(1, entities.size());\r\n        for (TimelineEntity entity : entities) {\r\n            assertEquals(\"entity2\", entity.getId());\r\n        }\r\n    } finally {\r\n        client.destroy();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-yarn-project\\hadoop-yarn\\hadoop-yarn-server\\hadoop-yarn-server-timelineservice-hbase-tests\\src\\test\\java\\org\\apache\\hadoop\\yarn\\server\\timelineservice\\reader",
  "methodName" : "testGetEntitiesInfoFilters",
  "errType" : null,
  "containingMethodsNum" : 47,
  "sourceCodeText" : "void testGetEntitiesInfoFilters() throws Exception\n{\r\n    Client client = createClient();\r\n    try {\r\n        URI uri = URI.create(\"http://localhost:\" + getServerPort() + \"/ws/v2/\" + \"timeline/clusters/cluster1/apps/application_1111111111_1111/\" + \"entities/type1?infofilters=info1%20eq%20cluster1%20OR%20info1%20eq\" + \"%20cluster2\");\r\n        ClientResponse resp = getResponse(client, uri);\r\n        Set<TimelineEntity> entities = resp.getEntity(new GenericType<Set<TimelineEntity>>() {\r\n        });\r\n        assertNotNull(entities);\r\n        assertEquals(2, entities.size());\r\n        for (TimelineEntity entity : entities) {\r\n            assertTrue(entity.getId().equals(\"entity1\") || entity.getId().equals(\"entity2\"));\r\n        }\r\n        uri = URI.create(\"http://localhost:\" + getServerPort() + \"/ws/v2/\" + \"timeline/clusters/cluster1/apps/application_1111111111_1111/\" + \"entities/type1?infofilters=info1%20eq%20cluster1%20AND%20info4%20\" + \"eq%2035000\");\r\n        resp = getResponse(client, uri);\r\n        entities = resp.getEntity(new GenericType<Set<TimelineEntity>>() {\r\n        });\r\n        assertNotNull(entities);\r\n        assertEquals(0, entities.size());\r\n        uri = URI.create(\"http://localhost:\" + getServerPort() + \"/ws/v2/\" + \"timeline/clusters/cluster1/apps/application_1111111111_1111/\" + \"entities/type1?infofilters=info4%20eq%2035000%20OR%20info4%20eq\" + \"%2036000\");\r\n        resp = getResponse(client, uri);\r\n        entities = resp.getEntity(new GenericType<Set<TimelineEntity>>() {\r\n        });\r\n        assertNotNull(entities);\r\n        assertEquals(2, entities.size());\r\n        for (TimelineEntity entity : entities) {\r\n            assertTrue(entity.getId().equals(\"entity1\") || entity.getId().equals(\"entity2\"));\r\n        }\r\n        uri = URI.create(\"http://localhost:\" + getServerPort() + \"/ws/v2/\" + \"timeline/clusters/cluster1/apps/application_1111111111_1111/\" + \"entities/type1?infofilters=(info1%20eq%20cluster1%20AND%20info4%20\" + \"eq%2035000)%20OR%20(info1%20eq%20cluster2%20AND%20info2%20eq%202.0\" + \")\");\r\n        resp = getResponse(client, uri);\r\n        entities = resp.getEntity(new GenericType<Set<TimelineEntity>>() {\r\n        });\r\n        assertNotNull(entities);\r\n        assertEquals(1, entities.size());\r\n        int infoCnt = 0;\r\n        for (TimelineEntity entity : entities) {\r\n            infoCnt += entity.getInfo().size();\r\n            assertEquals(\"entity2\", entity.getId());\r\n        }\r\n        assertEquals(2, infoCnt);\r\n        uri = URI.create(\"http://localhost:\" + getServerPort() + \"/ws/v2/\" + \"timeline/clusters/cluster1/apps/application_1111111111_1111/\" + \"entities/type1?infofilters=(info1%20eq%20cluster1%20AND%20info4%20\" + \"eq%2035000)%20OR%20(info1%20eq%20cluster2%20AND%20info2%20eq%20\" + \"2.0)&fields=INFO\");\r\n        resp = getResponse(client, uri);\r\n        entities = resp.getEntity(new GenericType<Set<TimelineEntity>>() {\r\n        });\r\n        assertNotNull(entities);\r\n        assertEquals(1, entities.size());\r\n        infoCnt = 0;\r\n        for (TimelineEntity entity : entities) {\r\n            infoCnt += entity.getInfo().size();\r\n            assertEquals(\"entity2\", entity.getId());\r\n        }\r\n        assertEquals(5, infoCnt);\r\n        uri = URI.create(\"http://localhost:\" + getServerPort() + \"/ws/v2/\" + \"timeline/clusters/cluster1/apps/application_1111111111_1111/\" + \"entities/type1?infofilters=info3%20ne%2039000\");\r\n        resp = getResponse(client, uri);\r\n        entities = resp.getEntity(new GenericType<Set<TimelineEntity>>() {\r\n        });\r\n        assertNotNull(entities);\r\n        assertEquals(2, entities.size());\r\n        for (TimelineEntity entity : entities) {\r\n            assertTrue(entity.getId().equals(\"entity1\") || entity.getId().equals(\"entity2\"));\r\n        }\r\n        uri = URI.create(\"http://localhost:\" + getServerPort() + \"/ws/v2/\" + \"timeline/clusters/cluster1/apps/application_1111111111_1111/\" + \"entities/type1?infofilters=info3%20ene%2039000\");\r\n        resp = getResponse(client, uri);\r\n        entities = resp.getEntity(new GenericType<Set<TimelineEntity>>() {\r\n        });\r\n        assertNotNull(entities);\r\n        assertEquals(1, entities.size());\r\n        for (TimelineEntity entity : entities) {\r\n            assertEquals(\"entity1\", entity.getId());\r\n        }\r\n    } finally {\r\n        client.destroy();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-yarn-project\\hadoop-yarn\\hadoop-yarn-server\\hadoop-yarn-server-timelineservice-hbase-tests\\src\\test\\java\\org\\apache\\hadoop\\yarn\\server\\timelineservice\\reader",
  "methodName" : "testGetEntitiesMetricFilters",
  "errType" : null,
  "containingMethodsNum" : 68,
  "sourceCodeText" : "void testGetEntitiesMetricFilters() throws Exception\n{\r\n    Client client = createClient();\r\n    try {\r\n        URI uri = URI.create(\"http://localhost:\" + getServerPort() + \"/ws/v2/\" + \"timeline/clusters/cluster1/apps/application_1111111111_1111/\" + \"entities/type1?metricfilters=HDFS_BYTES_READ%20lt%2060%20OR%20\" + \"HDFS_BYTES_READ%20eq%20157\");\r\n        ClientResponse resp = getResponse(client, uri);\r\n        Set<TimelineEntity> entities = resp.getEntity(new GenericType<Set<TimelineEntity>>() {\r\n        });\r\n        assertNotNull(entities);\r\n        assertEquals(2, entities.size());\r\n        for (TimelineEntity entity : entities) {\r\n            assertTrue(entity.getId().equals(\"entity1\") || entity.getId().equals(\"entity2\"));\r\n        }\r\n        uri = URI.create(\"http://localhost:\" + getServerPort() + \"/ws/v2/\" + \"timeline/clusters/cluster1/apps/application_1111111111_1111/\" + \"entities/type1?metricfilters=HDFS_BYTES_READ%20lt%2060%20AND%20\" + \"MAP_SLOT_MILLIS%20gt%2040\");\r\n        resp = getResponse(client, uri);\r\n        entities = resp.getEntity(new GenericType<Set<TimelineEntity>>() {\r\n        });\r\n        assertNotNull(entities);\r\n        assertEquals(0, entities.size());\r\n        uri = URI.create(\"http://localhost:\" + getServerPort() + \"/ws/v2/\" + \"timeline/clusters/cluster1/apps/application_1111111111_1111/\" + \"entities/type1?metricfilters=(HDFS_BYTES_READ%20lt%2060%20AND%20\" + \"MAP_SLOT_MILLIS%20gt%2040)%20OR%20(MAP1_SLOT_MILLIS%20ge\" + \"%20140%20AND%20MAP11_SLOT_MILLIS%20le%20122)\");\r\n        resp = getResponse(client, uri);\r\n        entities = resp.getEntity(new GenericType<Set<TimelineEntity>>() {\r\n        });\r\n        assertNotNull(entities);\r\n        assertEquals(1, entities.size());\r\n        int metricCnt = 0;\r\n        for (TimelineEntity entity : entities) {\r\n            metricCnt += entity.getMetrics().size();\r\n            assertEquals(\"entity2\", entity.getId());\r\n        }\r\n        assertEquals(0, metricCnt);\r\n        uri = URI.create(\"http://localhost:\" + getServerPort() + \"/ws/v2/\" + \"timeline/clusters/cluster1/apps/application_1111111111_1111/\" + \"entities/type1?metricfilters=(HDFS_BYTES_READ%20lt%2060%20AND%20\" + \"MAP_SLOT_MILLIS%20gt%2040)%20OR%20(MAP1_SLOT_MILLIS%20ge\" + \"%20140%20AND%20MAP11_SLOT_MILLIS%20le%20122)&fields=METRICS\");\r\n        resp = getResponse(client, uri);\r\n        entities = resp.getEntity(new GenericType<Set<TimelineEntity>>() {\r\n        });\r\n        assertNotNull(entities);\r\n        assertEquals(1, entities.size());\r\n        metricCnt = 0;\r\n        for (TimelineEntity entity : entities) {\r\n            metricCnt += entity.getMetrics().size();\r\n            assertEquals(\"entity2\", entity.getId());\r\n        }\r\n        assertEquals(3, metricCnt);\r\n        uri = URI.create(\"http://localhost:\" + getServerPort() + \"/ws/v2/\" + \"timeline/clusters/cluster1/apps/application_1111111111_1111/\" + \"entities/type1?metricfilters=(HDFS_BYTES_READ%20lt%2060%20AND%20\" + \"MAP_SLOT_MILLIS%20gt%2040)%20OR%20(MAP1_SLOT_MILLIS%20ge\" + \"%20140%20AND%20MAP11_SLOT_MILLIS%20le%20122)&metricstoretrieve=\" + \"!(HDFS)\");\r\n        resp = getResponse(client, uri);\r\n        entities = resp.getEntity(new GenericType<Set<TimelineEntity>>() {\r\n        });\r\n        assertNotNull(entities);\r\n        assertEquals(1, entities.size());\r\n        metricCnt = 0;\r\n        for (TimelineEntity entity : entities) {\r\n            metricCnt += entity.getMetrics().size();\r\n            assertEquals(\"entity2\", entity.getId());\r\n            for (TimelineMetric metric : entity.getMetrics()) {\r\n                assertTrue(metric.getId().startsWith(\"MAP1\"));\r\n                assertEquals(TimelineMetric.Type.SINGLE_VALUE, metric.getType());\r\n            }\r\n        }\r\n        assertEquals(2, metricCnt);\r\n        uri = URI.create(\"http://localhost:\" + getServerPort() + \"/ws/v2/\" + \"timeline/clusters/cluster1/apps/application_1111111111_1111/\" + \"entities/type1?metricfilters=(HDFS_BYTES_READ%20lt%2060%20AND%20\" + \"MAP_SLOT_MILLIS%20gt%2040)%20OR%20(MAP1_SLOT_MILLIS%20ge\" + \"%20140%20AND%20MAP11_SLOT_MILLIS%20le%20122)&metricstoretrieve=\" + \"!(HDFS)&metricslimit=10\");\r\n        resp = getResponse(client, uri);\r\n        entities = resp.getEntity(new GenericType<Set<TimelineEntity>>() {\r\n        });\r\n        assertNotNull(entities);\r\n        assertEquals(1, entities.size());\r\n        metricCnt = 0;\r\n        for (TimelineEntity entity : entities) {\r\n            metricCnt += entity.getMetrics().size();\r\n            assertEquals(\"entity2\", entity.getId());\r\n            for (TimelineMetric metric : entity.getMetrics()) {\r\n                assertTrue(metric.getId().startsWith(\"MAP1\"));\r\n                if (metric.getId().equals(\"MAP1_SLOT_MILLIS\")) {\r\n                    assertEquals(2, metric.getValues().size());\r\n                    assertEquals(TimelineMetric.Type.TIME_SERIES, metric.getType());\r\n                } else if (metric.getId().equals(\"MAP11_SLOT_MILLIS\")) {\r\n                    assertEquals(TimelineMetric.Type.SINGLE_VALUE, metric.getType());\r\n                } else {\r\n                    fail(\"Unexpected metric id\");\r\n                }\r\n            }\r\n        }\r\n        assertEquals(2, metricCnt);\r\n        uri = URI.create(\"http://localhost:\" + getServerPort() + \"/ws/v2/\" + \"timeline/clusters/cluster1/apps/application_1111111111_1111/\" + \"entities/type1?metricfilters=MAP11_SLOT_MILLIS%20ne%20100\");\r\n        resp = getResponse(client, uri);\r\n        entities = resp.getEntity(new GenericType<Set<TimelineEntity>>() {\r\n        });\r\n        assertNotNull(entities);\r\n        assertEquals(2, entities.size());\r\n        for (TimelineEntity entity : entities) {\r\n            assertTrue(entity.getId().equals(\"entity1\") || entity.getId().equals(\"entity2\"));\r\n        }\r\n        uri = URI.create(\"http://localhost:\" + getServerPort() + \"/ws/v2/\" + \"timeline/clusters/cluster1/apps/application_1111111111_1111/\" + \"entities/type1?metricfilters=MAP11_SLOT_MILLIS%20ene%20100\");\r\n        resp = getResponse(client, uri);\r\n        entities = resp.getEntity(new GenericType<Set<TimelineEntity>>() {\r\n        });\r\n        assertNotNull(entities);\r\n        assertEquals(1, entities.size());\r\n        for (TimelineEntity entity : entities) {\r\n            assertEquals(\"entity2\", entity.getId());\r\n        }\r\n    } finally {\r\n        client.destroy();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-yarn-project\\hadoop-yarn\\hadoop-yarn-server\\hadoop-yarn-server-timelineservice-hbase-tests\\src\\test\\java\\org\\apache\\hadoop\\yarn\\server\\timelineservice\\reader",
  "methodName" : "testGetEntitiesEventFilters",
  "errType" : null,
  "containingMethodsNum" : 25,
  "sourceCodeText" : "void testGetEntitiesEventFilters() throws Exception\n{\r\n    Client client = createClient();\r\n    try {\r\n        URI uri = URI.create(\"http://localhost:\" + getServerPort() + \"/ws/v2/\" + \"timeline/clusters/cluster1/apps/application_1111111111_1111/\" + \"entities/type1?eventfilters=event1,event3\");\r\n        ClientResponse resp = getResponse(client, uri);\r\n        Set<TimelineEntity> entities = resp.getEntity(new GenericType<Set<TimelineEntity>>() {\r\n        });\r\n        assertNotNull(entities);\r\n        assertEquals(2, entities.size());\r\n        for (TimelineEntity entity : entities) {\r\n            assertTrue(entity.getId().equals(\"entity1\") || entity.getId().equals(\"entity2\"));\r\n        }\r\n        uri = URI.create(\"http://localhost:\" + getServerPort() + \"/ws/v2/\" + \"timeline/clusters/cluster1/apps/application_1111111111_1111/\" + \"entities/type1?eventfilters=!(event1,event3)\");\r\n        resp = getResponse(client, uri);\r\n        entities = resp.getEntity(new GenericType<Set<TimelineEntity>>() {\r\n        });\r\n        assertNotNull(entities);\r\n        assertEquals(0, entities.size());\r\n        uri = URI.create(\"http://localhost:\" + getServerPort() + \"/ws/v2/\" + \"timeline/clusters/cluster1/apps/application_1111111111_1111/\" + \"entities/type1?eventfilters=!(event1,event3)%20OR%20event5,event6\");\r\n        resp = getResponse(client, uri);\r\n        entities = resp.getEntity(new GenericType<Set<TimelineEntity>>() {\r\n        });\r\n        assertNotNull(entities);\r\n        assertEquals(1, entities.size());\r\n        for (TimelineEntity entity : entities) {\r\n            assertEquals(\"entity2\", entity.getId());\r\n        }\r\n        uri = URI.create(\"http://localhost:\" + getServerPort() + \"/ws/v2/\" + \"timeline/clusters/cluster1/apps/application_1111111111_1111/\" + \"entities/type1?eventfilters=(!(event1,event3)%20OR%20event5,\" + \"event6)%20OR%20(event1,event2%20AND%20(event3,event4))\");\r\n        resp = getResponse(client, uri);\r\n        entities = resp.getEntity(new GenericType<Set<TimelineEntity>>() {\r\n        });\r\n        assertNotNull(entities);\r\n        assertEquals(2, entities.size());\r\n        for (TimelineEntity entity : entities) {\r\n            assertTrue(entity.getId().equals(\"entity1\") || entity.getId().equals(\"entity2\"));\r\n        }\r\n    } finally {\r\n        client.destroy();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-yarn-project\\hadoop-yarn\\hadoop-yarn-server\\hadoop-yarn-server-timelineservice-hbase-tests\\src\\test\\java\\org\\apache\\hadoop\\yarn\\server\\timelineservice\\reader",
  "methodName" : "testGetEntitiesRelationFilters",
  "errType" : null,
  "containingMethodsNum" : 37,
  "sourceCodeText" : "void testGetEntitiesRelationFilters() throws Exception\n{\r\n    Client client = createClient();\r\n    try {\r\n        URI uri = URI.create(\"http://localhost:\" + getServerPort() + \"/ws/v2/\" + \"timeline/clusters/cluster1/apps/application_1111111111_1111/\" + \"entities/type1?isrelatedto=type3:entity31,type2:entity21:entity22\");\r\n        ClientResponse resp = getResponse(client, uri);\r\n        Set<TimelineEntity> entities = resp.getEntity(new GenericType<Set<TimelineEntity>>() {\r\n        });\r\n        assertNotNull(entities);\r\n        assertEquals(2, entities.size());\r\n        for (TimelineEntity entity : entities) {\r\n            assertTrue(entity.getId().equals(\"entity1\") || entity.getId().equals(\"entity2\"));\r\n        }\r\n        uri = URI.create(\"http://localhost:\" + getServerPort() + \"/ws/v2/timeline/\" + \"clusters/cluster1/apps/application_1111111111_1111/entities/type1\" + \"?isrelatedto=!(type3:entity31,type2:entity21:entity22)\");\r\n        resp = getResponse(client, uri);\r\n        entities = resp.getEntity(new GenericType<Set<TimelineEntity>>() {\r\n        });\r\n        assertNotNull(entities);\r\n        assertEquals(0, entities.size());\r\n        uri = URI.create(\"http://localhost:\" + getServerPort() + \"/ws/v2/timeline/\" + \"clusters/cluster1/apps/application_1111111111_1111/entities/type1\" + \"?isrelatedto=!(type3:entity31,type2:entity21:entity22)%20OR%20\" + \"type5:entity51,type6:entity61:entity66\");\r\n        resp = getResponse(client, uri);\r\n        entities = resp.getEntity(new GenericType<Set<TimelineEntity>>() {\r\n        });\r\n        assertNotNull(entities);\r\n        assertEquals(1, entities.size());\r\n        for (TimelineEntity entity : entities) {\r\n            assertEquals(\"entity2\", entity.getId());\r\n        }\r\n        uri = URI.create(\"http://localhost:\" + getServerPort() + \"/ws/v2/timeline/\" + \"clusters/cluster1/apps/application_1111111111_1111/entities/type1\" + \"?isrelatedto=(!(type3:entity31,type2:entity21:entity22)%20OR%20\" + \"type5:entity51,type6:entity61:entity66)%20OR%20(type1:entity14,\" + \"type2:entity21:entity22%20AND%20(type3:entity32:entity35,\" + \"type4:entity42))\");\r\n        resp = getResponse(client, uri);\r\n        entities = resp.getEntity(new GenericType<Set<TimelineEntity>>() {\r\n        });\r\n        assertNotNull(entities);\r\n        assertEquals(2, entities.size());\r\n        for (TimelineEntity entity : entities) {\r\n            assertTrue(entity.getId().equals(\"entity1\") || entity.getId().equals(\"entity2\"));\r\n        }\r\n        uri = URI.create(\"http://localhost:\" + getServerPort() + \"/ws/v2/timeline/\" + \"clusters/cluster1/apps/application_1111111111_1111/entities/type1\" + \"?relatesto=!%20(type3:entity31,type2:entity21:entity22%20)%20OR%20\" + \"type5:entity51,type6:entity61:entity66\");\r\n        resp = getResponse(client, uri);\r\n        entities = resp.getEntity(new GenericType<Set<TimelineEntity>>() {\r\n        });\r\n        assertNotNull(entities);\r\n        assertEquals(1, entities.size());\r\n        for (TimelineEntity entity : entities) {\r\n            assertEquals(\"entity2\", entity.getId());\r\n        }\r\n        uri = URI.create(\"http://localhost:\" + getServerPort() + \"/ws/v2/timeline/\" + \"clusters/cluster1/apps/application_1111111111_1111/entities/type1\" + \"?relatesto=(!(%20type3:entity31,type2:entity21:entity22)%20OR%20\" + \"type5:entity51,type6:entity61:entity66%20)%20OR%20(type1:entity14,\" + \"type2:entity21:entity22%20AND%20(type3:entity32:entity35%20,%20\" + \"type4:entity42))\");\r\n        resp = getResponse(client, uri);\r\n        entities = resp.getEntity(new GenericType<Set<TimelineEntity>>() {\r\n        });\r\n        assertNotNull(entities);\r\n        assertEquals(2, entities.size());\r\n        for (TimelineEntity entity : entities) {\r\n            assertTrue(entity.getId().equals(\"entity1\") || entity.getId().equals(\"entity2\"));\r\n        }\r\n    } finally {\r\n        client.destroy();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-yarn-project\\hadoop-yarn\\hadoop-yarn-server\\hadoop-yarn-server-timelineservice-hbase-tests\\src\\test\\java\\org\\apache\\hadoop\\yarn\\server\\timelineservice\\reader",
  "methodName" : "verifyMetricCount",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void verifyMetricCount(TimelineEntity entity, int expectedMetricsCnt, int expectedMeticsValCnt)\n{\r\n    int metricsValCnt = 0;\r\n    for (TimelineMetric m : entity.getMetrics()) {\r\n        metricsValCnt += m.getValues().size();\r\n    }\r\n    assertEquals(expectedMetricsCnt, entity.getMetrics().size());\r\n    assertEquals(expectedMeticsValCnt, metricsValCnt);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-yarn-project\\hadoop-yarn\\hadoop-yarn-server\\hadoop-yarn-server-timelineservice-hbase-tests\\src\\test\\java\\org\\apache\\hadoop\\yarn\\server\\timelineservice\\reader",
  "methodName" : "verifyMetricsCount",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void verifyMetricsCount(Set<TimelineEntity> entities, int expectedMetricsCnt, int expectedMeticsValCnt)\n{\r\n    int metricsCnt = 0;\r\n    int metricsValCnt = 0;\r\n    for (TimelineEntity entity : entities) {\r\n        metricsCnt += entity.getMetrics().size();\r\n        for (TimelineMetric m : entity.getMetrics()) {\r\n            metricsValCnt += m.getValues().size();\r\n        }\r\n    }\r\n    assertEquals(expectedMetricsCnt, metricsCnt);\r\n    assertEquals(expectedMeticsValCnt, metricsValCnt);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-yarn-project\\hadoop-yarn\\hadoop-yarn-server\\hadoop-yarn-server-timelineservice-hbase-tests\\src\\test\\java\\org\\apache\\hadoop\\yarn\\server\\timelineservice\\reader",
  "methodName" : "testGetEntitiesMetricsTimeRange",
  "errType" : null,
  "containingMethodsNum" : 44,
  "sourceCodeText" : "void testGetEntitiesMetricsTimeRange() throws Exception\n{\r\n    Client client = createClient();\r\n    try {\r\n        URI uri = URI.create(\"http://localhost:\" + getServerPort() + \"/ws/v2/\" + \"timeline/clusters/cluster1/apps/application_1111111111_1111/\" + \"entities/type1?fields=ALL&metricslimit=100&metricstimestart=\" + (ts - 90000) + \"&metricstimeend=\" + (ts - 80000));\r\n        ClientResponse resp = getResponse(client, uri);\r\n        Set<TimelineEntity> entities = resp.getEntity(new GenericType<Set<TimelineEntity>>() {\r\n        });\r\n        assertNotNull(entities);\r\n        assertEquals(2, entities.size());\r\n        verifyMetricsCount(entities, 4, 4);\r\n        uri = URI.create(\"http://localhost:\" + getServerPort() + \"/ws/v2/\" + \"timeline/clusters/cluster1/apps/application_1111111111_1111/\" + \"entities/type1?fields=ALL&metricslimit=100&metricstimestart=\" + (ts - 100000) + \"&metricstimeend=\" + (ts - 80000));\r\n        resp = getResponse(client, uri);\r\n        entities = resp.getEntity(new GenericType<Set<TimelineEntity>>() {\r\n        });\r\n        assertNotNull(entities);\r\n        assertEquals(2, entities.size());\r\n        verifyMetricsCount(entities, 5, 9);\r\n        uri = URI.create(\"http://localhost:\" + getServerPort() + \"/ws/v2/\" + \"timeline/clusters/cluster1/apps/application_1111111111_1111/\" + \"entities/type1?fields=ALL&metricslimit=100&metricstimestart=\" + (ts - 100000));\r\n        resp = getResponse(client, uri);\r\n        entities = resp.getEntity(new GenericType<Set<TimelineEntity>>() {\r\n        });\r\n        assertNotNull(entities);\r\n        assertEquals(2, entities.size());\r\n        verifyMetricsCount(entities, 5, 9);\r\n        uri = URI.create(\"http://localhost:\" + getServerPort() + \"/ws/v2/\" + \"timeline/clusters/cluster1/apps/application_1111111111_1111/\" + \"entities/type1?fields=ALL&metricslimit=100&metricstimeend=\" + (ts - 90000));\r\n        resp = getResponse(client, uri);\r\n        entities = resp.getEntity(new GenericType<Set<TimelineEntity>>() {\r\n        });\r\n        assertNotNull(entities);\r\n        assertEquals(2, entities.size());\r\n        verifyMetricsCount(entities, 5, 5);\r\n        uri = URI.create(\"http://localhost:\" + getServerPort() + \"/ws/v2/\" + \"timeline/clusters/cluster1/apps/application_1111111111_1111/\" + \"entities/type1?fields=ALL&metricstimestart=\" + (ts - 100000));\r\n        resp = getResponse(client, uri);\r\n        entities = resp.getEntity(new GenericType<Set<TimelineEntity>>() {\r\n        });\r\n        assertNotNull(entities);\r\n        assertEquals(2, entities.size());\r\n        verifyMetricsCount(entities, 5, 5);\r\n        uri = URI.create(\"http://localhost:\" + getServerPort() + \"/ws/v2/\" + \"timeline/clusters/cluster1/apps/application_1111111111_1111/\" + \"entities/type1/entity2?fields=ALL&metricstimestart=\" + (ts - 100000) + \"&metricstimeend=\" + (ts - 80000));\r\n        resp = getResponse(client, uri);\r\n        TimelineEntity entity = resp.getEntity(TimelineEntity.class);\r\n        assertNotNull(entity);\r\n        verifyMetricCount(entity, 3, 3);\r\n        uri = URI.create(\"http://localhost:\" + getServerPort() + \"/ws/v2/\" + \"timeline/clusters/cluster1/apps/application_1111111111_1111/\" + \"entities/type1/entity2?fields=ALL&metricslimit=5&metricstimestart=\" + (ts - 100000) + \"&metricstimeend=\" + (ts - 80000));\r\n        resp = getResponse(client, uri);\r\n        entity = resp.getEntity(TimelineEntity.class);\r\n        assertNotNull(entity);\r\n        verifyMetricCount(entity, 3, 5);\r\n        uri = URI.create(\"http://localhost:\" + getServerPort() + \"/ws/v2/\" + \"timeline/clusters/cluster1/apps/application_1111111111_1111/\" + \"entities/type1?fields=ALL&metricslimit=100&metricstimestart=\" + (ts - 80000) + \"&metricstimeend=\" + (ts - 90000));\r\n        verifyHttpResponse(client, uri, Status.BAD_REQUEST);\r\n    } finally {\r\n        client.destroy();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-yarn-project\\hadoop-yarn\\hadoop-yarn-server\\hadoop-yarn-server-timelineservice-hbase-tests\\src\\test\\java\\org\\apache\\hadoop\\yarn\\server\\timelineservice\\reader",
  "methodName" : "testGetEntityDataToRetrieve",
  "errType" : null,
  "containingMethodsNum" : 50,
  "sourceCodeText" : "void testGetEntityDataToRetrieve() throws Exception\n{\r\n    Client client = createClient();\r\n    try {\r\n        URI uri = URI.create(\"http://localhost:\" + getServerPort() + \"/ws/v2/\" + \"timeline/clusters/cluster1/apps/application_1111111111_1111/\" + \"entities/type1/entity2?confstoretrieve=cfg_,configuration_\");\r\n        ClientResponse resp = getResponse(client, uri);\r\n        TimelineEntity entity = resp.getEntity(TimelineEntity.class);\r\n        assertNotNull(entity);\r\n        assertEquals(\"entity2\", entity.getId());\r\n        assertEquals(\"type1\", entity.getType());\r\n        assertEquals(2, entity.getConfigs().size());\r\n        for (String configKey : entity.getConfigs().keySet()) {\r\n            assertTrue(configKey.startsWith(\"configuration_\") || configKey.startsWith(\"cfg_\"));\r\n        }\r\n        uri = URI.create(\"http://localhost:\" + getServerPort() + \"/ws/v2/\" + \"timeline/clusters/cluster1/apps/application_1111111111_1111/\" + \"entities/type1/entity2?confstoretrieve=!(cfg_,configuration_)\");\r\n        resp = getResponse(client, uri);\r\n        entity = resp.getEntity(TimelineEntity.class);\r\n        assertNotNull(entity);\r\n        assertEquals(\"entity2\", entity.getId());\r\n        assertEquals(\"type1\", entity.getType());\r\n        assertEquals(1, entity.getConfigs().size());\r\n        for (String configKey : entity.getConfigs().keySet()) {\r\n            assertTrue(configKey.startsWith(\"config_\"));\r\n        }\r\n        uri = URI.create(\"http://localhost:\" + getServerPort() + \"/ws/v2/\" + \"timeline/clusters/cluster1/apps/application_1111111111_1111/\" + \"entities/type1/entity2?metricstoretrieve=MAP1_,HDFS_\");\r\n        resp = getResponse(client, uri);\r\n        entity = resp.getEntity(TimelineEntity.class);\r\n        assertNotNull(entity);\r\n        assertEquals(\"entity2\", entity.getId());\r\n        assertEquals(\"type1\", entity.getType());\r\n        assertEquals(2, entity.getMetrics().size());\r\n        for (TimelineMetric metric : entity.getMetrics()) {\r\n            assertTrue(metric.getId().startsWith(\"MAP1_\") || metric.getId().startsWith(\"HDFS_\"));\r\n        }\r\n        uri = URI.create(\"http://localhost:\" + getServerPort() + \"/ws/v2/\" + \"timeline/clusters/cluster1/apps/application_1111111111_1111/\" + \"entities/type1/entity2?metricstoretrieve=!(MAP1_,HDFS_)\");\r\n        resp = getResponse(client, uri);\r\n        entity = resp.getEntity(TimelineEntity.class);\r\n        assertNotNull(entity);\r\n        assertEquals(\"entity2\", entity.getId());\r\n        assertEquals(\"type1\", entity.getType());\r\n        assertEquals(1, entity.getMetrics().size());\r\n        for (TimelineMetric metric : entity.getMetrics()) {\r\n            assertTrue(metric.getId().startsWith(\"MAP11_\"));\r\n            assertEquals(TimelineMetric.Type.SINGLE_VALUE, metric.getType());\r\n            assertEquals(1, metric.getValues().size());\r\n        }\r\n        uri = URI.create(\"http://localhost:\" + getServerPort() + \"/ws/v2/\" + \"timeline/clusters/cluster1/apps/application_1111111111_1111/\" + \"entities/type1/entity2?metricstoretrieve=!(MAP1_,HDFS_)&\" + \"metricslimit=5\");\r\n        resp = getResponse(client, uri);\r\n        entity = resp.getEntity(TimelineEntity.class);\r\n        assertNotNull(entity);\r\n        assertEquals(\"entity2\", entity.getId());\r\n        assertEquals(\"type1\", entity.getType());\r\n        assertEquals(1, entity.getMetrics().size());\r\n        for (TimelineMetric metric : entity.getMetrics()) {\r\n            assertTrue(metric.getId().startsWith(\"MAP11_\"));\r\n            assertEquals(TimelineMetric.Type.SINGLE_VALUE, metric.getType());\r\n        }\r\n    } finally {\r\n        client.destroy();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-yarn-project\\hadoop-yarn\\hadoop-yarn-server\\hadoop-yarn-server-timelineservice-hbase-tests\\src\\test\\java\\org\\apache\\hadoop\\yarn\\server\\timelineservice\\reader",
  "methodName" : "testGetFlowRunApps",
  "errType" : null,
  "containingMethodsNum" : 30,
  "sourceCodeText" : "void testGetFlowRunApps() throws Exception\n{\r\n    Client client = createClient();\r\n    try {\r\n        URI uri = URI.create(\"http://localhost:\" + getServerPort() + \"/ws/v2/\" + \"timeline/clusters/cluster1/users/user1/flows/flow_name/runs/\" + \"1002345678919/apps?fields=ALL\");\r\n        ClientResponse resp = getResponse(client, uri);\r\n        Set<TimelineEntity> entities = resp.getEntity(new GenericType<Set<TimelineEntity>>() {\r\n        });\r\n        assertNotNull(entities);\r\n        assertEquals(2, entities.size());\r\n        for (TimelineEntity entity : entities) {\r\n            assertTrue(\"Unexpected app in result\", (entity.getId().equals(\"application_1111111111_1111\") && entity.getMetrics().size() == 3) || (entity.getId().equals(\"application_1111111111_2222\") && entity.getMetrics().size() == 1));\r\n            for (TimelineMetric metric : entity.getMetrics()) {\r\n                assertEquals(TimelineMetric.Type.SINGLE_VALUE, metric.getType());\r\n                assertEquals(1, metric.getValues().size());\r\n            }\r\n        }\r\n        uri = URI.create(\"http://localhost:\" + getServerPort() + \"/ws/v2/\" + \"timeline/clusters/cluster1/users/user1/flows/flow_name/runs/\" + \"1002345678919/apps?fields=ALL&metricslimit=2\");\r\n        resp = getResponse(client, uri);\r\n        entities = resp.getEntity(new GenericType<Set<TimelineEntity>>() {\r\n        });\r\n        assertNotNull(entities);\r\n        assertEquals(2, entities.size());\r\n        for (TimelineEntity entity : entities) {\r\n            assertTrue(\"Unexpected app in result\", (entity.getId().equals(\"application_1111111111_1111\") && entity.getMetrics().size() == 3) || (entity.getId().equals(\"application_1111111111_2222\") && entity.getMetrics().size() == 1));\r\n            for (TimelineMetric metric : entity.getMetrics()) {\r\n                assertTrue(metric.getValues().size() <= 2);\r\n                assertEquals(TimelineMetric.Type.TIME_SERIES, metric.getType());\r\n            }\r\n        }\r\n        uri = URI.create(\"http://localhost:\" + getServerPort() + \"/ws/v2/\" + \"timeline/users/user1/flows/flow_name/runs/1002345678919/apps\");\r\n        resp = getResponse(client, uri);\r\n        entities = resp.getEntity(new GenericType<Set<TimelineEntity>>() {\r\n        });\r\n        assertNotNull(entities);\r\n        assertEquals(2, entities.size());\r\n        uri = URI.create(\"http://localhost:\" + getServerPort() + \"/ws/v2/\" + \"timeline/users/user1/flows/flow_name/runs/1002345678919/\" + \"apps?limit=1\");\r\n        resp = getResponse(client, uri);\r\n        entities = resp.getEntity(new GenericType<Set<TimelineEntity>>() {\r\n        });\r\n        assertNotNull(entities);\r\n        assertEquals(1, entities.size());\r\n    } finally {\r\n        client.destroy();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-yarn-project\\hadoop-yarn\\hadoop-yarn-server\\hadoop-yarn-server-timelineservice-hbase-tests\\src\\test\\java\\org\\apache\\hadoop\\yarn\\server\\timelineservice\\reader",
  "methodName" : "testGetFlowApps",
  "errType" : null,
  "containingMethodsNum" : 53,
  "sourceCodeText" : "void testGetFlowApps() throws Exception\n{\r\n    Client client = createClient();\r\n    try {\r\n        URI uri = URI.create(\"http://localhost:\" + getServerPort() + \"/ws/v2/\" + \"timeline/clusters/cluster1/users/user1/flows/flow_name/apps?\" + \"fields=ALL\");\r\n        ClientResponse resp = getResponse(client, uri);\r\n        Set<TimelineEntity> entities = resp.getEntity(new GenericType<Set<TimelineEntity>>() {\r\n        });\r\n        assertNotNull(entities);\r\n        assertEquals(3, entities.size());\r\n        for (TimelineEntity entity : entities) {\r\n            assertTrue(\"Unexpected app in result\", (entity.getId().equals(\"application_1111111111_1111\") && entity.getConfigs().size() == 1 && entity.getConfigs().equals(ImmutableMap.of(\"cfg2\", \"value1\"))) || (entity.getId().equals(\"application_1111111111_2222\") && entity.getConfigs().size() == 1 && entity.getConfigs().equals(ImmutableMap.of(\"cfg1\", \"value1\"))) || (entity.getId().equals(\"application_1111111111_2224\") && entity.getConfigs().size() == 0));\r\n            for (TimelineMetric metric : entity.getMetrics()) {\r\n                if (entity.getId().equals(\"application_1111111111_1111\")) {\r\n                    TimelineMetric m1 = newMetric(TimelineMetric.Type.SINGLE_VALUE, \"HDFS_BYTES_READ\", ts - 80000, 57L);\r\n                    TimelineMetric m2 = newMetric(TimelineMetric.Type.SINGLE_VALUE, \"MAP_SLOT_MILLIS\", ts - 80000, 40L);\r\n                    TimelineMetric m3 = newMetric(TimelineMetric.Type.SINGLE_VALUE, \"MAP1_SLOT_MILLIS\", ts - 80000, 40L);\r\n                    assertTrue(verifyMetrics(metric, m1, m2, m3));\r\n                } else if (entity.getId().equals(\"application_1111111111_2222\")) {\r\n                    TimelineMetric m1 = newMetric(TimelineMetric.Type.SINGLE_VALUE, \"MAP_SLOT_MILLIS\", ts - 80000, 101L);\r\n                    assertTrue(verifyMetrics(metric, m1));\r\n                } else if (entity.getId().equals(\"application_1111111111_2224\")) {\r\n                    TimelineMetric m1 = newMetric(TimelineMetric.Type.SINGLE_VALUE, \"MAP_SLOT_MILLIS\", ts - 80000, 101L);\r\n                    assertTrue(verifyMetrics(metric, m1));\r\n                }\r\n            }\r\n        }\r\n        uri = URI.create(\"http://localhost:\" + getServerPort() + \"/ws/v2/\" + \"timeline/clusters/cluster1/users/user1/flows/flow_name/apps?\" + \"fields=ALL&metricslimit=6\");\r\n        resp = getResponse(client, uri);\r\n        entities = resp.getEntity(new GenericType<Set<TimelineEntity>>() {\r\n        });\r\n        assertNotNull(entities);\r\n        assertEquals(3, entities.size());\r\n        for (TimelineEntity entity : entities) {\r\n            assertTrue(\"Unexpected app in result\", (entity.getId().equals(\"application_1111111111_1111\") && entity.getConfigs().size() == 1 && entity.getConfigs().equals(ImmutableMap.of(\"cfg2\", \"value1\"))) || (entity.getId().equals(\"application_1111111111_2222\") && entity.getConfigs().size() == 1 && entity.getConfigs().equals(ImmutableMap.of(\"cfg1\", \"value1\"))) || (entity.getId().equals(\"application_1111111111_2224\") && entity.getConfigs().size() == 0));\r\n            for (TimelineMetric metric : entity.getMetrics()) {\r\n                if (entity.getId().equals(\"application_1111111111_1111\")) {\r\n                    TimelineMetric m1 = newMetric(TimelineMetric.Type.TIME_SERIES, \"HDFS_BYTES_READ\", ts - 80000, 57L);\r\n                    m1.addValue(ts - 100000, 31L);\r\n                    TimelineMetric m2 = newMetric(TimelineMetric.Type.TIME_SERIES, \"MAP_SLOT_MILLIS\", ts - 80000, 40L);\r\n                    m2.addValue(ts - 100000, 2L);\r\n                    TimelineMetric m3 = newMetric(TimelineMetric.Type.TIME_SERIES, \"MAP1_SLOT_MILLIS\", ts - 80000, 40L);\r\n                    m3.addValue(ts - 100000, 2L);\r\n                    assertTrue(verifyMetrics(metric, m1, m2, m3));\r\n                } else if (entity.getId().equals(\"application_1111111111_2222\")) {\r\n                    TimelineMetric m1 = newMetric(TimelineMetric.Type.TIME_SERIES, \"MAP_SLOT_MILLIS\", ts - 80000, 101L);\r\n                    m1.addValue(ts - 100000, 5L);\r\n                    assertTrue(verifyMetrics(metric, m1));\r\n                } else if (entity.getId().equals(\"application_1111111111_2224\")) {\r\n                    TimelineMetric m1 = newMetric(TimelineMetric.Type.TIME_SERIES, \"MAP_SLOT_MILLIS\", ts - 80000, 101L);\r\n                    m1.addValue(ts - 100000, 5L);\r\n                    assertTrue(verifyMetrics(metric, m1));\r\n                }\r\n            }\r\n        }\r\n        uri = URI.create(\"http://localhost:\" + getServerPort() + \"/ws/v2/\" + \"timeline/users/user1/flows/flow_name/apps\");\r\n        resp = getResponse(client, uri);\r\n        entities = resp.getEntity(new GenericType<Set<TimelineEntity>>() {\r\n        });\r\n        assertNotNull(entities);\r\n        assertEquals(3, entities.size());\r\n        uri = URI.create(\"http://localhost:\" + getServerPort() + \"/ws/v2/\" + \"timeline/users/user1/flows/flow_name/apps?limit=1\");\r\n        resp = getResponse(client, uri);\r\n        entities = resp.getEntity(new GenericType<Set<TimelineEntity>>() {\r\n        });\r\n        assertNotNull(entities);\r\n        assertEquals(1, entities.size());\r\n    } finally {\r\n        client.destroy();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-yarn-project\\hadoop-yarn\\hadoop-yarn-server\\hadoop-yarn-server-timelineservice-hbase-tests\\src\\test\\java\\org\\apache\\hadoop\\yarn\\server\\timelineservice\\reader",
  "methodName" : "testGetFlowAppsFilters",
  "errType" : null,
  "containingMethodsNum" : 21,
  "sourceCodeText" : "void testGetFlowAppsFilters() throws Exception\n{\r\n    Client client = createClient();\r\n    try {\r\n        String entityType = TimelineEntityType.YARN_APPLICATION.toString();\r\n        URI uri = URI.create(\"http://localhost:\" + getServerPort() + \"/ws/v2/\" + \"timeline/clusters/cluster1/users/user1/flows/flow_name/apps?\" + \"eventfilters=\" + ApplicationMetricsConstants.FINISHED_EVENT_TYPE);\r\n        ClientResponse resp = getResponse(client, uri);\r\n        Set<TimelineEntity> entities = resp.getEntity(new GenericType<Set<TimelineEntity>>() {\r\n        });\r\n        assertNotNull(entities);\r\n        assertEquals(1, entities.size());\r\n        assertTrue(\"Unexpected app in result\", entities.contains(newEntity(entityType, \"application_1111111111_1111\")));\r\n        uri = URI.create(\"http://localhost:\" + getServerPort() + \"/ws/v2/\" + \"timeline/clusters/cluster1/users/user1/flows/flow_name/apps?\" + \"metricfilters=HDFS_BYTES_READ%20ge%200\");\r\n        resp = getResponse(client, uri);\r\n        entities = resp.getEntity(new GenericType<Set<TimelineEntity>>() {\r\n        });\r\n        assertNotNull(entities);\r\n        assertEquals(1, entities.size());\r\n        assertTrue(\"Unexpected app in result\", entities.contains(newEntity(entityType, \"application_1111111111_1111\")));\r\n        uri = URI.create(\"http://localhost:\" + getServerPort() + \"/ws/v2/\" + \"timeline/clusters/cluster1/users/user1/flows/flow_name/apps?\" + \"conffilters=cfg1%20eq%20value1\");\r\n        resp = getResponse(client, uri);\r\n        entities = resp.getEntity(new GenericType<Set<TimelineEntity>>() {\r\n        });\r\n        assertNotNull(entities);\r\n        assertEquals(1, entities.size());\r\n        assertTrue(\"Unexpected app in result\", entities.contains(newEntity(entityType, \"application_1111111111_2222\")));\r\n    } finally {\r\n        client.destroy();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-yarn-project\\hadoop-yarn\\hadoop-yarn-server\\hadoop-yarn-server-timelineservice-hbase-tests\\src\\test\\java\\org\\apache\\hadoop\\yarn\\server\\timelineservice\\reader",
  "methodName" : "testGetFlowRunNotPresent",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void testGetFlowRunNotPresent() throws Exception\n{\r\n    Client client = createClient();\r\n    try {\r\n        URI uri = URI.create(\"http://localhost:\" + getServerPort() + \"/ws/v2/\" + \"timeline/clusters/cluster1/users/user1/flows/flow_name/runs/\" + \"1002345678929\");\r\n        verifyHttpResponse(client, uri, Status.NOT_FOUND);\r\n    } finally {\r\n        client.destroy();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-yarn-project\\hadoop-yarn\\hadoop-yarn-server\\hadoop-yarn-server-timelineservice-hbase-tests\\src\\test\\java\\org\\apache\\hadoop\\yarn\\server\\timelineservice\\reader",
  "methodName" : "testGetFlowsNotPresent",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void testGetFlowsNotPresent() throws Exception\n{\r\n    Client client = createClient();\r\n    try {\r\n        URI uri = URI.create(\"http://localhost:\" + getServerPort() + \"/ws/v2/\" + \"timeline/clusters/cluster2/flows\");\r\n        ClientResponse resp = getResponse(client, uri);\r\n        Set<FlowActivityEntity> entities = resp.getEntity(new GenericType<Set<FlowActivityEntity>>() {\r\n        });\r\n        assertEquals(MediaType.APPLICATION_JSON_TYPE + \"; charset=utf-8\", resp.getType().toString());\r\n        assertNotNull(entities);\r\n        assertEquals(0, entities.size());\r\n    } finally {\r\n        client.destroy();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-yarn-project\\hadoop-yarn\\hadoop-yarn-server\\hadoop-yarn-server-timelineservice-hbase-tests\\src\\test\\java\\org\\apache\\hadoop\\yarn\\server\\timelineservice\\reader",
  "methodName" : "testGetAppNotPresent",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void testGetAppNotPresent() throws Exception\n{\r\n    Client client = createClient();\r\n    try {\r\n        URI uri = URI.create(\"http://localhost:\" + getServerPort() + \"/ws/v2/\" + \"timeline/clusters/cluster1/apps/application_1111111111_1378\");\r\n        verifyHttpResponse(client, uri, Status.NOT_FOUND);\r\n    } finally {\r\n        client.destroy();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-yarn-project\\hadoop-yarn\\hadoop-yarn-server\\hadoop-yarn-server-timelineservice-hbase-tests\\src\\test\\java\\org\\apache\\hadoop\\yarn\\server\\timelineservice\\reader",
  "methodName" : "testGetFlowRunAppsNotPresent",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void testGetFlowRunAppsNotPresent() throws Exception\n{\r\n    Client client = createClient();\r\n    try {\r\n        URI uri = URI.create(\"http://localhost:\" + getServerPort() + \"/ws/v2/\" + \"timeline/clusters/cluster2/users/user1/flows/flow_name/runs/\" + \"1002345678919/apps\");\r\n        ClientResponse resp = getResponse(client, uri);\r\n        Set<TimelineEntity> entities = resp.getEntity(new GenericType<Set<TimelineEntity>>() {\r\n        });\r\n        assertEquals(MediaType.APPLICATION_JSON_TYPE + \"; charset=utf-8\", resp.getType().toString());\r\n        assertNotNull(entities);\r\n        assertEquals(0, entities.size());\r\n    } finally {\r\n        client.destroy();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-yarn-project\\hadoop-yarn\\hadoop-yarn-server\\hadoop-yarn-server-timelineservice-hbase-tests\\src\\test\\java\\org\\apache\\hadoop\\yarn\\server\\timelineservice\\reader",
  "methodName" : "testGetFlowAppsNotPresent",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void testGetFlowAppsNotPresent() throws Exception\n{\r\n    Client client = createClient();\r\n    try {\r\n        URI uri = URI.create(\"http://localhost:\" + getServerPort() + \"/ws/v2/\" + \"timeline/clusters/cluster2/users/user1/flows/flow_name55/apps\");\r\n        ClientResponse resp = getResponse(client, uri);\r\n        Set<TimelineEntity> entities = resp.getEntity(new GenericType<Set<TimelineEntity>>() {\r\n        });\r\n        assertEquals(MediaType.APPLICATION_JSON_TYPE + \"; charset=utf-8\", resp.getType().toString());\r\n        assertNotNull(entities);\r\n        assertEquals(0, entities.size());\r\n    } finally {\r\n        client.destroy();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-yarn-project\\hadoop-yarn\\hadoop-yarn-server\\hadoop-yarn-server-timelineservice-hbase-tests\\src\\test\\java\\org\\apache\\hadoop\\yarn\\server\\timelineservice\\reader",
  "methodName" : "testGenericEntitiesForPagination",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void testGenericEntitiesForPagination() throws Exception\n{\r\n    Client client = createClient();\r\n    try {\r\n        String resourceUri = \"http://localhost:\" + getServerPort() + \"/ws/v2/\" + \"timeline/clusters/cluster1/apps/application_1111111111_1111/\" + \"entities/entitytype\";\r\n        verifyEntitiesForPagination(client, resourceUri);\r\n        resourceUri = \"http://localhost:\" + getServerPort() + \"/ws/v2/\" + \"timeline/clusters/cluster1/users/\" + doAsUser + \"/entities/entitytype\";\r\n        verifyEntitiesForPagination(client, resourceUri);\r\n    } finally {\r\n        client.destroy();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-yarn-project\\hadoop-yarn\\hadoop-yarn-server\\hadoop-yarn-server-timelineservice-hbase-tests\\src\\test\\java\\org\\apache\\hadoop\\yarn\\server\\timelineservice\\reader",
  "methodName" : "verifyEntitiesForPagination",
  "errType" : null,
  "containingMethodsNum" : 23,
  "sourceCodeText" : "void verifyEntitiesForPagination(Client client, String resourceUri) throws Exception\n{\r\n    int limit = 10;\r\n    String queryParam = \"?limit=\" + limit;\r\n    URI uri = URI.create(resourceUri + queryParam);\r\n    ClientResponse resp = getResponse(client, uri);\r\n    List<TimelineEntity> entities = resp.getEntity(new GenericType<List<TimelineEntity>>() {\r\n    });\r\n    verifyPaginatedEntites(entities, limit, limit);\r\n    limit = 4;\r\n    queryParam = \"?limit=\" + limit;\r\n    uri = URI.create(resourceUri + queryParam);\r\n    resp = getResponse(client, uri);\r\n    entities = resp.getEntity(new GenericType<List<TimelineEntity>>() {\r\n    });\r\n    TimelineEntity entity = verifyPaginatedEntites(entities, limit, 10);\r\n    queryParam = \"?limit=\" + limit + \"&fromid=\" + entity.getInfo().get(TimelineReaderUtils.FROMID_KEY);\r\n    uri = URI.create(resourceUri + queryParam);\r\n    resp = getResponse(client, uri);\r\n    entities = resp.getEntity(new GenericType<List<TimelineEntity>>() {\r\n    });\r\n    entity = verifyPaginatedEntites(entities, limit, 7);\r\n    queryParam = \"?limit=\" + limit + \"&fromid=\" + entity.getInfo().get(TimelineReaderUtils.FROMID_KEY);\r\n    uri = URI.create(resourceUri + queryParam);\r\n    resp = getResponse(client, uri);\r\n    entities = resp.getEntity(new GenericType<List<TimelineEntity>>() {\r\n    });\r\n    entity = verifyPaginatedEntites(entities, limit, 4);\r\n    queryParam = \"?limit=\" + limit + \"&fromid=\" + entity.getInfo().get(TimelineReaderUtils.FROMID_KEY);\r\n    uri = URI.create(resourceUri + queryParam);\r\n    resp = getResponse(client, uri);\r\n    entities = resp.getEntity(new GenericType<List<TimelineEntity>>() {\r\n    });\r\n    entity = verifyPaginatedEntites(entities, 1, 1);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-yarn-project\\hadoop-yarn\\hadoop-yarn-server\\hadoop-yarn-server-timelineservice-hbase-tests\\src\\test\\java\\org\\apache\\hadoop\\yarn\\server\\timelineservice\\reader",
  "methodName" : "verifyPaginatedEntites",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "TimelineEntity verifyPaginatedEntites(List<TimelineEntity> entities, int limit, int startFrom)\n{\r\n    assertNotNull(entities);\r\n    assertEquals(limit, entities.size());\r\n    TimelineEntity entity = null;\r\n    for (TimelineEntity timelineEntity : entities) {\r\n        assertEquals(\"entitytype\", timelineEntity.getType());\r\n        assertEquals(\"entityid-\" + startFrom, timelineEntity.getId());\r\n        assertEquals(11 - startFrom--, timelineEntity.getIdPrefix());\r\n        entity = timelineEntity;\r\n    }\r\n    return entity;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-yarn-project\\hadoop-yarn\\hadoop-yarn-server\\hadoop-yarn-server-timelineservice-hbase-tests\\src\\test\\java\\org\\apache\\hadoop\\yarn\\server\\timelineservice\\reader",
  "methodName" : "verifyFlowEntites",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "List<FlowActivityEntity> verifyFlowEntites(Client client, URI uri, int noOfEntities, int[] a, String[] flowsInSequence) throws Exception\n{\r\n    ClientResponse resp = getResponse(client, uri);\r\n    List<FlowActivityEntity> entities = resp.getEntity(new GenericType<List<FlowActivityEntity>>() {\r\n    });\r\n    assertNotNull(entities);\r\n    assertEquals(noOfEntities, entities.size());\r\n    assertEquals(noOfEntities, flowsInSequence.length);\r\n    assertEquals(noOfEntities, a.length);\r\n    int count = 0;\r\n    for (FlowActivityEntity timelineEntity : entities) {\r\n        assertEquals(flowsInSequence[count], timelineEntity.getInfo().get(\"SYSTEM_INFO_FLOW_NAME\"));\r\n        assertEquals(a[count++], timelineEntity.getFlowRuns().size());\r\n    }\r\n    return entities;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-yarn-project\\hadoop-yarn\\hadoop-yarn-server\\hadoop-yarn-server-timelineservice-hbase-tests\\src\\test\\java\\org\\apache\\hadoop\\yarn\\server\\timelineservice\\reader",
  "methodName" : "testForFlowAppsPagination",
  "errType" : null,
  "containingMethodsNum" : 24,
  "sourceCodeText" : "void testForFlowAppsPagination() throws Exception\n{\r\n    Client client = createClient();\r\n    try {\r\n        int totalAppEntities = 15;\r\n        String resourceUri = \"http://localhost:\" + getServerPort() + \"/ws/v2/\" + \"timeline/clusters/cluster1/users/user1/flows/flow1/apps\";\r\n        URI uri = URI.create(resourceUri);\r\n        ClientResponse resp = getResponse(client, uri);\r\n        List<TimelineEntity> entities = resp.getEntity(new GenericType<List<TimelineEntity>>() {\r\n        });\r\n        assertNotNull(entities);\r\n        assertEquals(totalAppEntities, entities.size());\r\n        TimelineEntity entity1 = entities.get(0);\r\n        TimelineEntity entity15 = entities.get(totalAppEntities - 1);\r\n        int limit = 10;\r\n        String queryParam = \"?limit=\" + limit;\r\n        uri = URI.create(resourceUri + queryParam);\r\n        resp = getResponse(client, uri);\r\n        entities = resp.getEntity(new GenericType<List<TimelineEntity>>() {\r\n        });\r\n        assertNotNull(entities);\r\n        assertEquals(limit, entities.size());\r\n        assertEquals(entity1, entities.get(0));\r\n        TimelineEntity entity10 = entities.get(limit - 1);\r\n        uri = URI.create(resourceUri + queryParam + \"&fromid=\" + entity10.getInfo().get(TimelineReaderUtils.FROMID_KEY));\r\n        resp = getResponse(client, uri);\r\n        entities = resp.getEntity(new GenericType<List<TimelineEntity>>() {\r\n        });\r\n        assertNotNull(entities);\r\n        assertEquals(6, entities.size());\r\n        assertEquals(entity10, entities.get(0));\r\n        assertEquals(entity15, entities.get(5));\r\n    } finally {\r\n        client.destroy();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-yarn-project\\hadoop-yarn\\hadoop-yarn-server\\hadoop-yarn-server-timelineservice-hbase-tests\\src\\test\\java\\org\\apache\\hadoop\\yarn\\server\\timelineservice\\reader",
  "methodName" : "testForFlowRunAppsPagination",
  "errType" : null,
  "containingMethodsNum" : 24,
  "sourceCodeText" : "void testForFlowRunAppsPagination() throws Exception\n{\r\n    Client client = createClient();\r\n    try {\r\n        int totalAppEntities = 5;\r\n        String resourceUri = \"http://localhost:\" + getServerPort() + \"/ws/v2/\" + \"timeline/clusters/cluster1/users/user1/flows/flow1/runs/1/apps\";\r\n        URI uri = URI.create(resourceUri);\r\n        ClientResponse resp = getResponse(client, uri);\r\n        List<TimelineEntity> entities = resp.getEntity(new GenericType<List<TimelineEntity>>() {\r\n        });\r\n        assertNotNull(entities);\r\n        assertEquals(totalAppEntities, entities.size());\r\n        TimelineEntity entity1 = entities.get(0);\r\n        TimelineEntity entity5 = entities.get(totalAppEntities - 1);\r\n        int limit = 3;\r\n        String queryParam = \"?limit=\" + limit;\r\n        uri = URI.create(resourceUri + queryParam);\r\n        resp = getResponse(client, uri);\r\n        entities = resp.getEntity(new GenericType<List<TimelineEntity>>() {\r\n        });\r\n        assertNotNull(entities);\r\n        assertEquals(limit, entities.size());\r\n        assertEquals(entity1, entities.get(0));\r\n        TimelineEntity entity3 = entities.get(limit - 1);\r\n        uri = URI.create(resourceUri + queryParam + \"&fromid=\" + entity3.getInfo().get(TimelineReaderUtils.FROMID_KEY));\r\n        resp = getResponse(client, uri);\r\n        entities = resp.getEntity(new GenericType<List<TimelineEntity>>() {\r\n        });\r\n        assertNotNull(entities);\r\n        assertEquals(3, entities.size());\r\n        assertEquals(entity3, entities.get(0));\r\n        assertEquals(entity5, entities.get(2));\r\n    } finally {\r\n        client.destroy();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-yarn-project\\hadoop-yarn\\hadoop-yarn-server\\hadoop-yarn-server-timelineservice-hbase-tests\\src\\test\\java\\org\\apache\\hadoop\\yarn\\server\\timelineservice\\reader",
  "methodName" : "testForFlowRunsPagination",
  "errType" : null,
  "containingMethodsNum" : 30,
  "sourceCodeText" : "void testForFlowRunsPagination() throws Exception\n{\r\n    Client client = createClient();\r\n    try {\r\n        int totalRuns = 3;\r\n        String resourceUri = \"http://localhost:\" + getServerPort() + \"/ws/v2/\" + \"timeline/clusters/cluster1/users/user1/flows/flow1/runs\";\r\n        URI uri = URI.create(resourceUri);\r\n        ClientResponse resp = getResponse(client, uri);\r\n        List<TimelineEntity> entities = resp.getEntity(new GenericType<List<TimelineEntity>>() {\r\n        });\r\n        assertNotNull(entities);\r\n        assertEquals(totalRuns, entities.size());\r\n        TimelineEntity entity1 = entities.get(0);\r\n        TimelineEntity entity3 = entities.get(totalRuns - 1);\r\n        int limit = 2;\r\n        String queryParam = \"?limit=\" + limit;\r\n        uri = URI.create(resourceUri + queryParam);\r\n        resp = getResponse(client, uri);\r\n        entities = resp.getEntity(new GenericType<List<TimelineEntity>>() {\r\n        });\r\n        assertNotNull(entities);\r\n        assertEquals(limit, entities.size());\r\n        assertEquals(entity1, entities.get(0));\r\n        TimelineEntity entity2 = entities.get(limit - 1);\r\n        uri = URI.create(resourceUri + queryParam + \"&fromid=\" + entity2.getInfo().get(TimelineReaderUtils.FROMID_KEY));\r\n        resp = getResponse(client, uri);\r\n        entities = resp.getEntity(new GenericType<List<TimelineEntity>>() {\r\n        });\r\n        assertNotNull(entities);\r\n        assertEquals(limit, entities.size());\r\n        assertEquals(entity2, entities.get(0));\r\n        assertEquals(entity3, entities.get(1));\r\n        uri = URI.create(resourceUri + queryParam + \"&fromid=\" + entity3.getInfo().get(TimelineReaderUtils.FROMID_KEY));\r\n        resp = getResponse(client, uri);\r\n        entities = resp.getEntity(new GenericType<List<TimelineEntity>>() {\r\n        });\r\n        assertNotNull(entities);\r\n        assertEquals(1, entities.size());\r\n        assertEquals(entity3, entities.get(0));\r\n    } finally {\r\n        client.destroy();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-yarn-project\\hadoop-yarn\\hadoop-yarn-server\\hadoop-yarn-server-timelineservice-hbase-tests\\src\\test\\java\\org\\apache\\hadoop\\yarn\\server\\timelineservice\\reader",
  "methodName" : "testGetAppsMetricsRange",
  "errType" : null,
  "containingMethodsNum" : 45,
  "sourceCodeText" : "void testGetAppsMetricsRange() throws Exception\n{\r\n    Client client = createClient();\r\n    try {\r\n        URI uri = URI.create(\"http://localhost:\" + getServerPort() + \"/ws/v2/\" + \"timeline/clusters/cluster1/users/user1/flows/flow_name/runs/\" + \"1002345678919/apps?fields=ALL&metricslimit=100&metricstimestart=\" + (ts - 200000) + \"&metricstimeend=\" + (ts - 100000));\r\n        ClientResponse resp = getResponse(client, uri);\r\n        Set<TimelineEntity> entities = resp.getEntity(new GenericType<Set<TimelineEntity>>() {\r\n        });\r\n        assertNotNull(entities);\r\n        assertEquals(2, entities.size());\r\n        verifyMetricsCount(entities, 4, 4);\r\n        uri = URI.create(\"http://localhost:\" + getServerPort() + \"/ws/v2/\" + \"timeline/clusters/cluster1/users/user1/flows/flow_name/runs/\" + \"1002345678919/apps?fields=ALL&metricslimit=100\");\r\n        resp = getResponse(client, uri);\r\n        entities = resp.getEntity(new GenericType<Set<TimelineEntity>>() {\r\n        });\r\n        assertNotNull(entities);\r\n        assertEquals(2, entities.size());\r\n        verifyMetricsCount(entities, 4, 10);\r\n        uri = URI.create(\"http://localhost:\" + getServerPort() + \"/ws/v2/\" + \"timeline/clusters/cluster1/users/user1/flows/flow_name/\" + \"apps?fields=ALL&metricslimit=100&metricstimestart=\" + (ts - 200000) + \"&metricstimeend=\" + (ts - 100000));\r\n        resp = getResponse(client, uri);\r\n        entities = resp.getEntity(new GenericType<Set<TimelineEntity>>() {\r\n        });\r\n        assertNotNull(entities);\r\n        assertEquals(3, entities.size());\r\n        verifyMetricsCount(entities, 5, 5);\r\n        uri = URI.create(\"http://localhost:\" + getServerPort() + \"/ws/v2/\" + \"timeline/clusters/cluster1/users/user1/flows/flow_name/\" + \"apps?fields=ALL&metricslimit=100\");\r\n        resp = getResponse(client, uri);\r\n        entities = resp.getEntity(new GenericType<Set<TimelineEntity>>() {\r\n        });\r\n        assertNotNull(entities);\r\n        assertEquals(3, entities.size());\r\n        verifyMetricsCount(entities, 5, 12);\r\n        uri = URI.create(\"http://localhost:\" + getServerPort() + \"/ws/v2/\" + \"timeline/clusters/cluster1/users/user1/flows/flow_name/runs/\" + \"1002345678919/apps?fields=ALL&metricslimit=100&metricstimestart=\" + (ts - 200000));\r\n        resp = getResponse(client, uri);\r\n        entities = resp.getEntity(new GenericType<Set<TimelineEntity>>() {\r\n        });\r\n        assertNotNull(entities);\r\n        assertEquals(2, entities.size());\r\n        verifyMetricsCount(entities, 4, 10);\r\n        uri = URI.create(\"http://localhost:\" + getServerPort() + \"/ws/v2/\" + \"timeline/clusters/cluster1/users/user1/flows/flow_name/runs/\" + \"1002345678919/apps?fields=ALL&metricslimit=100&metricstimeend=\" + (ts - 100000));\r\n        resp = getResponse(client, uri);\r\n        entities = resp.getEntity(new GenericType<Set<TimelineEntity>>() {\r\n        });\r\n        assertNotNull(entities);\r\n        assertEquals(2, entities.size());\r\n        verifyMetricsCount(entities, 4, 4);\r\n        uri = URI.create(\"http://localhost:\" + getServerPort() + \"/ws/v2/\" + \"timeline/apps/application_1111111111_1111?userid=user1&fields=ALL\" + \"&flowname=flow_name&flowrunid=1002345678919&metricslimit=100\" + \"&metricstimestart=\" + (ts - 200000) + \"&metricstimeend=\" + (ts - 100000));\r\n        resp = getResponse(client, uri);\r\n        TimelineEntity entity = resp.getEntity(TimelineEntity.class);\r\n        assertNotNull(entity);\r\n        verifyMetricCount(entity, 3, 3);\r\n        uri = URI.create(\"http://localhost:\" + getServerPort() + \"/ws/v2/\" + \"timeline/clusters/cluster1/users/user1/flows/flow_name/\" + \"apps?fields=ALL&metricslimit=100&metricstimestart=\" + (ts - 100000) + \"&metricstimeend=\" + (ts - 200000));\r\n        verifyHttpResponse(client, uri, Status.BAD_REQUEST);\r\n    } finally {\r\n        client.destroy();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-yarn-project\\hadoop-yarn\\hadoop-yarn-server\\hadoop-yarn-server-timelineservice-hbase-tests\\src\\test\\java\\org\\apache\\hadoop\\yarn\\server\\timelineservice\\storage\\flow",
  "methodName" : "setupBeforeClass",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void setupBeforeClass() throws Exception\n{\r\n    util = new HBaseTestingUtility();\r\n    Configuration conf = util.getConfiguration();\r\n    conf.setInt(\"hfile.format.version\", 3);\r\n    util.startMiniCluster();\r\n    DataGeneratorForTest.createSchema(util.getConfiguration());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-yarn-project\\hadoop-yarn\\hadoop-yarn-server\\hadoop-yarn-server-timelineservice-hbase-tests\\src\\test\\java\\org\\apache\\hadoop\\yarn\\server\\timelineservice\\storage\\flow",
  "methodName" : "testWriteNonNumericData",
  "errType" : null,
  "containingMethodsNum" : 14,
  "sourceCodeText" : "void testWriteNonNumericData() throws Exception\n{\r\n    String rowKey = \"nonNumericRowKey\";\r\n    String column = \"nonNumericColumnName\";\r\n    String value = \"nonNumericValue\";\r\n    byte[] rowKeyBytes = Bytes.toBytes(rowKey);\r\n    byte[] columnNameBytes = Bytes.toBytes(column);\r\n    byte[] valueBytes = Bytes.toBytes(value);\r\n    Put p = new Put(rowKeyBytes);\r\n    p.addColumn(FlowRunColumnFamily.INFO.getBytes(), columnNameBytes, valueBytes);\r\n    Configuration hbaseConf = util.getConfiguration();\r\n    Connection conn = null;\r\n    conn = ConnectionFactory.createConnection(hbaseConf);\r\n    Table flowRunTable = conn.getTable(BaseTableRW.getTableName(hbaseConf, FlowRunTableRW.TABLE_NAME_CONF_NAME, FlowRunTableRW.DEFAULT_TABLE_NAME));\r\n    flowRunTable.put(p);\r\n    Get g = new Get(rowKeyBytes);\r\n    Result r = flowRunTable.get(g);\r\n    assertNotNull(r);\r\n    assertTrue(r.size() >= 1);\r\n    Cell actualValue = r.getColumnLatestCell(FlowRunColumnFamily.INFO.getBytes(), columnNameBytes);\r\n    assertNotNull(CellUtil.cloneValue(actualValue));\r\n    assertThat(Bytes.toString(CellUtil.cloneValue(actualValue))).isEqualTo(value);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-yarn-project\\hadoop-yarn\\hadoop-yarn-server\\hadoop-yarn-server-timelineservice-hbase-tests\\src\\test\\java\\org\\apache\\hadoop\\yarn\\server\\timelineservice\\storage\\flow",
  "methodName" : "testWriteScanBatchLimit",
  "errType" : null,
  "containingMethodsNum" : 68,
  "sourceCodeText" : "void testWriteScanBatchLimit() throws Exception\n{\r\n    String rowKey = \"nonNumericRowKey\";\r\n    String column = \"nonNumericColumnName\";\r\n    String value = \"nonNumericValue\";\r\n    String column2 = \"nonNumericColumnName2\";\r\n    String value2 = \"nonNumericValue2\";\r\n    String column3 = \"nonNumericColumnName3\";\r\n    String value3 = \"nonNumericValue3\";\r\n    String column4 = \"nonNumericColumnName4\";\r\n    String value4 = \"nonNumericValue4\";\r\n    byte[] rowKeyBytes = Bytes.toBytes(rowKey);\r\n    byte[] columnNameBytes = Bytes.toBytes(column);\r\n    byte[] valueBytes = Bytes.toBytes(value);\r\n    byte[] columnName2Bytes = Bytes.toBytes(column2);\r\n    byte[] value2Bytes = Bytes.toBytes(value2);\r\n    byte[] columnName3Bytes = Bytes.toBytes(column3);\r\n    byte[] value3Bytes = Bytes.toBytes(value3);\r\n    byte[] columnName4Bytes = Bytes.toBytes(column4);\r\n    byte[] value4Bytes = Bytes.toBytes(value4);\r\n    Put p = new Put(rowKeyBytes);\r\n    p.addColumn(FlowRunColumnFamily.INFO.getBytes(), columnNameBytes, valueBytes);\r\n    p.addColumn(FlowRunColumnFamily.INFO.getBytes(), columnName2Bytes, value2Bytes);\r\n    p.addColumn(FlowRunColumnFamily.INFO.getBytes(), columnName3Bytes, value3Bytes);\r\n    p.addColumn(FlowRunColumnFamily.INFO.getBytes(), columnName4Bytes, value4Bytes);\r\n    Configuration hbaseConf = util.getConfiguration();\r\n    Connection conn = null;\r\n    conn = ConnectionFactory.createConnection(hbaseConf);\r\n    Table flowRunTable = conn.getTable(BaseTableRW.getTableName(hbaseConf, FlowRunTableRW.TABLE_NAME_CONF_NAME, FlowRunTableRW.DEFAULT_TABLE_NAME));\r\n    flowRunTable.put(p);\r\n    String rowKey2 = \"nonNumericRowKey2\";\r\n    byte[] rowKey2Bytes = Bytes.toBytes(rowKey2);\r\n    p = new Put(rowKey2Bytes);\r\n    p.addColumn(FlowRunColumnFamily.INFO.getBytes(), columnNameBytes, valueBytes);\r\n    p.addColumn(FlowRunColumnFamily.INFO.getBytes(), columnName2Bytes, value2Bytes);\r\n    p.addColumn(FlowRunColumnFamily.INFO.getBytes(), columnName3Bytes, value3Bytes);\r\n    p.addColumn(FlowRunColumnFamily.INFO.getBytes(), columnName4Bytes, value4Bytes);\r\n    flowRunTable.put(p);\r\n    String rowKey3 = \"nonNumericRowKey3\";\r\n    byte[] rowKey3Bytes = Bytes.toBytes(rowKey3);\r\n    p = new Put(rowKey3Bytes);\r\n    p.addColumn(FlowRunColumnFamily.INFO.getBytes(), columnNameBytes, valueBytes);\r\n    p.addColumn(FlowRunColumnFamily.INFO.getBytes(), columnName2Bytes, value2Bytes);\r\n    p.addColumn(FlowRunColumnFamily.INFO.getBytes(), columnName3Bytes, value3Bytes);\r\n    p.addColumn(FlowRunColumnFamily.INFO.getBytes(), columnName4Bytes, value4Bytes);\r\n    flowRunTable.put(p);\r\n    Scan s = new Scan();\r\n    s.addFamily(FlowRunColumnFamily.INFO.getBytes());\r\n    s.setStartRow(rowKeyBytes);\r\n    int batchLimit = 2;\r\n    s.setBatch(batchLimit);\r\n    ResultScanner scanner = flowRunTable.getScanner(s);\r\n    for (Result result : scanner) {\r\n        assertNotNull(result);\r\n        assertTrue(!result.isEmpty());\r\n        assertTrue(result.rawCells().length <= batchLimit);\r\n        Map<byte[], byte[]> values = result.getFamilyMap(FlowRunColumnFamily.INFO.getBytes());\r\n        assertTrue(values.size() <= batchLimit);\r\n    }\r\n    s = new Scan();\r\n    s.addFamily(FlowRunColumnFamily.INFO.getBytes());\r\n    s.setStartRow(rowKeyBytes);\r\n    batchLimit = 3;\r\n    s.setBatch(batchLimit);\r\n    scanner = flowRunTable.getScanner(s);\r\n    for (Result result : scanner) {\r\n        assertNotNull(result);\r\n        assertTrue(!result.isEmpty());\r\n        assertTrue(result.rawCells().length <= batchLimit);\r\n        Map<byte[], byte[]> values = result.getFamilyMap(FlowRunColumnFamily.INFO.getBytes());\r\n        assertTrue(values.size() <= batchLimit);\r\n    }\r\n    s = new Scan();\r\n    s.addFamily(FlowRunColumnFamily.INFO.getBytes());\r\n    s.setStartRow(rowKeyBytes);\r\n    batchLimit = 1000;\r\n    s.setBatch(batchLimit);\r\n    scanner = flowRunTable.getScanner(s);\r\n    int rowCount = 0;\r\n    for (Result result : scanner) {\r\n        assertNotNull(result);\r\n        assertTrue(!result.isEmpty());\r\n        assertTrue(result.rawCells().length <= batchLimit);\r\n        Map<byte[], byte[]> values = result.getFamilyMap(FlowRunColumnFamily.INFO.getBytes());\r\n        assertTrue(values.size() <= batchLimit);\r\n        assertEquals(4, values.size());\r\n        rowCount++;\r\n    }\r\n    assertEquals(3, rowCount);\r\n    s = new Scan();\r\n    s.addFamily(FlowRunColumnFamily.INFO.getBytes());\r\n    s.setStartRow(rowKeyBytes);\r\n    batchLimit = -2992;\r\n    s.setBatch(batchLimit);\r\n    scanner = flowRunTable.getScanner(s);\r\n    rowCount = 0;\r\n    for (Result result : scanner) {\r\n        assertNotNull(result);\r\n        assertTrue(!result.isEmpty());\r\n        assertEquals(4, result.rawCells().length);\r\n        Map<byte[], byte[]> values = result.getFamilyMap(FlowRunColumnFamily.INFO.getBytes());\r\n        assertEquals(4, values.size());\r\n        rowCount++;\r\n    }\r\n    assertEquals(3, rowCount);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-yarn-project\\hadoop-yarn\\hadoop-yarn-server\\hadoop-yarn-server-timelineservice-hbase-tests\\src\\test\\java\\org\\apache\\hadoop\\yarn\\server\\timelineservice\\storage\\flow",
  "methodName" : "testWriteFlowRunCompaction",
  "errType" : null,
  "containingMethodsNum" : 20,
  "sourceCodeText" : "void testWriteFlowRunCompaction() throws Exception\n{\r\n    String cluster = \"kompaction_cluster1\";\r\n    String user = \"kompaction_FlowRun__user1\";\r\n    String flow = \"kompaction_flowRun_flow_name\";\r\n    String flowVersion = \"AF1021C19F1351\";\r\n    long runid = 1449526652000L;\r\n    int start = 10;\r\n    int count = 2000;\r\n    int appIdSuffix = 1;\r\n    HBaseTimelineWriterImpl hbi = null;\r\n    long insertTs = System.currentTimeMillis() - count;\r\n    Configuration c1 = util.getConfiguration();\r\n    TimelineEntities te1 = null;\r\n    TimelineEntity entityApp1 = null;\r\n    UserGroupInformation remoteUser = UserGroupInformation.createRemoteUser(user);\r\n    try {\r\n        hbi = new HBaseTimelineWriterImpl();\r\n        hbi.init(c1);\r\n        for (int i = start; i < start + count; i++) {\r\n            String appName = \"application_10240000000000_\" + appIdSuffix;\r\n            insertTs++;\r\n            te1 = new TimelineEntities();\r\n            entityApp1 = TestFlowDataGenerator.getEntityMetricsApp1(insertTs, c1);\r\n            te1.addEntity(entityApp1);\r\n            hbi.write(new TimelineCollectorContext(cluster, user, flow, flowVersion, runid, appName), te1, remoteUser);\r\n            appName = \"application_2048000000000_7\" + appIdSuffix;\r\n            insertTs++;\r\n            te1 = new TimelineEntities();\r\n            entityApp1 = TestFlowDataGenerator.getEntityMetricsApp2(insertTs);\r\n            te1.addEntity(entityApp1);\r\n            hbi.write(new TimelineCollectorContext(cluster, user, flow, flowVersion, runid, appName), te1, remoteUser);\r\n        }\r\n    } finally {\r\n        String appName = \"application_10240000000000_\" + appIdSuffix;\r\n        te1 = new TimelineEntities();\r\n        entityApp1 = TestFlowDataGenerator.getEntityMetricsApp1Complete(insertTs + 1, c1);\r\n        te1.addEntity(entityApp1);\r\n        if (hbi != null) {\r\n            hbi.write(new TimelineCollectorContext(cluster, user, flow, flowVersion, runid, appName), te1, remoteUser);\r\n            hbi.flush();\r\n            hbi.close();\r\n        }\r\n    }\r\n    TableName flowRunTable = BaseTableRW.getTableName(c1, FlowRunTableRW.TABLE_NAME_CONF_NAME, FlowRunTableRW.DEFAULT_TABLE_NAME);\r\n    HRegionServer server = util.getRSForFirstRegionInTable(flowRunTable);\r\n    int regionNum = HBaseTimelineServerUtils.flushCompactTableRegions(server, flowRunTable);\r\n    assertTrue(\"Didn't find any regions for primary table!\", regionNum > 0);\r\n    checkFlowRunTable(cluster, user, flow, runid, c1, 4);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-yarn-project\\hadoop-yarn\\hadoop-yarn-server\\hadoop-yarn-server-timelineservice-hbase-tests\\src\\test\\java\\org\\apache\\hadoop\\yarn\\server\\timelineservice\\storage\\flow",
  "methodName" : "checkFlowRunTable",
  "errType" : null,
  "containingMethodsNum" : 19,
  "sourceCodeText" : "void checkFlowRunTable(String cluster, String user, String flow, long runid, Configuration c1, int valueCount) throws IOException\n{\r\n    Scan s = new Scan();\r\n    s.addFamily(FlowRunColumnFamily.INFO.getBytes());\r\n    byte[] startRow = new FlowRunRowKey(cluster, user, flow, runid).getRowKey();\r\n    s.setStartRow(startRow);\r\n    String clusterStop = cluster + \"1\";\r\n    byte[] stopRow = new FlowRunRowKey(clusterStop, user, flow, runid).getRowKey();\r\n    s.setStopRow(stopRow);\r\n    Connection conn = ConnectionFactory.createConnection(c1);\r\n    Table table1 = conn.getTable(BaseTableRW.getTableName(c1, FlowRunTableRW.TABLE_NAME_CONF_NAME, FlowRunTableRW.DEFAULT_TABLE_NAME));\r\n    ResultScanner scanner = table1.getScanner(s);\r\n    int rowCount = 0;\r\n    for (Result result : scanner) {\r\n        assertNotNull(result);\r\n        assertTrue(!result.isEmpty());\r\n        Map<byte[], byte[]> values = result.getFamilyMap(FlowRunColumnFamily.INFO.getBytes());\r\n        assertEquals(valueCount, values.size());\r\n        rowCount++;\r\n        byte[] q = ColumnHelper.getColumnQualifier(FlowRunColumnPrefix.METRIC.getColumnPrefixBytes(), METRIC1);\r\n        assertTrue(values.containsKey(q));\r\n        assertEquals(141, Bytes.toLong(values.get(q)));\r\n        q = ColumnHelper.getColumnQualifier(FlowRunColumnPrefix.METRIC.getColumnPrefixBytes(), METRIC2);\r\n        assertTrue(values.containsKey(q));\r\n        assertEquals(57, Bytes.toLong(values.get(q)));\r\n    }\r\n    assertEquals(1, rowCount);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-yarn-project\\hadoop-yarn\\hadoop-yarn-server\\hadoop-yarn-server-timelineservice-hbase-tests\\src\\test\\java\\org\\apache\\hadoop\\yarn\\server\\timelineservice\\storage\\flow",
  "methodName" : "getFlowScannerForTestingCompaction",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "FlowScanner getFlowScannerForTestingCompaction()\n{\r\n    FlowScanner fs = new FlowScanner(null, null, FlowScannerOperation.MAJOR_COMPACTION);\r\n    assertNotNull(fs);\r\n    return fs;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-yarn-project\\hadoop-yarn\\hadoop-yarn-server\\hadoop-yarn-server-timelineservice-hbase-tests\\src\\test\\java\\org\\apache\\hadoop\\yarn\\server\\timelineservice\\storage\\flow",
  "methodName" : "checkProcessSummationMoreCellsSumFinal2",
  "errType" : null,
  "containingMethodsNum" : 38,
  "sourceCodeText" : "void checkProcessSummationMoreCellsSumFinal2() throws IOException\n{\r\n    long cellValue1 = 1236L;\r\n    long cellValue2 = 28L;\r\n    long cellValue3 = 1236L;\r\n    long cellValue4 = 1236L;\r\n    FlowScanner fs = getFlowScannerForTestingCompaction();\r\n    long currentTimestamp = System.currentTimeMillis();\r\n    long cell1Ts = 1200120L;\r\n    long cell2Ts = TimestampGenerator.getSupplementedTimestamp(System.currentTimeMillis(), \"application_123746661110_11202\");\r\n    long cell3Ts = 1277719L;\r\n    long cell4Ts = currentTimestamp - 10;\r\n    SortedSet<Cell> currentColumnCells = new TreeSet<Cell>(KeyValue.COMPARATOR);\r\n    List<Tag> tags = new ArrayList<>();\r\n    Tag t = HBaseTimelineServerUtils.createTag(AggregationOperation.SUM_FINAL.getTagType(), \"application_1234588888_91188\");\r\n    tags.add(t);\r\n    byte[] tagByteArray = HBaseTimelineServerUtils.convertTagListToByteArray(tags);\r\n    Cell c1 = HBaseTimelineServerUtils.createNewCell(aRowKey, aFamily, aQualifier, cell1Ts, Bytes.toBytes(cellValue1), tagByteArray);\r\n    currentColumnCells.add(c1);\r\n    tags = new ArrayList<>();\r\n    t = HBaseTimelineServerUtils.createTag(AggregationOperation.SUM_FINAL.getTagType(), \"application_12700000001_29102\");\r\n    tags.add(t);\r\n    tagByteArray = HBaseTimelineServerUtils.convertTagListToByteArray(tags);\r\n    Cell c2 = HBaseTimelineServerUtils.createNewCell(aRowKey, aFamily, aQualifier, cell2Ts, Bytes.toBytes(cellValue2), tagByteArray);\r\n    currentColumnCells.add(c2);\r\n    tags = new ArrayList<>();\r\n    t = HBaseTimelineServerUtils.createTag(AggregationOperation.SUM.getTagType(), \"application_191780000000001_8195\");\r\n    tags.add(t);\r\n    tagByteArray = HBaseTimelineServerUtils.convertTagListToByteArray(tags);\r\n    Cell c3 = HBaseTimelineServerUtils.createNewCell(aRowKey, aFamily, aQualifier, cell3Ts, Bytes.toBytes(cellValue3), tagByteArray);\r\n    currentColumnCells.add(c3);\r\n    tags = new ArrayList<>();\r\n    t = HBaseTimelineServerUtils.createTag(AggregationOperation.SUM.getTagType(), \"application_191780000000001_98104\");\r\n    tags.add(t);\r\n    tagByteArray = HBaseTimelineServerUtils.convertTagListToByteArray(tags);\r\n    Cell c4 = HBaseTimelineServerUtils.createNewCell(aRowKey, aFamily, aQualifier, cell4Ts, Bytes.toBytes(cellValue4), tagByteArray);\r\n    currentColumnCells.add(c4);\r\n    List<Cell> cells = fs.processSummationMajorCompaction(currentColumnCells, new LongConverter(), currentTimestamp);\r\n    assertNotNull(cells);\r\n    assertEquals(4, cells.size());\r\n    for (int i = 0; i < cells.size(); i++) {\r\n        Cell returnedCell = cells.get(0);\r\n        assertNotNull(returnedCell);\r\n        long returnTs = returnedCell.getTimestamp();\r\n        long returnValue = Bytes.toLong(CellUtil.cloneValue(returnedCell));\r\n        if (returnValue == cellValue2) {\r\n            assertTrue(returnTs == cell2Ts);\r\n        } else if (returnValue == cellValue3) {\r\n            assertTrue(returnTs == cell3Ts);\r\n        } else if (returnValue == cellValue4) {\r\n            assertTrue(returnTs == cell4Ts);\r\n        } else if (returnValue == cellValue1) {\r\n            assertTrue(returnTs != cell1Ts);\r\n            assertTrue(returnTs > cell1Ts);\r\n            assertTrue(returnTs >= currentTimestamp);\r\n        } else {\r\n            Assert.fail();\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-yarn-project\\hadoop-yarn\\hadoop-yarn-server\\hadoop-yarn-server-timelineservice-hbase-tests\\src\\test\\java\\org\\apache\\hadoop\\yarn\\server\\timelineservice\\storage\\flow",
  "methodName" : "checkProcessSummationMoreCellsSumFinalMany",
  "errType" : null,
  "containingMethodsNum" : 25,
  "sourceCodeText" : "void checkProcessSummationMoreCellsSumFinalMany() throws IOException\n{\r\n    FlowScanner fs = getFlowScannerForTestingCompaction();\r\n    int count = 200000;\r\n    long cellValueFinal = 1000L;\r\n    long cellValueNotFinal = 28L;\r\n    long currentTimestamp = System.currentTimeMillis();\r\n    long cellTsFinalStart = 10001120L;\r\n    long cellTsFinal = cellTsFinalStart;\r\n    long cellTsNotFinalStart = currentTimestamp - 5;\r\n    long cellTsNotFinal = cellTsNotFinalStart;\r\n    SortedSet<Cell> currentColumnCells = new TreeSet<Cell>(KeyValue.COMPARATOR);\r\n    List<Tag> tags = null;\r\n    Tag t = null;\r\n    Cell c1 = null;\r\n    for (int i = 0; i < count; i++) {\r\n        tags = new ArrayList<>();\r\n        t = HBaseTimelineServerUtils.createTag(AggregationOperation.SUM_FINAL.getTagType(), \"application_123450000\" + i + \"01_19\" + i);\r\n        tags.add(t);\r\n        byte[] tagByteArray = HBaseTimelineServerUtils.convertTagListToByteArray(tags);\r\n        c1 = HBaseTimelineServerUtils.createNewCell(aRowKey, aFamily, aQualifier, cellTsFinal, Bytes.toBytes(cellValueFinal), tagByteArray);\r\n        currentColumnCells.add(c1);\r\n        cellTsFinal++;\r\n    }\r\n    for (int i = 0; i < count; i++) {\r\n        tags = new ArrayList<>();\r\n        t = HBaseTimelineServerUtils.createTag(AggregationOperation.SUM.getTagType(), \"application_1987650000\" + i + \"83_911\" + i);\r\n        tags.add(t);\r\n        byte[] tagByteArray = HBaseTimelineServerUtils.convertTagListToByteArray(tags);\r\n        c1 = HBaseTimelineServerUtils.createNewCell(aRowKey, aFamily, aQualifier, cellTsNotFinal, Bytes.toBytes(cellValueNotFinal), tagByteArray);\r\n        currentColumnCells.add(c1);\r\n        cellTsNotFinal++;\r\n    }\r\n    List<Cell> cells = fs.processSummationMajorCompaction(currentColumnCells, new LongConverter(), currentTimestamp);\r\n    assertNotNull(cells);\r\n    assertEquals(count + 1, cells.size());\r\n    for (int i = 0; i < cells.size(); i++) {\r\n        Cell returnedCell = cells.get(0);\r\n        assertNotNull(returnedCell);\r\n        long returnTs = returnedCell.getTimestamp();\r\n        long returnValue = Bytes.toLong(CellUtil.cloneValue(returnedCell));\r\n        if (returnValue == (count * cellValueFinal)) {\r\n            assertTrue(returnTs > (cellTsFinalStart + count));\r\n            assertTrue(returnTs >= currentTimestamp);\r\n        } else if ((returnValue >= cellValueNotFinal) && (returnValue <= cellValueNotFinal * count)) {\r\n            assertTrue(returnTs >= cellTsNotFinalStart);\r\n            assertTrue(returnTs <= cellTsNotFinalStart * count);\r\n        } else {\r\n            Assert.fail();\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-yarn-project\\hadoop-yarn\\hadoop-yarn-server\\hadoop-yarn-server-timelineservice-hbase-tests\\src\\test\\java\\org\\apache\\hadoop\\yarn\\server\\timelineservice\\storage\\flow",
  "methodName" : "checkProcessSummationMoreCellsSumFinalVariedTags",
  "errType" : null,
  "containingMethodsNum" : 33,
  "sourceCodeText" : "void checkProcessSummationMoreCellsSumFinalVariedTags() throws IOException\n{\r\n    FlowScanner fs = getFlowScannerForTestingCompaction();\r\n    int countFinal = 20100;\r\n    int countNotFinal = 1000;\r\n    int countFinalNotExpire = 7009;\r\n    long cellValueFinal = 1000L;\r\n    long cellValueNotFinal = 28L;\r\n    long currentTimestamp = System.currentTimeMillis();\r\n    long cellTsFinalStart = 10001120L;\r\n    long cellTsFinal = cellTsFinalStart;\r\n    long cellTsFinalStartNotExpire = TimestampGenerator.getSupplementedTimestamp(System.currentTimeMillis(), \"application_10266666661166_118821\");\r\n    long cellTsFinalNotExpire = cellTsFinalStartNotExpire;\r\n    long cellTsNotFinalStart = currentTimestamp - 5;\r\n    long cellTsNotFinal = cellTsNotFinalStart;\r\n    SortedSet<Cell> currentColumnCells = new TreeSet<Cell>(KeyValue.COMPARATOR);\r\n    List<Tag> tags = null;\r\n    Tag t = null;\r\n    Cell c1 = null;\r\n    for (int i = 0; i < countFinal; i++) {\r\n        tags = new ArrayList<>();\r\n        t = HBaseTimelineServerUtils.createTag(AggregationOperation.SUM_FINAL.getTagType(), \"application_123450000\" + i + \"01_19\" + i);\r\n        tags.add(t);\r\n        byte[] tagByteArray = HBaseTimelineServerUtils.convertTagListToByteArray(tags);\r\n        c1 = HBaseTimelineServerUtils.createNewCell(aRowKey, aFamily, aQualifier, cellTsFinal, Bytes.toBytes(cellValueFinal), tagByteArray);\r\n        currentColumnCells.add(c1);\r\n        cellTsFinal++;\r\n    }\r\n    for (int i = 0; i < countFinalNotExpire; i++) {\r\n        tags = new ArrayList<>();\r\n        t = HBaseTimelineServerUtils.createTag(AggregationOperation.SUM_FINAL.getTagType(), \"application_123450000\" + i + \"01_19\" + i);\r\n        tags.add(t);\r\n        byte[] tagByteArray = HBaseTimelineServerUtils.convertTagListToByteArray(tags);\r\n        c1 = HBaseTimelineServerUtils.createNewCell(aRowKey, aFamily, aQualifier, cellTsFinalNotExpire, Bytes.toBytes(cellValueFinal), tagByteArray);\r\n        currentColumnCells.add(c1);\r\n        cellTsFinalNotExpire++;\r\n    }\r\n    for (int i = 0; i < countNotFinal; i++) {\r\n        tags = new ArrayList<>();\r\n        t = HBaseTimelineServerUtils.createTag(AggregationOperation.SUM.getTagType(), \"application_1987650000\" + i + \"83_911\" + i);\r\n        tags.add(t);\r\n        byte[] tagByteArray = HBaseTimelineServerUtils.convertTagListToByteArray(tags);\r\n        c1 = HBaseTimelineServerUtils.createNewCell(aRowKey, aFamily, aQualifier, cellTsNotFinal, Bytes.toBytes(cellValueNotFinal), tagByteArray);\r\n        currentColumnCells.add(c1);\r\n        cellTsNotFinal++;\r\n    }\r\n    List<Cell> cells = fs.processSummationMajorCompaction(currentColumnCells, new LongConverter(), currentTimestamp);\r\n    assertNotNull(cells);\r\n    assertEquals(countFinalNotExpire + countNotFinal + 1, cells.size());\r\n    for (int i = 0; i < cells.size(); i++) {\r\n        Cell returnedCell = cells.get(0);\r\n        assertNotNull(returnedCell);\r\n        long returnTs = returnedCell.getTimestamp();\r\n        long returnValue = Bytes.toLong(CellUtil.cloneValue(returnedCell));\r\n        if (returnValue == (countFinal * cellValueFinal)) {\r\n            assertTrue(returnTs > (cellTsFinalStart + countFinal));\r\n            assertTrue(returnTs >= currentTimestamp);\r\n        } else if (returnValue == cellValueNotFinal) {\r\n            assertTrue(returnTs >= cellTsNotFinalStart);\r\n            assertTrue(returnTs <= cellTsNotFinalStart + countNotFinal);\r\n        } else if (returnValue == cellValueFinal) {\r\n            assertTrue(returnTs >= cellTsFinalStartNotExpire);\r\n            assertTrue(returnTs <= cellTsFinalStartNotExpire + countFinalNotExpire);\r\n        } else {\r\n            Assert.fail();\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-yarn-project\\hadoop-yarn\\hadoop-yarn-server\\hadoop-yarn-server-timelineservice-hbase-tests\\src\\test\\java\\org\\apache\\hadoop\\yarn\\server\\timelineservice\\storage\\flow",
  "methodName" : "testProcessSummationMoreCellsSumFinal",
  "errType" : null,
  "containingMethodsNum" : 25,
  "sourceCodeText" : "void testProcessSummationMoreCellsSumFinal() throws IOException\n{\r\n    FlowScanner fs = getFlowScannerForTestingCompaction();\r\n    long currentTimestamp = System.currentTimeMillis();\r\n    long cellValue1 = 1236L;\r\n    long cellValue2 = 28L;\r\n    List<Tag> tags = new ArrayList<>();\r\n    Tag t = HBaseTimelineServerUtils.createTag(AggregationOperation.SUM_FINAL.getTagType(), \"application_1234588888_999888\");\r\n    tags.add(t);\r\n    byte[] tagByteArray = HBaseTimelineServerUtils.convertTagListToByteArray(tags);\r\n    SortedSet<Cell> currentColumnCells = new TreeSet<Cell>(KeyValue.COMPARATOR);\r\n    Cell c1 = HBaseTimelineServerUtils.createNewCell(aRowKey, aFamily, aQualifier, 120L, Bytes.toBytes(cellValue1), tagByteArray);\r\n    currentColumnCells.add(c1);\r\n    tags = new ArrayList<>();\r\n    t = HBaseTimelineServerUtils.createTag(AggregationOperation.SUM.getTagType(), \"application_100000000001_119101\");\r\n    tags.add(t);\r\n    tagByteArray = HBaseTimelineServerUtils.convertTagListToByteArray(tags);\r\n    Cell c2 = HBaseTimelineServerUtils.createNewCell(aRowKey, aFamily, aQualifier, 130L, Bytes.toBytes(cellValue2), tagByteArray);\r\n    currentColumnCells.add(c2);\r\n    List<Cell> cells = fs.processSummationMajorCompaction(currentColumnCells, new LongConverter(), currentTimestamp);\r\n    assertNotNull(cells);\r\n    assertEquals(2, cells.size());\r\n    Cell returnedCell = cells.get(0);\r\n    assertNotNull(returnedCell);\r\n    long inputTs1 = c1.getTimestamp();\r\n    long inputTs2 = c2.getTimestamp();\r\n    long returnTs = returnedCell.getTimestamp();\r\n    long returnValue = Bytes.toLong(CellUtil.cloneValue(returnedCell));\r\n    if (returnValue == cellValue2) {\r\n        assertTrue(returnTs == inputTs2);\r\n    } else if (returnValue == cellValue1) {\r\n        assertTrue(returnTs >= currentTimestamp);\r\n        assertTrue(returnTs != inputTs1);\r\n    } else {\r\n        Assert.fail();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-yarn-project\\hadoop-yarn\\hadoop-yarn-server\\hadoop-yarn-server-timelineservice-hbase-tests\\src\\test\\java\\org\\apache\\hadoop\\yarn\\server\\timelineservice\\storage\\flow",
  "methodName" : "testProcessSummationOneCellSumFinal",
  "errType" : null,
  "containingMethodsNum" : 16,
  "sourceCodeText" : "void testProcessSummationOneCellSumFinal() throws IOException\n{\r\n    FlowScanner fs = getFlowScannerForTestingCompaction();\r\n    long currentTimestamp = System.currentTimeMillis();\r\n    List<Tag> tags = new ArrayList<>();\r\n    Tag t = HBaseTimelineServerUtils.createTag(AggregationOperation.SUM_FINAL.getTagType(), \"application_123458888888_999888\");\r\n    tags.add(t);\r\n    byte[] tagByteArray = HBaseTimelineServerUtils.convertTagListToByteArray(tags);\r\n    SortedSet<Cell> currentColumnCells = new TreeSet<Cell>(KeyValue.COMPARATOR);\r\n    Cell c1 = HBaseTimelineServerUtils.createNewCell(aRowKey, aFamily, aQualifier, 120L, Bytes.toBytes(1110L), tagByteArray);\r\n    currentColumnCells.add(c1);\r\n    List<Cell> cells = fs.processSummationMajorCompaction(currentColumnCells, new LongConverter(), currentTimestamp);\r\n    assertNotNull(cells);\r\n    assertEquals(1, cells.size());\r\n    Cell returnedCell = cells.get(0);\r\n    assertNotEquals(c1, returnedCell);\r\n    long inputTs = c1.getTimestamp();\r\n    long returnTs = returnedCell.getTimestamp();\r\n    assertTrue(returnTs > inputTs);\r\n    assertTrue(returnTs >= currentTimestamp);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-yarn-project\\hadoop-yarn\\hadoop-yarn-server\\hadoop-yarn-server-timelineservice-hbase-tests\\src\\test\\java\\org\\apache\\hadoop\\yarn\\server\\timelineservice\\storage\\flow",
  "methodName" : "testProcessSummationOneCell",
  "errType" : null,
  "containingMethodsNum" : 13,
  "sourceCodeText" : "void testProcessSummationOneCell() throws IOException\n{\r\n    FlowScanner fs = getFlowScannerForTestingCompaction();\r\n    long currentTimestamp = System.currentTimeMillis();\r\n    List<Tag> tags = new ArrayList<>();\r\n    Tag t = HBaseTimelineServerUtils.createTag(AggregationOperation.SUM.getTagType(), \"application_123458888888_999888\");\r\n    tags.add(t);\r\n    byte[] tagByteArray = HBaseTimelineServerUtils.convertTagListToByteArray(tags);\r\n    SortedSet<Cell> currentColumnCells = new TreeSet<Cell>(KeyValue.COMPARATOR);\r\n    Cell c1 = HBaseTimelineServerUtils.createNewCell(aRowKey, aFamily, aQualifier, currentTimestamp, Bytes.toBytes(1110L), tagByteArray);\r\n    currentColumnCells.add(c1);\r\n    List<Cell> cells = fs.processSummationMajorCompaction(currentColumnCells, new LongConverter(), currentTimestamp);\r\n    assertNotNull(cells);\r\n    assertEquals(1, cells.size());\r\n    Cell c2 = cells.get(0);\r\n    assertEquals(c1, c2);\r\n    assertEquals(currentTimestamp, c2.getTimestamp());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-yarn-project\\hadoop-yarn\\hadoop-yarn-server\\hadoop-yarn-server-timelineservice-hbase-tests\\src\\test\\java\\org\\apache\\hadoop\\yarn\\server\\timelineservice\\storage\\flow",
  "methodName" : "testProcessSummationEmpty",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void testProcessSummationEmpty() throws IOException\n{\r\n    FlowScanner fs = getFlowScannerForTestingCompaction();\r\n    long currentTimestamp = System.currentTimeMillis();\r\n    LongConverter longConverter = new LongConverter();\r\n    SortedSet<Cell> currentColumnCells = null;\r\n    List<Cell> cells = fs.processSummationMajorCompaction(currentColumnCells, longConverter, currentTimestamp);\r\n    assertNotNull(cells);\r\n    assertEquals(0, cells.size());\r\n    currentColumnCells = new TreeSet<Cell>(KeyValue.COMPARATOR);\r\n    cells = fs.processSummationMajorCompaction(currentColumnCells, longConverter, currentTimestamp);\r\n    assertNotNull(cells);\r\n    assertEquals(0, cells.size());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-yarn-project\\hadoop-yarn\\hadoop-yarn-server\\hadoop-yarn-server-timelineservice-hbase-tests\\src\\test\\java\\org\\apache\\hadoop\\yarn\\server\\timelineservice\\storage\\flow",
  "methodName" : "tearDownAfterClass",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void tearDownAfterClass() throws Exception\n{\r\n    if (util != null) {\r\n        util.shutdownMiniCluster();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-yarn-project\\hadoop-yarn\\hadoop-yarn-server\\hadoop-yarn-server-timelineservice-hbase-tests\\src\\test\\java\\org\\apache\\hadoop\\yarn\\server\\timelineservice\\storage",
  "methodName" : "setupBeforeClass",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void setupBeforeClass() throws Exception\n{\r\n    util = new HBaseTestingUtility();\r\n    util.startMiniCluster();\r\n    DataGeneratorForTest.createSchema(util.getConfiguration());\r\n    DataGeneratorForTest.loadEntities(util, CURRENT_TIME);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-yarn-project\\hadoop-yarn\\hadoop-yarn-server\\hadoop-yarn-server-timelineservice-hbase-tests\\src\\test\\java\\org\\apache\\hadoop\\yarn\\server\\timelineservice\\storage",
  "methodName" : "init",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void init() throws Exception\n{\r\n    reader = new HBaseTimelineReaderImpl();\r\n    reader.init(util.getConfiguration());\r\n    reader.start();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-yarn-project\\hadoop-yarn\\hadoop-yarn-server\\hadoop-yarn-server-timelineservice-hbase-tests\\src\\test\\java\\org\\apache\\hadoop\\yarn\\server\\timelineservice\\storage",
  "methodName" : "stop",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void stop() throws Exception\n{\r\n    if (reader != null) {\r\n        reader.stop();\r\n        reader.close();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-yarn-project\\hadoop-yarn\\hadoop-yarn-server\\hadoop-yarn-server-timelineservice-hbase-tests\\src\\test\\java\\org\\apache\\hadoop\\yarn\\server\\timelineservice\\storage",
  "methodName" : "matchMetrics",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void matchMetrics(Map<Long, Number> m1, Map<Long, Number> m2)\n{\r\n    assertEquals(m1.size(), m2.size());\r\n    for (Map.Entry<Long, Number> entry : m2.entrySet()) {\r\n        Number val = m1.get(entry.getKey());\r\n        assertNotNull(val);\r\n        assertEquals(val.longValue(), entry.getValue().longValue());\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-yarn-project\\hadoop-yarn\\hadoop-yarn-server\\hadoop-yarn-server-timelineservice-hbase-tests\\src\\test\\java\\org\\apache\\hadoop\\yarn\\server\\timelineservice\\storage",
  "methodName" : "testWriteEntityToHBase",
  "errType" : null,
  "containingMethodsNum" : 111,
  "sourceCodeText" : "void testWriteEntityToHBase() throws Exception\n{\r\n    TimelineEntities te = new TimelineEntities();\r\n    TimelineEntity entity = new TimelineEntity();\r\n    String id = \"hello\";\r\n    String type = \"world\";\r\n    entity.setId(id);\r\n    entity.setType(type);\r\n    Long cTime = 1425016501000L;\r\n    entity.setCreatedTime(cTime);\r\n    Map<String, Object> infoMap = new HashMap<String, Object>();\r\n    infoMap.put(\"infoMapKey1\", \"infoMapValue1\");\r\n    infoMap.put(\"infoMapKey2\", 10);\r\n    entity.addInfo(infoMap);\r\n    String key = \"task\";\r\n    String value = \"is_related_to_entity_id_here\";\r\n    Set<String> isRelatedToSet = new HashSet<String>();\r\n    isRelatedToSet.add(value);\r\n    Map<String, Set<String>> isRelatedTo = new HashMap<String, Set<String>>();\r\n    isRelatedTo.put(key, isRelatedToSet);\r\n    entity.setIsRelatedToEntities(isRelatedTo);\r\n    key = \"container\";\r\n    value = \"relates_to_entity_id_here\";\r\n    Set<String> relatesToSet = new HashSet<String>();\r\n    relatesToSet.add(value);\r\n    value = \"relates_to_entity_id_here_Second\";\r\n    relatesToSet.add(value);\r\n    Map<String, Set<String>> relatesTo = new HashMap<String, Set<String>>();\r\n    relatesTo.put(key, relatesToSet);\r\n    entity.setRelatesToEntities(relatesTo);\r\n    Map<String, String> conf = new HashMap<String, String>();\r\n    conf.put(\"config_param1\", \"value1\");\r\n    conf.put(\"config_param2\", \"value2\");\r\n    entity.addConfigs(conf);\r\n    Set<TimelineMetric> metrics = new HashSet<>();\r\n    TimelineMetric m1 = new TimelineMetric();\r\n    m1.setId(\"MAP_SLOT_MILLIS\");\r\n    Map<Long, Number> metricValues = new HashMap<Long, Number>();\r\n    long ts = System.currentTimeMillis();\r\n    metricValues.put(ts - 120000, 100000000);\r\n    metricValues.put(ts - 100000, 200000000);\r\n    metricValues.put(ts - 80000, 300000000);\r\n    metricValues.put(ts - 60000, 400000000);\r\n    metricValues.put(ts - 40000, 50000000000L);\r\n    metricValues.put(ts - 20000, 60000000000L);\r\n    m1.setType(Type.TIME_SERIES);\r\n    m1.setValues(metricValues);\r\n    metrics.add(m1);\r\n    entity.addMetrics(metrics);\r\n    te.addEntity(new SubApplicationEntity(entity));\r\n    HBaseTimelineWriterImpl hbi = null;\r\n    try {\r\n        Configuration c1 = util.getConfiguration();\r\n        hbi = new HBaseTimelineWriterImpl();\r\n        hbi.init(c1);\r\n        hbi.start();\r\n        String cluster = \"cluster_test_write_entity\";\r\n        String user = \"user1\";\r\n        String subAppUser = \"subAppUser1\";\r\n        String flow = \"some_flow_name\";\r\n        String flowVersion = \"AB7822C10F1111\";\r\n        long runid = 1002345678919L;\r\n        String appName = HBaseTimelineSchemaUtils.convertApplicationIdToString(ApplicationId.newInstance(System.currentTimeMillis() + 9000000L, 1));\r\n        hbi.write(new TimelineCollectorContext(cluster, user, flow, flowVersion, runid, appName), te, UserGroupInformation.createRemoteUser(subAppUser));\r\n        hbi.stop();\r\n        Scan s = new Scan();\r\n        byte[] startRow = new EntityRowKeyPrefix(cluster, user, flow, runid, appName).getRowKeyPrefix();\r\n        s.setStartRow(startRow);\r\n        s.setMaxVersions(Integer.MAX_VALUE);\r\n        Connection conn = ConnectionFactory.createConnection(c1);\r\n        ResultScanner scanner = new EntityTableRW().getResultScanner(c1, conn, s);\r\n        int rowCount = 0;\r\n        int colCount = 0;\r\n        KeyConverter<String> stringKeyConverter = new StringKeyConverter();\r\n        for (Result result : scanner) {\r\n            if (result != null && !result.isEmpty()) {\r\n                rowCount++;\r\n                colCount += result.size();\r\n                byte[] row1 = result.getRow();\r\n                assertTrue(isRowKeyCorrect(row1, cluster, user, flow, runid, appName, entity));\r\n                String id1 = ColumnRWHelper.readResult(result, EntityColumn.ID).toString();\r\n                assertEquals(id, id1);\r\n                String type1 = ColumnRWHelper.readResult(result, EntityColumn.TYPE).toString();\r\n                assertEquals(type, type1);\r\n                Long cTime1 = (Long) ColumnRWHelper.readResult(result, EntityColumn.CREATED_TIME);\r\n                assertEquals(cTime1, cTime);\r\n                Map<String, Object> infoColumns = ColumnRWHelper.readResults(result, EntityColumnPrefix.INFO, new StringKeyConverter());\r\n                assertEquals(infoMap, infoColumns);\r\n                for (Map.Entry<String, Set<String>> isRelatedToEntry : isRelatedTo.entrySet()) {\r\n                    Object isRelatedToValue = ColumnRWHelper.readResult(result, EntityColumnPrefix.IS_RELATED_TO, isRelatedToEntry.getKey());\r\n                    String compoundValue = isRelatedToValue.toString();\r\n                    Set<String> isRelatedToValues = new HashSet<String>(Separator.VALUES.splitEncoded(compoundValue));\r\n                    assertEquals(isRelatedTo.get(isRelatedToEntry.getKey()).size(), isRelatedToValues.size());\r\n                    for (String v : isRelatedToEntry.getValue()) {\r\n                        assertTrue(isRelatedToValues.contains(v));\r\n                    }\r\n                }\r\n                for (Map.Entry<String, Set<String>> relatesToEntry : relatesTo.entrySet()) {\r\n                    String compoundValue = ColumnRWHelper.readResult(result, EntityColumnPrefix.RELATES_TO, relatesToEntry.getKey()).toString();\r\n                    Set<String> relatesToValues = new HashSet<String>(Separator.VALUES.splitEncoded(compoundValue));\r\n                    assertEquals(relatesTo.get(relatesToEntry.getKey()).size(), relatesToValues.size());\r\n                    for (String v : relatesToEntry.getValue()) {\r\n                        assertTrue(relatesToValues.contains(v));\r\n                    }\r\n                }\r\n                Map<String, Object> configColumns = ColumnRWHelper.readResults(result, EntityColumnPrefix.CONFIG, stringKeyConverter);\r\n                assertEquals(conf, configColumns);\r\n                NavigableMap<String, NavigableMap<Long, Number>> metricsResult = ColumnRWHelper.readResultsWithTimestamps(result, EntityColumnPrefix.METRIC, stringKeyConverter);\r\n                NavigableMap<Long, Number> metricMap = metricsResult.get(m1.getId());\r\n                matchMetrics(metricValues, metricMap);\r\n            }\r\n        }\r\n        assertEquals(1, rowCount);\r\n        assertEquals(16, colCount);\r\n        TimelineEntity e1 = reader.getEntity(new TimelineReaderContext(cluster, user, flow, runid, appName, entity.getType(), entity.getId()), new TimelineDataToRetrieve(null, null, EnumSet.of(Field.ALL), Integer.MAX_VALUE, null, null));\r\n        Set<TimelineEntity> es1 = reader.getEntities(new TimelineReaderContext(cluster, user, flow, runid, appName, entity.getType(), null), new TimelineEntityFilters.Builder().build(), new TimelineDataToRetrieve(null, null, EnumSet.of(Field.ALL), Integer.MAX_VALUE, null, null));\r\n        assertNotNull(e1);\r\n        assertEquals(1, es1.size());\r\n        assertEquals(id, e1.getId());\r\n        assertEquals(type, e1.getType());\r\n        assertEquals(cTime, e1.getCreatedTime());\r\n        Map<String, Object> infoMap2 = e1.getInfo();\r\n        infoMap2.remove(\"FROM_ID\");\r\n        assertEquals(infoMap, infoMap2);\r\n        Map<String, Set<String>> isRelatedTo2 = e1.getIsRelatedToEntities();\r\n        assertEquals(isRelatedTo, isRelatedTo2);\r\n        Map<String, Set<String>> relatesTo2 = e1.getRelatesToEntities();\r\n        assertEquals(relatesTo, relatesTo2);\r\n        Map<String, String> conf2 = e1.getConfigs();\r\n        assertEquals(conf, conf2);\r\n        Set<TimelineMetric> metrics2 = e1.getMetrics();\r\n        assertEquals(metrics, metrics2);\r\n        for (TimelineMetric metric2 : metrics2) {\r\n            Map<Long, Number> metricValues2 = metric2.getValues();\r\n            matchMetrics(metricValues, metricValues2);\r\n        }\r\n        e1 = reader.getEntity(new TimelineReaderContext(cluster, user, flow, runid, appName, entity.getType(), entity.getId()), new TimelineDataToRetrieve(null, null, EnumSet.of(Field.ALL), null, null, null));\r\n        assertNotNull(e1);\r\n        assertEquals(id, e1.getId());\r\n        assertEquals(type, e1.getType());\r\n        assertEquals(cTime, e1.getCreatedTime());\r\n        infoMap2 = e1.getInfo();\r\n        infoMap2.remove(\"FROM_ID\");\r\n        assertEquals(infoMap, infoMap2);\r\n        assertEquals(isRelatedTo, e1.getIsRelatedToEntities());\r\n        assertEquals(relatesTo, e1.getRelatesToEntities());\r\n        assertEquals(conf, e1.getConfigs());\r\n        for (TimelineMetric metric : e1.getMetrics()) {\r\n            assertEquals(TimelineMetric.Type.SINGLE_VALUE, metric.getType());\r\n            assertEquals(1, metric.getValues().size());\r\n            assertTrue(metric.getValues().containsKey(ts - 20000));\r\n            assertEquals(metricValues.get(ts - 20000), metric.getValues().get(ts - 20000));\r\n        }\r\n        verifySubApplicationTableEntities(cluster, user, flow, flowVersion, runid, appName, subAppUser, c1, entity, id, type, infoMap, isRelatedTo, relatesTo, conf, metricValues, metrics, cTime, m1);\r\n    } finally {\r\n        if (hbi != null) {\r\n            hbi.stop();\r\n            hbi.close();\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-yarn-project\\hadoop-yarn\\hadoop-yarn-server\\hadoop-yarn-server-timelineservice-hbase-tests\\src\\test\\java\\org\\apache\\hadoop\\yarn\\server\\timelineservice\\storage",
  "methodName" : "verifySubApplicationTableEntities",
  "errType" : null,
  "containingMethodsNum" : 37,
  "sourceCodeText" : "void verifySubApplicationTableEntities(String cluster, String user, String flow, String flowVersion, Long runid, String appName, String subAppUser, Configuration c1, TimelineEntity entity, String id, String type, Map<String, Object> infoMap, Map<String, Set<String>> isRelatedTo, Map<String, Set<String>> relatesTo, Map<String, String> conf, Map<Long, Number> metricValues, Set<TimelineMetric> metrics, Long cTime, TimelineMetric m1) throws IOException\n{\r\n    Scan s = new Scan();\r\n    byte[] startRow = new SubApplicationRowKeyPrefix(cluster, subAppUser, null, null, null, null).getRowKeyPrefix();\r\n    s.setStartRow(startRow);\r\n    s.setMaxVersions(Integer.MAX_VALUE);\r\n    Connection conn = ConnectionFactory.createConnection(c1);\r\n    ResultScanner scanner = new SubApplicationTableRW().getResultScanner(c1, conn, s);\r\n    int rowCount = 0;\r\n    int colCount = 0;\r\n    KeyConverter<String> stringKeyConverter = new StringKeyConverter();\r\n    for (Result result : scanner) {\r\n        if (result != null && !result.isEmpty()) {\r\n            rowCount++;\r\n            colCount += result.size();\r\n            byte[] row1 = result.getRow();\r\n            assertTrue(verifyRowKeyForSubApplication(row1, subAppUser, cluster, user, entity));\r\n            String id1 = ColumnRWHelper.readResult(result, SubApplicationColumn.ID).toString();\r\n            assertEquals(id, id1);\r\n            String type1 = ColumnRWHelper.readResult(result, SubApplicationColumn.TYPE).toString();\r\n            assertEquals(type, type1);\r\n            Long cTime1 = (Long) ColumnRWHelper.readResult(result, SubApplicationColumn.CREATED_TIME);\r\n            assertEquals(cTime1, cTime);\r\n            Map<String, Object> infoColumns = ColumnRWHelper.readResults(result, SubApplicationColumnPrefix.INFO, new StringKeyConverter());\r\n            assertEquals(infoMap, infoColumns);\r\n            for (Map.Entry<String, Set<String>> isRelatedToEntry : isRelatedTo.entrySet()) {\r\n                Object isRelatedToValue = ColumnRWHelper.readResult(result, SubApplicationColumnPrefix.IS_RELATED_TO, isRelatedToEntry.getKey());\r\n                String compoundValue = isRelatedToValue.toString();\r\n                Set<String> isRelatedToValues = new HashSet<String>(Separator.VALUES.splitEncoded(compoundValue));\r\n                assertEquals(isRelatedTo.get(isRelatedToEntry.getKey()).size(), isRelatedToValues.size());\r\n                for (String v : isRelatedToEntry.getValue()) {\r\n                    assertTrue(isRelatedToValues.contains(v));\r\n                }\r\n            }\r\n            for (Map.Entry<String, Set<String>> relatesToEntry : relatesTo.entrySet()) {\r\n                String compoundValue = ColumnRWHelper.readResult(result, SubApplicationColumnPrefix.RELATES_TO, relatesToEntry.getKey()).toString();\r\n                Set<String> relatesToValues = new HashSet<String>(Separator.VALUES.splitEncoded(compoundValue));\r\n                assertEquals(relatesTo.get(relatesToEntry.getKey()).size(), relatesToValues.size());\r\n                for (String v : relatesToEntry.getValue()) {\r\n                    assertTrue(relatesToValues.contains(v));\r\n                }\r\n            }\r\n            Map<String, Object> configColumns = ColumnRWHelper.readResults(result, SubApplicationColumnPrefix.CONFIG, stringKeyConverter);\r\n            assertEquals(conf, configColumns);\r\n            NavigableMap<String, NavigableMap<Long, Number>> metricsResult = ColumnRWHelper.readResultsWithTimestamps(result, SubApplicationColumnPrefix.METRIC, stringKeyConverter);\r\n            NavigableMap<Long, Number> metricMap = metricsResult.get(m1.getId());\r\n            matchMetrics(metricValues, metricMap);\r\n        }\r\n    }\r\n    assertEquals(1, rowCount);\r\n    assertEquals(16, colCount);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-yarn-project\\hadoop-yarn\\hadoop-yarn-server\\hadoop-yarn-server-timelineservice-hbase-tests\\src\\test\\java\\org\\apache\\hadoop\\yarn\\server\\timelineservice\\storage",
  "methodName" : "isRowKeyCorrect",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "boolean isRowKeyCorrect(byte[] rowKey, String cluster, String user, String flow, Long runid, String appName, TimelineEntity te)\n{\r\n    EntityRowKey key = EntityRowKey.parseRowKey(rowKey);\r\n    assertEquals(user, key.getUserId());\r\n    assertEquals(cluster, key.getClusterId());\r\n    assertEquals(flow, key.getFlowName());\r\n    assertEquals(runid, key.getFlowRunId());\r\n    assertEquals(appName, key.getAppId());\r\n    assertEquals(te.getType(), key.getEntityType());\r\n    assertEquals(te.getId(), key.getEntityId());\r\n    return true;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-yarn-project\\hadoop-yarn\\hadoop-yarn-server\\hadoop-yarn-server-timelineservice-hbase-tests\\src\\test\\java\\org\\apache\\hadoop\\yarn\\server\\timelineservice\\storage",
  "methodName" : "testEventsWithEmptyInfo",
  "errType" : null,
  "containingMethodsNum" : 42,
  "sourceCodeText" : "void testEventsWithEmptyInfo() throws IOException\n{\r\n    TimelineEvent event = new TimelineEvent();\r\n    String eventId = \"foo_ev e  nt_id\";\r\n    event.setId(eventId);\r\n    Long expTs = 1436512802000L;\r\n    event.setTimestamp(expTs);\r\n    final TimelineEntity entity = new TimelineEntity();\r\n    entity.setId(\"attempt_1329348432655_0001_m_000008_18\");\r\n    entity.setType(\"FOO_ATTEMPT\");\r\n    entity.addEvent(event);\r\n    TimelineEntities entities = new TimelineEntities();\r\n    entities.addEntity(entity);\r\n    HBaseTimelineWriterImpl hbi = null;\r\n    try {\r\n        Configuration c1 = util.getConfiguration();\r\n        hbi = new HBaseTimelineWriterImpl();\r\n        hbi.init(c1);\r\n        hbi.start();\r\n        String cluster = \"cluster_test_empty_eventkey\";\r\n        String user = \"user_emptyeventkey\";\r\n        String flow = \"other_flow_name\";\r\n        String flowVersion = \"1111F01C2287BA\";\r\n        long runid = 1009876543218L;\r\n        String appName = HBaseTimelineSchemaUtils.convertApplicationIdToString(ApplicationId.newInstance(System.currentTimeMillis() + 9000000L, 1));\r\n        byte[] startRow = new EntityRowKeyPrefix(cluster, user, flow, runid, appName).getRowKeyPrefix();\r\n        hbi.write(new TimelineCollectorContext(cluster, user, flow, flowVersion, runid, appName), entities, UserGroupInformation.createRemoteUser(user));\r\n        hbi.stop();\r\n        Scan s = new Scan();\r\n        s.setStartRow(startRow);\r\n        s.addFamily(EntityColumnFamily.INFO.getBytes());\r\n        Connection conn = ConnectionFactory.createConnection(c1);\r\n        ResultScanner scanner = new EntityTableRW().getResultScanner(c1, conn, s);\r\n        int rowCount = 0;\r\n        for (Result result : scanner) {\r\n            if (result != null && !result.isEmpty()) {\r\n                rowCount++;\r\n                byte[] row1 = result.getRow();\r\n                assertTrue(isRowKeyCorrect(row1, cluster, user, flow, runid, appName, entity));\r\n                Map<EventColumnName, Object> eventsResult = ColumnRWHelper.readResults(result, EntityColumnPrefix.EVENT, new EventColumnNameConverter());\r\n                assertEquals(1, eventsResult.size());\r\n                for (Map.Entry<EventColumnName, Object> e : eventsResult.entrySet()) {\r\n                    EventColumnName eventColumnName = e.getKey();\r\n                    assertEquals(eventId, eventColumnName.getId());\r\n                    assertEquals(expTs, eventColumnName.getTimestamp());\r\n                    assertNull(eventColumnName.getInfoKey());\r\n                    Object value = e.getValue();\r\n                    assertEquals(\"\", value.toString());\r\n                }\r\n            }\r\n        }\r\n        assertEquals(1, rowCount);\r\n        TimelineEntity e1 = reader.getEntity(new TimelineReaderContext(cluster, user, flow, runid, appName, entity.getType(), entity.getId()), new TimelineDataToRetrieve(null, null, EnumSet.of(Field.ALL), null, null, null));\r\n        Set<TimelineEntity> es1 = reader.getEntities(new TimelineReaderContext(cluster, user, flow, runid, appName, entity.getType(), null), new TimelineEntityFilters.Builder().build(), new TimelineDataToRetrieve(null, null, EnumSet.of(Field.ALL), null, null, null));\r\n        assertNotNull(e1);\r\n        assertEquals(1, es1.size());\r\n        NavigableSet<TimelineEvent> events = e1.getEvents();\r\n        assertEquals(1, events.size());\r\n        for (TimelineEvent e : events) {\r\n            assertEquals(eventId, e.getId());\r\n            assertEquals(expTs, Long.valueOf(e.getTimestamp()));\r\n            Map<String, Object> info = e.getInfo();\r\n            assertTrue(info == null || info.isEmpty());\r\n        }\r\n    } finally {\r\n        if (hbi != null) {\r\n            hbi.stop();\r\n            hbi.close();\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-yarn-project\\hadoop-yarn\\hadoop-yarn-server\\hadoop-yarn-server-timelineservice-hbase-tests\\src\\test\\java\\org\\apache\\hadoop\\yarn\\server\\timelineservice\\storage",
  "methodName" : "testEventsEscapeTs",
  "errType" : null,
  "containingMethodsNum" : 24,
  "sourceCodeText" : "void testEventsEscapeTs() throws IOException\n{\r\n    TimelineEvent event = new TimelineEvent();\r\n    String eventId = ApplicationMetricsConstants.CREATED_EVENT_TYPE;\r\n    event.setId(eventId);\r\n    long expTs = 1463567041056L;\r\n    event.setTimestamp(expTs);\r\n    String expKey = \"f==o o_e ve\\tnt\";\r\n    Object expVal = \"test\";\r\n    event.addInfo(expKey, expVal);\r\n    final TimelineEntity entity = new ApplicationEntity();\r\n    entity.setId(HBaseTimelineSchemaUtils.convertApplicationIdToString(ApplicationId.newInstance(0, 1)));\r\n    entity.addEvent(event);\r\n    TimelineEntities entities = new TimelineEntities();\r\n    entities.addEntity(entity);\r\n    HBaseTimelineWriterImpl hbi = null;\r\n    try {\r\n        Configuration c1 = util.getConfiguration();\r\n        hbi = new HBaseTimelineWriterImpl();\r\n        hbi.init(c1);\r\n        hbi.start();\r\n        String cluster = \"clus!ter_\\ttest_ev  ents\";\r\n        String user = \"user2\";\r\n        String flow = \"other_flow_name\";\r\n        String flowVersion = \"1111F01C2287BA\";\r\n        long runid = 1009876543218L;\r\n        String appName = \"application_123465899910_2001\";\r\n        hbi.write(new TimelineCollectorContext(cluster, user, flow, flowVersion, runid, appName), entities, UserGroupInformation.createRemoteUser(user));\r\n        hbi.stop();\r\n        TimelineEntity e1 = reader.getEntity(new TimelineReaderContext(cluster, user, flow, runid, appName, entity.getType(), entity.getId()), new TimelineDataToRetrieve(null, null, EnumSet.of(Field.ALL), null, null, null));\r\n        assertNotNull(e1);\r\n        NavigableSet<TimelineEvent> events = e1.getEvents();\r\n        assertEquals(1, events.size());\r\n        for (TimelineEvent e : events) {\r\n            assertEquals(eventId, e.getId());\r\n            assertEquals(expTs, e.getTimestamp());\r\n            Map<String, Object> info = e.getInfo();\r\n            assertEquals(1, info.size());\r\n            for (Map.Entry<String, Object> infoEntry : info.entrySet()) {\r\n                assertEquals(expKey, infoEntry.getKey());\r\n                assertEquals(expVal, infoEntry.getValue());\r\n            }\r\n        }\r\n    } finally {\r\n        if (hbi != null) {\r\n            hbi.stop();\r\n            hbi.close();\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-yarn-project\\hadoop-yarn\\hadoop-yarn-server\\hadoop-yarn-server-timelineservice-hbase-tests\\src\\test\\java\\org\\apache\\hadoop\\yarn\\server\\timelineservice\\storage",
  "methodName" : "testReadEntities",
  "errType" : null,
  "containingMethodsNum" : 24,
  "sourceCodeText" : "void testReadEntities() throws Exception\n{\r\n    TimelineEntity entity = reader.getEntity(new TimelineReaderContext(\"cluster1\", \"user1\", \"some_flow_name\", 1002345678919L, \"application_1231111111_1111\", \"world\", \"hello\"), new TimelineDataToRetrieve(null, null, EnumSet.of(Field.ALL), null, null, null));\r\n    assertNotNull(entity);\r\n    assertEquals(3, entity.getConfigs().size());\r\n    assertEquals(1, entity.getIsRelatedToEntities().size());\r\n    Set<TimelineEntity> entities = reader.getEntities(new TimelineReaderContext(\"cluster1\", \"user1\", \"some_flow_name\", 1002345678919L, \"application_1231111111_1111\", \"world\", null), new TimelineEntityFilters.Builder().build(), new TimelineDataToRetrieve(null, null, EnumSet.of(Field.ALL), null, null, null));\r\n    assertEquals(3, entities.size());\r\n    int cfgCnt = 0;\r\n    int metricCnt = 0;\r\n    int infoCnt = 0;\r\n    int eventCnt = 0;\r\n    int relatesToCnt = 0;\r\n    int isRelatedToCnt = 0;\r\n    for (TimelineEntity timelineEntity : entities) {\r\n        cfgCnt += (timelineEntity.getConfigs() == null) ? 0 : timelineEntity.getConfigs().size();\r\n        metricCnt += (timelineEntity.getMetrics() == null) ? 0 : timelineEntity.getMetrics().size();\r\n        infoCnt += (timelineEntity.getInfo() == null) ? 0 : timelineEntity.getInfo().size();\r\n        eventCnt += (timelineEntity.getEvents() == null) ? 0 : timelineEntity.getEvents().size();\r\n        relatesToCnt += (timelineEntity.getRelatesToEntities() == null) ? 0 : timelineEntity.getRelatesToEntities().size();\r\n        isRelatedToCnt += (timelineEntity.getIsRelatedToEntities() == null) ? 0 : timelineEntity.getIsRelatedToEntities().size();\r\n    }\r\n    assertEquals(5, cfgCnt);\r\n    assertEquals(3, metricCnt);\r\n    assertEquals(8, infoCnt);\r\n    assertEquals(4, eventCnt);\r\n    assertEquals(4, relatesToCnt);\r\n    assertEquals(4, isRelatedToCnt);\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-yarn-project\\hadoop-yarn\\hadoop-yarn-server\\hadoop-yarn-server-timelineservice-hbase-tests\\src\\test\\java\\org\\apache\\hadoop\\yarn\\server\\timelineservice\\storage",
  "methodName" : "testFilterEntitiesByCreatedTime",
  "errType" : null,
  "containingMethodsNum" : 15,
  "sourceCodeText" : "void testFilterEntitiesByCreatedTime() throws Exception\n{\r\n    Set<TimelineEntity> entities = reader.getEntities(new TimelineReaderContext(\"cluster1\", \"user1\", \"some_flow_name\", 1002345678919L, \"application_1231111111_1111\", \"world\", null), new TimelineEntityFilters.Builder().createdTimeBegin(1425016502000L).createTimeEnd(1425016502040L).build(), new TimelineDataToRetrieve());\r\n    assertEquals(3, entities.size());\r\n    for (TimelineEntity entity : entities) {\r\n        if (!entity.getId().equals(\"hello\") && !entity.getId().equals(\"hello1\") && !entity.getId().equals(\"hello2\")) {\r\n            Assert.fail(\"Entities with ids' hello, hello1 and hello2 should be\" + \" present\");\r\n        }\r\n    }\r\n    entities = reader.getEntities(new TimelineReaderContext(\"cluster1\", \"user1\", \"some_flow_name\", 1002345678919L, \"application_1231111111_1111\", \"world\", null), new TimelineEntityFilters.Builder().createdTimeBegin(1425016502015L).build(), new TimelineDataToRetrieve());\r\n    assertEquals(2, entities.size());\r\n    for (TimelineEntity entity : entities) {\r\n        if (!entity.getId().equals(\"hello1\") && !entity.getId().equals(\"hello2\")) {\r\n            Assert.fail(\"Entities with ids' hello1 and hello2 should be present\");\r\n        }\r\n    }\r\n    entities = reader.getEntities(new TimelineReaderContext(\"cluster1\", \"user1\", \"some_flow_name\", 1002345678919L, \"application_1231111111_1111\", \"world\", null), new TimelineEntityFilters.Builder().createTimeEnd(1425016502015L).build(), new TimelineDataToRetrieve());\r\n    assertEquals(1, entities.size());\r\n    for (TimelineEntity entity : entities) {\r\n        if (!entity.getId().equals(\"hello\")) {\r\n            Assert.fail(\"Entity with id hello should be present\");\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-yarn-project\\hadoop-yarn\\hadoop-yarn-server\\hadoop-yarn-server-timelineservice-hbase-tests\\src\\test\\java\\org\\apache\\hadoop\\yarn\\server\\timelineservice\\storage",
  "methodName" : "testReadEntitiesRelationsAndEventFiltersDefaultView",
  "errType" : null,
  "containingMethodsNum" : 15,
  "sourceCodeText" : "void testReadEntitiesRelationsAndEventFiltersDefaultView() throws Exception\n{\r\n    TimelineFilterList eventFilter = new TimelineFilterList();\r\n    eventFilter.addFilter(new TimelineExistsFilter(TimelineCompareOp.NOT_EQUAL, \"end_event\"));\r\n    TimelineFilterList relatesTo = new TimelineFilterList(Operator.OR);\r\n    relatesTo.addFilter(new TimelineKeyValuesFilter(TimelineCompareOp.EQUAL, \"container2\", new HashSet<Object>(Arrays.asList(\"relatesto7\"))));\r\n    relatesTo.addFilter(new TimelineKeyValuesFilter(TimelineCompareOp.EQUAL, \"container1\", new HashSet<Object>(Arrays.asList(\"relatesto4\"))));\r\n    TimelineFilterList isRelatedTo = new TimelineFilterList();\r\n    isRelatedTo.addFilter(new TimelineKeyValuesFilter(TimelineCompareOp.EQUAL, \"task1\", new HashSet<Object>(Arrays.asList(\"relatedto3\"))));\r\n    isRelatedTo.addFilter(new TimelineKeyValuesFilter(TimelineCompareOp.NOT_EQUAL, \"task1\", new HashSet<Object>(Arrays.asList(\"relatedto5\"))));\r\n    Set<TimelineEntity> entities = reader.getEntities(new TimelineReaderContext(\"cluster1\", \"user1\", \"some_flow_name\", 1002345678919L, \"application_1231111111_1111\", \"world\", null), new TimelineEntityFilters.Builder().relatesTo(relatesTo).isRelatedTo(isRelatedTo).eventFilters(eventFilter).build(), new TimelineDataToRetrieve());\r\n    assertEquals(1, entities.size());\r\n    int eventCnt = 0;\r\n    int isRelatedToCnt = 0;\r\n    int relatesToCnt = 0;\r\n    for (TimelineEntity timelineEntity : entities) {\r\n        eventCnt += timelineEntity.getEvents().size();\r\n        isRelatedToCnt += timelineEntity.getIsRelatedToEntities().size();\r\n        relatesToCnt += timelineEntity.getRelatesToEntities().size();\r\n        if (!timelineEntity.getId().equals(\"hello2\")) {\r\n            Assert.fail(\"Entity id should have been hello2\");\r\n        }\r\n    }\r\n    assertEquals(0, eventCnt);\r\n    assertEquals(0, isRelatedToCnt);\r\n    assertEquals(0, relatesToCnt);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-yarn-project\\hadoop-yarn\\hadoop-yarn-server\\hadoop-yarn-server-timelineservice-hbase-tests\\src\\test\\java\\org\\apache\\hadoop\\yarn\\server\\timelineservice\\storage",
  "methodName" : "testReadEntitiesEventFilters",
  "errType" : null,
  "containingMethodsNum" : 45,
  "sourceCodeText" : "void testReadEntitiesEventFilters() throws Exception\n{\r\n    TimelineFilterList ef = new TimelineFilterList();\r\n    ef.addFilter(new TimelineExistsFilter(TimelineCompareOp.EQUAL, \"update_event\"));\r\n    ef.addFilter(new TimelineExistsFilter(TimelineCompareOp.NOT_EQUAL, \"end_event\"));\r\n    Set<TimelineEntity> entities = reader.getEntities(new TimelineReaderContext(\"cluster1\", \"user1\", \"some_flow_name\", 1002345678919L, \"application_1231111111_1111\", \"world\", null), new TimelineEntityFilters.Builder().eventFilters(ef).build(), new TimelineDataToRetrieve(null, null, EnumSet.of(Field.ALL), null, null, null));\r\n    assertEquals(1, entities.size());\r\n    int eventCnt = 0;\r\n    for (TimelineEntity timelineEntity : entities) {\r\n        eventCnt += timelineEntity.getEvents().size();\r\n        if (!timelineEntity.getId().equals(\"hello2\")) {\r\n            Assert.fail(\"Entity id should have been hello2\");\r\n        }\r\n    }\r\n    assertEquals(1, eventCnt);\r\n    TimelineFilterList ef1 = new TimelineFilterList();\r\n    ef1.addFilter(new TimelineExistsFilter(TimelineCompareOp.EQUAL, \"update_event\"));\r\n    ef1.addFilter(new TimelineExistsFilter(TimelineCompareOp.NOT_EQUAL, \"end_event\"));\r\n    entities = reader.getEntities(new TimelineReaderContext(\"cluster1\", \"user1\", \"some_flow_name\", 1002345678919L, \"application_1231111111_1111\", \"world\", null), new TimelineEntityFilters.Builder().eventFilters(ef1).build(), new TimelineDataToRetrieve());\r\n    assertEquals(1, entities.size());\r\n    eventCnt = 0;\r\n    for (TimelineEntity timelineEntity : entities) {\r\n        eventCnt += timelineEntity.getEvents().size();\r\n        if (!timelineEntity.getId().equals(\"hello2\")) {\r\n            Assert.fail(\"Entity id should have been hello2\");\r\n        }\r\n    }\r\n    assertEquals(0, eventCnt);\r\n    TimelineFilterList ef2 = new TimelineFilterList();\r\n    ef2.addFilter(new TimelineExistsFilter(TimelineCompareOp.NOT_EQUAL, \"end_event\"));\r\n    entities = reader.getEntities(new TimelineReaderContext(\"cluster1\", \"user1\", \"some_flow_name\", 1002345678919L, \"application_1231111111_1111\", \"world\", null), new TimelineEntityFilters.Builder().eventFilters(ef2).build(), new TimelineDataToRetrieve());\r\n    assertEquals(2, entities.size());\r\n    eventCnt = 0;\r\n    for (TimelineEntity timelineEntity : entities) {\r\n        eventCnt += timelineEntity.getEvents().size();\r\n        if (!timelineEntity.getId().equals(\"hello\") && !timelineEntity.getId().equals(\"hello2\")) {\r\n            Assert.fail(\"Entity ids' should have been hello and hello2\");\r\n        }\r\n    }\r\n    assertEquals(0, eventCnt);\r\n    TimelineFilterList ef3 = new TimelineFilterList();\r\n    ef3.addFilter(new TimelineExistsFilter(TimelineCompareOp.EQUAL, \"update_event\"));\r\n    ef3.addFilter(new TimelineExistsFilter(TimelineCompareOp.EQUAL, \"dummy_event\"));\r\n    entities = reader.getEntities(new TimelineReaderContext(\"cluster1\", \"user1\", \"some_flow_name\", 1002345678919L, \"application_1231111111_1111\", \"world\", null), new TimelineEntityFilters.Builder().eventFilters(ef3).build(), new TimelineDataToRetrieve());\r\n    assertEquals(0, entities.size());\r\n    TimelineFilterList list1 = new TimelineFilterList();\r\n    list1.addFilter(new TimelineExistsFilter(TimelineCompareOp.EQUAL, \"update_event\"));\r\n    list1.addFilter(new TimelineExistsFilter(TimelineCompareOp.EQUAL, \"dummy_event\"));\r\n    TimelineFilterList list2 = new TimelineFilterList();\r\n    list2.addFilter(new TimelineExistsFilter(TimelineCompareOp.EQUAL, \"start_event\"));\r\n    TimelineFilterList ef4 = new TimelineFilterList(Operator.OR, list1, list2);\r\n    entities = reader.getEntities(new TimelineReaderContext(\"cluster1\", \"user1\", \"some_flow_name\", 1002345678919L, \"application_1231111111_1111\", \"world\", null), new TimelineEntityFilters.Builder().eventFilters(ef4).build(), new TimelineDataToRetrieve());\r\n    assertEquals(1, entities.size());\r\n    eventCnt = 0;\r\n    for (TimelineEntity timelineEntity : entities) {\r\n        eventCnt += timelineEntity.getEvents().size();\r\n        if (!timelineEntity.getId().equals(\"hello\")) {\r\n            Assert.fail(\"Entity id should have been hello\");\r\n        }\r\n    }\r\n    assertEquals(0, eventCnt);\r\n    TimelineFilterList ef5 = new TimelineFilterList();\r\n    ef5.addFilter(new TimelineExistsFilter(TimelineCompareOp.NOT_EQUAL, \"update_event\"));\r\n    ef5.addFilter(new TimelineExistsFilter(TimelineCompareOp.NOT_EQUAL, \"end_event\"));\r\n    entities = reader.getEntities(new TimelineReaderContext(\"cluster1\", \"user1\", \"some_flow_name\", 1002345678919L, \"application_1231111111_1111\", \"world\", null), new TimelineEntityFilters.Builder().eventFilters(ef5).build(), new TimelineDataToRetrieve());\r\n    assertEquals(1, entities.size());\r\n    eventCnt = 0;\r\n    for (TimelineEntity timelineEntity : entities) {\r\n        eventCnt += timelineEntity.getEvents().size();\r\n        if (!timelineEntity.getId().equals(\"hello\")) {\r\n            Assert.fail(\"Entity id should have been hello\");\r\n        }\r\n    }\r\n    assertEquals(0, eventCnt);\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-yarn-project\\hadoop-yarn\\hadoop-yarn-server\\hadoop-yarn-server-timelineservice-hbase-tests\\src\\test\\java\\org\\apache\\hadoop\\yarn\\server\\timelineservice\\storage",
  "methodName" : "testReadEntitiesIsRelatedTo",
  "errType" : null,
  "containingMethodsNum" : 49,
  "sourceCodeText" : "void testReadEntitiesIsRelatedTo() throws Exception\n{\r\n    TimelineFilterList irt = new TimelineFilterList(Operator.OR);\r\n    irt.addFilter(new TimelineKeyValuesFilter(TimelineCompareOp.EQUAL, \"task\", new HashSet<Object>(Arrays.asList(\"relatedto1\"))));\r\n    irt.addFilter(new TimelineKeyValuesFilter(TimelineCompareOp.EQUAL, \"task2\", new HashSet<Object>(Arrays.asList(\"relatedto4\"))));\r\n    Set<TimelineEntity> entities = reader.getEntities(new TimelineReaderContext(\"cluster1\", \"user1\", \"some_flow_name\", 1002345678919L, \"application_1231111111_1111\", \"world\", null), new TimelineEntityFilters.Builder().isRelatedTo(irt).build(), new TimelineDataToRetrieve(null, null, EnumSet.of(Field.ALL), null, null, null));\r\n    assertEquals(2, entities.size());\r\n    int isRelatedToCnt = 0;\r\n    for (TimelineEntity timelineEntity : entities) {\r\n        isRelatedToCnt += timelineEntity.getIsRelatedToEntities().size();\r\n        if (!timelineEntity.getId().equals(\"hello\") && !timelineEntity.getId().equals(\"hello1\")) {\r\n            Assert.fail(\"Entity ids' should have been hello and hello1\");\r\n        }\r\n    }\r\n    assertEquals(3, isRelatedToCnt);\r\n    TimelineFilterList irt1 = new TimelineFilterList();\r\n    irt1.addFilter(new TimelineKeyValuesFilter(TimelineCompareOp.EQUAL, \"task1\", new HashSet<Object>(Arrays.asList(\"relatedto3\"))));\r\n    irt1.addFilter(new TimelineKeyValuesFilter(TimelineCompareOp.NOT_EQUAL, \"task1\", new HashSet<Object>(Arrays.asList(\"relatedto5\"))));\r\n    entities = reader.getEntities(new TimelineReaderContext(\"cluster1\", \"user1\", \"some_flow_name\", 1002345678919L, \"application_1231111111_1111\", \"world\", null), new TimelineEntityFilters.Builder().isRelatedTo(irt1).build(), new TimelineDataToRetrieve());\r\n    assertEquals(1, entities.size());\r\n    isRelatedToCnt = 0;\r\n    for (TimelineEntity timelineEntity : entities) {\r\n        isRelatedToCnt += timelineEntity.getIsRelatedToEntities().size();\r\n        if (!timelineEntity.getId().equals(\"hello2\")) {\r\n            Assert.fail(\"Entity id should have been hello2\");\r\n        }\r\n    }\r\n    assertEquals(0, isRelatedToCnt);\r\n    TimelineFilterList irt2 = new TimelineFilterList(Operator.OR);\r\n    irt2.addFilter(new TimelineKeyValuesFilter(TimelineCompareOp.EQUAL, \"task\", new HashSet<Object>(Arrays.asList(\"relatedto1\"))));\r\n    irt2.addFilter(new TimelineKeyValuesFilter(TimelineCompareOp.EQUAL, \"task2\", new HashSet<Object>(Arrays.asList(\"relatedto4\"))));\r\n    entities = reader.getEntities(new TimelineReaderContext(\"cluster1\", \"user1\", \"some_flow_name\", 1002345678919L, \"application_1231111111_1111\", \"world\", null), new TimelineEntityFilters.Builder().isRelatedTo(irt2).build(), new TimelineDataToRetrieve());\r\n    assertEquals(2, entities.size());\r\n    isRelatedToCnt = 0;\r\n    for (TimelineEntity timelineEntity : entities) {\r\n        isRelatedToCnt += timelineEntity.getIsRelatedToEntities().size();\r\n        if (!timelineEntity.getId().equals(\"hello\") && !timelineEntity.getId().equals(\"hello1\")) {\r\n            Assert.fail(\"Entity ids' should have been hello and hello1\");\r\n        }\r\n    }\r\n    assertEquals(0, isRelatedToCnt);\r\n    TimelineFilterList irt3 = new TimelineFilterList();\r\n    irt3.addFilter(new TimelineKeyValuesFilter(TimelineCompareOp.EQUAL, \"task1\", new HashSet<Object>(Arrays.asList(\"relatedto3\", \"relatedto5\"))));\r\n    entities = reader.getEntities(new TimelineReaderContext(\"cluster1\", \"user1\", \"some_flow_name\", 1002345678919L, \"application_1231111111_1111\", \"world\", null), new TimelineEntityFilters.Builder().isRelatedTo(irt3).build(), new TimelineDataToRetrieve());\r\n    assertEquals(1, entities.size());\r\n    isRelatedToCnt = 0;\r\n    for (TimelineEntity timelineEntity : entities) {\r\n        isRelatedToCnt += timelineEntity.getIsRelatedToEntities().size();\r\n        if (!timelineEntity.getId().equals(\"hello1\")) {\r\n            Assert.fail(\"Entity id should have been hello1\");\r\n        }\r\n    }\r\n    assertEquals(0, isRelatedToCnt);\r\n    TimelineFilterList irt4 = new TimelineFilterList();\r\n    irt4.addFilter(new TimelineKeyValuesFilter(TimelineCompareOp.EQUAL, \"task1\", new HashSet<Object>(Arrays.asList(\"relatedto3\"))));\r\n    irt4.addFilter(new TimelineKeyValuesFilter(TimelineCompareOp.EQUAL, \"dummy_task\", new HashSet<Object>(Arrays.asList(\"relatedto5\"))));\r\n    entities = reader.getEntities(new TimelineReaderContext(\"cluster1\", \"user1\", \"some_flow_name\", 1002345678919L, \"application_1231111111_1111\", \"world\", null), new TimelineEntityFilters.Builder().isRelatedTo(irt4).build(), new TimelineDataToRetrieve());\r\n    assertEquals(0, entities.size());\r\n    TimelineFilterList irt5 = new TimelineFilterList();\r\n    irt5.addFilter(new TimelineKeyValuesFilter(TimelineCompareOp.EQUAL, \"task1\", new HashSet<Object>(Arrays.asList(\"relatedto3\", \"relatedto7\"))));\r\n    entities = reader.getEntities(new TimelineReaderContext(\"cluster1\", \"user1\", \"some_flow_name\", 1002345678919L, \"application_1231111111_1111\", \"world\", null), new TimelineEntityFilters.Builder().isRelatedTo(irt5).build(), new TimelineDataToRetrieve());\r\n    assertEquals(0, entities.size());\r\n    TimelineFilterList list1 = new TimelineFilterList();\r\n    list1.addFilter(new TimelineKeyValuesFilter(TimelineCompareOp.EQUAL, \"task\", new HashSet<Object>(Arrays.asList(\"relatedto1\"))));\r\n    list1.addFilter(new TimelineKeyValuesFilter(TimelineCompareOp.EQUAL, \"dummy_task\", new HashSet<Object>(Arrays.asList(\"relatedto4\"))));\r\n    TimelineFilterList list2 = new TimelineFilterList();\r\n    list2.addFilter(new TimelineKeyValuesFilter(TimelineCompareOp.EQUAL, \"task2\", new HashSet<Object>(Arrays.asList(\"relatedto4\"))));\r\n    TimelineFilterList irt6 = new TimelineFilterList(Operator.OR, list1, list2);\r\n    entities = reader.getEntities(new TimelineReaderContext(\"cluster1\", \"user1\", \"some_flow_name\", 1002345678919L, \"application_1231111111_1111\", \"world\", null), new TimelineEntityFilters.Builder().isRelatedTo(irt6).build(), new TimelineDataToRetrieve());\r\n    assertEquals(1, entities.size());\r\n    isRelatedToCnt = 0;\r\n    for (TimelineEntity timelineEntity : entities) {\r\n        isRelatedToCnt += timelineEntity.getIsRelatedToEntities().size();\r\n        if (!timelineEntity.getId().equals(\"hello1\")) {\r\n            Assert.fail(\"Entity id should have been hello1\");\r\n        }\r\n    }\r\n    assertEquals(0, isRelatedToCnt);\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-yarn-project\\hadoop-yarn\\hadoop-yarn-server\\hadoop-yarn-server-timelineservice-hbase-tests\\src\\test\\java\\org\\apache\\hadoop\\yarn\\server\\timelineservice\\storage",
  "methodName" : "testReadEntitiesRelatesTo",
  "errType" : null,
  "containingMethodsNum" : 60,
  "sourceCodeText" : "void testReadEntitiesRelatesTo() throws Exception\n{\r\n    TimelineFilterList rt = new TimelineFilterList(Operator.OR);\r\n    rt.addFilter(new TimelineKeyValuesFilter(TimelineCompareOp.EQUAL, \"container2\", new HashSet<Object>(Arrays.asList(\"relatesto7\"))));\r\n    rt.addFilter(new TimelineKeyValuesFilter(TimelineCompareOp.EQUAL, \"container1\", new HashSet<Object>(Arrays.asList(\"relatesto4\"))));\r\n    Set<TimelineEntity> entities = reader.getEntities(new TimelineReaderContext(\"cluster1\", \"user1\", \"some_flow_name\", 1002345678919L, \"application_1231111111_1111\", \"world\", null), new TimelineEntityFilters.Builder().relatesTo(rt).build(), new TimelineDataToRetrieve(null, null, EnumSet.of(Field.ALL), null, null, null));\r\n    assertEquals(2, entities.size());\r\n    int relatesToCnt = 0;\r\n    for (TimelineEntity timelineEntity : entities) {\r\n        relatesToCnt += timelineEntity.getRelatesToEntities().size();\r\n        if (!timelineEntity.getId().equals(\"hello\") && !timelineEntity.getId().equals(\"hello2\")) {\r\n            Assert.fail(\"Entity ids' should have been hello and hello2\");\r\n        }\r\n    }\r\n    assertEquals(3, relatesToCnt);\r\n    TimelineFilterList rt1 = new TimelineFilterList();\r\n    rt1.addFilter(new TimelineKeyValuesFilter(TimelineCompareOp.EQUAL, \"container\", new HashSet<Object>(Arrays.asList(\"relatesto1\"))));\r\n    rt1.addFilter(new TimelineKeyValuesFilter(TimelineCompareOp.NOT_EQUAL, \"container\", new HashSet<Object>(Arrays.asList(\"relatesto3\"))));\r\n    entities = reader.getEntities(new TimelineReaderContext(\"cluster1\", \"user1\", \"some_flow_name\", 1002345678919L, \"application_1231111111_1111\", \"world\", null), new TimelineEntityFilters.Builder().relatesTo(rt1).build(), new TimelineDataToRetrieve());\r\n    assertEquals(1, entities.size());\r\n    relatesToCnt = 0;\r\n    for (TimelineEntity timelineEntity : entities) {\r\n        relatesToCnt += timelineEntity.getRelatesToEntities().size();\r\n        if (!timelineEntity.getId().equals(\"hello1\")) {\r\n            Assert.fail(\"Entity id should have been hello1\");\r\n        }\r\n    }\r\n    assertEquals(0, relatesToCnt);\r\n    TimelineFilterList rt2 = new TimelineFilterList(Operator.OR);\r\n    rt2.addFilter(new TimelineKeyValuesFilter(TimelineCompareOp.EQUAL, \"container2\", new HashSet<Object>(Arrays.asList(\"relatesto7\"))));\r\n    rt2.addFilter(new TimelineKeyValuesFilter(TimelineCompareOp.EQUAL, \"container1\", new HashSet<Object>(Arrays.asList(\"relatesto4\"))));\r\n    entities = reader.getEntities(new TimelineReaderContext(\"cluster1\", \"user1\", \"some_flow_name\", 1002345678919L, \"application_1231111111_1111\", \"world\", null), new TimelineEntityFilters.Builder().relatesTo(rt2).build(), new TimelineDataToRetrieve());\r\n    assertEquals(2, entities.size());\r\n    relatesToCnt = 0;\r\n    for (TimelineEntity timelineEntity : entities) {\r\n        relatesToCnt += timelineEntity.getRelatesToEntities().size();\r\n        if (!timelineEntity.getId().equals(\"hello\") && !timelineEntity.getId().equals(\"hello2\")) {\r\n            Assert.fail(\"Entity ids' should have been hello and hello2\");\r\n        }\r\n    }\r\n    assertEquals(0, relatesToCnt);\r\n    TimelineFilterList rt3 = new TimelineFilterList();\r\n    rt3.addFilter(new TimelineKeyValuesFilter(TimelineCompareOp.EQUAL, \"container\", new HashSet<Object>(Arrays.asList(\"relatesto1\", \"relatesto3\"))));\r\n    entities = reader.getEntities(new TimelineReaderContext(\"cluster1\", \"user1\", \"some_flow_name\", 1002345678919L, \"application_1231111111_1111\", \"world\", null), new TimelineEntityFilters.Builder().relatesTo(rt3).build(), new TimelineDataToRetrieve());\r\n    assertEquals(1, entities.size());\r\n    relatesToCnt = 0;\r\n    for (TimelineEntity timelineEntity : entities) {\r\n        relatesToCnt += timelineEntity.getRelatesToEntities().size();\r\n        if (!timelineEntity.getId().equals(\"hello\")) {\r\n            Assert.fail(\"Entity id should have been hello\");\r\n        }\r\n    }\r\n    assertEquals(0, relatesToCnt);\r\n    TimelineFilterList rt4 = new TimelineFilterList();\r\n    rt4.addFilter(new TimelineKeyValuesFilter(TimelineCompareOp.EQUAL, \"container\", new HashSet<Object>(Arrays.asList(\"relatesto1\"))));\r\n    rt4.addFilter(new TimelineKeyValuesFilter(TimelineCompareOp.EQUAL, \"dummy_container\", new HashSet<Object>(Arrays.asList(\"relatesto5\"))));\r\n    entities = reader.getEntities(new TimelineReaderContext(\"cluster1\", \"user1\", \"some_flow_name\", 1002345678919L, \"application_1231111111_1111\", \"world\", null), new TimelineEntityFilters.Builder().relatesTo(rt4).build(), new TimelineDataToRetrieve());\r\n    assertEquals(0, entities.size());\r\n    TimelineFilterList rt5 = new TimelineFilterList();\r\n    rt5.addFilter(new TimelineKeyValuesFilter(TimelineCompareOp.EQUAL, \"container\", new HashSet<Object>(Arrays.asList(\"relatedto1\", \"relatesto8\"))));\r\n    entities = reader.getEntities(new TimelineReaderContext(\"cluster1\", \"user1\", \"some_flow_name\", 1002345678919L, \"application_1231111111_1111\", \"world\", null), new TimelineEntityFilters.Builder().relatesTo(rt5).build(), new TimelineDataToRetrieve());\r\n    assertEquals(0, entities.size());\r\n    TimelineFilterList list1 = new TimelineFilterList();\r\n    list1.addFilter(new TimelineKeyValuesFilter(TimelineCompareOp.EQUAL, \"container2\", new HashSet<Object>(Arrays.asList(\"relatesto7\"))));\r\n    list1.addFilter(new TimelineKeyValuesFilter(TimelineCompareOp.EQUAL, \"dummy_container\", new HashSet<Object>(Arrays.asList(\"relatesto4\"))));\r\n    TimelineFilterList list2 = new TimelineFilterList();\r\n    list2.addFilter(new TimelineKeyValuesFilter(TimelineCompareOp.EQUAL, \"container1\", new HashSet<Object>(Arrays.asList(\"relatesto4\"))));\r\n    TimelineFilterList rt6 = new TimelineFilterList(Operator.OR, list1, list2);\r\n    entities = reader.getEntities(new TimelineReaderContext(\"cluster1\", \"user1\", \"some_flow_name\", 1002345678919L, \"application_1231111111_1111\", \"world\", null), new TimelineEntityFilters.Builder().relatesTo(rt6).build(), new TimelineDataToRetrieve());\r\n    assertEquals(1, entities.size());\r\n    relatesToCnt = 0;\r\n    for (TimelineEntity timelineEntity : entities) {\r\n        relatesToCnt += timelineEntity.getRelatesToEntities().size();\r\n        if (!timelineEntity.getId().equals(\"hello\")) {\r\n            Assert.fail(\"Entity id should have been hello\");\r\n        }\r\n    }\r\n    assertEquals(0, relatesToCnt);\r\n    TimelineFilterList list3 = new TimelineFilterList();\r\n    list3.addFilter(new TimelineKeyValuesFilter(TimelineCompareOp.EQUAL, \"container\", new HashSet<Object>(Arrays.asList(\"relatesto1\"))));\r\n    list3.addFilter(new TimelineKeyValuesFilter(TimelineCompareOp.EQUAL, \"container1\", new HashSet<Object>(Arrays.asList(\"relatesto4\"))));\r\n    TimelineFilterList list4 = new TimelineFilterList();\r\n    list4.addFilter(new TimelineKeyValuesFilter(TimelineCompareOp.EQUAL, \"container\", new HashSet<Object>(Arrays.asList(\"relatesto1\"))));\r\n    list4.addFilter(new TimelineKeyValuesFilter(TimelineCompareOp.EQUAL, \"container\", new HashSet<Object>(Arrays.asList(\"relatesto2\"))));\r\n    TimelineFilterList combinedList = new TimelineFilterList(Operator.OR, list3, list4);\r\n    TimelineFilterList rt7 = new TimelineFilterList(Operator.AND, combinedList, new TimelineKeyValuesFilter(TimelineCompareOp.NOT_EQUAL, \"container\", new HashSet<Object>(Arrays.asList(\"relatesto3\"))));\r\n    entities = reader.getEntities(new TimelineReaderContext(\"cluster1\", \"user1\", \"some_flow_name\", 1002345678919L, \"application_1231111111_1111\", \"world\", null), new TimelineEntityFilters.Builder().relatesTo(rt7).build(), new TimelineDataToRetrieve());\r\n    assertEquals(1, entities.size());\r\n    relatesToCnt = 0;\r\n    for (TimelineEntity timelineEntity : entities) {\r\n        relatesToCnt += timelineEntity.getRelatesToEntities().size();\r\n        if (!timelineEntity.getId().equals(\"hello1\")) {\r\n            Assert.fail(\"Entity id should have been hello1\");\r\n        }\r\n    }\r\n    assertEquals(0, relatesToCnt);\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-yarn-project\\hadoop-yarn\\hadoop-yarn-server\\hadoop-yarn-server-timelineservice-hbase-tests\\src\\test\\java\\org\\apache\\hadoop\\yarn\\server\\timelineservice\\storage",
  "methodName" : "testReadEntitiesDefaultView",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void testReadEntitiesDefaultView() throws Exception\n{\r\n    TimelineEntity e1 = reader.getEntity(new TimelineReaderContext(\"cluster1\", \"user1\", \"some_flow_name\", 1002345678919L, \"application_1231111111_1111\", \"world\", \"hello\"), new TimelineDataToRetrieve());\r\n    assertNotNull(e1);\r\n    assertEquals(1, e1.getInfo().size());\r\n    assertTrue(e1.getConfigs().isEmpty() && e1.getMetrics().isEmpty() && e1.getIsRelatedToEntities().isEmpty() && e1.getRelatesToEntities().isEmpty());\r\n    Set<TimelineEntity> es1 = reader.getEntities(new TimelineReaderContext(\"cluster1\", \"user1\", \"some_flow_name\", 1002345678919L, \"application_1231111111_1111\", \"world\", null), new TimelineEntityFilters.Builder().build(), new TimelineDataToRetrieve());\r\n    assertEquals(3, es1.size());\r\n    for (TimelineEntity e : es1) {\r\n        assertTrue(e.getConfigs().isEmpty() && e.getMetrics().isEmpty() && e.getIsRelatedToEntities().isEmpty() && e.getRelatesToEntities().isEmpty());\r\n        assertEquals(1, e.getInfo().size());\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-yarn-project\\hadoop-yarn\\hadoop-yarn-server\\hadoop-yarn-server-timelineservice-hbase-tests\\src\\test\\java\\org\\apache\\hadoop\\yarn\\server\\timelineservice\\storage",
  "methodName" : "testReadEntitiesByFields",
  "errType" : null,
  "containingMethodsNum" : 12,
  "sourceCodeText" : "void testReadEntitiesByFields() throws Exception\n{\r\n    TimelineEntity e1 = reader.getEntity(new TimelineReaderContext(\"cluster1\", \"user1\", \"some_flow_name\", 1002345678919L, \"application_1231111111_1111\", \"world\", \"hello\"), new TimelineDataToRetrieve(null, null, EnumSet.of(Field.INFO, Field.CONFIGS), null, null, null));\r\n    assertNotNull(e1);\r\n    assertEquals(3, e1.getConfigs().size());\r\n    assertEquals(0, e1.getIsRelatedToEntities().size());\r\n    Set<TimelineEntity> es1 = reader.getEntities(new TimelineReaderContext(\"cluster1\", \"user1\", \"some_flow_name\", 1002345678919L, \"application_1231111111_1111\", \"world\", null), new TimelineEntityFilters.Builder().build(), new TimelineDataToRetrieve(null, null, EnumSet.of(Field.IS_RELATED_TO, Field.METRICS), null, null, null));\r\n    assertEquals(3, es1.size());\r\n    int metricsCnt = 0;\r\n    int isRelatedToCnt = 0;\r\n    int infoCnt = 0;\r\n    for (TimelineEntity entity : es1) {\r\n        metricsCnt += entity.getMetrics().size();\r\n        isRelatedToCnt += entity.getIsRelatedToEntities().size();\r\n        infoCnt += entity.getInfo().size();\r\n    }\r\n    assertEquals(3, infoCnt);\r\n    assertEquals(4, isRelatedToCnt);\r\n    assertEquals(3, metricsCnt);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-yarn-project\\hadoop-yarn\\hadoop-yarn-server\\hadoop-yarn-server-timelineservice-hbase-tests\\src\\test\\java\\org\\apache\\hadoop\\yarn\\server\\timelineservice\\storage",
  "methodName" : "testReadEntitiesConfigPrefix",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void testReadEntitiesConfigPrefix() throws Exception\n{\r\n    TimelineFilterList list = new TimelineFilterList(Operator.OR, new TimelinePrefixFilter(TimelineCompareOp.EQUAL, \"cfg_\"));\r\n    TimelineEntity e1 = reader.getEntity(new TimelineReaderContext(\"cluster1\", \"user1\", \"some_flow_name\", 1002345678919L, \"application_1231111111_1111\", \"world\", \"hello\"), new TimelineDataToRetrieve(list, null, null, null, null, null));\r\n    assertNotNull(e1);\r\n    assertEquals(1, e1.getConfigs().size());\r\n    Set<TimelineEntity> es1 = reader.getEntities(new TimelineReaderContext(\"cluster1\", \"user1\", \"some_flow_name\", 1002345678919L, \"application_1231111111_1111\", \"world\", null), new TimelineEntityFilters.Builder().build(), new TimelineDataToRetrieve(list, null, null, null, null, null));\r\n    int cfgCnt = 0;\r\n    for (TimelineEntity entity : es1) {\r\n        cfgCnt += entity.getConfigs().size();\r\n        for (String confKey : entity.getConfigs().keySet()) {\r\n            assertTrue(\"Config key returned should start with cfg_\", confKey.startsWith(\"cfg_\"));\r\n        }\r\n    }\r\n    assertEquals(3, cfgCnt);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-yarn-project\\hadoop-yarn\\hadoop-yarn-server\\hadoop-yarn-server-timelineservice-hbase-tests\\src\\test\\java\\org\\apache\\hadoop\\yarn\\server\\timelineservice\\storage",
  "methodName" : "testReadEntitiesConfigFilters",
  "errType" : null,
  "containingMethodsNum" : 24,
  "sourceCodeText" : "void testReadEntitiesConfigFilters() throws Exception\n{\r\n    TimelineFilterList list1 = new TimelineFilterList();\r\n    list1.addFilter(new TimelineKeyValueFilter(TimelineCompareOp.EQUAL, \"cfg_param1\", \"value1\"));\r\n    list1.addFilter(new TimelineKeyValueFilter(TimelineCompareOp.EQUAL, \"cfg_param2\", \"value2\"));\r\n    TimelineFilterList list2 = new TimelineFilterList();\r\n    list2.addFilter(new TimelineKeyValueFilter(TimelineCompareOp.EQUAL, \"cfg_param1\", \"value3\"));\r\n    list2.addFilter(new TimelineKeyValueFilter(TimelineCompareOp.EQUAL, \"config_param2\", \"value2\"));\r\n    TimelineFilterList confFilterList = new TimelineFilterList(Operator.OR, list1, list2);\r\n    Set<TimelineEntity> entities = reader.getEntities(new TimelineReaderContext(\"cluster1\", \"user1\", \"some_flow_name\", 1002345678919L, \"application_1231111111_1111\", \"world\", null), new TimelineEntityFilters.Builder().configFilters(confFilterList).build(), new TimelineDataToRetrieve(null, null, EnumSet.of(Field.CONFIGS), null, null, null));\r\n    assertEquals(2, entities.size());\r\n    int cfgCnt = 0;\r\n    for (TimelineEntity entity : entities) {\r\n        cfgCnt += entity.getConfigs().size();\r\n    }\r\n    assertEquals(5, cfgCnt);\r\n    entities = reader.getEntities(new TimelineReaderContext(\"cluster1\", \"user1\", \"some_flow_name\", 1002345678919L, \"application_1231111111_1111\", \"world\", null), new TimelineEntityFilters.Builder().configFilters(confFilterList).build(), new TimelineDataToRetrieve(null, null, EnumSet.of(Field.ALL), null, null, null));\r\n    assertEquals(2, entities.size());\r\n    cfgCnt = 0;\r\n    for (TimelineEntity entity : entities) {\r\n        cfgCnt += entity.getConfigs().size();\r\n    }\r\n    assertEquals(5, cfgCnt);\r\n    TimelineFilterList confFilterList1 = new TimelineFilterList(new TimelineKeyValueFilter(TimelineCompareOp.NOT_EQUAL, \"cfg_param1\", \"value1\"));\r\n    entities = reader.getEntities(new TimelineReaderContext(\"cluster1\", \"user1\", \"some_flow_name\", 1002345678919L, \"application_1231111111_1111\", \"world\", null), new TimelineEntityFilters.Builder().configFilters(confFilterList1).build(), new TimelineDataToRetrieve(null, null, EnumSet.of(Field.CONFIGS), null, null, null));\r\n    assertEquals(1, entities.size());\r\n    cfgCnt = 0;\r\n    for (TimelineEntity entity : entities) {\r\n        cfgCnt += entity.getConfigs().size();\r\n    }\r\n    assertEquals(3, cfgCnt);\r\n    TimelineFilterList confFilterList2 = new TimelineFilterList(new TimelineKeyValueFilter(TimelineCompareOp.NOT_EQUAL, \"cfg_param1\", \"value1\"), new TimelineKeyValueFilter(TimelineCompareOp.NOT_EQUAL, \"config_param2\", \"value2\"));\r\n    entities = reader.getEntities(new TimelineReaderContext(\"cluster1\", \"user1\", \"some_flow_name\", 1002345678919L, \"application_1231111111_1111\", \"world\", null), new TimelineEntityFilters.Builder().configFilters(confFilterList2).build(), new TimelineDataToRetrieve(null, null, EnumSet.of(Field.CONFIGS), null, null, null));\r\n    assertEquals(0, entities.size());\r\n    TimelineFilterList confFilterList3 = new TimelineFilterList(new TimelineKeyValueFilter(TimelineCompareOp.EQUAL, \"dummy_config\", \"value1\"));\r\n    entities = reader.getEntities(new TimelineReaderContext(\"cluster1\", \"user1\", \"some_flow_name\", 1002345678919L, \"application_1231111111_1111\", \"world\", null), new TimelineEntityFilters.Builder().configFilters(confFilterList3).build(), new TimelineDataToRetrieve(null, null, EnumSet.of(Field.CONFIGS), null, null, null));\r\n    assertEquals(0, entities.size());\r\n    TimelineFilterList confFilterList4 = new TimelineFilterList(new TimelineKeyValueFilter(TimelineCompareOp.NOT_EQUAL, \"dummy_config\", \"value1\"));\r\n    entities = reader.getEntities(new TimelineReaderContext(\"cluster1\", \"user1\", \"some_flow_name\", 1002345678919L, \"application_1231111111_1111\", \"world\", null), new TimelineEntityFilters.Builder().configFilters(confFilterList4).build(), new TimelineDataToRetrieve(null, null, EnumSet.of(Field.CONFIGS), null, null, null));\r\n    assertEquals(0, entities.size());\r\n    TimelineFilterList confFilterList5 = new TimelineFilterList(new TimelineKeyValueFilter(TimelineCompareOp.NOT_EQUAL, \"dummy_config\", \"value1\", false));\r\n    entities = reader.getEntities(new TimelineReaderContext(\"cluster1\", \"user1\", \"some_flow_name\", 1002345678919L, \"application_1231111111_1111\", \"world\", null), new TimelineEntityFilters.Builder().configFilters(confFilterList5).build(), new TimelineDataToRetrieve(null, null, EnumSet.of(Field.CONFIGS), null, null, null));\r\n    assertEquals(3, entities.size());\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-yarn-project\\hadoop-yarn\\hadoop-yarn-server\\hadoop-yarn-server-timelineservice-hbase-tests\\src\\test\\java\\org\\apache\\hadoop\\yarn\\server\\timelineservice\\storage",
  "methodName" : "testReadEntitiesConfigFilterPrefix",
  "errType" : null,
  "containingMethodsNum" : 17,
  "sourceCodeText" : "void testReadEntitiesConfigFilterPrefix() throws Exception\n{\r\n    TimelineFilterList confFilterList = new TimelineFilterList();\r\n    confFilterList.addFilter(new TimelineKeyValueFilter(TimelineCompareOp.EQUAL, \"cfg_param1\", \"value1\"));\r\n    TimelineFilterList list = new TimelineFilterList(Operator.OR, new TimelinePrefixFilter(TimelineCompareOp.EQUAL, \"cfg_\"));\r\n    Set<TimelineEntity> entities = reader.getEntities(new TimelineReaderContext(\"cluster1\", \"user1\", \"some_flow_name\", 1002345678919L, \"application_1231111111_1111\", \"world\", null), new TimelineEntityFilters.Builder().configFilters(confFilterList).build(), new TimelineDataToRetrieve(list, null, null, null, null, null));\r\n    assertEquals(1, entities.size());\r\n    int cfgCnt = 0;\r\n    for (TimelineEntity entity : entities) {\r\n        cfgCnt += entity.getConfigs().size();\r\n        for (String confKey : entity.getConfigs().keySet()) {\r\n            assertTrue(\"Config key returned should start with cfg_\", confKey.startsWith(\"cfg_\"));\r\n        }\r\n    }\r\n    assertEquals(2, cfgCnt);\r\n    TimelineFilterList list1 = new TimelineFilterList();\r\n    list1.addFilter(new TimelineKeyValueFilter(TimelineCompareOp.EQUAL, \"cfg_param1\", \"value1\"));\r\n    list1.addFilter(new TimelineKeyValueFilter(TimelineCompareOp.EQUAL, \"cfg_param2\", \"value2\"));\r\n    TimelineFilterList list2 = new TimelineFilterList();\r\n    list2.addFilter(new TimelineKeyValueFilter(TimelineCompareOp.EQUAL, \"cfg_param1\", \"value3\"));\r\n    list2.addFilter(new TimelineKeyValueFilter(TimelineCompareOp.EQUAL, \"config_param2\", \"value2\"));\r\n    TimelineFilterList confFilterList1 = new TimelineFilterList(Operator.OR, list1, list2);\r\n    TimelineFilterList confsToRetrieve = new TimelineFilterList(Operator.OR, new TimelinePrefixFilter(TimelineCompareOp.EQUAL, \"config_\"));\r\n    entities = reader.getEntities(new TimelineReaderContext(\"cluster1\", \"user1\", \"some_flow_name\", 1002345678919L, \"application_1231111111_1111\", \"world\", null), new TimelineEntityFilters.Builder().configFilters(confFilterList1).build(), new TimelineDataToRetrieve(confsToRetrieve, null, null, null, null, null));\r\n    assertEquals(2, entities.size());\r\n    cfgCnt = 0;\r\n    for (TimelineEntity entity : entities) {\r\n        cfgCnt += entity.getConfigs().size();\r\n        for (String confKey : entity.getConfigs().keySet()) {\r\n            assertTrue(\"Config key returned should start with config_\", confKey.startsWith(\"config_\"));\r\n        }\r\n    }\r\n    assertEquals(2, cfgCnt);\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-yarn-project\\hadoop-yarn\\hadoop-yarn-server\\hadoop-yarn-server-timelineservice-hbase-tests\\src\\test\\java\\org\\apache\\hadoop\\yarn\\server\\timelineservice\\storage",
  "methodName" : "testReadEntitiesMetricPrefix",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void testReadEntitiesMetricPrefix() throws Exception\n{\r\n    TimelineFilterList list = new TimelineFilterList(Operator.OR, new TimelinePrefixFilter(TimelineCompareOp.EQUAL, \"MAP1_\"));\r\n    TimelineEntity e1 = reader.getEntity(new TimelineReaderContext(\"cluster1\", \"user1\", \"some_flow_name\", 1002345678919L, \"application_1231111111_1111\", \"world\", \"hello\"), new TimelineDataToRetrieve(null, list, null, null, null, null));\r\n    assertNotNull(e1);\r\n    assertEquals(1, e1.getMetrics().size());\r\n    Set<TimelineEntity> es1 = reader.getEntities(new TimelineReaderContext(\"cluster1\", \"user1\", \"some_flow_name\", 1002345678919L, \"application_1231111111_1111\", \"world\", null), new TimelineEntityFilters.Builder().build(), new TimelineDataToRetrieve(null, list, null, null, null, null));\r\n    int metricCnt = 0;\r\n    for (TimelineEntity entity : es1) {\r\n        metricCnt += entity.getMetrics().size();\r\n        for (TimelineMetric metric : entity.getMetrics()) {\r\n            assertTrue(\"Metric Id returned should start with MAP1_\", metric.getId().startsWith(\"MAP1_\"));\r\n        }\r\n    }\r\n    assertEquals(2, metricCnt);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-yarn-project\\hadoop-yarn\\hadoop-yarn-server\\hadoop-yarn-server-timelineservice-hbase-tests\\src\\test\\java\\org\\apache\\hadoop\\yarn\\server\\timelineservice\\storage",
  "methodName" : "testReadEntitiesMetricTimeRange",
  "errType" : null,
  "containingMethodsNum" : 24,
  "sourceCodeText" : "void testReadEntitiesMetricTimeRange() throws Exception\n{\r\n    Set<TimelineEntity> entities = reader.getEntities(new TimelineReaderContext(\"cluster1\", \"user1\", \"some_flow_name\", 1002345678919L, \"application_1231111111_1111\", \"world\", null), new TimelineEntityFilters.Builder().build(), new TimelineDataToRetrieve(null, null, EnumSet.of(Field.METRICS), 100, null, null));\r\n    assertEquals(3, entities.size());\r\n    int metricTimeSeriesCnt = 0;\r\n    int metricCnt = 0;\r\n    for (TimelineEntity entity : entities) {\r\n        metricCnt += entity.getMetrics().size();\r\n        for (TimelineMetric m : entity.getMetrics()) {\r\n            metricTimeSeriesCnt += m.getValues().size();\r\n        }\r\n    }\r\n    assertEquals(3, metricCnt);\r\n    assertEquals(13, metricTimeSeriesCnt);\r\n    entities = reader.getEntities(new TimelineReaderContext(\"cluster1\", \"user1\", \"some_flow_name\", 1002345678919L, \"application_1231111111_1111\", \"world\", null), new TimelineEntityFilters.Builder().build(), new TimelineDataToRetrieve(null, null, EnumSet.of(Field.METRICS), 100, CURRENT_TIME - 40000, CURRENT_TIME));\r\n    assertEquals(3, entities.size());\r\n    metricCnt = 0;\r\n    metricTimeSeriesCnt = 0;\r\n    for (TimelineEntity entity : entities) {\r\n        metricCnt += entity.getMetrics().size();\r\n        for (TimelineMetric m : entity.getMetrics()) {\r\n            for (Long ts : m.getValues().keySet()) {\r\n                assertTrue(ts >= CURRENT_TIME - 40000 && ts <= CURRENT_TIME);\r\n            }\r\n            metricTimeSeriesCnt += m.getValues().size();\r\n        }\r\n    }\r\n    assertEquals(3, metricCnt);\r\n    assertEquals(5, metricTimeSeriesCnt);\r\n    TimelineEntity entity = reader.getEntity(new TimelineReaderContext(\"cluster1\", \"user1\", \"some_flow_name\", 1002345678919L, \"application_1231111111_1111\", \"world\", \"hello\"), new TimelineDataToRetrieve(null, null, EnumSet.of(Field.METRICS), 100, CURRENT_TIME - 40000, CURRENT_TIME));\r\n    assertNotNull(entity);\r\n    assertEquals(2, entity.getMetrics().size());\r\n    metricTimeSeriesCnt = 0;\r\n    for (TimelineMetric m : entity.getMetrics()) {\r\n        for (Long ts : m.getValues().keySet()) {\r\n            assertTrue(ts >= CURRENT_TIME - 40000 && ts <= CURRENT_TIME);\r\n        }\r\n        metricTimeSeriesCnt += m.getValues().size();\r\n    }\r\n    assertEquals(3, metricTimeSeriesCnt);\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-yarn-project\\hadoop-yarn\\hadoop-yarn-server\\hadoop-yarn-server-timelineservice-hbase-tests\\src\\test\\java\\org\\apache\\hadoop\\yarn\\server\\timelineservice\\storage",
  "methodName" : "testReadEntitiesMetricFilters",
  "errType" : null,
  "containingMethodsNum" : 23,
  "sourceCodeText" : "void testReadEntitiesMetricFilters() throws Exception\n{\r\n    TimelineFilterList list1 = new TimelineFilterList();\r\n    list1.addFilter(new TimelineCompareFilter(TimelineCompareOp.GREATER_OR_EQUAL, \"MAP1_SLOT_MILLIS\", 50000000900L));\r\n    TimelineFilterList list2 = new TimelineFilterList();\r\n    list2.addFilter(new TimelineCompareFilter(TimelineCompareOp.LESS_THAN, \"MAP_SLOT_MILLIS\", 80000000000L));\r\n    list2.addFilter(new TimelineCompareFilter(TimelineCompareOp.EQUAL, \"MAP1_BYTES\", 50));\r\n    TimelineFilterList metricFilterList = new TimelineFilterList(Operator.OR, list1, list2);\r\n    Set<TimelineEntity> entities = reader.getEntities(new TimelineReaderContext(\"cluster1\", \"user1\", \"some_flow_name\", 1002345678919L, \"application_1231111111_1111\", \"world\", null), new TimelineEntityFilters.Builder().metricFilters(metricFilterList).build(), new TimelineDataToRetrieve(null, null, EnumSet.of(Field.METRICS), null, null, null));\r\n    assertEquals(2, entities.size());\r\n    int metricCnt = 0;\r\n    for (TimelineEntity entity : entities) {\r\n        metricCnt += entity.getMetrics().size();\r\n    }\r\n    assertEquals(3, metricCnt);\r\n    entities = reader.getEntities(new TimelineReaderContext(\"cluster1\", \"user1\", \"some_flow_name\", 1002345678919L, \"application_1231111111_1111\", \"world\", null), new TimelineEntityFilters.Builder().metricFilters(metricFilterList).build(), new TimelineDataToRetrieve(null, null, EnumSet.of(Field.ALL), null, null, null));\r\n    assertEquals(2, entities.size());\r\n    metricCnt = 0;\r\n    for (TimelineEntity entity : entities) {\r\n        metricCnt += entity.getMetrics().size();\r\n    }\r\n    assertEquals(3, metricCnt);\r\n    TimelineFilterList metricFilterList1 = new TimelineFilterList(new TimelineCompareFilter(TimelineCompareOp.LESS_OR_EQUAL, \"MAP_SLOT_MILLIS\", 80000000000L), new TimelineCompareFilter(TimelineCompareOp.NOT_EQUAL, \"MAP1_BYTES\", 30));\r\n    entities = reader.getEntities(new TimelineReaderContext(\"cluster1\", \"user1\", \"some_flow_name\", 1002345678919L, \"application_1231111111_1111\", \"world\", null), new TimelineEntityFilters.Builder().metricFilters(metricFilterList1).build(), new TimelineDataToRetrieve(null, null, EnumSet.of(Field.METRICS), null, null, null));\r\n    assertEquals(1, entities.size());\r\n    metricCnt = 0;\r\n    for (TimelineEntity entity : entities) {\r\n        metricCnt += entity.getMetrics().size();\r\n    }\r\n    assertEquals(2, metricCnt);\r\n    TimelineFilterList metricFilterList2 = new TimelineFilterList(new TimelineCompareFilter(TimelineCompareOp.LESS_THAN, \"MAP_SLOT_MILLIS\", 40000000000L), new TimelineCompareFilter(TimelineCompareOp.NOT_EQUAL, \"MAP1_BYTES\", 30));\r\n    entities = reader.getEntities(new TimelineReaderContext(\"cluster1\", \"user1\", \"some_flow_name\", 1002345678919L, \"application_1231111111_1111\", \"world\", null), new TimelineEntityFilters.Builder().metricFilters(metricFilterList2).build(), new TimelineDataToRetrieve(null, null, EnumSet.of(Field.METRICS), null, null, null));\r\n    assertEquals(0, entities.size());\r\n    TimelineFilterList metricFilterList3 = new TimelineFilterList(new TimelineCompareFilter(TimelineCompareOp.EQUAL, \"dummy_metric\", 5));\r\n    entities = reader.getEntities(new TimelineReaderContext(\"cluster1\", \"user1\", \"some_flow_name\", 1002345678919L, \"application_1231111111_1111\", \"world\", null), new TimelineEntityFilters.Builder().metricFilters(metricFilterList3).build(), new TimelineDataToRetrieve(null, null, EnumSet.of(Field.METRICS), null, null, null));\r\n    assertEquals(0, entities.size());\r\n    TimelineFilterList metricFilterList4 = new TimelineFilterList(new TimelineCompareFilter(TimelineCompareOp.NOT_EQUAL, \"dummy_metric\", 5));\r\n    entities = reader.getEntities(new TimelineReaderContext(\"cluster1\", \"user1\", \"some_flow_name\", 1002345678919L, \"application_1231111111_1111\", \"world\", null), new TimelineEntityFilters.Builder().metricFilters(metricFilterList4).build(), new TimelineDataToRetrieve(null, null, EnumSet.of(Field.METRICS), null, null, null));\r\n    assertEquals(0, entities.size());\r\n    TimelineFilterList metricFilterList5 = new TimelineFilterList(new TimelineCompareFilter(TimelineCompareOp.NOT_EQUAL, \"dummy_metric\", 5, false));\r\n    entities = reader.getEntities(new TimelineReaderContext(\"cluster1\", \"user1\", \"some_flow_name\", 1002345678919L, \"application_1231111111_1111\", \"world\", null), new TimelineEntityFilters.Builder().metricFilters(metricFilterList5).build(), new TimelineDataToRetrieve(null, null, EnumSet.of(Field.METRICS), null, null, null));\r\n    assertEquals(3, entities.size());\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-yarn-project\\hadoop-yarn\\hadoop-yarn-server\\hadoop-yarn-server-timelineservice-hbase-tests\\src\\test\\java\\org\\apache\\hadoop\\yarn\\server\\timelineservice\\storage",
  "methodName" : "testReadEntitiesMetricFilterPrefix",
  "errType" : null,
  "containingMethodsNum" : 26,
  "sourceCodeText" : "void testReadEntitiesMetricFilterPrefix() throws Exception\n{\r\n    TimelineFilterList metricFilterList = new TimelineFilterList();\r\n    metricFilterList.addFilter(new TimelineCompareFilter(TimelineCompareOp.GREATER_OR_EQUAL, \"MAP1_SLOT_MILLIS\", 0L));\r\n    TimelineFilterList list = new TimelineFilterList(Operator.OR, new TimelinePrefixFilter(TimelineCompareOp.EQUAL, \"MAP1_\"));\r\n    Set<TimelineEntity> entities = reader.getEntities(new TimelineReaderContext(\"cluster1\", \"user1\", \"some_flow_name\", 1002345678919L, \"application_1231111111_1111\", \"world\", null), new TimelineEntityFilters.Builder().metricFilters(metricFilterList).build(), new TimelineDataToRetrieve(null, list, null, null, null, null));\r\n    assertEquals(1, entities.size());\r\n    int metricCnt = 0;\r\n    for (TimelineEntity entity : entities) {\r\n        metricCnt += entity.getMetrics().size();\r\n        for (TimelineMetric metric : entity.getMetrics()) {\r\n            assertTrue(\"Metric Id returned should start with MAP1_\", metric.getId().startsWith(\"MAP1_\"));\r\n        }\r\n    }\r\n    assertEquals(1, metricCnt);\r\n    TimelineFilterList list1 = new TimelineFilterList();\r\n    list1.addFilter(new TimelineCompareFilter(TimelineCompareOp.GREATER_OR_EQUAL, \"MAP1_SLOT_MILLIS\", 50000000900L));\r\n    TimelineFilterList list2 = new TimelineFilterList();\r\n    list2.addFilter(new TimelineCompareFilter(TimelineCompareOp.LESS_THAN, \"MAP_SLOT_MILLIS\", 80000000000L));\r\n    list2.addFilter(new TimelineCompareFilter(TimelineCompareOp.EQUAL, \"MAP1_BYTES\", 50));\r\n    TimelineFilterList metricFilterList1 = new TimelineFilterList(Operator.OR, list1, list2);\r\n    TimelineFilterList metricsToRetrieve = new TimelineFilterList(Operator.OR, new TimelinePrefixFilter(TimelineCompareOp.EQUAL, \"MAP1_\"));\r\n    entities = reader.getEntities(new TimelineReaderContext(\"cluster1\", \"user1\", \"some_flow_name\", 1002345678919L, \"application_1231111111_1111\", \"world\", null), new TimelineEntityFilters.Builder().metricFilters(metricFilterList1).build(), new TimelineDataToRetrieve(null, metricsToRetrieve, EnumSet.of(Field.METRICS), null, null, null));\r\n    assertEquals(2, entities.size());\r\n    metricCnt = 0;\r\n    for (TimelineEntity entity : entities) {\r\n        metricCnt += entity.getMetrics().size();\r\n        for (TimelineMetric metric : entity.getMetrics()) {\r\n            assertEquals(TimelineMetric.Type.SINGLE_VALUE, metric.getType());\r\n            assertEquals(1, metric.getValues().size());\r\n            assertTrue(\"Metric Id returned should start with MAP1_\", metric.getId().startsWith(\"MAP1_\"));\r\n        }\r\n    }\r\n    assertEquals(2, metricCnt);\r\n    entities = reader.getEntities(new TimelineReaderContext(\"cluster1\", \"user1\", \"some_flow_name\", 1002345678919L, \"application_1231111111_1111\", \"world\", null), new TimelineEntityFilters.Builder().metricFilters(metricFilterList1).build(), new TimelineDataToRetrieve(null, metricsToRetrieve, EnumSet.of(Field.METRICS), Integer.MAX_VALUE, null, null));\r\n    assertEquals(2, entities.size());\r\n    metricCnt = 0;\r\n    int metricValCnt = 0;\r\n    for (TimelineEntity entity : entities) {\r\n        metricCnt += entity.getMetrics().size();\r\n        for (TimelineMetric metric : entity.getMetrics()) {\r\n            metricValCnt += metric.getValues().size();\r\n            assertTrue(\"Metric Id returned should start with MAP1_\", metric.getId().startsWith(\"MAP1_\"));\r\n        }\r\n    }\r\n    assertEquals(2, metricCnt);\r\n    assertEquals(7, metricValCnt);\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-yarn-project\\hadoop-yarn\\hadoop-yarn-server\\hadoop-yarn-server-timelineservice-hbase-tests\\src\\test\\java\\org\\apache\\hadoop\\yarn\\server\\timelineservice\\storage",
  "methodName" : "testReadEntitiesInfoFilters",
  "errType" : null,
  "containingMethodsNum" : 20,
  "sourceCodeText" : "void testReadEntitiesInfoFilters() throws Exception\n{\r\n    TimelineFilterList list1 = new TimelineFilterList();\r\n    list1.addFilter(new TimelineKeyValueFilter(TimelineCompareOp.EQUAL, \"infoMapKey3\", 71.4));\r\n    list1.addFilter(new TimelineKeyValueFilter(TimelineCompareOp.EQUAL, \"infoMapKey1\", \"infoMapValue2\"));\r\n    TimelineFilterList list2 = new TimelineFilterList();\r\n    list2.addFilter(new TimelineKeyValueFilter(TimelineCompareOp.EQUAL, \"infoMapKey1\", \"infoMapValue1\"));\r\n    list2.addFilter(new TimelineKeyValueFilter(TimelineCompareOp.EQUAL, \"infoMapKey2\", 10));\r\n    TimelineFilterList infoFilterList = new TimelineFilterList(Operator.OR, list1, list2);\r\n    Set<TimelineEntity> entities = reader.getEntities(new TimelineReaderContext(\"cluster1\", \"user1\", \"some_flow_name\", 1002345678919L, \"application_1231111111_1111\", \"world\", null), new TimelineEntityFilters.Builder().infoFilters(infoFilterList).build(), new TimelineDataToRetrieve(null, null, EnumSet.of(Field.INFO), null, null, null));\r\n    assertEquals(2, entities.size());\r\n    int infoCnt = 0;\r\n    for (TimelineEntity entity : entities) {\r\n        infoCnt += entity.getInfo().size();\r\n    }\r\n    assertEquals(7, infoCnt);\r\n    TimelineFilterList infoFilterList1 = new TimelineFilterList(new TimelineKeyValueFilter(TimelineCompareOp.NOT_EQUAL, \"infoMapKey1\", \"infoMapValue1\"));\r\n    entities = reader.getEntities(new TimelineReaderContext(\"cluster1\", \"user1\", \"some_flow_name\", 1002345678919L, \"application_1231111111_1111\", \"world\", null), new TimelineEntityFilters.Builder().infoFilters(infoFilterList1).build(), new TimelineDataToRetrieve(null, null, EnumSet.of(Field.INFO), null, null, null));\r\n    assertEquals(1, entities.size());\r\n    infoCnt = 0;\r\n    for (TimelineEntity entity : entities) {\r\n        infoCnt += entity.getInfo().size();\r\n    }\r\n    assertEquals(4, infoCnt);\r\n    TimelineFilterList infoFilterList2 = new TimelineFilterList(new TimelineKeyValueFilter(TimelineCompareOp.NOT_EQUAL, \"infoMapKey1\", \"infoMapValue2\"), new TimelineKeyValueFilter(TimelineCompareOp.NOT_EQUAL, \"infoMapKey3\", 71.4));\r\n    entities = reader.getEntities(new TimelineReaderContext(\"cluster1\", \"user1\", \"some_flow_name\", 1002345678919L, \"application_1231111111_1111\", \"world\", null), new TimelineEntityFilters.Builder().infoFilters(infoFilterList2).build(), new TimelineDataToRetrieve(null, null, EnumSet.of(Field.INFO), null, null, null));\r\n    assertEquals(0, entities.size());\r\n    TimelineFilterList infoFilterList3 = new TimelineFilterList(new TimelineKeyValueFilter(TimelineCompareOp.EQUAL, \"dummy_info\", \"some_value\"));\r\n    entities = reader.getEntities(new TimelineReaderContext(\"cluster1\", \"user1\", \"some_flow_name\", 1002345678919L, \"application_1231111111_1111\", \"world\", null), new TimelineEntityFilters.Builder().infoFilters(infoFilterList3).build(), new TimelineDataToRetrieve(null, null, EnumSet.of(Field.INFO), null, null, null));\r\n    assertEquals(0, entities.size());\r\n    TimelineFilterList infoFilterList4 = new TimelineFilterList(new TimelineKeyValueFilter(TimelineCompareOp.NOT_EQUAL, \"dummy_info\", \"some_value\"));\r\n    entities = reader.getEntities(new TimelineReaderContext(\"cluster1\", \"user1\", \"some_flow_name\", 1002345678919L, \"application_1231111111_1111\", \"world\", null), new TimelineEntityFilters.Builder().infoFilters(infoFilterList4).build(), new TimelineDataToRetrieve(null, null, EnumSet.of(Field.INFO), null, null, null));\r\n    assertEquals(0, entities.size());\r\n    TimelineFilterList infoFilterList5 = new TimelineFilterList(new TimelineKeyValueFilter(TimelineCompareOp.NOT_EQUAL, \"dummy_info\", \"some_value\", false));\r\n    entities = reader.getEntities(new TimelineReaderContext(\"cluster1\", \"user1\", \"some_flow_name\", 1002345678919L, \"application_1231111111_1111\", \"world\", null), new TimelineEntityFilters.Builder().infoFilters(infoFilterList5).build(), new TimelineDataToRetrieve(null, null, EnumSet.of(Field.INFO), null, null, null));\r\n    assertEquals(3, entities.size());\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-yarn-project\\hadoop-yarn\\hadoop-yarn-server\\hadoop-yarn-server-timelineservice-hbase-tests\\src\\test\\java\\org\\apache\\hadoop\\yarn\\server\\timelineservice\\storage",
  "methodName" : "testListTypesInApp",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void testListTypesInApp() throws Exception\n{\r\n    Set<String> types = reader.getEntityTypes(new TimelineReaderContext(\"cluster1\", \"user1\", \"some_flow_name\", 1002345678919L, \"application_1231111111_1111\", null, null));\r\n    assertEquals(4, types.size());\r\n    types = reader.getEntityTypes(new TimelineReaderContext(\"cluster1\", null, null, null, \"application_1231111111_1111\", null, null));\r\n    assertEquals(4, types.size());\r\n    types = reader.getEntityTypes(new TimelineReaderContext(\"cluster1\", null, null, null, \"application_1231111111_1112\", null, null));\r\n    assertEquals(4, types.size());\r\n    types = reader.getEntityTypes(new TimelineReaderContext(\"cluster1\", \"user1\", \"some_flow_name\", 1002345678919L, \"application_1231111111_1113\", null, null));\r\n    assertEquals(0, types.size());\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-yarn-project\\hadoop-yarn\\hadoop-yarn-server\\hadoop-yarn-server-timelineservice-hbase-tests\\src\\test\\java\\org\\apache\\hadoop\\yarn\\server\\timelineservice\\storage",
  "methodName" : "tearDownAfterClass",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void tearDownAfterClass() throws Exception\n{\r\n    if (util != null) {\r\n        util.shutdownMiniCluster();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-yarn-project\\hadoop-yarn\\hadoop-yarn-server\\hadoop-yarn-server-timelineservice-hbase-tests\\src\\test\\java\\org\\apache\\hadoop\\yarn\\server\\timelineservice\\storage",
  "methodName" : "verifyRowKeyForSubApplication",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "boolean verifyRowKeyForSubApplication(byte[] rowKey, String suAppUser, String cluster, String user, TimelineEntity te)\n{\r\n    SubApplicationRowKey key = SubApplicationRowKey.parseRowKey(rowKey);\r\n    assertEquals(suAppUser, key.getSubAppUserId());\r\n    assertEquals(cluster, key.getClusterId());\r\n    assertEquals(te.getType(), key.getEntityType());\r\n    assertEquals(te.getId(), key.getEntityId());\r\n    assertEquals(user, key.getUserId());\r\n    return true;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-yarn-project\\hadoop-yarn\\hadoop-yarn-server\\hadoop-yarn-server-timelineservice-hbase-tests\\src\\test\\java\\org\\apache\\hadoop\\yarn\\server\\timelineservice\\reader",
  "methodName" : "addGetEntitiesLatency",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void addGetEntitiesLatency(long durationMs, boolean succeeded)\n{\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-yarn-project\\hadoop-yarn\\hadoop-yarn-server\\hadoop-yarn-server-timelineservice-hbase-tests\\src\\test\\java\\org\\apache\\hadoop\\yarn\\server\\timelineservice\\reader",
  "methodName" : "addGetEntityTypesLatency",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void addGetEntityTypesLatency(long durationMs, boolean succeeded)\n{\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-yarn-project\\hadoop-yarn\\hadoop-yarn-server\\hadoop-yarn-server-timelineservice-hbase-tests\\src\\test\\java\\org\\apache\\hadoop\\yarn\\server\\timelineservice\\storage\\flow",
  "methodName" : "setupBeforeClass",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void setupBeforeClass() throws Exception\n{\r\n    util = new HBaseTestingUtility();\r\n    Configuration conf = util.getConfiguration();\r\n    conf.setInt(\"hfile.format.version\", 3);\r\n    util.startMiniCluster();\r\n    DataGeneratorForTest.createSchema(util.getConfiguration());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-yarn-project\\hadoop-yarn\\hadoop-yarn-server\\hadoop-yarn-server-timelineservice-hbase-tests\\src\\test\\java\\org\\apache\\hadoop\\yarn\\server\\timelineservice\\storage\\flow",
  "methodName" : "testWriteFlowRunMinMax",
  "errType" : null,
  "containingMethodsNum" : 45,
  "sourceCodeText" : "void testWriteFlowRunMinMax() throws Exception\n{\r\n    TimelineEntities te = new TimelineEntities();\r\n    te.addEntity(TestFlowDataGenerator.getEntity1());\r\n    HBaseTimelineWriterImpl hbi = null;\r\n    Configuration c1 = util.getConfiguration();\r\n    String cluster = \"testWriteFlowRunMinMaxToHBase_cluster1\";\r\n    String user = \"testWriteFlowRunMinMaxToHBase_user1\";\r\n    String flow = \"testing_flowRun_flow_name\";\r\n    String flowVersion = \"CF7022C10F1354\";\r\n    long runid = 1002345678919L;\r\n    String appName = \"application_100000000000_1111\";\r\n    long minStartTs = 1424995200300L;\r\n    long greaterStartTs = 1424995200300L + 864000L;\r\n    long endTs = 1424995200300L + 86000000L;\r\n    TimelineEntity entityMinStartTime = TestFlowDataGenerator.getEntityMinStartTime(minStartTs);\r\n    try {\r\n        hbi = new HBaseTimelineWriterImpl();\r\n        hbi.init(c1);\r\n        UserGroupInformation remoteUser = UserGroupInformation.createRemoteUser(user);\r\n        hbi.write(new TimelineCollectorContext(cluster, user, flow, flowVersion, runid, appName), te, remoteUser);\r\n        te = new TimelineEntities();\r\n        te.addEntity(entityMinStartTime);\r\n        appName = \"application_100000000000_3333\";\r\n        hbi.write(new TimelineCollectorContext(cluster, user, flow, flowVersion, runid, appName), te, remoteUser);\r\n        TimelineEntity entityMaxEndTime = TestFlowDataGenerator.getEntityMaxEndTime(endTs);\r\n        te = new TimelineEntities();\r\n        te.addEntity(entityMaxEndTime);\r\n        appName = \"application_100000000000_4444\";\r\n        hbi.write(new TimelineCollectorContext(cluster, user, flow, flowVersion, runid, appName), te, remoteUser);\r\n        TimelineEntity entityGreaterStartTime = TestFlowDataGenerator.getEntityGreaterStartTime(greaterStartTs);\r\n        te = new TimelineEntities();\r\n        te.addEntity(entityGreaterStartTime);\r\n        appName = \"application_1000000000000000_2222\";\r\n        hbi.write(new TimelineCollectorContext(cluster, user, flow, flowVersion, runid, appName), te, remoteUser);\r\n        hbi.flush();\r\n    } finally {\r\n        if (hbi != null) {\r\n            hbi.close();\r\n        }\r\n    }\r\n    Connection conn = ConnectionFactory.createConnection(c1);\r\n    Table table1 = conn.getTable(BaseTableRW.getTableName(c1, FlowActivityTableRW.TABLE_NAME_CONF_NAME, FlowActivityTableRW.DEFAULT_TABLE_NAME));\r\n    byte[] startRow = new FlowActivityRowKey(cluster, minStartTs, user, flow).getRowKey();\r\n    Get g = new Get(startRow);\r\n    Result r1 = table1.get(g);\r\n    assertNotNull(r1);\r\n    assertTrue(!r1.isEmpty());\r\n    Map<byte[], byte[]> values = r1.getFamilyMap(FlowActivityColumnFamily.INFO.getBytes());\r\n    assertEquals(1, values.size());\r\n    byte[] row = r1.getRow();\r\n    FlowActivityRowKey flowActivityRowKey = FlowActivityRowKey.parseRowKey(row);\r\n    assertNotNull(flowActivityRowKey);\r\n    assertEquals(cluster, flowActivityRowKey.getClusterId());\r\n    assertEquals(user, flowActivityRowKey.getUserId());\r\n    assertEquals(flow, flowActivityRowKey.getFlowName());\r\n    Long dayTs = HBaseTimelineSchemaUtils.getTopOfTheDayTimestamp(minStartTs);\r\n    assertEquals(dayTs, flowActivityRowKey.getDayTimestamp());\r\n    assertEquals(1, values.size());\r\n    checkFlowActivityRunId(runid, flowVersion, values);\r\n    HBaseTimelineReaderImpl hbr = null;\r\n    try {\r\n        hbr = new HBaseTimelineReaderImpl();\r\n        hbr.init(c1);\r\n        hbr.start();\r\n        Set<TimelineEntity> entities = hbr.getEntities(new TimelineReaderContext(cluster, null, null, null, null, TimelineEntityType.YARN_FLOW_ACTIVITY.toString(), null), new TimelineEntityFilters.Builder().entityLimit(10L).build(), new TimelineDataToRetrieve());\r\n        assertEquals(1, entities.size());\r\n        for (TimelineEntity e : entities) {\r\n            FlowActivityEntity flowActivity = (FlowActivityEntity) e;\r\n            assertEquals(cluster, flowActivity.getCluster());\r\n            assertEquals(user, flowActivity.getUser());\r\n            assertEquals(flow, flowActivity.getFlowName());\r\n            assertEquals(dayTs, Long.valueOf(flowActivity.getDate().getTime()));\r\n            Set<FlowRunEntity> flowRuns = flowActivity.getFlowRuns();\r\n            assertEquals(1, flowRuns.size());\r\n        }\r\n    } finally {\r\n        if (hbr != null) {\r\n            hbr.close();\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 2,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-yarn-project\\hadoop-yarn\\hadoop-yarn-server\\hadoop-yarn-server-timelineservice-hbase-tests\\src\\test\\java\\org\\apache\\hadoop\\yarn\\server\\timelineservice\\storage\\flow",
  "methodName" : "testWriteFlowActivityOneFlow",
  "errType" : null,
  "containingMethodsNum" : 17,
  "sourceCodeText" : "void testWriteFlowActivityOneFlow() throws Exception\n{\r\n    String cluster = \"testWriteFlowActivityOneFlow_cluster1\";\r\n    String user = \"testWriteFlowActivityOneFlow_user1\";\r\n    String flow = \"flow_activity_test_flow_name\";\r\n    String flowVersion = \"A122110F135BC4\";\r\n    long runid = 1001111178919L;\r\n    TimelineEntities te = new TimelineEntities();\r\n    long appCreatedTime = 1425016501000L;\r\n    TimelineEntity entityApp1 = TestFlowDataGenerator.getFlowApp1(appCreatedTime);\r\n    te.addEntity(entityApp1);\r\n    HBaseTimelineWriterImpl hbi = null;\r\n    Configuration c1 = util.getConfiguration();\r\n    try {\r\n        hbi = new HBaseTimelineWriterImpl();\r\n        hbi.init(c1);\r\n        String appName = \"application_1111999999_1234\";\r\n        hbi.write(new TimelineCollectorContext(cluster, user, flow, flowVersion, runid, appName), te, UserGroupInformation.createRemoteUser(user));\r\n        hbi.flush();\r\n    } finally {\r\n        if (hbi != null) {\r\n            hbi.close();\r\n        }\r\n    }\r\n    checkFlowActivityTable(cluster, user, flow, flowVersion, runid, c1, appCreatedTime);\r\n    HBaseTimelineReaderImpl hbr = null;\r\n    try {\r\n        hbr = new HBaseTimelineReaderImpl();\r\n        hbr.init(c1);\r\n        hbr.start();\r\n        Set<TimelineEntity> entities = hbr.getEntities(new TimelineReaderContext(cluster, user, flow, null, null, TimelineEntityType.YARN_FLOW_ACTIVITY.toString(), null), new TimelineEntityFilters.Builder().entityLimit(10L).build(), new TimelineDataToRetrieve());\r\n        assertEquals(1, entities.size());\r\n        for (TimelineEntity e : entities) {\r\n            FlowActivityEntity entity = (FlowActivityEntity) e;\r\n            NavigableSet<FlowRunEntity> flowRuns = entity.getFlowRuns();\r\n            assertEquals(1, flowRuns.size());\r\n            for (FlowRunEntity flowRun : flowRuns) {\r\n                assertEquals(runid, flowRun.getRunId());\r\n                assertEquals(flowVersion, flowRun.getVersion());\r\n            }\r\n        }\r\n    } finally {\r\n        if (hbr != null) {\r\n            hbr.close();\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 2,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-yarn-project\\hadoop-yarn\\hadoop-yarn-server\\hadoop-yarn-server-timelineservice-hbase-tests\\src\\test\\java\\org\\apache\\hadoop\\yarn\\server\\timelineservice\\storage\\flow",
  "methodName" : "checkFlowActivityTable",
  "errType" : null,
  "containingMethodsNum" : 22,
  "sourceCodeText" : "void checkFlowActivityTable(String cluster, String user, String flow, String flowVersion, long runid, Configuration c1, long appCreatedTime) throws IOException\n{\r\n    Scan s = new Scan();\r\n    s.addFamily(FlowActivityColumnFamily.INFO.getBytes());\r\n    byte[] startRow = new FlowActivityRowKey(cluster, appCreatedTime, user, flow).getRowKey();\r\n    s.setStartRow(startRow);\r\n    String clusterStop = cluster + \"1\";\r\n    byte[] stopRow = new FlowActivityRowKey(clusterStop, appCreatedTime, user, flow).getRowKey();\r\n    s.setStopRow(stopRow);\r\n    Connection conn = ConnectionFactory.createConnection(c1);\r\n    Table table1 = conn.getTable(BaseTableRW.getTableName(c1, FlowActivityTableRW.TABLE_NAME_CONF_NAME, FlowActivityTableRW.DEFAULT_TABLE_NAME));\r\n    ResultScanner scanner = table1.getScanner(s);\r\n    int rowCount = 0;\r\n    for (Result result : scanner) {\r\n        assertNotNull(result);\r\n        assertTrue(!result.isEmpty());\r\n        Map<byte[], byte[]> values = result.getFamilyMap(FlowActivityColumnFamily.INFO.getBytes());\r\n        rowCount++;\r\n        byte[] row = result.getRow();\r\n        FlowActivityRowKey flowActivityRowKey = FlowActivityRowKey.parseRowKey(row);\r\n        assertNotNull(flowActivityRowKey);\r\n        assertEquals(cluster, flowActivityRowKey.getClusterId());\r\n        assertEquals(user, flowActivityRowKey.getUserId());\r\n        assertEquals(flow, flowActivityRowKey.getFlowName());\r\n        Long dayTs = HBaseTimelineSchemaUtils.getTopOfTheDayTimestamp(appCreatedTime);\r\n        assertEquals(dayTs, flowActivityRowKey.getDayTimestamp());\r\n        assertEquals(1, values.size());\r\n        checkFlowActivityRunId(runid, flowVersion, values);\r\n    }\r\n    assertEquals(1, rowCount);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-yarn-project\\hadoop-yarn\\hadoop-yarn-server\\hadoop-yarn-server-timelineservice-hbase-tests\\src\\test\\java\\org\\apache\\hadoop\\yarn\\server\\timelineservice\\storage\\flow",
  "methodName" : "testFlowActivityTableOneFlowMultipleRunIds",
  "errType" : null,
  "containingMethodsNum" : 31,
  "sourceCodeText" : "void testFlowActivityTableOneFlowMultipleRunIds() throws IOException\n{\r\n    String cluster = \"testManyRunsFlowActivity_cluster1\";\r\n    String user = \"testManyRunsFlowActivity_c_user1\";\r\n    String flow = \"flow_activity_test_flow_name\";\r\n    String flowVersion1 = \"A122110F135BC4\";\r\n    long runid1 = 11111111111L;\r\n    String flowVersion2 = \"A12222222222C4\";\r\n    long runid2 = 2222222222222L;\r\n    String flowVersion3 = \"A1333333333C4\";\r\n    long runid3 = 3333333333333L;\r\n    TimelineEntities te = new TimelineEntities();\r\n    long appCreatedTime = 1425016501000L;\r\n    TimelineEntity entityApp1 = TestFlowDataGenerator.getFlowApp1(appCreatedTime);\r\n    te.addEntity(entityApp1);\r\n    HBaseTimelineWriterImpl hbi = null;\r\n    Configuration c1 = util.getConfiguration();\r\n    try {\r\n        hbi = new HBaseTimelineWriterImpl();\r\n        hbi.init(c1);\r\n        UserGroupInformation remoteUser = UserGroupInformation.createRemoteUser(user);\r\n        String appName = \"application_11888888888_1111\";\r\n        hbi.write(new TimelineCollectorContext(cluster, user, flow, flowVersion1, runid1, appName), te, remoteUser);\r\n        te = new TimelineEntities();\r\n        te.addEntity(entityApp1);\r\n        appName = \"application_11888888888_2222\";\r\n        hbi.write(new TimelineCollectorContext(cluster, user, flow, flowVersion2, runid2, appName), te, remoteUser);\r\n        te = new TimelineEntities();\r\n        te.addEntity(entityApp1);\r\n        appName = \"application_11888888888_3333\";\r\n        hbi.write(new TimelineCollectorContext(cluster, user, flow, flowVersion3, runid3, appName), te, remoteUser);\r\n        hbi.flush();\r\n    } finally {\r\n        if (hbi != null) {\r\n            hbi.close();\r\n        }\r\n    }\r\n    checkFlowActivityTableSeveralRuns(cluster, user, flow, c1, flowVersion1, runid1, flowVersion2, runid2, flowVersion3, runid3, appCreatedTime);\r\n    HBaseTimelineReaderImpl hbr = null;\r\n    try {\r\n        hbr = new HBaseTimelineReaderImpl();\r\n        hbr.init(c1);\r\n        hbr.start();\r\n        Set<TimelineEntity> entities = hbr.getEntities(new TimelineReaderContext(cluster, null, null, null, null, TimelineEntityType.YARN_FLOW_ACTIVITY.toString(), null), new TimelineEntityFilters.Builder().entityLimit(10L).build(), new TimelineDataToRetrieve());\r\n        assertEquals(1, entities.size());\r\n        for (TimelineEntity e : entities) {\r\n            FlowActivityEntity flowActivity = (FlowActivityEntity) e;\r\n            assertEquals(cluster, flowActivity.getCluster());\r\n            assertEquals(user, flowActivity.getUser());\r\n            assertEquals(flow, flowActivity.getFlowName());\r\n            long dayTs = HBaseTimelineSchemaUtils.getTopOfTheDayTimestamp(appCreatedTime);\r\n            assertEquals(dayTs, flowActivity.getDate().getTime());\r\n            Set<FlowRunEntity> flowRuns = flowActivity.getFlowRuns();\r\n            assertEquals(3, flowRuns.size());\r\n            for (FlowRunEntity flowRun : flowRuns) {\r\n                long runId = flowRun.getRunId();\r\n                String version = flowRun.getVersion();\r\n                if (runId == runid1) {\r\n                    assertEquals(flowVersion1, version);\r\n                } else if (runId == runid2) {\r\n                    assertEquals(flowVersion2, version);\r\n                } else if (runId == runid3) {\r\n                    assertEquals(flowVersion3, version);\r\n                } else {\r\n                    fail(\"unknown run id: \" + runId);\r\n                }\r\n            }\r\n        }\r\n    } finally {\r\n        if (hbr != null) {\r\n            hbr.close();\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 2,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-yarn-project\\hadoop-yarn\\hadoop-yarn-server\\hadoop-yarn-server-timelineservice-hbase-tests\\src\\test\\java\\org\\apache\\hadoop\\yarn\\server\\timelineservice\\storage\\flow",
  "methodName" : "checkFlowActivityTableSeveralRuns",
  "errType" : null,
  "containingMethodsNum" : 24,
  "sourceCodeText" : "void checkFlowActivityTableSeveralRuns(String cluster, String user, String flow, Configuration c1, String flowVersion1, long runid1, String flowVersion2, long runid2, String flowVersion3, long runid3, long appCreatedTime) throws IOException\n{\r\n    Scan s = new Scan();\r\n    s.addFamily(FlowActivityColumnFamily.INFO.getBytes());\r\n    byte[] startRow = new FlowActivityRowKey(cluster, appCreatedTime, user, flow).getRowKey();\r\n    s.setStartRow(startRow);\r\n    String clusterStop = cluster + \"1\";\r\n    byte[] stopRow = new FlowActivityRowKey(clusterStop, appCreatedTime, user, flow).getRowKey();\r\n    s.setStopRow(stopRow);\r\n    Connection conn = ConnectionFactory.createConnection(c1);\r\n    Table table1 = conn.getTable(BaseTableRW.getTableName(c1, FlowActivityTableRW.TABLE_NAME_CONF_NAME, FlowActivityTableRW.DEFAULT_TABLE_NAME));\r\n    ResultScanner scanner = table1.getScanner(s);\r\n    int rowCount = 0;\r\n    for (Result result : scanner) {\r\n        assertNotNull(result);\r\n        assertTrue(!result.isEmpty());\r\n        byte[] row = result.getRow();\r\n        FlowActivityRowKey flowActivityRowKey = FlowActivityRowKey.parseRowKey(row);\r\n        assertNotNull(flowActivityRowKey);\r\n        assertEquals(cluster, flowActivityRowKey.getClusterId());\r\n        assertEquals(user, flowActivityRowKey.getUserId());\r\n        assertEquals(flow, flowActivityRowKey.getFlowName());\r\n        Long dayTs = HBaseTimelineSchemaUtils.getTopOfTheDayTimestamp(appCreatedTime);\r\n        assertEquals(dayTs, flowActivityRowKey.getDayTimestamp());\r\n        Map<byte[], byte[]> values = result.getFamilyMap(FlowActivityColumnFamily.INFO.getBytes());\r\n        rowCount++;\r\n        assertEquals(3, values.size());\r\n        checkFlowActivityRunId(runid1, flowVersion1, values);\r\n        checkFlowActivityRunId(runid2, flowVersion2, values);\r\n        checkFlowActivityRunId(runid3, flowVersion3, values);\r\n    }\r\n    assertEquals(1, rowCount);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-yarn-project\\hadoop-yarn\\hadoop-yarn-server\\hadoop-yarn-server-timelineservice-hbase-tests\\src\\test\\java\\org\\apache\\hadoop\\yarn\\server\\timelineservice\\storage\\flow",
  "methodName" : "checkFlowActivityRunId",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void checkFlowActivityRunId(long runid, String flowVersion, Map<byte[], byte[]> values) throws IOException\n{\r\n    byte[] rq = ColumnHelper.getColumnQualifier(FlowActivityColumnPrefix.RUN_ID.getColumnPrefixBytes(), GenericObjectMapper.write(runid));\r\n    for (Map.Entry<byte[], byte[]> k : values.entrySet()) {\r\n        String actualQ = Bytes.toString(k.getKey());\r\n        if (Bytes.toString(rq).equals(actualQ)) {\r\n            String actualV = (String) GenericObjectMapper.read(k.getValue());\r\n            assertEquals(flowVersion, actualV);\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-yarn-project\\hadoop-yarn\\hadoop-yarn-server\\hadoop-yarn-server-timelineservice-hbase-tests\\src\\test\\java\\org\\apache\\hadoop\\yarn\\server\\timelineservice\\storage\\flow",
  "methodName" : "tearDownAfterClass",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void tearDownAfterClass() throws Exception\n{\r\n    if (util != null) {\r\n        util.shutdownMiniCluster();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
} ]
[ {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\test\\java\\org\\apache\\hadoop\\test",
  "methodName" : "dummy",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void dummy()\n{\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\test\\java\\org\\apache\\hadoop\\test",
  "methodName" : "delete",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void delete(File file) throws IOException\n{\r\n    if (file.getAbsolutePath().length() < 5) {\r\n        throw new IllegalArgumentException(MessageFormat.format(\"Path [{0}] is too short, not deleting\", file.getAbsolutePath()));\r\n    }\r\n    if (file.exists()) {\r\n        if (file.isDirectory()) {\r\n            File[] children = file.listFiles();\r\n            if (children != null) {\r\n                for (File child : children) {\r\n                    delete(child);\r\n                }\r\n            }\r\n        }\r\n        if (!file.delete()) {\r\n            throw new RuntimeException(MessageFormat.format(\"Could not delete path [{0}]\", file.getAbsolutePath()));\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\test\\java\\org\\apache\\hadoop\\test",
  "methodName" : "apply",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "Statement apply(final Statement statement, final FrameworkMethod frameworkMethod, final Object o)\n{\r\n    return new Statement() {\r\n\r\n        @Override\r\n        public void evaluate() throws Throwable {\r\n            File testDir = null;\r\n            TestDir testDirAnnotation = frameworkMethod.getAnnotation(TestDir.class);\r\n            if (testDirAnnotation != null) {\r\n                testDir = resetTestCaseDir(frameworkMethod.getName());\r\n            }\r\n            try {\r\n                TEST_DIR_TL.set(testDir);\r\n                statement.evaluate();\r\n            } finally {\r\n                TEST_DIR_TL.remove();\r\n            }\r\n        }\r\n    };\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\test\\java\\org\\apache\\hadoop\\test",
  "methodName" : "getTestDir",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "File getTestDir()\n{\r\n    File testDir = TEST_DIR_TL.get();\r\n    if (testDir == null) {\r\n        throw new IllegalStateException(\"This test does not use @TestDir\");\r\n    }\r\n    return testDir;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\test\\java\\org\\apache\\hadoop\\test",
  "methodName" : "resetTestCaseDir",
  "errType" : [ "IOException" ],
  "containingMethodsNum" : 6,
  "sourceCodeText" : "File resetTestCaseDir(String testName)\n{\r\n    File dir = new File(TEST_DIR_ROOT);\r\n    dir = new File(dir, testName + \"-\" + counter.getAndIncrement());\r\n    dir = dir.getAbsoluteFile();\r\n    try {\r\n        delete(dir);\r\n    } catch (IOException ex) {\r\n        throw new RuntimeException(MessageFormat.format(\"Could not delete test dir[{0}], {1}\", dir, ex.getMessage()), ex);\r\n    }\r\n    if (!dir.mkdirs()) {\r\n        throw new RuntimeException(MessageFormat.format(\"Could not create test dir[{0}]\", dir));\r\n    }\r\n    return dir;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\test\\java\\org\\apache\\hadoop\\fs\\http\\client",
  "methodName" : "cleanUp",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void cleanUp() throws Exception\n{\r\n    new File(classpathDir, \"ssl-client.xml\").delete();\r\n    new File(classpathDir, \"ssl-server.xml\").delete();\r\n    KeyStoreTestUtil.cleanupSSLConfig(keyStoreDir, classpathDir);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\test\\java\\org\\apache\\hadoop\\fs\\http\\client",
  "methodName" : "getFileSystemClass",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Class getFileSystemClass()\n{\r\n    return SWebHdfsFileSystem.class;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\test\\java\\org\\apache\\hadoop\\fs\\http\\client",
  "methodName" : "getScheme",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getScheme()\n{\r\n    return \"swebhdfs\";\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\test\\java\\org\\apache\\hadoop\\fs\\http\\client",
  "methodName" : "getHttpFSFileSystem",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "FileSystem getHttpFSFileSystem() throws Exception\n{\r\n    Configuration conf = new Configuration(sslConf);\r\n    conf.set(\"fs.swebhdfs.impl\", getFileSystemClass().getName());\r\n    URI uri = new URI(\"swebhdfs://\" + TestJettyHelper.getJettyURL().toURI().getAuthority());\r\n    return FileSystem.get(uri, conf);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\test\\java\\org\\apache\\hadoop\\test",
  "methodName" : "apply",
  "errType" : [ "Exception" ],
  "containingMethodsNum" : 8,
  "sourceCodeText" : "Statement apply(final Statement statement, final FrameworkMethod frameworkMethod, final Object o)\n{\r\n    return new Statement() {\r\n\r\n        @Override\r\n        public void evaluate() throws Throwable {\r\n            TestJetty testJetty = frameworkMethod.getAnnotation(TestJetty.class);\r\n            if (testJetty != null) {\r\n                server = createJettyServer();\r\n            }\r\n            try {\r\n                TEST_JETTY_TL.set(TestJettyHelper.this);\r\n                statement.evaluate();\r\n            } finally {\r\n                TEST_JETTY_TL.remove();\r\n                if (server != null && server.isRunning()) {\r\n                    try {\r\n                        server.stop();\r\n                    } catch (Exception ex) {\r\n                        throw new RuntimeException(\"Could not stop embedded servlet container, \" + ex.getMessage(), ex);\r\n                    }\r\n                }\r\n            }\r\n        }\r\n    };\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 2,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\test\\java\\org\\apache\\hadoop\\test",
  "methodName" : "createJettyServer",
  "errType" : [ "Exception" ],
  "containingMethodsNum" : 17,
  "sourceCodeText" : "Server createJettyServer()\n{\r\n    try {\r\n        InetAddress localhost = InetAddress.getByName(\"localhost\");\r\n        String host = \"localhost\";\r\n        ServerSocket ss = new ServerSocket(0, 50, localhost);\r\n        int port = ss.getLocalPort();\r\n        ss.close();\r\n        Server server = new Server();\r\n        ServerConnector conn = new ServerConnector(server);\r\n        HttpConfiguration http_config = new HttpConfiguration();\r\n        http_config.setRequestHeaderSize(JettyUtils.HEADER_SIZE);\r\n        http_config.setResponseHeaderSize(JettyUtils.HEADER_SIZE);\r\n        http_config.setSecureScheme(\"https\");\r\n        http_config.addCustomizer(new SecureRequestCustomizer());\r\n        ConnectionFactory connFactory = new HttpConnectionFactory(http_config);\r\n        conn.addConnectionFactory(connFactory);\r\n        conn.setHost(host);\r\n        conn.setPort(port);\r\n        if (ssl) {\r\n            SslContextFactory.Server sslContextFactory = new SslContextFactory.Server();\r\n            sslContextFactory.setNeedClientAuth(false);\r\n            sslContextFactory.setKeyStorePath(keyStore);\r\n            sslContextFactory.setKeyStoreType(keyStoreType);\r\n            sslContextFactory.setKeyStorePassword(keyStorePassword);\r\n            conn.addFirstConnectionFactory(new SslConnectionFactory(sslContextFactory, HttpVersion.HTTP_1_1.asString()));\r\n        }\r\n        server.addConnector(conn);\r\n        return server;\r\n    } catch (Exception ex) {\r\n        throw new RuntimeException(\"Could not start embedded servlet container, \" + ex.getMessage(), ex);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\test\\java\\org\\apache\\hadoop\\test",
  "methodName" : "getAuthority",
  "errType" : [ "UnknownHostException" ],
  "containingMethodsNum" : 3,
  "sourceCodeText" : "InetSocketAddress getAuthority()\n{\r\n    Server server = getJettyServer();\r\n    try {\r\n        InetAddress add = InetAddress.getByName(((ServerConnector) server.getConnectors()[0]).getHost());\r\n        int port = ((ServerConnector) server.getConnectors()[0]).getPort();\r\n        return new InetSocketAddress(add, port);\r\n    } catch (UnknownHostException ex) {\r\n        throw new RuntimeException(ex);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\test\\java\\org\\apache\\hadoop\\test",
  "methodName" : "getJettyServer",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Server getJettyServer()\n{\r\n    TestJettyHelper helper = TEST_JETTY_TL.get();\r\n    if (helper == null || helper.server == null) {\r\n        throw new IllegalStateException(\"This test does not use @TestJetty\");\r\n    }\r\n    return helper.server;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\test\\java\\org\\apache\\hadoop\\test",
  "methodName" : "getJettyURL",
  "errType" : [ "MalformedURLException" ],
  "containingMethodsNum" : 4,
  "sourceCodeText" : "URL getJettyURL()\n{\r\n    TestJettyHelper helper = TEST_JETTY_TL.get();\r\n    if (helper == null || helper.server == null) {\r\n        throw new IllegalStateException(\"This test does not use @TestJetty\");\r\n    }\r\n    try {\r\n        String scheme = (helper.ssl) ? \"https\" : \"http\";\r\n        return new URL(scheme + \"://\" + ((ServerConnector) helper.server.getConnectors()[0]).getHost() + \":\" + ((ServerConnector) helper.server.getConnectors()[0]).getPort());\r\n    } catch (MalformedURLException ex) {\r\n        throw new RuntimeException(\"It should never happen, \" + ex.getMessage(), ex);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\test\\java\\org\\apache\\hadoop\\fs\\http\\server",
  "methodName" : "putUpload",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void putUpload() throws Exception\n{\r\n    test(\"PUT\", HttpFSFileSystem.Operation.CREATE.toString(), \"application/octet-stream\", true, false);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\test\\java\\org\\apache\\hadoop\\fs\\http\\server",
  "methodName" : "postUpload",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void postUpload() throws Exception\n{\r\n    test(\"POST\", HttpFSFileSystem.Operation.APPEND.toString(), \"APPLICATION/OCTET-STREAM\", true, false);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\test\\java\\org\\apache\\hadoop\\fs\\http\\server",
  "methodName" : "putUploadWrong",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void putUploadWrong() throws Exception\n{\r\n    test(\"PUT\", HttpFSFileSystem.Operation.CREATE.toString(), \"plain/text\", false, false);\r\n    test(\"PUT\", HttpFSFileSystem.Operation.CREATE.toString(), \"plain/text\", true, true);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\test\\java\\org\\apache\\hadoop\\fs\\http\\server",
  "methodName" : "postUploadWrong",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void postUploadWrong() throws Exception\n{\r\n    test(\"POST\", HttpFSFileSystem.Operation.APPEND.toString(), \"plain/text\", false, false);\r\n    test(\"POST\", HttpFSFileSystem.Operation.APPEND.toString(), \"plain/text\", true, true);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\test\\java\\org\\apache\\hadoop\\fs\\http\\server",
  "methodName" : "getOther",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void getOther() throws Exception\n{\r\n    test(\"GET\", HttpFSFileSystem.Operation.GETHOMEDIRECTORY.toString(), \"plain/text\", false, false);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\test\\java\\org\\apache\\hadoop\\fs\\http\\server",
  "methodName" : "putOther",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void putOther() throws Exception\n{\r\n    test(\"PUT\", HttpFSFileSystem.Operation.MKDIRS.toString(), \"plain/text\", false, false);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\test\\java\\org\\apache\\hadoop\\fs\\http\\server",
  "methodName" : "test",
  "errType" : null,
  "containingMethodsNum" : 11,
  "sourceCodeText" : "void test(String method, String operation, String contentType, boolean upload, boolean error) throws Exception\n{\r\n    HttpServletRequest request = Mockito.mock(HttpServletRequest.class);\r\n    HttpServletResponse response = Mockito.mock(HttpServletResponse.class);\r\n    Mockito.reset(request);\r\n    Mockito.when(request.getMethod()).thenReturn(method);\r\n    Mockito.when(request.getParameter(HttpFSFileSystem.OP_PARAM)).thenReturn(operation);\r\n    Mockito.when(request.getParameter(HttpFSParametersProvider.DataParam.NAME)).thenReturn(Boolean.toString(upload));\r\n    Mockito.when(request.getContentType()).thenReturn(contentType);\r\n    FilterChain chain = Mockito.mock(FilterChain.class);\r\n    Filter filter = new CheckUploadContentTypeFilter();\r\n    filter.doFilter(request, response, chain);\r\n    if (error) {\r\n        Mockito.verify(response).sendError(Mockito.eq(HttpServletResponse.SC_BAD_REQUEST), Mockito.contains(\"Data upload\"));\r\n    } else {\r\n        Mockito.verify(chain).doFilter(request, response);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\test\\java\\org\\apache\\hadoop\\lib\\util",
  "methodName" : "constructors",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void constructors() throws Exception\n{\r\n    Configuration conf = new Configuration(false);\r\n    assertEquals(conf.size(), 0);\r\n    byte[] bytes = \"<configuration><property><name>a</name><value>A</value></property></configuration>\".getBytes();\r\n    InputStream is = new ByteArrayInputStream(bytes);\r\n    conf = new Configuration(false);\r\n    ConfigurationUtils.load(conf, is);\r\n    assertEquals(conf.size(), 1);\r\n    assertEquals(conf.get(\"a\"), \"A\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\test\\java\\org\\apache\\hadoop\\lib\\util",
  "methodName" : "constructors3",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void constructors3() throws Exception\n{\r\n    InputStream is = new ByteArrayInputStream(\"<xxx><property name=\\\"key1\\\" value=\\\"val1\\\"/></xxx>\".getBytes());\r\n    Configuration conf = new Configuration(false);\r\n    ConfigurationUtils.load(conf, is);\r\n    assertEquals(\"val1\", conf.get(\"key1\"));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\test\\java\\org\\apache\\hadoop\\lib\\util",
  "methodName" : "copy",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void copy() throws Exception\n{\r\n    Configuration srcConf = new Configuration(false);\r\n    Configuration targetConf = new Configuration(false);\r\n    srcConf.set(\"testParameter1\", \"valueFromSource\");\r\n    srcConf.set(\"testParameter2\", \"valueFromSource\");\r\n    targetConf.set(\"testParameter2\", \"valueFromTarget\");\r\n    targetConf.set(\"testParameter3\", \"valueFromTarget\");\r\n    ConfigurationUtils.copy(srcConf, targetConf);\r\n    assertEquals(\"valueFromSource\", targetConf.get(\"testParameter1\"));\r\n    assertEquals(\"valueFromSource\", targetConf.get(\"testParameter2\"));\r\n    assertEquals(\"valueFromTarget\", targetConf.get(\"testParameter3\"));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\test\\java\\org\\apache\\hadoop\\lib\\util",
  "methodName" : "injectDefaults",
  "errType" : null,
  "containingMethodsNum" : 11,
  "sourceCodeText" : "void injectDefaults() throws Exception\n{\r\n    Configuration srcConf = new Configuration(false);\r\n    Configuration targetConf = new Configuration(false);\r\n    srcConf.set(\"testParameter1\", \"valueFromSource\");\r\n    srcConf.set(\"testParameter2\", \"valueFromSource\");\r\n    targetConf.set(\"testParameter2\", \"originalValueFromTarget\");\r\n    targetConf.set(\"testParameter3\", \"originalValueFromTarget\");\r\n    ConfigurationUtils.injectDefaults(srcConf, targetConf);\r\n    assertEquals(\"valueFromSource\", targetConf.get(\"testParameter1\"));\r\n    assertEquals(\"originalValueFromTarget\", targetConf.get(\"testParameter2\"));\r\n    assertEquals(\"originalValueFromTarget\", targetConf.get(\"testParameter3\"));\r\n    assertEquals(\"valueFromSource\", srcConf.get(\"testParameter1\"));\r\n    assertEquals(\"valueFromSource\", srcConf.get(\"testParameter2\"));\r\n    assertNull(srcConf.get(\"testParameter3\"));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\test\\java\\org\\apache\\hadoop\\lib\\util",
  "methodName" : "resolve",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void resolve()\n{\r\n    Configuration conf = new Configuration(false);\r\n    conf.set(\"a\", \"A\");\r\n    conf.set(\"b\", \"${a}\");\r\n    assertEquals(conf.getRaw(\"a\"), \"A\");\r\n    assertEquals(conf.getRaw(\"b\"), \"${a}\");\r\n    conf = ConfigurationUtils.resolve(conf);\r\n    assertEquals(conf.getRaw(\"a\"), \"A\");\r\n    assertEquals(conf.getRaw(\"b\"), \"A\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\test\\java\\org\\apache\\hadoop\\lib\\util",
  "methodName" : "testVarResolutionAndSysProps",
  "errType" : null,
  "containingMethodsNum" : 14,
  "sourceCodeText" : "void testVarResolutionAndSysProps()\n{\r\n    String userName = System.getProperty(\"user.name\");\r\n    Configuration conf = new Configuration(false);\r\n    conf.set(\"a\", \"A\");\r\n    conf.set(\"b\", \"${a}\");\r\n    conf.set(\"c\", \"${user.name}\");\r\n    conf.set(\"d\", \"${aaa}\");\r\n    assertEquals(conf.getRaw(\"a\"), \"A\");\r\n    assertEquals(conf.getRaw(\"b\"), \"${a}\");\r\n    assertEquals(conf.getRaw(\"c\"), \"${user.name}\");\r\n    assertEquals(conf.get(\"a\"), \"A\");\r\n    assertEquals(conf.get(\"b\"), \"A\");\r\n    assertEquals(conf.get(\"c\"), userName);\r\n    assertEquals(conf.get(\"d\"), \"${aaa}\");\r\n    conf.set(\"user.name\", \"foo\");\r\n    assertEquals(conf.get(\"user.name\"), \"foo\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\test\\java\\org\\apache\\hadoop\\lib\\util",
  "methodName" : "testCompactFormatProperty",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void testCompactFormatProperty() throws IOException\n{\r\n    final String testfile = \"test-compact-format-property.xml\";\r\n    Configuration conf = new Configuration(false);\r\n    assertEquals(0, conf.size());\r\n    ConfigurationUtils.load(conf, Thread.currentThread().getContextClassLoader().getResource(testfile).openStream());\r\n    assertEquals(2, conf.size());\r\n    assertEquals(\"val1\", conf.get(\"key.1\"));\r\n    assertEquals(\"val2\", conf.get(\"key.2\"));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\test\\java\\org\\apache\\hadoop\\lib\\service\\security",
  "methodName" : "service",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void service() throws Exception\n{\r\n    String dir = TestDirHelper.getTestDir().getAbsolutePath();\r\n    Configuration conf = new Configuration(false);\r\n    conf.set(\"server.services\", StringUtils.join(\",\", Arrays.asList(GroupsService.class.getName())));\r\n    Server server = new Server(\"server\", dir, dir, dir, dir, conf);\r\n    server.init();\r\n    Groups groups = server.get(Groups.class);\r\n    assertNotNull(groups);\r\n    List<String> g = groups.getGroups(System.getProperty(\"user.name\"));\r\n    assertNotSame(g.size(), 0);\r\n    server.destroy();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\test\\java\\org\\apache\\hadoop\\lib\\service\\security",
  "methodName" : "invalidGroupsMapping",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void invalidGroupsMapping() throws Exception\n{\r\n    String dir = TestDirHelper.getTestDir().getAbsolutePath();\r\n    Configuration conf = new Configuration(false);\r\n    conf.set(\"server.services\", StringUtils.join(\",\", Arrays.asList(GroupsService.class.getName())));\r\n    conf.set(\"server.groups.hadoop.security.group.mapping\", String.class.getName());\r\n    Server server = new Server(\"server\", dir, dir, dir, dir, conf);\r\n    server.init();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\test\\java\\org\\apache\\hadoop\\lib\\servlet",
  "methodName" : "hostname",
  "errType" : null,
  "containingMethodsNum" : 11,
  "sourceCodeText" : "void hostname() throws Exception\n{\r\n    ServletRequest request = Mockito.mock(ServletRequest.class);\r\n    Mockito.when(request.getRemoteAddr()).thenReturn(\"localhost\");\r\n    ServletResponse response = Mockito.mock(ServletResponse.class);\r\n    final AtomicBoolean invoked = new AtomicBoolean();\r\n    FilterChain chain = new FilterChain() {\r\n\r\n        @Override\r\n        public void doFilter(ServletRequest servletRequest, ServletResponse servletResponse) throws IOException, ServletException {\r\n            assertTrue(HostnameFilter.get().contains(\"localhost\") || HostnameFilter.get().contains(\"127.0.0.1\"));\r\n            invoked.set(true);\r\n        }\r\n    };\r\n    Filter filter = new HostnameFilter();\r\n    filter.init(null);\r\n    assertNull(HostnameFilter.get());\r\n    filter.doFilter(request, response, chain);\r\n    assertTrue(invoked.get());\r\n    assertNull(HostnameFilter.get());\r\n    filter.destroy();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\test\\java\\org\\apache\\hadoop\\lib\\servlet",
  "methodName" : "testMissingHostname",
  "errType" : null,
  "containingMethodsNum" : 11,
  "sourceCodeText" : "void testMissingHostname() throws Exception\n{\r\n    ServletRequest request = Mockito.mock(ServletRequest.class);\r\n    Mockito.when(request.getRemoteAddr()).thenReturn(null);\r\n    ServletResponse response = Mockito.mock(ServletResponse.class);\r\n    final AtomicBoolean invoked = new AtomicBoolean();\r\n    FilterChain chain = new FilterChain() {\r\n\r\n        @Override\r\n        public void doFilter(ServletRequest servletRequest, ServletResponse servletResponse) throws IOException, ServletException {\r\n            assertTrue(HostnameFilter.get().contains(\"???\"));\r\n            invoked.set(true);\r\n        }\r\n    };\r\n    Filter filter = new HostnameFilter();\r\n    filter.init(null);\r\n    assertNull(HostnameFilter.get());\r\n    filter.doFilter(request, response, chain);\r\n    assertTrue(invoked.get());\r\n    assertNull(HostnameFilter.get());\r\n    filter.destroy();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\test\\java\\org\\apache\\hadoop\\lib\\wsrs",
  "methodName" : "test",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void test() throws Exception\n{\r\n    JSONMapProvider p = new JSONMapProvider();\r\n    assertTrue(p.isWriteable(Map.class, null, null, null));\r\n    assertFalse(p.isWriteable(this.getClass(), null, null, null));\r\n    assertEquals(p.getSize(null, null, null, null, null), -1);\r\n    ByteArrayOutputStream baos = new ByteArrayOutputStream();\r\n    JSONObject json = new JSONObject();\r\n    json.put(\"a\", \"A\");\r\n    p.writeTo(json, JSONObject.class, null, null, null, null, baos);\r\n    baos.close();\r\n    assertEquals(new String(baos.toByteArray()).trim(), \"{\\\"a\\\":\\\"A\\\"}\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\test\\java\\org\\apache\\hadoop\\test",
  "methodName" : "testDirNoAnnotation",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testDirNoAnnotation() throws Exception\n{\r\n    TestDirHelper.getTestDir();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\test\\java\\org\\apache\\hadoop\\test",
  "methodName" : "testJettyNoAnnotation",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testJettyNoAnnotation() throws Exception\n{\r\n    TestJettyHelper.getJettyServer();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\test\\java\\org\\apache\\hadoop\\test",
  "methodName" : "testJettyNoAnnotation2",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testJettyNoAnnotation2() throws Exception\n{\r\n    TestJettyHelper.getJettyURL();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\test\\java\\org\\apache\\hadoop\\test",
  "methodName" : "testHdfsNoAnnotation",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testHdfsNoAnnotation() throws Exception\n{\r\n    TestHdfsHelper.getHdfsConf();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\test\\java\\org\\apache\\hadoop\\test",
  "methodName" : "testHdfsNoAnnotation2",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testHdfsNoAnnotation2() throws Exception\n{\r\n    TestHdfsHelper.getHdfsTestDir();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\test\\java\\org\\apache\\hadoop\\test",
  "methodName" : "testDirAnnotation",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testDirAnnotation() throws Exception\n{\r\n    assertNotNull(TestDirHelper.getTestDir());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\test\\java\\org\\apache\\hadoop\\test",
  "methodName" : "waitFor",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void waitFor()\n{\r\n    long start = Time.now();\r\n    long waited = waitFor(1000, new Predicate() {\r\n\r\n        @Override\r\n        public boolean evaluate() throws Exception {\r\n            return true;\r\n        }\r\n    });\r\n    long end = Time.now();\r\n    assertEquals(waited, 0, 50);\r\n    assertEquals(end - start - waited, 0, 50);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\test\\java\\org\\apache\\hadoop\\test",
  "methodName" : "waitForTimeOutRatio1",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void waitForTimeOutRatio1()\n{\r\n    setWaitForRatio(1);\r\n    long start = Time.now();\r\n    long waited = waitFor(200, new Predicate() {\r\n\r\n        @Override\r\n        public boolean evaluate() throws Exception {\r\n            return false;\r\n        }\r\n    });\r\n    long end = Time.now();\r\n    assertEquals(waited, -1);\r\n    assertEquals(end - start, 200, 50);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\test\\java\\org\\apache\\hadoop\\test",
  "methodName" : "waitForTimeOutRatio2",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void waitForTimeOutRatio2()\n{\r\n    setWaitForRatio(2);\r\n    long start = Time.now();\r\n    long waited = waitFor(200, new Predicate() {\r\n\r\n        @Override\r\n        public boolean evaluate() throws Exception {\r\n            return false;\r\n        }\r\n    });\r\n    long end = Time.now();\r\n    assertEquals(waited, -1);\r\n    assertEquals(end - start, 200 * getWaitForRatio(), 50 * getWaitForRatio());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\test\\java\\org\\apache\\hadoop\\test",
  "methodName" : "sleepRatio1",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void sleepRatio1()\n{\r\n    setWaitForRatio(1);\r\n    long start = Time.now();\r\n    sleep(100);\r\n    long end = Time.now();\r\n    assertEquals(end - start, 100, 50);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\test\\java\\org\\apache\\hadoop\\test",
  "methodName" : "sleepRatio2",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void sleepRatio2()\n{\r\n    setWaitForRatio(1);\r\n    long start = Time.now();\r\n    sleep(100);\r\n    long end = Time.now();\r\n    assertEquals(end - start, 100 * getWaitForRatio(), 50 * getWaitForRatio());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\test\\java\\org\\apache\\hadoop\\test",
  "methodName" : "testHadoopFileSystem",
  "errType" : null,
  "containingMethodsNum" : 10,
  "sourceCodeText" : "void testHadoopFileSystem() throws Exception\n{\r\n    Configuration conf = TestHdfsHelper.getHdfsConf();\r\n    FileSystem fs = FileSystem.get(conf);\r\n    try {\r\n        OutputStream os = fs.create(new Path(TestHdfsHelper.getHdfsTestDir(), \"foo\"));\r\n        os.write(new byte[] { 1 });\r\n        os.close();\r\n        InputStream is = fs.open(new Path(TestHdfsHelper.getHdfsTestDir(), \"foo\"));\r\n        assertEquals(is.read(), 1);\r\n        assertEquals(is.read(), -1);\r\n        is.close();\r\n    } finally {\r\n        fs.close();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\test\\java\\org\\apache\\hadoop\\test",
  "methodName" : "testJetty",
  "errType" : null,
  "containingMethodsNum" : 11,
  "sourceCodeText" : "void testJetty() throws Exception\n{\r\n    ServletContextHandler context = new ServletContextHandler();\r\n    context.setContextPath(\"/\");\r\n    context.addServlet(MyServlet.class, \"/bar\");\r\n    Server server = TestJettyHelper.getJettyServer();\r\n    server.setHandler(context);\r\n    server.start();\r\n    URL url = new URL(TestJettyHelper.getJettyURL(), \"/bar\");\r\n    HttpURLConnection conn = (HttpURLConnection) url.openConnection();\r\n    assertEquals(conn.getResponseCode(), HttpURLConnection.HTTP_OK);\r\n    BufferedReader reader = new BufferedReader(new InputStreamReader(conn.getInputStream()));\r\n    assertEquals(reader.readLine(), \"foo\");\r\n    reader.close();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\test\\java\\org\\apache\\hadoop\\test",
  "methodName" : "testException0",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void testException0()\n{\r\n    throw new RuntimeException(\"foo\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\test\\java\\org\\apache\\hadoop\\test",
  "methodName" : "testException1",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void testException1()\n{\r\n    throw new RuntimeException(\"foo\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\test\\java\\org\\apache\\hadoop\\fs\\http\\server",
  "methodName" : "server",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void server() throws Exception\n{\r\n    String dir = TestDirHelper.getTestDir().getAbsolutePath();\r\n    Configuration httpfsConf = new Configuration(false);\r\n    HttpFSServerWebApp server = new HttpFSServerWebApp(dir, dir, dir, dir, httpfsConf);\r\n    server.init();\r\n    server.destroy();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\test\\java\\org\\apache\\hadoop\\fs\\http\\server",
  "methodName" : "createHttpFSConf",
  "errType" : null,
  "containingMethodsNum" : 24,
  "sourceCodeText" : "Configuration createHttpFSConf(boolean addDelegationTokenAuthHandler, boolean sslEnabled) throws Exception\n{\r\n    File homeDir = TestDirHelper.getTestDir();\r\n    Assert.assertTrue(new File(homeDir, \"conf\").mkdir());\r\n    Assert.assertTrue(new File(homeDir, \"log\").mkdir());\r\n    Assert.assertTrue(new File(homeDir, \"temp\").mkdir());\r\n    HttpFSServerWebApp.setHomeDirForCurrentThread(homeDir.getAbsolutePath());\r\n    File secretFile = new File(new File(homeDir, \"conf\"), \"secret\");\r\n    Writer w = new FileWriter(secretFile);\r\n    w.write(\"secret\");\r\n    w.close();\r\n    File hadoopConfDir = new File(new File(homeDir, \"conf\"), \"hadoop-conf\");\r\n    hadoopConfDir.mkdirs();\r\n    Configuration hdfsConf = TestHdfsHelper.getHdfsConf();\r\n    Configuration conf = new Configuration(hdfsConf);\r\n    conf.setBoolean(DFSConfigKeys.DFS_NAMENODE_ACLS_ENABLED_KEY, true);\r\n    conf.setBoolean(DFSConfigKeys.DFS_NAMENODE_XATTRS_ENABLED_KEY, true);\r\n    conf.set(DFSConfigKeys.DFS_STORAGE_POLICY_SATISFIER_MODE_KEY, StoragePolicySatisfierMode.EXTERNAL.toString());\r\n    File hdfsSite = new File(hadoopConfDir, \"hdfs-site.xml\");\r\n    OutputStream os = new FileOutputStream(hdfsSite);\r\n    conf.writeXml(os);\r\n    os.close();\r\n    conf = new Configuration(false);\r\n    if (addDelegationTokenAuthHandler) {\r\n        conf.set(HttpFSAuthenticationFilter.HADOOP_HTTP_CONF_PREFIX + AuthenticationFilter.AUTH_TYPE, HttpFSKerberosAuthenticationHandlerForTesting.class.getName());\r\n    }\r\n    conf.set(\"httpfs.services.ext\", MockGroups.class.getName());\r\n    conf.set(\"httpfs.admin.group\", HadoopUsersConfTestHelper.getHadoopUserGroups(HadoopUsersConfTestHelper.getHadoopUsers()[0])[0]);\r\n    conf.set(\"httpfs.proxyuser.\" + HadoopUsersConfTestHelper.getHadoopProxyUser() + \".groups\", HadoopUsersConfTestHelper.getHadoopProxyUserGroups());\r\n    conf.set(\"httpfs.proxyuser.\" + HadoopUsersConfTestHelper.getHadoopProxyUser() + \".hosts\", HadoopUsersConfTestHelper.getHadoopProxyUserHosts());\r\n    conf.set(HttpFSAuthenticationFilter.HADOOP_HTTP_CONF_PREFIX + AuthenticationFilter.SIGNATURE_SECRET_FILE, secretFile.getAbsolutePath());\r\n    conf.set(\"httpfs.hadoop.config.dir\", hadoopConfDir.toString());\r\n    if (sslEnabled) {\r\n        conf.set(\"httpfs.ssl.enabled\", \"true\");\r\n    }\r\n    File httpfsSite = new File(new File(homeDir, \"conf\"), \"httpfs-site.xml\");\r\n    os = new FileOutputStream(httpfsSite);\r\n    conf.writeXml(os);\r\n    os.close();\r\n    return conf;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\test\\java\\org\\apache\\hadoop\\fs\\http\\server",
  "methodName" : "writeConf",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void writeConf(Configuration conf, String sitename) throws Exception\n{\r\n    File homeDir = TestDirHelper.getTestDir();\r\n    File hadoopConfDir = new File(new File(homeDir, \"conf\"), \"hadoop-conf\");\r\n    Assert.assertTrue(hadoopConfDir.exists());\r\n    File siteFile = new File(hadoopConfDir, sitename);\r\n    OutputStream os = new FileOutputStream(siteFile);\r\n    conf.writeXml(os);\r\n    os.close();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\test\\java\\org\\apache\\hadoop\\fs\\http\\server",
  "methodName" : "createHttpFSServer",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "Server createHttpFSServer(boolean addDelegationTokenAuthHandler, boolean sslEnabled) throws Exception\n{\r\n    Configuration conf = createHttpFSConf(addDelegationTokenAuthHandler, sslEnabled);\r\n    ClassLoader cl = Thread.currentThread().getContextClassLoader();\r\n    URL url = cl.getResource(\"webapp\");\r\n    WebAppContext context = new WebAppContext(url.getPath(), \"/webhdfs\");\r\n    Server server = TestJettyHelper.getJettyServer();\r\n    server.setHandler(context);\r\n    server.start();\r\n    if (addDelegationTokenAuthHandler) {\r\n        HttpFSServerWebApp.get().setAuthority(TestJettyHelper.getAuthority());\r\n    }\r\n    return server;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\test\\java\\org\\apache\\hadoop\\fs\\http\\server",
  "methodName" : "getSignedTokenString",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "String getSignedTokenString() throws Exception\n{\r\n    AuthenticationToken token = new AuthenticationToken(\"u\", \"p\", new KerberosDelegationTokenAuthenticationHandler().getType());\r\n    token.setExpires(System.currentTimeMillis() + 100000000);\r\n    SignerSecretProvider secretProvider = StringSignerSecretProviderCreator.newStringSignerSecretProvider();\r\n    Properties secretProviderProps = new Properties();\r\n    secretProviderProps.setProperty(AuthenticationFilter.SIGNATURE_SECRET, \"secret\");\r\n    secretProvider.init(secretProviderProps, null, -1);\r\n    Signer signer = new Signer(secretProvider);\r\n    return signer.sign(token.toString());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\test\\java\\org\\apache\\hadoop\\fs\\http\\server",
  "methodName" : "delegationTokenCommonTests",
  "errType" : null,
  "containingMethodsNum" : 39,
  "sourceCodeText" : "void delegationTokenCommonTests(boolean sslEnabled) throws Exception\n{\r\n    URL url = new URL(TestJettyHelper.getJettyURL(), \"/webhdfs/v1/?op=GETHOMEDIRECTORY\");\r\n    HttpURLConnection conn = (HttpURLConnection) url.openConnection();\r\n    Assert.assertEquals(HttpURLConnection.HTTP_UNAUTHORIZED, conn.getResponseCode());\r\n    String tokenSigned = getSignedTokenString();\r\n    url = new URL(TestJettyHelper.getJettyURL(), \"/webhdfs/v1/?op=GETDELEGATIONTOKEN\");\r\n    conn = (HttpURLConnection) url.openConnection();\r\n    conn.setRequestProperty(\"Cookie\", AuthenticatedURL.AUTH_COOKIE + \"=\" + tokenSigned);\r\n    Assert.assertEquals(HttpURLConnection.HTTP_OK, conn.getResponseCode());\r\n    JSONObject json = (JSONObject) new JSONParser().parse(new InputStreamReader(conn.getInputStream()));\r\n    json = (JSONObject) json.get(DelegationTokenAuthenticator.DELEGATION_TOKEN_JSON);\r\n    String tokenStr = (String) json.get(DelegationTokenAuthenticator.DELEGATION_TOKEN_URL_STRING_JSON);\r\n    Token<AbstractDelegationTokenIdentifier> dToken = new Token<AbstractDelegationTokenIdentifier>();\r\n    dToken.decodeFromUrlString(tokenStr);\r\n    Assert.assertEquals(sslEnabled ? WebHdfsConstants.SWEBHDFS_TOKEN_KIND : WebHdfsConstants.WEBHDFS_TOKEN_KIND, dToken.getKind());\r\n    url = new URL(TestJettyHelper.getJettyURL(), \"/webhdfs/v1/?op=GETHOMEDIRECTORY&delegation=\" + tokenStr);\r\n    conn = (HttpURLConnection) url.openConnection();\r\n    Assert.assertEquals(HttpURLConnection.HTTP_OK, conn.getResponseCode());\r\n    url = new URL(TestJettyHelper.getJettyURL(), \"/webhdfs/v1/?op=RENEWDELEGATIONTOKEN&token=\" + tokenStr);\r\n    conn = (HttpURLConnection) url.openConnection();\r\n    conn.setRequestMethod(\"PUT\");\r\n    Assert.assertEquals(HttpURLConnection.HTTP_UNAUTHORIZED, conn.getResponseCode());\r\n    url = new URL(TestJettyHelper.getJettyURL(), \"/webhdfs/v1/?op=RENEWDELEGATIONTOKEN&token=\" + tokenStr);\r\n    conn = (HttpURLConnection) url.openConnection();\r\n    conn.setRequestMethod(\"PUT\");\r\n    conn.setRequestProperty(\"Cookie\", AuthenticatedURL.AUTH_COOKIE + \"=\" + tokenSigned);\r\n    Assert.assertEquals(HttpURLConnection.HTTP_OK, conn.getResponseCode());\r\n    url = new URL(TestJettyHelper.getJettyURL(), \"/webhdfs/v1/?op=CANCELDELEGATIONTOKEN&token=\" + tokenStr);\r\n    conn = (HttpURLConnection) url.openConnection();\r\n    conn.setRequestMethod(\"PUT\");\r\n    Assert.assertEquals(HttpURLConnection.HTTP_OK, conn.getResponseCode());\r\n    url = new URL(TestJettyHelper.getJettyURL(), \"/webhdfs/v1/?op=GETHOMEDIRECTORY&delegation=\" + tokenStr);\r\n    conn = (HttpURLConnection) url.openConnection();\r\n    Assert.assertEquals(HttpURLConnection.HTTP_FORBIDDEN, conn.getResponseCode());\r\n    url = new URL(TestJettyHelper.getJettyURL(), \"/webhdfs/v1/?op=GETTRASHROOT&delegation=\" + tokenStr);\r\n    conn = (HttpURLConnection) url.openConnection();\r\n    Assert.assertEquals(HttpURLConnection.HTTP_FORBIDDEN, conn.getResponseCode());\r\n    url = new URL(TestJettyHelper.getJettyURL(), \"/webhdfs/v1/?op=GETTRASHROOT\");\r\n    conn = (HttpURLConnection) url.openConnection();\r\n    conn.setRequestProperty(\"Cookie\", AuthenticatedURL.AUTH_COOKIE + \"=\" + tokenSigned);\r\n    Assert.assertEquals(HttpURLConnection.HTTP_OK, conn.getResponseCode());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\test\\java\\org\\apache\\hadoop\\fs\\http\\server",
  "methodName" : "instrumentation",
  "errType" : null,
  "containingMethodsNum" : 17,
  "sourceCodeText" : "void instrumentation() throws Exception\n{\r\n    createHttpFSServer(false, false);\r\n    URL url = new URL(TestJettyHelper.getJettyURL(), MessageFormat.format(\"/webhdfs/v1?user.name={0}&op=instrumentation\", \"nobody\"));\r\n    HttpURLConnection conn = (HttpURLConnection) url.openConnection();\r\n    Assert.assertEquals(conn.getResponseCode(), HttpURLConnection.HTTP_UNAUTHORIZED);\r\n    url = new URL(TestJettyHelper.getJettyURL(), MessageFormat.format(\"/webhdfs/v1?user.name={0}&op=instrumentation\", HadoopUsersConfTestHelper.getHadoopUsers()[0]));\r\n    conn = (HttpURLConnection) url.openConnection();\r\n    Assert.assertEquals(conn.getResponseCode(), HttpURLConnection.HTTP_OK);\r\n    BufferedReader reader = new BufferedReader(new InputStreamReader(conn.getInputStream()));\r\n    String line = reader.readLine();\r\n    reader.close();\r\n    Assert.assertTrue(line.contains(\"\\\"counters\\\":{\"));\r\n    url = new URL(TestJettyHelper.getJettyURL(), MessageFormat.format(\"/webhdfs/v1/foo?user.name={0}&op=instrumentation\", HadoopUsersConfTestHelper.getHadoopUsers()[0]));\r\n    conn = (HttpURLConnection) url.openConnection();\r\n    Assert.assertEquals(conn.getResponseCode(), HttpURLConnection.HTTP_BAD_REQUEST);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\test\\java\\org\\apache\\hadoop\\fs\\http\\server",
  "methodName" : "testHdfsAccess",
  "errType" : null,
  "containingMethodsNum" : 11,
  "sourceCodeText" : "void testHdfsAccess() throws Exception\n{\r\n    createHttpFSServer(false, false);\r\n    long oldOpsListStatus = metricsGetter.get(\"LISTSTATUS\").call();\r\n    String user = HadoopUsersConfTestHelper.getHadoopUsers()[0];\r\n    URL url = new URL(TestJettyHelper.getJettyURL(), MessageFormat.format(\"/webhdfs/v1/?user.name={0}&op=liststatus\", user));\r\n    HttpURLConnection conn = (HttpURLConnection) url.openConnection();\r\n    Assert.assertEquals(conn.getResponseCode(), HttpURLConnection.HTTP_OK);\r\n    BufferedReader reader = new BufferedReader(new InputStreamReader(conn.getInputStream()));\r\n    reader.readLine();\r\n    reader.close();\r\n    Assert.assertEquals(1 + oldOpsListStatus, (long) metricsGetter.get(\"LISTSTATUS\").call());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\test\\java\\org\\apache\\hadoop\\fs\\http\\server",
  "methodName" : "testMkdirs",
  "errType" : null,
  "containingMethodsNum" : 12,
  "sourceCodeText" : "void testMkdirs() throws Exception\n{\r\n    createHttpFSServer(false, false);\r\n    long oldMkdirOpsStat = metricsGetter.get(\"MKDIRS\").call();\r\n    String user = HadoopUsersConfTestHelper.getHadoopUsers()[0];\r\n    URL url = new URL(TestJettyHelper.getJettyURL(), MessageFormat.format(\"/webhdfs/v1/tmp/sub-tmp?user.name={0}&op=MKDIRS\", user));\r\n    HttpURLConnection conn = (HttpURLConnection) url.openConnection();\r\n    conn.setRequestMethod(\"PUT\");\r\n    conn.connect();\r\n    Assert.assertEquals(conn.getResponseCode(), HttpURLConnection.HTTP_OK);\r\n    getStatus(\"/tmp/sub-tmp\", \"LISTSTATUS\");\r\n    long opsStat = metricsGetter.get(\"MKDIRS\").call();\r\n    Assert.assertEquals(1 + oldMkdirOpsStat, opsStat);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\test\\java\\org\\apache\\hadoop\\fs\\http\\server",
  "methodName" : "testGlobFilter",
  "errType" : null,
  "containingMethodsNum" : 14,
  "sourceCodeText" : "void testGlobFilter() throws Exception\n{\r\n    createHttpFSServer(false, false);\r\n    long oldOpsListStatus = metricsGetter.get(\"LISTSTATUS\").call();\r\n    FileSystem fs = FileSystem.get(TestHdfsHelper.getHdfsConf());\r\n    fs.mkdirs(new Path(\"/tmp\"));\r\n    fs.create(new Path(\"/tmp/foo.txt\")).close();\r\n    String user = HadoopUsersConfTestHelper.getHadoopUsers()[0];\r\n    URL url = new URL(TestJettyHelper.getJettyURL(), MessageFormat.format(\"/webhdfs/v1/tmp?user.name={0}&op=liststatus&filter=f*\", user));\r\n    HttpURLConnection conn = (HttpURLConnection) url.openConnection();\r\n    Assert.assertEquals(conn.getResponseCode(), HttpURLConnection.HTTP_OK);\r\n    BufferedReader reader = new BufferedReader(new InputStreamReader(conn.getInputStream()));\r\n    reader.readLine();\r\n    reader.close();\r\n    Assert.assertEquals(1 + oldOpsListStatus, (long) metricsGetter.get(\"LISTSTATUS\").call());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\test\\java\\org\\apache\\hadoop\\fs\\http\\server",
  "methodName" : "createWithHttp",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void createWithHttp(String filename, String perms) throws Exception\n{\r\n    createWithHttp(filename, perms, null);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\test\\java\\org\\apache\\hadoop\\fs\\http\\server",
  "methodName" : "createWithHttp",
  "errType" : null,
  "containingMethodsNum" : 11,
  "sourceCodeText" : "void createWithHttp(String filename, String perms, String unmaskedPerms) throws Exception\n{\r\n    String user = HadoopUsersConfTestHelper.getHadoopUsers()[0];\r\n    if (filename.charAt(0) == '/') {\r\n        filename = filename.substring(1);\r\n    }\r\n    String pathOps;\r\n    if (perms == null) {\r\n        pathOps = MessageFormat.format(\"/webhdfs/v1/{0}?user.name={1}&op=CREATE\", filename, user);\r\n    } else {\r\n        pathOps = MessageFormat.format(\"/webhdfs/v1/{0}?user.name={1}&permission={2}&op=CREATE\", filename, user, perms);\r\n    }\r\n    if (unmaskedPerms != null) {\r\n        pathOps = pathOps + \"&unmaskedpermission=\" + unmaskedPerms;\r\n    }\r\n    URL url = new URL(TestJettyHelper.getJettyURL(), pathOps);\r\n    HttpURLConnection conn = (HttpURLConnection) url.openConnection();\r\n    conn.addRequestProperty(\"Content-Type\", \"application/octet-stream\");\r\n    conn.setRequestMethod(\"PUT\");\r\n    conn.connect();\r\n    Assert.assertEquals(HttpURLConnection.HTTP_CREATED, conn.getResponseCode());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\test\\java\\org\\apache\\hadoop\\fs\\http\\server",
  "methodName" : "createDirWithHttp",
  "errType" : null,
  "containingMethodsNum" : 12,
  "sourceCodeText" : "void createDirWithHttp(String dirname, String perms, String unmaskedPerms) throws Exception\n{\r\n    long oldOpsMkdir = metricsGetter.get(\"MKDIRS\").call();\r\n    String user = HadoopUsersConfTestHelper.getHadoopUsers()[0];\r\n    if (dirname.charAt(0) == '/') {\r\n        dirname = dirname.substring(1);\r\n    }\r\n    String pathOps;\r\n    if (perms == null) {\r\n        pathOps = MessageFormat.format(\"/webhdfs/v1/{0}?user.name={1}&op=MKDIRS\", dirname, user);\r\n    } else {\r\n        pathOps = MessageFormat.format(\"/webhdfs/v1/{0}?user.name={1}&permission={2}&op=MKDIRS\", dirname, user, perms);\r\n    }\r\n    if (unmaskedPerms != null) {\r\n        pathOps = pathOps + \"&unmaskedpermission=\" + unmaskedPerms;\r\n    }\r\n    URL url = new URL(TestJettyHelper.getJettyURL(), pathOps);\r\n    HttpURLConnection conn = (HttpURLConnection) url.openConnection();\r\n    conn.setRequestMethod(\"PUT\");\r\n    conn.connect();\r\n    Assert.assertEquals(HttpURLConnection.HTTP_OK, conn.getResponseCode());\r\n    Assert.assertEquals(1 + oldOpsMkdir, (long) metricsGetter.get(\"MKDIRS\").call());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\test\\java\\org\\apache\\hadoop\\fs\\http\\server",
  "methodName" : "getStatus",
  "errType" : null,
  "containingMethodsNum" : 13,
  "sourceCodeText" : "String getStatus(String filename, String command) throws Exception\n{\r\n    long oldOpsStat = metricsGetter.getOrDefault(command, defaultEntryMetricGetter).call();\r\n    String user = HadoopUsersConfTestHelper.getHadoopUsers()[0];\r\n    if (filename.charAt(0) == '/') {\r\n        filename = filename.substring(1);\r\n    }\r\n    String pathOps = MessageFormat.format(\"/webhdfs/v1/{0}?user.name={1}&op={2}\", filename, user, command);\r\n    URL url = new URL(TestJettyHelper.getJettyURL(), pathOps);\r\n    HttpURLConnection conn = (HttpURLConnection) url.openConnection();\r\n    conn.connect();\r\n    Assert.assertEquals(HttpURLConnection.HTTP_OK, conn.getResponseCode());\r\n    BufferedReader reader = new BufferedReader(new InputStreamReader(conn.getInputStream()));\r\n    long opsStat = metricsGetter.getOrDefault(command, defaultExitMetricGetter).call();\r\n    Assert.assertEquals(oldOpsStat + 1L, opsStat);\r\n    return reader.readLine();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\test\\java\\org\\apache\\hadoop\\fs\\http\\server",
  "methodName" : "putCmd",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void putCmd(String filename, String command, String params) throws Exception\n{\r\n    Assert.assertEquals(HttpURLConnection.HTTP_OK, putCmdWithReturn(filename, command, params).getResponseCode());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\test\\java\\org\\apache\\hadoop\\fs\\http\\server",
  "methodName" : "putCmdWithReturn",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "HttpURLConnection putCmdWithReturn(String filename, String command, String params) throws Exception\n{\r\n    String user = HadoopUsersConfTestHelper.getHadoopUsers()[0];\r\n    if (filename.charAt(0) == '/') {\r\n        filename = filename.substring(1);\r\n    }\r\n    String pathOps = MessageFormat.format(\"/webhdfs/v1/{0}?user.name={1}{2}{3}&op={4}\", filename, user, (params == null) ? \"\" : \"&\", (params == null) ? \"\" : params, command);\r\n    URL url = new URL(TestJettyHelper.getJettyURL(), pathOps);\r\n    HttpURLConnection conn = (HttpURLConnection) url.openConnection();\r\n    conn.setRequestMethod(\"PUT\");\r\n    conn.connect();\r\n    return conn;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\test\\java\\org\\apache\\hadoop\\fs\\http\\server",
  "methodName" : "getPerms",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "String getPerms(String statusJson) throws Exception\n{\r\n    JSONParser parser = new JSONParser();\r\n    JSONObject jsonObject = (JSONObject) parser.parse(statusJson);\r\n    JSONObject details = (JSONObject) jsonObject.get(\"FileStatus\");\r\n    return (String) details.get(\"permission\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\test\\java\\org\\apache\\hadoop\\fs\\http\\server",
  "methodName" : "getPath",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "String getPath(String statusJson) throws Exception\n{\r\n    JSONParser parser = new JSONParser();\r\n    JSONObject details = (JSONObject) parser.parse(statusJson);\r\n    return (String) details.get(\"Path\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\test\\java\\org\\apache\\hadoop\\fs\\http\\server",
  "methodName" : "getAclEntries",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "List<String> getAclEntries(String statusJson) throws Exception\n{\r\n    List<String> entries = new ArrayList<String>();\r\n    JSONParser parser = new JSONParser();\r\n    JSONObject jsonObject = (JSONObject) parser.parse(statusJson);\r\n    JSONObject details = (JSONObject) jsonObject.get(\"AclStatus\");\r\n    JSONArray jsonEntries = (JSONArray) details.get(\"entries\");\r\n    if (jsonEntries != null) {\r\n        for (Object e : jsonEntries) {\r\n            entries.add(e.toString());\r\n        }\r\n    }\r\n    return entries;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\test\\java\\org\\apache\\hadoop\\fs\\http\\server",
  "methodName" : "getXAttrs",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "Map<String, byte[]> getXAttrs(String statusJson) throws Exception\n{\r\n    Map<String, byte[]> xAttrs = Maps.newHashMap();\r\n    JSONParser parser = new JSONParser();\r\n    JSONObject jsonObject = (JSONObject) parser.parse(statusJson);\r\n    JSONArray jsonXAttrs = (JSONArray) jsonObject.get(\"XAttrs\");\r\n    if (jsonXAttrs != null) {\r\n        for (Object a : jsonXAttrs) {\r\n            String name = (String) ((JSONObject) a).get(\"name\");\r\n            String value = (String) ((JSONObject) a).get(\"value\");\r\n            xAttrs.put(name, decodeXAttrValue(value));\r\n        }\r\n    }\r\n    return xAttrs;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\test\\java\\org\\apache\\hadoop\\fs\\http\\server",
  "methodName" : "decodeXAttrValue",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "byte[] decodeXAttrValue(String value) throws IOException\n{\r\n    if (value != null) {\r\n        return XAttrCodec.decodeValue(value);\r\n    } else {\r\n        return new byte[0];\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\test\\java\\org\\apache\\hadoop\\fs\\http\\server",
  "methodName" : "findAclWithName",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "AclEntry findAclWithName(AclStatus stat, String name) throws IOException\n{\r\n    AclEntry relevantAcl = null;\r\n    Iterator<AclEntry> it = stat.getEntries().iterator();\r\n    while (it.hasNext()) {\r\n        AclEntry e = it.next();\r\n        if (e.getName().equals(name)) {\r\n            relevantAcl = e;\r\n            break;\r\n        }\r\n    }\r\n    return relevantAcl;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\test\\java\\org\\apache\\hadoop\\fs\\http\\server",
  "methodName" : "testPerms",
  "errType" : null,
  "containingMethodsNum" : 15,
  "sourceCodeText" : "void testPerms() throws Exception\n{\r\n    createHttpFSServer(false, false);\r\n    FileSystem fs = FileSystem.get(TestHdfsHelper.getHdfsConf());\r\n    fs.mkdirs(new Path(\"/perm\"));\r\n    createWithHttp(\"/perm/none\", null);\r\n    String statusJson = getStatus(\"/perm/none\", \"GETFILESTATUS\");\r\n    Assert.assertTrue(\"755\".equals(getPerms(statusJson)));\r\n    createWithHttp(\"/perm/p-777\", \"777\");\r\n    statusJson = getStatus(\"/perm/p-777\", \"GETFILESTATUS\");\r\n    Assert.assertTrue(\"777\".equals(getPerms(statusJson)));\r\n    createWithHttp(\"/perm/p-654\", \"654\");\r\n    statusJson = getStatus(\"/perm/p-654\", \"GETFILESTATUS\");\r\n    Assert.assertTrue(\"654\".equals(getPerms(statusJson)));\r\n    createWithHttp(\"/perm/p-321\", \"321\");\r\n    statusJson = getStatus(\"/perm/p-321\", \"GETFILESTATUS\");\r\n    Assert.assertTrue(\"321\".equals(getPerms(statusJson)));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\test\\java\\org\\apache\\hadoop\\fs\\http\\server",
  "methodName" : "testXAttrs",
  "errType" : null,
  "containingMethodsNum" : 23,
  "sourceCodeText" : "void testXAttrs() throws Exception\n{\r\n    final String name1 = \"user.a1\";\r\n    final byte[] value1 = new byte[] { 0x31, 0x32, 0x33 };\r\n    final String name2 = \"user.a2\";\r\n    final byte[] value2 = new byte[] { 0x41, 0x42, 0x43 };\r\n    final String dir = \"/xattrTest\";\r\n    final String path = dir + \"/file\";\r\n    createHttpFSServer(false, false);\r\n    FileSystem fs = FileSystem.get(TestHdfsHelper.getHdfsConf());\r\n    fs.mkdirs(new Path(dir));\r\n    createWithHttp(path, null);\r\n    String statusJson = getStatus(path, \"GETXATTRS\");\r\n    Map<String, byte[]> xAttrs = getXAttrs(statusJson);\r\n    Assert.assertEquals(0, xAttrs.size());\r\n    putCmd(path, \"SETXATTR\", setXAttrParam(name1, value1));\r\n    putCmd(path, \"SETXATTR\", setXAttrParam(name2, value2));\r\n    statusJson = getStatus(path, \"GETXATTRS\");\r\n    xAttrs = getXAttrs(statusJson);\r\n    Assert.assertEquals(2, xAttrs.size());\r\n    Assert.assertArrayEquals(value1, xAttrs.get(name1));\r\n    Assert.assertArrayEquals(value2, xAttrs.get(name2));\r\n    putCmd(path, \"REMOVEXATTR\", \"xattr.name=\" + name1);\r\n    statusJson = getStatus(path, \"GETXATTRS\");\r\n    xAttrs = getXAttrs(statusJson);\r\n    Assert.assertEquals(1, xAttrs.size());\r\n    Assert.assertArrayEquals(value2, xAttrs.get(name2));\r\n    putCmd(path, \"REMOVEXATTR\", \"xattr.name=\" + name2);\r\n    statusJson = getStatus(path, \"GETXATTRS\");\r\n    xAttrs = getXAttrs(statusJson);\r\n    Assert.assertEquals(0, xAttrs.size());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\test\\java\\org\\apache\\hadoop\\fs\\http\\server",
  "methodName" : "setXAttrParam",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String setXAttrParam(String name, byte[] value) throws IOException\n{\r\n    return \"xattr.name=\" + name + \"&xattr.value=\" + XAttrCodec.encodeValue(value, XAttrCodec.HEX) + \"&encoding=hex&flag=create\";\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\test\\java\\org\\apache\\hadoop\\fs\\http\\server",
  "methodName" : "testFileAcls",
  "errType" : null,
  "containingMethodsNum" : 42,
  "sourceCodeText" : "void testFileAcls() throws Exception\n{\r\n    final String aclUser1 = \"user:foo:rw-\";\r\n    final String remAclUser1 = \"user:foo:\";\r\n    final String aclUser2 = \"user:bar:r--\";\r\n    final String aclGroup1 = \"group::r--\";\r\n    final String aclSpec = \"aclspec=user::rwx,\" + aclUser1 + \",\" + aclGroup1 + \",other::---\";\r\n    final String modAclSpec = \"aclspec=\" + aclUser2;\r\n    final String remAclSpec = \"aclspec=\" + remAclUser1;\r\n    final String dir = \"/aclFileTest\";\r\n    final String path = dir + \"/test\";\r\n    String statusJson;\r\n    List<String> aclEntries;\r\n    createHttpFSServer(false, false);\r\n    FileSystem fs = FileSystem.get(TestHdfsHelper.getHdfsConf());\r\n    fs.mkdirs(new Path(dir));\r\n    createWithHttp(path, null);\r\n    statusJson = getStatus(path, \"GETFILESTATUS\");\r\n    Assert.assertEquals(-1, statusJson.indexOf(\"aclBit\"));\r\n    statusJson = getStatus(dir, \"LISTSTATUS\");\r\n    Assert.assertEquals(-1, statusJson.indexOf(\"aclBit\"));\r\n    statusJson = getStatus(path, \"GETACLSTATUS\");\r\n    aclEntries = getAclEntries(statusJson);\r\n    Assert.assertTrue(aclEntries.size() == 0);\r\n    putCmd(path, \"SETACL\", aclSpec);\r\n    statusJson = getStatus(path, \"GETFILESTATUS\");\r\n    Assert.assertNotEquals(-1, statusJson.indexOf(\"aclBit\"));\r\n    statusJson = getStatus(dir, \"LISTSTATUS\");\r\n    Assert.assertNotEquals(-1, statusJson.indexOf(\"aclBit\"));\r\n    statusJson = getStatus(path, \"GETACLSTATUS\");\r\n    aclEntries = getAclEntries(statusJson);\r\n    Assert.assertTrue(aclEntries.size() == 2);\r\n    Assert.assertTrue(aclEntries.contains(aclUser1));\r\n    Assert.assertTrue(aclEntries.contains(aclGroup1));\r\n    putCmd(path, \"MODIFYACLENTRIES\", modAclSpec);\r\n    statusJson = getStatus(path, \"GETACLSTATUS\");\r\n    aclEntries = getAclEntries(statusJson);\r\n    Assert.assertTrue(aclEntries.size() == 3);\r\n    Assert.assertTrue(aclEntries.contains(aclUser1));\r\n    Assert.assertTrue(aclEntries.contains(aclUser2));\r\n    Assert.assertTrue(aclEntries.contains(aclGroup1));\r\n    putCmd(path, \"REMOVEACLENTRIES\", remAclSpec);\r\n    statusJson = getStatus(path, \"GETACLSTATUS\");\r\n    aclEntries = getAclEntries(statusJson);\r\n    Assert.assertTrue(aclEntries.size() == 2);\r\n    Assert.assertTrue(aclEntries.contains(aclUser2));\r\n    Assert.assertTrue(aclEntries.contains(aclGroup1));\r\n    putCmd(path, \"REMOVEACL\", null);\r\n    statusJson = getStatus(path, \"GETACLSTATUS\");\r\n    aclEntries = getAclEntries(statusJson);\r\n    Assert.assertTrue(aclEntries.size() == 0);\r\n    statusJson = getStatus(path, \"GETFILESTATUS\");\r\n    Assert.assertEquals(-1, statusJson.indexOf(\"aclBit\"));\r\n    statusJson = getStatus(dir, \"LISTSTATUS\");\r\n    Assert.assertEquals(-1, statusJson.indexOf(\"aclBit\"));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\test\\java\\org\\apache\\hadoop\\fs\\http\\server",
  "methodName" : "testDirAcls",
  "errType" : null,
  "containingMethodsNum" : 21,
  "sourceCodeText" : "void testDirAcls() throws Exception\n{\r\n    final String defUser1 = \"default:user:glarch:r-x\";\r\n    final String defSpec1 = \"aclspec=\" + defUser1;\r\n    final String dir = \"/aclDirTest\";\r\n    String statusJson;\r\n    List<String> aclEntries;\r\n    createHttpFSServer(false, false);\r\n    FileSystem fs = FileSystem.get(TestHdfsHelper.getHdfsConf());\r\n    fs.mkdirs(new Path(dir));\r\n    statusJson = getStatus(dir, \"GETFILESTATUS\");\r\n    Assert.assertEquals(-1, statusJson.indexOf(\"aclBit\"));\r\n    statusJson = getStatus(dir, \"GETACLSTATUS\");\r\n    aclEntries = getAclEntries(statusJson);\r\n    Assert.assertTrue(aclEntries.size() == 0);\r\n    putCmd(dir, \"SETACL\", defSpec1);\r\n    statusJson = getStatus(dir, \"GETFILESTATUS\");\r\n    Assert.assertNotEquals(-1, statusJson.indexOf(\"aclBit\"));\r\n    statusJson = getStatus(dir, \"GETACLSTATUS\");\r\n    aclEntries = getAclEntries(statusJson);\r\n    Assert.assertTrue(aclEntries.size() == 5);\r\n    Assert.assertTrue(aclEntries.contains(defUser1));\r\n    putCmd(dir, \"REMOVEDEFAULTACL\", null);\r\n    statusJson = getStatus(dir, \"GETFILESTATUS\");\r\n    Assert.assertEquals(-1, statusJson.indexOf(\"aclBit\"));\r\n    statusJson = getStatus(dir, \"GETACLSTATUS\");\r\n    aclEntries = getAclEntries(statusJson);\r\n    Assert.assertTrue(aclEntries.size() == 0);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\test\\java\\org\\apache\\hadoop\\fs\\http\\server",
  "methodName" : "testCustomizedUserAndGroupNames",
  "errType" : null,
  "containingMethodsNum" : 15,
  "sourceCodeText" : "void testCustomizedUserAndGroupNames() throws Exception\n{\r\n    Server server = createHttpFSServer(false, false);\r\n    final Configuration conf = HttpFSServerWebApp.get().get(FileSystemAccess.class).getFileSystemConfiguration();\r\n    conf.set(HdfsClientConfigKeys.DFS_WEBHDFS_USER_PATTERN_KEY, \"^[A-Za-z0-9_][A-Za-z0-9._-]*[$]?$\");\r\n    conf.set(HdfsClientConfigKeys.DFS_WEBHDFS_ACL_PERMISSION_PATTERN_KEY, \"^(default:)?(user|group|mask|other):\" + \"[[0-9A-Za-z_][@A-Za-z0-9._-]]*:([rwx-]{3})?(,(default:)?\" + \"(user|group|mask|other):[[0-9A-Za-z_][@A-Za-z0-9._-]]*:\" + \"([rwx-]{3})?)*$\");\r\n    writeConf(conf, \"hdfs-site.xml\");\r\n    server.stop();\r\n    server.start();\r\n    final String aclUser = \"user:123:rw-\";\r\n    final String aclGroup = \"group:foo@bar:r--\";\r\n    final String aclSpec = \"aclspec=user::rwx,\" + aclUser + \",group::rwx,\" + aclGroup + \",other::---\";\r\n    final String dir = \"/aclFileTestCustom\";\r\n    final String path = dir + \"/test\";\r\n    FileSystem fs = FileSystem.get(conf);\r\n    fs.mkdirs(new Path(dir));\r\n    createWithHttp(path, null);\r\n    putCmd(path, \"SETACL\", aclSpec);\r\n    String statusJson = getStatus(path, \"GETACLSTATUS\");\r\n    List<String> aclEntries = getAclEntries(statusJson);\r\n    Assert.assertTrue(aclEntries.contains(aclUser));\r\n    Assert.assertTrue(aclEntries.contains(aclGroup));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\test\\java\\org\\apache\\hadoop\\fs\\http\\server",
  "methodName" : "testOpenOffsetLength",
  "errType" : null,
  "containingMethodsNum" : 15,
  "sourceCodeText" : "void testOpenOffsetLength() throws Exception\n{\r\n    createHttpFSServer(false, false);\r\n    byte[] array = new byte[] { 0, 1, 2, 3 };\r\n    FileSystem fs = FileSystem.get(TestHdfsHelper.getHdfsConf());\r\n    fs.mkdirs(new Path(\"/tmp\"));\r\n    OutputStream os = fs.create(new Path(\"/tmp/foo\"));\r\n    os.write(array);\r\n    os.close();\r\n    String user = HadoopUsersConfTestHelper.getHadoopUsers()[0];\r\n    URL url = new URL(TestJettyHelper.getJettyURL(), MessageFormat.format(\"/webhdfs/v1/tmp/foo?user.name={0}&op=open&offset=1&length=2\", user));\r\n    HttpURLConnection conn = (HttpURLConnection) url.openConnection();\r\n    Assert.assertEquals(HttpURLConnection.HTTP_OK, conn.getResponseCode());\r\n    InputStream is = conn.getInputStream();\r\n    Assert.assertEquals(1, is.read());\r\n    Assert.assertEquals(2, is.read());\r\n    Assert.assertEquals(-1, is.read());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\test\\java\\org\\apache\\hadoop\\fs\\http\\server",
  "methodName" : "testCreateFileWithUnmaskedPermissions",
  "errType" : null,
  "containingMethodsNum" : 15,
  "sourceCodeText" : "void testCreateFileWithUnmaskedPermissions() throws Exception\n{\r\n    createHttpFSServer(false, false);\r\n    FileSystem fs = FileSystem.get(TestHdfsHelper.getHdfsConf());\r\n    fs.mkdirs(new Path(\"/tmp\"));\r\n    AclEntry acl = new org.apache.hadoop.fs.permission.AclEntry.Builder().setType(AclEntryType.USER).setScope(AclEntryScope.DEFAULT).setName(\"user2\").setPermission(FsAction.READ_WRITE).build();\r\n    fs.setAcl(new Path(\"/tmp\"), new ArrayList<AclEntry>(Arrays.asList(acl)));\r\n    String notUnmaskedFile = \"/tmp/notUnmasked\";\r\n    String unmaskedFile = \"/tmp/unmasked\";\r\n    createWithHttp(notUnmaskedFile, \"700\");\r\n    AclStatus aclStatus = fs.getAclStatus(new Path(notUnmaskedFile));\r\n    AclEntry theAcl = findAclWithName(aclStatus, \"user2\");\r\n    Assert.assertNotNull(theAcl);\r\n    Assert.assertEquals(FsAction.NONE, aclStatus.getEffectivePermission(theAcl));\r\n    createWithHttp(unmaskedFile, \"700\", \"777\");\r\n    aclStatus = fs.getAclStatus(new Path(unmaskedFile));\r\n    theAcl = findAclWithName(aclStatus, \"user2\");\r\n    Assert.assertNotNull(theAcl);\r\n    Assert.assertEquals(FsAction.READ_WRITE, aclStatus.getEffectivePermission(theAcl));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\test\\java\\org\\apache\\hadoop\\fs\\http\\server",
  "methodName" : "testMkdirWithUnmaskedPermissions",
  "errType" : null,
  "containingMethodsNum" : 15,
  "sourceCodeText" : "void testMkdirWithUnmaskedPermissions() throws Exception\n{\r\n    createHttpFSServer(false, false);\r\n    FileSystem fs = FileSystem.get(TestHdfsHelper.getHdfsConf());\r\n    fs.mkdirs(new Path(\"/tmp\"));\r\n    AclEntry acl = new org.apache.hadoop.fs.permission.AclEntry.Builder().setType(AclEntryType.USER).setScope(AclEntryScope.DEFAULT).setName(\"user2\").setPermission(FsAction.READ_WRITE).build();\r\n    fs.setAcl(new Path(\"/tmp\"), new ArrayList<AclEntry>(Arrays.asList(acl)));\r\n    String notUnmaskedDir = \"/tmp/notUnmaskedDir\";\r\n    String unmaskedDir = \"/tmp/unmaskedDir\";\r\n    createDirWithHttp(notUnmaskedDir, \"700\", null);\r\n    AclStatus aclStatus = fs.getAclStatus(new Path(notUnmaskedDir));\r\n    AclEntry theAcl = findAclWithName(aclStatus, \"user2\");\r\n    Assert.assertNotNull(theAcl);\r\n    Assert.assertEquals(FsAction.NONE, aclStatus.getEffectivePermission(theAcl));\r\n    createDirWithHttp(unmaskedDir, \"700\", \"777\");\r\n    aclStatus = fs.getAclStatus(new Path(unmaskedDir));\r\n    theAcl = findAclWithName(aclStatus, \"user2\");\r\n    Assert.assertNotNull(theAcl);\r\n    Assert.assertEquals(FsAction.READ_WRITE, aclStatus.getEffectivePermission(theAcl));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\test\\java\\org\\apache\\hadoop\\fs\\http\\server",
  "methodName" : "testPutNoOperation",
  "errType" : null,
  "containingMethodsNum" : 9,
  "sourceCodeText" : "void testPutNoOperation() throws Exception\n{\r\n    createHttpFSServer(false, false);\r\n    String user = HadoopUsersConfTestHelper.getHadoopUsers()[0];\r\n    URL url = new URL(TestJettyHelper.getJettyURL(), MessageFormat.format(\"/webhdfs/v1/foo?user.name={0}\", user));\r\n    HttpURLConnection conn = (HttpURLConnection) url.openConnection();\r\n    conn.setDoInput(true);\r\n    conn.setDoOutput(true);\r\n    conn.setRequestMethod(\"PUT\");\r\n    Assert.assertEquals(conn.getResponseCode(), HttpURLConnection.HTTP_BAD_REQUEST);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\test\\java\\org\\apache\\hadoop\\fs\\http\\server",
  "methodName" : "testGetTrashRoot",
  "errType" : null,
  "containingMethodsNum" : 16,
  "sourceCodeText" : "void testGetTrashRoot() throws Exception\n{\r\n    String user = HadoopUsersConfTestHelper.getHadoopUsers()[0];\r\n    createHttpFSServer(false, false);\r\n    String trashJson = getStatus(\"/\", \"GETTRASHROOT\");\r\n    String trashPath = getPath(trashJson);\r\n    Path expectedPath = new Path(FileSystem.USER_HOME_PREFIX, new Path(user, FileSystem.TRASH_PREFIX));\r\n    Assert.assertEquals(expectedPath.toUri().getPath(), trashPath);\r\n    byte[] array = new byte[] { 0, 1, 2, 3 };\r\n    FileSystem fs = FileSystem.get(TestHdfsHelper.getHdfsConf());\r\n    fs.mkdirs(new Path(\"/tmp\"));\r\n    OutputStream os = fs.create(new Path(\"/tmp/foo\"));\r\n    os.write(array);\r\n    os.close();\r\n    trashJson = getStatus(\"/tmp/foo\", \"GETTRASHROOT\");\r\n    trashPath = getPath(trashJson);\r\n    Assert.assertEquals(expectedPath.toUri().getPath(), trashPath);\r\n    final Path ezFile = TestHdfsHelper.ENCRYPTED_FILE;\r\n    final Path ezPath = TestHdfsHelper.ENCRYPTION_ZONE;\r\n    trashJson = getStatus(ezFile.toUri().getPath(), \"GETTRASHROOT\");\r\n    trashPath = getPath(trashJson);\r\n    expectedPath = new Path(ezPath, new Path(FileSystem.TRASH_PREFIX, user));\r\n    Assert.assertEquals(expectedPath.toUri().getPath(), trashPath);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\test\\java\\org\\apache\\hadoop\\fs\\http\\server",
  "methodName" : "testDelegationTokenOperations",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void testDelegationTokenOperations() throws Exception\n{\r\n    createHttpFSServer(true, false);\r\n    delegationTokenCommonTests(false);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\test\\java\\org\\apache\\hadoop\\fs\\http\\server",
  "methodName" : "snapshotTestPreconditions",
  "errType" : null,
  "containingMethodsNum" : 14,
  "sourceCodeText" : "HttpURLConnection snapshotTestPreconditions(String httpMethod, String snapOperation, String additionalParams) throws Exception\n{\r\n    String user = HadoopUsersConfTestHelper.getHadoopUsers()[0];\r\n    URL url = new URL(TestJettyHelper.getJettyURL(), MessageFormat.format(\"/webhdfs/v1/tmp/tmp-snap-test/subdir?user.name={0}&op=MKDIRS\", user));\r\n    HttpURLConnection conn = (HttpURLConnection) url.openConnection();\r\n    conn.setRequestMethod(\"PUT\");\r\n    conn.connect();\r\n    Assert.assertEquals(conn.getResponseCode(), HttpURLConnection.HTTP_OK);\r\n    Path snapshottablePath = new Path(\"/tmp/tmp-snap-test\");\r\n    DistributedFileSystem dfs = (DistributedFileSystem) FileSystem.get(snapshottablePath.toUri(), TestHdfsHelper.getHdfsConf());\r\n    dfs.allowSnapshot(snapshottablePath);\r\n    url = new URL(TestJettyHelper.getJettyURL(), MessageFormat.format(\"/webhdfs/v1/tmp/tmp-snap-test?user.name={0}&op={1}&{2}\", user, snapOperation, additionalParams));\r\n    conn = (HttpURLConnection) url.openConnection();\r\n    conn.setRequestMethod(httpMethod);\r\n    conn.connect();\r\n    return conn;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\test\\java\\org\\apache\\hadoop\\fs\\http\\server",
  "methodName" : "testAllowSnapshot",
  "errType" : null,
  "containingMethodsNum" : 13,
  "sourceCodeText" : "void testAllowSnapshot() throws Exception\n{\r\n    createHttpFSServer(false, false);\r\n    String pathString = \"/tmp/tmp-snap-allow-test\";\r\n    createDirWithHttp(pathString, \"700\", null);\r\n    Path path = new Path(pathString);\r\n    DistributedFileSystem dfs = (DistributedFileSystem) FileSystem.get(path.toUri(), TestHdfsHelper.getHdfsConf());\r\n    Assert.assertFalse(dfs.getFileStatus(path).isSnapshotEnabled());\r\n    String user = HadoopUsersConfTestHelper.getHadoopUsers()[0];\r\n    URL url = new URL(TestJettyHelper.getJettyURL(), MessageFormat.format(\"/webhdfs/v1{0}?user.name={1}&op=ALLOWSNAPSHOT\", pathString, user));\r\n    HttpURLConnection conn = (HttpURLConnection) url.openConnection();\r\n    conn.setRequestMethod(\"PUT\");\r\n    conn.connect();\r\n    Assert.assertEquals(conn.getResponseCode(), HttpURLConnection.HTTP_OK);\r\n    Assert.assertTrue(dfs.getFileStatus(path).isSnapshotEnabled());\r\n    dfs.delete(path, true);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\test\\java\\org\\apache\\hadoop\\fs\\http\\server",
  "methodName" : "testDisallowSnapshot",
  "errType" : null,
  "containingMethodsNum" : 14,
  "sourceCodeText" : "void testDisallowSnapshot() throws Exception\n{\r\n    createHttpFSServer(false, false);\r\n    String pathString = \"/tmp/tmp-snap-disallow-test\";\r\n    createDirWithHttp(pathString, \"700\", null);\r\n    Path path = new Path(pathString);\r\n    DistributedFileSystem dfs = (DistributedFileSystem) FileSystem.get(path.toUri(), TestHdfsHelper.getHdfsConf());\r\n    dfs.allowSnapshot(path);\r\n    Assert.assertTrue(dfs.getFileStatus(path).isSnapshotEnabled());\r\n    String user = HadoopUsersConfTestHelper.getHadoopUsers()[0];\r\n    URL url = new URL(TestJettyHelper.getJettyURL(), MessageFormat.format(\"/webhdfs/v1{0}?user.name={1}&op=DISALLOWSNAPSHOT\", pathString, user));\r\n    HttpURLConnection conn = (HttpURLConnection) url.openConnection();\r\n    conn.setRequestMethod(\"PUT\");\r\n    conn.connect();\r\n    Assert.assertEquals(conn.getResponseCode(), HttpURLConnection.HTTP_OK);\r\n    Assert.assertFalse(dfs.getFileStatus(path).isSnapshotEnabled());\r\n    dfs.delete(path, true);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\test\\java\\org\\apache\\hadoop\\fs\\http\\server",
  "methodName" : "testDisallowSnapshotException",
  "errType" : null,
  "containingMethodsNum" : 18,
  "sourceCodeText" : "void testDisallowSnapshotException() throws Exception\n{\r\n    createHttpFSServer(false, false);\r\n    String pathString = \"/tmp/tmp-snap-disallow-exception-test\";\r\n    createDirWithHttp(pathString, \"700\", null);\r\n    Path path = new Path(pathString);\r\n    DistributedFileSystem dfs = (DistributedFileSystem) FileSystem.get(path.toUri(), TestHdfsHelper.getHdfsConf());\r\n    dfs.allowSnapshot(path);\r\n    Assert.assertTrue(dfs.getFileStatus(path).isSnapshotEnabled());\r\n    dfs.createSnapshot(path, \"snap-01\");\r\n    dfs.createSnapshot(path, \"snap-02\");\r\n    String user = HadoopUsersConfTestHelper.getHadoopUsers()[0];\r\n    URL url = new URL(TestJettyHelper.getJettyURL(), MessageFormat.format(\"/webhdfs/v1{0}?user.name={1}&op=DISALLOWSNAPSHOT\", pathString, user));\r\n    HttpURLConnection conn = (HttpURLConnection) url.openConnection();\r\n    conn.setRequestMethod(\"PUT\");\r\n    conn.connect();\r\n    Assert.assertNotEquals(conn.getResponseCode(), HttpURLConnection.HTTP_OK);\r\n    Assert.assertTrue(dfs.getFileStatus(path).isSnapshotEnabled());\r\n    dfs.deleteSnapshot(path, \"snap-02\");\r\n    dfs.deleteSnapshot(path, \"snap-01\");\r\n    dfs.delete(path, true);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\test\\java\\org\\apache\\hadoop\\fs\\http\\server",
  "methodName" : "testCreateSnapshot",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void testCreateSnapshot() throws Exception\n{\r\n    createHttpFSServer(false, false);\r\n    final HttpURLConnection conn = snapshotTestPreconditions(\"PUT\", \"CREATESNAPSHOT\", \"snapshotname=snap-with-name\");\r\n    Assert.assertEquals(conn.getResponseCode(), HttpURLConnection.HTTP_OK);\r\n    final BufferedReader reader = new BufferedReader(new InputStreamReader(conn.getInputStream()));\r\n    String result = reader.readLine();\r\n    Assert.assertTrue(result.equals(\"{\\\"Path\\\":\\\"/tmp/tmp-snap-test/.snapshot/snap-with-name\\\"}\"));\r\n    result = getStatus(\"/tmp/tmp-snap-test/.snapshot\", \"LISTSTATUS\");\r\n    Assert.assertTrue(result.contains(\"snap-with-name\"));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\test\\java\\org\\apache\\hadoop\\fs\\http\\server",
  "methodName" : "testCreateSnapshotNoSnapshotName",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void testCreateSnapshotNoSnapshotName() throws Exception\n{\r\n    createHttpFSServer(false, false);\r\n    final HttpURLConnection conn = snapshotTestPreconditions(\"PUT\", \"CREATESNAPSHOT\", \"\");\r\n    Assert.assertEquals(conn.getResponseCode(), HttpURLConnection.HTTP_OK);\r\n    final BufferedReader reader = new BufferedReader(new InputStreamReader(conn.getInputStream()));\r\n    String result = reader.readLine();\r\n    Assert.assertTrue(Pattern.matches(\"(\\\\{\\\\\\\"Path\\\\\\\"\\\\:\\\\\\\"/tmp/tmp-snap-test/.snapshot/s)\" + \"(\\\\d{8})(-)(\\\\d{6})(\\\\.)(\\\\d{3})(\\\\\\\"\\\\})\", result));\r\n    result = getStatus(\"/tmp/tmp-snap-test/.snapshot\", \"LISTSTATUS\");\r\n    Assert.assertTrue(Pattern.matches(\"(.+)(\\\\\\\"pathSuffix\\\\\\\":\\\\\\\"s)\" + \"(\\\\d{8})(-)(\\\\d{6})(\\\\.)(\\\\d{3})(\\\\\\\")(.+)\", result));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\test\\java\\org\\apache\\hadoop\\fs\\http\\server",
  "methodName" : "testRenameSnapshot",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void testRenameSnapshot() throws Exception\n{\r\n    createHttpFSServer(false, false);\r\n    HttpURLConnection conn = snapshotTestPreconditions(\"PUT\", \"CREATESNAPSHOT\", \"snapshotname=snap-to-rename\");\r\n    Assert.assertEquals(conn.getResponseCode(), HttpURLConnection.HTTP_OK);\r\n    conn = snapshotTestPreconditions(\"PUT\", \"RENAMESNAPSHOT\", \"oldsnapshotname=snap-to-rename\" + \"&snapshotname=snap-renamed\");\r\n    Assert.assertEquals(conn.getResponseCode(), HttpURLConnection.HTTP_OK);\r\n    String result = getStatus(\"/tmp/tmp-snap-test/.snapshot\", \"LISTSTATUS\");\r\n    Assert.assertTrue(result.contains(\"snap-renamed\"));\r\n    Assert.assertFalse(result.contains(\"snap-to-rename\"));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\test\\java\\org\\apache\\hadoop\\fs\\http\\server",
  "methodName" : "testDelegationTokenOperationsSsl",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void testDelegationTokenOperationsSsl() throws Exception\n{\r\n    createHttpFSServer(true, true);\r\n    delegationTokenCommonTests(true);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\test\\java\\org\\apache\\hadoop\\fs\\http\\server",
  "methodName" : "testDeleteSnapshot",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void testDeleteSnapshot() throws Exception\n{\r\n    createHttpFSServer(false, false);\r\n    HttpURLConnection conn = snapshotTestPreconditions(\"PUT\", \"CREATESNAPSHOT\", \"snapshotname=snap-to-delete\");\r\n    Assert.assertEquals(conn.getResponseCode(), HttpURLConnection.HTTP_OK);\r\n    conn = snapshotTestPreconditions(\"DELETE\", \"DELETESNAPSHOT\", \"snapshotname=snap-to-delete\");\r\n    Assert.assertEquals(conn.getResponseCode(), HttpURLConnection.HTTP_OK);\r\n    String result = getStatus(\"/tmp/tmp-snap-test/.snapshot\", \"LISTSTATUS\");\r\n    Assert.assertFalse(result.contains(\"snap-to-delete\"));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\test\\java\\org\\apache\\hadoop\\fs\\http\\server",
  "methodName" : "sendRequestToHttpFSServer",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "HttpURLConnection sendRequestToHttpFSServer(String path, String op, String additionalParams) throws Exception\n{\r\n    String user = HadoopUsersConfTestHelper.getHadoopUsers()[0];\r\n    URL url = new URL(TestJettyHelper.getJettyURL(), MessageFormat.format(\"/webhdfs/v1{0}?user.name={1}&op={2}&{3}\", path, user, op, additionalParams));\r\n    HttpURLConnection conn = (HttpURLConnection) url.openConnection();\r\n    conn.setRequestMethod(\"GET\");\r\n    conn.connect();\r\n    return conn;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\test\\java\\org\\apache\\hadoop\\fs\\http\\server",
  "methodName" : "sendRequestGetSnapshotDiff",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "HttpURLConnection sendRequestGetSnapshotDiff(String path, String oldsnapshotname, String snapshotname) throws Exception\n{\r\n    return sendRequestToHttpFSServer(path, \"GETSNAPSHOTDIFF\", MessageFormat.format(\"oldsnapshotname={0}&snapshotname={1}\", oldsnapshotname, snapshotname));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\test\\java\\org\\apache\\hadoop\\fs\\http\\server",
  "methodName" : "testGetSnapshotDiff",
  "errType" : null,
  "containingMethodsNum" : 18,
  "sourceCodeText" : "void testGetSnapshotDiff() throws Exception\n{\r\n    createHttpFSServer(false, false);\r\n    String pathStr = \"/tmp/tmp-snap-diff-test\";\r\n    createDirWithHttp(pathStr, \"700\", null);\r\n    Path path = new Path(pathStr);\r\n    DistributedFileSystem dfs = (DistributedFileSystem) FileSystem.get(path.toUri(), TestHdfsHelper.getHdfsConf());\r\n    dfs.allowSnapshot(path);\r\n    Assert.assertTrue(dfs.getFileStatus(path).isSnapshotEnabled());\r\n    String file1 = pathStr + \"/file1\";\r\n    createWithHttp(file1, null);\r\n    dfs.createSnapshot(path, \"snap1\");\r\n    String file2 = pathStr + \"/file2\";\r\n    createWithHttp(file2, null);\r\n    dfs.createSnapshot(path, \"snap2\");\r\n    HttpURLConnection conn = sendRequestGetSnapshotDiff(pathStr, \"snap1\", \"snap2\");\r\n    Assert.assertEquals(conn.getResponseCode(), HttpURLConnection.HTTP_OK);\r\n    BufferedReader reader = new BufferedReader(new InputStreamReader(conn.getInputStream()));\r\n    String result = reader.readLine();\r\n    SnapshotDiffReport dfsDiffReport = dfs.getSnapshotDiffReport(path, \"snap1\", \"snap2\");\r\n    Assert.assertEquals(result, JsonUtil.toJsonString(dfsDiffReport));\r\n    dfs.deleteSnapshot(path, \"snap2\");\r\n    dfs.deleteSnapshot(path, \"snap1\");\r\n    dfs.delete(path, true);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\test\\java\\org\\apache\\hadoop\\fs\\http\\server",
  "methodName" : "testGetSnapshotDiffIllegalParam",
  "errType" : null,
  "containingMethodsNum" : 14,
  "sourceCodeText" : "void testGetSnapshotDiffIllegalParam() throws Exception\n{\r\n    createHttpFSServer(false, false);\r\n    String pathStr = \"/tmp/tmp-snap-diff-exc-test\";\r\n    createDirWithHttp(pathStr, \"700\", null);\r\n    Path path = new Path(pathStr);\r\n    DistributedFileSystem dfs = (DistributedFileSystem) FileSystem.get(path.toUri(), TestHdfsHelper.getHdfsConf());\r\n    dfs.allowSnapshot(path);\r\n    Assert.assertTrue(dfs.getFileStatus(path).isSnapshotEnabled());\r\n    HttpURLConnection conn = sendRequestGetSnapshotDiff(pathStr, \"\", \"\");\r\n    Assert.assertNotEquals(conn.getResponseCode(), HttpURLConnection.HTTP_OK);\r\n    sendRequestGetSnapshotDiff(pathStr, \"snap1\", \"\");\r\n    Assert.assertNotEquals(conn.getResponseCode(), HttpURLConnection.HTTP_OK);\r\n    sendRequestGetSnapshotDiff(pathStr, \"\", \"snap2\");\r\n    Assert.assertNotEquals(conn.getResponseCode(), HttpURLConnection.HTTP_OK);\r\n    sendRequestGetSnapshotDiff(pathStr, \"snap1\", \"snap2\");\r\n    Assert.assertNotEquals(conn.getResponseCode(), HttpURLConnection.HTTP_OK);\r\n    dfs.delete(path, true);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\test\\java\\org\\apache\\hadoop\\fs\\http\\server",
  "methodName" : "verifyGetSnapshottableDirectoryList",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void verifyGetSnapshottableDirectoryList(DistributedFileSystem dfs) throws Exception\n{\r\n    HttpURLConnection conn = sendRequestToHttpFSServer(\"/\", \"GETSNAPSHOTTABLEDIRECTORYLIST\", \"\");\r\n    Assert.assertEquals(conn.getResponseCode(), HttpURLConnection.HTTP_OK);\r\n    BufferedReader reader = new BufferedReader(new InputStreamReader(conn.getInputStream()));\r\n    String dirLst = reader.readLine();\r\n    SnapshottableDirectoryStatus[] dfsDirLst = dfs.getSnapshottableDirListing();\r\n    Assert.assertEquals(dirLst, JsonUtil.toJsonString(dfsDirLst));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\test\\java\\org\\apache\\hadoop\\fs\\http\\server",
  "methodName" : "verifyGetSnapshotList",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void verifyGetSnapshotList(DistributedFileSystem dfs, Path path) throws Exception\n{\r\n    HttpURLConnection conn = sendRequestToHttpFSServer(path.toString(), \"GETSNAPSHOTLIST\", \"\");\r\n    Assert.assertEquals(conn.getResponseCode(), HttpURLConnection.HTTP_OK);\r\n    BufferedReader reader = new BufferedReader(new InputStreamReader(conn.getInputStream()));\r\n    String dirLst = reader.readLine();\r\n    SnapshotStatus[] dfsDirLst = dfs.getSnapshotListing(path);\r\n    Assert.assertEquals(dirLst, JsonUtil.toJsonString(dfsDirLst));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\test\\java\\org\\apache\\hadoop\\fs\\http\\server",
  "methodName" : "testGetSnapshottableDirectoryList",
  "errType" : null,
  "containingMethodsNum" : 15,
  "sourceCodeText" : "void testGetSnapshottableDirectoryList() throws Exception\n{\r\n    createHttpFSServer(false, false);\r\n    String pathStr1 = \"/tmp/tmp-snap-dirlist-test-1\";\r\n    createDirWithHttp(pathStr1, \"700\", null);\r\n    Path path1 = new Path(pathStr1);\r\n    String pathStr2 = \"/tmp/tmp-snap-dirlist-test-2\";\r\n    createDirWithHttp(pathStr2, \"700\", null);\r\n    Path path2 = new Path(pathStr2);\r\n    DistributedFileSystem dfs = (DistributedFileSystem) FileSystem.get(path1.toUri(), TestHdfsHelper.getHdfsConf());\r\n    verifyGetSnapshottableDirectoryList(dfs);\r\n    dfs.allowSnapshot(path1);\r\n    Assert.assertTrue(dfs.getFileStatus(path1).isSnapshotEnabled());\r\n    verifyGetSnapshottableDirectoryList(dfs);\r\n    dfs.allowSnapshot(path2);\r\n    Assert.assertTrue(dfs.getFileStatus(path2).isSnapshotEnabled());\r\n    verifyGetSnapshottableDirectoryList(dfs);\r\n    dfs.delete(path2, true);\r\n    verifyGetSnapshottableDirectoryList(dfs);\r\n    dfs.delete(path1, true);\r\n    verifyGetSnapshottableDirectoryList(dfs);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\test\\java\\org\\apache\\hadoop\\fs\\http\\server",
  "methodName" : "testGetSnapshotList",
  "errType" : null,
  "containingMethodsNum" : 11,
  "sourceCodeText" : "void testGetSnapshotList() throws Exception\n{\r\n    createHttpFSServer(false, false);\r\n    String pathStr = \"/tmp/tmp-snap-list-test-1\";\r\n    createDirWithHttp(pathStr, \"700\", null);\r\n    Path path = new Path(pathStr);\r\n    DistributedFileSystem dfs = (DistributedFileSystem) FileSystem.get(path.toUri(), TestHdfsHelper.getHdfsConf());\r\n    dfs.allowSnapshot(path);\r\n    Assert.assertTrue(dfs.getFileStatus(path).isSnapshotEnabled());\r\n    verifyGetSnapshotList(dfs, path);\r\n    String file1 = pathStr + \"/file1\";\r\n    createWithHttp(file1, null);\r\n    dfs.createSnapshot(path, \"snap1\");\r\n    String file2 = pathStr + \"/file2\";\r\n    createWithHttp(file2, null);\r\n    dfs.createSnapshot(path, \"snap2\");\r\n    verifyGetSnapshotList(dfs, path);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\test\\java\\org\\apache\\hadoop\\fs\\http\\server",
  "methodName" : "testNoRedirect",
  "errType" : null,
  "containingMethodsNum" : 63,
  "sourceCodeText" : "void testNoRedirect() throws Exception\n{\r\n    createHttpFSServer(false, false);\r\n    final String testContent = \"Test content\";\r\n    final String path = \"/testfile.txt\";\r\n    final String username = HadoopUsersConfTestHelper.getHadoopUsers()[0];\r\n    URL url = new URL(TestJettyHelper.getJettyURL(), MessageFormat.format(\"/webhdfs/v1{0}?user.name={1}&op=CREATE&noredirect=true\", path, username));\r\n    HttpURLConnection conn = (HttpURLConnection) url.openConnection();\r\n    conn.setRequestMethod(HttpMethod.PUT);\r\n    conn.connect();\r\n    Assert.assertEquals(HttpURLConnection.HTTP_OK, conn.getResponseCode());\r\n    JSONObject json = (JSONObject) new JSONParser().parse(new InputStreamReader(conn.getInputStream()));\r\n    String location = (String) json.get(\"Location\");\r\n    Assert.assertTrue(location.contains(DataParam.NAME));\r\n    Assert.assertFalse(location.contains(NoRedirectParam.NAME));\r\n    Assert.assertTrue(location.contains(\"CREATE\"));\r\n    Assert.assertTrue(\"Wrong location: \" + location, location.startsWith(TestJettyHelper.getJettyURL().toString()));\r\n    url = new URL(location);\r\n    conn = (HttpURLConnection) url.openConnection();\r\n    conn.setRequestMethod(HttpMethod.PUT);\r\n    conn.setRequestProperty(\"Content-Type\", MediaType.APPLICATION_OCTET_STREAM);\r\n    conn.setDoOutput(true);\r\n    conn.connect();\r\n    OutputStream os = conn.getOutputStream();\r\n    os.write(testContent.getBytes());\r\n    os.close();\r\n    Assert.assertEquals(HttpURLConnection.HTTP_CREATED, conn.getResponseCode());\r\n    json = (JSONObject) new JSONParser().parse(new InputStreamReader(conn.getInputStream()));\r\n    location = (String) json.get(\"Location\");\r\n    Assert.assertEquals(TestJettyHelper.getJettyURL() + \"/webhdfs/v1\" + path, location);\r\n    url = new URL(TestJettyHelper.getJettyURL(), MessageFormat.format(\"/webhdfs/v1{0}?user.name={1}&op=OPEN&noredirect=true\", path, username));\r\n    conn = (HttpURLConnection) url.openConnection();\r\n    conn.setRequestMethod(HttpMethod.GET);\r\n    conn.connect();\r\n    Assert.assertEquals(HttpURLConnection.HTTP_OK, conn.getResponseCode());\r\n    json = (JSONObject) new JSONParser().parse(new InputStreamReader(conn.getInputStream()));\r\n    location = (String) json.get(\"Location\");\r\n    Assert.assertTrue(!location.contains(NoRedirectParam.NAME));\r\n    Assert.assertTrue(location.contains(\"OPEN\"));\r\n    Assert.assertTrue(\"Wrong location: \" + location, location.startsWith(TestJettyHelper.getJettyURL().toString()));\r\n    url = new URL(location);\r\n    conn = (HttpURLConnection) url.openConnection();\r\n    conn.setRequestMethod(HttpMethod.GET);\r\n    conn.connect();\r\n    Assert.assertEquals(HttpURLConnection.HTTP_OK, conn.getResponseCode());\r\n    String content = IOUtils.toString(conn.getInputStream(), StandardCharsets.UTF_8);\r\n    Assert.assertEquals(testContent, content);\r\n    url = new URL(TestJettyHelper.getJettyURL(), MessageFormat.format(\"/webhdfs/v1{0}?user.name={1}&op=GETFILECHECKSUM&noredirect=true\", path, username));\r\n    conn = (HttpURLConnection) url.openConnection();\r\n    conn.setRequestMethod(HttpMethod.GET);\r\n    conn.connect();\r\n    Assert.assertEquals(HttpURLConnection.HTTP_OK, conn.getResponseCode());\r\n    json = (JSONObject) new JSONParser().parse(new InputStreamReader(conn.getInputStream()));\r\n    location = (String) json.get(\"Location\");\r\n    Assert.assertTrue(!location.contains(NoRedirectParam.NAME));\r\n    Assert.assertTrue(location.contains(\"GETFILECHECKSUM\"));\r\n    Assert.assertTrue(\"Wrong location: \" + location, location.startsWith(TestJettyHelper.getJettyURL().toString()));\r\n    url = new URL(location);\r\n    conn = (HttpURLConnection) url.openConnection();\r\n    conn.setRequestMethod(HttpMethod.GET);\r\n    conn.connect();\r\n    Assert.assertEquals(HttpURLConnection.HTTP_OK, conn.getResponseCode());\r\n    json = (JSONObject) new JSONParser().parse(new InputStreamReader(conn.getInputStream()));\r\n    JSONObject checksum = (JSONObject) json.get(\"FileChecksum\");\r\n    Assert.assertEquals(\"0000020000000000000000001b9c0a445fed3c0bf1e1aa7438d96b1500000000\", checksum.get(\"bytes\"));\r\n    Assert.assertEquals(28L, checksum.get(\"length\"));\r\n    Assert.assertEquals(\"MD5-of-0MD5-of-512CRC32C\", checksum.get(\"algorithm\"));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\test\\java\\org\\apache\\hadoop\\fs\\http\\server",
  "methodName" : "verifyGetServerDefaults",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void verifyGetServerDefaults(DistributedFileSystem dfs) throws Exception\n{\r\n    HttpURLConnection conn = sendRequestToHttpFSServer(\"/\", \"GETSERVERDEFAULTS\", \"\");\r\n    Assert.assertEquals(conn.getResponseCode(), HttpURLConnection.HTTP_OK);\r\n    BufferedReader reader = new BufferedReader(new InputStreamReader(conn.getInputStream()));\r\n    String dirLst = reader.readLine();\r\n    FsServerDefaults dfsDirLst = dfs.getServerDefaults();\r\n    Assert.assertNotNull(dfsDirLst);\r\n    Assert.assertEquals(dirLst, JsonUtil.toJsonString(dfsDirLst));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\test\\java\\org\\apache\\hadoop\\fs\\http\\server",
  "methodName" : "testGetServerDefaults",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void testGetServerDefaults() throws Exception\n{\r\n    createHttpFSServer(false, false);\r\n    String pathStr1 = \"/\";\r\n    Path path1 = new Path(pathStr1);\r\n    DistributedFileSystem dfs = (DistributedFileSystem) FileSystem.get(path1.toUri(), TestHdfsHelper.getHdfsConf());\r\n    verifyGetServerDefaults(dfs);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\test\\java\\org\\apache\\hadoop\\fs\\http\\server",
  "methodName" : "testAccess",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void testAccess() throws Exception\n{\r\n    createHttpFSServer(false, false);\r\n    final String dir = \"/xattrTest\";\r\n    Path path1 = new Path(dir);\r\n    DistributedFileSystem dfs = (DistributedFileSystem) FileSystem.get(path1.toUri(), TestHdfsHelper.getHdfsConf());\r\n    dfs.mkdirs(new Path(dir));\r\n    HttpURLConnection conn = sendRequestToHttpFSServer(dir, \"CHECKACCESS\", \"fsaction=r--\");\r\n    Assert.assertEquals(HttpURLConnection.HTTP_OK, conn.getResponseCode());\r\n    HttpURLConnection conn1 = sendRequestToHttpFSServer(dir, \"CHECKACCESS\", \"fsaction=-w-\");\r\n    Assert.assertEquals(HttpURLConnection.HTTP_OK, conn1.getResponseCode());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\test\\java\\org\\apache\\hadoop\\fs\\http\\server",
  "methodName" : "testECPolicy",
  "errType" : null,
  "containingMethodsNum" : 16,
  "sourceCodeText" : "void testECPolicy() throws Exception\n{\r\n    createHttpFSServer(false, false);\r\n    final ErasureCodingPolicy ecPolicy = SystemErasureCodingPolicies.getByID(SystemErasureCodingPolicies.RS_3_2_POLICY_ID);\r\n    final String ecPolicyName = ecPolicy.getName();\r\n    final Path ecDir = new Path(\"/ec\");\r\n    DistributedFileSystem dfs = (DistributedFileSystem) FileSystem.get(ecDir.toUri(), TestHdfsHelper.getHdfsConf());\r\n    Path ecFile = new Path(ecDir, \"ec_file.txt\");\r\n    dfs.mkdirs(ecDir);\r\n    dfs.enableErasureCodingPolicy(ecPolicyName);\r\n    dfs.setErasureCodingPolicy(ecDir, ecPolicyName);\r\n    DFSTestUtil.createFile(dfs, ecFile, 1024, (short) 1, 0);\r\n    String getFileStatusResponse = getStatus(ecFile.toString(), \"GETFILESTATUS\");\r\n    JSONParser parser = new JSONParser();\r\n    JSONObject jsonObject = (JSONObject) parser.parse(getFileStatusResponse);\r\n    JSONObject details = (JSONObject) jsonObject.get(\"FileStatus\");\r\n    String ecpolicyForECfile = (String) details.get(\"ecPolicy\");\r\n    assertEquals(\"EC policy for ecFile should match the set EC policy\", ecpolicyForECfile, ecPolicyName);\r\n    WebHdfsFileSystem httpfsWebHdfs = (WebHdfsFileSystem) FileSystem.get(new URI(\"webhdfs://\" + TestJettyHelper.getJettyURL().toURI().getAuthority()), TestHdfsHelper.getHdfsConf());\r\n    HdfsFileStatus httpfsFileStatus = (HdfsFileStatus) httpfsWebHdfs.getFileStatus(ecFile);\r\n    assertNotNull(httpfsFileStatus.getErasureCodingPolicy());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\test\\java\\org\\apache\\hadoop\\fs\\http\\server",
  "methodName" : "testErasureCodingPolicy",
  "errType" : null,
  "containingMethodsNum" : 32,
  "sourceCodeText" : "void testErasureCodingPolicy() throws Exception\n{\r\n    createHttpFSServer(false, false);\r\n    final String dir = \"/ecPolicy\";\r\n    Path path1 = new Path(dir);\r\n    final ErasureCodingPolicy ecPolicy = SystemErasureCodingPolicies.getByID(SystemErasureCodingPolicies.RS_3_2_POLICY_ID);\r\n    final String ecPolicyName = ecPolicy.getName();\r\n    DistributedFileSystem dfs = (DistributedFileSystem) FileSystem.get(path1.toUri(), TestHdfsHelper.getHdfsConf());\r\n    dfs.mkdirs(new Path(dir));\r\n    dfs.enableErasureCodingPolicy(ecPolicyName);\r\n    HttpURLConnection conn = putCmdWithReturn(dir, \"SETECPOLICY\", \"ecpolicy=\" + ecPolicyName);\r\n    Assert.assertEquals(HttpURLConnection.HTTP_OK, conn.getResponseCode());\r\n    HttpURLConnection conn1 = sendRequestToHttpFSServer(dir, \"GETECPOLICY\", \"\");\r\n    Assert.assertEquals(conn1.getResponseCode(), HttpURLConnection.HTTP_OK);\r\n    BufferedReader reader = new BufferedReader(new InputStreamReader(conn1.getInputStream()));\r\n    String dirLst = reader.readLine();\r\n    ErasureCodingPolicy dfsDirLst = dfs.getErasureCodingPolicy(path1);\r\n    Assert.assertNotNull(dfsDirLst);\r\n    Assert.assertEquals(dirLst, JsonUtil.toJsonString(dfsDirLst));\r\n    String user = HadoopUsersConfTestHelper.getHadoopUsers()[0];\r\n    URL url = new URL(TestJettyHelper.getJettyURL(), MessageFormat.format(\"/webhdfs/v1{0}?user.name={1}&op={2}&{3}\", dir, user, \"UNSETECPOLICY\", \"\"));\r\n    HttpURLConnection conn2 = (HttpURLConnection) url.openConnection();\r\n    conn2.setRequestMethod(\"POST\");\r\n    conn2.connect();\r\n    Assert.assertEquals(HttpURLConnection.HTTP_OK, conn2.getResponseCode());\r\n    dfsDirLst = dfs.getErasureCodingPolicy(path1);\r\n    Assert.assertNull(dfsDirLst);\r\n    final String dir1 = \"/\";\r\n    HttpURLConnection conn3 = putCmdWithReturn(dir1, \"SETECPOLICY\", \"ecpolicy=\" + ecPolicyName);\r\n    Assert.assertEquals(HttpURLConnection.HTTP_OK, conn3.getResponseCode());\r\n    final String dir2 = \"/\";\r\n    URL url1 = new URL(TestJettyHelper.getJettyURL(), MessageFormat.format(\"/webhdfs/v1{0}?user.name={1}&op={2}&{3}\", dir2, user, \"UNSETECPOLICY\", \"\"));\r\n    HttpURLConnection conn4 = (HttpURLConnection) url1.openConnection();\r\n    conn4.setRequestMethod(\"POST\");\r\n    conn4.connect();\r\n    Assert.assertEquals(HttpURLConnection.HTTP_OK, conn4.getResponseCode());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\test\\java\\org\\apache\\hadoop\\fs\\http\\server",
  "methodName" : "testStoragePolicySatisfier",
  "errType" : null,
  "containingMethodsNum" : 11,
  "sourceCodeText" : "void testStoragePolicySatisfier() throws Exception\n{\r\n    createHttpFSServer(false, false);\r\n    final String dir = \"/parent\";\r\n    Path path1 = new Path(dir);\r\n    String file = \"/parent/file\";\r\n    Path filePath = new Path(file);\r\n    DistributedFileSystem dfs = (DistributedFileSystem) FileSystem.get(path1.toUri(), TestHdfsHelper.getHdfsConf());\r\n    dfs.mkdirs(path1);\r\n    dfs.create(filePath).close();\r\n    dfs.setStoragePolicy(filePath, HdfsConstants.COLD_STORAGE_POLICY_NAME);\r\n    BlockStoragePolicy storagePolicy = (BlockStoragePolicy) dfs.getStoragePolicy(filePath);\r\n    assertEquals(HdfsConstants.COLD_STORAGE_POLICY_NAME, storagePolicy.getName());\r\n    HttpURLConnection conn = putCmdWithReturn(dir, \"SATISFYSTORAGEPOLICY\", \"\");\r\n    Assert.assertEquals(HttpURLConnection.HTTP_OK, conn.getResponseCode());\r\n    Map<String, byte[]> xAttrs = dfs.getXAttrs(path1);\r\n    assertTrue(xAttrs.containsKey(HdfsServerConstants.XATTR_SATISFY_STORAGE_POLICY));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\test\\java\\org\\apache\\hadoop\\fs\\http\\server",
  "methodName" : "testNoRedirectWithData",
  "errType" : null,
  "containingMethodsNum" : 26,
  "sourceCodeText" : "void testNoRedirectWithData() throws Exception\n{\r\n    createHttpFSServer(false, false);\r\n    final String path = \"/file\";\r\n    final String username = HadoopUsersConfTestHelper.getHadoopUsers()[0];\r\n    URL url = new URL(TestJettyHelper.getJettyURL(), MessageFormat.format(\"/webhdfs/v1{0}?user.name={1}&op=CREATE&data=true&noredirect=true\", path, username));\r\n    HttpURLConnection conn = (HttpURLConnection) url.openConnection();\r\n    conn.setRequestMethod(HttpMethod.PUT);\r\n    conn.setRequestProperty(\"Content-Type\", MediaType.APPLICATION_OCTET_STREAM);\r\n    conn.setDoOutput(true);\r\n    conn.connect();\r\n    Assert.assertEquals(HttpURLConnection.HTTP_OK, conn.getResponseCode());\r\n    JSONObject json = (JSONObject) new JSONParser().parse(new InputStreamReader(conn.getInputStream()));\r\n    String location = (String) json.get(\"Location\");\r\n    Assert.assertTrue(location.contains(DataParam.NAME));\r\n    Assert.assertTrue(location.contains(\"CREATE\"));\r\n    url = new URL(location);\r\n    conn = (HttpURLConnection) url.openConnection();\r\n    conn.setRequestMethod(HttpMethod.PUT);\r\n    conn.setRequestProperty(\"Content-Type\", MediaType.APPLICATION_OCTET_STREAM);\r\n    conn.setDoOutput(true);\r\n    conn.connect();\r\n    final String writeStr = \"write some content\";\r\n    OutputStream os = conn.getOutputStream();\r\n    os.write(writeStr.getBytes());\r\n    os.close();\r\n    Assert.assertEquals(HttpURLConnection.HTTP_CREATED, conn.getResponseCode());\r\n    json = (JSONObject) new JSONParser().parse(new InputStreamReader(conn.getInputStream()));\r\n    location = (String) json.get(\"Location\");\r\n    Assert.assertEquals(TestJettyHelper.getJettyURL() + \"/webhdfs/v1\" + path, location);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\test\\java\\org\\apache\\hadoop\\fs\\http\\server",
  "methodName" : "testContentType",
  "errType" : null,
  "containingMethodsNum" : 12,
  "sourceCodeText" : "void testContentType() throws Exception\n{\r\n    createHttpFSServer(false, false);\r\n    FileSystem fs = FileSystem.get(TestHdfsHelper.getHdfsConf());\r\n    Path dir = new Path(\"/tmp\");\r\n    Path file = new Path(dir, \"foo\");\r\n    fs.mkdirs(dir);\r\n    fs.create(file);\r\n    String user = HadoopUsersConfTestHelper.getHadoopUsers()[0];\r\n    URL url = new URL(TestJettyHelper.getJettyURL(), MessageFormat.format(\"/webhdfs/v1/tmp/foo?user.name={0}&op=open&offset=1&length=2\", user));\r\n    final HttpURLConnection conn = (HttpURLConnection) url.openConnection();\r\n    conn.setRequestMethod(Operation.OPEN.getMethod());\r\n    conn.connect();\r\n    LambdaTestUtils.intercept(IOException.class, \"Content-Type \\\"text/html;charset=iso-8859-1\\\" \" + \"is incompatible with \\\"application/json\\\"\", () -> HttpFSUtils.jsonParse(conn));\r\n    conn.disconnect();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\test\\java\\org\\apache\\hadoop\\test",
  "methodName" : "testDirNoAnnotation",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testDirNoAnnotation() throws Exception\n{\r\n    TestDirHelper.getTestDir();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\test\\java\\org\\apache\\hadoop\\test",
  "methodName" : "testJettyNoAnnotation",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testJettyNoAnnotation() throws Exception\n{\r\n    TestJettyHelper.getJettyServer();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\test\\java\\org\\apache\\hadoop\\test",
  "methodName" : "testJettyNoAnnotation2",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testJettyNoAnnotation2() throws Exception\n{\r\n    TestJettyHelper.getJettyURL();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\test\\java\\org\\apache\\hadoop\\test",
  "methodName" : "testDirAnnotation",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testDirAnnotation() throws Exception\n{\r\n    assertNotNull(TestDirHelper.getTestDir());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\test\\java\\org\\apache\\hadoop\\test",
  "methodName" : "waitFor",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void waitFor()\n{\r\n    long start = Time.now();\r\n    long waited = waitFor(1000, new Predicate() {\r\n\r\n        @Override\r\n        public boolean evaluate() throws Exception {\r\n            return true;\r\n        }\r\n    });\r\n    long end = Time.now();\r\n    assertEquals(waited, 0, 50);\r\n    assertEquals(end - start - waited, 0, 50);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\test\\java\\org\\apache\\hadoop\\test",
  "methodName" : "waitForTimeOutRatio1",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void waitForTimeOutRatio1()\n{\r\n    setWaitForRatio(1);\r\n    long start = Time.now();\r\n    long waited = waitFor(200, new Predicate() {\r\n\r\n        @Override\r\n        public boolean evaluate() throws Exception {\r\n            return false;\r\n        }\r\n    });\r\n    long end = Time.now();\r\n    assertEquals(waited, -1);\r\n    assertEquals(end - start, 200, 50);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\test\\java\\org\\apache\\hadoop\\test",
  "methodName" : "waitForTimeOutRatio2",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void waitForTimeOutRatio2()\n{\r\n    setWaitForRatio(2);\r\n    long start = Time.now();\r\n    long waited = waitFor(200, new Predicate() {\r\n\r\n        @Override\r\n        public boolean evaluate() throws Exception {\r\n            return false;\r\n        }\r\n    });\r\n    long end = Time.now();\r\n    assertEquals(waited, -1);\r\n    assertEquals(end - start, 200 * getWaitForRatio(), 50 * getWaitForRatio());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\test\\java\\org\\apache\\hadoop\\test",
  "methodName" : "sleepRatio1",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void sleepRatio1()\n{\r\n    setWaitForRatio(1);\r\n    long start = Time.now();\r\n    sleep(100);\r\n    long end = Time.now();\r\n    assertEquals(end - start, 100, 50);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\test\\java\\org\\apache\\hadoop\\test",
  "methodName" : "sleepRatio2",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void sleepRatio2()\n{\r\n    setWaitForRatio(1);\r\n    long start = Time.now();\r\n    sleep(100);\r\n    long end = Time.now();\r\n    assertEquals(end - start, 100 * getWaitForRatio(), 50 * getWaitForRatio());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\test\\java\\org\\apache\\hadoop\\test",
  "methodName" : "testJetty",
  "errType" : null,
  "containingMethodsNum" : 11,
  "sourceCodeText" : "void testJetty() throws Exception\n{\r\n    ServletContextHandler context = new ServletContextHandler();\r\n    context.setContextPath(\"/\");\r\n    context.addServlet(MyServlet.class, \"/bar\");\r\n    Server server = TestJettyHelper.getJettyServer();\r\n    server.setHandler(context);\r\n    server.start();\r\n    URL url = new URL(TestJettyHelper.getJettyURL(), \"/bar\");\r\n    HttpURLConnection conn = (HttpURLConnection) url.openConnection();\r\n    assertEquals(conn.getResponseCode(), HttpURLConnection.HTTP_OK);\r\n    BufferedReader reader = new BufferedReader(new InputStreamReader(conn.getInputStream()));\r\n    assertEquals(reader.readLine(), \"foo\");\r\n    reader.close();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\test\\java\\org\\apache\\hadoop\\test",
  "methodName" : "testException0",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void testException0()\n{\r\n    throw new RuntimeException(\"foo\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\test\\java\\org\\apache\\hadoop\\test",
  "methodName" : "testException1",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void testException1()\n{\r\n    throw new RuntimeException(\"foo\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\test\\java\\org\\apache\\hadoop\\fs\\http\\server",
  "methodName" : "init",
  "errType" : null,
  "containingMethodsNum" : 14,
  "sourceCodeText" : "void init() throws Exception\n{\r\n    File homeDir = GenericTestUtils.setupTestRootDir(TestHttpFSServerWebServer.class);\r\n    File confDir = new File(homeDir, \"etc/hadoop\");\r\n    File logsDir = new File(homeDir, \"logs\");\r\n    File tempDir = new File(homeDir, \"temp\");\r\n    confDir.mkdirs();\r\n    logsDir.mkdirs();\r\n    tempDir.mkdirs();\r\n    if (Shell.WINDOWS) {\r\n        File binDir = new File(homeDir, \"bin\");\r\n        binDir.mkdirs();\r\n        File winutils = Shell.getWinUtilsFile();\r\n        if (winutils.exists()) {\r\n            FileUtils.copyFileToDirectory(winutils, binDir);\r\n        }\r\n    }\r\n    System.setProperty(\"hadoop.home.dir\", homeDir.getAbsolutePath());\r\n    System.setProperty(\"hadoop.log.dir\", logsDir.getAbsolutePath());\r\n    System.setProperty(\"httpfs.home.dir\", homeDir.getAbsolutePath());\r\n    System.setProperty(\"httpfs.log.dir\", logsDir.getAbsolutePath());\r\n    System.setProperty(\"httpfs.config.dir\", confDir.getAbsolutePath());\r\n    secretFile = new File(System.getProperty(\"httpfs.config.dir\"), \"httpfs-signature-custom.secret\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\test\\java\\org\\apache\\hadoop\\fs\\http\\server",
  "methodName" : "teardown",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void teardown() throws Exception\n{\r\n    if (webServer != null) {\r\n        webServer.stop();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\test\\java\\org\\apache\\hadoop\\fs\\http\\server",
  "methodName" : "testStartStop",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void testStartStop() throws Exception\n{\r\n    webServer = createWebServer(createConfigurationWithRandomSecret());\r\n    webServer.start();\r\n    webServer.stop();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\test\\java\\org\\apache\\hadoop\\fs\\http\\server",
  "methodName" : "testJustStop",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void testJustStop() throws Exception\n{\r\n    webServer = createWebServer(createConfigurationWithRandomSecret());\r\n    webServer.stop();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\test\\java\\org\\apache\\hadoop\\fs\\http\\server",
  "methodName" : "testDoubleStop",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void testDoubleStop() throws Exception\n{\r\n    webServer = createWebServer(createConfigurationWithRandomSecret());\r\n    webServer.start();\r\n    webServer.stop();\r\n    webServer.stop();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\test\\java\\org\\apache\\hadoop\\fs\\http\\server",
  "methodName" : "testDoubleStart",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void testDoubleStart() throws Exception\n{\r\n    webServer = createWebServer(createConfigurationWithRandomSecret());\r\n    webServer.start();\r\n    webServer.start();\r\n    webServer.stop();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\test\\java\\org\\apache\\hadoop\\fs\\http\\server",
  "methodName" : "testServiceWithSecretFile",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void testServiceWithSecretFile() throws Exception\n{\r\n    createSecretFile(\"foo\");\r\n    webServer = createWebServer(createConfigurationWithSecretFile());\r\n    webServer.start();\r\n    assertServiceRespondsWithOK(webServer.getUrl());\r\n    assertSignerSecretProviderType(webServer.getHttpServer(), FileSignerSecretProvider.class);\r\n    webServer.stop();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\test\\java\\org\\apache\\hadoop\\fs\\http\\server",
  "methodName" : "testServiceWithSecretFileWithDeprecatedConfigOnly",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void testServiceWithSecretFileWithDeprecatedConfigOnly() throws Exception\n{\r\n    createSecretFile(\"foo\");\r\n    Configuration conf = createConfiguration();\r\n    setDeprecatedSecretFile(conf, secretFile.getAbsolutePath());\r\n    webServer = createWebServer(conf);\r\n    webServer.start();\r\n    assertServiceRespondsWithOK(webServer.getUrl());\r\n    assertSignerSecretProviderType(webServer.getHttpServer(), FileSignerSecretProvider.class);\r\n    webServer.stop();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\test\\java\\org\\apache\\hadoop\\fs\\http\\server",
  "methodName" : "testServiceWithSecretFileWithBothConfigOptions",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void testServiceWithSecretFileWithBothConfigOptions() throws Exception\n{\r\n    createSecretFile(\"foo\");\r\n    Configuration conf = createConfigurationWithSecretFile();\r\n    setDeprecatedSecretFile(conf, secretFile.getAbsolutePath());\r\n    webServer = createWebServer(conf);\r\n    webServer.start();\r\n    assertServiceRespondsWithOK(webServer.getUrl());\r\n    assertSignerSecretProviderType(webServer.getHttpServer(), FileSignerSecretProvider.class);\r\n    webServer.stop();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\test\\java\\org\\apache\\hadoop\\fs\\http\\server",
  "methodName" : "testServiceWithMissingSecretFile",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void testServiceWithMissingSecretFile() throws Exception\n{\r\n    webServer = createWebServer(createConfigurationWithSecretFile());\r\n    webServer.start();\r\n    assertServiceRespondsWithOK(webServer.getUrl());\r\n    assertSignerSecretProviderType(webServer.getHttpServer(), RandomSignerSecretProvider.class);\r\n    webServer.stop();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\test\\java\\org\\apache\\hadoop\\fs\\http\\server",
  "methodName" : "testServiceWithEmptySecretFile",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void testServiceWithEmptySecretFile() throws Exception\n{\r\n    createSecretFile(\"\");\r\n    webServer = createWebServer(createConfigurationWithSecretFile());\r\n    webServer.start();\r\n    assertServiceRespondsWithOK(webServer.getUrl());\r\n    assertSignerSecretProviderType(webServer.getHttpServer(), RandomSignerSecretProvider.class);\r\n    webServer.stop();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\test\\java\\org\\apache\\hadoop\\fs\\http\\server",
  "methodName" : "assertSignerSecretProviderType",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void assertSignerSecretProviderType(HttpServer2 server, Class<T> expected)\n{\r\n    SignerSecretProvider secretProvider = (SignerSecretProvider) server.getWebAppContext().getServletContext().getAttribute(SIGNER_SECRET_PROVIDER_ATTRIBUTE);\r\n    Assert.assertNotNull(\"The secret provider must not be null\", secretProvider);\r\n    Assert.assertEquals(\"The secret provider must match the following\", expected, secretProvider.getClass());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\test\\java\\org\\apache\\hadoop\\fs\\http\\server",
  "methodName" : "assertServiceRespondsWithOK",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void assertServiceRespondsWithOK(URL serviceURL) throws Exception\n{\r\n    String user = HadoopUsersConfTestHelper.getHadoopUsers()[0];\r\n    URL url = new URL(serviceURL, MessageFormat.format(\"/webhdfs/v1/?user.name={0}&op=liststatus\", user));\r\n    HttpURLConnection conn = (HttpURLConnection) url.openConnection();\r\n    Assert.assertEquals(HttpURLConnection.HTTP_OK, conn.getResponseCode());\r\n    try (BufferedReader reader = new BufferedReader(new InputStreamReader(conn.getInputStream()))) {\r\n        reader.readLine();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\test\\java\\org\\apache\\hadoop\\fs\\http\\server",
  "methodName" : "setDeprecatedSecretFile",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void setDeprecatedSecretFile(Configuration conf, String path)\n{\r\n    conf.set(HttpFSAuthenticationFilter.CONF_PREFIX + AuthenticationFilter.SIGNATURE_SECRET_FILE, path);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\test\\java\\org\\apache\\hadoop\\fs\\http\\server",
  "methodName" : "createConfigurationWithRandomSecret",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "Configuration createConfigurationWithRandomSecret()\n{\r\n    Configuration conf = createConfiguration();\r\n    conf.set(HttpFSAuthenticationFilter.HADOOP_HTTP_CONF_PREFIX + AuthenticationFilter.SIGNER_SECRET_PROVIDER, \"random\");\r\n    return conf;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\test\\java\\org\\apache\\hadoop\\fs\\http\\server",
  "methodName" : "createConfigurationWithSecretFile",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "Configuration createConfigurationWithSecretFile()\n{\r\n    Configuration conf = createConfiguration();\r\n    conf.set(HttpFSAuthenticationFilter.HADOOP_HTTP_CONF_PREFIX + AuthenticationFilter.SIGNATURE_SECRET_FILE, secretFile.getAbsolutePath());\r\n    return conf;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\test\\java\\org\\apache\\hadoop\\fs\\http\\server",
  "methodName" : "createConfiguration",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "Configuration createConfiguration()\n{\r\n    Configuration conf = new Configuration(false);\r\n    conf.set(HttpFSServerWebServer.HTTP_HOSTNAME_KEY, \"localhost\");\r\n    conf.setInt(HttpFSServerWebServer.HTTP_PORT_KEY, 0);\r\n    return conf;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\test\\java\\org\\apache\\hadoop\\fs\\http\\server",
  "methodName" : "createWebServer",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "HttpFSServerWebServer createWebServer(Configuration conf) throws Exception\n{\r\n    Configuration sslConf = new Configuration(false);\r\n    try (FileOutputStream os = new FileOutputStream(new File(System.getProperty(\"httpfs.config.dir\"), \"httpfs-site.xml\"))) {\r\n        conf.writeXml(os);\r\n    }\r\n    return new HttpFSServerWebServer(conf, sslConf);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\test\\java\\org\\apache\\hadoop\\fs\\http\\server",
  "methodName" : "createSecretFile",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void createSecretFile(String content) throws IOException\n{\r\n    Assert.assertTrue(secretFile.createNewFile());\r\n    FileUtils.writeStringToFile(secretFile, content, StandardCharsets.UTF_8);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\test\\java\\org\\apache\\hadoop\\test",
  "methodName" : "getRealm",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String getRealm()\n{\r\n    return System.getProperty(REALM, \"LOCALHOST\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\test\\java\\org\\apache\\hadoop\\test",
  "methodName" : "getClientPrincipal",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "String getClientPrincipal()\n{\r\n    return System.getProperty(CLIENT_PRINCIPAL, \"client\") + \"@\" + getRealm();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\test\\java\\org\\apache\\hadoop\\test",
  "methodName" : "getServerPrincipal",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "String getServerPrincipal()\n{\r\n    return System.getProperty(SERVER_PRINCIPAL, \"HTTP/localhost\") + \"@\" + getRealm();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\test\\java\\org\\apache\\hadoop\\test",
  "methodName" : "getKeytabFile",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "String getKeytabFile()\n{\r\n    String keytabFile = new File(System.getProperty(\"user.home\"), System.getProperty(\"user.name\") + \".keytab\").toString();\r\n    return System.getProperty(KEYTAB_FILE, keytabFile);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\test\\java\\org\\apache\\hadoop\\test",
  "methodName" : "doAs",
  "errType" : [ "PrivilegedActionException" ],
  "containingMethodsNum" : 6,
  "sourceCodeText" : "T doAs(String principal, final Callable<T> callable) throws Exception\n{\r\n    LoginContext loginContext = null;\r\n    try {\r\n        Set<Principal> principals = new HashSet<Principal>();\r\n        principals.add(new KerberosPrincipal(KerberosTestUtils.getClientPrincipal()));\r\n        Subject subject = new Subject(false, principals, new HashSet<Object>(), new HashSet<Object>());\r\n        loginContext = new LoginContext(\"\", subject, null, new KerberosConfiguration(principal));\r\n        loginContext.login();\r\n        subject = loginContext.getSubject();\r\n        return Subject.doAs(subject, new PrivilegedExceptionAction<T>() {\r\n\r\n            @Override\r\n            public T run() throws Exception {\r\n                return callable.call();\r\n            }\r\n        });\r\n    } catch (PrivilegedActionException ex) {\r\n        throw ex.getException();\r\n    } finally {\r\n        if (loginContext != null) {\r\n            loginContext.logout();\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\test\\java\\org\\apache\\hadoop\\test",
  "methodName" : "doAsClient",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "T doAsClient(Callable<T> callable) throws Exception\n{\r\n    return doAs(getClientPrincipal(), callable);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\test\\java\\org\\apache\\hadoop\\test",
  "methodName" : "doAsServer",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "T doAsServer(Callable<T> callable) throws Exception\n{\r\n    return doAs(getServerPrincipal(), callable);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\test\\java\\org\\apache\\hadoop\\lib\\server",
  "methodName" : "constructorFailParams",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Collection constructorFailParams()\n{\r\n    return Arrays.asList(new Object[][] { { null, null, null, null, null, null }, { \"\", null, null, null, null, null }, { null, null, null, null, null, null }, { \"server\", null, null, null, null, null }, { \"server\", \"\", null, null, null, null }, { \"server\", \"foo\", null, null, null, null }, { \"server\", \"/tmp\", null, null, null, null }, { \"server\", \"/tmp\", \"\", null, null, null }, { \"server\", \"/tmp\", \"foo\", null, null, null }, { \"server\", \"/tmp\", \"/tmp\", null, null, null }, { \"server\", \"/tmp\", \"/tmp\", \"\", null, null }, { \"server\", \"/tmp\", \"/tmp\", \"foo\", null, null }, { \"server\", \"/tmp\", \"/tmp\", \"/tmp\", null, null }, { \"server\", \"/tmp\", \"/tmp\", \"/tmp\", \"\", null }, { \"server\", \"/tmp\", \"/tmp\", \"/tmp\", \"foo\", null } });\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\test\\java\\org\\apache\\hadoop\\lib\\server",
  "methodName" : "constructorFail",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void constructorFail()\n{\r\n    new Server(name, homeDir, configDir, logDir, tempDir, conf);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\test\\java\\org\\apache\\hadoop\\lib\\servlet",
  "methodName" : "mdc",
  "errType" : null,
  "containingMethodsNum" : 38,
  "sourceCodeText" : "void mdc() throws Exception\n{\r\n    HttpServletRequest request = Mockito.mock(HttpServletRequest.class);\r\n    Mockito.when(request.getUserPrincipal()).thenReturn(null);\r\n    Mockito.when(request.getMethod()).thenReturn(\"METHOD\");\r\n    Mockito.when(request.getPathInfo()).thenReturn(\"/pathinfo\");\r\n    ServletResponse response = Mockito.mock(ServletResponse.class);\r\n    final AtomicBoolean invoked = new AtomicBoolean();\r\n    FilterChain chain = new FilterChain() {\r\n\r\n        @Override\r\n        public void doFilter(ServletRequest servletRequest, ServletResponse servletResponse) throws IOException, ServletException {\r\n            assertEquals(MDC.get(\"hostname\"), null);\r\n            assertEquals(MDC.get(\"user\"), null);\r\n            assertEquals(MDC.get(\"method\"), \"METHOD\");\r\n            assertEquals(MDC.get(\"path\"), \"/pathinfo\");\r\n            invoked.set(true);\r\n        }\r\n    };\r\n    MDC.clear();\r\n    Filter filter = new MDCFilter();\r\n    filter.init(null);\r\n    filter.doFilter(request, response, chain);\r\n    assertTrue(invoked.get());\r\n    assertNull(MDC.get(\"hostname\"));\r\n    assertNull(MDC.get(\"user\"));\r\n    assertNull(MDC.get(\"method\"));\r\n    assertNull(MDC.get(\"path\"));\r\n    Mockito.when(request.getUserPrincipal()).thenReturn(new Principal() {\r\n\r\n        @Override\r\n        public String getName() {\r\n            return \"name\";\r\n        }\r\n    });\r\n    invoked.set(false);\r\n    chain = new FilterChain() {\r\n\r\n        @Override\r\n        public void doFilter(ServletRequest servletRequest, ServletResponse servletResponse) throws IOException, ServletException {\r\n            assertEquals(MDC.get(\"hostname\"), null);\r\n            assertEquals(MDC.get(\"user\"), \"name\");\r\n            assertEquals(MDC.get(\"method\"), \"METHOD\");\r\n            assertEquals(MDC.get(\"path\"), \"/pathinfo\");\r\n            invoked.set(true);\r\n        }\r\n    };\r\n    filter.doFilter(request, response, chain);\r\n    assertTrue(invoked.get());\r\n    HostnameFilter.HOSTNAME_TL.set(\"HOST\");\r\n    invoked.set(false);\r\n    chain = new FilterChain() {\r\n\r\n        @Override\r\n        public void doFilter(ServletRequest servletRequest, ServletResponse servletResponse) throws IOException, ServletException {\r\n            assertEquals(MDC.get(\"hostname\"), \"HOST\");\r\n            assertEquals(MDC.get(\"user\"), \"name\");\r\n            assertEquals(MDC.get(\"method\"), \"METHOD\");\r\n            assertEquals(MDC.get(\"path\"), \"/pathinfo\");\r\n            invoked.set(true);\r\n        }\r\n    };\r\n    filter.doFilter(request, response, chain);\r\n    assertTrue(invoked.get());\r\n    HostnameFilter.HOSTNAME_TL.remove();\r\n    filter.destroy();\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\test\\java\\org\\apache\\hadoop\\test",
  "methodName" : "init",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void init()\n{\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\test\\java\\org\\apache\\hadoop\\fs\\http\\server",
  "methodName" : "startMiniDFS",
  "errType" : null,
  "containingMethodsNum" : 15,
  "sourceCodeText" : "void startMiniDFS() throws Exception\n{\r\n    File testDirRoot = TestDirHelper.getTestDir();\r\n    if (System.getProperty(\"hadoop.log.dir\") == null) {\r\n        System.setProperty(\"hadoop.log.dir\", new File(testDirRoot, \"hadoop-log\").getAbsolutePath());\r\n    }\r\n    if (System.getProperty(\"test.build.data\") == null) {\r\n        System.setProperty(\"test.build.data\", new File(testDirRoot, \"hadoop-data\").getAbsolutePath());\r\n    }\r\n    Configuration conf = HadoopUsersConfTestHelper.getBaseConf();\r\n    HadoopUsersConfTestHelper.addUserConf(conf);\r\n    conf.set(\"fs.hdfs.impl.disable.cache\", \"true\");\r\n    conf.set(\"dfs.block.access.token.enable\", \"false\");\r\n    conf.set(\"dfs.permissions\", \"true\");\r\n    conf.set(\"hadoop.security.authentication\", \"simple\");\r\n    conf.setBoolean(DFSConfigKeys.DFS_NAMENODE_ACLS_ENABLED_KEY, false);\r\n    MiniDFSCluster.Builder builder = new MiniDFSCluster.Builder(conf);\r\n    builder.numDataNodes(2);\r\n    miniDfs = builder.build();\r\n    nnConf = miniDfs.getConfiguration(0);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\test\\java\\org\\apache\\hadoop\\fs\\http\\server",
  "methodName" : "createHttpFSServer",
  "errType" : null,
  "containingMethodsNum" : 25,
  "sourceCodeText" : "void createHttpFSServer() throws Exception\n{\r\n    File homeDir = TestDirHelper.getTestDir();\r\n    Assert.assertTrue(new File(homeDir, \"conf\").mkdir());\r\n    Assert.assertTrue(new File(homeDir, \"log\").mkdir());\r\n    Assert.assertTrue(new File(homeDir, \"temp\").mkdir());\r\n    HttpFSServerWebApp.setHomeDirForCurrentThread(homeDir.getAbsolutePath());\r\n    File secretFile = new File(new File(homeDir, \"conf\"), \"secret\");\r\n    Writer w = new FileWriter(secretFile);\r\n    w.write(\"secret\");\r\n    w.close();\r\n    File hadoopConfDir = new File(new File(homeDir, \"conf\"), \"hadoop-conf\");\r\n    if (!hadoopConfDir.mkdirs()) {\r\n        throw new IOException();\r\n    }\r\n    String fsDefaultName = nnConf.get(CommonConfigurationKeysPublic.FS_DEFAULT_NAME_KEY);\r\n    Configuration conf = new Configuration(false);\r\n    conf.set(CommonConfigurationKeysPublic.FS_DEFAULT_NAME_KEY, fsDefaultName);\r\n    conf.setBoolean(DFSConfigKeys.DFS_NAMENODE_ACLS_ENABLED_KEY, false);\r\n    File hdfsSite = new File(hadoopConfDir, \"hdfs-site.xml\");\r\n    OutputStream os = new FileOutputStream(hdfsSite);\r\n    conf.writeXml(os);\r\n    os.close();\r\n    conf = new Configuration(false);\r\n    conf.set(\"httpfs.hadoop.config.dir\", hadoopConfDir.toString());\r\n    conf.set(\"httpfs.proxyuser.\" + HadoopUsersConfTestHelper.getHadoopProxyUser() + \".groups\", HadoopUsersConfTestHelper.getHadoopProxyUserGroups());\r\n    conf.set(\"httpfs.proxyuser.\" + HadoopUsersConfTestHelper.getHadoopProxyUser() + \".hosts\", HadoopUsersConfTestHelper.getHadoopProxyUserHosts());\r\n    conf.set(HttpFSAuthenticationFilter.HADOOP_HTTP_CONF_PREFIX + AuthenticationFilter.SIGNATURE_SECRET_FILE, secretFile.getAbsolutePath());\r\n    File httpfsSite = new File(new File(homeDir, \"conf\"), \"httpfs-site.xml\");\r\n    os = new FileOutputStream(httpfsSite);\r\n    conf.writeXml(os);\r\n    os.close();\r\n    ClassLoader cl = Thread.currentThread().getContextClassLoader();\r\n    URL url = cl.getResource(\"webapp\");\r\n    if (url == null) {\r\n        throw new IOException();\r\n    }\r\n    WebAppContext context = new WebAppContext(url.getPath(), \"/webhdfs\");\r\n    Server server = TestJettyHelper.getJettyServer();\r\n    server.setHandler(context);\r\n    server.start();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\test\\java\\org\\apache\\hadoop\\fs\\http\\server",
  "methodName" : "getStatus",
  "errType" : null,
  "containingMethodsNum" : 18,
  "sourceCodeText" : "void getStatus(String filename, String command, boolean expectOK) throws Exception\n{\r\n    String user = HadoopUsersConfTestHelper.getHadoopUsers()[0];\r\n    if (filename.charAt(0) == '/') {\r\n        filename = filename.substring(1);\r\n    }\r\n    String pathOps = MessageFormat.format(\"/webhdfs/v1/{0}?user.name={1}&op={2}\", filename, user, command);\r\n    URL url = new URL(TestJettyHelper.getJettyURL(), pathOps);\r\n    HttpURLConnection conn = (HttpURLConnection) url.openConnection();\r\n    conn.connect();\r\n    int resp = conn.getResponseCode();\r\n    BufferedReader reader;\r\n    if (expectOK) {\r\n        Assert.assertEquals(HttpURLConnection.HTTP_OK, resp);\r\n        reader = new BufferedReader(new InputStreamReader(conn.getInputStream()));\r\n        String res = reader.readLine();\r\n        Assert.assertTrue(!res.contains(\"aclBit\"));\r\n        Assert.assertTrue(res.contains(\"owner\"));\r\n    } else {\r\n        Assert.assertEquals(HttpURLConnection.HTTP_INTERNAL_ERROR, resp);\r\n        reader = new BufferedReader(new InputStreamReader(conn.getErrorStream()));\r\n        String res = reader.readLine();\r\n        Assert.assertTrue(res.contains(\"AclException\"));\r\n        Assert.assertTrue(res.contains(\"Support for ACLs has been disabled\"));\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\test\\java\\org\\apache\\hadoop\\fs\\http\\server",
  "methodName" : "putCmd",
  "errType" : null,
  "containingMethodsNum" : 15,
  "sourceCodeText" : "void putCmd(String filename, String command, String params, boolean expectOK) throws Exception\n{\r\n    String user = HadoopUsersConfTestHelper.getHadoopUsers()[0];\r\n    if (filename.charAt(0) == '/') {\r\n        filename = filename.substring(1);\r\n    }\r\n    String pathOps = MessageFormat.format(\"/webhdfs/v1/{0}?user.name={1}{2}{3}&op={4}\", filename, user, (params == null) ? \"\" : \"&\", (params == null) ? \"\" : params, command);\r\n    URL url = new URL(TestJettyHelper.getJettyURL(), pathOps);\r\n    HttpURLConnection conn = (HttpURLConnection) url.openConnection();\r\n    conn.setRequestMethod(\"PUT\");\r\n    conn.connect();\r\n    int resp = conn.getResponseCode();\r\n    if (expectOK) {\r\n        Assert.assertEquals(HttpURLConnection.HTTP_OK, resp);\r\n    } else {\r\n        Assert.assertEquals(HttpURLConnection.HTTP_INTERNAL_ERROR, resp);\r\n        BufferedReader reader;\r\n        reader = new BufferedReader(new InputStreamReader(conn.getErrorStream()));\r\n        String err = reader.readLine();\r\n        Assert.assertTrue(err.contains(\"AclException\"));\r\n        Assert.assertTrue(err.contains(\"Support for ACLs has been disabled\"));\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\test\\java\\org\\apache\\hadoop\\fs\\http\\server",
  "methodName" : "testWithNoAcls",
  "errType" : null,
  "containingMethodsNum" : 17,
  "sourceCodeText" : "void testWithNoAcls() throws Exception\n{\r\n    final String aclUser1 = \"user:foo:rw-\";\r\n    final String rmAclUser1 = \"user:foo:\";\r\n    final String aclUser2 = \"user:bar:r--\";\r\n    final String aclGroup1 = \"group::r--\";\r\n    final String aclSpec = \"aclspec=user::rwx,\" + aclUser1 + \",\" + aclGroup1 + \",other::---\";\r\n    final String modAclSpec = \"aclspec=\" + aclUser2;\r\n    final String remAclSpec = \"aclspec=\" + rmAclUser1;\r\n    final String defUser1 = \"default:user:glarch:r-x\";\r\n    final String defSpec1 = \"aclspec=\" + defUser1;\r\n    final String dir = \"/noACLs\";\r\n    final String path = dir + \"/foo\";\r\n    startMiniDFS();\r\n    createHttpFSServer();\r\n    FileSystem fs = FileSystem.get(nnConf);\r\n    fs.mkdirs(new Path(dir));\r\n    OutputStream os = fs.create(new Path(path));\r\n    os.write(1);\r\n    os.close();\r\n    getStatus(path, \"GETFILESTATUS\", true);\r\n    getStatus(dir, \"LISTSTATUS\", true);\r\n    getStatus(path, \"GETACLSTATUS\", false);\r\n    putCmd(path, \"SETACL\", aclSpec, false);\r\n    putCmd(path, \"MODIFYACLENTRIES\", modAclSpec, false);\r\n    putCmd(path, \"REMOVEACLENTRIES\", remAclSpec, false);\r\n    putCmd(path, \"REMOVEACL\", null, false);\r\n    putCmd(dir, \"SETACL\", defSpec1, false);\r\n    putCmd(dir, \"REMOVEDEFAULTACL\", null, false);\r\n    miniDfs.shutdown();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\test\\java\\org\\apache\\hadoop\\fs\\http\\server",
  "methodName" : "startMiniDFS",
  "errType" : null,
  "containingMethodsNum" : 14,
  "sourceCodeText" : "void startMiniDFS() throws Exception\n{\r\n    File testDirRoot = TestDirHelper.getTestDir();\r\n    if (System.getProperty(\"hadoop.log.dir\") == null) {\r\n        System.setProperty(\"hadoop.\" + \"log.dir\", new File(testDirRoot, \"hadoop-log\").getAbsolutePath());\r\n    }\r\n    if (System.getProperty(\"test.build.data\") == null) {\r\n        System.setProperty(\"test.build.data\", new File(testDirRoot, \"hadoop-data\").getAbsolutePath());\r\n    }\r\n    Configuration conf = HadoopUsersConfTestHelper.getBaseConf();\r\n    HadoopUsersConfTestHelper.addUserConf(conf);\r\n    conf.set(\"fs.hdfs.impl.disable.cache\", \"true\");\r\n    conf.set(\"dfs.block.access.token.enable\", \"false\");\r\n    conf.set(\"dfs.permissions\", \"true\");\r\n    conf.set(\"hadoop.security.authentication\", \"simple\");\r\n    MiniDFSCluster.Builder builder = new MiniDFSCluster.Builder(conf);\r\n    builder.numDataNodes(2);\r\n    miniDfs = builder.build();\r\n    nnConf = miniDfs.getConfiguration(0);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\test\\java\\org\\apache\\hadoop\\fs\\http\\server",
  "methodName" : "createHttpFSServer",
  "errType" : null,
  "containingMethodsNum" : 24,
  "sourceCodeText" : "void createHttpFSServer() throws Exception\n{\r\n    File homeDir = TestDirHelper.getTestDir();\r\n    Assert.assertTrue(new File(homeDir, \"conf\").mkdir());\r\n    Assert.assertTrue(new File(homeDir, \"log\").mkdir());\r\n    Assert.assertTrue(new File(homeDir, \"temp\").mkdir());\r\n    HttpFSServerWebApp.setHomeDirForCurrentThread(homeDir.getAbsolutePath());\r\n    File secretFile = new File(new File(homeDir, \"conf\"), \"secret\");\r\n    Writer w = new FileWriter(secretFile);\r\n    w.write(\"secret\");\r\n    w.close();\r\n    File hadoopConfDir = new File(new File(homeDir, \"conf\"), \"hadoop-conf\");\r\n    if (!hadoopConfDir.mkdirs()) {\r\n        throw new IOException();\r\n    }\r\n    String fsDefaultName = nnConf.get(CommonConfigurationKeysPublic.FS_DEFAULT_NAME_KEY);\r\n    Configuration conf = new Configuration(false);\r\n    conf.set(CommonConfigurationKeysPublic.FS_DEFAULT_NAME_KEY, fsDefaultName);\r\n    File hdfsSite = new File(hadoopConfDir, \"hdfs-site.xml\");\r\n    OutputStream os = new FileOutputStream(hdfsSite);\r\n    conf.writeXml(os);\r\n    os.close();\r\n    conf = new Configuration(false);\r\n    conf.set(\"httpfs.hadoop.config.dir\", hadoopConfDir.toString());\r\n    conf.set(\"httpfs.proxyuser.\" + HadoopUsersConfTestHelper.getHadoopProxyUser() + \".groups\", HadoopUsersConfTestHelper.getHadoopProxyUserGroups());\r\n    conf.set(\"httpfs.proxyuser.\" + HadoopUsersConfTestHelper.getHadoopProxyUser() + \".hosts\", HadoopUsersConfTestHelper.getHadoopProxyUserHosts());\r\n    conf.set(HttpFSAuthenticationFilter.HADOOP_HTTP_CONF_PREFIX + AuthenticationFilter.SIGNATURE_SECRET_FILE, secretFile.getAbsolutePath());\r\n    File httpfsSite = new File(new File(homeDir, \"conf\"), \"httpfs-site.xml\");\r\n    os = new FileOutputStream(httpfsSite);\r\n    conf.writeXml(os);\r\n    os.close();\r\n    ClassLoader cl = Thread.currentThread().getContextClassLoader();\r\n    URL url = cl.getResource(\"webapp\");\r\n    if (url == null) {\r\n        throw new IOException();\r\n    }\r\n    WebAppContext context = new WebAppContext(url.getPath(), \"/webhdfs\");\r\n    Server server = TestJettyHelper.getJettyServer();\r\n    server.setHandler(context);\r\n    server.start();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\test\\java\\org\\apache\\hadoop\\fs\\http\\server",
  "methodName" : "getCmd",
  "errType" : null,
  "containingMethodsNum" : 11,
  "sourceCodeText" : "void getCmd(String filename, String message, String command, boolean expectOK) throws Exception\n{\r\n    String user = HadoopUsersConfTestHelper.getHadoopUsers()[0];\r\n    String outMsg = message + \" (\" + command + \")\";\r\n    if (filename.charAt(0) == '/') {\r\n        filename = filename.substring(1);\r\n    }\r\n    String pathOps = MessageFormat.format(\"/webhdfs/v1/{0}?user.name={1}&op={2}\", filename, user, command);\r\n    URL url = new URL(TestJettyHelper.getJettyURL(), pathOps);\r\n    HttpURLConnection conn = (HttpURLConnection) url.openConnection();\r\n    conn.setRequestMethod(\"GET\");\r\n    conn.connect();\r\n    int resp = conn.getResponseCode();\r\n    if (expectOK) {\r\n        Assert.assertEquals(outMsg, HttpURLConnection.HTTP_OK, resp);\r\n    } else {\r\n        Assert.assertEquals(outMsg, HttpURLConnection.HTTP_FORBIDDEN, resp);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\test\\java\\org\\apache\\hadoop\\fs\\http\\server",
  "methodName" : "putCmd",
  "errType" : null,
  "containingMethodsNum" : 11,
  "sourceCodeText" : "void putCmd(String filename, String message, String command, String params, boolean expectOK) throws Exception\n{\r\n    String user = HadoopUsersConfTestHelper.getHadoopUsers()[0];\r\n    String outMsg = message + \" (\" + command + \")\";\r\n    if (filename.charAt(0) == '/') {\r\n        filename = filename.substring(1);\r\n    }\r\n    String pathOps = MessageFormat.format(\"/webhdfs/v1/{0}?user.name={1}{2}{3}&op={4}\", filename, user, (params == null) ? \"\" : \"&\", (params == null) ? \"\" : params, command);\r\n    URL url = new URL(TestJettyHelper.getJettyURL(), pathOps);\r\n    HttpURLConnection conn = (HttpURLConnection) url.openConnection();\r\n    conn.setRequestMethod(\"PUT\");\r\n    conn.connect();\r\n    int resp = conn.getResponseCode();\r\n    if (expectOK) {\r\n        Assert.assertEquals(outMsg, HttpURLConnection.HTTP_OK, resp);\r\n    } else {\r\n        Assert.assertEquals(outMsg, HttpURLConnection.HTTP_FORBIDDEN, resp);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\test\\java\\org\\apache\\hadoop\\fs\\http\\server",
  "methodName" : "deleteCmd",
  "errType" : null,
  "containingMethodsNum" : 11,
  "sourceCodeText" : "void deleteCmd(String filename, String message, String command, String params, boolean expectOK) throws Exception\n{\r\n    String user = HadoopUsersConfTestHelper.getHadoopUsers()[0];\r\n    String outMsg = message + \" (\" + command + \")\";\r\n    if (filename.charAt(0) == '/') {\r\n        filename = filename.substring(1);\r\n    }\r\n    String pathOps = MessageFormat.format(\"/webhdfs/v1/{0}?user.name={1}{2}{3}&op={4}\", filename, user, (params == null) ? \"\" : \"&\", (params == null) ? \"\" : params, command);\r\n    URL url = new URL(TestJettyHelper.getJettyURL(), pathOps);\r\n    HttpURLConnection conn = (HttpURLConnection) url.openConnection();\r\n    conn.setRequestMethod(\"DELETE\");\r\n    conn.connect();\r\n    int resp = conn.getResponseCode();\r\n    if (expectOK) {\r\n        Assert.assertEquals(outMsg, HttpURLConnection.HTTP_OK, resp);\r\n    } else {\r\n        Assert.assertEquals(outMsg, HttpURLConnection.HTTP_FORBIDDEN, resp);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\test\\java\\org\\apache\\hadoop\\fs\\http\\server",
  "methodName" : "postCmd",
  "errType" : null,
  "containingMethodsNum" : 11,
  "sourceCodeText" : "void postCmd(String filename, String message, String command, String params, boolean expectOK) throws Exception\n{\r\n    String user = HadoopUsersConfTestHelper.getHadoopUsers()[0];\r\n    String outMsg = message + \" (\" + command + \")\";\r\n    if (filename.charAt(0) == '/') {\r\n        filename = filename.substring(1);\r\n    }\r\n    String pathOps = MessageFormat.format(\"/webhdfs/v1/{0}?user.name={1}{2}{3}&op={4}\", filename, user, (params == null) ? \"\" : \"&\", (params == null) ? \"\" : params, command);\r\n    URL url = new URL(TestJettyHelper.getJettyURL(), pathOps);\r\n    HttpURLConnection conn = (HttpURLConnection) url.openConnection();\r\n    conn.setRequestMethod(\"POST\");\r\n    conn.connect();\r\n    int resp = conn.getResponseCode();\r\n    if (expectOK) {\r\n        Assert.assertEquals(outMsg, HttpURLConnection.HTTP_OK, resp);\r\n    } else {\r\n        Assert.assertEquals(outMsg, HttpURLConnection.HTTP_FORBIDDEN, resp);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\test\\java\\org\\apache\\hadoop\\fs\\http\\server",
  "methodName" : "testAcessControlledFS",
  "errType" : null,
  "containingMethodsNum" : 37,
  "sourceCodeText" : "void testAcessControlledFS() throws Exception\n{\r\n    final String testRwMsg = \"Test read-write \";\r\n    final String testRoMsg = \"Test read-only \";\r\n    final String testWoMsg = \"Test write-only \";\r\n    final String defUser1 = \"default:user:glarch:r-x\";\r\n    final String dir = \"/testAccess\";\r\n    final String pathRW = dir + \"/foo-rw\";\r\n    final String pathWO = dir + \"/foo-wo\";\r\n    final String pathRO = dir + \"/foo-ro\";\r\n    final String setPermSpec = \"744\";\r\n    final String snapshopSpec = \"snapshotname=test-snap\";\r\n    startMiniDFS();\r\n    createHttpFSServer();\r\n    FileSystem fs = FileSystem.get(nnConf);\r\n    fs.mkdirs(new Path(dir));\r\n    OutputStream os = fs.create(new Path(pathRW));\r\n    os.write(1);\r\n    os.close();\r\n    os = fs.create(new Path(pathWO));\r\n    os.write(1);\r\n    os.close();\r\n    os = fs.create(new Path(pathRO));\r\n    os.write(1);\r\n    os.close();\r\n    Configuration conf = HttpFSServerWebApp.get().getConfig();\r\n    conf.setStrings(\"httpfs.access.mode\", \"read-write\");\r\n    getCmd(pathRW, testRwMsg + \"GET\", \"GETFILESTATUS\", true);\r\n    getCmd(pathRW, testRwMsg + \"GET\", \"LISTSTATUS\", true);\r\n    getCmd(pathRW, testRwMsg + \"GET\", \"GETXATTRS\", true);\r\n    putCmd(pathRW, testRwMsg + \"PUT\", \"SETPERMISSION\", setPermSpec, true);\r\n    postCmd(pathRW, testRwMsg + \"POST\", \"UNSETSTORAGEPOLICY\", null, true);\r\n    deleteCmd(pathRW, testRwMsg + \"DELETE\", \"DELETE\", null, true);\r\n    conf.setStrings(\"httpfs.access.mode\", \"write-only\");\r\n    getCmd(pathWO, testWoMsg + \"GET\", \"GETFILESTATUS\", true);\r\n    getCmd(pathWO, testWoMsg + \"GET\", \"LISTSTATUS\", true);\r\n    getCmd(pathWO, testWoMsg + \"GET\", \"GETXATTRS\", false);\r\n    putCmd(pathWO, testWoMsg + \"PUT\", \"SETPERMISSION\", setPermSpec, true);\r\n    postCmd(pathWO, testWoMsg + \"POST\", \"UNSETSTORAGEPOLICY\", null, true);\r\n    deleteCmd(pathWO, testWoMsg + \"DELETE\", \"DELETE\", null, true);\r\n    conf.setStrings(\"httpfs.access.mode\", \"read-only\");\r\n    getCmd(pathRO, testRoMsg + \"GET\", \"GETFILESTATUS\", true);\r\n    getCmd(pathRO, testRoMsg + \"GET\", \"LISTSTATUS\", true);\r\n    getCmd(pathRO, testRoMsg + \"GET\", \"GETXATTRS\", true);\r\n    putCmd(pathRO, testRoMsg + \"PUT\", \"SETPERMISSION\", setPermSpec, false);\r\n    postCmd(pathRO, testRoMsg + \"POST\", \"UNSETSTORAGEPOLICY\", null, false);\r\n    deleteCmd(pathRO, testRoMsg + \"DELETE\", \"DELETE\", null, false);\r\n    conf.setStrings(\"httpfs.access.mode\", \"read-write\");\r\n    miniDfs.shutdown();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\test\\java\\org\\apache\\hadoop\\lib\\server",
  "methodName" : "constructorsGetters",
  "errType" : null,
  "containingMethodsNum" : 42,
  "sourceCodeText" : "void constructorsGetters() throws Exception\n{\r\n    Server server = new Server(\"server\", getAbsolutePath(\"/a\"), getAbsolutePath(\"/b\"), getAbsolutePath(\"/c\"), getAbsolutePath(\"/d\"), new Configuration(false));\r\n    assertEquals(server.getHomeDir(), getAbsolutePath(\"/a\"));\r\n    assertEquals(server.getConfigDir(), getAbsolutePath(\"/b\"));\r\n    assertEquals(server.getLogDir(), getAbsolutePath(\"/c\"));\r\n    assertEquals(server.getTempDir(), getAbsolutePath(\"/d\"));\r\n    assertEquals(server.getName(), \"server\");\r\n    assertEquals(server.getPrefix(), \"server\");\r\n    assertEquals(server.getPrefixedName(\"name\"), \"server.name\");\r\n    assertNotNull(server.getConfig());\r\n    server = new Server(\"server\", getAbsolutePath(\"/a\"), getAbsolutePath(\"/b\"), getAbsolutePath(\"/c\"), getAbsolutePath(\"/d\"));\r\n    assertEquals(server.getHomeDir(), getAbsolutePath(\"/a\"));\r\n    assertEquals(server.getConfigDir(), getAbsolutePath(\"/b\"));\r\n    assertEquals(server.getLogDir(), getAbsolutePath(\"/c\"));\r\n    assertEquals(server.getTempDir(), getAbsolutePath(\"/d\"));\r\n    assertEquals(server.getName(), \"server\");\r\n    assertEquals(server.getPrefix(), \"server\");\r\n    assertEquals(server.getPrefixedName(\"name\"), \"server.name\");\r\n    assertNull(server.getConfig());\r\n    server = new Server(\"server\", TestDirHelper.getTestDir().getAbsolutePath(), new Configuration(false));\r\n    assertEquals(server.getHomeDir(), TestDirHelper.getTestDir().getAbsolutePath());\r\n    assertEquals(server.getConfigDir(), TestDirHelper.getTestDir() + \"/conf\");\r\n    assertEquals(server.getLogDir(), TestDirHelper.getTestDir() + \"/log\");\r\n    assertEquals(server.getTempDir(), TestDirHelper.getTestDir() + \"/temp\");\r\n    assertEquals(server.getName(), \"server\");\r\n    assertEquals(server.getPrefix(), \"server\");\r\n    assertEquals(server.getPrefixedName(\"name\"), \"server.name\");\r\n    assertNotNull(server.getConfig());\r\n    server = new Server(\"server\", TestDirHelper.getTestDir().getAbsolutePath());\r\n    assertEquals(server.getHomeDir(), TestDirHelper.getTestDir().getAbsolutePath());\r\n    assertEquals(server.getConfigDir(), TestDirHelper.getTestDir() + \"/conf\");\r\n    assertEquals(server.getLogDir(), TestDirHelper.getTestDir() + \"/log\");\r\n    assertEquals(server.getTempDir(), TestDirHelper.getTestDir() + \"/temp\");\r\n    assertEquals(server.getName(), \"server\");\r\n    assertEquals(server.getPrefix(), \"server\");\r\n    assertEquals(server.getPrefixedName(\"name\"), \"server.name\");\r\n    assertNull(server.getConfig());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\test\\java\\org\\apache\\hadoop\\lib\\server",
  "methodName" : "initNoHomeDir",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void initNoHomeDir() throws Exception\n{\r\n    File homeDir = new File(TestDirHelper.getTestDir(), \"home\");\r\n    Configuration conf = new Configuration(false);\r\n    conf.set(\"server.services\", TestService.class.getName());\r\n    Server server = new Server(\"server\", homeDir.getAbsolutePath(), conf);\r\n    server.init();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\test\\java\\org\\apache\\hadoop\\lib\\server",
  "methodName" : "initHomeDirNotDir",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void initHomeDirNotDir() throws Exception\n{\r\n    File homeDir = new File(TestDirHelper.getTestDir(), \"home\");\r\n    new FileOutputStream(homeDir).close();\r\n    Configuration conf = new Configuration(false);\r\n    conf.set(\"server.services\", TestService.class.getName());\r\n    Server server = new Server(\"server\", homeDir.getAbsolutePath(), conf);\r\n    server.init();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\test\\java\\org\\apache\\hadoop\\lib\\server",
  "methodName" : "initNoConfigDir",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void initNoConfigDir() throws Exception\n{\r\n    File homeDir = new File(TestDirHelper.getTestDir(), \"home\");\r\n    assertTrue(homeDir.mkdir());\r\n    assertTrue(new File(homeDir, \"log\").mkdir());\r\n    assertTrue(new File(homeDir, \"temp\").mkdir());\r\n    Configuration conf = new Configuration(false);\r\n    conf.set(\"server.services\", TestService.class.getName());\r\n    Server server = new Server(\"server\", homeDir.getAbsolutePath(), conf);\r\n    server.init();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\test\\java\\org\\apache\\hadoop\\lib\\server",
  "methodName" : "initConfigDirNotDir",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void initConfigDirNotDir() throws Exception\n{\r\n    File homeDir = new File(TestDirHelper.getTestDir(), \"home\");\r\n    assertTrue(homeDir.mkdir());\r\n    assertTrue(new File(homeDir, \"log\").mkdir());\r\n    assertTrue(new File(homeDir, \"temp\").mkdir());\r\n    File configDir = new File(homeDir, \"conf\");\r\n    new FileOutputStream(configDir).close();\r\n    Configuration conf = new Configuration(false);\r\n    conf.set(\"server.services\", TestService.class.getName());\r\n    Server server = new Server(\"server\", homeDir.getAbsolutePath(), conf);\r\n    server.init();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\test\\java\\org\\apache\\hadoop\\lib\\server",
  "methodName" : "initNoLogDir",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void initNoLogDir() throws Exception\n{\r\n    File homeDir = new File(TestDirHelper.getTestDir(), \"home\");\r\n    assertTrue(homeDir.mkdir());\r\n    assertTrue(new File(homeDir, \"conf\").mkdir());\r\n    assertTrue(new File(homeDir, \"temp\").mkdir());\r\n    Configuration conf = new Configuration(false);\r\n    conf.set(\"server.services\", TestService.class.getName());\r\n    Server server = new Server(\"server\", homeDir.getAbsolutePath(), conf);\r\n    server.init();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\test\\java\\org\\apache\\hadoop\\lib\\server",
  "methodName" : "initLogDirNotDir",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void initLogDirNotDir() throws Exception\n{\r\n    File homeDir = new File(TestDirHelper.getTestDir(), \"home\");\r\n    assertTrue(homeDir.mkdir());\r\n    assertTrue(new File(homeDir, \"conf\").mkdir());\r\n    assertTrue(new File(homeDir, \"temp\").mkdir());\r\n    File logDir = new File(homeDir, \"log\");\r\n    new FileOutputStream(logDir).close();\r\n    Configuration conf = new Configuration(false);\r\n    conf.set(\"server.services\", TestService.class.getName());\r\n    Server server = new Server(\"server\", homeDir.getAbsolutePath(), conf);\r\n    server.init();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\test\\java\\org\\apache\\hadoop\\lib\\server",
  "methodName" : "initNoTempDir",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void initNoTempDir() throws Exception\n{\r\n    File homeDir = new File(TestDirHelper.getTestDir(), \"home\");\r\n    assertTrue(homeDir.mkdir());\r\n    assertTrue(new File(homeDir, \"conf\").mkdir());\r\n    assertTrue(new File(homeDir, \"log\").mkdir());\r\n    Configuration conf = new Configuration(false);\r\n    conf.set(\"server.services\", TestService.class.getName());\r\n    Server server = new Server(\"server\", homeDir.getAbsolutePath(), conf);\r\n    server.init();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\test\\java\\org\\apache\\hadoop\\lib\\server",
  "methodName" : "initTempDirNotDir",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void initTempDirNotDir() throws Exception\n{\r\n    File homeDir = new File(TestDirHelper.getTestDir(), \"home\");\r\n    assertTrue(homeDir.mkdir());\r\n    assertTrue(new File(homeDir, \"conf\").mkdir());\r\n    assertTrue(new File(homeDir, \"log\").mkdir());\r\n    File tempDir = new File(homeDir, \"temp\");\r\n    new FileOutputStream(tempDir).close();\r\n    Configuration conf = new Configuration(false);\r\n    conf.set(\"server.services\", TestService.class.getName());\r\n    Server server = new Server(\"server\", homeDir.getAbsolutePath(), conf);\r\n    server.init();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\test\\java\\org\\apache\\hadoop\\lib\\server",
  "methodName" : "siteFileNotAFile",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void siteFileNotAFile() throws Exception\n{\r\n    String homeDir = TestDirHelper.getTestDir().getAbsolutePath();\r\n    File siteFile = new File(homeDir, \"server-site.xml\");\r\n    assertTrue(siteFile.mkdir());\r\n    Server server = new Server(\"server\", homeDir, homeDir, homeDir, homeDir);\r\n    server.init();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\test\\java\\org\\apache\\hadoop\\lib\\server",
  "methodName" : "createServer",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "Server createServer(Configuration conf)\n{\r\n    return new Server(\"server\", TestDirHelper.getTestDir().getAbsolutePath(), TestDirHelper.getTestDir().getAbsolutePath(), TestDirHelper.getTestDir().getAbsolutePath(), TestDirHelper.getTestDir().getAbsolutePath(), conf);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\test\\java\\org\\apache\\hadoop\\lib\\server",
  "methodName" : "log4jFile",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void log4jFile() throws Exception\n{\r\n    InputStream is = Server.getResource(\"default-log4j.properties\");\r\n    OutputStream os = new FileOutputStream(new File(TestDirHelper.getTestDir(), \"server-log4j.properties\"));\r\n    IOUtils.copyBytes(is, os, 1024, true);\r\n    Configuration conf = new Configuration(false);\r\n    Server server = createServer(conf);\r\n    server.init();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\test\\java\\org\\apache\\hadoop\\lib\\server",
  "methodName" : "lifeCycle",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void lifeCycle() throws Exception\n{\r\n    Configuration conf = new Configuration(false);\r\n    conf.set(\"server.services\", LifeCycleService.class.getName());\r\n    Server server = createServer(conf);\r\n    assertEquals(server.getStatus(), Server.Status.UNDEF);\r\n    server.init();\r\n    assertNotNull(server.get(LifeCycleService.class));\r\n    assertEquals(server.getStatus(), Server.Status.NORMAL);\r\n    server.destroy();\r\n    assertEquals(server.getStatus(), Server.Status.SHUTDOWN);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\test\\java\\org\\apache\\hadoop\\lib\\server",
  "methodName" : "startWithStatusNotNormal",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void startWithStatusNotNormal() throws Exception\n{\r\n    Configuration conf = new Configuration(false);\r\n    conf.set(\"server.startup.status\", \"ADMIN\");\r\n    Server server = createServer(conf);\r\n    server.init();\r\n    assertEquals(server.getStatus(), Server.Status.ADMIN);\r\n    server.destroy();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\test\\java\\org\\apache\\hadoop\\lib\\server",
  "methodName" : "nonSeteableStatus",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void nonSeteableStatus() throws Exception\n{\r\n    Configuration conf = new Configuration(false);\r\n    Server server = createServer(conf);\r\n    server.init();\r\n    server.setStatus(Server.Status.SHUTDOWN);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\test\\java\\org\\apache\\hadoop\\lib\\server",
  "methodName" : "changeStatus",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void changeStatus() throws Exception\n{\r\n    TestService.LIFECYCLE.clear();\r\n    Configuration conf = new Configuration(false);\r\n    conf.set(\"server.services\", TestService.class.getName());\r\n    Server server = createServer(conf);\r\n    server.init();\r\n    server.setStatus(Server.Status.ADMIN);\r\n    assertTrue(TestService.LIFECYCLE.contains(\"serverStatusChange\"));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\test\\java\\org\\apache\\hadoop\\lib\\server",
  "methodName" : "changeStatusServiceException",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void changeStatusServiceException() throws Exception\n{\r\n    TestService.LIFECYCLE.clear();\r\n    Configuration conf = new Configuration(false);\r\n    conf.set(\"server.services\", TestServiceExceptionOnStatusChange.class.getName());\r\n    Server server = createServer(conf);\r\n    server.init();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\test\\java\\org\\apache\\hadoop\\lib\\server",
  "methodName" : "setSameStatus",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void setSameStatus() throws Exception\n{\r\n    Configuration conf = new Configuration(false);\r\n    conf.set(\"server.services\", TestService.class.getName());\r\n    Server server = createServer(conf);\r\n    server.init();\r\n    TestService.LIFECYCLE.clear();\r\n    server.setStatus(server.getStatus());\r\n    assertFalse(TestService.LIFECYCLE.contains(\"serverStatusChange\"));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\test\\java\\org\\apache\\hadoop\\lib\\server",
  "methodName" : "serviceLifeCycle",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void serviceLifeCycle() throws Exception\n{\r\n    TestService.LIFECYCLE.clear();\r\n    Configuration conf = new Configuration(false);\r\n    conf.set(\"server.services\", TestService.class.getName());\r\n    Server server = createServer(conf);\r\n    server.init();\r\n    assertNotNull(server.get(TestService.class));\r\n    server.destroy();\r\n    assertEquals(TestService.LIFECYCLE, Arrays.asList(\"init\", \"postInit\", \"serverStatusChange\", \"destroy\"));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\test\\java\\org\\apache\\hadoop\\lib\\server",
  "methodName" : "loadingDefaultConfig",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void loadingDefaultConfig() throws Exception\n{\r\n    String dir = TestDirHelper.getTestDir().getAbsolutePath();\r\n    Server server = new Server(\"testserver\", dir, dir, dir, dir);\r\n    server.init();\r\n    assertEquals(server.getConfig().get(\"testserver.a\"), \"default\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\test\\java\\org\\apache\\hadoop\\lib\\server",
  "methodName" : "loadingSiteConfig",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void loadingSiteConfig() throws Exception\n{\r\n    String dir = TestDirHelper.getTestDir().getAbsolutePath();\r\n    File configFile = new File(dir, \"testserver-site.xml\");\r\n    Writer w = new FileWriter(configFile);\r\n    w.write(\"<configuration><property><name>testserver.a</name><value>site</value></property></configuration>\");\r\n    w.close();\r\n    Server server = new Server(\"testserver\", dir, dir, dir, dir);\r\n    server.init();\r\n    assertEquals(server.getConfig().get(\"testserver.a\"), \"site\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\test\\java\\org\\apache\\hadoop\\lib\\server",
  "methodName" : "loadingSysPropConfig",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void loadingSysPropConfig() throws Exception\n{\r\n    try {\r\n        System.setProperty(\"testserver.a\", \"sysprop\");\r\n        String dir = TestDirHelper.getTestDir().getAbsolutePath();\r\n        File configFile = new File(dir, \"testserver-site.xml\");\r\n        Writer w = new FileWriter(configFile);\r\n        w.write(\"<configuration><property><name>testserver.a</name><value>site</value></property></configuration>\");\r\n        w.close();\r\n        Server server = new Server(\"testserver\", dir, dir, dir, dir);\r\n        server.init();\r\n        assertEquals(server.getConfig().get(\"testserver.a\"), \"sysprop\");\r\n    } finally {\r\n        System.getProperties().remove(\"testserver.a\");\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\test\\java\\org\\apache\\hadoop\\lib\\server",
  "methodName" : "illegalState1",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void illegalState1() throws Exception\n{\r\n    Server server = new Server(\"server\", TestDirHelper.getTestDir().getAbsolutePath(), new Configuration(false));\r\n    server.destroy();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\test\\java\\org\\apache\\hadoop\\lib\\server",
  "methodName" : "illegalState2",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void illegalState2() throws Exception\n{\r\n    Server server = new Server(\"server\", TestDirHelper.getTestDir().getAbsolutePath(), new Configuration(false));\r\n    server.get(Object.class);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\test\\java\\org\\apache\\hadoop\\lib\\server",
  "methodName" : "illegalState3",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void illegalState3() throws Exception\n{\r\n    Server server = new Server(\"server\", TestDirHelper.getTestDir().getAbsolutePath(), new Configuration(false));\r\n    server.setService(null);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\test\\java\\org\\apache\\hadoop\\lib\\server",
  "methodName" : "illegalState4",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void illegalState4() throws Exception\n{\r\n    String dir = TestDirHelper.getTestDir().getAbsolutePath();\r\n    Server server = new Server(\"server\", dir, dir, dir, dir, new Configuration(false));\r\n    server.init();\r\n    server.init();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\test\\java\\org\\apache\\hadoop\\lib\\server",
  "methodName" : "invalidSservice",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void invalidSservice() throws Exception\n{\r\n    String dir = TestDirHelper.getTestDir().getAbsolutePath();\r\n    Configuration conf = new Configuration(false);\r\n    conf.set(\"server.services\", \"foo\");\r\n    Server server = new Server(\"server\", dir, dir, dir, dir, conf);\r\n    server.init();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\test\\java\\org\\apache\\hadoop\\lib\\server",
  "methodName" : "serviceWithNoDefaultConstructor",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void serviceWithNoDefaultConstructor() throws Exception\n{\r\n    String dir = TestDirHelper.getTestDir().getAbsolutePath();\r\n    Configuration conf = new Configuration(false);\r\n    conf.set(\"server.services\", MyService7.class.getName());\r\n    Server server = new Server(\"server\", dir, dir, dir, dir, conf);\r\n    server.init();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\test\\java\\org\\apache\\hadoop\\lib\\server",
  "methodName" : "serviceNotImplementingServiceInterface",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void serviceNotImplementingServiceInterface() throws Exception\n{\r\n    String dir = TestDirHelper.getTestDir().getAbsolutePath();\r\n    Configuration conf = new Configuration(false);\r\n    conf.set(\"server.services\", MyService4.class.getName());\r\n    Server server = new Server(\"server\", dir, dir, dir, dir, conf);\r\n    server.init();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\test\\java\\org\\apache\\hadoop\\lib\\server",
  "methodName" : "serviceWithMissingDependency",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void serviceWithMissingDependency() throws Exception\n{\r\n    String dir = TestDirHelper.getTestDir().getAbsolutePath();\r\n    Configuration conf = new Configuration(false);\r\n    String services = StringUtils.join(\",\", Arrays.asList(MyService3.class.getName(), MyService6.class.getName()));\r\n    conf.set(\"server.services\", services);\r\n    Server server = new Server(\"server\", dir, dir, dir, dir, conf);\r\n    server.init();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\test\\java\\org\\apache\\hadoop\\lib\\server",
  "methodName" : "services",
  "errType" : [ "ServerException", "Exception", "ServerException", "Exception" ],
  "containingMethodsNum" : 103,
  "sourceCodeText" : "void services() throws Exception\n{\r\n    String dir = TestDirHelper.getTestDir().getAbsolutePath();\r\n    Configuration conf;\r\n    Server server;\r\n    ORDER.clear();\r\n    conf = new Configuration(false);\r\n    server = new Server(\"server\", dir, dir, dir, dir, conf);\r\n    server.init();\r\n    assertEquals(ORDER.size(), 0);\r\n    ORDER.clear();\r\n    String services = StringUtils.join(\",\", Arrays.asList(MyService1.class.getName(), MyService3.class.getName()));\r\n    conf = new Configuration(false);\r\n    conf.set(\"server.services\", services);\r\n    server = new Server(\"server\", dir, dir, dir, dir, conf);\r\n    server.init();\r\n    assertEquals(server.get(MyService1.class).getInterface(), MyService1.class);\r\n    assertEquals(server.get(MyService3.class).getInterface(), MyService3.class);\r\n    assertEquals(ORDER.size(), 4);\r\n    assertEquals(ORDER.get(0), \"s1.init\");\r\n    assertEquals(ORDER.get(1), \"s3.init\");\r\n    assertEquals(ORDER.get(2), \"s1.postInit\");\r\n    assertEquals(ORDER.get(3), \"s3.postInit\");\r\n    server.destroy();\r\n    assertEquals(ORDER.size(), 6);\r\n    assertEquals(ORDER.get(4), \"s3.destroy\");\r\n    assertEquals(ORDER.get(5), \"s1.destroy\");\r\n    ORDER.clear();\r\n    services = StringUtils.join(\",\", Arrays.asList(MyService1.class.getName(), MyService2.class.getName(), MyService3.class.getName()));\r\n    conf = new Configuration(false);\r\n    conf.set(\"server.services\", services);\r\n    server = new Server(\"server\", dir, dir, dir, dir, conf);\r\n    try {\r\n        server.init();\r\n        fail();\r\n    } catch (ServerException ex) {\r\n        assertEquals(MyService2.class, ex.getError().getClass());\r\n    } catch (Exception ex) {\r\n        fail();\r\n    }\r\n    assertEquals(ORDER.size(), 3);\r\n    assertEquals(ORDER.get(0), \"s1.init\");\r\n    assertEquals(ORDER.get(1), \"s2.init\");\r\n    assertEquals(ORDER.get(2), \"s1.destroy\");\r\n    ORDER.clear();\r\n    services = StringUtils.join(\",\", Arrays.asList(MyService1.class.getName(), MyService5.class.getName()));\r\n    conf = new Configuration(false);\r\n    conf.set(\"server.services\", services);\r\n    server = new Server(\"server\", dir, dir, dir, dir, conf);\r\n    server.init();\r\n    assertEquals(ORDER.size(), 4);\r\n    assertEquals(ORDER.get(0), \"s1.init\");\r\n    assertEquals(ORDER.get(1), \"s5.init\");\r\n    assertEquals(ORDER.get(2), \"s1.postInit\");\r\n    assertEquals(ORDER.get(3), \"s5.postInit\");\r\n    server.destroy();\r\n    assertEquals(ORDER.size(), 6);\r\n    assertEquals(ORDER.get(4), \"s5.destroy\");\r\n    assertEquals(ORDER.get(5), \"s1.destroy\");\r\n    ORDER.clear();\r\n    services = StringUtils.join(\",\", Arrays.asList(MyService1.class.getName(), MyService3.class.getName()));\r\n    String servicesExt = StringUtils.join(\",\", Arrays.asList(MyService1a.class.getName()));\r\n    conf = new Configuration(false);\r\n    conf.set(\"server.services\", services);\r\n    conf.set(\"server.services.ext\", servicesExt);\r\n    server = new Server(\"server\", dir, dir, dir, dir, conf);\r\n    server.init();\r\n    assertEquals(server.get(MyService1.class).getClass(), MyService1a.class);\r\n    assertEquals(ORDER.size(), 4);\r\n    assertEquals(ORDER.get(0), \"s1a.init\");\r\n    assertEquals(ORDER.get(1), \"s3.init\");\r\n    assertEquals(ORDER.get(2), \"s1a.postInit\");\r\n    assertEquals(ORDER.get(3), \"s3.postInit\");\r\n    server.destroy();\r\n    assertEquals(ORDER.size(), 6);\r\n    assertEquals(ORDER.get(4), \"s3.destroy\");\r\n    assertEquals(ORDER.get(5), \"s1a.destroy\");\r\n    ORDER.clear();\r\n    services = StringUtils.join(\",\", Arrays.asList(MyService1.class.getName(), MyService3.class.getName()));\r\n    conf = new Configuration(false);\r\n    conf.set(\"server.services\", services);\r\n    server = new Server(\"server\", dir, dir, dir, dir, conf);\r\n    server.init();\r\n    server.setService(MyService1a.class);\r\n    assertEquals(ORDER.size(), 6);\r\n    assertEquals(ORDER.get(4), \"s1.destroy\");\r\n    assertEquals(ORDER.get(5), \"s1a.init\");\r\n    assertEquals(server.get(MyService1.class).getClass(), MyService1a.class);\r\n    server.destroy();\r\n    assertEquals(ORDER.size(), 8);\r\n    assertEquals(ORDER.get(6), \"s3.destroy\");\r\n    assertEquals(ORDER.get(7), \"s1a.destroy\");\r\n    ORDER.clear();\r\n    services = StringUtils.join(\",\", Arrays.asList(MyService1.class.getName(), MyService3.class.getName()));\r\n    conf = new Configuration(false);\r\n    conf.set(\"server.services\", services);\r\n    server = new Server(\"server\", dir, dir, dir, dir, conf);\r\n    server.init();\r\n    server.setService(MyService5.class);\r\n    assertEquals(ORDER.size(), 5);\r\n    assertEquals(ORDER.get(4), \"s5.init\");\r\n    assertEquals(server.get(MyService5.class).getClass(), MyService5.class);\r\n    server.destroy();\r\n    assertEquals(ORDER.size(), 8);\r\n    assertEquals(ORDER.get(5), \"s5.destroy\");\r\n    assertEquals(ORDER.get(6), \"s3.destroy\");\r\n    assertEquals(ORDER.get(7), \"s1.destroy\");\r\n    ORDER.clear();\r\n    services = StringUtils.join(\",\", Arrays.asList(MyService1.class.getName(), MyService3.class.getName()));\r\n    conf = new Configuration(false);\r\n    conf.set(\"server.services\", services);\r\n    server = new Server(\"server\", dir, dir, dir, dir, conf);\r\n    server.init();\r\n    try {\r\n        server.setService(MyService7.class);\r\n        fail();\r\n    } catch (ServerException ex) {\r\n        assertEquals(ServerException.ERROR.S09, ex.getError());\r\n    } catch (Exception ex) {\r\n        fail();\r\n    }\r\n    assertEquals(ORDER.size(), 6);\r\n    assertEquals(ORDER.get(4), \"s3.destroy\");\r\n    assertEquals(ORDER.get(5), \"s1.destroy\");\r\n    ORDER.clear();\r\n    services = StringUtils.join(\",\", Arrays.asList(MyService1.class.getName(), MyService6.class.getName()));\r\n    conf = new Configuration(false);\r\n    conf.set(\"server.services\", services);\r\n    server = new Server(\"server\", dir, dir, dir, dir, conf);\r\n    server.init();\r\n    assertEquals(server.get(MyService1.class).getInterface(), MyService1.class);\r\n    assertEquals(server.get(MyService6.class).getInterface(), MyService6.class);\r\n    server.destroy();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 2,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\test\\java\\org\\apache\\hadoop\\lib\\server",
  "methodName" : "getAbsolutePath",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String getAbsolutePath(String relativePath)\n{\r\n    return new File(TestDirHelper.getTestDir(), relativePath).getAbsolutePath();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\test\\java\\org\\apache\\hadoop\\lib\\wsrs",
  "methodName" : "test",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void test() throws Exception\n{\r\n    JSONProvider p = new JSONProvider();\r\n    assertTrue(p.isWriteable(JSONObject.class, null, null, null));\r\n    assertFalse(p.isWriteable(this.getClass(), null, null, null));\r\n    assertEquals(p.getSize(null, null, null, null, null), -1);\r\n    ByteArrayOutputStream baos = new ByteArrayOutputStream();\r\n    JSONObject json = new JSONObject();\r\n    json.put(\"a\", \"A\");\r\n    p.writeTo(json, JSONObject.class, null, null, null, null, baos);\r\n    baos.close();\r\n    assertEquals(new String(baos.toByteArray()).trim(), \"{\\\"a\\\":\\\"A\\\"}\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\test\\java\\org\\apache\\hadoop\\lib\\server",
  "methodName" : "baseService",
  "errType" : null,
  "containingMethodsNum" : 14,
  "sourceCodeText" : "void baseService() throws Exception\n{\r\n    BaseService service = new MyService();\r\n    assertNull(service.getInterface());\r\n    assertEquals(service.getPrefix(), \"myservice\");\r\n    assertEquals(service.getServiceDependencies().length, 0);\r\n    Server server = Mockito.mock(Server.class);\r\n    Configuration conf = new Configuration(false);\r\n    conf.set(\"server.myservice.foo\", \"FOO\");\r\n    conf.set(\"server.myservice1.bar\", \"BAR\");\r\n    Mockito.when(server.getConfig()).thenReturn(conf);\r\n    Mockito.when(server.getPrefixedName(\"myservice.foo\")).thenReturn(\"server.myservice.foo\");\r\n    Mockito.when(server.getPrefixedName(\"myservice.\")).thenReturn(\"server.myservice.\");\r\n    service.init(server);\r\n    assertEquals(service.getPrefixedName(\"foo\"), \"server.myservice.foo\");\r\n    assertEquals(service.getServiceConfig().size(), 1);\r\n    assertEquals(service.getServiceConfig().get(\"foo\"), \"FOO\");\r\n    assertTrue(MyService.INIT);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\test\\java\\org\\apache\\hadoop\\test",
  "methodName" : "dummy",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void dummy()\n{\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\test\\java\\org\\apache\\hadoop\\test",
  "methodName" : "apply",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "Statement apply(Statement statement, FrameworkMethod frameworkMethod, Object o)\n{\r\n    TestHdfs testHdfsAnnotation = frameworkMethod.getAnnotation(TestHdfs.class);\r\n    if (testHdfsAnnotation != null) {\r\n        this.statement = new HdfsStatement(statement, frameworkMethod.getName());\r\n        statement = this.statement;\r\n    }\r\n    return super.apply(statement, frameworkMethod, o);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\test\\java\\org\\apache\\hadoop\\test",
  "methodName" : "getMiniDFSCluster",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "MiniDFSCluster getMiniDFSCluster()\n{\r\n    return statement.getMiniDFSCluster();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\test\\java\\org\\apache\\hadoop\\test",
  "methodName" : "getHdfsTestDir",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Path getHdfsTestDir()\n{\r\n    Path testDir = HDFS_TEST_DIR_TL.get();\r\n    if (testDir == null) {\r\n        throw new IllegalStateException(\"This test does not use @TestHdfs\");\r\n    }\r\n    return testDir;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\test\\java\\org\\apache\\hadoop\\test",
  "methodName" : "getHdfsConf",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Configuration getHdfsConf()\n{\r\n    Configuration conf = HDFS_CONF_TL.get();\r\n    if (conf == null) {\r\n        throw new IllegalStateException(\"This test does not use @TestHdfs\");\r\n    }\r\n    return new Configuration(conf);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\test\\java\\org\\apache\\hadoop\\test",
  "methodName" : "startMiniHdfs",
  "errType" : null,
  "containingMethodsNum" : 35,
  "sourceCodeText" : "MiniDFSCluster startMiniHdfs(Configuration conf) throws Exception\n{\r\n    if (MINI_DFS == null) {\r\n        if (System.getProperty(\"hadoop.log.dir\") == null) {\r\n            System.setProperty(\"hadoop.log.dir\", new File(TEST_DIR_ROOT, \"hadoop-log\").getAbsolutePath());\r\n        }\r\n        if (System.getProperty(\"test.build.data\") == null) {\r\n            System.setProperty(\"test.build.data\", new File(TEST_DIR_ROOT, \"hadoop-data\").getAbsolutePath());\r\n        }\r\n        conf = new Configuration(conf);\r\n        HadoopUsersConfTestHelper.addUserConf(conf);\r\n        conf.set(\"fs.hdfs.impl.disable.cache\", \"true\");\r\n        conf.set(\"dfs.block.access.token.enable\", \"false\");\r\n        conf.set(\"dfs.permissions\", \"true\");\r\n        conf.set(\"hadoop.security.authentication\", \"simple\");\r\n        conf.setBoolean(DFSConfigKeys.DFS_NAMENODE_ACLS_ENABLED_KEY, true);\r\n        conf.setBoolean(DFSConfigKeys.DFS_NAMENODE_XATTRS_ENABLED_KEY, true);\r\n        conf.set(DFSConfigKeys.DFS_STORAGE_POLICY_SATISFIER_MODE_KEY, StoragePolicySatisfierMode.EXTERNAL.toString());\r\n        conf.set(HdfsClientConfigKeys.DFS_WEBHDFS_USER_PATTERN_KEY, \"^[A-Za-z0-9_][A-Za-z0-9._-]*[$]?$\");\r\n        conf.set(HdfsClientConfigKeys.DFS_WEBHDFS_ACL_PERMISSION_PATTERN_KEY, \"^(default:)?(user|group|mask|other):\" + \"[[0-9A-Za-z_][@A-Za-z0-9._-]]*:([rwx-]{3})?(,(default:)?\" + \"(user|group|mask|other):[[0-9A-Za-z_][@A-Za-z0-9._-]]*:\" + \"([rwx-]{3})?)*$\");\r\n        FileSystemTestHelper helper = new FileSystemTestHelper();\r\n        Path targetFile = new Path(new File(helper.getTestRootDir()).getAbsolutePath(), \"test.jks\");\r\n        final String jceksPath = JavaKeyStoreProvider.SCHEME_NAME + \"://file\" + targetFile.toUri();\r\n        conf.set(CommonConfigurationKeysPublic.HADOOP_SECURITY_KEY_PROVIDER_PATH, jceksPath);\r\n        MiniDFSCluster.Builder builder = new MiniDFSCluster.Builder(conf);\r\n        int totalDataNodes = ERASURE_CODING_POLICY.getNumDataUnits() + ERASURE_CODING_POLICY.getNumParityUnits();\r\n        builder.numDataNodes(totalDataNodes);\r\n        MiniDFSCluster miniHdfs = builder.build();\r\n        final String testkey = \"testkey\";\r\n        DFSTestUtil.createKey(testkey, miniHdfs, conf);\r\n        DistributedFileSystem fileSystem = miniHdfs.getFileSystem();\r\n        fileSystem.enableErasureCodingPolicy(ERASURE_CODING_POLICY.getName());\r\n        fileSystem.getClient().setKeyProvider(miniHdfs.getNameNode().getNamesystem().getProvider());\r\n        fileSystem.mkdirs(new Path(\"/tmp\"));\r\n        fileSystem.mkdirs(new Path(\"/user\"));\r\n        fileSystem.setPermission(new Path(\"/tmp\"), FsPermission.valueOf(\"-rwxrwxrwx\"));\r\n        fileSystem.setPermission(new Path(\"/user\"), FsPermission.valueOf(\"-rwxrwxrwx\"));\r\n        fileSystem.mkdirs(ENCRYPTION_ZONE);\r\n        fileSystem.createEncryptionZone(ENCRYPTION_ZONE, testkey);\r\n        fileSystem.create(ENCRYPTED_FILE).close();\r\n        fileSystem.mkdirs(ERASURE_CODING_DIR);\r\n        fileSystem.setErasureCodingPolicy(ERASURE_CODING_DIR, ERASURE_CODING_POLICY.getName());\r\n        fileSystem.create(ERASURE_CODING_FILE).close();\r\n        MINI_DFS = miniHdfs;\r\n    }\r\n    return MINI_DFS;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\test\\java\\org\\apache\\hadoop\\fs\\http\\client",
  "methodName" : "getProxiedFSTestDir",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Path getProxiedFSTestDir()",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\test\\java\\org\\apache\\hadoop\\fs\\http\\client",
  "methodName" : "getProxiedFSURI",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getProxiedFSURI()",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\test\\java\\org\\apache\\hadoop\\fs\\http\\client",
  "methodName" : "getProxiedFSConf",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Configuration getProxiedFSConf()",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\test\\java\\org\\apache\\hadoop\\fs\\http\\client",
  "methodName" : "isLocalFS",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "boolean isLocalFS()\n{\r\n    return getProxiedFSURI().startsWith(\"file://\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\test\\java\\org\\apache\\hadoop\\fs\\http\\client",
  "methodName" : "createHttpFSServer",
  "errType" : null,
  "containingMethodsNum" : 26,
  "sourceCodeText" : "void createHttpFSServer() throws Exception\n{\r\n    File homeDir = TestDirHelper.getTestDir();\r\n    assertTrue(new File(homeDir, \"conf\").mkdir());\r\n    assertTrue(new File(homeDir, \"log\").mkdir());\r\n    assertTrue(new File(homeDir, \"temp\").mkdir());\r\n    HttpFSServerWebApp.setHomeDirForCurrentThread(homeDir.getAbsolutePath());\r\n    File secretFile = new File(new File(homeDir, \"conf\"), \"secret\");\r\n    Writer w = new FileWriter(secretFile);\r\n    w.write(\"secret\");\r\n    w.close();\r\n    String fsDefaultName = getProxiedFSURI();\r\n    Configuration conf = new Configuration(false);\r\n    conf.set(CommonConfigurationKeysPublic.FS_DEFAULT_NAME_KEY, fsDefaultName);\r\n    conf.setBoolean(DFSConfigKeys.DFS_NAMENODE_ACLS_ENABLED_KEY, true);\r\n    conf.setBoolean(DFSConfigKeys.DFS_NAMENODE_XATTRS_ENABLED_KEY, true);\r\n    conf.set(HdfsClientConfigKeys.DFS_WEBHDFS_USER_PATTERN_KEY, \"^[A-Za-z0-9_][A-Za-z0-9._-]*[$]?$\");\r\n    conf.set(HdfsClientConfigKeys.DFS_WEBHDFS_ACL_PERMISSION_PATTERN_KEY, \"^(default:)?(user|group|mask|other):\" + \"[[0-9A-Za-z_][@A-Za-z0-9._-]]*:([rwx-]{3})?(,(default:)?\" + \"(user|group|mask|other):[[0-9A-Za-z_][@A-Za-z0-9._-]]*:\" + \"([rwx-]{3})?)*$\");\r\n    File hdfsSite = new File(new File(homeDir, \"conf\"), \"hdfs-site.xml\");\r\n    OutputStream os = new FileOutputStream(hdfsSite);\r\n    conf.writeXml(os);\r\n    os.close();\r\n    conf = new Configuration(false);\r\n    conf.set(\"httpfs.proxyuser.\" + HadoopUsersConfTestHelper.getHadoopProxyUser() + \".groups\", HadoopUsersConfTestHelper.getHadoopProxyUserGroups());\r\n    conf.set(\"httpfs.proxyuser.\" + HadoopUsersConfTestHelper.getHadoopProxyUser() + \".hosts\", HadoopUsersConfTestHelper.getHadoopProxyUserHosts());\r\n    conf.set(HttpFSAuthenticationFilter.HADOOP_HTTP_CONF_PREFIX + AuthenticationFilter.SIGNATURE_SECRET_FILE, secretFile.getAbsolutePath());\r\n    File httpfsSite = new File(new File(homeDir, \"conf\"), \"httpfs-site.xml\");\r\n    os = new FileOutputStream(httpfsSite);\r\n    conf.writeXml(os);\r\n    os.close();\r\n    ClassLoader cl = Thread.currentThread().getContextClassLoader();\r\n    URL url = cl.getResource(\"webapp\");\r\n    WebAppContext context = new WebAppContext(url.getPath(), \"/webhdfs\");\r\n    Server server = TestJettyHelper.getJettyServer();\r\n    server.setHandler(context);\r\n    server.start();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\test\\java\\org\\apache\\hadoop\\fs\\http\\client",
  "methodName" : "getFileSystemClass",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Class getFileSystemClass()\n{\r\n    return HttpFSFileSystem.class;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\test\\java\\org\\apache\\hadoop\\fs\\http\\client",
  "methodName" : "getScheme",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getScheme()\n{\r\n    return \"webhdfs\";\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\test\\java\\org\\apache\\hadoop\\fs\\http\\client",
  "methodName" : "getHttpFSFileSystem",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "FileSystem getHttpFSFileSystem(Configuration conf) throws Exception\n{\r\n    conf.set(\"fs.webhdfs.impl\", getFileSystemClass().getName());\r\n    URI uri = new URI(getScheme() + \"://\" + TestJettyHelper.getJettyURL().toURI().getAuthority());\r\n    return FileSystem.get(uri, conf);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\test\\java\\org\\apache\\hadoop\\fs\\http\\client",
  "methodName" : "getHttpFSFileSystem",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "FileSystem getHttpFSFileSystem() throws Exception\n{\r\n    Configuration conf = new Configuration();\r\n    return getHttpFSFileSystem(conf);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\test\\java\\org\\apache\\hadoop\\fs\\http\\client",
  "methodName" : "testGet",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void testGet() throws Exception\n{\r\n    FileSystem fs = getHttpFSFileSystem();\r\n    Assert.assertNotNull(fs);\r\n    URI uri = new URI(getScheme() + \"://\" + TestJettyHelper.getJettyURL().toURI().getAuthority());\r\n    assertEquals(fs.getUri(), uri);\r\n    fs.close();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\test\\java\\org\\apache\\hadoop\\fs\\http\\client",
  "methodName" : "testOpen",
  "errType" : null,
  "containingMethodsNum" : 11,
  "sourceCodeText" : "void testOpen() throws Exception\n{\r\n    FileSystem fs = FileSystem.get(getProxiedFSConf());\r\n    Path path = new Path(getProxiedFSTestDir(), \"foo.txt\");\r\n    OutputStream os = fs.create(path);\r\n    os.write(1);\r\n    os.close();\r\n    fs.close();\r\n    fs = getHttpFSFileSystem();\r\n    InputStream is = fs.open(new Path(path.toUri().getPath()));\r\n    assertEquals(is.read(), 1);\r\n    is.close();\r\n    fs.close();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\test\\java\\org\\apache\\hadoop\\fs\\http\\client",
  "methodName" : "testCreate",
  "errType" : null,
  "containingMethodsNum" : 15,
  "sourceCodeText" : "void testCreate(Path path, boolean override) throws Exception\n{\r\n    FileSystem fs = getHttpFSFileSystem();\r\n    FsPermission permission = new FsPermission(FsAction.READ_WRITE, FsAction.NONE, FsAction.NONE);\r\n    OutputStream os = fs.create(new Path(path.toUri().getPath()), permission, override, 1024, (short) 2, 100 * 1024 * 1024, null);\r\n    os.write(1);\r\n    os.close();\r\n    fs.close();\r\n    fs = FileSystem.get(getProxiedFSConf());\r\n    FileStatus status = fs.getFileStatus(path);\r\n    if (!isLocalFS()) {\r\n        assertEquals(status.getReplication(), 2);\r\n        assertEquals(status.getBlockSize(), 100 * 1024 * 1024);\r\n    }\r\n    assertEquals(status.getPermission(), permission);\r\n    InputStream is = fs.open(path);\r\n    assertEquals(is.read(), 1);\r\n    is.close();\r\n    fs.close();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\test\\java\\org\\apache\\hadoop\\fs\\http\\client",
  "methodName" : "testCreate",
  "errType" : [ "IOException", "Exception" ],
  "containingMethodsNum" : 9,
  "sourceCodeText" : "void testCreate() throws Exception\n{\r\n    Path path = new Path(getProxiedFSTestDir(), \"foo.txt\");\r\n    FileSystem fs = FileSystem.get(getProxiedFSConf());\r\n    fs.delete(path, true);\r\n    testCreate(path, false);\r\n    testCreate(path, true);\r\n    try {\r\n        testCreate(path, false);\r\n        Assert.fail(\"the create should have failed because the file exists \" + \"and override is FALSE\");\r\n    } catch (IOException ex) {\r\n        System.out.println(\"#\");\r\n    } catch (Exception ex) {\r\n        Assert.fail(ex.toString());\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\test\\java\\org\\apache\\hadoop\\fs\\http\\client",
  "methodName" : "testAppend",
  "errType" : null,
  "containingMethodsNum" : 20,
  "sourceCodeText" : "void testAppend() throws Exception\n{\r\n    if (!isLocalFS()) {\r\n        FileSystem fs = FileSystem.get(getProxiedFSConf());\r\n        fs.mkdirs(getProxiedFSTestDir());\r\n        Path path = new Path(getProxiedFSTestDir(), \"foo.txt\");\r\n        OutputStream os = fs.create(path);\r\n        os.write(1);\r\n        os.close();\r\n        fs.close();\r\n        fs = getHttpFSFileSystem();\r\n        os = fs.append(new Path(path.toUri().getPath()));\r\n        os.write(2);\r\n        os.close();\r\n        fs.close();\r\n        fs = FileSystem.get(getProxiedFSConf());\r\n        InputStream is = fs.open(path);\r\n        assertEquals(is.read(), 1);\r\n        assertEquals(is.read(), 2);\r\n        assertEquals(is.read(), -1);\r\n        is.close();\r\n        fs.close();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\test\\java\\org\\apache\\hadoop\\fs\\http\\client",
  "methodName" : "testTruncate",
  "errType" : null,
  "containingMethodsNum" : 12,
  "sourceCodeText" : "void testTruncate() throws Exception\n{\r\n    if (!isLocalFS()) {\r\n        final short repl = 3;\r\n        final int blockSize = 1024;\r\n        final int numOfBlocks = 2;\r\n        FileSystem fs = FileSystem.get(getProxiedFSConf());\r\n        fs.mkdirs(getProxiedFSTestDir());\r\n        Path file = new Path(getProxiedFSTestDir(), \"foo.txt\");\r\n        final byte[] data = FileSystemTestHelper.getFileData(numOfBlocks, blockSize);\r\n        FileSystemTestHelper.createFile(fs, file, data, blockSize, repl);\r\n        final int newLength = blockSize;\r\n        boolean isReady = fs.truncate(file, newLength);\r\n        assertTrue(\"Recovery is not expected.\", isReady);\r\n        FileStatus fileStatus = fs.getFileStatus(file);\r\n        assertEquals(fileStatus.getLen(), newLength);\r\n        AppendTestUtil.checkFullFile(fs, file, newLength, data, file.toString());\r\n        fs.close();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\test\\java\\org\\apache\\hadoop\\fs\\http\\client",
  "methodName" : "testConcat",
  "errType" : null,
  "containingMethodsNum" : 17,
  "sourceCodeText" : "void testConcat() throws Exception\n{\r\n    Configuration config = getProxiedFSConf();\r\n    config.setLong(DFSConfigKeys.DFS_BLOCK_SIZE_KEY, 1024);\r\n    if (!isLocalFS()) {\r\n        FileSystem fs = FileSystem.get(config);\r\n        fs.mkdirs(getProxiedFSTestDir());\r\n        Path path1 = new Path(\"/test/foo.txt\");\r\n        Path path2 = new Path(\"/test/bar.txt\");\r\n        Path path3 = new Path(\"/test/derp.txt\");\r\n        DFSTestUtil.createFile(fs, path1, 1024, (short) 3, 0);\r\n        DFSTestUtil.createFile(fs, path2, 1024, (short) 3, 0);\r\n        DFSTestUtil.createFile(fs, path3, 1024, (short) 3, 0);\r\n        fs.close();\r\n        fs = getHttpFSFileSystem();\r\n        fs.concat(path1, new Path[] { path2, path3 });\r\n        fs.close();\r\n        fs = FileSystem.get(config);\r\n        assertTrue(fs.exists(path1));\r\n        assertFalse(fs.exists(path2));\r\n        assertFalse(fs.exists(path3));\r\n        fs.close();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\test\\java\\org\\apache\\hadoop\\fs\\http\\client",
  "methodName" : "testRename",
  "errType" : null,
  "containingMethodsNum" : 13,
  "sourceCodeText" : "void testRename() throws Exception\n{\r\n    FileSystem fs = FileSystem.get(getProxiedFSConf());\r\n    Path path = new Path(getProxiedFSTestDir(), \"foo\");\r\n    fs.mkdirs(path);\r\n    fs.close();\r\n    fs = getHttpFSFileSystem();\r\n    Path oldPath = new Path(path.toUri().getPath());\r\n    Path newPath = new Path(path.getParent(), \"bar\");\r\n    fs.rename(oldPath, newPath);\r\n    fs.close();\r\n    fs = FileSystem.get(getProxiedFSConf());\r\n    assertFalse(fs.exists(oldPath));\r\n    assertTrue(fs.exists(newPath));\r\n    fs.close();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\test\\java\\org\\apache\\hadoop\\fs\\http\\client",
  "methodName" : "testDelete",
  "errType" : [ "IOException", "Exception" ],
  "containingMethodsNum" : 21,
  "sourceCodeText" : "void testDelete() throws Exception\n{\r\n    Path foo = new Path(getProxiedFSTestDir(), \"foo\");\r\n    Path bar = new Path(getProxiedFSTestDir(), \"bar\");\r\n    Path foe = new Path(getProxiedFSTestDir(), \"foe\");\r\n    FileSystem fs = FileSystem.get(getProxiedFSConf());\r\n    fs.mkdirs(foo);\r\n    fs.mkdirs(new Path(bar, \"a\"));\r\n    fs.mkdirs(foe);\r\n    FileSystem hoopFs = getHttpFSFileSystem();\r\n    assertTrue(hoopFs.delete(new Path(foo.toUri().getPath()), false));\r\n    assertFalse(fs.exists(foo));\r\n    try {\r\n        hoopFs.delete(new Path(bar.toUri().getPath()), false);\r\n        Assert.fail();\r\n    } catch (IOException ex) {\r\n    } catch (Exception ex) {\r\n        Assert.fail();\r\n    }\r\n    assertTrue(fs.exists(bar));\r\n    assertTrue(hoopFs.delete(new Path(bar.toUri().getPath()), true));\r\n    assertFalse(fs.exists(bar));\r\n    assertTrue(fs.exists(foe));\r\n    assertTrue(hoopFs.delete(foe, true));\r\n    assertFalse(fs.exists(foe));\r\n    hoopFs.close();\r\n    fs.close();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\test\\java\\org\\apache\\hadoop\\fs\\http\\client",
  "methodName" : "testListSymLinkStatus",
  "errType" : null,
  "containingMethodsNum" : 18,
  "sourceCodeText" : "void testListSymLinkStatus() throws Exception\n{\r\n    if (isLocalFS()) {\r\n        return;\r\n    }\r\n    FileSystem fs = FileSystem.get(getProxiedFSConf());\r\n    boolean isWebhdfs = fs instanceof WebHdfsFileSystem;\r\n    Path path = new Path(getProxiedFSTestDir() + \"-symlink\", \"targetFoo.txt\");\r\n    OutputStream os = fs.create(path);\r\n    os.write(1);\r\n    os.close();\r\n    Path linkPath = new Path(getProxiedFSTestDir() + \"-symlink\", \"symlinkFoo.txt\");\r\n    fs.createSymlink(path, linkPath, false);\r\n    fs = getHttpFSFileSystem();\r\n    FileStatus linkStatus = fs.getFileStatus(linkPath);\r\n    FileStatus status1 = fs.getFileStatus(path);\r\n    FileStatus[] stati = fs.listStatus(path.getParent());\r\n    assertEquals(2, stati.length);\r\n    int countSymlink = 0;\r\n    for (int i = 0; i < stati.length; i++) {\r\n        FileStatus fStatus = stati[i];\r\n        countSymlink += fStatus.isSymlink() ? 1 : 0;\r\n    }\r\n    assertEquals(1, countSymlink);\r\n    assertFalse(status1.isSymlink());\r\n    if (isWebhdfs) {\r\n        assertTrue(linkStatus.isSymlink());\r\n    }\r\n    fs.close();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\test\\java\\org\\apache\\hadoop\\fs\\http\\client",
  "methodName" : "testListStatus",
  "errType" : null,
  "containingMethodsNum" : 31,
  "sourceCodeText" : "void testListStatus() throws Exception\n{\r\n    FileSystem fs = FileSystem.get(getProxiedFSConf());\r\n    boolean isDFS = fs instanceof DistributedFileSystem;\r\n    Path path = new Path(getProxiedFSTestDir(), \"foo.txt\");\r\n    OutputStream os = fs.create(path);\r\n    os.write(1);\r\n    os.close();\r\n    FileStatus status1 = fs.getFileStatus(path);\r\n    fs.close();\r\n    fs = getHttpFSFileSystem();\r\n    FileStatus status2 = fs.getFileStatus(new Path(path.toUri().getPath()));\r\n    fs.close();\r\n    assertEquals(status2.getPermission(), status1.getPermission());\r\n    assertEquals(status2.getPath().toUri().getPath(), status1.getPath().toUri().getPath());\r\n    assertEquals(status2.getReplication(), status1.getReplication());\r\n    assertEquals(status2.getBlockSize(), status1.getBlockSize());\r\n    assertEquals(status2.getAccessTime(), status1.getAccessTime());\r\n    assertEquals(status2.getModificationTime(), status1.getModificationTime());\r\n    assertEquals(status2.getOwner(), status1.getOwner());\r\n    assertEquals(status2.getGroup(), status1.getGroup());\r\n    assertEquals(status2.getLen(), status1.getLen());\r\n    if (isDFS && status2 instanceof HdfsFileStatus) {\r\n        assertTrue(status1 instanceof HdfsFileStatus);\r\n        HdfsFileStatus hdfsFileStatus1 = (HdfsFileStatus) status1;\r\n        HdfsFileStatus hdfsFileStatus2 = (HdfsFileStatus) status2;\r\n        assertEquals(hdfsFileStatus2.getChildrenNum(), hdfsFileStatus1.getChildrenNum());\r\n        assertEquals(hdfsFileStatus2.getFileId(), hdfsFileStatus1.getFileId());\r\n        assertEquals(hdfsFileStatus2.getStoragePolicy(), hdfsFileStatus1.getStoragePolicy());\r\n    }\r\n    FileStatus[] stati = fs.listStatus(path.getParent());\r\n    assertEquals(1, stati.length);\r\n    assertEquals(stati[0].getPath().getName(), path.getName());\r\n    FileStatus[] statl = fs.listStatus(path);\r\n    Assert.assertEquals(1, statl.length);\r\n    Assert.assertEquals(status2.getPath(), statl[0].getPath());\r\n    Assert.assertEquals(statl[0].getPath().getName(), path.getName());\r\n    Assert.assertEquals(stati[0].getPath(), statl[0].getPath());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\test\\java\\org\\apache\\hadoop\\fs\\http\\client",
  "methodName" : "testFileStatusAttr",
  "errType" : null,
  "containingMethodsNum" : 12,
  "sourceCodeText" : "void testFileStatusAttr() throws Exception\n{\r\n    if (!this.isLocalFS()) {\r\n        Path path = new Path(\"/tmp/tmp-snap-test\");\r\n        DistributedFileSystem distributedFs = (DistributedFileSystem) FileSystem.get(path.toUri(), this.getProxiedFSConf());\r\n        distributedFs.mkdirs(path);\r\n        FileSystem fs = this.getHttpFSFileSystem();\r\n        assertFalse(\"Snapshot should be disallowed by default\", fs.getFileStatus(path).isSnapshotEnabled());\r\n        distributedFs.allowSnapshot(path);\r\n        assertTrue(\"Snapshot enabled bit is not set in FileStatus\", fs.getFileStatus(path).isSnapshotEnabled());\r\n        distributedFs.disallowSnapshot(path);\r\n        assertFalse(\"Snapshot enabled bit is not cleared in FileStatus\", fs.getFileStatus(path).isSnapshotEnabled());\r\n        fs.delete(path, true);\r\n        fs.close();\r\n        distributedFs.close();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\test\\java\\org\\apache\\hadoop\\fs\\http\\client",
  "methodName" : "assertSameListing",
  "errType" : null,
  "containingMethodsNum" : 11,
  "sourceCodeText" : "void assertSameListing(FileSystem expected, FileSystem actual, Path p) throws IOException\n{\r\n    RemoteIterator<FileStatus> exIt = expected.listStatusIterator(p);\r\n    List<FileStatus> exStatuses = new ArrayList<>();\r\n    while (exIt.hasNext()) {\r\n        exStatuses.add(exIt.next());\r\n    }\r\n    RemoteIterator<FileStatus> acIt = actual.listStatusIterator(p);\r\n    List<FileStatus> acStatuses = new ArrayList<>();\r\n    while (acIt.hasNext()) {\r\n        acStatuses.add(acIt.next());\r\n    }\r\n    assertEquals(exStatuses.size(), acStatuses.size());\r\n    for (int i = 0; i < exStatuses.size(); i++) {\r\n        FileStatus expectedStatus = exStatuses.get(i);\r\n        FileStatus actualStatus = acStatuses.get(i);\r\n        assertEquals(expectedStatus.getPath().toUri().getPath(), actualStatus.getPath().toUri().getPath());\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\test\\java\\org\\apache\\hadoop\\fs\\http\\client",
  "methodName" : "testListStatusBatch",
  "errType" : null,
  "containingMethodsNum" : 21,
  "sourceCodeText" : "void testListStatusBatch() throws Exception\n{\r\n    Assume.assumeFalse(isLocalFS());\r\n    FileSystem proxyFs = FileSystem.get(getProxiedFSConf());\r\n    Configuration conf = new Configuration();\r\n    conf.setInt(DFSConfigKeys.DFS_LIST_LIMIT, 2);\r\n    FileSystem httpFs = getHttpFSFileSystem(conf);\r\n    Path dir = new Path(getProxiedFSTestDir(), \"dir\");\r\n    proxyFs.mkdirs(dir);\r\n    assertSameListing(proxyFs, httpFs, dir);\r\n    for (int i = 0; i < 10; i++) {\r\n        proxyFs.create(new Path(dir, \"file\" + i)).close();\r\n        assertSameListing(proxyFs, httpFs, dir);\r\n    }\r\n    Path dir1 = new Path(getProxiedFSTestDir(), \"dir1\");\r\n    proxyFs.mkdirs(dir1);\r\n    Path file1 = new Path(dir1, \"file1\");\r\n    proxyFs.create(file1).close();\r\n    RemoteIterator<FileStatus> si = proxyFs.listStatusIterator(dir1);\r\n    FileStatus statusl = si.next();\r\n    FileStatus status = proxyFs.getFileStatus(file1);\r\n    Assert.assertEquals(file1.getName(), statusl.getPath().getName());\r\n    Assert.assertEquals(status.getPath(), statusl.getPath());\r\n    si = proxyFs.listStatusIterator(file1);\r\n    statusl = si.next();\r\n    Assert.assertEquals(file1.getName(), statusl.getPath().getName());\r\n    Assert.assertEquals(status.getPath(), statusl.getPath());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\test\\java\\org\\apache\\hadoop\\fs\\http\\client",
  "methodName" : "testWorkingdirectory",
  "errType" : null,
  "containingMethodsNum" : 20,
  "sourceCodeText" : "void testWorkingdirectory() throws Exception\n{\r\n    FileSystem fs = FileSystem.get(getProxiedFSConf());\r\n    Path workingDir = fs.getWorkingDirectory();\r\n    fs.close();\r\n    fs = getHttpFSFileSystem();\r\n    if (isLocalFS()) {\r\n        fs.setWorkingDirectory(workingDir);\r\n    }\r\n    Path httpFSWorkingDir = fs.getWorkingDirectory();\r\n    fs.close();\r\n    assertEquals(httpFSWorkingDir.toUri().getPath(), workingDir.toUri().getPath());\r\n    fs = getHttpFSFileSystem();\r\n    fs.setWorkingDirectory(new Path(\"/tmp\"));\r\n    workingDir = fs.getWorkingDirectory();\r\n    assertEquals(workingDir.toUri().getPath(), new Path(\"/tmp\").toUri().getPath());\r\n    final FileSystem httpFs = getHttpFSFileSystem();\r\n    LambdaTestUtils.intercept(IllegalArgumentException.class, \"Invalid DFS directory name /foo:bar\", () -> httpFs.setWorkingDirectory(new Path(\"/foo:bar\")));\r\n    fs.setWorkingDirectory(new Path(\"/bar\"));\r\n    workingDir = fs.getWorkingDirectory();\r\n    httpFs.close();\r\n    fs.close();\r\n    assertEquals(workingDir.toUri().getPath(), new Path(\"/bar\").toUri().getPath());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\test\\java\\org\\apache\\hadoop\\fs\\http\\client",
  "methodName" : "testTrashRoot",
  "errType" : null,
  "containingMethodsNum" : 16,
  "sourceCodeText" : "void testTrashRoot() throws Exception\n{\r\n    if (!isLocalFS()) {\r\n        FileSystem fs = FileSystem.get(getProxiedFSConf());\r\n        final Path rootDir = new Path(\"/\");\r\n        final Path fooPath = new Path(getProxiedFSTestDir(), \"foo.txt\");\r\n        OutputStream os = fs.create(fooPath);\r\n        os.write(1);\r\n        os.close();\r\n        Path trashPath = fs.getTrashRoot(rootDir);\r\n        Path fooTrashPath = fs.getTrashRoot(fooPath);\r\n        fs.close();\r\n        fs = getHttpFSFileSystem();\r\n        Path httpFSTrashPath = fs.getTrashRoot(rootDir);\r\n        Path httpFSFooTrashPath = fs.getTrashRoot(fooPath);\r\n        fs.close();\r\n        assertEquals(trashPath.toUri().getPath(), httpFSTrashPath.toUri().getPath());\r\n        assertEquals(fooTrashPath.toUri().getPath(), httpFSFooTrashPath.toUri().getPath());\r\n        assertEquals(trashPath.toUri().getPath(), fooTrashPath.toUri().getPath());\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\test\\java\\org\\apache\\hadoop\\fs\\http\\client",
  "methodName" : "testMkdirs",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void testMkdirs() throws Exception\n{\r\n    Path path = new Path(getProxiedFSTestDir(), \"foo\");\r\n    FileSystem fs = getHttpFSFileSystem();\r\n    fs.mkdirs(path);\r\n    fs.close();\r\n    fs = FileSystem.get(getProxiedFSConf());\r\n    assertTrue(fs.exists(path));\r\n    fs.close();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\test\\java\\org\\apache\\hadoop\\fs\\http\\client",
  "methodName" : "testSetTimes",
  "errType" : null,
  "containingMethodsNum" : 20,
  "sourceCodeText" : "void testSetTimes() throws Exception\n{\r\n    if (!isLocalFS()) {\r\n        FileSystem fs = FileSystem.get(getProxiedFSConf());\r\n        Path path = new Path(getProxiedFSTestDir(), \"foo.txt\");\r\n        OutputStream os = fs.create(path);\r\n        os.write(1);\r\n        os.close();\r\n        FileStatus status1 = fs.getFileStatus(path);\r\n        fs.close();\r\n        long at = status1.getAccessTime();\r\n        long mt = status1.getModificationTime();\r\n        fs = getHttpFSFileSystem();\r\n        fs.setTimes(path, mt - 10, at - 20);\r\n        fs.close();\r\n        fs = FileSystem.get(getProxiedFSConf());\r\n        status1 = fs.getFileStatus(path);\r\n        fs.close();\r\n        long atNew = status1.getAccessTime();\r\n        long mtNew = status1.getModificationTime();\r\n        assertEquals(mtNew, mt - 10);\r\n        assertEquals(atNew, at - 20);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\test\\java\\org\\apache\\hadoop\\fs\\http\\client",
  "methodName" : "testSetPermission",
  "errType" : null,
  "containingMethodsNum" : 20,
  "sourceCodeText" : "void testSetPermission() throws Exception\n{\r\n    FileSystem fs = FileSystem.get(getProxiedFSConf());\r\n    Path path = new Path(getProxiedFSTestDir(), \"foodir\");\r\n    fs.mkdirs(path);\r\n    fs = getHttpFSFileSystem();\r\n    FsPermission permission1 = new FsPermission(FsAction.READ_WRITE, FsAction.NONE, FsAction.NONE);\r\n    fs.setPermission(path, permission1);\r\n    fs.close();\r\n    fs = FileSystem.get(getProxiedFSConf());\r\n    FileStatus status1 = fs.getFileStatus(path);\r\n    fs.close();\r\n    FsPermission permission2 = status1.getPermission();\r\n    assertEquals(permission2, permission1);\r\n    fs = getHttpFSFileSystem();\r\n    permission1 = new FsPermission(FsAction.READ_WRITE, FsAction.NONE, FsAction.NONE, true);\r\n    fs.setPermission(path, permission1);\r\n    fs.close();\r\n    fs = FileSystem.get(getProxiedFSConf());\r\n    status1 = fs.getFileStatus(path);\r\n    fs.close();\r\n    permission2 = status1.getPermission();\r\n    assertTrue(permission2.getStickyBit());\r\n    assertEquals(permission2, permission1);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\test\\java\\org\\apache\\hadoop\\fs\\http\\client",
  "methodName" : "testSetOwner",
  "errType" : null,
  "containingMethodsNum" : 18,
  "sourceCodeText" : "void testSetOwner() throws Exception\n{\r\n    if (!isLocalFS()) {\r\n        FileSystem fs = FileSystem.get(getProxiedFSConf());\r\n        fs.mkdirs(getProxiedFSTestDir());\r\n        Path path = new Path(getProxiedFSTestDir(), \"foo.txt\");\r\n        OutputStream os = fs.create(path);\r\n        os.write(1);\r\n        os.close();\r\n        fs.close();\r\n        fs = getHttpFSFileSystem();\r\n        String user = HadoopUsersConfTestHelper.getHadoopUsers()[1];\r\n        String group = HadoopUsersConfTestHelper.getHadoopUserGroups(user)[0];\r\n        fs.setOwner(path, user, group);\r\n        fs.close();\r\n        fs = FileSystem.get(getProxiedFSConf());\r\n        FileStatus status1 = fs.getFileStatus(path);\r\n        fs.close();\r\n        assertEquals(status1.getOwner(), user);\r\n        assertEquals(status1.getGroup(), group);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\test\\java\\org\\apache\\hadoop\\fs\\http\\client",
  "methodName" : "testSetReplication",
  "errType" : null,
  "containingMethodsNum" : 14,
  "sourceCodeText" : "void testSetReplication() throws Exception\n{\r\n    FileSystem fs = FileSystem.get(getProxiedFSConf());\r\n    Path path = new Path(getProxiedFSTestDir(), \"foo.txt\");\r\n    OutputStream os = fs.create(path);\r\n    os.write(1);\r\n    os.close();\r\n    fs.setReplication(path, (short) 2);\r\n    fs.close();\r\n    fs = getHttpFSFileSystem();\r\n    fs.setReplication(path, (short) 1);\r\n    fs.close();\r\n    fs = FileSystem.get(getProxiedFSConf());\r\n    FileStatus status1 = fs.getFileStatus(path);\r\n    fs.close();\r\n    assertEquals(status1.getReplication(), (short) 1);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\test\\java\\org\\apache\\hadoop\\fs\\http\\client",
  "methodName" : "testChecksum",
  "errType" : null,
  "containingMethodsNum" : 15,
  "sourceCodeText" : "void testChecksum() throws Exception\n{\r\n    if (!isLocalFS()) {\r\n        FileSystem fs = FileSystem.get(getProxiedFSConf());\r\n        fs.mkdirs(getProxiedFSTestDir());\r\n        Path path = new Path(getProxiedFSTestDir(), \"foo.txt\");\r\n        OutputStream os = fs.create(path);\r\n        os.write(1);\r\n        os.close();\r\n        FileChecksum hdfsChecksum = fs.getFileChecksum(path);\r\n        fs.close();\r\n        fs = getHttpFSFileSystem();\r\n        FileChecksum httpChecksum = fs.getFileChecksum(path);\r\n        fs.close();\r\n        assertEquals(httpChecksum.getAlgorithmName(), hdfsChecksum.getAlgorithmName());\r\n        assertEquals(httpChecksum.getLength(), hdfsChecksum.getLength());\r\n        assertArrayEquals(httpChecksum.getBytes(), hdfsChecksum.getBytes());\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\test\\java\\org\\apache\\hadoop\\fs\\http\\client",
  "methodName" : "testContentSummary",
  "errType" : null,
  "containingMethodsNum" : 17,
  "sourceCodeText" : "void testContentSummary() throws Exception\n{\r\n    FileSystem fs = FileSystem.get(getProxiedFSConf());\r\n    Path path = new Path(getProxiedFSTestDir(), \"foo.txt\");\r\n    OutputStream os = fs.create(path);\r\n    os.write(1);\r\n    os.close();\r\n    ContentSummary hdfsContentSummary = fs.getContentSummary(path);\r\n    fs.close();\r\n    fs = getHttpFSFileSystem();\r\n    ContentSummary httpContentSummary = fs.getContentSummary(path);\r\n    fs.close();\r\n    assertEquals(hdfsContentSummary.getDirectoryCount(), httpContentSummary.getDirectoryCount());\r\n    assertEquals(hdfsContentSummary.getErasureCodingPolicy(), httpContentSummary.getErasureCodingPolicy());\r\n    assertEquals(hdfsContentSummary.getFileCount(), httpContentSummary.getFileCount());\r\n    assertEquals(hdfsContentSummary.getLength(), httpContentSummary.getLength());\r\n    assertEquals(hdfsContentSummary.getQuota(), httpContentSummary.getQuota());\r\n    assertEquals(hdfsContentSummary.getSpaceConsumed(), httpContentSummary.getSpaceConsumed());\r\n    assertEquals(hdfsContentSummary.getSpaceQuota(), httpContentSummary.getSpaceQuota());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\test\\java\\org\\apache\\hadoop\\fs\\http\\client",
  "methodName" : "testQuotaUsage",
  "errType" : null,
  "containingMethodsNum" : 22,
  "sourceCodeText" : "void testQuotaUsage() throws Exception\n{\r\n    if (isLocalFS()) {\r\n        return;\r\n    }\r\n    DistributedFileSystem dfs = (DistributedFileSystem) FileSystem.get(getProxiedFSConf());\r\n    Path path = new Path(getProxiedFSTestDir(), \"foo\");\r\n    dfs.mkdirs(path);\r\n    dfs.setQuota(path, 20, 600 * 1024 * 1024);\r\n    for (int i = 0; i < 10; i++) {\r\n        dfs.createNewFile(new Path(path, \"test_file_\" + i));\r\n    }\r\n    FSDataOutputStream out = dfs.create(new Path(path, \"test_file\"));\r\n    out.writeUTF(\"Hello World\");\r\n    out.close();\r\n    dfs.setQuotaByStorageType(path, StorageType.SSD, 100000);\r\n    dfs.setQuotaByStorageType(path, StorageType.DISK, 200000);\r\n    QuotaUsage hdfsQuotaUsage = dfs.getQuotaUsage(path);\r\n    dfs.close();\r\n    FileSystem fs = getHttpFSFileSystem();\r\n    QuotaUsage httpQuotaUsage = fs.getQuotaUsage(path);\r\n    fs.close();\r\n    assertEquals(hdfsQuotaUsage.getFileAndDirectoryCount(), httpQuotaUsage.getFileAndDirectoryCount());\r\n    assertEquals(hdfsQuotaUsage.getQuota(), httpQuotaUsage.getQuota());\r\n    assertEquals(hdfsQuotaUsage.getSpaceConsumed(), httpQuotaUsage.getSpaceConsumed());\r\n    assertEquals(hdfsQuotaUsage.getSpaceQuota(), httpQuotaUsage.getSpaceQuota());\r\n    assertEquals(hdfsQuotaUsage.getTypeQuota(StorageType.SSD), httpQuotaUsage.getTypeQuota(StorageType.SSD));\r\n    assertEquals(hdfsQuotaUsage.getTypeQuota(StorageType.DISK), httpQuotaUsage.getTypeQuota(StorageType.DISK));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\test\\java\\org\\apache\\hadoop\\fs\\http\\client",
  "methodName" : "testSetXAttr",
  "errType" : [ "IOException", "IllegalArgumentException" ],
  "containingMethodsNum" : 24,
  "sourceCodeText" : "void testSetXAttr() throws Exception\n{\r\n    if (!isLocalFS()) {\r\n        FileSystem fs = FileSystem.get(getProxiedFSConf());\r\n        fs.mkdirs(getProxiedFSTestDir());\r\n        Path path = new Path(getProxiedFSTestDir(), \"foo.txt\");\r\n        OutputStream os = fs.create(path);\r\n        os.write(1);\r\n        os.close();\r\n        fs.close();\r\n        final String name1 = \"user.a1\";\r\n        final byte[] value1 = new byte[] { 0x31, 0x32, 0x33 };\r\n        final String name2 = \"user.a2\";\r\n        final byte[] value2 = new byte[] { 0x41, 0x42, 0x43 };\r\n        final String name3 = \"user.a3\";\r\n        final byte[] value3 = null;\r\n        final String name4 = \"trusted.a1\";\r\n        final byte[] value4 = new byte[] { 0x31, 0x32, 0x33 };\r\n        final String name5 = \"a1\";\r\n        fs = getHttpFSFileSystem();\r\n        fs.setXAttr(path, name1, value1);\r\n        fs.setXAttr(path, name2, value2);\r\n        fs.setXAttr(path, name3, value3);\r\n        fs.setXAttr(path, name4, value4);\r\n        try {\r\n            fs.setXAttr(path, name5, value1);\r\n            Assert.fail(\"Set xAttr with incorrect name format should fail.\");\r\n        } catch (IOException e) {\r\n        } catch (IllegalArgumentException e) {\r\n        }\r\n        fs.close();\r\n        fs = FileSystem.get(getProxiedFSConf());\r\n        Map<String, byte[]> xAttrs = fs.getXAttrs(path);\r\n        fs.close();\r\n        assertEquals(4, xAttrs.size());\r\n        assertArrayEquals(value1, xAttrs.get(name1));\r\n        assertArrayEquals(value2, xAttrs.get(name2));\r\n        assertArrayEquals(new byte[0], xAttrs.get(name3));\r\n        assertArrayEquals(value4, xAttrs.get(name4));\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\test\\java\\org\\apache\\hadoop\\fs\\http\\client",
  "methodName" : "testGetXAttrs",
  "errType" : [ "IOException", "IllegalArgumentException" ],
  "containingMethodsNum" : 41,
  "sourceCodeText" : "void testGetXAttrs() throws Exception\n{\r\n    if (!isLocalFS()) {\r\n        FileSystem fs = FileSystem.get(getProxiedFSConf());\r\n        fs.mkdirs(getProxiedFSTestDir());\r\n        Path path = new Path(getProxiedFSTestDir(), \"foo.txt\");\r\n        OutputStream os = fs.create(path);\r\n        os.write(1);\r\n        os.close();\r\n        fs.close();\r\n        final String name1 = \"user.a1\";\r\n        final byte[] value1 = new byte[] { 0x31, 0x32, 0x33 };\r\n        final String name2 = \"user.a2\";\r\n        final byte[] value2 = new byte[] { 0x41, 0x42, 0x43 };\r\n        final String name3 = \"user.a3\";\r\n        final byte[] value3 = null;\r\n        final String name4 = \"trusted.a1\";\r\n        final byte[] value4 = new byte[] { 0x31, 0x32, 0x33 };\r\n        fs = FileSystem.get(getProxiedFSConf());\r\n        fs.setXAttr(path, name1, value1);\r\n        fs.setXAttr(path, name2, value2);\r\n        fs.setXAttr(path, name3, value3);\r\n        fs.setXAttr(path, name4, value4);\r\n        fs.close();\r\n        fs = getHttpFSFileSystem();\r\n        List<String> names = Lists.newArrayList();\r\n        names.add(name1);\r\n        names.add(name2);\r\n        names.add(name3);\r\n        names.add(name4);\r\n        Map<String, byte[]> xAttrs = fs.getXAttrs(path, names);\r\n        fs.close();\r\n        assertEquals(4, xAttrs.size());\r\n        assertArrayEquals(value1, xAttrs.get(name1));\r\n        assertArrayEquals(value2, xAttrs.get(name2));\r\n        assertArrayEquals(new byte[0], xAttrs.get(name3));\r\n        assertArrayEquals(value4, xAttrs.get(name4));\r\n        fs = getHttpFSFileSystem();\r\n        byte[] value = fs.getXAttr(path, name1);\r\n        assertArrayEquals(value1, value);\r\n        final String name5 = \"a1\";\r\n        try {\r\n            value = fs.getXAttr(path, name5);\r\n            Assert.fail(\"Get xAttr with incorrect name format should fail.\");\r\n        } catch (IOException e) {\r\n        } catch (IllegalArgumentException e) {\r\n        }\r\n        fs.close();\r\n        fs = getHttpFSFileSystem();\r\n        xAttrs = fs.getXAttrs(path);\r\n        fs.close();\r\n        assertEquals(4, xAttrs.size());\r\n        assertArrayEquals(value1, xAttrs.get(name1));\r\n        assertArrayEquals(value2, xAttrs.get(name2));\r\n        assertArrayEquals(new byte[0], xAttrs.get(name3));\r\n        assertArrayEquals(value4, xAttrs.get(name4));\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\test\\java\\org\\apache\\hadoop\\fs\\http\\client",
  "methodName" : "testRemoveXAttr",
  "errType" : [ "IOException", "IllegalArgumentException" ],
  "containingMethodsNum" : 25,
  "sourceCodeText" : "void testRemoveXAttr() throws Exception\n{\r\n    if (!isLocalFS()) {\r\n        FileSystem fs = FileSystem.get(getProxiedFSConf());\r\n        fs.mkdirs(getProxiedFSTestDir());\r\n        Path path = new Path(getProxiedFSTestDir(), \"foo.txt\");\r\n        OutputStream os = fs.create(path);\r\n        os.write(1);\r\n        os.close();\r\n        fs.close();\r\n        final String name1 = \"user.a1\";\r\n        final byte[] value1 = new byte[] { 0x31, 0x32, 0x33 };\r\n        final String name2 = \"user.a2\";\r\n        final byte[] value2 = new byte[] { 0x41, 0x42, 0x43 };\r\n        final String name3 = \"user.a3\";\r\n        final byte[] value3 = null;\r\n        final String name4 = \"trusted.a1\";\r\n        final byte[] value4 = new byte[] { 0x31, 0x32, 0x33 };\r\n        final String name5 = \"a1\";\r\n        fs = FileSystem.get(getProxiedFSConf());\r\n        fs.setXAttr(path, name1, value1);\r\n        fs.setXAttr(path, name2, value2);\r\n        fs.setXAttr(path, name3, value3);\r\n        fs.setXAttr(path, name4, value4);\r\n        fs.close();\r\n        fs = getHttpFSFileSystem();\r\n        fs.removeXAttr(path, name1);\r\n        fs.removeXAttr(path, name3);\r\n        fs.removeXAttr(path, name4);\r\n        try {\r\n            fs.removeXAttr(path, name5);\r\n            Assert.fail(\"Remove xAttr with incorrect name format should fail.\");\r\n        } catch (IOException e) {\r\n        } catch (IllegalArgumentException e) {\r\n        }\r\n        fs = FileSystem.get(getProxiedFSConf());\r\n        Map<String, byte[]> xAttrs = fs.getXAttrs(path);\r\n        fs.close();\r\n        assertEquals(1, xAttrs.size());\r\n        assertArrayEquals(value2, xAttrs.get(name2));\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\test\\java\\org\\apache\\hadoop\\fs\\http\\client",
  "methodName" : "testListXAttrs",
  "errType" : null,
  "containingMethodsNum" : 21,
  "sourceCodeText" : "void testListXAttrs() throws Exception\n{\r\n    if (!isLocalFS()) {\r\n        FileSystem fs = FileSystem.get(getProxiedFSConf());\r\n        fs.mkdirs(getProxiedFSTestDir());\r\n        Path path = new Path(getProxiedFSTestDir(), \"foo.txt\");\r\n        OutputStream os = fs.create(path);\r\n        os.write(1);\r\n        os.close();\r\n        fs.close();\r\n        final String name1 = \"user.a1\";\r\n        final byte[] value1 = new byte[] { 0x31, 0x32, 0x33 };\r\n        final String name2 = \"user.a2\";\r\n        final byte[] value2 = new byte[] { 0x41, 0x42, 0x43 };\r\n        final String name3 = \"user.a3\";\r\n        final byte[] value3 = null;\r\n        final String name4 = \"trusted.a1\";\r\n        final byte[] value4 = new byte[] { 0x31, 0x32, 0x33 };\r\n        fs = FileSystem.get(getProxiedFSConf());\r\n        fs.setXAttr(path, name1, value1);\r\n        fs.setXAttr(path, name2, value2);\r\n        fs.setXAttr(path, name3, value3);\r\n        fs.setXAttr(path, name4, value4);\r\n        fs.close();\r\n        fs = getHttpFSFileSystem();\r\n        List<String> names = fs.listXAttrs(path);\r\n        assertEquals(4, names.size());\r\n        assertTrue(names.contains(name1));\r\n        assertTrue(names.contains(name2));\r\n        assertTrue(names.contains(name3));\r\n        assertTrue(names.contains(name4));\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\test\\java\\org\\apache\\hadoop\\fs\\http\\client",
  "methodName" : "assertSameAcls",
  "errType" : null,
  "containingMethodsNum" : 9,
  "sourceCodeText" : "void assertSameAcls(AclStatus a, AclStatus b) throws Exception\n{\r\n    assertEquals(a.getOwner(), b.getOwner());\r\n    assertEquals(a.getGroup(), b.getGroup());\r\n    assertEquals(a.getPermission(), b.getPermission());\r\n    assertEquals(a.isStickyBit(), b.isStickyBit());\r\n    assertEquals(a.getEntries().size(), b.getEntries().size());\r\n    for (AclEntry e : a.getEntries()) {\r\n        assertTrue(b.getEntries().contains(e));\r\n    }\r\n    for (AclEntry e : b.getEntries()) {\r\n        assertTrue(a.getEntries().contains(e));\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\test\\java\\org\\apache\\hadoop\\fs\\http\\client",
  "methodName" : "assertSameAcls",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void assertSameAcls(FileSystem expected, FileSystem actual, Path path) throws IOException\n{\r\n    FileStatus expectedFileStatus = expected.getFileStatus(path);\r\n    FileStatus actualFileStatus = actual.getFileStatus(path);\r\n    assertEquals(actualFileStatus.hasAcl(), expectedFileStatus.hasAcl());\r\n    assertEquals(actualFileStatus.getPermission().getAclBit(), expectedFileStatus.getPermission().getAclBit());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\test\\java\\org\\apache\\hadoop\\fs\\http\\client",
  "methodName" : "testFileAcls",
  "errType" : null,
  "containingMethodsNum" : 31,
  "sourceCodeText" : "void testFileAcls() throws Exception\n{\r\n    if (isLocalFS()) {\r\n        return;\r\n    }\r\n    final String aclUser1 = \"user:foo:rw-\";\r\n    final String rmAclUser1 = \"user:foo:\";\r\n    final String aclUser2 = \"user:bar:r--\";\r\n    final String aclGroup1 = \"group::r--\";\r\n    final String aclSet = \"user::rwx,\" + aclUser1 + \",\" + aclGroup1 + \",other::---\";\r\n    FileSystem proxyFs = FileSystem.get(getProxiedFSConf());\r\n    FileSystem httpfs = getHttpFSFileSystem();\r\n    Path path = new Path(getProxiedFSTestDir(), \"testAclStatus.txt\");\r\n    OutputStream os = proxyFs.create(path);\r\n    os.write(1);\r\n    os.close();\r\n    AclStatus proxyAclStat = proxyFs.getAclStatus(path);\r\n    AclStatus httpfsAclStat = httpfs.getAclStatus(path);\r\n    assertSameAcls(httpfsAclStat, proxyAclStat);\r\n    assertSameAcls(httpfs, proxyFs, path);\r\n    httpfs.setAcl(path, AclEntry.parseAclSpec(aclSet, true));\r\n    proxyAclStat = proxyFs.getAclStatus(path);\r\n    httpfsAclStat = httpfs.getAclStatus(path);\r\n    assertSameAcls(httpfsAclStat, proxyAclStat);\r\n    assertSameAcls(httpfs, proxyFs, path);\r\n    httpfs.modifyAclEntries(path, AclEntry.parseAclSpec(aclUser2, true));\r\n    proxyAclStat = proxyFs.getAclStatus(path);\r\n    httpfsAclStat = httpfs.getAclStatus(path);\r\n    assertSameAcls(httpfsAclStat, proxyAclStat);\r\n    assertSameAcls(httpfs, proxyFs, path);\r\n    httpfs.removeAclEntries(path, AclEntry.parseAclSpec(rmAclUser1, false));\r\n    proxyAclStat = proxyFs.getAclStatus(path);\r\n    httpfsAclStat = httpfs.getAclStatus(path);\r\n    assertSameAcls(httpfsAclStat, proxyAclStat);\r\n    assertSameAcls(httpfs, proxyFs, path);\r\n    httpfs.removeAcl(path);\r\n    proxyAclStat = proxyFs.getAclStatus(path);\r\n    httpfsAclStat = httpfs.getAclStatus(path);\r\n    assertSameAcls(httpfsAclStat, proxyAclStat);\r\n    assertSameAcls(httpfs, proxyFs, path);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\test\\java\\org\\apache\\hadoop\\fs\\http\\client",
  "methodName" : "testDirAcls",
  "errType" : null,
  "containingMethodsNum" : 18,
  "sourceCodeText" : "void testDirAcls() throws Exception\n{\r\n    if (isLocalFS()) {\r\n        return;\r\n    }\r\n    final String defUser1 = \"default:user:glarch:r-x\";\r\n    FileSystem proxyFs = FileSystem.get(getProxiedFSConf());\r\n    FileSystem httpfs = getHttpFSFileSystem();\r\n    Path dir = getProxiedFSTestDir();\r\n    AclStatus proxyAclStat = proxyFs.getAclStatus(dir);\r\n    AclStatus httpfsAclStat = httpfs.getAclStatus(dir);\r\n    assertSameAcls(httpfsAclStat, proxyAclStat);\r\n    assertSameAcls(httpfs, proxyFs, dir);\r\n    httpfs.setAcl(dir, (AclEntry.parseAclSpec(defUser1, true)));\r\n    proxyAclStat = proxyFs.getAclStatus(dir);\r\n    httpfsAclStat = httpfs.getAclStatus(dir);\r\n    assertSameAcls(httpfsAclStat, proxyAclStat);\r\n    assertSameAcls(httpfs, proxyFs, dir);\r\n    httpfs.removeDefaultAcl(dir);\r\n    proxyAclStat = proxyFs.getAclStatus(dir);\r\n    httpfsAclStat = httpfs.getAclStatus(dir);\r\n    assertSameAcls(httpfsAclStat, proxyAclStat);\r\n    assertSameAcls(httpfs, proxyFs, dir);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\test\\java\\org\\apache\\hadoop\\fs\\http\\client",
  "methodName" : "testEncryption",
  "errType" : null,
  "containingMethodsNum" : 11,
  "sourceCodeText" : "void testEncryption() throws Exception\n{\r\n    if (isLocalFS()) {\r\n        return;\r\n    }\r\n    FileSystem proxyFs = FileSystem.get(getProxiedFSConf());\r\n    FileSystem httpFs = getHttpFSFileSystem();\r\n    FileStatus proxyStatus = proxyFs.getFileStatus(TestHdfsHelper.ENCRYPTED_FILE);\r\n    assertTrue(proxyStatus.isEncrypted());\r\n    FileStatus httpStatus = httpFs.getFileStatus(TestHdfsHelper.ENCRYPTED_FILE);\r\n    assertTrue(httpStatus.isEncrypted());\r\n    proxyStatus = proxyFs.getFileStatus(new Path(\"/\"));\r\n    httpStatus = httpFs.getFileStatus(new Path(\"/\"));\r\n    assertFalse(proxyStatus.isEncrypted());\r\n    assertFalse(httpStatus.isEncrypted());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\test\\java\\org\\apache\\hadoop\\fs\\http\\client",
  "methodName" : "testErasureCoding",
  "errType" : null,
  "containingMethodsNum" : 11,
  "sourceCodeText" : "void testErasureCoding() throws Exception\n{\r\n    Assume.assumeFalse(\"Assume its not a local FS!\", isLocalFS());\r\n    FileSystem proxyFs = FileSystem.get(getProxiedFSConf());\r\n    FileSystem httpFS = getHttpFSFileSystem();\r\n    Path filePath = new Path(getProxiedFSTestDir(), \"foo.txt\");\r\n    proxyFs.create(filePath).close();\r\n    ContractTestUtils.assertNotErasureCoded(httpFS, getProxiedFSTestDir());\r\n    ContractTestUtils.assertNotErasureCoded(httpFS, filePath);\r\n    ContractTestUtils.assertErasureCoded(httpFS, TestHdfsHelper.ERASURE_CODING_DIR);\r\n    ContractTestUtils.assertErasureCoded(httpFS, TestHdfsHelper.ERASURE_CODING_FILE);\r\n    proxyFs.close();\r\n    httpFS.close();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\test\\java\\org\\apache\\hadoop\\fs\\http\\client",
  "methodName" : "testStoragePolicy",
  "errType" : null,
  "containingMethodsNum" : 16,
  "sourceCodeText" : "void testStoragePolicy() throws Exception\n{\r\n    Assume.assumeFalse(\"Assume its not a local FS\", isLocalFS());\r\n    FileSystem fs = FileSystem.get(getProxiedFSConf());\r\n    fs.mkdirs(getProxiedFSTestDir());\r\n    Path path = new Path(getProxiedFSTestDir(), \"policy.txt\");\r\n    FileSystem httpfs = getHttpFSFileSystem();\r\n    Assert.assertArrayEquals(\"Policy array returned from the DFS and HttpFS should be equals\", fs.getAllStoragePolicies().toArray(), httpfs.getAllStoragePolicies().toArray());\r\n    DFSTestUtil.createFile(fs, path, 0, (short) 1, 0L);\r\n    BlockStoragePolicySpi defaultdfsPolicy = fs.getStoragePolicy(path);\r\n    httpfs.setStoragePolicy(path, HdfsConstants.COLD_STORAGE_POLICY_NAME);\r\n    BlockStoragePolicySpi dfsPolicy = fs.getStoragePolicy(path);\r\n    BlockStoragePolicySpi httpFsPolicy = httpfs.getStoragePolicy(path);\r\n    Assert.assertEquals(\"Storage policy returned from the get API should\" + \" be same as set policy\", HdfsConstants.COLD_STORAGE_POLICY_NAME.toString(), httpFsPolicy.getName());\r\n    Assert.assertEquals(\"Storage policy returned from the DFS and HttpFS should be equals\", httpFsPolicy, dfsPolicy);\r\n    httpfs.unsetStoragePolicy(path);\r\n    Assert.assertEquals(\"After unset storage policy, the get API shoudld\" + \" return the default policy\", defaultdfsPolicy, httpfs.getStoragePolicy(path));\r\n    fs.close();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\test\\java\\org\\apache\\hadoop\\fs\\http\\client",
  "methodName" : "operation",
  "errType" : null,
  "containingMethodsNum" : 47,
  "sourceCodeText" : "void operation(Operation op) throws Exception\n{\r\n    switch(op) {\r\n        case GET:\r\n            testGet();\r\n            break;\r\n        case OPEN:\r\n            testOpen();\r\n            break;\r\n        case CREATE:\r\n            testCreate();\r\n            break;\r\n        case APPEND:\r\n            testAppend();\r\n            break;\r\n        case TRUNCATE:\r\n            testTruncate();\r\n            break;\r\n        case CONCAT:\r\n            testConcat();\r\n            break;\r\n        case RENAME:\r\n            testRename();\r\n            break;\r\n        case DELETE:\r\n            testDelete();\r\n            break;\r\n        case LIST_STATUS:\r\n            testListStatus();\r\n            testListSymLinkStatus();\r\n            break;\r\n        case WORKING_DIRECTORY:\r\n            testWorkingdirectory();\r\n            break;\r\n        case MKDIRS:\r\n            testMkdirs();\r\n            break;\r\n        case SET_TIMES:\r\n            testSetTimes();\r\n            break;\r\n        case SET_PERMISSION:\r\n            testSetPermission();\r\n            break;\r\n        case SET_OWNER:\r\n            testSetOwner();\r\n            break;\r\n        case SET_REPLICATION:\r\n            testSetReplication();\r\n            break;\r\n        case CHECKSUM:\r\n            testChecksum();\r\n            break;\r\n        case CONTENT_SUMMARY:\r\n            testContentSummary();\r\n            break;\r\n        case QUOTA_USAGE:\r\n            testQuotaUsage();\r\n            break;\r\n        case FILEACLS:\r\n            testFileAclsCustomizedUserAndGroupNames();\r\n            testFileAcls();\r\n            break;\r\n        case DIRACLS:\r\n            testDirAcls();\r\n            break;\r\n        case SET_XATTR:\r\n            testSetXAttr();\r\n            break;\r\n        case REMOVE_XATTR:\r\n            testRemoveXAttr();\r\n            break;\r\n        case GET_XATTRS:\r\n            testGetXAttrs();\r\n            break;\r\n        case LIST_XATTRS:\r\n            testListXAttrs();\r\n            break;\r\n        case ENCRYPTION:\r\n            testEncryption();\r\n            break;\r\n        case LIST_STATUS_BATCH:\r\n            testListStatusBatch();\r\n            break;\r\n        case GETTRASHROOT:\r\n            testTrashRoot();\r\n            break;\r\n        case STORAGEPOLICY:\r\n            testStoragePolicy();\r\n            break;\r\n        case ERASURE_CODING:\r\n            testErasureCoding();\r\n            break;\r\n        case CREATE_SNAPSHOT:\r\n            testCreateSnapshot();\r\n            break;\r\n        case RENAME_SNAPSHOT:\r\n            testRenameSnapshot();\r\n            break;\r\n        case DELETE_SNAPSHOT:\r\n            testDeleteSnapshot();\r\n            break;\r\n        case ALLOW_SNAPSHOT:\r\n            testAllowSnapshot();\r\n            break;\r\n        case DISALLOW_SNAPSHOT:\r\n            testDisallowSnapshot();\r\n            break;\r\n        case DISALLOW_SNAPSHOT_EXCEPTION:\r\n            testDisallowSnapshotException();\r\n            break;\r\n        case FILE_STATUS_ATTR:\r\n            testFileStatusAttr();\r\n            break;\r\n        case GET_SNAPSHOT_DIFF:\r\n            testGetSnapshotDiff();\r\n            testGetSnapshotDiffIllegalParam();\r\n            break;\r\n        case GET_SNAPSHOTTABLE_DIRECTORY_LIST:\r\n            testGetSnapshottableDirListing();\r\n            break;\r\n        case GET_SNAPSHOT_LIST:\r\n            testGetSnapshotListing();\r\n            break;\r\n        case GET_SERVERDEFAULTS:\r\n            testGetServerDefaults();\r\n            break;\r\n        case CHECKACCESS:\r\n            testAccess();\r\n            break;\r\n        case SETECPOLICY:\r\n            testErasureCodingPolicy();\r\n            break;\r\n        case SATISFYSTORAGEPOLICY:\r\n            testStoragePolicySatisfier();\r\n            break;\r\n        case GET_SNAPSHOT_DIFF_LISTING:\r\n            testGetSnapshotDiffListing();\r\n            break;\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\test\\java\\org\\apache\\hadoop\\fs\\http\\client",
  "methodName" : "operations",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "Collection operations()\n{\r\n    Object[][] ops = new Object[Operation.values().length][];\r\n    for (int i = 0; i < Operation.values().length; i++) {\r\n        ops[i] = new Object[] { Operation.values()[i] };\r\n    }\r\n    return Arrays.asList(ops);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\test\\java\\org\\apache\\hadoop\\fs\\http\\client",
  "methodName" : "testOperation",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void testOperation() throws Exception\n{\r\n    createHttpFSServer();\r\n    operation(operation);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\test\\java\\org\\apache\\hadoop\\fs\\http\\client",
  "methodName" : "testOperationDoAs",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void testOperationDoAs() throws Exception\n{\r\n    createHttpFSServer();\r\n    UserGroupInformation ugi = UserGroupInformation.createProxyUser(HadoopUsersConfTestHelper.getHadoopUsers()[0], UserGroupInformation.getCurrentUser());\r\n    ugi.doAs(new PrivilegedExceptionAction<Void>() {\r\n\r\n        @Override\r\n        public Void run() throws Exception {\r\n            operation(operation);\r\n            return null;\r\n        }\r\n    });\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\test\\java\\org\\apache\\hadoop\\fs\\http\\client",
  "methodName" : "testCreateSnapshot",
  "errType" : null,
  "containingMethodsNum" : 11,
  "sourceCodeText" : "void testCreateSnapshot(String snapshotName) throws Exception\n{\r\n    if (!this.isLocalFS()) {\r\n        Path snapshottablePath = new Path(\"/tmp/tmp-snap-test\");\r\n        createSnapshotTestsPreconditions(snapshottablePath);\r\n        FileSystem fs = this.getHttpFSFileSystem();\r\n        if (snapshotName == null) {\r\n            fs.createSnapshot(snapshottablePath);\r\n        } else {\r\n            fs.createSnapshot(snapshottablePath, snapshotName);\r\n        }\r\n        Path snapshotsDir = new Path(\"/tmp/tmp-snap-test/.snapshot\");\r\n        FileStatus[] snapshotItems = fs.listStatus(snapshotsDir);\r\n        assertTrue(\"Should have exactly one snapshot.\", snapshotItems.length == 1);\r\n        String resultingSnapName = snapshotItems[0].getPath().getName();\r\n        if (snapshotName == null) {\r\n            assertTrue(\"Snapshot auto generated name not matching pattern\", Pattern.matches(\"(s)(\\\\d{8})(-)(\\\\d{6})(\\\\.)(\\\\d{3})\", resultingSnapName));\r\n        } else {\r\n            assertTrue(\"Snapshot name is not same as passed name.\", snapshotName.equals(resultingSnapName));\r\n        }\r\n        cleanSnapshotTests(snapshottablePath, resultingSnapName);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\test\\java\\org\\apache\\hadoop\\fs\\http\\client",
  "methodName" : "testCreateSnapshot",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void testCreateSnapshot() throws Exception\n{\r\n    testCreateSnapshot(null);\r\n    testCreateSnapshot(\"snap-with-name\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\test\\java\\org\\apache\\hadoop\\fs\\http\\client",
  "methodName" : "createSnapshotTestsPreconditions",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void createSnapshotTestsPreconditions(Path snapshottablePath, Boolean allowSnapshot) throws Exception\n{\r\n    DistributedFileSystem distributedFs = (DistributedFileSystem) FileSystem.get(snapshottablePath.toUri(), this.getProxiedFSConf());\r\n    distributedFs.mkdirs(snapshottablePath);\r\n    if (allowSnapshot) {\r\n        distributedFs.allowSnapshot(snapshottablePath);\r\n    }\r\n    Path subdirPath = new Path(\"/tmp/tmp-snap-test/subdir\");\r\n    distributedFs.mkdirs(subdirPath);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\test\\java\\org\\apache\\hadoop\\fs\\http\\client",
  "methodName" : "createSnapshotTestsPreconditions",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void createSnapshotTestsPreconditions(Path snapshottablePath) throws Exception\n{\r\n    createSnapshotTestsPreconditions(snapshottablePath, true);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\test\\java\\org\\apache\\hadoop\\fs\\http\\client",
  "methodName" : "cleanSnapshotTests",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void cleanSnapshotTests(Path snapshottablePath, String resultingSnapName) throws Exception\n{\r\n    DistributedFileSystem distributedFs = (DistributedFileSystem) FileSystem.get(snapshottablePath.toUri(), this.getProxiedFSConf());\r\n    distributedFs.deleteSnapshot(snapshottablePath, resultingSnapName);\r\n    distributedFs.delete(snapshottablePath, true);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\test\\java\\org\\apache\\hadoop\\fs\\http\\client",
  "methodName" : "testRenameSnapshot",
  "errType" : null,
  "containingMethodsNum" : 10,
  "sourceCodeText" : "void testRenameSnapshot() throws Exception\n{\r\n    if (!this.isLocalFS()) {\r\n        Path snapshottablePath = new Path(\"/tmp/tmp-snap-test\");\r\n        createSnapshotTestsPreconditions(snapshottablePath);\r\n        FileSystem fs = this.getHttpFSFileSystem();\r\n        fs.createSnapshot(snapshottablePath, \"snap-to-rename\");\r\n        fs.renameSnapshot(snapshottablePath, \"snap-to-rename\", \"snap-new-name\");\r\n        Path snapshotsDir = new Path(\"/tmp/tmp-snap-test/.snapshot\");\r\n        FileStatus[] snapshotItems = fs.listStatus(snapshotsDir);\r\n        assertTrue(\"Should have exactly one snapshot.\", snapshotItems.length == 1);\r\n        String resultingSnapName = snapshotItems[0].getPath().getName();\r\n        assertTrue(\"Snapshot name is not same as passed name.\", \"snap-new-name\".equals(resultingSnapName));\r\n        cleanSnapshotTests(snapshottablePath, resultingSnapName);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\test\\java\\org\\apache\\hadoop\\fs\\http\\client",
  "methodName" : "testDeleteSnapshot",
  "errType" : null,
  "containingMethodsNum" : 10,
  "sourceCodeText" : "void testDeleteSnapshot() throws Exception\n{\r\n    if (!this.isLocalFS()) {\r\n        Path snapshottablePath = new Path(\"/tmp/tmp-snap-test\");\r\n        createSnapshotTestsPreconditions(snapshottablePath);\r\n        FileSystem fs = this.getHttpFSFileSystem();\r\n        fs.createSnapshot(snapshottablePath, \"snap-to-delete\");\r\n        Path snapshotsDir = new Path(\"/tmp/tmp-snap-test/.snapshot\");\r\n        FileStatus[] snapshotItems = fs.listStatus(snapshotsDir);\r\n        assertTrue(\"Should have exactly one snapshot.\", snapshotItems.length == 1);\r\n        fs.deleteSnapshot(snapshottablePath, \"snap-to-delete\");\r\n        snapshotItems = fs.listStatus(snapshotsDir);\r\n        assertTrue(\"There should be no snapshot anymore.\", snapshotItems.length == 0);\r\n        fs.delete(snapshottablePath, true);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\test\\java\\org\\apache\\hadoop\\fs\\http\\client",
  "methodName" : "testAllowSnapshot",
  "errType" : null,
  "containingMethodsNum" : 9,
  "sourceCodeText" : "void testAllowSnapshot() throws Exception\n{\r\n    if (!this.isLocalFS()) {\r\n        Path path = new Path(\"/tmp/tmp-snap-test\");\r\n        createSnapshotTestsPreconditions(path, false);\r\n        FileSystem fs = this.getHttpFSFileSystem();\r\n        assertFalse(\"Snapshot should be disallowed by default\", fs.getFileStatus(path).isSnapshotEnabled());\r\n        if (fs instanceof HttpFSFileSystem) {\r\n            HttpFSFileSystem httpFS = (HttpFSFileSystem) fs;\r\n            httpFS.allowSnapshot(path);\r\n        } else if (fs instanceof WebHdfsFileSystem) {\r\n            WebHdfsFileSystem webHdfsFileSystem = (WebHdfsFileSystem) fs;\r\n            webHdfsFileSystem.allowSnapshot(path);\r\n        } else {\r\n            Assert.fail(fs.getClass().getSimpleName() + \" doesn't support allowSnapshot\");\r\n        }\r\n        assertTrue(\"allowSnapshot failed\", fs.getFileStatus(path).isSnapshotEnabled());\r\n        fs.delete(path, true);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\test\\java\\org\\apache\\hadoop\\fs\\http\\client",
  "methodName" : "testDisallowSnapshot",
  "errType" : null,
  "containingMethodsNum" : 9,
  "sourceCodeText" : "void testDisallowSnapshot() throws Exception\n{\r\n    if (!this.isLocalFS()) {\r\n        Path path = new Path(\"/tmp/tmp-snap-test\");\r\n        createSnapshotTestsPreconditions(path);\r\n        FileSystem fs = this.getHttpFSFileSystem();\r\n        assertTrue(\"Snapshot should be allowed by DFS\", fs.getFileStatus(path).isSnapshotEnabled());\r\n        if (fs instanceof HttpFSFileSystem) {\r\n            HttpFSFileSystem httpFS = (HttpFSFileSystem) fs;\r\n            httpFS.disallowSnapshot(path);\r\n        } else if (fs instanceof WebHdfsFileSystem) {\r\n            WebHdfsFileSystem webHdfsFileSystem = (WebHdfsFileSystem) fs;\r\n            webHdfsFileSystem.disallowSnapshot(path);\r\n        } else {\r\n            Assert.fail(fs.getClass().getSimpleName() + \" doesn't support disallowSnapshot\");\r\n        }\r\n        assertFalse(\"disallowSnapshot failed\", fs.getFileStatus(path).isSnapshotEnabled());\r\n        fs.delete(path, true);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\test\\java\\org\\apache\\hadoop\\fs\\http\\client",
  "methodName" : "testDisallowSnapshotException",
  "errType" : [ "SnapshotException", "SnapshotException" ],
  "containingMethodsNum" : 14,
  "sourceCodeText" : "void testDisallowSnapshotException() throws Exception\n{\r\n    if (!this.isLocalFS()) {\r\n        Path path = new Path(\"/tmp/tmp-snap-test\");\r\n        createSnapshotTestsPreconditions(path);\r\n        FileSystem fs = this.getHttpFSFileSystem();\r\n        assertTrue(\"Snapshot should be allowed by DFS\", fs.getFileStatus(path).isSnapshotEnabled());\r\n        fs.createSnapshot(path, \"snap-01\");\r\n        fs.createSnapshot(path, \"snap-02\");\r\n        boolean disallowSuccess = false;\r\n        if (fs instanceof HttpFSFileSystem) {\r\n            HttpFSFileSystem httpFS = (HttpFSFileSystem) fs;\r\n            try {\r\n                httpFS.disallowSnapshot(path);\r\n                disallowSuccess = true;\r\n            } catch (SnapshotException e) {\r\n            }\r\n        } else if (fs instanceof WebHdfsFileSystem) {\r\n            WebHdfsFileSystem webHdfsFileSystem = (WebHdfsFileSystem) fs;\r\n            try {\r\n                webHdfsFileSystem.disallowSnapshot(path);\r\n                disallowSuccess = true;\r\n            } catch (SnapshotException e) {\r\n            }\r\n        } else {\r\n            Assert.fail(fs.getClass().getSimpleName() + \" doesn't support disallowSnapshot\");\r\n        }\r\n        if (disallowSuccess) {\r\n            Assert.fail(\"disallowSnapshot doesn't throw SnapshotException when \" + \"disallowing snapshot on a directory with at least one snapshot\");\r\n        }\r\n        assertTrue(\"disallowSnapshot should not have succeeded\", fs.getFileStatus(path).isSnapshotEnabled());\r\n        fs.deleteSnapshot(path, \"snap-02\");\r\n        fs.deleteSnapshot(path, \"snap-01\");\r\n        fs.delete(path, true);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 2,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\test\\java\\org\\apache\\hadoop\\fs\\http\\client",
  "methodName" : "testGetSnapshotDiff",
  "errType" : null,
  "containingMethodsNum" : 17,
  "sourceCodeText" : "void testGetSnapshotDiff() throws Exception\n{\r\n    if (!this.isLocalFS()) {\r\n        Path path = new Path(\"/tmp/tmp-snap-test\");\r\n        createSnapshotTestsPreconditions(path);\r\n        FileSystem fs = this.getHttpFSFileSystem();\r\n        Assert.assertTrue(fs.getFileStatus(path).isSnapshotEnabled());\r\n        Path file1 = new Path(path, \"file1\");\r\n        testCreate(file1, false);\r\n        fs.createSnapshot(path, \"snap1\");\r\n        Path file2 = new Path(path, \"file2\");\r\n        testCreate(file2, false);\r\n        fs.createSnapshot(path, \"snap2\");\r\n        try {\r\n            SnapshotDiffReport diffReport = null;\r\n            if (fs instanceof HttpFSFileSystem) {\r\n                HttpFSFileSystem httpFS = (HttpFSFileSystem) fs;\r\n                diffReport = httpFS.getSnapshotDiffReport(path, \"snap1\", \"snap2\");\r\n            } else if (fs instanceof WebHdfsFileSystem) {\r\n                WebHdfsFileSystem webHdfsFileSystem = (WebHdfsFileSystem) fs;\r\n                diffReport = webHdfsFileSystem.getSnapshotDiffReport(path, \"snap1\", \"snap2\");\r\n            } else {\r\n                Assert.fail(fs.getClass().getSimpleName() + \" doesn't support getSnapshotDiff\");\r\n            }\r\n            DistributedFileSystem dfs = (DistributedFileSystem) FileSystem.get(path.toUri(), this.getProxiedFSConf());\r\n            SnapshotDiffReport dfsDiffReport = dfs.getSnapshotDiffReport(path, \"snap1\", \"snap2\");\r\n            Assert.assertEquals(diffReport.toString(), dfsDiffReport.toString());\r\n        } finally {\r\n            fs.deleteSnapshot(path, \"snap2\");\r\n            fs.deleteSnapshot(path, \"snap1\");\r\n            fs.delete(path, true);\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\test\\java\\org\\apache\\hadoop\\fs\\http\\client",
  "methodName" : "testGetSnapshotDiffIllegalParamCase",
  "errType" : [ "SnapshotException|IllegalArgumentException|RemoteException" ],
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void testGetSnapshotDiffIllegalParamCase(FileSystem fs, Path path, String oldsnapshotname, String snapshotname) throws IOException\n{\r\n    try {\r\n        if (fs instanceof HttpFSFileSystem) {\r\n            HttpFSFileSystem httpFS = (HttpFSFileSystem) fs;\r\n            httpFS.getSnapshotDiffReport(path, oldsnapshotname, snapshotname);\r\n        } else if (fs instanceof WebHdfsFileSystem) {\r\n            WebHdfsFileSystem webHdfsFileSystem = (WebHdfsFileSystem) fs;\r\n            webHdfsFileSystem.getSnapshotDiffReport(path, oldsnapshotname, snapshotname);\r\n        } else {\r\n            Assert.fail(fs.getClass().getSimpleName() + \" doesn't support getSnapshotDiff\");\r\n        }\r\n    } catch (SnapshotException | IllegalArgumentException | RemoteException e) {\r\n        if (e instanceof RemoteException) {\r\n            Assert.assertEquals(((RemoteException) e).getClassName().compareTo(java.lang.IllegalArgumentException.class.getName()), 0);\r\n        }\r\n        return;\r\n    }\r\n    Assert.fail(\"getSnapshotDiff illegal param didn't throw Exception\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\test\\java\\org\\apache\\hadoop\\fs\\http\\client",
  "methodName" : "testGetSnapshotDiffIllegalParam",
  "errType" : null,
  "containingMethodsNum" : 10,
  "sourceCodeText" : "void testGetSnapshotDiffIllegalParam() throws Exception\n{\r\n    if (!this.isLocalFS()) {\r\n        Path path = new Path(\"/tmp/tmp-snap-test\");\r\n        createSnapshotTestsPreconditions(path);\r\n        FileSystem fs = this.getHttpFSFileSystem();\r\n        assertTrue(\"Snapshot should be allowed by DFS\", fs.getFileStatus(path).isSnapshotEnabled());\r\n        Assert.assertTrue(fs.getFileStatus(path).isSnapshotEnabled());\r\n        testGetSnapshotDiffIllegalParamCase(fs, path, \"\", \"\");\r\n        testGetSnapshotDiffIllegalParamCase(fs, path, \"snap1\", \"\");\r\n        testGetSnapshotDiffIllegalParamCase(fs, path, \"\", \"snap2\");\r\n        testGetSnapshotDiffIllegalParamCase(fs, path, \"snap1\", \"snap2\");\r\n        fs.delete(path, true);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\test\\java\\org\\apache\\hadoop\\fs\\http\\client",
  "methodName" : "verifyGetSnapshottableDirListing",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void verifyGetSnapshottableDirListing(FileSystem fs, DistributedFileSystem dfs) throws Exception\n{\r\n    SnapshottableDirectoryStatus[] sds = null;\r\n    if (fs instanceof HttpFSFileSystem) {\r\n        HttpFSFileSystem httpFS = (HttpFSFileSystem) fs;\r\n        sds = httpFS.getSnapshottableDirectoryList();\r\n    } else if (fs instanceof WebHdfsFileSystem) {\r\n        WebHdfsFileSystem webHdfsFileSystem = (WebHdfsFileSystem) fs;\r\n        sds = webHdfsFileSystem.getSnapshottableDirectoryList();\r\n    } else {\r\n        Assert.fail(fs.getClass().getSimpleName() + \" doesn't support getSnapshottableDirListing\");\r\n    }\r\n    SnapshottableDirectoryStatus[] dfssds = dfs.getSnapshottableDirListing();\r\n    Assert.assertEquals(JsonUtil.toJsonString(sds), JsonUtil.toJsonString(dfssds));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\test\\java\\org\\apache\\hadoop\\fs\\http\\client",
  "methodName" : "testGetSnapshotListing",
  "errType" : null,
  "containingMethodsNum" : 17,
  "sourceCodeText" : "void testGetSnapshotListing() throws Exception\n{\r\n    if (!this.isLocalFS()) {\r\n        Path path = new Path(\"/tmp/tmp-snap-test\");\r\n        createSnapshotTestsPreconditions(path);\r\n        FileSystem fs = this.getHttpFSFileSystem();\r\n        Assert.assertTrue(fs.getFileStatus(path).isSnapshotEnabled());\r\n        Path file1 = new Path(path, \"file1\");\r\n        testCreate(file1, false);\r\n        fs.createSnapshot(path, \"snap1\");\r\n        Path file2 = new Path(path, \"file2\");\r\n        testCreate(file2, false);\r\n        fs.createSnapshot(path, \"snap2\");\r\n        SnapshotStatus[] snapshotStatus = null;\r\n        if (fs instanceof HttpFSFileSystem) {\r\n            HttpFSFileSystem httpFS = (HttpFSFileSystem) fs;\r\n            snapshotStatus = httpFS.getSnapshotListing(path);\r\n        } else if (fs instanceof WebHdfsFileSystem) {\r\n            WebHdfsFileSystem webHdfsFileSystem = (WebHdfsFileSystem) fs;\r\n            snapshotStatus = webHdfsFileSystem.getSnapshotListing(path);\r\n        } else {\r\n            Assert.fail(fs.getClass().getSimpleName() + \" doesn't support getSnapshotDiff\");\r\n        }\r\n        DistributedFileSystem dfs = (DistributedFileSystem) FileSystem.get(path.toUri(), this.getProxiedFSConf());\r\n        SnapshotStatus[] dfsStatus = dfs.getSnapshotListing(path);\r\n        Assert.assertEquals(JsonUtil.toJsonString(snapshotStatus), JsonUtil.toJsonString(dfsStatus));\r\n        fs.deleteSnapshot(path, \"snap2\");\r\n        fs.deleteSnapshot(path, \"snap1\");\r\n        fs.delete(path, true);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\test\\java\\org\\apache\\hadoop\\fs\\http\\client",
  "methodName" : "testGetSnapshottableDirListing",
  "errType" : null,
  "containingMethodsNum" : 14,
  "sourceCodeText" : "void testGetSnapshottableDirListing() throws Exception\n{\r\n    if (!this.isLocalFS()) {\r\n        FileSystem fs = this.getHttpFSFileSystem();\r\n        Path path1 = new Path(\"/tmp/tmp-snap-dirlist-test-1\");\r\n        DistributedFileSystem dfs = (DistributedFileSystem) FileSystem.get(path1.toUri(), this.getProxiedFSConf());\r\n        verifyGetSnapshottableDirListing(fs, dfs);\r\n        createSnapshotTestsPreconditions(path1);\r\n        Assert.assertTrue(fs.getFileStatus(path1).isSnapshotEnabled());\r\n        verifyGetSnapshottableDirListing(fs, dfs);\r\n        Path path2 = new Path(\"/tmp/tmp-snap-dirlist-test-2\");\r\n        createSnapshotTestsPreconditions(path2);\r\n        Assert.assertTrue(fs.getFileStatus(path2).isSnapshotEnabled());\r\n        verifyGetSnapshottableDirListing(fs, dfs);\r\n        fs.delete(path2, true);\r\n        verifyGetSnapshottableDirListing(fs, dfs);\r\n        fs.delete(path1, true);\r\n        verifyGetSnapshottableDirListing(fs, dfs);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\test\\java\\org\\apache\\hadoop\\fs\\http\\client",
  "methodName" : "testFileAclsCustomizedUserAndGroupNames",
  "errType" : null,
  "containingMethodsNum" : 20,
  "sourceCodeText" : "void testFileAclsCustomizedUserAndGroupNames() throws Exception\n{\r\n    if (isLocalFS()) {\r\n        return;\r\n    }\r\n    MiniDFSCluster miniDFSCluster = ((TestHdfsHelper) hdfsTestHelper).getMiniDFSCluster();\r\n    Configuration conf = miniDFSCluster.getConfiguration(0);\r\n    FileSystem httpfs = getHttpFSFileSystem(conf);\r\n    if (!(httpfs instanceof WebHdfsFileSystem) && !(httpfs instanceof HttpFSFileSystem)) {\r\n        Assert.fail(httpfs.getClass().getSimpleName() + \" doesn't support custom user and group name pattern. \" + \"Only WebHdfsFileSystem and HttpFSFileSystem support it.\");\r\n    }\r\n    final String aclUser = \"user:123:rwx\";\r\n    final String aclGroup = \"group:foo@bar:r--\";\r\n    final String aclSet = \"user::rwx,\" + aclUser + \",group::r--,\" + aclGroup + \",other::r--\";\r\n    final String dir = \"/aclFileTestCustom\";\r\n    FileSystem proxyFs = FileSystem.get(conf);\r\n    proxyFs.mkdirs(new Path(dir));\r\n    Path path = new Path(dir, \"/testACL\");\r\n    OutputStream os = proxyFs.create(path);\r\n    os.write(1);\r\n    os.close();\r\n    httpfs.setAcl(path, AclEntry.parseAclSpec(aclSet, true));\r\n    AclStatus proxyAclStat = proxyFs.getAclStatus(path);\r\n    AclStatus httpfsAclStat = httpfs.getAclStatus(path);\r\n    assertSameAcls(httpfsAclStat, proxyAclStat);\r\n    assertSameAcls(httpfs, proxyFs, path);\r\n    List<String> strEntries = new ArrayList<>();\r\n    for (AclEntry aclEntry : httpfsAclStat.getEntries()) {\r\n        strEntries.add(aclEntry.toStringStable());\r\n    }\r\n    Assert.assertTrue(strEntries.contains(aclUser));\r\n    Assert.assertTrue(strEntries.contains(aclGroup));\r\n    proxyFs.delete(new Path(dir), true);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\test\\java\\org\\apache\\hadoop\\fs\\http\\client",
  "methodName" : "verifyGetServerDefaults",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void verifyGetServerDefaults(FileSystem fs, DistributedFileSystem dfs) throws Exception\n{\r\n    FsServerDefaults sds = null;\r\n    if (fs instanceof HttpFSFileSystem) {\r\n        HttpFSFileSystem httpFS = (HttpFSFileSystem) fs;\r\n        sds = httpFS.getServerDefaults();\r\n    } else if (fs instanceof WebHdfsFileSystem) {\r\n        WebHdfsFileSystem webHdfsFileSystem = (WebHdfsFileSystem) fs;\r\n        sds = webHdfsFileSystem.getServerDefaults();\r\n    } else {\r\n        Assert.fail(fs.getClass().getSimpleName() + \" doesn't support getServerDefaults\");\r\n    }\r\n    FsServerDefaults dfssds = dfs.getServerDefaults();\r\n    Assert.assertEquals(JsonUtil.toJsonString(sds), JsonUtil.toJsonString(dfssds));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\test\\java\\org\\apache\\hadoop\\fs\\http\\client",
  "methodName" : "testGetServerDefaults",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void testGetServerDefaults() throws Exception\n{\r\n    if (!this.isLocalFS()) {\r\n        FileSystem fs = this.getHttpFSFileSystem();\r\n        Path path1 = new Path(\"/\");\r\n        DistributedFileSystem dfs = (DistributedFileSystem) FileSystem.get(path1.toUri(), this.getProxiedFSConf());\r\n        verifyGetServerDefaults(fs, dfs);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\test\\java\\org\\apache\\hadoop\\fs\\http\\client",
  "methodName" : "testAccess",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void testAccess() throws Exception\n{\r\n    if (!this.isLocalFS()) {\r\n        FileSystem fs = this.getHttpFSFileSystem();\r\n        Path path1 = new Path(\"/\");\r\n        DistributedFileSystem dfs = (DistributedFileSystem) FileSystem.get(path1.toUri(), this.getProxiedFSConf());\r\n        verifyAccess(fs, dfs);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\test\\java\\org\\apache\\hadoop\\fs\\http\\client",
  "methodName" : "verifyAccess",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void verifyAccess(FileSystem fs, DistributedFileSystem dfs) throws Exception\n{\r\n    Path p1 = new Path(\"/p1\");\r\n    dfs.mkdirs(p1);\r\n    dfs.setOwner(p1, \"user1\", \"group1\");\r\n    dfs.setPermission(p1, new FsPermission((short) 0444));\r\n    if (fs instanceof HttpFSFileSystem) {\r\n        HttpFSFileSystem httpFS = (HttpFSFileSystem) fs;\r\n        httpFS.access(p1, FsAction.READ);\r\n    } else if (fs instanceof WebHdfsFileSystem) {\r\n        WebHdfsFileSystem webHdfsFileSystem = (WebHdfsFileSystem) fs;\r\n        webHdfsFileSystem.access(p1, FsAction.READ);\r\n    } else {\r\n        Assert.fail(fs.getClass().getSimpleName() + \" doesn't support access\");\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\test\\java\\org\\apache\\hadoop\\fs\\http\\client",
  "methodName" : "testErasureCodingPolicy",
  "errType" : null,
  "containingMethodsNum" : 20,
  "sourceCodeText" : "void testErasureCodingPolicy() throws Exception\n{\r\n    if (!this.isLocalFS()) {\r\n        FileSystem fs = this.getHttpFSFileSystem();\r\n        Path path1 = new Path(\"/\");\r\n        DistributedFileSystem dfs = (DistributedFileSystem) FileSystem.get(path1.toUri(), this.getProxiedFSConf());\r\n        final String dir = \"/xattrTest\";\r\n        Path p1 = new Path(dir);\r\n        final ErasureCodingPolicy ecPolicy = SystemErasureCodingPolicies.getByID(SystemErasureCodingPolicies.RS_3_2_POLICY_ID);\r\n        final String ecPolicyName = ecPolicy.getName();\r\n        dfs.mkdirs(new Path(dir));\r\n        dfs.enableErasureCodingPolicy(ecPolicyName);\r\n        if (fs instanceof HttpFSFileSystem) {\r\n            HttpFSFileSystem httpFS = (HttpFSFileSystem) fs;\r\n            httpFS.setErasureCodingPolicy(p1, ecPolicyName);\r\n            ErasureCodingPolicy ecPolicy1 = httpFS.getErasureCodingPolicy(p1);\r\n            assertEquals(ecPolicy, ecPolicy1);\r\n            httpFS.unsetErasureCodingPolicy(p1);\r\n            ecPolicy1 = httpFS.getErasureCodingPolicy(p1);\r\n            Assert.assertNull(ecPolicy1);\r\n        } else if (fs instanceof WebHdfsFileSystem) {\r\n            WebHdfsFileSystem webHdfsFileSystem = (WebHdfsFileSystem) fs;\r\n            webHdfsFileSystem.setErasureCodingPolicy(p1, ecPolicyName);\r\n            ErasureCodingPolicy ecPolicy1 = webHdfsFileSystem.getErasureCodingPolicy(p1);\r\n            assertEquals(ecPolicy, ecPolicy1);\r\n            webHdfsFileSystem.unsetErasureCodingPolicy(p1);\r\n            ecPolicy1 = dfs.getErasureCodingPolicy(p1);\r\n            Assert.assertNull(ecPolicy1);\r\n        } else {\r\n            Assert.fail(fs.getClass().getSimpleName() + \" doesn't support access\");\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\test\\java\\org\\apache\\hadoop\\fs\\http\\client",
  "methodName" : "testStoragePolicySatisfier",
  "errType" : null,
  "containingMethodsNum" : 16,
  "sourceCodeText" : "void testStoragePolicySatisfier() throws Exception\n{\r\n    final String dir = \"/parent\";\r\n    Path path1 = new Path(dir);\r\n    String file = \"/parent/file\";\r\n    Path filePath = new Path(file);\r\n    if (!this.isLocalFS()) {\r\n        FileSystem fs = this.getHttpFSFileSystem();\r\n        DistributedFileSystem dfs = (DistributedFileSystem) FileSystem.get(path1.toUri(), this.getProxiedFSConf());\r\n        dfs.mkdirs(path1);\r\n        dfs.create(filePath).close();\r\n        dfs.setStoragePolicy(filePath, HdfsConstants.COLD_STORAGE_POLICY_NAME);\r\n        BlockStoragePolicy storagePolicy = (BlockStoragePolicy) dfs.getStoragePolicy(filePath);\r\n        assertEquals(HdfsConstants.COLD_STORAGE_POLICY_NAME, storagePolicy.getName());\r\n        Map<String, byte[]> xAttrs;\r\n        if (fs instanceof HttpFSFileSystem) {\r\n            HttpFSFileSystem httpFS = (HttpFSFileSystem) fs;\r\n            httpFS.satisfyStoragePolicy(path1);\r\n            xAttrs = httpFS.getXAttrs(path1);\r\n            assertTrue(xAttrs.containsKey(HdfsServerConstants.XATTR_SATISFY_STORAGE_POLICY));\r\n        } else if (fs instanceof WebHdfsFileSystem) {\r\n            WebHdfsFileSystem webHdfsFileSystem = (WebHdfsFileSystem) fs;\r\n            webHdfsFileSystem.satisfyStoragePolicy(path1);\r\n            xAttrs = webHdfsFileSystem.getXAttrs(path1);\r\n            assertTrue(xAttrs.containsKey(HdfsServerConstants.XATTR_SATISFY_STORAGE_POLICY));\r\n        } else {\r\n            Assert.fail(fs.getClass().getSimpleName() + \" doesn't support access\");\r\n        }\r\n        dfs.delete(path1, true);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\test\\java\\org\\apache\\hadoop\\fs\\http\\client",
  "methodName" : "testGetSnapshotDiffListing",
  "errType" : null,
  "containingMethodsNum" : 17,
  "sourceCodeText" : "void testGetSnapshotDiffListing() throws Exception\n{\r\n    if (!this.isLocalFS()) {\r\n        Path path = new Path(\"/tmp/tmp-snap-test\");\r\n        createSnapshotTestsPreconditions(path);\r\n        FileSystem fs = this.getHttpFSFileSystem();\r\n        Assert.assertTrue(fs.getFileStatus(path).isSnapshotEnabled());\r\n        Path file1 = new Path(path, \"file1\");\r\n        testCreate(file1, false);\r\n        fs.createSnapshot(path, \"snap1\");\r\n        Path file2 = new Path(path, \"file2\");\r\n        testCreate(file2, false);\r\n        fs.createSnapshot(path, \"snap2\");\r\n        try {\r\n            SnapshotDiffReportListing diffReportListing = null;\r\n            byte[] emptyBytes = new byte[] {};\r\n            if (fs instanceof HttpFSFileSystem) {\r\n                HttpFSFileSystem httpFS = (HttpFSFileSystem) fs;\r\n                diffReportListing = httpFS.getSnapshotDiffReportListing(path, \"snap1\", \"snap2\", emptyBytes, -1);\r\n            } else if (fs instanceof WebHdfsFileSystem) {\r\n                WebHdfsFileSystem webHdfsFileSystem = (WebHdfsFileSystem) fs;\r\n                diffReportListing = webHdfsFileSystem.getSnapshotDiffReportListing(path.toUri().getPath(), \"snap1\", \"snap2\", emptyBytes, -1);\r\n            } else {\r\n                Assert.fail(fs.getClass().getSimpleName() + \" doesn't support getSnapshotDiff\");\r\n            }\r\n            DistributedFileSystem dfs = (DistributedFileSystem) FileSystem.get(path.toUri(), this.getProxiedFSConf());\r\n            SnapshotDiffReportListing dfsDiffReportListing = dfs.getSnapshotDiffReportListing(path, \"snap1\", \"snap2\", DFSUtil.bytes2String(emptyBytes), -1);\r\n            assertHttpFsReportListingWithDfsClient(diffReportListing, dfsDiffReportListing);\r\n        } finally {\r\n            fs.deleteSnapshot(path, \"snap2\");\r\n            fs.deleteSnapshot(path, \"snap1\");\r\n            fs.delete(path, true);\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : true,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\test\\java\\org\\apache\\hadoop\\fs\\http\\client",
  "methodName" : "assertHttpFsReportListingWithDfsClient",
  "errType" : null,
  "containingMethodsNum" : 21,
  "sourceCodeText" : "void assertHttpFsReportListingWithDfsClient(SnapshotDiffReportListing diffReportListing, SnapshotDiffReportListing dfsDiffReportListing)\n{\r\n    Assert.assertEquals(diffReportListing.getCreateList().size(), dfsDiffReportListing.getCreateList().size());\r\n    Assert.assertEquals(diffReportListing.getDeleteList().size(), dfsDiffReportListing.getDeleteList().size());\r\n    Assert.assertEquals(diffReportListing.getModifyList().size(), dfsDiffReportListing.getModifyList().size());\r\n    Assert.assertEquals(diffReportListing.getIsFromEarlier(), dfsDiffReportListing.getIsFromEarlier());\r\n    Assert.assertEquals(diffReportListing.getLastIndex(), dfsDiffReportListing.getLastIndex());\r\n    Assert.assertEquals(DFSUtil.bytes2String(diffReportListing.getLastPath()), DFSUtil.bytes2String(dfsDiffReportListing.getLastPath()));\r\n    int i = 0;\r\n    for (SnapshotDiffReportListing.DiffReportListingEntry entry : diffReportListing.getCreateList()) {\r\n        SnapshotDiffReportListing.DiffReportListingEntry dfsDiffEntry = dfsDiffReportListing.getCreateList().get(i);\r\n        Assert.assertEquals(entry.getDirId(), dfsDiffEntry.getDirId());\r\n        Assert.assertEquals(entry.getFileId(), dfsDiffEntry.getFileId());\r\n        Assert.assertArrayEquals(DFSUtilClient.byteArray2bytes(entry.getSourcePath()), DFSUtilClient.byteArray2bytes(dfsDiffEntry.getSourcePath()));\r\n        i++;\r\n    }\r\n    i = 0;\r\n    for (SnapshotDiffReportListing.DiffReportListingEntry entry : diffReportListing.getDeleteList()) {\r\n        SnapshotDiffReportListing.DiffReportListingEntry dfsDiffEntry = dfsDiffReportListing.getDeleteList().get(i);\r\n        Assert.assertEquals(entry.getDirId(), dfsDiffEntry.getDirId());\r\n        Assert.assertEquals(entry.getFileId(), dfsDiffEntry.getFileId());\r\n        Assert.assertArrayEquals(DFSUtilClient.byteArray2bytes(entry.getSourcePath()), DFSUtilClient.byteArray2bytes(dfsDiffEntry.getSourcePath()));\r\n        i++;\r\n    }\r\n    i = 0;\r\n    for (SnapshotDiffReportListing.DiffReportListingEntry entry : diffReportListing.getModifyList()) {\r\n        SnapshotDiffReportListing.DiffReportListingEntry dfsDiffEntry = dfsDiffReportListing.getModifyList().get(i);\r\n        Assert.assertEquals(entry.getDirId(), dfsDiffEntry.getDirId());\r\n        Assert.assertEquals(entry.getFileId(), dfsDiffEntry.getFileId());\r\n        Assert.assertArrayEquals(DFSUtilClient.byteArray2bytes(entry.getSourcePath()), DFSUtilClient.byteArray2bytes(dfsDiffEntry.getSourcePath()));\r\n        i++;\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\test\\java\\org\\apache\\hadoop\\fs\\http\\client",
  "methodName" : "getFileSystemClass",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Class getFileSystemClass()\n{\r\n    return WebHdfsFileSystem.class;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\test\\java\\org\\apache\\hadoop\\fs\\http\\server",
  "methodName" : "init",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void init(Properties config) throws ServletException\n{\r\n    initTokenManager(config);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\test\\java\\org\\apache\\hadoop\\fs\\http\\server",
  "methodName" : "destroy",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void destroy()\n{\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\test\\java\\org\\apache\\hadoop\\fs\\http\\server",
  "methodName" : "startMiniDFS",
  "errType" : null,
  "containingMethodsNum" : 15,
  "sourceCodeText" : "void startMiniDFS() throws Exception\n{\r\n    File testDirRoot = TestDirHelper.getTestDir();\r\n    if (System.getProperty(\"hadoop.log.dir\") == null) {\r\n        System.setProperty(\"hadoop.log.dir\", new File(testDirRoot, \"hadoop-log\").getAbsolutePath());\r\n    }\r\n    if (System.getProperty(\"test.build.data\") == null) {\r\n        System.setProperty(\"test.build.data\", new File(testDirRoot, \"hadoop-data\").getAbsolutePath());\r\n    }\r\n    Configuration conf = HadoopUsersConfTestHelper.getBaseConf();\r\n    HadoopUsersConfTestHelper.addUserConf(conf);\r\n    conf.set(\"fs.hdfs.impl.disable.cache\", \"true\");\r\n    conf.set(\"dfs.block.access.token.enable\", \"false\");\r\n    conf.set(\"dfs.permissions\", \"true\");\r\n    conf.set(\"hadoop.security.authentication\", \"simple\");\r\n    conf.setBoolean(DFSConfigKeys.DFS_NAMENODE_XATTRS_ENABLED_KEY, false);\r\n    MiniDFSCluster.Builder builder = new MiniDFSCluster.Builder(conf);\r\n    builder.numDataNodes(2);\r\n    miniDfs = builder.build();\r\n    nnConf = miniDfs.getConfiguration(0);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\test\\java\\org\\apache\\hadoop\\fs\\http\\server",
  "methodName" : "createHttpFSServer",
  "errType" : null,
  "containingMethodsNum" : 25,
  "sourceCodeText" : "void createHttpFSServer() throws Exception\n{\r\n    File homeDir = TestDirHelper.getTestDir();\r\n    Assert.assertTrue(new File(homeDir, \"conf\").mkdir());\r\n    Assert.assertTrue(new File(homeDir, \"log\").mkdir());\r\n    Assert.assertTrue(new File(homeDir, \"temp\").mkdir());\r\n    HttpFSServerWebApp.setHomeDirForCurrentThread(homeDir.getAbsolutePath());\r\n    File secretFile = new File(new File(homeDir, \"conf\"), \"secret\");\r\n    Writer w = new FileWriter(secretFile);\r\n    w.write(\"secret\");\r\n    w.close();\r\n    File hadoopConfDir = new File(new File(homeDir, \"conf\"), \"hadoop-conf\");\r\n    if (!hadoopConfDir.mkdirs()) {\r\n        throw new IOException();\r\n    }\r\n    String fsDefaultName = nnConf.get(CommonConfigurationKeysPublic.FS_DEFAULT_NAME_KEY);\r\n    Configuration conf = new Configuration(false);\r\n    conf.set(CommonConfigurationKeysPublic.FS_DEFAULT_NAME_KEY, fsDefaultName);\r\n    conf.setBoolean(DFSConfigKeys.DFS_NAMENODE_XATTRS_ENABLED_KEY, false);\r\n    File hdfsSite = new File(hadoopConfDir, \"hdfs-site.xml\");\r\n    OutputStream os = new FileOutputStream(hdfsSite);\r\n    conf.writeXml(os);\r\n    os.close();\r\n    conf = new Configuration(false);\r\n    conf.set(\"httpfs.hadoop.config.dir\", hadoopConfDir.toString());\r\n    conf.set(\"httpfs.proxyuser.\" + HadoopUsersConfTestHelper.getHadoopProxyUser() + \".groups\", HadoopUsersConfTestHelper.getHadoopProxyUserGroups());\r\n    conf.set(\"httpfs.proxyuser.\" + HadoopUsersConfTestHelper.getHadoopProxyUser() + \".hosts\", HadoopUsersConfTestHelper.getHadoopProxyUserHosts());\r\n    conf.set(HttpFSAuthenticationFilter.HADOOP_HTTP_CONF_PREFIX + AuthenticationFilter.SIGNATURE_SECRET_FILE, secretFile.getAbsolutePath());\r\n    File httpfsSite = new File(new File(homeDir, \"conf\"), \"httpfs-site.xml\");\r\n    os = new FileOutputStream(httpfsSite);\r\n    conf.writeXml(os);\r\n    os.close();\r\n    ClassLoader cl = Thread.currentThread().getContextClassLoader();\r\n    URL url = cl.getResource(\"webapp\");\r\n    if (url == null) {\r\n        throw new IOException();\r\n    }\r\n    WebAppContext context = new WebAppContext(url.getPath(), \"/webhdfs\");\r\n    Server server = TestJettyHelper.getJettyServer();\r\n    server.setHandler(context);\r\n    server.start();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\test\\java\\org\\apache\\hadoop\\fs\\http\\server",
  "methodName" : "getStatus",
  "errType" : null,
  "containingMethodsNum" : 14,
  "sourceCodeText" : "void getStatus(String filename, String command) throws Exception\n{\r\n    String user = HadoopUsersConfTestHelper.getHadoopUsers()[0];\r\n    if (filename.charAt(0) == '/') {\r\n        filename = filename.substring(1);\r\n    }\r\n    String pathOps = MessageFormat.format(\"/webhdfs/v1/{0}?user.name={1}&op={2}\", filename, user, command);\r\n    URL url = new URL(TestJettyHelper.getJettyURL(), pathOps);\r\n    HttpURLConnection conn = (HttpURLConnection) url.openConnection();\r\n    conn.connect();\r\n    int resp = conn.getResponseCode();\r\n    BufferedReader reader;\r\n    Assert.assertEquals(HttpURLConnection.HTTP_INTERNAL_ERROR, resp);\r\n    reader = new BufferedReader(new InputStreamReader(conn.getErrorStream()));\r\n    String res = reader.readLine();\r\n    Assert.assertTrue(res.contains(\"RemoteException\"));\r\n    Assert.assertTrue(res.contains(\"XAttr\"));\r\n    Assert.assertTrue(res.contains(\"rejected\"));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\test\\java\\org\\apache\\hadoop\\fs\\http\\server",
  "methodName" : "putCmd",
  "errType" : null,
  "containingMethodsNum" : 15,
  "sourceCodeText" : "void putCmd(String filename, String command, String params) throws Exception\n{\r\n    String user = HadoopUsersConfTestHelper.getHadoopUsers()[0];\r\n    if (filename.charAt(0) == '/') {\r\n        filename = filename.substring(1);\r\n    }\r\n    String pathOps = MessageFormat.format(\"/webhdfs/v1/{0}?user.name={1}{2}{3}&op={4}\", filename, user, (params == null) ? \"\" : \"&\", (params == null) ? \"\" : params, command);\r\n    URL url = new URL(TestJettyHelper.getJettyURL(), pathOps);\r\n    HttpURLConnection conn = (HttpURLConnection) url.openConnection();\r\n    conn.setRequestMethod(\"PUT\");\r\n    conn.connect();\r\n    int resp = conn.getResponseCode();\r\n    Assert.assertEquals(HttpURLConnection.HTTP_INTERNAL_ERROR, resp);\r\n    BufferedReader reader;\r\n    reader = new BufferedReader(new InputStreamReader(conn.getErrorStream()));\r\n    String err = reader.readLine();\r\n    Assert.assertTrue(err.contains(\"RemoteException\"));\r\n    Assert.assertTrue(err.contains(\"XAttr\"));\r\n    Assert.assertTrue(err.contains(\"rejected\"));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\test\\java\\org\\apache\\hadoop\\fs\\http\\server",
  "methodName" : "testWithXAttrs",
  "errType" : null,
  "containingMethodsNum" : 10,
  "sourceCodeText" : "void testWithXAttrs() throws Exception\n{\r\n    final String name1 = \"user.a1\";\r\n    final byte[] value1 = new byte[] { 0x31, 0x32, 0x33 };\r\n    final String dir = \"/noXAttr\";\r\n    final String path = dir + \"/file\";\r\n    startMiniDFS();\r\n    createHttpFSServer();\r\n    FileSystem fs = FileSystem.get(nnConf);\r\n    fs.mkdirs(new Path(dir));\r\n    OutputStream os = fs.create(new Path(path));\r\n    os.write(1);\r\n    os.close();\r\n    getStatus(path, \"GETXATTRS\");\r\n    putCmd(path, \"SETXATTR\", TestHttpFSServer.setXAttrParam(name1, value1));\r\n    putCmd(path, \"REMOVEXATTR\", \"xattr.name=\" + name1);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\test\\java\\org\\apache\\hadoop\\fs\\http\\client",
  "methodName" : "getFileSystemClass",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "Class getFileSystemClass()\n{\r\n    return HttpFSFileSystem.class;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\test\\java\\org\\apache\\hadoop\\fs\\http\\client",
  "methodName" : "getProxiedFSTestDir",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Path getProxiedFSTestDir()\n{\r\n    return TestHdfsHelper.getHdfsTestDir();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\test\\java\\org\\apache\\hadoop\\fs\\http\\client",
  "methodName" : "getProxiedFSURI",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String getProxiedFSURI()\n{\r\n    return TestHdfsHelper.getHdfsConf().get(CommonConfigurationKeysPublic.FS_DEFAULT_NAME_KEY);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\test\\java\\org\\apache\\hadoop\\fs\\http\\client",
  "methodName" : "getProxiedFSConf",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Configuration getProxiedFSConf()\n{\r\n    return TestHdfsHelper.getHdfsConf();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\test\\java\\org\\apache\\hadoop\\fs\\http\\client",
  "methodName" : "getProxiedFSTestDir",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Path getProxiedFSTestDir()\n{\r\n    return addPrefix(new Path(TestDirHelper.getTestDir().getAbsolutePath()));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\test\\java\\org\\apache\\hadoop\\fs\\http\\client",
  "methodName" : "getProxiedFSURI",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "String getProxiedFSURI()\n{\r\n    return \"file:///\";\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\test\\java\\org\\apache\\hadoop\\fs\\http\\client",
  "methodName" : "getProxiedFSConf",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Configuration getProxiedFSConf()\n{\r\n    Configuration conf = new Configuration(false);\r\n    conf.set(CommonConfigurationKeysPublic.FS_DEFAULT_NAME_KEY, getProxiedFSURI());\r\n    return conf;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\test\\java\\org\\apache\\hadoop\\fs\\http\\client",
  "methodName" : "addPrefix",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "Path addPrefix(Path path)\n{\r\n    return Path.mergePaths(new Path(PATH_PREFIX), path);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\test\\java\\org\\apache\\hadoop\\fs\\http\\client",
  "methodName" : "testSetPermission",
  "errType" : null,
  "containingMethodsNum" : 12,
  "sourceCodeText" : "void testSetPermission() throws Exception\n{\r\n    if (Path.WINDOWS) {\r\n        FileSystem fs = FileSystem.get(getProxiedFSConf());\r\n        Path path = new Path(getProxiedFSTestDir(), \"foodir\");\r\n        fs.mkdirs(path);\r\n        fs = getHttpFSFileSystem();\r\n        FsPermission permission1 = new FsPermission(FsAction.READ_WRITE, FsAction.NONE, FsAction.NONE);\r\n        fs.setPermission(path, permission1);\r\n        fs.close();\r\n        fs = FileSystem.get(getProxiedFSConf());\r\n        FileStatus status1 = fs.getFileStatus(path);\r\n        fs.close();\r\n        FsPermission permission2 = status1.getPermission();\r\n        Assert.assertEquals(permission2, permission1);\r\n    } else {\r\n        super.testSetPermission();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\test\\java\\org\\apache\\hadoop\\test",
  "methodName" : "setWaitForRatio",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void setWaitForRatio(float ratio)\n{\r\n    waitForRatio = ratio;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\test\\java\\org\\apache\\hadoop\\test",
  "methodName" : "getWaitForRatio",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "float getWaitForRatio()\n{\r\n    return waitForRatio;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\test\\java\\org\\apache\\hadoop\\test",
  "methodName" : "sleep",
  "errType" : [ "InterruptedException" ],
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void sleep(long time)\n{\r\n    try {\r\n        Thread.sleep((long) (getWaitForRatio() * time));\r\n    } catch (InterruptedException ex) {\r\n        System.err.println(MessageFormat.format(\"Sleep interrupted, {0}\", ex.toString()));\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\test\\java\\org\\apache\\hadoop\\test",
  "methodName" : "waitFor",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "long waitFor(int timeout, Predicate predicate)\n{\r\n    return waitFor(timeout, false, predicate);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\test\\java\\org\\apache\\hadoop\\test",
  "methodName" : "waitFor",
  "errType" : [ "Exception" ],
  "containingMethodsNum" : 15,
  "sourceCodeText" : "long waitFor(int timeout, boolean failIfTimeout, Predicate predicate)\n{\r\n    long started = Time.now();\r\n    long mustEnd = Time.now() + (long) (getWaitForRatio() * timeout);\r\n    long lastEcho = 0;\r\n    try {\r\n        long waiting = mustEnd - Time.now();\r\n        System.out.println(MessageFormat.format(\"Waiting up to [{0}] msec\", waiting));\r\n        boolean eval;\r\n        while (!(eval = predicate.evaluate()) && Time.now() < mustEnd) {\r\n            if ((Time.now() - lastEcho) > 5000) {\r\n                waiting = mustEnd - Time.now();\r\n                System.out.println(MessageFormat.format(\"Waiting up to [{0}] msec\", waiting));\r\n                lastEcho = Time.now();\r\n            }\r\n            Thread.sleep(100);\r\n        }\r\n        if (!eval) {\r\n            if (failIfTimeout) {\r\n                fail(MessageFormat.format(\"Waiting timed out after [{0}] msec\", timeout));\r\n            } else {\r\n                System.out.println(MessageFormat.format(\"Waiting timed out after [{0}] msec\", timeout));\r\n            }\r\n        }\r\n        return (eval) ? Time.now() - started : -1;\r\n    } catch (Exception ex) {\r\n        throw new RuntimeException(ex);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\test\\java\\org\\apache\\hadoop\\lib\\servlet",
  "methodName" : "getHomeDirNotDef",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void getHomeDirNotDef()\n{\r\n    ServerWebApp.getHomeDir(\"TestServerWebApp00\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\test\\java\\org\\apache\\hadoop\\lib\\servlet",
  "methodName" : "getHomeDir",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void getHomeDir()\n{\r\n    System.setProperty(\"TestServerWebApp0.home.dir\", \"/tmp\");\r\n    assertEquals(ServerWebApp.getHomeDir(\"TestServerWebApp0\"), \"/tmp\");\r\n    assertEquals(ServerWebApp.getDir(\"TestServerWebApp0\", \".log.dir\", \"/tmp/log\"), \"/tmp/log\");\r\n    System.setProperty(\"TestServerWebApp0.log.dir\", \"/tmplog\");\r\n    assertEquals(ServerWebApp.getDir(\"TestServerWebApp0\", \".log.dir\", \"/tmp/log\"), \"/tmplog\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\test\\java\\org\\apache\\hadoop\\lib\\servlet",
  "methodName" : "lifecycle",
  "errType" : null,
  "containingMethodsNum" : 10,
  "sourceCodeText" : "void lifecycle() throws Exception\n{\r\n    String dir = TestDirHelper.getTestDir().getAbsolutePath();\r\n    System.setProperty(\"TestServerWebApp1.home.dir\", dir);\r\n    System.setProperty(\"TestServerWebApp1.config.dir\", dir);\r\n    System.setProperty(\"TestServerWebApp1.log.dir\", dir);\r\n    System.setProperty(\"TestServerWebApp1.temp.dir\", dir);\r\n    ServerWebApp server = new ServerWebApp(\"TestServerWebApp1\") {\r\n    };\r\n    assertEquals(server.getStatus(), Server.Status.UNDEF);\r\n    server.contextInitialized(null);\r\n    assertEquals(server.getStatus(), Server.Status.NORMAL);\r\n    server.contextDestroyed(null);\r\n    assertEquals(server.getStatus(), Server.Status.SHUTDOWN);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\test\\java\\org\\apache\\hadoop\\lib\\servlet",
  "methodName" : "failedInit",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void failedInit() throws Exception\n{\r\n    String dir = TestDirHelper.getTestDir().getAbsolutePath();\r\n    System.setProperty(\"TestServerWebApp2.home.dir\", dir);\r\n    System.setProperty(\"TestServerWebApp2.config.dir\", dir);\r\n    System.setProperty(\"TestServerWebApp2.log.dir\", dir);\r\n    System.setProperty(\"TestServerWebApp2.temp.dir\", dir);\r\n    System.setProperty(\"testserverwebapp2.services\", \"FOO\");\r\n    ServerWebApp server = new ServerWebApp(\"TestServerWebApp2\") {\r\n    };\r\n    server.contextInitialized(null);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\test\\java\\org\\apache\\hadoop\\lib\\servlet",
  "methodName" : "testResolveAuthority",
  "errType" : null,
  "containingMethodsNum" : 10,
  "sourceCodeText" : "void testResolveAuthority() throws Exception\n{\r\n    String dir = TestDirHelper.getTestDir().getAbsolutePath();\r\n    System.setProperty(\"TestServerWebApp3.home.dir\", dir);\r\n    System.setProperty(\"TestServerWebApp3.config.dir\", dir);\r\n    System.setProperty(\"TestServerWebApp3.log.dir\", dir);\r\n    System.setProperty(\"TestServerWebApp3.temp.dir\", dir);\r\n    System.setProperty(\"testserverwebapp3.http.hostname\", \"localhost\");\r\n    System.setProperty(\"testserverwebapp3.http.port\", \"14000\");\r\n    ServerWebApp server = new ServerWebApp(\"TestServerWebApp3\") {\r\n    };\r\n    InetSocketAddress address = server.resolveAuthority();\r\n    Assert.assertEquals(\"localhost\", address.getHostName());\r\n    Assert.assertEquals(14000, address.getPort());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : true
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\test\\java\\org\\apache\\hadoop\\lib\\service\\scheduler",
  "methodName" : "service",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void service() throws Exception\n{\r\n    String dir = TestDirHelper.getTestDir().getAbsolutePath();\r\n    Configuration conf = new Configuration(false);\r\n    conf.set(\"server.services\", StringUtils.join(\",\", Arrays.asList(InstrumentationService.class.getName(), SchedulerService.class.getName())));\r\n    Server server = new Server(\"server\", dir, dir, dir, dir, conf);\r\n    server.init();\r\n    assertNotNull(server.get(Scheduler.class));\r\n    server.destroy();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\test\\java\\org\\apache\\hadoop\\lib\\service\\hadoop",
  "methodName" : "createHadoopConf",
  "errType" : null,
  "containingMethodsNum" : 3,
  "sourceCodeText" : "void createHadoopConf(Configuration hadoopConf) throws Exception\n{\r\n    String dir = TestDirHelper.getTestDir().getAbsolutePath();\r\n    File hdfsSite = new File(dir, \"hdfs-site.xml\");\r\n    OutputStream os = new FileOutputStream(hdfsSite);\r\n    hadoopConf.writeXml(os);\r\n    os.close();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\test\\java\\org\\apache\\hadoop\\lib\\service\\hadoop",
  "methodName" : "createHadoopConf",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void createHadoopConf() throws Exception\n{\r\n    Configuration hadoopConf = new Configuration(false);\r\n    hadoopConf.set(\"foo\", \"FOO\");\r\n    createHadoopConf(hadoopConf);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\test\\java\\org\\apache\\hadoop\\lib\\service\\hadoop",
  "methodName" : "simpleSecurity",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void simpleSecurity() throws Exception\n{\r\n    String dir = TestDirHelper.getTestDir().getAbsolutePath();\r\n    String services = StringUtils.join(\",\", Arrays.asList(InstrumentationService.class.getName(), SchedulerService.class.getName(), FileSystemAccessService.class.getName()));\r\n    Configuration conf = new Configuration(false);\r\n    conf.set(\"server.services\", services);\r\n    Server server = new Server(\"server\", dir, dir, dir, dir, conf);\r\n    server.init();\r\n    Assert.assertNotNull(server.get(FileSystemAccess.class));\r\n    server.destroy();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\test\\java\\org\\apache\\hadoop\\lib\\service\\hadoop",
  "methodName" : "noKerberosKeytabProperty",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void noKerberosKeytabProperty() throws Exception\n{\r\n    String dir = TestDirHelper.getTestDir().getAbsolutePath();\r\n    String services = StringUtils.join(\",\", Arrays.asList(InstrumentationService.class.getName(), SchedulerService.class.getName(), FileSystemAccessService.class.getName()));\r\n    Configuration conf = new Configuration(false);\r\n    conf.set(\"server.services\", services);\r\n    conf.set(\"server.hadoop.authentication.type\", \"kerberos\");\r\n    conf.set(\"server.hadoop.authentication.kerberos.keytab\", \" \");\r\n    Server server = new Server(\"server\", dir, dir, dir, dir, conf);\r\n    server.init();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\test\\java\\org\\apache\\hadoop\\lib\\service\\hadoop",
  "methodName" : "noKerberosPrincipalProperty",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void noKerberosPrincipalProperty() throws Exception\n{\r\n    String dir = TestDirHelper.getTestDir().getAbsolutePath();\r\n    String services = StringUtils.join(\",\", Arrays.asList(InstrumentationService.class.getName(), SchedulerService.class.getName(), FileSystemAccessService.class.getName()));\r\n    Configuration conf = new Configuration(false);\r\n    conf.set(\"server.services\", services);\r\n    conf.set(\"server.hadoop.authentication.type\", \"kerberos\");\r\n    conf.set(\"server.hadoop.authentication.kerberos.keytab\", \"/tmp/foo\");\r\n    conf.set(\"server.hadoop.authentication.kerberos.principal\", \" \");\r\n    Server server = new Server(\"server\", dir, dir, dir, dir, conf);\r\n    server.init();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\test\\java\\org\\apache\\hadoop\\lib\\service\\hadoop",
  "methodName" : "kerberosInitializationFailure",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void kerberosInitializationFailure() throws Exception\n{\r\n    String dir = TestDirHelper.getTestDir().getAbsolutePath();\r\n    String services = StringUtils.join(\",\", Arrays.asList(InstrumentationService.class.getName(), SchedulerService.class.getName(), FileSystemAccessService.class.getName()));\r\n    Configuration conf = new Configuration(false);\r\n    conf.set(\"server.services\", services);\r\n    conf.set(\"server.hadoop.authentication.type\", \"kerberos\");\r\n    conf.set(\"server.hadoop.authentication.kerberos.keytab\", \"/tmp/foo\");\r\n    conf.set(\"server.hadoop.authentication.kerberos.principal\", \"foo@FOO\");\r\n    Server server = new Server(\"server\", dir, dir, dir, dir, conf);\r\n    server.init();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\test\\java\\org\\apache\\hadoop\\lib\\service\\hadoop",
  "methodName" : "invalidSecurity",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void invalidSecurity() throws Exception\n{\r\n    String dir = TestDirHelper.getTestDir().getAbsolutePath();\r\n    String services = StringUtils.join(\",\", Arrays.asList(InstrumentationService.class.getName(), SchedulerService.class.getName(), FileSystemAccessService.class.getName()));\r\n    Configuration conf = new Configuration(false);\r\n    conf.set(\"server.services\", services);\r\n    conf.set(\"server.hadoop.authentication.type\", \"foo\");\r\n    Server server = new Server(\"server\", dir, dir, dir, dir, conf);\r\n    server.init();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\test\\java\\org\\apache\\hadoop\\lib\\service\\hadoop",
  "methodName" : "serviceHadoopConf",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void serviceHadoopConf() throws Exception\n{\r\n    String dir = TestDirHelper.getTestDir().getAbsolutePath();\r\n    String services = StringUtils.join(\",\", Arrays.asList(InstrumentationService.class.getName(), SchedulerService.class.getName(), FileSystemAccessService.class.getName()));\r\n    Configuration conf = new Configuration(false);\r\n    conf.set(\"server.services\", services);\r\n    Server server = new Server(\"server\", dir, dir, dir, dir, conf);\r\n    server.init();\r\n    FileSystemAccessService fsAccess = (FileSystemAccessService) server.get(FileSystemAccess.class);\r\n    Assert.assertEquals(fsAccess.serviceHadoopConf.get(\"foo\"), \"FOO\");\r\n    server.destroy();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\test\\java\\org\\apache\\hadoop\\lib\\service\\hadoop",
  "methodName" : "serviceHadoopConfCustomDir",
  "errType" : null,
  "containingMethodsNum" : 13,
  "sourceCodeText" : "void serviceHadoopConfCustomDir() throws Exception\n{\r\n    String dir = TestDirHelper.getTestDir().getAbsolutePath();\r\n    String hadoopConfDir = new File(dir, \"confx\").getAbsolutePath();\r\n    new File(hadoopConfDir).mkdirs();\r\n    String services = StringUtils.join(\",\", Arrays.asList(InstrumentationService.class.getName(), SchedulerService.class.getName(), FileSystemAccessService.class.getName()));\r\n    Configuration conf = new Configuration(false);\r\n    conf.set(\"server.services\", services);\r\n    conf.set(\"server.hadoop.config.dir\", hadoopConfDir);\r\n    File hdfsSite = new File(hadoopConfDir, \"hdfs-site.xml\");\r\n    OutputStream os = new FileOutputStream(hdfsSite);\r\n    Configuration hadoopConf = new Configuration(false);\r\n    hadoopConf.set(\"foo\", \"BAR\");\r\n    hadoopConf.writeXml(os);\r\n    os.close();\r\n    Server server = new Server(\"server\", dir, dir, dir, dir, conf);\r\n    server.init();\r\n    FileSystemAccessService fsAccess = (FileSystemAccessService) server.get(FileSystemAccess.class);\r\n    Assert.assertEquals(fsAccess.serviceHadoopConf.get(\"foo\"), \"BAR\");\r\n    server.destroy();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\test\\java\\org\\apache\\hadoop\\lib\\service\\hadoop",
  "methodName" : "inWhitelists",
  "errType" : null,
  "containingMethodsNum" : 19,
  "sourceCodeText" : "void inWhitelists() throws Exception\n{\r\n    String dir = TestDirHelper.getTestDir().getAbsolutePath();\r\n    String services = StringUtils.join(\",\", Arrays.asList(InstrumentationService.class.getName(), SchedulerService.class.getName(), FileSystemAccessService.class.getName()));\r\n    Configuration conf = new Configuration(false);\r\n    conf.set(\"server.services\", services);\r\n    Server server = new Server(\"server\", dir, dir, dir, dir, conf);\r\n    server.init();\r\n    FileSystemAccessService fsAccess = (FileSystemAccessService) server.get(FileSystemAccess.class);\r\n    fsAccess.validateNamenode(\"NN\");\r\n    server.destroy();\r\n    conf = new Configuration(false);\r\n    conf.set(\"server.services\", services);\r\n    conf.set(\"server.hadoop.name.node.whitelist\", \"*\");\r\n    server = new Server(\"server\", dir, dir, dir, dir, conf);\r\n    server.init();\r\n    fsAccess = (FileSystemAccessService) server.get(FileSystemAccess.class);\r\n    fsAccess.validateNamenode(\"NN\");\r\n    server.destroy();\r\n    conf = new Configuration(false);\r\n    conf.set(\"server.services\", services);\r\n    conf.set(\"server.hadoop.name.node.whitelist\", \"NN\");\r\n    server = new Server(\"server\", dir, dir, dir, dir, conf);\r\n    server.init();\r\n    fsAccess = (FileSystemAccessService) server.get(FileSystemAccess.class);\r\n    fsAccess.validateNamenode(\"NN\");\r\n    server.destroy();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\test\\java\\org\\apache\\hadoop\\lib\\service\\hadoop",
  "methodName" : "NameNodeNotinWhitelists",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void NameNodeNotinWhitelists() throws Exception\n{\r\n    String dir = TestDirHelper.getTestDir().getAbsolutePath();\r\n    String services = StringUtils.join(\",\", Arrays.asList(InstrumentationService.class.getName(), SchedulerService.class.getName(), FileSystemAccessService.class.getName()));\r\n    Configuration conf = new Configuration(false);\r\n    conf.set(\"server.services\", services);\r\n    conf.set(\"server.hadoop.name.node.whitelist\", \"NN\");\r\n    Server server = new Server(\"server\", dir, dir, dir, dir, conf);\r\n    server.init();\r\n    FileSystemAccessService fsAccess = (FileSystemAccessService) server.get(FileSystemAccess.class);\r\n    fsAccess.validateNamenode(\"NNx\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\test\\java\\org\\apache\\hadoop\\lib\\service\\hadoop",
  "methodName" : "createFileSystem",
  "errType" : [ "IOException", "Exception" ],
  "containingMethodsNum" : 16,
  "sourceCodeText" : "void createFileSystem() throws Exception\n{\r\n    String dir = TestDirHelper.getTestDir().getAbsolutePath();\r\n    String services = StringUtils.join(\",\", Arrays.asList(InstrumentationService.class.getName(), SchedulerService.class.getName(), FileSystemAccessService.class.getName()));\r\n    Configuration hadoopConf = new Configuration(false);\r\n    hadoopConf.set(CommonConfigurationKeysPublic.FS_DEFAULT_NAME_KEY, TestHdfsHelper.getHdfsConf().get(CommonConfigurationKeysPublic.FS_DEFAULT_NAME_KEY));\r\n    createHadoopConf(hadoopConf);\r\n    Configuration conf = new Configuration(false);\r\n    conf.set(\"server.services\", services);\r\n    conf.set(\"server.hadoop.filesystem.cache.purge.timeout\", \"0\");\r\n    Server server = new Server(\"server\", dir, dir, dir, dir, conf);\r\n    server.init();\r\n    FileSystemAccess hadoop = server.get(FileSystemAccess.class);\r\n    FileSystem fs = hadoop.createFileSystem(\"u\", hadoop.getFileSystemConfiguration());\r\n    Assert.assertNotNull(fs);\r\n    fs.mkdirs(new Path(\"/tmp/foo\"));\r\n    hadoop.releaseFileSystem(fs);\r\n    try {\r\n        fs.mkdirs(new Path(\"/tmp/foo\"));\r\n        Assert.fail();\r\n    } catch (IOException ex) {\r\n    } catch (Exception ex) {\r\n        Assert.fail();\r\n    }\r\n    server.destroy();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\test\\java\\org\\apache\\hadoop\\lib\\service\\hadoop",
  "methodName" : "fileSystemExecutor",
  "errType" : [ "IOException", "Exception" ],
  "containingMethodsNum" : 13,
  "sourceCodeText" : "void fileSystemExecutor() throws Exception\n{\r\n    String dir = TestDirHelper.getTestDir().getAbsolutePath();\r\n    String services = StringUtils.join(\",\", Arrays.asList(InstrumentationService.class.getName(), SchedulerService.class.getName(), FileSystemAccessService.class.getName()));\r\n    Configuration hadoopConf = new Configuration(false);\r\n    hadoopConf.set(CommonConfigurationKeysPublic.FS_DEFAULT_NAME_KEY, TestHdfsHelper.getHdfsConf().get(CommonConfigurationKeysPublic.FS_DEFAULT_NAME_KEY));\r\n    createHadoopConf(hadoopConf);\r\n    Configuration conf = new Configuration(false);\r\n    conf.set(\"server.services\", services);\r\n    conf.set(\"server.hadoop.filesystem.cache.purge.timeout\", \"0\");\r\n    Server server = new Server(\"server\", dir, dir, dir, dir, conf);\r\n    server.init();\r\n    FileSystemAccess hadoop = server.get(FileSystemAccess.class);\r\n    final FileSystem[] fsa = new FileSystem[1];\r\n    hadoop.execute(\"u\", hadoop.getFileSystemConfiguration(), new FileSystemAccess.FileSystemExecutor<Void>() {\r\n\r\n        @Override\r\n        public Void execute(FileSystem fs) throws IOException {\r\n            fs.mkdirs(new Path(\"/tmp/foo\"));\r\n            fsa[0] = fs;\r\n            return null;\r\n        }\r\n    });\r\n    try {\r\n        fsa[0].mkdirs(new Path(\"/tmp/foo\"));\r\n        Assert.fail();\r\n    } catch (IOException ex) {\r\n    } catch (Exception ex) {\r\n        Assert.fail();\r\n    }\r\n    server.destroy();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 1,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\test\\java\\org\\apache\\hadoop\\lib\\service\\hadoop",
  "methodName" : "fileSystemExecutorNoNameNode",
  "errType" : null,
  "containingMethodsNum" : 10,
  "sourceCodeText" : "void fileSystemExecutorNoNameNode() throws Exception\n{\r\n    String dir = TestDirHelper.getTestDir().getAbsolutePath();\r\n    String services = StringUtils.join(\",\", Arrays.asList(InstrumentationService.class.getName(), SchedulerService.class.getName(), FileSystemAccessService.class.getName()));\r\n    Configuration hadoopConf = new Configuration(false);\r\n    hadoopConf.set(CommonConfigurationKeysPublic.FS_DEFAULT_NAME_KEY, TestHdfsHelper.getHdfsConf().get(CommonConfigurationKeysPublic.FS_DEFAULT_NAME_KEY));\r\n    createHadoopConf(hadoopConf);\r\n    Configuration conf = new Configuration(false);\r\n    conf.set(\"server.services\", services);\r\n    Server server = new Server(\"server\", dir, dir, dir, dir, conf);\r\n    server.init();\r\n    FileSystemAccess fsAccess = server.get(FileSystemAccess.class);\r\n    Configuration hdfsConf = fsAccess.getFileSystemConfiguration();\r\n    hdfsConf.set(CommonConfigurationKeysPublic.FS_DEFAULT_NAME_KEY, \"\");\r\n    fsAccess.execute(\"u\", hdfsConf, new FileSystemAccess.FileSystemExecutor<Void>() {\r\n\r\n        @Override\r\n        public Void execute(FileSystem fs) throws IOException {\r\n            return null;\r\n        }\r\n    });\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\test\\java\\org\\apache\\hadoop\\lib\\service\\hadoop",
  "methodName" : "fileSystemExecutorException",
  "errType" : [ "FileSystemAccessException", "Exception", "IOException", "Exception" ],
  "containingMethodsNum" : 16,
  "sourceCodeText" : "void fileSystemExecutorException() throws Exception\n{\r\n    String dir = TestDirHelper.getTestDir().getAbsolutePath();\r\n    String services = StringUtils.join(\",\", Arrays.asList(InstrumentationService.class.getName(), SchedulerService.class.getName(), FileSystemAccessService.class.getName()));\r\n    Configuration hadoopConf = new Configuration(false);\r\n    hadoopConf.set(CommonConfigurationKeysPublic.FS_DEFAULT_NAME_KEY, TestHdfsHelper.getHdfsConf().get(CommonConfigurationKeysPublic.FS_DEFAULT_NAME_KEY));\r\n    createHadoopConf(hadoopConf);\r\n    Configuration conf = new Configuration(false);\r\n    conf.set(\"server.services\", services);\r\n    conf.set(\"server.hadoop.filesystem.cache.purge.timeout\", \"0\");\r\n    Server server = new Server(\"server\", dir, dir, dir, dir, conf);\r\n    server.init();\r\n    FileSystemAccess hadoop = server.get(FileSystemAccess.class);\r\n    final FileSystem[] fsa = new FileSystem[1];\r\n    try {\r\n        hadoop.execute(\"u\", hadoop.getFileSystemConfiguration(), new FileSystemAccess.FileSystemExecutor<Void>() {\r\n\r\n            @Override\r\n            public Void execute(FileSystem fs) throws IOException {\r\n                fsa[0] = fs;\r\n                throw new IOException();\r\n            }\r\n        });\r\n        Assert.fail();\r\n    } catch (FileSystemAccessException ex) {\r\n        Assert.assertEquals(ex.getError(), FileSystemAccessException.ERROR.H03);\r\n    } catch (Exception ex) {\r\n        Assert.fail();\r\n    }\r\n    try {\r\n        fsa[0].mkdirs(new Path(\"/tmp/foo\"));\r\n        Assert.fail();\r\n    } catch (IOException ex) {\r\n    } catch (Exception ex) {\r\n        Assert.fail();\r\n    }\r\n    server.destroy();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 2,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\test\\java\\org\\apache\\hadoop\\lib\\service\\hadoop",
  "methodName" : "fileSystemCache",
  "errType" : [ "IOException", "Exception" ],
  "containingMethodsNum" : 26,
  "sourceCodeText" : "void fileSystemCache() throws Exception\n{\r\n    String dir = TestDirHelper.getTestDir().getAbsolutePath();\r\n    String services = StringUtils.join(\",\", Arrays.asList(InstrumentationService.class.getName(), SchedulerService.class.getName(), FileSystemAccessService.class.getName()));\r\n    Configuration hadoopConf = new Configuration(false);\r\n    hadoopConf.set(CommonConfigurationKeysPublic.FS_DEFAULT_NAME_KEY, TestHdfsHelper.getHdfsConf().get(CommonConfigurationKeysPublic.FS_DEFAULT_NAME_KEY));\r\n    createHadoopConf(hadoopConf);\r\n    Configuration conf = new Configuration(false);\r\n    conf.set(\"server.services\", services);\r\n    conf.set(\"server.hadoop.filesystem.cache.purge.frequency\", \"1\");\r\n    conf.set(\"server.hadoop.filesystem.cache.purge.timeout\", \"1\");\r\n    Server server = new Server(\"server\", dir, dir, dir, dir, conf);\r\n    try {\r\n        server.init();\r\n        FileSystemAccess hadoop = server.get(FileSystemAccess.class);\r\n        FileSystem fs1 = hadoop.createFileSystem(\"u\", hadoop.getFileSystemConfiguration());\r\n        Assert.assertNotNull(fs1);\r\n        fs1.mkdirs(new Path(\"/tmp/foo1\"));\r\n        hadoop.releaseFileSystem(fs1);\r\n        fs1.mkdirs(new Path(\"/tmp/foo2\"));\r\n        FileSystem fs2 = hadoop.createFileSystem(\"u\", hadoop.getFileSystemConfiguration());\r\n        Assert.assertEquals(fs1, fs2);\r\n        Thread.sleep(4 * 1000);\r\n        fs1.mkdirs(new Path(\"/tmp/foo2\"));\r\n        Thread.sleep(4 * 1000);\r\n        fs2.mkdirs(new Path(\"/tmp/foo\"));\r\n        hadoop.releaseFileSystem(fs2);\r\n        Thread.sleep(4 * 1000);\r\n        try {\r\n            fs2.mkdirs(new Path(\"/tmp/foo\"));\r\n            Assert.fail();\r\n        } catch (IOException ex) {\r\n        } catch (Exception ex) {\r\n            Assert.fail();\r\n        }\r\n    } finally {\r\n        server.destroy();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 2,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\test\\java\\org\\apache\\hadoop\\test",
  "methodName" : "getHadoopProxyUser",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String getHadoopProxyUser()\n{\r\n    return System.getProperty(HADOOP_PROXYUSER, System.getProperty(\"user.name\"));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\test\\java\\org\\apache\\hadoop\\test",
  "methodName" : "getHadoopProxyUserHosts",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String getHadoopProxyUserHosts()\n{\r\n    return System.getProperty(HADOOP_PROXYUSER_HOSTS, \"*\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\test\\java\\org\\apache\\hadoop\\test",
  "methodName" : "getHadoopProxyUserGroups",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "String getHadoopProxyUserGroups()\n{\r\n    return System.getProperty(HADOOP_PROXYUSER_GROUPS, \"*\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\test\\java\\org\\apache\\hadoop\\test",
  "methodName" : "getHadoopUsers",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "String[] getHadoopUsers()\n{\r\n    List<String> users = new ArrayList<String>();\r\n    for (String name : System.getProperties().stringPropertyNames()) {\r\n        if (name.startsWith(HADOOP_USER_PREFIX)) {\r\n            users.add(name.substring(HADOOP_USER_PREFIX.length()));\r\n        }\r\n    }\r\n    return (users.size() != 0) ? users.toArray(new String[users.size()]) : DEFAULT_USERS;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\test\\java\\org\\apache\\hadoop\\test",
  "methodName" : "getHadoopUserGroups",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "String[] getHadoopUserGroups(String user)\n{\r\n    if (getHadoopUsers() == DEFAULT_USERS) {\r\n        for (String defaultUser : DEFAULT_USERS) {\r\n            if (defaultUser.equals(user)) {\r\n                return DEFAULT_USERS_GROUP;\r\n            }\r\n        }\r\n        return new String[0];\r\n    } else {\r\n        String groups = System.getProperty(HADOOP_USER_PREFIX + user);\r\n        return (groups != null) ? groups.split(\",\") : new String[0];\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\test\\java\\org\\apache\\hadoop\\test",
  "methodName" : "getBaseConf",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "Configuration getBaseConf()\n{\r\n    Configuration conf = new Configuration();\r\n    for (String name : System.getProperties().stringPropertyNames()) {\r\n        conf.set(name, System.getProperty(name));\r\n    }\r\n    return conf;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\test\\java\\org\\apache\\hadoop\\test",
  "methodName" : "addUserConf",
  "errType" : null,
  "containingMethodsNum" : 6,
  "sourceCodeText" : "void addUserConf(Configuration conf)\n{\r\n    conf.set(\"hadoop.security.authentication\", \"simple\");\r\n    conf.set(\"hadoop.proxyuser.\" + HadoopUsersConfTestHelper.getHadoopProxyUser() + \".hosts\", HadoopUsersConfTestHelper.getHadoopProxyUserHosts());\r\n    conf.set(\"hadoop.proxyuser.\" + HadoopUsersConfTestHelper.getHadoopProxyUser() + \".groups\", HadoopUsersConfTestHelper.getHadoopProxyUserGroups());\r\n    for (String user : HadoopUsersConfTestHelper.getHadoopUsers()) {\r\n        String[] groups = HadoopUsersConfTestHelper.getHadoopUserGroups(user);\r\n        UserGroupInformation.createUserForTesting(user, groups);\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\test\\java\\org\\apache\\hadoop\\lib\\lang",
  "methodName" : "testXException",
  "errType" : null,
  "containingMethodsNum" : 12,
  "sourceCodeText" : "void testXException() throws Exception\n{\r\n    XException ex = new XException(TestERROR.TC);\r\n    assertEquals(ex.getError(), TestERROR.TC);\r\n    assertEquals(ex.getMessage(), \"TC: {0}\");\r\n    assertNull(ex.getCause());\r\n    ex = new XException(TestERROR.TC, \"msg\");\r\n    assertEquals(ex.getError(), TestERROR.TC);\r\n    assertEquals(ex.getMessage(), \"TC: msg\");\r\n    assertNull(ex.getCause());\r\n    Exception cause = new Exception();\r\n    ex = new XException(TestERROR.TC, cause);\r\n    assertEquals(ex.getError(), TestERROR.TC);\r\n    assertEquals(ex.getMessage(), \"TC: \" + cause.toString());\r\n    assertEquals(ex.getCause(), cause);\r\n    XException xcause = ex;\r\n    ex = new XException(xcause);\r\n    assertEquals(ex.getError(), TestERROR.TC);\r\n    assertEquals(ex.getMessage(), xcause.getMessage());\r\n    assertEquals(ex.getCause(), xcause);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\test\\java\\org\\apache\\hadoop\\lib\\util",
  "methodName" : "notNullNotNull",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void notNullNotNull()\n{\r\n    assertEquals(Check.notNull(\"value\", \"name\"), \"value\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\test\\java\\org\\apache\\hadoop\\lib\\util",
  "methodName" : "notNullNull",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void notNullNull()\n{\r\n    Check.notNull(null, \"name\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\test\\java\\org\\apache\\hadoop\\lib\\util",
  "methodName" : "notNullElementsNotNull",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void notNullElementsNotNull()\n{\r\n    Check.notNullElements(new ArrayList<String>(), \"name\");\r\n    Check.notNullElements(Arrays.asList(\"a\"), \"name\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\test\\java\\org\\apache\\hadoop\\lib\\util",
  "methodName" : "notNullElementsNullList",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void notNullElementsNullList()\n{\r\n    Check.notNullElements(null, \"name\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\test\\java\\org\\apache\\hadoop\\lib\\util",
  "methodName" : "notNullElementsNullElements",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void notNullElementsNullElements()\n{\r\n    Check.notNullElements(Arrays.asList(\"a\", \"\", null), \"name\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\test\\java\\org\\apache\\hadoop\\lib\\util",
  "methodName" : "notEmptyElementsNotNull",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void notEmptyElementsNotNull()\n{\r\n    Check.notEmptyElements(new ArrayList<String>(), \"name\");\r\n    Check.notEmptyElements(Arrays.asList(\"a\"), \"name\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\test\\java\\org\\apache\\hadoop\\lib\\util",
  "methodName" : "notEmptyElementsNullList",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void notEmptyElementsNullList()\n{\r\n    Check.notEmptyElements(null, \"name\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\test\\java\\org\\apache\\hadoop\\lib\\util",
  "methodName" : "notEmptyElementsNullElements",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void notEmptyElementsNullElements()\n{\r\n    Check.notEmptyElements(Arrays.asList(\"a\", null), \"name\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\test\\java\\org\\apache\\hadoop\\lib\\util",
  "methodName" : "notEmptyElementsEmptyElements",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void notEmptyElementsEmptyElements()\n{\r\n    Check.notEmptyElements(Arrays.asList(\"a\", \"\"), \"name\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\test\\java\\org\\apache\\hadoop\\lib\\util",
  "methodName" : "notEmptyNotEmtpy",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void notEmptyNotEmtpy()\n{\r\n    assertEquals(Check.notEmpty(\"value\", \"name\"), \"value\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\test\\java\\org\\apache\\hadoop\\lib\\util",
  "methodName" : "notEmptyNull",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void notEmptyNull()\n{\r\n    Check.notEmpty(null, \"name\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\test\\java\\org\\apache\\hadoop\\lib\\util",
  "methodName" : "notEmptyEmpty",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void notEmptyEmpty()\n{\r\n    Check.notEmpty(\"\", \"name\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\test\\java\\org\\apache\\hadoop\\lib\\util",
  "methodName" : "validIdentifierValid",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void validIdentifierValid() throws Exception\n{\r\n    assertEquals(Check.validIdentifier(\"a\", 1, \"\"), \"a\");\r\n    assertEquals(Check.validIdentifier(\"a1\", 2, \"\"), \"a1\");\r\n    assertEquals(Check.validIdentifier(\"a_\", 3, \"\"), \"a_\");\r\n    assertEquals(Check.validIdentifier(\"_\", 1, \"\"), \"_\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\test\\java\\org\\apache\\hadoop\\lib\\util",
  "methodName" : "validIdentifierInvalid1",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void validIdentifierInvalid1() throws Exception\n{\r\n    Check.validIdentifier(\"!\", 1, \"\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\test\\java\\org\\apache\\hadoop\\lib\\util",
  "methodName" : "validIdentifierInvalid2",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void validIdentifierInvalid2() throws Exception\n{\r\n    Check.validIdentifier(\"a1\", 1, \"\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\test\\java\\org\\apache\\hadoop\\lib\\util",
  "methodName" : "validIdentifierInvalid3",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void validIdentifierInvalid3() throws Exception\n{\r\n    Check.validIdentifier(\"1\", 1, \"\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\test\\java\\org\\apache\\hadoop\\lib\\util",
  "methodName" : "checkGTZeroGreater",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void checkGTZeroGreater()\n{\r\n    assertEquals(Check.gt0(120, \"test\"), 120);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\test\\java\\org\\apache\\hadoop\\lib\\util",
  "methodName" : "checkGTZeroZero",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void checkGTZeroZero()\n{\r\n    Check.gt0(0, \"test\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\test\\java\\org\\apache\\hadoop\\lib\\util",
  "methodName" : "checkGTZeroLessThanZero",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void checkGTZeroLessThanZero()\n{\r\n    Check.gt0(-1, \"test\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\test\\java\\org\\apache\\hadoop\\lib\\util",
  "methodName" : "checkGEZero",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void checkGEZero()\n{\r\n    assertEquals(Check.ge0(120, \"test\"), 120);\r\n    assertEquals(Check.ge0(0, \"test\"), 0);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\test\\java\\org\\apache\\hadoop\\lib\\util",
  "methodName" : "checkGELessThanZero",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void checkGELessThanZero()\n{\r\n    Check.ge0(-1, \"test\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\test\\java\\org\\apache\\hadoop\\lib\\wsrs",
  "methodName" : "test",
  "errType" : [ "IllegalArgumentException", "Exception", "IllegalArgumentException", "Exception" ],
  "containingMethodsNum" : 12,
  "sourceCodeText" : "void test(Param<T> param, String name, String domain, T defaultValue, T validValue, String invalidStrValue, String outOfRangeValue) throws Exception\n{\r\n    assertEquals(name, param.getName());\r\n    assertEquals(domain, param.getDomain());\r\n    assertEquals(defaultValue, param.value());\r\n    assertEquals(defaultValue, param.parseParam(\"\"));\r\n    assertEquals(defaultValue, param.parseParam(null));\r\n    assertEquals(validValue, param.parseParam(validValue.toString()));\r\n    if (invalidStrValue != null) {\r\n        try {\r\n            param.parseParam(invalidStrValue);\r\n            fail();\r\n        } catch (IllegalArgumentException ex) {\r\n        } catch (Exception ex) {\r\n            fail();\r\n        }\r\n    }\r\n    if (outOfRangeValue != null) {\r\n        try {\r\n            param.parseParam(outOfRangeValue);\r\n            fail();\r\n        } catch (IllegalArgumentException ex) {\r\n        } catch (Exception ex) {\r\n            fail();\r\n        }\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 2,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\test\\java\\org\\apache\\hadoop\\lib\\wsrs",
  "methodName" : "testBoolean",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testBoolean() throws Exception\n{\r\n    Param<Boolean> param = new BooleanParam(\"b\", false) {\r\n    };\r\n    test(param, \"b\", \"a boolean\", false, true, \"x\", null);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\test\\java\\org\\apache\\hadoop\\lib\\wsrs",
  "methodName" : "testByte",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testByte() throws Exception\n{\r\n    Param<Byte> param = new ByteParam(\"B\", (byte) 1) {\r\n    };\r\n    test(param, \"B\", \"a byte\", (byte) 1, (byte) 2, \"x\", \"256\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\test\\java\\org\\apache\\hadoop\\lib\\wsrs",
  "methodName" : "testShort",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void testShort() throws Exception\n{\r\n    Param<Short> param = new ShortParam(\"S\", (short) 1) {\r\n    };\r\n    test(param, \"S\", \"a short\", (short) 1, (short) 2, \"x\", \"\" + ((int) Short.MAX_VALUE + 1));\r\n    param = new ShortParam(\"S\", (short) 1, 8) {\r\n    };\r\n    assertEquals(new Short((short) 01777), param.parse(\"01777\"));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\test\\java\\org\\apache\\hadoop\\lib\\wsrs",
  "methodName" : "testInteger",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testInteger() throws Exception\n{\r\n    Param<Integer> param = new IntegerParam(\"I\", 1) {\r\n    };\r\n    test(param, \"I\", \"an integer\", 1, 2, \"x\", \"\" + ((long) Integer.MAX_VALUE + 1));\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\test\\java\\org\\apache\\hadoop\\lib\\wsrs",
  "methodName" : "testLong",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testLong() throws Exception\n{\r\n    Param<Long> param = new LongParam(\"L\", 1L) {\r\n    };\r\n    test(param, \"L\", \"a long\", 1L, 2L, \"x\", null);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\test\\java\\org\\apache\\hadoop\\lib\\wsrs",
  "methodName" : "testEnum",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testEnum() throws Exception\n{\r\n    EnumParam<ENUM> param = new EnumParam<ENUM>(\"e\", ENUM.class, ENUM.FOO) {\r\n    };\r\n    test(param, \"e\", \"FOO,BAR\", ENUM.FOO, ENUM.BAR, \"x\", null);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\test\\java\\org\\apache\\hadoop\\lib\\wsrs",
  "methodName" : "testString",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testString() throws Exception\n{\r\n    Param<String> param = new StringParam(\"s\", \"foo\") {\r\n    };\r\n    test(param, \"s\", \"a string\", \"foo\", \"bar\", null, null);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\test\\java\\org\\apache\\hadoop\\lib\\wsrs",
  "methodName" : "testRegEx",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void testRegEx() throws Exception\n{\r\n    Param<String> param = new StringParam(\"r\", \"aa\", Pattern.compile(\"..\")) {\r\n    };\r\n    test(param, \"r\", \"..\", \"aa\", \"bb\", \"c\", null);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\test\\java\\org\\apache\\hadoop\\lib\\lang",
  "methodName" : "runnable",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void runnable() throws Exception\n{\r\n    R r = new R();\r\n    RunnableCallable rc = new RunnableCallable(r);\r\n    rc.run();\r\n    assertTrue(r.RUN);\r\n    r = new R();\r\n    rc = new RunnableCallable(r);\r\n    rc.call();\r\n    assertTrue(r.RUN);\r\n    assertEquals(rc.toString(), \"R\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\test\\java\\org\\apache\\hadoop\\lib\\lang",
  "methodName" : "callable",
  "errType" : null,
  "containingMethodsNum" : 5,
  "sourceCodeText" : "void callable() throws Exception\n{\r\n    C c = new C();\r\n    RunnableCallable rc = new RunnableCallable(c);\r\n    rc.run();\r\n    assertTrue(c.RUN);\r\n    c = new C();\r\n    rc = new RunnableCallable(c);\r\n    rc.call();\r\n    assertTrue(c.RUN);\r\n    assertEquals(rc.toString(), \"C\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\test\\java\\org\\apache\\hadoop\\lib\\lang",
  "methodName" : "callableExRun",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void callableExRun() throws Exception\n{\r\n    CEx c = new CEx();\r\n    RunnableCallable rc = new RunnableCallable(c);\r\n    rc.run();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\test\\java\\org\\apache\\hadoop\\fs\\http\\server",
  "methodName" : "resetUGI",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void resetUGI()\n{\r\n    Configuration conf = new Configuration();\r\n    UserGroupInformation.setConfiguration(conf);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\test\\java\\org\\apache\\hadoop\\fs\\http\\server",
  "methodName" : "createHttpFSServer",
  "errType" : null,
  "containingMethodsNum" : 25,
  "sourceCodeText" : "void createHttpFSServer() throws Exception\n{\r\n    File homeDir = TestDirHelper.getTestDir();\r\n    Assert.assertTrue(new File(homeDir, \"conf\").mkdir());\r\n    Assert.assertTrue(new File(homeDir, \"log\").mkdir());\r\n    Assert.assertTrue(new File(homeDir, \"temp\").mkdir());\r\n    HttpFSServerWebApp.setHomeDirForCurrentThread(homeDir.getAbsolutePath());\r\n    File secretFile = new File(new File(homeDir, \"conf\"), \"secret\");\r\n    Writer w = new FileWriter(secretFile);\r\n    w.write(\"secret\");\r\n    w.close();\r\n    File hadoopConfDir = new File(new File(homeDir, \"conf\"), \"hadoop-conf\");\r\n    hadoopConfDir.mkdirs();\r\n    String fsDefaultName = TestHdfsHelper.getHdfsConf().get(CommonConfigurationKeysPublic.FS_DEFAULT_NAME_KEY);\r\n    Configuration conf = new Configuration(false);\r\n    conf.set(CommonConfigurationKeysPublic.FS_DEFAULT_NAME_KEY, fsDefaultName);\r\n    File hdfsSite = new File(hadoopConfDir, \"hdfs-site.xml\");\r\n    OutputStream os = new FileOutputStream(hdfsSite);\r\n    conf.writeXml(os);\r\n    os.close();\r\n    conf = new Configuration(false);\r\n    conf.set(\"httpfs.proxyuser.client.hosts\", \"*\");\r\n    conf.set(\"httpfs.proxyuser.client.groups\", \"*\");\r\n    conf.set(\"httpfs.authentication.type\", \"kerberos\");\r\n    conf.set(\"httpfs.authentication.signature.secret.file\", secretFile.getAbsolutePath());\r\n    File httpfsSite = new File(new File(homeDir, \"conf\"), \"httpfs-site.xml\");\r\n    os = new FileOutputStream(httpfsSite);\r\n    conf.writeXml(os);\r\n    os.close();\r\n    ClassLoader cl = Thread.currentThread().getContextClassLoader();\r\n    URL url = cl.getResource(\"webapp\");\r\n    WebAppContext context = new WebAppContext(url.getPath(), \"/webhdfs\");\r\n    Server server = TestJettyHelper.getJettyServer();\r\n    server.setHandler(context);\r\n    server.start();\r\n    HttpFSServerWebApp.get().setAuthority(TestJettyHelper.getAuthority());\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\test\\java\\org\\apache\\hadoop\\fs\\http\\server",
  "methodName" : "testValidHttpFSAccess",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void testValidHttpFSAccess() throws Exception\n{\r\n    createHttpFSServer();\r\n    KerberosTestUtils.doAsClient(new Callable<Void>() {\r\n\r\n        @Override\r\n        public Void call() throws Exception {\r\n            URL url = new URL(TestJettyHelper.getJettyURL(), \"/webhdfs/v1/?op=GETHOMEDIRECTORY\");\r\n            AuthenticatedURL aUrl = new AuthenticatedURL();\r\n            AuthenticatedURL.Token aToken = new AuthenticatedURL.Token();\r\n            HttpURLConnection conn = aUrl.openConnection(url, aToken);\r\n            Assert.assertEquals(conn.getResponseCode(), HttpURLConnection.HTTP_OK);\r\n            return null;\r\n        }\r\n    });\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\test\\java\\org\\apache\\hadoop\\fs\\http\\server",
  "methodName" : "testInvalidadHttpFSAccess",
  "errType" : null,
  "containingMethodsNum" : 4,
  "sourceCodeText" : "void testInvalidadHttpFSAccess() throws Exception\n{\r\n    createHttpFSServer();\r\n    URL url = new URL(TestJettyHelper.getJettyURL(), \"/webhdfs/v1/?op=GETHOMEDIRECTORY\");\r\n    HttpURLConnection conn = (HttpURLConnection) url.openConnection();\r\n    Assert.assertEquals(conn.getResponseCode(), HttpURLConnection.HTTP_UNAUTHORIZED);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\test\\java\\org\\apache\\hadoop\\fs\\http\\server",
  "methodName" : "testDelegationTokenHttpFSAccess",
  "errType" : null,
  "containingMethodsNum" : 2,
  "sourceCodeText" : "void testDelegationTokenHttpFSAccess() throws Exception\n{\r\n    createHttpFSServer();\r\n    KerberosTestUtils.doAsClient(new Callable<Void>() {\r\n\r\n        @Override\r\n        public Void call() throws Exception {\r\n            URL url = new URL(TestJettyHelper.getJettyURL(), \"/webhdfs/v1/?op=GETDELEGATIONTOKEN\");\r\n            AuthenticatedURL aUrl = new AuthenticatedURL();\r\n            AuthenticatedURL.Token aToken = new AuthenticatedURL.Token();\r\n            HttpURLConnection conn = aUrl.openConnection(url, aToken);\r\n            Assert.assertEquals(conn.getResponseCode(), HttpURLConnection.HTTP_OK);\r\n            JSONObject json = (JSONObject) new JSONParser().parse(new InputStreamReader(conn.getInputStream()));\r\n            json = (JSONObject) json.get(DelegationTokenAuthenticator.DELEGATION_TOKEN_JSON);\r\n            String tokenStr = (String) json.get(DelegationTokenAuthenticator.DELEGATION_TOKEN_URL_STRING_JSON);\r\n            url = new URL(TestJettyHelper.getJettyURL(), \"/webhdfs/v1/?op=GETHOMEDIRECTORY&delegation=\" + tokenStr);\r\n            conn = (HttpURLConnection) url.openConnection();\r\n            Assert.assertEquals(conn.getResponseCode(), HttpURLConnection.HTTP_OK);\r\n            url = new URL(TestJettyHelper.getJettyURL(), \"/webhdfs/v1/?op=RENEWDELEGATIONTOKEN&token=\" + tokenStr);\r\n            conn = (HttpURLConnection) url.openConnection();\r\n            conn.setRequestMethod(\"PUT\");\r\n            Assert.assertEquals(conn.getResponseCode(), HttpURLConnection.HTTP_UNAUTHORIZED);\r\n            url = new URL(TestJettyHelper.getJettyURL(), \"/webhdfs/v1/?op=RENEWDELEGATIONTOKEN&token=\" + tokenStr);\r\n            conn = aUrl.openConnection(url, aToken);\r\n            conn.setRequestMethod(\"PUT\");\r\n            Assert.assertEquals(conn.getResponseCode(), HttpURLConnection.HTTP_OK);\r\n            url = new URL(TestJettyHelper.getJettyURL(), \"/webhdfs/v1/?op=CANCELDELEGATIONTOKEN&token=\" + tokenStr);\r\n            conn = (HttpURLConnection) url.openConnection();\r\n            conn.setRequestMethod(\"PUT\");\r\n            Assert.assertEquals(conn.getResponseCode(), HttpURLConnection.HTTP_OK);\r\n            url = new URL(TestJettyHelper.getJettyURL(), \"/webhdfs/v1/?op=GETHOMEDIRECTORY&delegation=\" + tokenStr);\r\n            conn = (HttpURLConnection) url.openConnection();\r\n            Assert.assertEquals(conn.getResponseCode(), HttpURLConnection.HTTP_UNAUTHORIZED);\r\n            return null;\r\n        }\r\n    });\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\test\\java\\org\\apache\\hadoop\\fs\\http\\server",
  "methodName" : "testDelegationTokenWithFS",
  "errType" : null,
  "containingMethodsNum" : 12,
  "sourceCodeText" : "void testDelegationTokenWithFS(Class fileSystemClass) throws Exception\n{\r\n    createHttpFSServer();\r\n    Configuration conf = new Configuration();\r\n    conf.set(\"fs.webhdfs.impl\", fileSystemClass.getName());\r\n    conf.set(\"fs.hdfs.impl.disable.cache\", \"true\");\r\n    URI uri = new URI(\"webhdfs://\" + TestJettyHelper.getJettyURL().toURI().getAuthority());\r\n    FileSystem fs = FileSystem.get(uri, conf);\r\n    Token<?>[] tokens = fs.addDelegationTokens(\"foo\", null);\r\n    fs.close();\r\n    Assert.assertEquals(1, tokens.length);\r\n    fs = FileSystem.get(uri, conf);\r\n    ((DelegationTokenRenewer.Renewable) fs).setDelegationToken(tokens[0]);\r\n    fs.listStatus(new Path(\"/\"));\r\n    fs.close();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\test\\java\\org\\apache\\hadoop\\fs\\http\\server",
  "methodName" : "testDelegationTokenWithinDoAs",
  "errType" : null,
  "containingMethodsNum" : 7,
  "sourceCodeText" : "void testDelegationTokenWithinDoAs(final Class fileSystemClass, boolean proxyUser) throws Exception\n{\r\n    Configuration conf = new Configuration();\r\n    conf.set(\"hadoop.security.authentication\", \"kerberos\");\r\n    UserGroupInformation.setConfiguration(conf);\r\n    UserGroupInformation.loginUserFromKeytab(\"client\", \"/Users/tucu/tucu.keytab\");\r\n    UserGroupInformation ugi = UserGroupInformation.getLoginUser();\r\n    if (proxyUser) {\r\n        ugi = UserGroupInformation.createProxyUser(\"foo\", ugi);\r\n    }\r\n    conf = new Configuration();\r\n    UserGroupInformation.setConfiguration(conf);\r\n    ugi.doAs(new PrivilegedExceptionAction<Void>() {\r\n\r\n        @Override\r\n        public Void run() throws Exception {\r\n            testDelegationTokenWithFS(fileSystemClass);\r\n            return null;\r\n        }\r\n    });\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\test\\java\\org\\apache\\hadoop\\fs\\http\\server",
  "methodName" : "testDelegationTokenWithHttpFSFileSystem",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testDelegationTokenWithHttpFSFileSystem() throws Exception\n{\r\n    testDelegationTokenWithinDoAs(HttpFSFileSystem.class, false);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\test\\java\\org\\apache\\hadoop\\fs\\http\\server",
  "methodName" : "testDelegationTokenWithWebhdfsFileSystem",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testDelegationTokenWithWebhdfsFileSystem() throws Exception\n{\r\n    testDelegationTokenWithinDoAs(WebHdfsFileSystem.class, false);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\test\\java\\org\\apache\\hadoop\\fs\\http\\server",
  "methodName" : "testDelegationTokenWithHttpFSFileSystemProxyUser",
  "errType" : null,
  "containingMethodsNum" : 1,
  "sourceCodeText" : "void testDelegationTokenWithHttpFSFileSystemProxyUser() throws Exception\n{\r\n    testDelegationTokenWithinDoAs(HttpFSFileSystem.class, true);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\test\\java\\org\\apache\\hadoop\\lib\\wsrs",
  "methodName" : "test",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void test() throws Exception\n{\r\n    InputStream is = new ByteArrayInputStream(\"abc\".getBytes());\r\n    ByteArrayOutputStream baos = new ByteArrayOutputStream();\r\n    InputStreamEntity i = new InputStreamEntity(is);\r\n    i.write(baos);\r\n    baos.close();\r\n    assertEquals(new String(baos.toByteArray()), \"abc\");\r\n    is = new ByteArrayInputStream(\"abc\".getBytes());\r\n    baos = new ByteArrayOutputStream();\r\n    i = new InputStreamEntity(is, 1, 1);\r\n    i.write(baos);\r\n    baos.close();\r\n    assertEquals(baos.toByteArray()[0], 'b');\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\test\\java\\org\\apache\\hadoop\\lib\\service\\instrumentation",
  "methodName" : "getWaitForRatio",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "float getWaitForRatio()\n{\r\n    return 1;\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\test\\java\\org\\apache\\hadoop\\lib\\service\\instrumentation",
  "methodName" : "cron",
  "errType" : [ "IllegalStateException", "Exception", "IllegalStateException", "Exception" ],
  "containingMethodsNum" : 34,
  "sourceCodeText" : "void cron()\n{\r\n    InstrumentationService.Cron cron = new InstrumentationService.Cron();\r\n    assertEquals(cron.start, 0);\r\n    assertEquals(cron.lapStart, 0);\r\n    assertEquals(cron.own, 0);\r\n    assertEquals(cron.total, 0);\r\n    long begin = Time.now();\r\n    assertEquals(cron.start(), cron);\r\n    assertEquals(cron.start(), cron);\r\n    assertEquals(cron.start, begin, 20);\r\n    assertEquals(cron.start, cron.lapStart);\r\n    sleep(100);\r\n    assertEquals(cron.stop(), cron);\r\n    long end = Time.now();\r\n    long delta = end - begin;\r\n    assertEquals(cron.own, delta, 20);\r\n    assertEquals(cron.total, 0);\r\n    assertEquals(cron.lapStart, 0);\r\n    sleep(100);\r\n    long reStart = Time.now();\r\n    cron.start();\r\n    assertEquals(cron.start, begin, 20);\r\n    assertEquals(cron.lapStart, reStart, 20);\r\n    sleep(100);\r\n    cron.stop();\r\n    long reEnd = Time.now();\r\n    delta += reEnd - reStart;\r\n    assertEquals(cron.own, delta, 20);\r\n    assertEquals(cron.total, 0);\r\n    assertEquals(cron.lapStart, 0);\r\n    cron.end();\r\n    assertEquals(cron.total, reEnd - begin, 20);\r\n    try {\r\n        cron.start();\r\n        fail();\r\n    } catch (IllegalStateException ex) {\r\n    } catch (Exception ex) {\r\n        fail();\r\n    }\r\n    try {\r\n        cron.stop();\r\n        fail();\r\n    } catch (IllegalStateException ex) {\r\n    } catch (Exception ex) {\r\n        fail();\r\n    }\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 2,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\test\\java\\org\\apache\\hadoop\\lib\\service\\instrumentation",
  "methodName" : "timer",
  "errType" : null,
  "containingMethodsNum" : 66,
  "sourceCodeText" : "void timer() throws Exception\n{\r\n    InstrumentationService.Timer timer = new InstrumentationService.Timer(2);\r\n    InstrumentationService.Cron cron = new InstrumentationService.Cron();\r\n    long ownStart;\r\n    long ownEnd;\r\n    long totalStart;\r\n    long totalEnd;\r\n    long ownDelta;\r\n    long totalDelta;\r\n    long avgTotal;\r\n    long avgOwn;\r\n    cron.start();\r\n    ownStart = Time.now();\r\n    totalStart = ownStart;\r\n    ownDelta = 0;\r\n    sleep(100);\r\n    cron.stop();\r\n    ownEnd = Time.now();\r\n    ownDelta += ownEnd - ownStart;\r\n    sleep(100);\r\n    cron.start();\r\n    ownStart = Time.now();\r\n    sleep(100);\r\n    cron.stop();\r\n    ownEnd = Time.now();\r\n    ownDelta += ownEnd - ownStart;\r\n    totalEnd = ownEnd;\r\n    totalDelta = totalEnd - totalStart;\r\n    avgTotal = totalDelta;\r\n    avgOwn = ownDelta;\r\n    timer.addCron(cron);\r\n    long[] values = timer.getValues();\r\n    assertEquals(values[InstrumentationService.Timer.LAST_TOTAL], totalDelta, 20);\r\n    assertEquals(values[InstrumentationService.Timer.LAST_OWN], ownDelta, 20);\r\n    assertEquals(values[InstrumentationService.Timer.AVG_TOTAL], avgTotal, 20);\r\n    assertEquals(values[InstrumentationService.Timer.AVG_OWN], avgOwn, 20);\r\n    cron = new InstrumentationService.Cron();\r\n    cron.start();\r\n    ownStart = Time.now();\r\n    totalStart = ownStart;\r\n    ownDelta = 0;\r\n    sleep(200);\r\n    cron.stop();\r\n    ownEnd = Time.now();\r\n    ownDelta += ownEnd - ownStart;\r\n    sleep(200);\r\n    cron.start();\r\n    ownStart = Time.now();\r\n    sleep(200);\r\n    cron.stop();\r\n    ownEnd = Time.now();\r\n    ownDelta += ownEnd - ownStart;\r\n    totalEnd = ownEnd;\r\n    totalDelta = totalEnd - totalStart;\r\n    avgTotal = (avgTotal * 1 + totalDelta) / 2;\r\n    avgOwn = (avgOwn * 1 + ownDelta) / 2;\r\n    timer.addCron(cron);\r\n    values = timer.getValues();\r\n    assertEquals(values[InstrumentationService.Timer.LAST_TOTAL], totalDelta, 20);\r\n    assertEquals(values[InstrumentationService.Timer.LAST_OWN], ownDelta, 20);\r\n    assertEquals(values[InstrumentationService.Timer.AVG_TOTAL], avgTotal, 20);\r\n    assertEquals(values[InstrumentationService.Timer.AVG_OWN], avgOwn, 20);\r\n    avgTotal = totalDelta;\r\n    avgOwn = ownDelta;\r\n    cron = new InstrumentationService.Cron();\r\n    cron.start();\r\n    ownStart = Time.now();\r\n    totalStart = ownStart;\r\n    ownDelta = 0;\r\n    sleep(300);\r\n    cron.stop();\r\n    ownEnd = Time.now();\r\n    ownDelta += ownEnd - ownStart;\r\n    sleep(300);\r\n    cron.start();\r\n    ownStart = Time.now();\r\n    sleep(300);\r\n    cron.stop();\r\n    ownEnd = Time.now();\r\n    ownDelta += ownEnd - ownStart;\r\n    totalEnd = ownEnd;\r\n    totalDelta = totalEnd - totalStart;\r\n    avgTotal = (avgTotal * 1 + totalDelta) / 2;\r\n    avgOwn = (avgOwn * 1 + ownDelta) / 2;\r\n    cron.stop();\r\n    timer.addCron(cron);\r\n    values = timer.getValues();\r\n    assertEquals(values[InstrumentationService.Timer.LAST_TOTAL], totalDelta, 20);\r\n    assertEquals(values[InstrumentationService.Timer.LAST_OWN], ownDelta, 20);\r\n    assertEquals(values[InstrumentationService.Timer.AVG_TOTAL], avgTotal, 20);\r\n    assertEquals(values[InstrumentationService.Timer.AVG_OWN], avgOwn, 20);\r\n    JSONObject json = (JSONObject) new JSONParser().parse(timer.toJSONString());\r\n    assertEquals(json.size(), 4);\r\n    assertEquals(json.get(\"lastTotal\"), values[InstrumentationService.Timer.LAST_TOTAL]);\r\n    assertEquals(json.get(\"lastOwn\"), values[InstrumentationService.Timer.LAST_OWN]);\r\n    assertEquals(json.get(\"avgTotal\"), values[InstrumentationService.Timer.AVG_TOTAL]);\r\n    assertEquals(json.get(\"avgOwn\"), values[InstrumentationService.Timer.AVG_OWN]);\r\n    StringWriter writer = new StringWriter();\r\n    timer.writeJSONString(writer);\r\n    writer.close();\r\n    json = (JSONObject) new JSONParser().parse(writer.toString());\r\n    assertEquals(json.size(), 4);\r\n    assertEquals(json.get(\"lastTotal\"), values[InstrumentationService.Timer.LAST_TOTAL]);\r\n    assertEquals(json.get(\"lastOwn\"), values[InstrumentationService.Timer.LAST_OWN]);\r\n    assertEquals(json.get(\"avgTotal\"), values[InstrumentationService.Timer.AVG_TOTAL]);\r\n    assertEquals(json.get(\"avgOwn\"), values[InstrumentationService.Timer.AVG_OWN]);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\test\\java\\org\\apache\\hadoop\\lib\\service\\instrumentation",
  "methodName" : "sampler",
  "errType" : null,
  "containingMethodsNum" : 22,
  "sourceCodeText" : "void sampler() throws Exception\n{\r\n    final long[] value = new long[1];\r\n    Instrumentation.Variable<Long> var = new Instrumentation.Variable<Long>() {\r\n\r\n        @Override\r\n        public Long getValue() {\r\n            return value[0];\r\n        }\r\n    };\r\n    InstrumentationService.Sampler sampler = new InstrumentationService.Sampler();\r\n    sampler.init(4, var);\r\n    assertEquals(sampler.getRate(), 0f, 0.0001);\r\n    sampler.sample();\r\n    assertEquals(sampler.getRate(), 0f, 0.0001);\r\n    value[0] = 1;\r\n    sampler.sample();\r\n    assertEquals(sampler.getRate(), (0d + 1) / 2, 0.0001);\r\n    value[0] = 2;\r\n    sampler.sample();\r\n    assertEquals(sampler.getRate(), (0d + 1 + 2) / 3, 0.0001);\r\n    value[0] = 3;\r\n    sampler.sample();\r\n    assertEquals(sampler.getRate(), (0d + 1 + 2 + 3) / 4, 0.0001);\r\n    value[0] = 4;\r\n    sampler.sample();\r\n    assertEquals(sampler.getRate(), (4d + 1 + 2 + 3) / 4, 0.0001);\r\n    JSONObject json = (JSONObject) new JSONParser().parse(sampler.toJSONString());\r\n    assertEquals(json.size(), 2);\r\n    assertEquals(json.get(\"sampler\"), sampler.getRate());\r\n    assertEquals(json.get(\"size\"), 4L);\r\n    StringWriter writer = new StringWriter();\r\n    sampler.writeJSONString(writer);\r\n    writer.close();\r\n    json = (JSONObject) new JSONParser().parse(writer.toString());\r\n    assertEquals(json.size(), 2);\r\n    assertEquals(json.get(\"sampler\"), sampler.getRate());\r\n    assertEquals(json.get(\"size\"), 4L);\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\test\\java\\org\\apache\\hadoop\\lib\\service\\instrumentation",
  "methodName" : "variableHolder",
  "errType" : null,
  "containingMethodsNum" : 8,
  "sourceCodeText" : "void variableHolder() throws Exception\n{\r\n    InstrumentationService.VariableHolder<String> variableHolder = new InstrumentationService.VariableHolder<String>();\r\n    variableHolder.var = new Instrumentation.Variable<String>() {\r\n\r\n        @Override\r\n        public String getValue() {\r\n            return \"foo\";\r\n        }\r\n    };\r\n    JSONObject json = (JSONObject) new JSONParser().parse(variableHolder.toJSONString());\r\n    assertEquals(json.size(), 1);\r\n    assertEquals(json.get(\"value\"), \"foo\");\r\n    StringWriter writer = new StringWriter();\r\n    variableHolder.writeJSONString(writer);\r\n    writer.close();\r\n    json = (JSONObject) new JSONParser().parse(writer.toString());\r\n    assertEquals(json.size(), 1);\r\n    assertEquals(json.get(\"value\"), \"foo\");\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\test\\java\\org\\apache\\hadoop\\lib\\service\\instrumentation",
  "methodName" : "service",
  "errType" : null,
  "containingMethodsNum" : 46,
  "sourceCodeText" : "void service() throws Exception\n{\r\n    String dir = TestDirHelper.getTestDir().getAbsolutePath();\r\n    String services = StringUtils.join(\",\", Arrays.asList(InstrumentationService.class.getName()));\r\n    Configuration conf = new Configuration(false);\r\n    conf.set(\"server.services\", services);\r\n    Server server = new Server(\"server\", dir, dir, dir, dir, conf);\r\n    server.init();\r\n    Instrumentation instrumentation = server.get(Instrumentation.class);\r\n    assertNotNull(instrumentation);\r\n    instrumentation.incr(\"g\", \"c\", 1);\r\n    instrumentation.incr(\"g\", \"c\", 2);\r\n    instrumentation.incr(\"g\", \"c1\", 2);\r\n    Instrumentation.Cron cron = instrumentation.createCron();\r\n    cron.start();\r\n    sleep(100);\r\n    cron.stop();\r\n    instrumentation.addCron(\"g\", \"t\", cron);\r\n    cron = instrumentation.createCron();\r\n    cron.start();\r\n    sleep(200);\r\n    cron.stop();\r\n    instrumentation.addCron(\"g\", \"t\", cron);\r\n    Instrumentation.Variable<String> var = new Instrumentation.Variable<String>() {\r\n\r\n        @Override\r\n        public String getValue() {\r\n            return \"foo\";\r\n        }\r\n    };\r\n    instrumentation.addVariable(\"g\", \"v\", var);\r\n    Instrumentation.Variable<Long> varToSample = new Instrumentation.Variable<Long>() {\r\n\r\n        @Override\r\n        public Long getValue() {\r\n            return 1L;\r\n        }\r\n    };\r\n    instrumentation.addSampler(\"g\", \"s\", 10, varToSample);\r\n    Map<String, ?> snapshot = instrumentation.getSnapshot();\r\n    assertNotNull(snapshot.get(\"os-env\"));\r\n    assertNotNull(snapshot.get(\"sys-props\"));\r\n    assertNotNull(snapshot.get(\"jvm\"));\r\n    assertNotNull(snapshot.get(\"counters\"));\r\n    assertNotNull(snapshot.get(\"timers\"));\r\n    assertNotNull(snapshot.get(\"variables\"));\r\n    assertNotNull(snapshot.get(\"samplers\"));\r\n    assertNotNull(((Map<String, String>) snapshot.get(\"os-env\")).get(\"PATH\"));\r\n    assertNotNull(((Map<String, String>) snapshot.get(\"sys-props\")).get(\"java.version\"));\r\n    assertNotNull(((Map<String, ?>) snapshot.get(\"jvm\")).get(\"free.memory\"));\r\n    assertNotNull(((Map<String, ?>) snapshot.get(\"jvm\")).get(\"max.memory\"));\r\n    assertNotNull(((Map<String, ?>) snapshot.get(\"jvm\")).get(\"total.memory\"));\r\n    assertNotNull(((Map<String, Map<String, Object>>) snapshot.get(\"counters\")).get(\"g\"));\r\n    assertNotNull(((Map<String, Map<String, Object>>) snapshot.get(\"timers\")).get(\"g\"));\r\n    assertNotNull(((Map<String, Map<String, Object>>) snapshot.get(\"variables\")).get(\"g\"));\r\n    assertNotNull(((Map<String, Map<String, Object>>) snapshot.get(\"samplers\")).get(\"g\"));\r\n    assertNotNull(((Map<String, Map<String, Object>>) snapshot.get(\"counters\")).get(\"g\").get(\"c\"));\r\n    assertNotNull(((Map<String, Map<String, Object>>) snapshot.get(\"counters\")).get(\"g\").get(\"c1\"));\r\n    assertNotNull(((Map<String, Map<String, Object>>) snapshot.get(\"timers\")).get(\"g\").get(\"t\"));\r\n    assertNotNull(((Map<String, Map<String, Object>>) snapshot.get(\"variables\")).get(\"g\").get(\"v\"));\r\n    assertNotNull(((Map<String, Map<String, Object>>) snapshot.get(\"samplers\")).get(\"g\").get(\"s\"));\r\n    StringWriter writer = new StringWriter();\r\n    JSONObject.writeJSONString(snapshot, writer);\r\n    writer.close();\r\n    server.destroy();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\test\\java\\org\\apache\\hadoop\\lib\\service\\instrumentation",
  "methodName" : "sampling",
  "errType" : null,
  "containingMethodsNum" : 15,
  "sourceCodeText" : "void sampling() throws Exception\n{\r\n    String dir = TestDirHelper.getTestDir().getAbsolutePath();\r\n    String services = StringUtils.join(\",\", Arrays.asList(InstrumentationService.class.getName(), SchedulerService.class.getName()));\r\n    Configuration conf = new Configuration(false);\r\n    conf.set(\"server.services\", services);\r\n    Server server = new Server(\"server\", dir, dir, dir, dir, conf);\r\n    server.init();\r\n    Instrumentation instrumentation = server.get(Instrumentation.class);\r\n    final AtomicInteger count = new AtomicInteger();\r\n    Instrumentation.Variable<Long> varToSample = new Instrumentation.Variable<Long>() {\r\n\r\n        @Override\r\n        public Long getValue() {\r\n            return (long) count.incrementAndGet();\r\n        }\r\n    };\r\n    instrumentation.addSampler(\"g\", \"s\", 10, varToSample);\r\n    sleep(2000);\r\n    int i = count.get();\r\n    assertTrue(i > 0);\r\n    Map<String, Map<String, ?>> snapshot = instrumentation.getSnapshot();\r\n    Map<String, Map<String, Object>> samplers = (Map<String, Map<String, Object>>) snapshot.get(\"samplers\");\r\n    InstrumentationService.Sampler sampler = (InstrumentationService.Sampler) samplers.get(\"g\").get(\"s\");\r\n    assertTrue(sampler.getRate() > 0);\r\n    server.destroy();\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : true,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\test\\java\\org\\apache\\hadoop\\test",
  "methodName" : "dummy",
  "errType" : null,
  "containingMethodsNum" : 0,
  "sourceCodeText" : "void dummy()\n{\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : false,
  "tryCatchBlockNum" : 0,
  "logged" : false
}, {
  "directory" : "E:\\MyPaper\\hadoop\\hadoop-hdfs-project\\hadoop-hdfs-httpfs\\src\\test\\java\\org\\apache\\hadoop\\test",
  "methodName" : "apply",
  "errType" : [ "Throwable" ],
  "containingMethodsNum" : 11,
  "sourceCodeText" : "Statement apply(final Statement statement, final FrameworkMethod frameworkMethod, final Object o)\n{\r\n    return new Statement() {\r\n\r\n        @Override\r\n        public void evaluate() throws Throwable {\r\n            TestException testExceptionAnnotation = frameworkMethod.getAnnotation(TestException.class);\r\n            try {\r\n                statement.evaluate();\r\n                if (testExceptionAnnotation != null) {\r\n                    Class<? extends Throwable> klass = testExceptionAnnotation.exception();\r\n                    fail(\"Expected Exception: \" + klass.getSimpleName());\r\n                }\r\n            } catch (Throwable ex) {\r\n                if (testExceptionAnnotation != null) {\r\n                    Class<? extends Throwable> klass = testExceptionAnnotation.exception();\r\n                    if (klass.isInstance(ex)) {\r\n                        String regExp = testExceptionAnnotation.msgRegExp();\r\n                        Pattern pattern = Pattern.compile(regExp);\r\n                        if (!pattern.matcher(ex.getMessage()).find()) {\r\n                            fail(\"Expected Exception Message pattern: \" + regExp + \" got message: \" + ex.getMessage());\r\n                        }\r\n                    } else {\r\n                        fail(\"Expected Exception: \" + klass.getSimpleName() + \" got: \" + ex.getClass().getSimpleName());\r\n                    }\r\n                } else {\r\n                    throw ex;\r\n                }\r\n            }\r\n        }\r\n    };\r\n}\n",
  "settingFlag" : false,
  "hasThrow" : false,
  "returnSpecialValue" : true,
  "tryCatchBlockNum" : 1,
  "logged" : false
} ]